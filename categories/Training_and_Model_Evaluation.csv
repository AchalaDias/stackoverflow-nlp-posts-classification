Title,Description,category,combined_text
Multi-Label emotion classification,"<p>i'm working on an Multi-Label Emotion Classification problem to be solved by word2vec. this is my code that i've learned from a couple of tutorials. now the accuracy is very low. about 0.02 which is telling me something is wrong in my code. but i cannot find it. can anyone find any mistake? i tried this code for TF-IDF and BOW (obviously except word2vec part) and i got much better accuracy scores such as 0.28, but it seems this one is somehow wrong:</p>

<pre><code>#Pre-Processor Function
pre_processor = TextPreProcessor(
    omit=['url', 'email', 'percent', 'money', 'phone', 'user',
        'time', 'url', 'date', 'number'],

    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',
        'time', 'url', 'date', 'number'],

    segmenter=""twitter"", 

    corrector=""twitter"", 

    unpack_hashtags=True,
    unpack_contractions=True,

    tokenizer=SocialTokenizer(lowercase=True).tokenize,

    dicts=[emoticons]
)

#Averaging Words Vectors to Create Sentence Embedding
class AverageEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        self.dim =300 # because we use 100 embedding points 

    def fit(self, X, y):
        return self

    def transform(self, X):
        return numpy.array([
            numpy.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [numpy.zeros(self.dim)], axis=0)
            for words in X
        ])

#Loading data
raw_train_tweets = pandas.read_excel('E:\\train.xlsx').iloc[:,1] #Loading all train tweets
train_labels = pandas.read_excel('E:\\train.xlsx').iloc[:,2:13] #Loading corresponding train labels (11 emotions)

raw_test_tweets = pandas.read_excel('E:\\test.xlsx').iloc[:300,1] #Loading 300 test tweets
test_gold_labels = pandas.read_excel('E:\\test.xlsx').iloc[:300,2:13] #Loading corresponding test labels (11 emotions)
print(""please wait"")

#Pre-Processing
train_tweets=[]
test_tweets=[]
for tweets in raw_train_tweets:
    train_tweets.append(pre_processor.pre_process_doc(tweets))
train_tweets=pandas.Series(train_tweets)

for tweets in raw_test_tweets:
    test_tweets.append(pre_processor.pre_process_doc(tweets))
test_tweets=pandas.Series(test_tweets)
all_tweets = train_tweets.append(test_tweets)
all_tweets=(all_tweets.apply(lambda x:str(x).strip().split()))

#Vectorizing 
vector_maker = Word2Vec(all_tweets, size=300, min_count=2, workers=6) #Vectorizer
words_vectors=dict(zip(vector_maker.wv.index2word, vector_maker.wv.vectors))

pipe1=Pipeline([(""wordVectz"",AverageEmbeddingVectorizer(words_vectors)),(""multilabel"",tree.DecisionTreeClassifier())])
pipe1.fit(train_tweets,train_labels)

predicted=pipe1.predict(test_tweets)
print(""Accuracy = "",accuracy_score(test_gold_labels,predicted))
</code></pre>
",Training and Model Evaluation,multi label emotion classification working multi label emotion classification problem solved word vec code learned couple tutorial accuracy low telling something wrong code find anyone find mistake tried code tf idf bow obviously except word vec part got much better accuracy score seems one somehow wrong
AllenNLP all models about ccg_supertagger are unavailable. How to fix or download it?,"<p>I am trying to use AllenNLP models to parse a file to create a CCG dataset, because as a student I can't afford the CCGBank dataset, However I have to, cuz I need a dataset to help me to train a model to resolve syntactic ambiguities, parsing the sentence to ccg format is an inevitable step. I really need the model like
predictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/ccg_supertagger-2020.02.10.tar.gz&quot;)
or if you have better option , I am willing to have a try!
It's my code below</p>
<pre><code>import pandas as pd
from allennlp.predictors.predictor import Predictor
import allennlp_models.tagging

# 读取原始 CSV 文件
input_path = &quot;validation.csv&quot;  # 替换为你的本地路径
df = pd.read_csv(input_path)
sentences = df[&quot;sentence&quot;].tolist()

# 加载 AllenNLP 的预训练 CCG Supertagger 模型
predictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/ccg_supertagger-2020.02.10.tar.gz&quot;)

# 定义预测函数：输入句子，输出 “词/范畴” 序列
def get_ccg_tags(sentence):
    output = predictor.predict(sentence=sentence)
    tokens = output[&quot;words&quot;]
    tags = output[&quot;ccg_tags&quot;]
    tagged = [f&quot;{w}/{t}&quot; for w, t in zip(tokens, tags)]
    return &quot; &quot;.join(tagged)

# 批量处理每个句子，添加 ccg_tags 列
df[&quot;ccg_tags&quot;] = df[&quot;sentence&quot;].apply(get_ccg_tags)

# 保存结果到新文件
output_path = &quot;validation_with_allennlp_ccg.csv&quot;
df.to_csv(output_path, index=False)

print(f&quot; AllenNLP CCG ：{output_path}&quot;)
</code></pre>
",Training and Model Evaluation,allennlp model ccg supertagger unavailable fix download trying use allennlp model parse file create ccg dataset student afford ccgbank dataset however cuz need dataset help train model resolve syntactic ambiguity parsing sentence ccg format inevitable step really need model like predictor predictor path better option willing try code
Training a model to identify names appearing in a sentence,"<p>I have a dataset containing the names of about 238583 people. The names can contain more than one word for example:
 <code>Willie Enriquez , James J Johnson, D.J. Khaled</code>.
 My problem is to identify these names when it appears in a sentence. I am trying to create a machine learning model that can identify if the input is a name or not. My trouble is figuring the input and output of this model. Since I have a bunch of names I can train a model which can recognise a name when the input is a name, but what about the other words that are part of this sentence. The model should also be able to identify words that are not names. Assuming the sentences can have any other words in it, what would be the ideal dataset for this purpose? Does it make sense to train a model on a random bunch of words and tag it as NonNames?<br>
(The entire sentences in which the names appear is not available. The user can type absolutely anything he/she wants)   </p>

<p>Thankyou.</p>
",Training and Model Evaluation,training model identify name appearing sentence dataset containing name people name contain one word example problem identify name appears sentence trying create machine learning model identify input name trouble figuring input output model since bunch name train model recognise name input name word part sentence model also able identify word name assuming sentence word would ideal dataset purpose doe make sense train model random bunch word tag nonnames entire sentence name appear available user type absolutely anything want thankyou
My model&#39;s loss value decreases slowly .how to reduce my loss faster while training?,"<p>when I train the model the loss decrease from 0.9 to 0.5 in 2500 epochs. Is it normal? </p>

<p>my model:</p>

<pre><code>    model = Sequential()

    model.add(Embedding(vocab_size , emd_dim, weights=[emd_matrix], input_length=maxLen,trainable=False))

    model.add(LSTM(256,return_sequences=True,activation=""relu"",kernel_regularizer=regularizers.l2(0.01),kernel_initializer=keras.initializers.glorot_normal(seed=None)))
    model.add(LSTM(256,return_sequences=True,activation=""relu"",kernel_regularizer=regularizers.l2(0.01),kernel_initializer=keras.initializers.glorot_normal(seed=None)))

    model.add(LSTM(256,return_sequences=False,activation=""relu"",kernel_regularizer=regularizers.l2(0.01),kernel_initializer=keras.initializers.glorot_normal(seed=None)))
    model.add(Dense(l_h2i,activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer=""adam"", metrics=['accuracy'])
    filepath = ""F:/checkpoints/""+modelname+""/lstm-{epoch:02d}-{loss:0.3f}-{acc:0.3f}-{val_loss:0.3f}-{val_acc:0.3f}.hdf5""
    checkpoint = ModelCheckpoint(filepath, monitor=""loss"", verbose=1, save_best_only=True, mode='min')
    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=2, min_lr=0.000001)
    print(model.summary())
    history=model.fit(X_train_indices, Y_train_oh, batch_size=batch_size ,
                      epochs=epochs , validation_split=0.1, shuffle=True,
                      callbacks=[checkpoint, reduce_lr])
</code></pre>

<p>some part of the results are as shown here : </p>

<pre><code>loss improved from 0.54275 to 0.54272
loss: 0.5427 - acc: 0.8524 - val_loss: 1.1198 - val_acc: 0.7610

loss improved from 0.54272 to 0.54268
loss: 0.5427 - acc: 0.8525 - val_loss: 1.1195 - val_acc: 0.7311

loss improved from 0.54268 to 0.54251
loss: 0.5425 - acc: 0.8519 - val_loss: 1.1218 - val_acc: 0.7420

loss improved from 0.54251 to 0.54249
loss: 0.5425 - acc: 0.8517 - val_loss: 1.1210 - val_acc: 0.7518
</code></pre>
",Training and Model Evaluation,model loss value decrease slowly reduce loss faster training train model loss decrease epoch normal model part result shown
Fine-tuning a Pretrained Model with Quantization and AMP: Scaler Error &quot;Attempting to Unscale FP16 Gradients&quot;,"<p>I am trying to fine-tune a pretrained model with limited VRAM. To achieve this, I am using quantization and automatic mixed precision (AMP). However, I am encountering an issue that I can't seem to resolve. Could you please help me identify the problem?</p>
<p>Here is a minimal example:</p>
<pre class=""lang-none prettyprint-override""><code>import os
from transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast
import torch
from torch.cuda.amp import GradScaler, autocast

model_name = &quot;facebook/opt-1.3b&quot;
cache_dir = './models'
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;7&quot;

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.float16
)

pretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, 
                                                    cache_dir=cache_dir,                                                     
                                                    quantization_config=quantization_config)
tokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name,
                                                    cache_dir=cache_dir)
optimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4)
scaler = GradScaler()
input_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0)
labels = torch.LongTensor([[1, 2, 3, 4]]).to(0)
with torch.autocast(device_type='cuda'):
    out = pretrained_model(input_ids=input_ids, labels=labels)
    loss = out.loss
scaler.scale(out.loss).backward()
scaler.step(optimizer) 
scaler.update()
optimizer.zero_grad()

print(f'End')
</code></pre>
<p>At the line <code>scaler.step(optimizer)</code>, an error occurs:</p>
<pre><code>Exception has occurred: ValueError: Attempting to unscale FP16 gradients.

</code></pre>
",Training and Model Evaluation,fine tuning pretrained model quantization amp scaler error attempting unscale fp gradient trying fine tune pretrained model limited vram achieve using quantization automatic mixed precision amp however encountering issue seem resolve could please help identify problem minimal example line error occurs
Understanding the Difference Between Entropy and Cross-Entropy in Language Models: Practical Example with Character-Level Unigram Model,"<p>I'm trying to understand the difference between entropy and cross-entropy, as I often hear about the entropy of a language and the cross-entropy of a language model, and I want to understand the link between the two.</p>
<p>To simplify things, let's consider a language (with a vocabulary) and a language model trained on that language.</p>
<p>We'll work at the character level (which gives us 26 characters), and a limited number of words (let's take the 20 names below).</p>
<pre><code>prenoms = [
    &quot;Alice&quot;, &quot;Alfred&quot;, &quot;Alina&quot;, &quot;Aline&quot;, &quot;Alexandre&quot;, 
    &quot;Alicia&quot;, &quot;Alison&quot;, &quot;Alma&quot;, &quot;Alva&quot;, &quot;Elise&quot;, 
    &quot;Elisa&quot;, &quot;Eliane&quot;, &quot;Alain&quot;, &quot;Amélie&quot;, &quot;Arline&quot;, 
    &quot;Olivier&quot;, &quot;Oline&quot;, &quot;Alva&quot;, &quot;Eliott&quot;, &quot;Julien&quot;
]
</code></pre>
<p>How do we calculate the entropy over these 20 names (i.e., the entropy of our language) and the cross-entropy for our language model (let's take a unigram model or any language model you prefer to help me to understand)?</p>
<p>If you have a more relevant example, I’m open to it.</p>
<p>PS: My confusion comes from the fact that, in general definitions, we talk about a (language) distribution P when calculating entropy (without quite knowing how to calculate it), and about two distributions P and Q when calculating cross-entropy (where P is a one-hot encoding vector in this case, when calculating cross-entropy-loss).</p>
<p>PS2:  A python code could help me to well understand, here is my understanding. I based it on y understanding of Jurasky Book (and <a href=""https://huggingface.co/docs/transformers/perplexity"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/perplexity</a>)</p>
<pre class=""lang-py prettyprint-override""><code>def distribution_ngrams(text, n=4):
    &quot;&quot;&quot;
    &quot;&quot;&quot;
    import math
    from collections import Counter 

    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]
    counts = Counter(ngrams)
    total = len(ngrams)
    
    # Calculate the distribution of n-grams
    distribution = {ngram: count/total for ngram, count in counts.items()}
    return distribution

def language_entropy_ngrams(text, n_approx=4):
    &quot;&quot;&quot;
    Calculate an estimate of the entropy of a text using n-grams (normally, we take a very large n and consider an infinite sequence L)
    &quot;&quot;&quot;
    import math
    distribution = distribution_ngrams(text, n_approx)
    # Calculate entropy
    entropy = -sum((p * math.log2(p)) for ngram,p in distribution.items())
    entropy_rate = entropy / n_approx  # normalize by the size of the n-gram
    return entropy_rate  

def model_cross_entropy(text,n_approx=4):
    &quot;&quot;&quot;
    Calculate the cross-entropy between the true text and the model's predictions
    &quot;&quot;&quot;
    import math
    unigram_model_distribution =  distribution_ngrams(text, 1)
    language_model_distribution_approximation = distribution_ngrams(text, n_approx)

    q = {}
    cross_entropy = 0
    for ngram,p in language_model_distribution_approximation.items():
        q[ngram] = 1
        for c in ngram:
            q[ngram] = q[ngram]*unigram_model_distribution[c]
        cross_entropy -= p*math.log2(q[ngram])
        
    return cross_entropy/n_approx

if __name__ == &quot;__main__&quot;:
    prenoms = [&quot;Alice&quot;, &quot;Alfred&quot;, &quot;Alina&quot;, &quot;Aline&quot;, &quot;Alexandre&quot;, &quot;Alicia&quot;, 
            &quot;Alison&quot;, &quot;Alma&quot;, &quot;Alva&quot;, &quot;Elise&quot;, &quot;Elisa&quot;, &quot;Eliane&quot;, &quot;Alain&quot;, 
            &quot;Amélie&quot;, &quot;Arline&quot;, &quot;Olivier&quot;, &quot;Oline&quot;, &quot;Alva&quot;, &quot;Eliott&quot;, &quot;Julien&quot;] #each prenonm can be seen as a sequence of characters


    L = ''.join(prenoms).lower() #the corpus/language L can be seen as the concatenation of the sequences
    print(language_entropy_ngrams(L))
    print(model_cross_entropy(L))



</code></pre>
",Training and Model Evaluation,understanding difference entropy cross entropy language model practical example character level unigram model trying understand difference entropy cross entropy often hear entropy language cross entropy language model want understand link two simplify thing let consider language vocabulary language model trained language work character level give u character limited number word let take name calculate entropy name e entropy language cross entropy language model let take unigram model language model prefer help understand relevant example open p confusion come fact general definition talk language distribution p calculating entropy without quite knowing calculate two distribution p q calculating cross entropy p one hot encoding vector case calculating cross entropy loss p python code could help well understand understanding based understanding jurasky book
MiniBatchKMeans BERTopic not returning topics for half of data,"<p>I am trying to topic a dataset of tweets. I have around 50 million tweets. Unfortunately, such a large dataset will not fit in ram (even 128GB) due to the embeddings. Therefore, I have been working on making an incremental BERTopic as per the <a href=""https://maartengr.github.io/BERTopic/getting_started/online/online.html"" rel=""nofollow noreferrer"">docs</a></p>
<p>As such:</p>
<pre class=""lang-none prettyprint-override""><code>from bertopic.vectorizers import OnlineCountVectorizer
from bertopic.vectorizers import ClassTfidfTransformer
from sklearn.cluster import MiniBatchKMeans
import numpy as np


class SafeIncrementalPCA(IncrementalPCA):
    def partial_fit(self, X, y=None):
        # Ensure the input is contiguous and in float64
        X = np.ascontiguousarray(X, dtype=np.float64)
        return super().partial_fit(X, y)
    
    def transform(self, X):
        result = super().transform(X)
        # Force the output to be float64 and contiguous
        return np.ascontiguousarray(result, dtype=np.float64)


vectorizer_model = OnlineCountVectorizer(stop_words=&quot;english&quot;)
ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)
umap_model = SafeIncrementalPCA(n_components=100)
cluster_model = MiniBatchKMeans(n_clusters=1000, random_state=0)

from bertopic import BERTopic

topic_model = BERTopic(umap_model=umap_model,
                       hdbscan_model=cluster_model,

for docs_delayed, emb_delayed in tqdm(zip(docs_partitions, embeddings_partitions), total=len(docs_partitions)):

    docs_pdf = docs_delayed.compute()
    emb_pdf = emb_delayed.compute()

    docs = docs_pdf[&quot;text&quot;].tolist()
    embeddings = np.vstack(emb_pdf['embeddings'].tolist())
    
    # Partial fit your model (make sure your model supports partial_fit, like many scikit-learn estimators do)
    topic_model.partial_fit(docs, embeddings)

</code></pre>
<p>and then transforming the dataset into a SQL database:</p>
<pre class=""lang-none prettyprint-override""><code>
for docs_delayed, emb_delayed in tqdm(zip(docs_partitions, embeddings_partitions), total=len(docs_partitions)):

    docs_pdf = docs_delayed.compute()
    emb_pdf = emb_delayed.compute()
    docs = docs_pdf[&quot;text&quot;].tolist()
    embeddings = np.vstack(emb_pdf['embeddings'].tolist())

    # 3) Apply BERTopic on this shard
    topics, probs = topic_model.transform(docs, embeddings)

    # Save topics to DataFrame
    df_topics = pd.DataFrame({
        &quot;tweet_id&quot;: docs_pdf[&quot;id&quot;].tolist(),
        &quot;topic&quot;: topics,
        &quot;probability&quot;: probs
    })

    ## Merge &amp; store in DB
    docs_pdf[&quot;topic&quot;] = df_topics[&quot;topic&quot;]
    docs_pdf[&quot;probability&quot;] = df_topics[&quot;probability&quot;]
    docs_pdf.to_sql(&quot;tweets&quot;, engine, if_exists=&quot;append&quot;, index=False)
</code></pre>
<p>I've been trying to do this for a quite a while and this is the closest working example I have gotten. The only issue is half of the dataset has null topics in the database at the end. From what I understand of the theory, MiniBatchKMeans should not have any outliers and therefore all tweets should be assigned to at least one topic, right? I've checked out the unclassified tweets in question and there is nothing in their doc that should suggest it would be hard to classify (relative to others that are classified).</p>
<p>I would be very happy to hear any sort of suggestion on what could be going wrong and how I could fix this!</p>
<p>Thanks!</p>
",Training and Model Evaluation,minibatchkmeans bertopic returning topic half data trying topic dataset tweet around million tweet unfortunately large dataset fit ram even gb due embeddings therefore working making incremental bertopic per doc transforming dataset sql database trying quite closest working example gotten issue half dataset ha null topic database end understand theory minibatchkmeans outlier therefore tweet assigned least one topic right checked unclassified tweet question nothing doc suggest would hard classify relative others classified would happy hear sort suggestion could going wrong could fix thanks
Training llm for Query Generation in a Graph Database,"<p>If I have developed a graph database which has its own query language. I have to find a way to feed llm the graph and then llm should be able to generate the queries of our database.</p>
<p>I have found something similar in langchain that we can feed it the rdf file and then it will generate the sparql queries.</p>
<p>So I have many doubts regarding this as I am very new to this:</p>
<p>Is it possible to train a llm on an entirely new technology like here it is our database. If it is possible then how.</p>
<p>I know that we have to provide the training data to the llm. So in this case, will it be the dataset with our database queries. If yes , then how many queries we have to provide in a dataset.</p>
<p>Sorry if the question is not detailed , its only my second time asking here.</p>
",Training and Model Evaluation,training llm query generation graph database developed graph database ha query language find way feed llm graph llm able generate query database found something similar langchain feed rdf file generate sparql query many doubt regarding new possible train llm entirely new technology like database possible know provide training data llm case dataset database query yes many query provide dataset sorry question detailed second time asking
Data Annotation for Machine Learning,"<p>I am going to develop a machine learning model. I have large data sets(Text). I need overall better accuracy F1 score etc. I am using data annotation tools(Dataturks). Which approach will be good to label the data as single label per entity or multiple label per entity (like there has been 5 times GUI so we have to label it 1 time or 5 times for better overall score). Your help will be highly appreciated.</p>
",Training and Model Evaluation,data annotation machine learning going develop machine learning model large data set text need overall better accuracy f score etc using data annotation tool dataturks approach good label data single label per entity multiple label per entity like ha time gui label time time better overall score help highly appreciated
Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged,"<p>I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes.</p>
<p>However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue.</p>
<p>This is several information about the dataset:</p>
<p>Embedding size: (41151, 100)</p>
<p>Maximum sequence length: 731</p>
<p>Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106}</p>
<p>Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106}</p>
<p>Total x training set (padded): (140997, 200)</p>
<p>Total x validation set (padded): (17625, 200)</p>
<p>Total x testing set (padded): (17625, 200)</p>
<p>Total y training set (one hot): (140997, 3)</p>
<p>Total y validation set (one hot): (17625, 3)</p>
<p>Total y testing set (one hot): (17625, 2003</p>
<p>This is my full code:
<a href=""https://www.kaggle.com/code/mathiasyeremia/sentiment-analysis-model"" rel=""nofollow noreferrer"">enter link description here</a></p>
<p>This is my highlight code for this issue:</p>
<pre><code>lstm_model = Sequential()
lstm_model.add(Input(shape=(max_len,)))
lstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))
lstm_model.add(LSTM(256, return_sequences=True))
lstm_model.add(LSTM(128, return_sequences=True))
lstm_model.add(LSTM(64))
lstm_model.add(Dense(128, activation='relu'))
lstm_model.add(Dense(units=3, activation='softmax'))

lstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

lstm_model.summary()
</code></pre>
<p><a href=""https://i.sstatic.net/T6vCZ9Jj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/T6vCZ9Jj.png"" alt=""enter image description here"" /></a></p>
",Training and Model Evaluation,underfitting pre trained glove lstm model accurcacy unchanged sentiment classification using pre trained glove lstm model use google play review scrap resulting k text implement random sampling minority class however train lstm model training accuracy remain unchanged several epoch need insight fix issue several information dataset embedding size maximum sequence length label distribution random sampling positive negative neutral label distribution random sampling positive negative neutral total x training set padded total x validation set padded total x testing set padded total training set one hot total validation set one hot total testing set one hot full code enter link description highlight code issue
What is the loss function used in Trainer from the Transformers library of Hugging Face?,"<p>What is the loss function used in Trainer from the Transformers library of Hugging Face?</p>
<p>I am trying to fine tune a BERT model using the <strong>Trainer class</strong> from the Transformers library of Hugging Face.</p>
<p>In their <a href=""https://huggingface.co/docs/transformers/main_classes/trainer"" rel=""nofollow noreferrer"">documentation</a>, they mention that one can specify a customized loss function by overriding the <code>compute_loss</code> method in the class. However, if I do not do the method override and use the Trainer to fine tine a BERT model directly for sentiment classification, what is the default loss function being use? Is it the categorical crossentropy? Thanks!</p>
",Training and Model Evaluation,loss function used trainer transformer library hugging face loss function used trainer transformer library hugging face trying fine tune bert model using trainer class transformer library hugging face documentation mention one specify customized loss function overriding method class however method override use trainer fine tine bert model directly sentiment classification default loss function use categorical crossentropy thanks
Why is my BERT model producing NaN loss during training for multi-label classification on imbalanced data?,"<p>I’m running into a frustrating issue while training a BERT-based multi-label text classification model on an imbalanced dataset. After a few epochs, the training loss suddenly becomes NaN, and I can’t seem to figure out why. I’ve tried a bunch of different things, but nothing has worked so far. Hoping someone here has dealt with this before.
<strong>Dataset Setup</strong>
Number of samples is around 100K
Number of labels is around 50
Imbalance: Some labels are super common (appear in 80% of samples), while others are barely there (less than 0.5%)</p>
<p><strong>My Setup</strong>
I’m using Hugging Face’s bert-base-uncased with a custom classification head.</p>
<pre><code>from transformers import BertModel
import torch.nn as nn

class MultiLabelClassifier(nn.Module):
    def __init__(self, num_labels):
        super(MultiLabelClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        logits = self.classifier(outputs.pooler_output)  # Using the [CLS] token
        return logits

</code></pre>
<p>I’m using ***BCEWithLogitsLoss ***with a weighted loss function to deal with the imbalance:</p>
<pre><code>class_weights = torch.tensor([1.0 / (freq + 1e-5) for freq in label_frequencies]).to(device)
criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)

</code></pre>
<p>after 2 or 3 epochs;</p>
<ul>
<li>The loss starts off fine but becomes NaN</li>
<li>Some logits are ridiculously large or small (1e20, -1e20) before the NaN happens.</li>
<li>Gradients also seem to explode right before the NaN loss kicks in.</li>
</ul>
<p>to solve this issue,</p>
<ol>
<li>Added gradient clipping, which helps a bit but doesn’t fully fix the issue
<code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)</code></li>
<li>Tried reducing the learning rate to 5e-6, which delays the NaN issue but doesn’t stop it completely</li>
<li>Thought the issue might be with the classifier weights, so I reinitialized them like this,</li>
</ol>
<pre><code>for layer in model.classifier.parameters():
    if isinstance(layer, nn.Linear):
        nn.init.xavier_normal_(layer.weight)

</code></pre>
<ol start=""4"">
<li>Rewrote the loss calculation using torch.logsigmoid for numerical stability...</li>
</ol>
<pre><code>loss = -labels * torch.logsigmoid(logits) - (1 - labels) * torch.logsigmoid(-logits)
loss = loss.mean()
</code></pre>
<p>Nothing seems to solve the problem completely.</p>
<p><strong>------my questions---</strong></p>
<ol>
<li>Why is this happening? Is it because of the extreme imbalance in my dataset or something else?</li>
<li>How can I fix it? Should I try something like label smoothing, or is there a better way to stabilize the training?</li>
</ol>
<p>here is a snippet of my training loop for context.</p>
<pre><code>optimizer = AdamW(model.parameters(), lr=5e-5)
scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=num_training_steps)

for epoch in range(num_epochs):
    model.train()
    for batch in dataloader:
        input_ids, attention_mask, labels = batch[&quot;input_ids&quot;], batch[&quot;attention_mask&quot;], batch[&quot;labels&quot;]
        logits = model(input_ids, attention_mask)
        loss = criterion(logits, labels)

        optimizer.zero_grad()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()
        scheduler.step()

</code></pre>
<p>---<strong>I Need</strong>--
I’m looking for practical suggestions to <strong>prevent the NaN loss issue entirely</strong> and <strong>stabilize training when working with imbalanced multi-label datasets</strong>.</p>
<p>If anyone has faced this issue or knows a good fix, I’d really appreciate the help. Thanks!</p>
",Training and Model Evaluation,bert model producing nan loss training multi label classification imbalanced data running frustrating issue training bert based multi label text classification model imbalanced dataset epoch training loss suddenly becomes nan seem figure tried bunch different thing nothing ha worked far hoping someone ha dealt dataset setup number sample around k number label around imbalance label super common appear sample others barely le setup using hugging face bert base uncased custom classification head using bcewithlogitsloss weighted loss function deal imbalance epoch loss start fine becomes nan logits ridiculously large small e e nan happens gradient also seem explode right nan loss kick solve issue added gradient clipping help bit fully fix issue tried reducing learning rate e delay nan issue stop completely thought issue might classifier weight reinitialized like rewrote loss calculation using torch logsigmoid numerical stability nothing seems solve problem completely question happening extreme imbalance dataset something else fix try something like label smoothing better way stabilize training snippet training loop context need looking practical suggestion prevent nan loss issue entirely stabilize training working imbalanced multi label datasets anyone ha faced issue know good fix really appreciate help thanks
Why does my contrastive learning model&#39;s loss and gradients explode during training?,"<p>I am fine-tuning an embedding model using contrastive learning. For the loss function, I’m using <code>torch.nn.CrossEntropyLoss</code>.</p>
<p>The training process initially seems to work fine — the loss decreases steadily on average. However, at some point during training (usually around step 16,000 in this case), the loss and gradients explode. After this, the model is unable to stabilize, and training becomes unusable.</p>
<p>Here is a graph showing the behavior:</p>
<p><a href=""https://i.sstatic.net/o0g7mnA4.png"" rel=""nofollow noreferrer"">Tensorboard loss by step graph</a></p>
<h4>What I have tried so far:</h4>
<ol>
<li><p><strong>Data preprocessing</strong>:</p>
<ul>
<li><p>Removed outliers (e.g., very long texts).</p>
</li>
<li><p>Cleaned and filtered the dataset for consistency.</p>
</li>
</ul>
</li>
<li><p><strong>Hyperparameter tuning</strong>:</p>
<ul>
<li><p>Adjusted the learning rate and tried different values.</p>
</li>
<li><p>Changed the optimizer (e.g., switching from Adam to SGD)</p>
</li>
</ul>
</li>
<li><p><strong>Gradient clipping</strong>:</p>
<ul>
<li>Clipped gradients to a max norm of 1 using <code>torch.nn.utils.clip_grad_norm_</code>.</li>
</ul>
</li>
</ol>
<h4>My setup:</h4>
<ul>
<li><p><strong>Dataset size</strong>: ~14,000 samples</p>
</li>
<li><p><strong>Model architecture</strong>: Transformer-based embedding model</p>
</li>
<li><p><strong>Batch size</strong>: 1 (given my gpu capacity)</p>
</li>
<li><p><strong>Learning rate</strong>: 1e-5</p>
</li>
<li><p><strong>Optimizer</strong>: Adam with weight decay</p>
</li>
</ul>
<p><strong>Training loop (relevant part):</strong></p>
<pre><code>for epoch in range(epochs):
    model.train()
    epoch_loss = 0.0
    for step, batch in enumerate(dataset_train):
        temperature = max(0.1, 0.05 * (1 - step / num_training_steps))

        # Move data to device
        anchor_input_ids = batch[&quot;anchor_input_ids&quot;].to(device)
        anchor_attention_mask = batch[&quot;anchor_attention_mask&quot;].to(device)
        positive_input_ids = batch[&quot;positive_input_ids&quot;].to(device)
        positive_attention_mask = batch[&quot;positive_attention_mask&quot;].to(device)
        negative_input_ids = batch[&quot;negative_input_ids&quot;].to(device)
        negative_attention_mask = batch[&quot;negative_attention_mask&quot;].to(device)

        anchor_input_ids = anchor_input_ids.unsqueeze(0)  # Add a dimension for the batch
        anchor_attention_mask = anchor_attention_mask.unsqueeze(0)  # Add a dimension for the batch
        positive_input_ids = positive_input_ids.unsqueeze(0)  # Add a dimension for the batch
        positive_attention_mask = positive_attention_mask.unsqueeze(0)  # Add a dimension for the batch
        negative_input_ids = negative_input_ids.unsqueeze(0)  # Add a dimension for the batch
        negative_attention_mask = negative_attention_mask.unsqueeze(0)  # Add a dimension for the batch

        optimizer.zero_grad()

        # Generate embeddings
        anchor_embeddings = model.forward(anchor_input_ids, anchor_attention_mask)
        positive_embeddings = model.forward(positive_input_ids, positive_attention_mask)
        negative_embeddings = model.forward(negative_input_ids, negative_attention_mask)

        # Calculate cosine similarities
        pos_sim = cosine_similarity(anchor_embeddings, positive_embeddings)
        neg_sim = cosine_similarity(anchor_embeddings, negative_embeddings)

        # Calculate logits
        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim.unsqueeze(1)], dim=1) / temperature
        labels = torch.zeros(logits.size(0), dtype=torch.long).to(device)  # The positive class is always the first

        # Calculate InfoNCE loss
        loss = torch.nn.CrossEntropyLoss()(logits, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
</code></pre>
<p><strong>Dataset generation:</strong></p>
<pre><code>import torch
from torch.utils.data import Dataset
import random

class FineTuneContrastiveDataset(Dataset):
    def __init__(self, pairs_data, df_1, df_2, tokenizer, max_tokens=512):
        &quot;&quot;&quot;
        pairs_data: List of tuples (id_1, id_2, label).
        df_1: DataFrame containing text data associated with id_1.
        df_2: DataFrame containing text data associated with id_2.
        tokenizer: Hugging Face tokenizer.
        max_tokens: Maximum allowed length for the tokenized text.
        &quot;&quot;&quot;
        self.pairs_data = pairs_data
        self.df_1 = df_1.set_index(&quot;id_1&quot;)
        self.df_2 = df_2.set_index(&quot;id_2&quot;)
        self.tokenizer = tokenizer
        self.max_tokens = max_tokens
        self.id_2_list = list(self.df_2.index)  # For selecting negative samples

    def __len__(self):
        return len(self.pairs_data)

    def __getitem__(self, idx):
        # Retrieve data from the pair
        id_1, id_2_positive, label = self.pairs_data[idx]
        
        # Text associated with id_1 (anchor)
        text_1 = &quot; &quot;.join(self.df_1.loc[id_1][&quot;chunks&quot;])

        # Positive text associated with id_2
        text_2_positive = &quot; &quot;.join(self.df_2.loc[id_2_positive][&quot;chunks&quot;])

        # Generate a negative sample from id_2
        id_2_negative = random.choice(
            [candidate_id for candidate_id in self.id_2_list if candidate_id != id_2_positive]
        )
        text_2_negative = &quot; &quot;.join(self.df_2.loc[id_2_negative][&quot;chunks&quot;])

        # Tokenize inputs
        inputs_anchor = self.tokenizer(
            text_1, truncation=True, max_length=self.max_tokens, 
            padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;
        )
        inputs_positive = self.tokenizer(
            text_2_positive, truncation=True, max_length=self.max_tokens, 
            padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;
        )
        inputs_negative = self.tokenizer(
            text_2_negative, truncation=True, max_length=self.max_tokens, 
            padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;
        )

        return {
            &quot;anchor_input_ids&quot;: inputs_anchor[&quot;input_ids&quot;].squeeze(0),
            &quot;anchor_attention_mask&quot;: inputs_anchor[&quot;attention_mask&quot;].squeeze(0),
            &quot;positive_input_ids&quot;: inputs_positive[&quot;input_ids&quot;].squeeze(0),
            &quot;positive_attention_mask&quot;: inputs_positive[&quot;attention_mask&quot;].squeeze(0),
            &quot;negative_input_ids&quot;: inputs_negative[&quot;input_ids&quot;].squeeze(0),
            &quot;negative_attention_mask&quot;: inputs_negative[&quot;attention_mask&quot;].squeeze(0),
            &quot;label&quot;: torch.tensor(label, dtype=torch.float),
            &quot;id_1&quot;: id_1,
        }
</code></pre>
",Training and Model Evaluation,doe contrastive learning model loss gradient explode training fine tuning embedding model using contrastive learning loss function using training process initially seems work fine loss decrease average however point training usually around step case loss gradient explode model unable stabilize training becomes unusable graph showing behavior tensorboard loss step graph tried far data preprocessing removed outlier e g long text cleaned filtered dataset consistency hyperparameter tuning adjusted learning rate tried different value changed optimizer e g switching adam sgd gradient clipping clipped gradient max norm using setup dataset size sample model architecture transformer based embedding model batch size given gpu capacity learning rate e optimizer adam weight decay training loop relevant part dataset generation
How to save checkpoints for thie transformer gpt2 to continue training?,"<p>I am retraining the GPT2 language model, and am following this blog :</p>
<p><a href=""https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171"" rel=""nofollow noreferrer"">https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171</a></p>
<p>Here, they have trained a network on GPT2, and I am trying to recreate a same. However, my dataset is too large(250Mb), so I want to continue training in intervals. In other words, I want to checkpoint the model training. How could I do this?</p>
",Training and Model Evaluation,save checkpoint thie transformer gpt continue training retraining gpt language model following blog trained network gpt trying recreate however dataset large mb want continue training interval word want checkpoint model training could
Avoiding overlap in frequency and document frequency count in Quanteda,"<p>Below is a dummy corpus of 4 documents.</p>
<p>The dictionary was developed to identify the frequency of words or phrases in the corpus, as well as the number of documents a word or phrases occurs in.</p>
<p>The world 'Australians' occurs in two dictionary keys (peep, indig). Key content is intended to be mutually exclusive.</p>
<p>Similarly 'Australia' (oz and Australia Post), foreign (foreign and multinat) and farm/farmers (dairy and farmers) occur in two dictionary keys each,
but are intended to be counted once, according to the dictionary.</p>
<p>The expected overall frequency count is (extracted from the 'pattern&quot; column of the kwic table) and reported as x2 below. Note the word industry appears but is not allocated to industry because it is define din the indig key.</p>
<p>Dairy is the most frequency occuring key, occuring in three documents. This can calculated from unique rows in the kwic table 'doc names' column for each key.</p>
<p>I have three questions:</p>
<ol>
<li>are there any problems/issues that could affect output accuracy using this approach?</li>
<li>is there a better/more parsimonius approach to achieve what I am trying to do?</li>
<li>what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table?</li>
</ol>
<pre><code>        library (quanteda)
        library(quanteda.textstats)

        txt &lt;- c(doc1 = &quot;A significant percent of all farms in Australia, are dairy. 
         Although there are a lot of dairy farms in this country, 
         it is not the biggest farm industry. The life of a farmer is not easy, a dairy 
        farmer has to be an early riser. &quot;,
         doc2 = &quot;Australian people like milk so a healthy dairy industry is important in 
         our country&quot;,
         doc3 = &quot;Dairy and sheep farms developed at the expense of Indigenous 
         Australians. Further many companies  are now foreign-owned&quot;,
         doc4 = &quot;Some farmers are lucky to receive a service from Australia Post. Mail is 
         sent to many foreign countries and received more quickly than 
         delivered in some locations in Australia.&quot;)



         x &lt;- x %&gt;%
         tokens_compound(phrase(&quot;dairy farmers&quot;), concatenator = &quot; &quot;) %&gt;%
         tokens_compound(phrase(&quot;dairy farms&quot;), concatenator = &quot; &quot;) %&gt;%
         tokens_compound(phrase(&quot;dairy farm&quot;), concatenator = &quot; &quot;) %&gt;%
         tokens_compound(phrase(&quot;dairy farming&quot;), concatenator = &quot; &quot;) %&gt;%
         tokens_compound(phrase(&quot;dairy industry&quot;), concatenator = &quot; &quot;) %&gt;%
         tokens_compound(phrase(&quot;indigenous australians&quot;), concatenator = &quot; &quot;) %&gt;%
         tokens_compound(phrase(&quot;australia post&quot;), concatenator = &quot; &quot;) %&gt;%
         tokens_compound(phrase(&quot;dairy farmer&quot;), concatenator = &quot; &quot;)
              x

         dict &lt;- dictionary(list(multinat = c(&quot;offshore petroleum companies&quot;, &quot;foreign- 
         owned&quot;, &quot;foreign owned&quot;, &quot;foreign companies&quot;, &quot;multinational&quot;, &quot;multinational 
         oil companies&quot;, &quot;multinationals&quot;, &quot;transnational&quot;),
         dairy = c(&quot;dairy farmers&quot;, &quot;dairy farms&quot;,&quot;dairy farm&quot;,&quot;dairy farming&quot;,&quot;dairy 
         industry&quot;, &quot;dairy farmer&quot;,&quot;dairy&quot;, &quot;milk&quot;),
         auspost = &quot;australia post&quot;,
         oz = c(&quot;australia&quot;, &quot;this country&quot;, &quot;our country&quot;),
         farmers = c(&quot;farmers&quot;, &quot;farmer&quot;, &quot;farm&quot;, &quot;farms&quot;),
         foreign = c(&quot;foreign&quot;, &quot;foreigner&quot;, &quot;foreigners&quot;), 
         business =c(&quot;small business&quot;, &quot;business&quot;, &quot;businesses&quot;, &quot;company&quot;, &quot;companies&quot;),
         indig = c(&quot;aboriginal&quot;, &quot;aboriginals&quot;, &quot;indigenous australians&quot;, &quot;torres 
         strait&quot;),
         peep = c(&quot;australians&quot;, &quot;people of australia&quot;, &quot;australian people&quot;, &quot;people of 
         this nation&quot;, &quot;people of this country&quot;),
         industry = c(&quot;industry&quot;, &quot;industries&quot;)))

        kwicdict &lt;- kwic(x, pattern = dict, window = 4)
        write.csv (kwicdict, &quot;D:/Output/TEST.csv&quot;)

       DF &lt;- read.csv(&quot;D://Output/TEST.csv&quot;,header=T)

       ## obtaining frequency count of KWIC table 'pattern ' values
       &gt; x2 &lt;- DF[,8]
       &gt; 
       &gt; table (x2)
       x2
       auspost business    dairy  farmers  foreign    indig industry multinat  oz  peep    
          1        1        6        5        1        1        1        1     5    2 
</code></pre>
",Training and Model Evaluation,avoiding overlap frequency document frequency count quanteda dummy corpus document dictionary wa developed identify frequency word phrase corpus well number document word phrase occurs world australian occurs two dictionary key peep indig key content intended mutually exclusive similarly australia australia post foreign foreign multinat farm farmer dairy farmer occur two dictionary key intended counted according dictionary expected overall frequency count extracted pattern column kwic table reported x note word industry appears allocated industry define din indig key dairy frequency occuring key occuring three document calculated unique row kwic table doc name column key three question problem issue could affect output accuracy using approach better parsimonius approach achieve trying would best way extract equivalent tetxstat frequency count data kwic table
RuntimeError: &quot;element 0 of tensors does not require grad and does not have a grad_fn&quot;,"<p>I am facing an issue while training a comment classification model using PyTorch Lightning with a pre-trained BERT model.</p>
<p>I encountered the following error during the training process:</p>
<pre><code>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>To provide some context, I have already enabled gradients for all parameters of the model using the function enable_gradients(model). However, the error still persists.</p>
<p>The model I am using is based on the aubmindlab/bert-base-arabertv02-twitter pre-trained model, and I noticed that some weights of the BERT model were not initialized properly upon loading. I have ensured that I am using the latest versions of PyTorch, Transformers, and PyTorch Lightning.</p>
<p>I attempted to pretrain the BERT model on a downstream task before training my specific model, but the error remains unresolved.</p>
<p>How to resolve this issue?</p>
<p>this is my code :</p>
<pre class=""lang-py prettyprint-override""><code>from pytorch_lightning import Trainer

def enable_gradients(model):
    for param in model.parameters():
        param.requires_grad = True

# datamodule
ucc_data_module = UCC_Data_Module(train_path, val_path, test_path, attributes=attributes, batch_size=config['batch_size'])
ucc_data_module.setup()

# model
model = UCC_Comment_Classifier()

enable_gradients(model)

# trainer and fit
# Instantiation of the Lightning Trainer
trainer = Trainer(max_epochs=config['n_epochs'], accelerator='gpu', num_sanity_val_steps=1)

try:
    trainer.fit(model, ucc_data_module)
    torch.save(model.state_dict(), PATH)
except RuntimeError as e:
    print(e)
</code></pre>
<p>This is the error :</p>
<pre><code>ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py&quot;, line 69, in _wrap
    fn(i, *args)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py&quot;, line 
147, in _wrapping_function
    results = function(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py&quot;, line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py&quot;, line 973, in _run
    results = self._run_stage()
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py&quot;, line 1016, in _run_stage
    self.fit_loop.run()
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py&quot;, line 201, in run
    self.advance()
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py&quot;, line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py&quot;, line 133, in run
    self.advance(data_fetcher)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py&quot;, line 218, in 
advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py&quot;, line 185, in 
run
    self._optimizer_step(kwargs.get(&quot;batch_idx&quot;, 0), closure)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py&quot;, line 260, in 
_optimizer_step
    call._call_lightning_module_hook(
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py&quot;, line 144, in 
_call_lightning_module_hook
    output = fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/module.py&quot;, line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py&quot;, line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py&quot;, line 256, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py&quot;, line 225, in 
optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py&quot;, line 114,
in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py&quot;, line 69, in wrapper
    return wrapped(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py&quot;, line 280, in wrapper
    out = func(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context
    return func(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/transformers/optimization.py&quot;, line 439, in step
    loss = closure()
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py&quot;, line 101,
in _wrap_closure
    closure_result = closure()
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py&quot;, line 140, in 
__call__
    self._result = self.closure(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py&quot;, line 135, in 
closure
    self._backward_fn(step_output.closure_loss)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py&quot;, line 232, in 
backward_fn
    call._call_strategy_hook(self.trainer, &quot;backward&quot;, loss, optimizer)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py&quot;, line 291, in 
_call_strategy_hook
    output = fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py&quot;, line 200, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py&quot;, line 67, 
in backward
    model.backward(tensor, *args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/module.py&quot;, line 1046, in backward
    loss.backward(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/_tensor.py&quot;, line 487, in backward
    torch.autograd.backward(
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py&quot;, line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
",Training and Model Evaluation,runtimeerror element tensor doe require grad doe grad fn facing issue training comment classification model using pytorch lightning pre trained bert model encountered following error training process provide context already enabled gradient parameter model using function enable gradient model however error still persists model using based aubmindlab bert base arabertv twitter pre trained model noticed weight bert model initialized properly upon loading ensured using latest version pytorch transformer pytorch lightning attempted pretrain bert model downstream task training specific model error remains unresolved resolve issue code error
NLP - Specific Text Extraction,"<p>I have to identify the country name from a random text. I have the country list.</p>
<p>I am struggling to find a solution that can train the model on the country list and when I provide a random text to that model as an input, it identifies the country name as an output.</p>
<p>eg:-</p>
<ul>
<li>&quot;I live in India&quot; will give &quot;India&quot;</li>
<li>&quot;London is the capital of United Kingdom&quot; will give &quot;United Kingdom&quot;</li>
</ul>
",Training and Model Evaluation,nlp specific text extraction identify country name random text country list struggling find solution train model country list provide random text model input identifies country name output eg live india give india london capital united kingdom give united kingdom
How to decide correct NLP approach for a project,"<p>I'm working on an NLP project. My task is to determine the category and Sentiment Score of Turkish Call Center conversations from the conversations themselves. We are using Python as our programming language. However, I'm stuck in the initial stages of the project among hundreds of alternative solutions. I have 300,000 rows of Customer Representative and Customer Text data, and I have cleaned them nicely through the preprocessing stage. All are currently in a sentence tokenized form and have been through other standard preprocessing stages. Customer representative and customer conversations are ready in different columns in a ~600 MB csv. Before deciding on modeling algorithms, my manager expects me to prepare the training dataset. The dataset should contain the following information, and we will decide later which ones are necessary and which ones are unnecessary, and then we will finalize the dataset:</p>
<ol>
<li>Word vectors should be extracted</li>
<li>Sentence vectors should be extracted</li>
<li>Summaries of the conversations should be extracted, and sentence and word vectors of these summaries should be extracted</li>
<li>NER counts should be extracted</li>
<li>POS counts should be extracted</li>
<li>Morphological analysis should be extracted, and affixes, conjunctions, etc. should be shown numerically</li>
<li>Sentiment score should be extracted in terms of both subjectivity and polarity</li>
<li>Keywords and their vectors should be extracted</li>
<li>Topic modeling should be done, and which topic each conversation is closest to should be added to the dataset</li>
<li>Similarity scores between summary and main conversations should be extracted</li>
<li>The rarity ratio of the words mentioned in a conversation should be extracted and the average should be taken sentence by sentence (We think it will give an idea about how rich the sentence is in terms of meaning)</li>
</ol>
<p>The problems I'm facing are as follows:</p>
<ol>
<li><p>What is the best word vector extraction library for the Turkish language? There are techniques like Word2vec, NGram, GloVe, etc. There are also relatively newer techniques like Fasttext. BERT is also an option. Which one should I choose? How will I compare their performance? Should I train my own model, or should I prefer pre-trained models? (E.g., Fasttext has models trained on Turkish language) Which one is superior or more current to which? Which article or research should I base on, if they all used a different technique?</p>
</li>
<li><p>Gensim seems like a library with tools that provide solutions to a lot of NLP problems. It's such a big library that I couldn't even grasp its full capabilities. Should I just proceed using Gensim, or should I use different tools together? Will it meet all my needs? How will I know?</p>
</li>
<li><p>There are a lot of tools doing lemmatizing, these tools are also vectorizing, since I will also do lemmatizing, should I use their vectorization features, or should I benefit from the most mentioned vectorization tools above? Which one gives the best result? I've read a lot of comparison articles, they all talk about different results.</p>
</li>
<li><p>There are SBERT models trained on Turkish for extracting sentence vectors. Will the vectors I will get when I use SBERT become meaningless when I extract word vectors with another tool? After all, I will extract these with different methods and they will be in the same dataset.</p>
</li>
</ol>
<p>Due to the ambiguity of the superiority of such alternative solutions to each other, I'm confused.</p>
<p>How should an NLP project be conducted correctly?</p>
",Training and Model Evaluation,decide correct nlp approach project working nlp project task determine category sentiment score turkish call center conversation conversation using python programming language however stuck initial stage project among hundred alternative solution row customer representative customer text data cleaned nicely preprocessing stage currently sentence tokenized form standard preprocessing stage customer representative customer conversation ready different column mb csv deciding modeling algorithm manager expects prepare training dataset dataset contain following information decide later one necessary one unnecessary finalize dataset word vector extracted sentence vector extracted summary conversation extracted sentence word vector summary extracted ner count extracted po count extracted morphological analysis extracted affix conjunction etc shown numerically sentiment score extracted term subjectivity polarity keywords vector extracted topic modeling done topic conversation closest added dataset similarity score summary main conversation extracted rarity ratio word mentioned conversation extracted average taken sentence sentence think give idea rich sentence term meaning problem facing follows best word vector extraction library turkish language technique like word vec ngram glove etc also relatively newer technique like fasttext bert also option one choose compare performance train model prefer pre trained model e g fasttext ha model trained turkish language one superior current article research base used different technique gensim seems like library tool provide solution lot nlp problem big library even grasp full capability proceed using gensim use different tool together meet need know lot tool lemmatizing tool also vectorizing since also lemmatizing use vectorization feature benefit mentioned vectorization tool one give best result read lot comparison article talk different result sbert model trained turkish extracting sentence vector vector get use sbert become meaningless extract word vector another tool extract different method dataset due ambiguity superiority alternative solution confused nlp project conducted correctly
NLP software for classification of large datasets,"<p>For years I've been using my own Bayesian-like methods to categorize new items from external sources based on a large and continually updated training dataset.</p>
<p>There are three types of categorization done for each item:</p>
<ol>
<li>30 categories, where each item must belong to one category, and at most two categories.</li>
<li>10 other categories, where each item is only associated with a category if there is a strong match, and each item can belong to as many categories as match.</li>
<li>4 other categories, where each item must belong to only one category, and if there isn't a strong match the item is assigned to a default category.</li>
</ol>
<p>Each item consists of English text of around 2,000 characters.  In my training dataset there are about 265,000 items, which contain a rough estimate of 10,000,000 features (unique three word phrases).</p>
<p>My homebrew methods have been fairly successful, but definitely have room for improvement.  I've read the NLTK book's chapter &quot;Learning to Classify Text&quot;, which was great and gave me a good overview of NLP classification techniques.  I'd like to be able to experiment with different methods and parameters until I get the best classification results possible for my data.</p>
<h2>The Question</h2>
<p>What off-the-shelf NLP tools are available that can efficiently classify such a large dataset?</p>
<p>Those I've tried so far:</p>
<ul>
<li>NLTK</li>
<li>TIMBL</li>
</ul>
<p>I tried to train them with a dataset that consisted of less than 1% of the available training data: 1,700 items, 375,000 features.  For NLTK I used a sparse binary format, and a similarly compact format for TIMBL.</p>
<p>Both seemed to rely on doing everything in memory, and quickly consumed all system memory.  I can get them to work with tiny datasets, but nothing large.  I suspect that if I tried incrementally adding the training data the same problem would occur either then or when doing the actual classification.</p>
<p>I've looked at Google's Prediction API, which seem to do much of what I'm looking for but not everything.  I'd also like to avoid relying on an external service if possible.</p>
<p>About the choice of features: in testing with my homebrew methods over the years, three word phrases produced by far the best results.  Although I could reduce the number of features by using words or two word phrases, that would most likely produce inferior results and would still be a large number of features.</p>
",Training and Model Evaluation,nlp software classification large datasets year using bayesian like method categorize new item external source based large continually updated training dataset three type categorization done item category item must belong one category two category category item associated category strong match item belong many category match category item must belong one category strong match item assigned default category item consists english text around character training dataset item contain rough estimate feature unique three word phrase homebrew method fairly successful definitely room improvement read nltk book chapter learning classify text wa great gave good overview nlp classification technique like able experiment different method parameter get best classification result possible data question shelf nlp tool available efficiently classify large dataset tried far nltk timbl tried train dataset consisted le available training data item feature nltk used sparse binary format similarly compact format timbl seemed rely everything memory quickly consumed system memory get work tiny datasets nothing large suspect tried incrementally adding training data problem would occur either actual classification looked google prediction api seem much looking everything also like avoid relying external service possible choice feature testing homebrew method year three word phrase produced far best result although could reduce number feature using word two word phrase would likely produce inferior result would still large number feature
How to recognize if string is human name?,"<p>So I have some text data that's been messily parsed, and due to that I get names mixed in with the actual data. Is there any kind of package/library that helps identify whether a word is a name or not? (In this case, I would be assuming US/western/euro-centric names)</p>
<p>Otherwise, what would be a good way to flag this? Maybe train a model on a corpus of names and assign each word in the dataset a classification? Just not sure the best way to approach this problem/what kind of model would be suited, or if a solution already exists</p>
",Training and Model Evaluation,recognize string human name text data messily parsed due get name mixed actual data kind package library help identify whether word name case would assuming u western euro centric name otherwise would good way flag maybe train model corpus name assign word dataset classification sure best way approach problem kind model would suited solution already exists
Montreal Forced Aligner(MFA) taking too much time(almost 18 days still going on) to train a 33 GB corpus,"<p>WE are  using Montreal Forced Aligner (MFA) 3.x to train an acoustic model on a large dataset (~33GB of audio and transcripts in an Indian language). The training process takes an extremely long time(almost 18 days still going on) to complete.</p>
<p>Here are the details of my setup:</p>
<ul>
<li><p><strong>Hardware:</strong> [16 vCPU, 16 GB RAM]</p>
</li>
<li><p><strong>Audio Format:</strong> WAV files</p>
</li>
<li><p><strong>Corpus Details:</strong> 33GB.</p>
</li>
<li><p><strong>Command Used:</strong> <code>mfa train</code> with default parameters.</p>
</li>
<li><p><strong>Version:</strong> MFA 3.x</p>
</li>
</ul>
<p>Can we reduce the training time? Can we predict a ballpark time for the total time taken for training?</p>
",Training and Model Evaluation,montreal forced aligner mfa taking much time almost day still going train gb corpus using montreal forced aligner mfa x train acoustic model large dataset gb audio transcript indian language training process take extremely long time almost day still going complete detail setup hardware vcpu gb ram audio format wav file corpus detail gb command used default parameter version mfa x reduce training time predict ballpark time total time taken training
why does num_train_epoch always save the latest value instead of corresponding value of maximum acuracy. how to capture the correct epoch value,"<p>here are my arguments</p>
<pre><code>args = TrainingArguments(
    output_dir=&quot;bert-finetuned-ner&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    save_total_limit=3,
    metric_for_best_model=&quot;eval_accuracy&quot;, # load model with highest accuracy score
    load_best_model_at_end=True, # load model with highest accuracy score
    learning_rate=best_params['learning_rate'],
    num_train_epochs=best_params['num-train_epoch'],  
    weight_decay=0.01,
    logging_steps=10,
    logging_strategy=&quot;epoch&quot;,
    fp16=True
)
trainer = Trainer(
    model_init=model_init,
    args=args,
    train_dataset=tokenized_datasets[&quot;train&quot;],
    eval_dataset=tokenized_datasets[&quot;test&quot;],
    compute_metrics=compute_metrics_all,
    tokenizer=tokenizer
    )
</code></pre>
<p>this is after the objective function where in training to capture</p>
<pre><code>with mlflow.start_run(run_id=parent_run.info.run_id, nested=True) as run:
    # log run name
    mlflow.set_tag('mlflow.runName', run_name)
    
    # Log tokenizer parameters
    tokenizer_params = {
        &quot;padding_side&quot;: tokenizer.padding_side,
        &quot;truncation_side&quot;: tokenizer.truncation_side
    }
    mlflow.log_params(tokenizer_params)

    # train
    trainer.train()

    # log best params
    mlflow.log_params(best_params)

    # Evaluate and log all metrics
    results = trainer.evaluate()
    for key, value in results.items():
        mlflow.log_metric(key, value)
</code></pre>
<p>how to capture the correct epoch value in mlflow instead of the latest epoch value of the best trial with epoch value corresponding to highest accuracy. If it needs to corrected in the objective fuction itself then please illustrate how to do that correction step.when logging also this causes the issue.df1 = study.trials_dataframe()</p>
",Training and Model Evaluation,doe num train epoch always save latest value instead corresponding value maximum acuracy capture correct epoch value argument objective function training capture capture correct epoch value mlflow instead latest epoch value best trial epoch value corresponding highest accuracy need corrected objective fuction please illustrate correction step logging also cause issue df study trial dataframe
How can I adjust the performance of tokenizer?,"<p>Working with the tokenizer from the <code>transformers</code> library of Hugging Face. The tokenizer works fine in most cases, but in some cases, it does not.</p>
<p>I'm wondering if I can <strong>&quot;adjust&quot;</strong> (not train a new tokenizer from scratch) the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to.</p>
<p>To be more specific, the type of tokenizer is <code>transformers.XLMRobertaTokenizerFast</code>, which is a unigram tokenizer, and the model is <code>paraphrase-multilingual-mpnet-base-v2</code>.</p>
",Training and Model Evaluation,adjust performance tokenizer working tokenizer library hugging face tokenizer work fine case case doe wondering adjust train new tokenizer scratch performance tokenizer handle bad case still maintaining good performance case used specific type tokenizer unigram tokenizer model
Varying embedding dim due to changing padding in batch size,"<p>I want to train a simple neural network, which has <strong>embedding_dim</strong> as a parameter:</p>
<pre><code>class BoolQNN(nn.Module):
    def __init__(self, embedding_dim):
        super(BoolQNN, self).__init__()
        self.fc1 = nn.Linear(embedding_dim, 64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, 1)

    def forward(self, question_emb, passage_emb):
        combined = torch.cat((question_emb, passage_emb), dim=1)
        x = self.fc1(combined)
        x = self.relu(x)
        x = self.fc2(x)
        return torch.sigmoid(x)
</code></pre>
<p>To load the data I used torchs DataLoader with a custom collate_fn.</p>
<pre><code>train_dataset = BoolQDataset(train_data, pretrained_embeddings)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn_padd)

model = BoolQNN(301)
</code></pre>
<p>The collate_fn_padd function looks the following:</p>
<pre><code>def collate_fn_padd(batch):

  questions, passages, labels = zip(*batch)

  questions = [torch.tensor(q) for q in questions]
  passages = [torch.tensor(p) for p in passages]

  padded_questions = pad_sequence(questions, batch_first=True, padding_value=0)
  padded_passages = pad_sequence(passages, batch_first=True, padding_value=0)

  labels = torch.tensor(labels, dtype=torch.float32)
  
  return padded_questions, padded_passages, labels

</code></pre>
<p><strong>The problem:</strong> For every batch I want to train my model with, the embedded text gets padded differently long (it takes the longest sequence of the current batch).</p>
<p>That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch, althoug I want the size to be the same for every batch.</p>
<p>Due to that, I receive errors like that: <strong>mat1 and mat2 shapes cannot be multiplied (16x182 and 301x64)</strong></p>
<p>Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size, independet of the batch size?</p>
",Training and Model Evaluation,varying embedding dim due changing padding batch size want train simple neural network ha embedding dim parameter load data used torch dataloader custom collate fn collate fn padd function look following problem every batch want train model embedded text get padded differently long take longest sequence current batch mean embedding dim input size linear layer neural network change batch batch althoug want size every batch due receive error like mat mat shape multiplied x x possible adjust collate fn pad function padds sequence size independet batch size
Large Language Model Perplexity,"<p>i am currently using GPT-3 and i am trying to compare its capabilities to related language models for my masters thesis.
Unfortunatly GPT-3 is an API based application, so i am not really able to extract metrics such as perplexity.</p>
<p>Over the API i have acces to these three metrics and of course the models outputs:</p>
<ul>
<li><p>training_loss: loss on the training batch</p>
</li>
<li><p>training_sequence_accuracy: the percentage of completions in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67</p>
</li>
<li><p>training_token_accuracy: the percentage of tokens in the training batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83</p>
</li>
</ul>
<p>Is there any possibility to calculate the perplexity of my model using python?</p>
<p>Thank you.</p>
",Training and Model Evaluation,large language model perplexity currently using gpt trying compare capability related language model master thesis unfortunatly gpt api based application really able extract metric perplexity api acces three metric course model output training loss loss training batch training sequence accuracy percentage completion training batch model predicted token matched true completion token exactly example batch size data contains completion model predicted accuracy training token accuracy percentage token training batch correctly predicted model example batch size data contains completion model predicted accuracy possibility calculate perplexity model using python thank
AutoModelForSequenceClassification loss not decrease,"<pre><code>from datasets import load_dataset
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm import tqdm

def train_one_epoch(model, dataloader, optimizer):
    model.train()
    loss_list = []
    for batch in tqdm(dataloader):
        batch_data = {
            'input_ids': batch['input_ids'],
            'attention_mask': batch['attention_mask'],
            'labels': batch['labels']
        }
        loss = model(**batch_data).loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        loss_list.append(loss.detach().item())
    avg_loss = sum(loss_list) / len(loss_list)
    print('avg loss in epoch:', avg_loss)

def evaluate(model, dataloader):
    model.eval()
    all_labels = []
    all_predictions = []
    for batch in dataloader:
        with torch.no_grad():
            batch_data = {
                'input_ids': batch['input_ids'],
                'attention_mask': batch['attention_mask']
            }
            logits = model(**batch_data).logits
            predictions = torch.argmax(logits, dim=-1)
            labels = batch['labels']
            all_labels.extend(labels)
            all_predictions.extend(predictions)
    accuracy = compute_accuracy(all_predictions, all_labels)
    print(&quot;Accuracy&quot;, accuracy)
    return accuracy

def compute_accuracy(predictions, labels):
    correct = 0
    for pred, label in zip(predictions, labels):
        if pred == label:
            correct += 1
    return correct / len(labels)

def my_collate_fn(batched_samples):
    texts = [example['text'] for example in batched_samples]
    labels = [example['label'] for example in batched_samples]
    text_encoding = tokenizer(texts, max_length=128, truncation=True, padding=True, return_tensors='pt')
    labels = torch.LongTensor(labels)
    return {
        'input_ids': text_encoding['input_ids'].cuda(),
        'attention_mask': text_encoding['attention_mask'].cuda(),
        'labels': labels.cuda()
    }

torch.manual_seed(64)
batch_size = 16
learning_rate = 5e-5
num_epochs = 10
model_name = &quot;roberta-base&quot;

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

model = model.cuda()

optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate, eps=1e-8)

datasets = load_dataset(&quot;gpt3mix/sst2&quot;)

train_dataloader = DataLoader(
    datasets['train'],
    batch_size=8,
    shuffle=True,
    collate_fn=my_collate_fn,
    num_workers=0
)

validation_dataloader = DataLoader(
    datasets['validation'],
    batch_size=8,
    shuffle=False,
    collate_fn=my_collate_fn,
    num_workers=0
)

best_acc = 0.0
for epoch in range(1, num_epochs + 1):
    train_one_epoch(model, train_dataloader, optimizer)
    valid_acc = evaluate(model, validation_dataloader)

</code></pre>
<pre><code>100%|██████████| 865/865 [01:27&lt;00:00,  9.89it/s]


avg loss in epoch: 0.6746856869559068


Accuracy 0.4908256880733945


100%|██████████| 865/865 [01:25&lt;00:00, 10.09it/s]


avg loss in epoch: 0.6922555248516833


Accuracy 0.4908256880733945


100%|██████████| 865/865 [01:27&lt;00:00,  9.89it/s]


avg loss in epoch: 0.6976809655310791


Accuracy 0.5091743119266054
</code></pre>
<p>Changing learning rate also does not work.</p>
",Training and Model Evaluation,automodelforsequenceclassification loss decrease changing learning rate also doe work
NaN loss when training LSTM-Attention,"<p>During model training, the loss value suddenly became Nan. Even though I change the parameters a lot, it still failed.</p>
<p>I checked the error when training and it prints the error in the output, not the input, so I think the error is in the parameters or my model.</p>
<pre><code>&gt;&gt;&gt; model_name: atae_lstm
&gt;&gt;&gt; dataset: comment
&gt;&gt;&gt; optimizer: &lt;class 'torch.optim.adam.Adam'&gt;
&gt;&gt;&gt; initializer: &lt;function kaiming_normal_ at 0x0000022D0817E0C0&gt;
&gt;&gt;&gt; lr: 1e-08
&gt;&gt;&gt; dropout: 0.1
&gt;&gt;&gt; l2reg: 0.01
&gt;&gt;&gt; num_epoch: 10
&gt;&gt;&gt; batch_size: 8
&gt;&gt;&gt; log_step: 10
&gt;&gt;&gt; embed_dim: 300
&gt;&gt;&gt; hidden_dim: 300
&gt;&gt;&gt; bert_dim: 768
&gt;&gt;&gt; pretrained_bert_name: bert-base-uncased
&gt;&gt;&gt; max_seq_len: 85
&gt;&gt;&gt; polarities_dim: 3
&gt;&gt;&gt; hops: 3
&gt;&gt;&gt; patience: 5
&gt;&gt;&gt; device: cuda
&gt;&gt;&gt; seed: 1234
&gt;&gt;&gt; valset_ratio: 0
&gt;&gt;&gt; local_context_focus: cdm
&gt;&gt;&gt; SRD: 3
&gt;&gt;&gt; model_class: &lt;class 'models.atae_lstm.ATAE_LSTM'&gt;
&gt;&gt;&gt; dataset_file: {'train': './datasets/comment/train.raw', 'test': './datasets/comment/test.raw'}
&gt;&gt;&gt; inputs_cols: ['text_indices', 'aspect_indices']
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
epoch: 0
loss: 1.4378, acc: 0.1375
loss: 1.4600, acc: 0.1125
loss: 1.4595, acc: 0.1167
loss: 1.4558, acc: 0.1187
loss: 1.4623, acc: 0.1125
loss: 1.4695, acc: 0.1062
loss: 1.4511, acc: 0.1232
loss: 1.4533, acc: 0.1203
loss: 1.4610, acc: 0.1139
loss: 1.4598, acc: 0.1150
loss: 1.4659, acc: 0.1102
**loss: nan, acc: 0.1073
loss: nan, acc: 0.1077
loss: nan, acc: 0.1152
loss: nan, acc: 0.1133**
[[ 74   0   0]
 [ 90   0   0]
 [367   0   0]]
&gt; val_acc: 0.1394, val_f1: 0.0815
&gt;&gt; saved: state_dict/atae_lstm_comment_val_acc_0.1394
</code></pre>
<p>input for model</p>
<pre><code>text_indices = tokenizer.text_to_sequence(text_left + &quot; &quot; + aspect + &quot; &quot; + text_right)
text_indices = text_indices[:tokenizer.max_seq_len]  # Truncate if longer than max_seq_len
aspect_indices = tokenizer.text_to_sequence(aspect)
aspect_indices = aspect_indices[:tokenizer.max_seq_len]

input_colses = 'atae_lstm': ['text_indices', 'aspect_indices'],
</code></pre>
<p>This is LSTM-ATTENTION model</p>
<pre><code>from layers.attention import Attention, NoQueryAttention
from layers.dynamic_rnn import DynamicLSTM
import torch
import torch.nn as nn

from layers.squeeze_embedding import SqueezeEmbedding


class ATAE_LSTM(nn.Module):
    def __init__(self, embedding_matrix, opt):
        super(ATAE_LSTM, self).__init__()
        self.opt = opt
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.squeeze_embedding = SqueezeEmbedding()
        self.lstm = DynamicLSTM(opt.embed_dim * 2, opt.hidden_dim, num_layers=1, batch_first=True)
        self.attention = NoQueryAttention(opt.hidden_dim + opt.embed_dim, score_function='bi_linear')
        self.dense = nn.Linear(opt.hidden_dim, opt.polarities_dim)
        #self.activation = nn.ReLU()  
        self.activation = nn.Tanh() 


    def forward(self, inputs):
        text_indices, aspect_indices = inputs[0], inputs[1]
        x_len = torch.sum(text_indices != 0, dim=-1)
        x_len_max = torch.max(x_len)
        aspect_len = torch.sum(aspect_indices != 0, dim=-1).float()

        x = self.embed(text_indices)
        x = self.squeeze_embedding(x, x_len)
        aspect = self.embed(aspect_indices)
        aspect_pool = torch.div(torch.sum(aspect, dim=1), aspect_len.unsqueeze(1))
        aspect = aspect_pool.unsqueeze(1).expand(-1, x_len_max, -1)
        x = torch.cat((aspect, x), dim=-1)

        # Ensure x_len is on CPU and of type int64
        h, (_, _) = self.lstm(x, x_len.cpu().to(torch.int64))
        ha = torch.cat((h, aspect), dim=-1)
        _, score = self.attention(ha)
        output = torch.squeeze(torch.bmm(score, h), dim=1)

        output = self.activation(output)  

        out = self.dense(output)
        return out
</code></pre>
<p>this is parameters I change</p>
<pre><code>    if opt.model_name.lower() == 'atae_lstm':
        opt.batch_size = 8
        opt.num_epoch = 10
        opt.lr = 1e-8
        opt.initializer= 'kaiming_normal_'
</code></pre>
",Training and Model Evaluation,nan loss training lstm attention model training loss value suddenly became nan even though change parameter lot still failed checked error training print error output input think error parameter model input model lstm attention model parameter change
spacy-llm &amp; SpanCat for address parsing,"<p>I'm currently developing a project to standardize and correct a dataset of inconsistently formatted addresses using spaCy-LLM and spaCy.SpanCat.v3. The goal is to train a model on examples of correctly labeled addresses so it can automatically normalize and categorize each component of an address into predefined labels: [&quot;NAME&quot;, &quot;STREET&quot;, &quot;BUILDING&quot;, &quot;LOCALITY&quot;, &quot;SUBAREA&quot;, &quot;AREA&quot;, &quot;CITY&quot;].</p>
<p>The dataset includes addresses in incorrect order/format and various other anomalies. An example of the raw data is: Building 8c lane No 1 mart building phase 6 bara bukhari DHA</p>
<p>The expected output for this row would be:
Name: Building 8c
Street: lane No 1
Building: mart building
Locality: bara bukhari
Subarea: phase 6
Area: DHA
City: Karachi</p>
<p>My end objective is for the model to process an input(input.txt) where the model will parse and correctly label each part of the address. I am seeking insights or advice from anyone who has used spacy-llm for similar tasks, especially for parsing addresses and formatting tasks. Additionally, I'm interested in incorporating LangChain/Ollama models into this workflow but am not planning to use Prodigy.</p>
<p>Any guidance or tips on setting up this system effectively would be greatly appreciated!</p>
<p>I set up a spaCy configuration to use a large language model (LLM) for address parsing. My config defined the model, task, and label details, and I provided example data in a JSON file.</p>
<pre><code>    [nlp]
    lang = &quot;en&quot;
    pipeline = [&quot;llm&quot;]

    [components]

    [components.llm]
    factory = &quot;llm&quot;

    [components.llm.model]
    @llm_models = &quot;spacy.Dolly.v1&quot;
    name = &quot;dolly-v2-3b&quot;

    [components.llm.task]
    @llm_tasks = &quot;my_namespace.AddressParsing.v1&quot;
    labels = [&quot;NAME&quot;, &quot;STREET&quot;, &quot;BUILDING&quot;, &quot;LOCALITY&quot;, &quot;SUBAREA&quot;, &quot;AREA&quot;, &quot;CITY&quot;]
    description = The main delivery_address column contains all the labels,
                This column has to be standardized.

    my_other_config_val = 0.3

    [components.llm.task.label_definitions]
    NAME = &quot;The exact address (plot number) of place itself.&quot;
    STREET = &quot;The name or number of the street.&quot;
    BUILDING = &quot;The name of the building.&quot;
    LOCALITY = &quot;The locality or neighborhood.&quot;
    SUBAREA = &quot;A subdivision of the area.&quot;
    AREA = &quot;A larger division of the locality.&quot;
    CITY = &quot;The name of the city.&quot;

    [components.llm.task.examples]
    @misc = &quot;spacy.FewShotReader.v1&quot;
    path = &quot;examples.json&quot;
</code></pre>
<p>I expect the system to standardize and label address data correctly based on this setup but how do i train the model. Im stuck with that part of the implementation. helppp!</p>
",Training and Model Evaluation,spacy llm spancat address parsing currently developing project standardize correct dataset inconsistently formatted address using spacy llm spacy spancat v goal train model example correctly labeled address automatically normalize categorize component address predefined label name street building locality subarea area city dataset includes address incorrect order format various anomaly example raw data building c lane mart building phase bara bukhari dha expected output row would name building c street lane building mart building locality bara bukhari subarea phase area dha city karachi end objective model process input input txt model parse correctly label part address seeking insight advice anyone ha used spacy llm similar task especially parsing address formatting task additionally interested incorporating langchain ollama model workflow planning use prodigy guidance tip setting system effectively would greatly appreciated set spacy configuration use large language model llm address parsing config defined model task label detail provided example data json file expect system standardize label address data correctly based setup train model im stuck part implementation helppp
How to Export Stanza to ONNX format?,"<p>How to export Stanza to ONNX format?
It seems impossible to just simply train the model.</p>
",Training and Model Evaluation,export stanza onnx format export stanza onnx format seems impossible simply train model
Dataset for transformer,"<p>I'm making a model using transformer for code generation. I'm confused how to get input and output of data to train decoder only transformer my db is :-</p>
<pre><code>[
    {
      &quot;code&quot;: &quot;def bubble_sort(arr):\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n-i-1):\n            if arr[j] &gt; arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n    return arr&quot;,
      &quot;function_name&quot;: &quot;bubble_sort&quot;,
      &quot;docstring&quot;: &quot;Bubble Sort repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. This pass through the list is repeated until the list is sorted.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;bubble sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def insertion_sort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j &gt;= 0 and key &lt; arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr&quot;,
      &quot;function_name&quot;: &quot;insertion_sort&quot;,
      &quot;docstring&quot;: &quot;Insertion Sort builds the sorted array  one element at a time by repeatedly picking the next element and inserting it into its correct position in the already sorted part of the array.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;insertion sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def selection_sort(arr):\n    for i in range(len(arr)):\n        min_idx = i\n        for j in range(i+1, len(arr)):\n            if arr[j] &lt; arr[min_idx]:\n                min_idx = j\n        arr[i], arr[min_idx] = arr[min_idx], arr[i]\n    return arr&quot;,
      &quot;function_name&quot;: &quot;selection_sort&quot;,
      &quot;docstring&quot;: &quot;Selection Sort repeatedly selects the smallest element from the unsorted part of the list and swaps it with the first unsorted element, effectively growing the sorted portion of the list.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;selection sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def merge_sort(arr):\n    if len(arr) &gt; 1:\n        mid = len(arr) // 2\n        L = arr[:mid]\n        R = arr[mid:]\n\n        merge_sort(L)\n        merge_sort(R)\n\n        i = j = k = 0\n\n        while i &lt; len(L) and j &lt; len(R):\n            if L[i] &lt; R[j]:\n                arr[k] = L[i]\n                i += 1\n            else:\n                arr[k] = R[j]\n                j += 1\n            k += 1\n\n        while i &lt; len(L):\n            arr[k] = L[i]\n            i += 1\n            k += 1\n\n        while j &lt; len(R):\n            arr[k] = R[j]\n            j += 1\n            k += 1\n    return arr&quot;,
      &quot;function_name&quot;: &quot;merge_sort&quot;,
      &quot;docstring&quot;: &quot;Merge Sort is a divide-and-conquer algorithm that recursively splits the list into halves, sorts each half, and then merges the sorted halves back together.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;merge sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def quick_sort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    else:\n        pivot = arr[len(arr) // 2]\n        left = [x for x in arr if x &lt; pivot]\n        middle = [x for x in arr if x == pivot]\n        right = [x for x in arr if x &gt; pivot]\n        return quick_sort(left) + middle + quick_sort(right)&quot;,
      &quot;function_name&quot;: &quot;quick_sort&quot;,
      &quot;docstring&quot;: &quot;Quick Sort is a divide-and-conquer algorithm. It selects a 'pivot' element from the list, partitions the other elements into those less than the pivot and those greater, and then recursively sorts the sub-arrays.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;quick sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def heapify(arr, n, i):\n    largest = i\n    left = 2 * i + 1\n    right = 2 * i + 2\n\n    if left &lt; n and arr[left] &gt; arr[largest]:\n        largest = left\n\n    if right &lt; n and arr[right] &gt; arr[largest]:\n        largest = right\n\n    if largest != i:\n        arr[i], arr[largest] = arr[largest], arr[i]\n        heapify(arr, n, largest)\n\n\ndef heap_sort(arr):\n    n = len(arr)\n    for i in range(n // 2 - 1, -1, -1):\n        heapify(arr, n, i)\n\n    for i in range(n-1, 0, -1):\n        arr[i], arr[0] = arr[0], arr[i]\n        heapify(arr, i, 0)\n    return arr&quot;,
      &quot;function_name&quot;: &quot;heap_sort&quot;,
      &quot;docstring&quot;: &quot;Heap Sort builds a max heap from the list, then repeatedly extracts the largest element (the root of the heap) and rebuilds the heap, thereby sorting the list.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;heap sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def shell_sort(arr):\n    n = len(arr)\n    gap = n // 2\n    while gap &gt; 0:\n        for i in range(gap, n):\n            temp = arr[i]\n            j = i\n            while j &gt;= gap and arr[j - gap] &gt; temp:\n                arr[j] = arr[j - gap]\n                j -= gap\n            arr[j] = temp\n        gap //= 2\n    return arr&quot;,
      &quot;function_name&quot;: &quot;shell_sort&quot;,
      &quot;docstring&quot;: &quot;Shell Sort is an extension of Insertion Sort that allows the exchange of far apart elements. It improves on Insertion Sort by comparing elements distant apart, gradually reducing the gap between elements to be compared.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;shell sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def counting_sort_for_radix(arr, exp):\n    n = len(arr)\n    output = [0] * n\n    count = [0] * 10\n\n    for i in range(n):\n        index = arr[i] // exp\n        count[index % 10] += 1\n\n    for i in range(1, 10):\n        count[i] += count[i - 1]\n\n    i = n - 1\n    while i &gt;= 0:\n        index = arr[i] // exp\n        output[count[index % 10] - 1] = arr[i]\n        count[index % 10] -= 1\n        i -= 1\n\n    for i in range(len(arr)):\n        arr[i] = output[i]\n\n\ndef radix_sort(arr):\n    max_val = max(arr)\n    exp = 1\n    while max_val // exp &gt; 0:\n        counting_sort_for_radix(arr, exp)\n        exp *= 10\n    return arr&quot;,
      &quot;function_name&quot;: &quot;radix_sort&quot;,
      &quot;docstring&quot;: &quot;Radix Sort processes the list digit by digit, starting from the least significant digit to the most significant digit, grouping numbers by each digit's value. It uses Counting Sort as a subroutine to sort based on individual digits.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;radix sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def counting_sort(arr):\n    max_val = max(arr)\n    count = [0] * (max_val + 1)\n    output = [0] * len(arr)\n\n    for num in arr:\n        count[num] += 1\n\n    for i in range(1, len(count)):\n        count[i] += count[i - 1]\n\n    for num in reversed(arr):\n        output[count[num] - 1] = num\n        count[num] -= 1\n\n    return output&quot;,
      &quot;function_name&quot;: &quot;counting_sort&quot;,
      &quot;docstring&quot;: &quot;Counting Sort counts the occurrences of each unique element in the list, and then uses this count information to place each element in its correct position in the output array.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;counting sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def bucket_sort(arr):\n    bucket = []\n    n = len(arr)\n    for i in range(n):\n        bucket.append([])\n\n    for j in arr:\n        index = int(n * j)\n        bucket[index].append(j)\n\n    for i in range(n):\n        bucket[i] = sorted(bucket[i])\n\n    k = 0\n    for i in range(n):\n        for j in range(len(bucket[i])):\n            arr[k] = bucket[i][j]\n            k += 1\n    return arr&quot;,
      &quot;function_name&quot;: &quot;bucket_sort&quot;,
      &quot;docstring&quot;: &quot;Bucket Sort divides the elements into several buckets. Each bucket is then sorted individually, either using a different sorting algorithm or by recursively applying Bucket Sort.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;bucket sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    }
  ]
</code></pre>
<p>so it is simple db of sorting algo can someone pls help to change it into input and output form</p>
<p>What should be input and output to pass to model from the db</p>
",Training and Model Evaluation,dataset transformer making model using transformer code generation confused get input output data train decoder transformer db simple db sorting algo someone pls help change input output form input output pas model db
How to get the labels for my LLavaOneVision model?,"<p>I am trying to train a LLavaOneVision model, unfortunately, I am not being able to understand how to process the <strong>labels to put it into the model</strong> for fine tuning the LLavaOneVision( <a href=""https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf"" rel=""nofollow noreferrer"">https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf</a> ). The collate function,the model's forward function and the training step is given below is given below.</p>
<pre><code>def collate_fn(self, batch):
    images = []
    texts = []
    answers = []
    for example in batch:
        question, answer, rgb_image_np = example
        images.append(rgb_image_np)
        answers.append(answer)
        conversation = [
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: [
                    {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: question},  # Add question text
                    {&quot;type&quot;: &quot;image&quot;},                  # Add the image as a type
                ],
            },
            {
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;content&quot;: [
                    {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: answer},   # Add the assistant's answer
                ],
            }
        ]
        text_prompt = self.processor.apply_chat_template(conversation)
        texts.append(text_prompt)

    # Prepare inputs (image + text)
    model_inputs = self.processor(
            images=images,
            text=texts,
            return_tensors=&quot;pt&quot;,
            padding=True
        ).to(torch.float16)

    # Prepare labels
    labels = ???  # How to prepare the labels?

    # Add labels to the batch dictionary
    return {
            &quot;input_ids&quot;: model_inputs[&quot;input_ids&quot;],
            &quot;labels&quot;: labels
        }
</code></pre>
<pre><code>class LlavaOnevisionModule(pl.LightningModule):
    def __init__(self, model_name, processor, learning_rate=2e-5):
        super().__init__()
        self.model_name = model_name
        self.learning_rate = learning_rate
        
        self.processor = processor
        self.model = LlavaOnevisionForConditionalGeneration.from_pretrained(
            model_name, 
            low_cpu_mem_usage=True,
            # use_flash_attention_2=True,

            # torch_dtype=torch.float16
        )

        self.config = self.model.config
        
        self.pad_token_id = (self.processor.tokenizer.eos_token_id 
                             if self.processor.tokenizer.pad_token_id is None 
                             else self.processor.tokenizer.pad_token_id)

    def forward(self, input_ids, labels):
        # Create a dictionary for inputs to match the expected input format of the model
        inputs = {
            'input_ids': input_ids.to(self.device),
            'labels': labels.to(self.device)  # Move labels to the correct device
        }

        # Pass the inputs through the model (which expects input_ids and labels)
        outputs = self.model(**inputs)

        return outputs


    def training_step(self, batch, batch_idx):
        # Unpack the batch
        input_ids = batch['input_ids']
        labels = batch['labels']
        
        # Forward pass
        outputs = self(input_ids=input_ids, labels=labels)
        
        # Calculate loss
        loss = outputs.loss
        
        # Log training loss
        self.log('train_loss', loss)
        return loss
</code></pre>
<p>I tried cloning the input_ids and passing it as the labels. I am not sure if thats the correct way. I have also heard that I might have to do a right shifting of labels, however I believe thats already implemented inside the model's own forward function.  I have also tried using the <code>processor.tokenizer(text=answers,return_tensors=&quot;pt&quot;,padding=True,return_token_type_ids=False)</code>. However this also returns saying something like the input_id length and the label length are not equal.</p>
<p>How do I process the labels then?</p>
",Training and Model Evaluation,get label llavaonevision model trying train llavaonevision model unfortunately able understand process label put model fine tuning llavaonevision collate function model forward function training step given given tried cloning input id passing label sure thats correct way also heard might right shifting label however believe thats already implemented inside model forward function also tried using however also return saying something like input id length label length equal process label
SBERT Fine-tuning always stops before finish all epochs,"<p>I'm working on a project using the SBERT pre-trained models (specifically <a href=""https://huggingface.co/nreimers/MiniLM-L6-H384-uncased"" rel=""nofollow noreferrer"">MiniLM</a>) for a text classification project with 995 classifications. I am following the steps laid out <a href=""https://www.sbert.net/docs/sentence_transformer/training_overview.html#why-finetune"" rel=""nofollow noreferrer"">here</a> for the most part and everything seems to run.</p>
<p>My issue occurs when actually training the model. No matter what values I set in the training arguments the training always seems to end early and never completes all the batches. For example, I set <code>num_train_epochs=1</code> but it only gets up to 0.49 epochs. If <code>num_train_epochs=4</code>, it always ends at 3.49 epochs.</p>
<p>Here is my code:</p>
<pre><code>from datasets import load_dataset
from sentence_transformers import (
    SentenceTransformer,
    SentenceTransformerTrainer,
    SentenceTransformerTrainingArguments,
    SentenceTransformerModelCardData,
)
from sentence_transformers.losses import BatchAllTripletLoss
from sentence_transformers.training_args import BatchSamplers
from sentence_transformers.evaluation import TripletEvaluator

model = SentenceTransformer(
    &quot;nreimers/MiniLM-L6-H384-uncased&quot;,
    model_card_data=SentenceTransformerModelCardData(
        language=&quot;en&quot;,
        license=&quot;apache-2.0&quot;,
        model_name=&quot;all-MiniLM-L6-v2&quot;,
    )
)

loss = BatchAllTripletLoss(model)
# Loss overview: https://www.sbert.net/docs/sentence_transformer/loss_overview.html
# This particular loss method: https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#batchalltripletloss


# training args: https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments
args = SentenceTransformerTrainingArguments(
    # Required parameter:
    output_dir=&quot;finetune/model20240924&quot;,
    # Optional training parameters:
    num_train_epochs=1,
    max_steps = -1,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=1e-5,
    warmup_ratio=0.1,
    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16
    bf16=False,  # Set to True if you have a GPU that supports BF16
    batch_sampler=BatchSamplers.GROUP_BY_LABEL,  # 
    # Optional tracking/debugging parameters:
    eval_strategy=&quot;no&quot;,
    eval_steps=100,
    save_strategy=&quot;epoch&quot;,
   # save_steps=100,
    save_total_limit=2,
    logging_steps=100,
    run_name=&quot;miniLm-triplet&quot;,  # Will be used in W&amp;B if `wandb` is installed
)

trainer = SentenceTransformerTrainer(
    model=model,
    args=args,
    train_dataset=trainDataset,
    eval_dataset=devDataset,
    loss=loss,
    #evaluator=dev_evaluator,
)
trainer.train()
</code></pre>
<p>Note that I am not using an evaluator because we are creating the model and testing it after the fact with a dedicated test set of values. My dataset is structured as:</p>
<pre><code>Dataset({
    features: ['Title', 'Body', 'label'],
    num_rows: 23961
})
</code></pre>
<p>with the <code>dev</code> dataset being the same structure, only with fewer rows. This gives the following output:</p>
<pre><code> [1473/2996 57:06 &lt; 59:07, 0.43 it/s, Epoch 0/1]
Step    Training Loss
100     1.265600
200     0.702700
300     0.633900
400     0.505200
500     0.481900
600     0.306800
700     0.535600
800     0.369800
900     0.265400
1000    0.345300
1100    0.516700
1200    0.372600
1300    0.392300
1400    0.421900

TrainOutput(global_step=1473, training_loss=0.5003972503496366, metrics={'train_runtime': 3427.9198, 'train_samples_per_second': 6.99, 'train_steps_per_second': 0.874, 'total_flos': 0.0, 'train_loss': 0.5003972503496366, 'epoch': 0.4916555407209613})
</code></pre>
<p>As much as I adjust the values I cannot get it to complete all of the batches. How to resolve this issue?</p>
",Training and Model Evaluation,sbert fine tuning always stop finish epoch working project using sbert pre trained model specifically minilm text classification project classification following step part everything seems run issue occurs actually training model matter value set training argument training always seems end early never completes batch example set get epoch always end epoch code note using evaluator creating model testing fact dedicated test set value dataset structured dataset structure fewer row give following output much adjust value get complete batch resolve issue
How to Dynamically Replace Placeholders in Text Using TensorFlow and SentencePiece,"<p>Description:
I’m working on a sequence-to-sequence model using TensorFlow and SentencePiece to dynamically replace placeholders like {NAMESPACE} in user inputs. My training data includes examples with both the placeholder and actual values. During inference, I want the model to replace {NAMESPACE} with the user-specified value without hardcoding it. How can I ensure the model learns this pattern effectively and handles dynamic replacements during inference?</p>
<p>Input : list pods in namespace abc
Actual Output : oc get pods -n def</p>
<p>Expected output : oc get pods -n abc</p>
<p>I know I can implement NER .. just wanted to know if it can be done through model training
I don’t want to use pre trained models.</p>
",Training and Model Evaluation,dynamically replace placeholder text using tensorflow sentencepiece description working sequence sequence model using tensorflow sentencepiece dynamically replace placeholder like namespace user input training data includes example placeholder actual value inference want model replace namespace user specified value without hardcoding ensure model learns pattern effectively handle dynamic replacement inference input list pod namespace abc actual output oc get pod n def expected output oc get pod n abc know implement ner wanted know done model training want use pre trained model
Which Deep Learning Algorithm does Spacy uses when we train Custom model?,"<p>When we train custom model, I do see we have dropout and n_iter parameters to tune, but which deep learning algorithm does Spacy Uses to train Custom Models? Also, when Adding new Entity type is it good to create blank or train it on existing model?</p>
",Training and Model Evaluation,deep learning algorithm doe spacy us train custom model train custom model see dropout n iter parameter tune deep learning algorithm doe spacy us train custom model also adding new entity type good create blank train existing model
write_csv=True does not write the csv for the result while using SentenceTransformerEvaluator,"<p>I am trying to train a sentence transformer model with custom data to use it for semantic search application. My data includes a chunk of text and corresponding search string queries. I am using the <a href=""https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#informationretrievalevaluator"" rel=""nofollow noreferrer"">InformationRetrievalEvaluator</a>.</p>
<p>Below is my training script:</p>
<pre><code># Enable logging
logging.basicConfig(format='%(asctime)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S',
                    level=logging.INFO,
                    handlers=[LoggingHandler()])


description = [txt for txt in train_df.llm_image_description]
query = [q for q in train_df.llm_search_strs_with]

train_examples = Dataset.from_dict(
    {
    &quot;description&quot;: description,
    &quot;query&quot;: query
    }
)

corpus = load_from_pickle(&quot;data/IR_eval_data/corpus.pkl&quot;)
queries = load_from_pickle(&quot;data/IR_eval_data/queries.pkl&quot;)
qrels = load_from_pickle(&quot;data/IR_eval_data/qrels.pkl&quot;)

evaluator = InformationRetrievalEvaluator(queries=queries, corpus=corpus, relevant_docs=qrels,
                                          name='eval',show_progress_bar=True, write_csv=True,precision_recall_at_k=[1,2,3,4,5],mrr_at_k=[2,3,4,5],accuracy_at_k=[1,2,3,4,5],)

# Initialize a pre-trained model
# model = SentenceTransformer('distilbert-base-nli-mean-tokens') 
model = SentenceTransformer(&quot;multi-qa-mpnet-base-cos-v1&quot;) #https://www.sbert.net/docs/sentence_transformer/pretrained_models.html
                                                                                                                                                
# Define the loss function
train_loss = losses.MultipleNegativesRankingLoss(model=model)

# Define training arguments
training_args = SentenceTransformerTrainingArguments(
    eval_strategy='epoch',
    logging_strategy='epoch',
    save_strategy='epoch',
    save_total_limit=2,
    seed=10,
    load_best_model_at_end=True,
    metric_for_best_model='eval_cosine_recall@2',
    num_train_epochs=num_epochs,
    per_device_train_batch_size=train_batch_size,
    output_dir=model_save_path,
)
#This is the latest function
trainer = SentenceTransformerTrainer(
    model=model,
    args=training_args,
    loss=train_loss,
    train_dataset=train_examples,
    evaluator=evaluator,
)

train_result = trainer.train()
</code></pre>
<p>Below output is being printed at the end of each epoch:</p>
<p><a href=""https://i.sstatic.net/OTqmYk18.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OTqmYk18.png"" alt=""Training evaluation output"" /></a></p>
<p>I have below 2 questions:</p>
<ol>
<li>Even when the <code>write_csv=True</code>, at the end of training I am not able to find any evaluation csv file in my output file path. I am not sure why it is not writing it out. I am also not able to retrieve the above result from the <strong>trainer</strong> or <strong>evaluator</strong> object methods. Can someone please tell me how can I : either get the csv to be written out to output path or   get the dictionary/dataframe from <strong>trainer</strong> or <strong>evaluator</strong> objects?</li>
<li>My result shows validation loss column with no information. How can I enable logging for validation loss? it will be important to track it.</li>
</ol>
<p>I am using below versions:</p>
<pre><code>transformers version: 4.44.2
sentence_transformers version: 3.0.1
PyTorch version: 2.2.0
</code></pre>
",Training and Model Evaluation,write csv true doe write csv result using sentencetransformerevaluator trying train sentence transformer model custom data use semantic search application data includes chunk text corresponding search string query using informationretrievalevaluator training script output printed end epoch question even end training able find evaluation csv file output file path sure writing also able retrieve result trainer evaluator object method someone please tell either get csv written output path get dictionary dataframe trainer evaluator object result show validation loss column information enable logging validation loss important track using version
RASA [3.6.20] NLU model &quot;training successfully&quot; immediately but only returning &#39;null&#39;,"<p>I am trying to upgrade from RASA 2 to RASA 3, and am having a problem I don’t understand. I’m trying to train an NLU model, and when I run the ‘rasa train’ command I get terminal output indicating that my model has trained successfully–but the epoch bar never displays, and training seems to complete immediately. When I open the model using ‘rasa shell’, it returns null with 0 confidence for any input.</p>
<p>Asked at the RASA forums, but haven't gotten any responses. Any ideas what could be causing this issue? Thanks.</p>
<p>I’m running RASA version 3.6.20 with spaCy version 3.7.6 in a python 3.9 virtual environment. Here’s my config file:</p>
<pre><code>version: “3.1” 
recipe: “default.v1” 
language: en 
pipeline:
name: SpacyNLP 
model: en_core_web_sm
name: SpacyTokenizer
name: SpacyFeaturizer
name: SpacyEntityExtractor
name: RegexFeaturizer 
analyzer: char_wb 
min_ngram: 1 
max_ngram: 4
name: DIETClassifier 
epochs: 100 
constrain_similarities: true
name: EntitySynonymMapper 
policies:
name: MemoizationPolicy 
assistant_id: 20240813-193423-felt-modal
</code></pre>
<p>here is the last part of the terminal output from training, note no epoch bar:</p>
<pre><code>2024-08-29 17:45:41 INFO rasa.nlu.utils.spacy_utils - Trying to load SpaCy model with name ‘en_core_web_sm’. 
2024-08-29 17:45:42 INFO rasa.nlu.utils.spacy_utils - Trying to load SpaCy model with name ‘en_core_web_sm’. 
2024-08-29 17:45:43 INFO rasa.engine.training.hooks - Starting to train component ‘RegexFeaturizer’. 
2024-08-29 17:45:43 INFO rasa.engine.training.hooks - Finished training component ‘RegexFeaturizer’. 
2024-08-29 17:45:43 INFO rasa.engine.training.hooks - Starting to train component ‘DIETClassifier’. 
2024-08-29 17:45:43 INFO rasa.engine.training.hooks - Finished training component ‘DIETClassifier’. 
2024-08-29 17:45:43 INFO rasa.engine.training.hooks - Starting to train component ‘EntitySynonymMapper’. 
2024-08-29 17:45:43 INFO rasa.engine.training.hooks - Finished training component ‘EntitySynonymMapper’. 
Your Rasa model is trained and saved at ‘models/nlu-20240829-174541-heartless-bayou.tar.gz’. 
Rasa model training completed successfully.
</code></pre>
<p>Here’s the terminal output from running ‘rasa shell’ with a sample input:</p>
<pre><code>2024-08-29 18:05:26 INFO rasa.core.processor - Loading model models/nlu-20240829-        174541-heartless-bayou.tar.gz… 
2024-08-29 18:05:26 INFO rasa.nlu.utils.spacy_utils - Trying to load SpaCy model with name ‘en_core_web_sm’. 
2024-08-29 18:05:28 INFO rasa.nlu.utils.spacy_utils - Trying to load SpaCy model with name ‘en_core_web_sm’. 
NLU model loaded. Type a message and press enter to parse it. 
Next message: Hello, this is a test message. 
{ “text”: “Hello, this is a test message.”, 
“intent”: { “name”: null, “confidence”: 0.0 }, 
“entities”: , 
“text_tokens”: [ [ 0, 5 ], [ 5, 6 ], [ 7, 11 ], [ 12, 14 ], [ 15, 16 ], [ 17, 21 ], [ 22, 29 ], [ 29, 30 ] ], 
“intent_ranking”:  } 
Next message:
</code></pre>
<p>Additionally, the models produced vary in size considerably. Here are one of the 'null' response only RASA 3 models, another RASA 3 model that trained for 3 hours and only produces very low confidence results, and a RASA 2 model I trained today (in 20 minutes) for size comparison purposes.First number (e.g., 2456, is filesize). The RASA 2 model is almost twice the size of the RASA 3 model that sort of works, and many times larger than the only RASA 3 models I've been able to create since then.</p>
<pre><code>RASA 3, 'null' only: 2456 Aug 30 14:20 nlu-20240830-142056-poky-turret.tar.gz
RASA 3, 3+ hour train:19923109 Aug 13 16:08 nlu-20240730-180020-hot-event.tar.gz
RASA 2: 37456829 Sep 3 18:24 nlu-20240903-182449.tar.gz
</code></pre>
",Training and Model Evaluation,rasa nlu model training successfully immediately returning null trying upgrade rasa rasa problem understand trying train nlu model run rasa train command get terminal output indicating model ha trained successfully epoch bar never display training seems complete immediately open model using rasa shell return null confidence input asked rasa forum gotten response idea could causing issue thanks running rasa version spacy version python virtual environment config file last part terminal output training note epoch bar terminal output running rasa shell sample input additionally model produced vary size considerably one null response rasa model another rasa model trained hour produce low confidence result rasa model trained today minute size comparison purpose first number e g filesize rasa model almost twice size rasa model sort work many time larger rasa model able create since
Are there any potential issues training a T5-small from scratch on a task with very limited vocabulary?,"<p>Suppose you would like to train a sequence-to-sequence model like T5-small <strong>from scratch</strong> on a task where the vocabulary is quite limited compared to the tokenizer of T5 which was trained on much larger vocabulary.</p>
<p>For instance, the data have the following format:</p>
<pre><code>Can you please add A and B?

e.g.
Can you please add 45 and 56?
Can you please add 87 and 34?
</code></pre>
<p><code>A</code> and <code>B</code> are just placeholders for integer numbers.</p>
<p>Instead the tokenizer of T5 was trained to represent a vocabulary of approximately something like 32-50K tokens.</p>
<p>What would be some consideration and issues taken into account since in the data only a few tokens change every time?</p>
<p>Basically only tokens <code>A</code> and <code>B</code> change every time.</p>
<p>Is that still possible?</p>
",Training and Model Evaluation,potential issue training small scratch task limited vocabulary suppose would like train sequence sequence model like small scratch task vocabulary quite limited compared tokenizer wa trained much larger vocabulary instance data following format placeholder integer number instead tokenizer wa trained represent vocabulary approximately something like k token would consideration issue taken account since data token change every time basically token change every time still possible
Issue with Vanna AI Handling Multiple Schemas in the Same Database,"<p>I am working on implementing the “Talk to Data” feature using Vanna AI for our organization. As part of our setup, we are using a multi-tenant database architecture where each tenant’s data is stored in a separate schema within the same PostgreSQL database.</p>
<p>To accommodate this structure, I planned to create separate Vanna instances for each schema within our PostgreSQL database. Each instance is trained with a distinct training dataset specific to its corresponding schema. The intention behind this approach is to ensure that Vanna queries are schema-specific and do not cross-reference or combine data from different schemas or tenants.</p>
<p>However, I have encountered an issue where Vanna AI appears to be mixing up training datasets across different schemas. For example, when querying the instance that should only access Schema ‘X’, I noticed that Vanna is incorporating elements from the training data of Schema ‘Y’. This is happening even though I have explicitly trained each Vanna instance separately, ensuring that each has access only to its respective schema and training dataset.</p>
<p>Here’s a high-level illustration of what I’m seeing:</p>
<ul>
<li>Schema X: Intended to have its own Vanna instance trained with only Schema X data.</li>
<li>Schema Y: Intended to have a separate Vanna instance trained with only Schema Y data.</li>
</ul>
<p>But during operation, queries targeting Schema X sometimes incorrectly use patterns or insights from the Schema Y dataset and vice versa. For example:</p>
<ul>
<li>When querying customer information from Schema X, Vanna unexpectedly references fields or entities that exist only in Schema Y.</li>
<li>Similarly, queries that should strictly adhere to Schema X’s context are influenced by data relationships unique to Schema Y, leading to incorrect or mixed query results.</li>
</ul>
<p>Why is this happening? It seems that Vanna AI is not isolating the training datasets as expected, leading to cross-schema contamination in its queries.</p>
<p>suggest a solution to ensure that each Vanna instance operates within its defined schema boundaries without cross-referencing other schemas’ training data.</p>
<p>I can show you a demo code of what I did</p>
<pre><code>from vanna_ai import VannaManager

# Simulated function to initialize Vanna instances for each schema
def initialize_vanna_instances(schemas):
    vanna_instances = {}
    for schema in schemas:
        # Initialize a Vanna instance for each schema
        vanna_instance = VannaManager(schema_name=schema)
        # Train each instance with the schema-specific dataset
        vanna_instance.train(training_data_path=f&quot;/path/to/{schema}_training_data.csv&quot;)
        vanna_instances[schema] = vanna_instance
    return vanna_instances

# Simulated function to query a specific Vanna instance
def query_vanna_instance(vanna_instance, query):
    # Generate SQL using the Vanna instance
    sql_query = vanna_instance.generate_sql(question=query)
    print(f&quot;Generated SQL for schema {vanna_instance.schema_name}: {sql_query}&quot;)
    # Run SQL query against the schema-specific database
    result = vanna_instance.run_sql(sql=sql_query)
    return result

# Initialize Vanna instances for two different schemas
schemas = ['schema_x', 'schema_y']
vanna_instances = initialize_vanna_instances(schemas)

# Example queries for different schemas
query_x = &quot;SELECT * FROM customers WHERE purchase_date &gt; '2024-01-01';&quot;
query_y = &quot;SELECT * FROM orders WHERE order_amount &gt; 100;&quot;

# Query the instance for schema_x
result_x = query_vanna_instance(vanna_instances['schema_x'], query_x)
print(f&quot;Result for Schema X: {result_x}&quot;)

# Query the instance for schema_y
result_y = query_vanna_instance(vanna_instances['schema_y'], query_y)
print(f&quot;Result for Schema Y: {result_y}&quot;)
</code></pre>
<p>Explanation of the Code</p>
<ol>
<li><p>Initialize Vanna Instances: The initialize_vanna_instances function initializes a Vanna AI instance for each schema provided in the schemas list. Each instance is trained separately with its own schema-specific dataset.</p>
</li>
<li><p>Query Vanna Instance: The query_vanna_instance function demonstrates how a query is executed using the Vanna instance tied to a specific schema. It generates an SQL query using Vanna AI’s capabilities and runs it against the schema-specific database.</p>
</li>
<li><p>Potential Issue Illustration:</p>
<p>•   Schema X and Schema Y have their own Vanna instances and training datasets.</p>
<p>•   When querying Schema X with query_x, we expect it to only use data and context from Schema X.</p>
<p>•   Similarly, querying Schema Y with query_y should only reference Schema Y’s data.</p>
<p>•   Issue: Despite separate instances and datasets, you notice that queries are mixing data or context from different schemas.</p>
</li>
</ol>
",Training and Model Evaluation,issue vanna ai handling multiple schema database working implementing talk data feature using vanna ai organization part setup using multi tenant database architecture tenant data stored separate schema within postgresql database accommodate structure planned create separate vanna instance schema within postgresql database instance trained distinct training dataset specific corresponding schema intention behind approach ensure vanna query schema specific cross reference combine data different schema tenant however encountered issue vanna ai appears mixing training datasets across different schema example querying instance access schema x noticed vanna incorporating element training data schema happening even though explicitly trained vanna instance separately ensuring ha access respective schema training dataset high level illustration seeing schema x intended vanna instance trained schema x data schema intended separate vanna instance trained schema data operation query targeting schema x sometimes incorrectly use pattern insight schema dataset vice versa example querying customer information schema x vanna unexpectedly reference field entity exist schema similarly query strictly adhere schema x context influenced data relationship unique schema leading incorrect mixed query result happening seems vanna ai isolating training datasets expected leading cross schema contamination query suggest solution ensure vanna instance operates within defined schema boundary without cross referencing schema training data show demo code explanation code initialize vanna instance initialize vanna instance function initializes vanna ai instance schema provided schema list instance trained separately schema specific dataset query vanna instance query vanna instance function demonstrates query executed using vanna instance tied specific schema generates sql query using vanna ai capability run schema specific database potential issue illustration schema x schema vanna instance training datasets querying schema x query x expect use data context schema x similarly querying schema query reference schema data issue despite separate instance datasets notice query mixing data context different schema
How to Improve Apache OpenNLP Output Using Custom Trained Models in Spring Boot?,"<p>I'm currently using Apache OpenNLP with a Spring Boot REST API, and I'm facing some issues with the output of the model. Here's what I'm experiencing:</p>
<ul>
<li><p><strong>Input text:</strong> <code>&quot;I'm Arun Vinc. What is your name?&quot;</code></p>
</li>
<li><p><strong>Output:</strong> <code>'m ? arun i is name vinc what your</code></p>
</li>
</ul>
<p>The output isn't coherent, and I suspect that the pre-trained models I'm using might not be well-suited for my application. After researching online, I found that I might need to train my own <code>.bin</code> models using appropriate corpora datasets to get better results.</p>
<p><strong>Spring Boot REST API:</strong> This is the code I’m using to load the models and perform the NLP tasks:</p>
<pre><code>@Service
public class OpenNLPService {

    private SentenceDetectorME sentenceDetector;
    private TokenizerME tokenizer;
    private POSTaggerME posTagger;

    public OpenNLPService() throws Exception {
        try (InputStream sentenceModelIn = getClass().getClassLoader().getResourceAsStream(&quot;models/en-sent.bin&quot;);
             InputStream tokenModelIn = getClass().getClassLoader().getResourceAsStream(&quot;models/en-token.bin&quot;);
             InputStream posModelIn = getClass().getClassLoader().getResourceAsStream(&quot;models/en-pos-maxent.bin&quot;)) {

            SentenceModel sentenceModel = new SentenceModel(sentenceModelIn);
            TokenizerModel tokenModel = new TokenizerModel(tokenModelIn);
            POSModel posModel = new POSModel(posModelIn);

            this.sentenceDetector = new SentenceDetectorME(sentenceModel);
            this.tokenizer = new TokenizerME(tokenModel);
            this.posTagger = new POSTaggerME(posModel);
        }
    }

    public String processText(String text) {
        String[] sentences = sentenceDetector.sentDetect(text);
        StringBuilder result = new StringBuilder();

        for (String sentence : sentences) {
            String[] tokens = tokenizer.tokenize(sentence);
            String[] posTags = posTagger.tag(tokens);
            result.append(String.join(&quot; &quot;, tokens)).append(&quot;\n&quot;);
        }

        return result.toString();
    }
}
</code></pre>
<h3>What I’ve Tried</h3>
<ol>
<li><p><strong>Pre-Trained Models:</strong> I’m currently using the default pre-trained models (<code>en-sent.bin</code>, <code>en-token.bin</code>, <code>en-pos-maxent.bin</code>) from Apache OpenNLP.</p>
</li>
<li><p><strong>Research:</strong> I’ve read that the accuracy and relevance of these models can be improved by training them with a more suitable corpus. However, I’m not sure which corpora would be best for general English text processing, nor how to properly train these models.</p>
</li>
</ol>
<h3>Error Messages and Debugging</h3>
<ul>
<li><strong>No specific errors</strong>, but the output is not meaningful, which suggests that the model may not be well-suited for the input data.</li>
</ul>
",Training and Model Evaluation,improve apache opennlp output using custom trained model spring boot currently using apache opennlp spring boot rest api facing issue output model experiencing input text output output coherent suspect pre trained model using might well suited application researching online found might need train model using appropriate corpus datasets get better result spring boot rest api code using load model perform nlp task tried pre trained model currently using default pre trained model apache opennlp research read accuracy relevance model improved training suitable corpus however sure corpus would best general english text processing properly train model error message debugging specific error output meaningful suggests model may well suited input data
How to evaluate SciSpaCy&#39;s entity linking,"<p>I'm using <a href=""https://github.com/allenai/scispacy#entitylinker"" rel=""nofollow noreferrer"">SciSpaCy's Entity Linker</a> with a custom knowledge base. As I'm updating some components of my application (e.g. the underlying language model, sentence tokenization pipeline, the knowledge base itself, etc), I'm noticing that (1) the number of entities that the application picks up changes and (2) the linked concepts themselves change (not the detected entities but the concepts that are linked to these entities). With this in mind, I'd like to be able to evaluate my entity-linking application.</p>
<p>Unfortunately, I cannot seem to find any resources for that. I was hoping to find either an evaluation library of some sort (assuming we are not just interested in a confusion matrix) or a &quot;gold standard&quot; dataset with entities in various forms (e.g. abbreviated, inflected, etc) and the expected linked concept.</p>
<p>I'm afraid I'm a novice in this field which is why I'm reaching out here, hoping that anyone might be able to point me to a set of useful resources or share some tips with me.</p>
<p>Many thanks in advance.</p>
",Training and Model Evaluation,evaluate scispacy entity linking using scispacy entity linker custom knowledge base updating component application e g underlying language model sentence tokenization pipeline knowledge base etc noticing number entity application pick change linked concept change detected entity concept linked entity mind like able evaluate entity linking application unfortunately seem find resource wa hoping find either evaluation library sort assuming interested confusion matrix gold standard dataset entity various form e g abbreviated inflected etc expected linked concept afraid novice field reaching hoping anyone might able point set useful resource share tip many thanks advance
High VRAM usage after loading peft weights with the base model,"<p>I have fine-tuned llama-3 using qlora technique and wanted to do inference.
I use the below code for inference.</p>
<pre><code>bnb_config = BitsAndBytesConfig(
    load_in_4bit= True,
    bnb_4bit_quant_type= &quot;nf4&quot;,
    bnb_4bit_compute_dtype= torch.bfloat16,
    bnb_4bit_use_double_quant= False)

access_token = ''

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,    quantization_config = bnb_config,
    device_map=&quot;auto&quot;,    #use_auth_token=True/
    token = access_token
    )

peft_model = PeftModel.from_pretrained(base_model, path)
</code></pre>
<p>When I load the base model in 4-bit precision, the VRAM usage is around 6 GB. However, when I load the PEFT model using the same function, the VRAM usage spikes to 19 GB. During fine-tuning, only 11% of the parameters are trainable.</p>
<p>I'm trying to understand why there's such a large increase in VRAM usage after loading the adapter weights. Could it be because the QLoRA adapter weights are being loaded in 16-bit precision? Is there a way to load these adapter weights in 8-bit?</p>
<p>Any insights or suggestions would be greatly appreciated!</p>
",Training and Model Evaluation,high vram usage loading peft weight base model fine tuned llama using qlora technique wanted inference use code inference load base model bit precision vram usage around gb however load peft model using function vram usage spike gb fine tuning parameter trainable trying understand large increase vram usage loading adapter weight could qlora adapter weight loaded bit precision way load adapter weight bit insight suggestion would greatly appreciated
How to do NLP fill-mask with restricted possible inputs,"<p>I want to use NLP to fill in a masked word in a text, but instead of choosing from all possible words I want to find which is the more likely of two candidate words. For example, imagine I have a sentence &quot;The [MASK] was stuck in the tree&quot; and I want to evaluate whether &quot;kite&quot; or &quot;bike&quot; is the more likely word.</p>
<p>I know how to find the globally most probable words using hugging face's fill-mask pipeline</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM

# Define the input sentence with a masked word
input_text = &quot;The [MASK] was stuck in the tree&quot;

# Load the pre-trained model and tokenizer
model_name = &quot;bert-base-cased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForMaskedLM.from_pretrained(model_name)

# Tokenize the input sentence
tokenized_text = tokenizer.tokenize(input_text)

# Use the pipeline to generate a list of predicted words and probabilities
mlm = pipeline(&quot;fill-mask&quot;, model=model, tokenizer=tokenizer)
results = mlm(input_text)

# Print outputs
for result in results:
    token = result[&quot;token_str&quot;]
    print(f&quot;{token:&lt;15} {result['score']}&quot;)
</code></pre>
<p>However if &quot;bike&quot; and &quot;kite&quot; arent in the first few most probable words this doesnt help.</p>
<p>How can I use fill-mask to find the probability of specific masks?</p>
<p>P.S. I'm not sure if overflow is the best place to post this question, there doesnt seem to be a place for nlp specific questions.</p>
",Training and Model Evaluation,nlp fill mask restricted possible input want use nlp fill masked word text instead choosing possible word want find likely two candidate word example imagine sentence mask wa stuck tree want evaluate whether kite bike likely word know find globally probable word using hugging face fill mask pipeline however bike kite arent first probable word doesnt help use fill mask find probability specific mask p sure overflow best place post question doesnt seem place nlp specific question
Get contextual entropy of each word in a sentence,"<p>I am trying to get the contextual entropy of each word within sentences of a dataset.
I am following the definition of Contextual Entropy provided in <a href=""https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00612/118718"" rel=""nofollow noreferrer"">this</a> paper, so for each position within a sentence, I compute the probability of each word of the vocabulary given the sentence (left) context, and then I sum them. This process is being very time-consuming though, so I need advice on alternative ways of computing the CE.</p>
<p>So far my script looks roughly like this:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;google-bert/bert-base-chinese&quot;)
model = AutoModelForMaskedLM.from_pretrained(&quot;google-bert/bert-base-chinese&quot;)

pipe = pipeline(&quot;fill-mask&quot;, model=&quot;google-bert/bert-base-chinese&quot;)

BERTvocab = tokenizer.get_vocab()

for sentence in all_sentences:
    for i,word in enumerate(sentence):
        # for each word the context is the left context + [MASK]
        context = sentence[:i+1]
        context = context+'[MASK]'

        for vw in BERTvocab:
            # get the probability distribution of each word in the [MASK] position
            teres = pipe(context, targets= [vw])
            softtyent = teres[0][&quot;score&quot;]
            
            # compute the p(w|context)*log(p(w|context))
            temp_tyent = softtyent*math.log(softtyent)
            singles.append(temp_tyent)
        
         entropy = -sum(singles)

</code></pre>
<p>My questions are:</p>
<ol>
<li>Is my understanding of the Contextual Entropy right?</li>
<li>Considering that the dataset has 400 sentences (long 15 words on average), is there a faster and computationally &quot;more elegant&quot; way to do this? It's taking me ages to run the script...</li>
</ol>
<p>Thanks!</p>
",Training and Model Evaluation,get contextual entropy word sentence trying get contextual entropy word within sentence dataset following definition contextual entropy provided paper position within sentence compute probability word vocabulary given sentence left context sum process time consuming though need advice alternative way computing ce far script look roughly like question understanding contextual entropy right considering dataset ha sentence long word average faster computationally elegant way taking age run script thanks
"Is it necessary for torch_dtype when loading a model and the precision for trainable weights to be different? If so, why?","<p>According to <a href=""https://github.com/huggingface/peft/issues/341#issuecomment-1884911753"" rel=""nofollow noreferrer"">this comment</a> in the huggingface/peft package, if a model is loaded in fp16, the trainable weights must be cast to fp32. From this comment, I understand that generally, the <code>torch_dtype</code> used when loading a model and the precision used for training must be different. Why is it necessary to change the precision? Also, does this principle apply to both fine-tuning and continual pretraining?</p>
<p>As a minimal working example, I'm attempting to perform a continual pretraining on <a href=""https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/"" rel=""nofollow noreferrer"">microsoft/Phi-3-mini-128k-instruct</a>, whose default <code>torch_dtype</code> is <a href=""https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/blob/bb5bf1e4001277a606e11debca0ef80323e5f824/config.json#L135"" rel=""nofollow noreferrer"">bfloat16</a>. When loading the model with <code>torch_dtype=torch.float16</code>, training commenced when the precision for trainable weights was set to <code>TrainingArguments(fp16=False, bf16=True)</code> (i.e. different precision for model loading and trainable weights). However, when the precision for trainable weights was set to <code>TrainingArguments(fp16=True, bf16=False)</code> (i.e. same precision for model loading and trainable weights), an error <code>raise ValueError(&quot;Attempting to unscale FP16 gradients.&quot;)</code> occurred, preventing the start of training. The execution environment was an NVIDIA RTX3060 with only 12GB of vRAM. For continual pretraining of the Phi-3 model, how should the <code>torch_dtype</code> be set when loading the model and for trainable weights to minimize vRAM usage? For instance, should the model be loaded with <code>torch_dtype=fp32</code> and the precision for trainable weights set to <code>TrainingArguments(fp16=True, bf16=False)</code>, or should the model be loaded with <code>load_in_8bit</code> and the precision for trainable weights set to <code>TrainingArguments(fp16=True, bf16=False)</code>? I would like to know effective and feasible combinations.</p>
<h1>MWE</h1>
<h2>train_deepspeed.py</h2>
<pre class=""lang-py prettyprint-override""><code>import argparse
import os
import warnings
from typing import Dict, List
import deepspeed
import torch
from datasets import load_dataset
from omegaconf import OmegaConf
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    PreTrainedTokenizer,
    Trainer,
    TrainingArguments,
)
import gc
from utils import seed_everything

warnings.filterwarnings(&quot;ignore&quot;)

os.environ[&quot;TOKENIZERS_PARALLELISM&quot;] = &quot;false&quot;

def preprocess_function(
    examples: Dict[str, List[str]],
    tokenizer: PreTrainedTokenizer,
    max_length: int,
) -&gt; Dict[str, List[int]]:
    inputs = tokenizer(
        examples[&quot;text&quot;],
        truncation=True,
        padding=&quot;max_length&quot;,
        max_length=max_length,
    )
    inputs[&quot;labels&quot;] = inputs.input_ids.copy()
    return inputs


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        &quot;--train_config&quot;,
        &quot;-p&quot;,
        type=str,
        default=&quot;./configs/train_configs/train_base.yaml&quot;,
    )
    parser.add_argument(
        &quot;--local_rank&quot;,
        &quot;-l&quot;,
        type=int,
        default=0,
    )
    args = parser.parse_args()

    config = OmegaConf.load(args.train_config)

    # distributed learning
    deepspeed.init_distributed()

    # set seed
    seed_everything(config.seed)

    # load model
    model = AutoModelForCausalLM.from_pretrained(
        config.model.model,
        torch_dtype=torch.float16,
        use_cache=config.model.use_cache,
        device_map={&quot;&quot;: 0},
        attn_implementation=&quot;flash_attention_2&quot;,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        config.model.tokenizer,
        add_eos_token=True,
    )

    # load dataset
    dataset = load_dataset(
        path=config.dataset.path,
        name=config.dataset.subset,
        split=config.dataset.split,
        cache_dir=config.dataset.cache_dir,
    )

    # transform dataset
    dataset = dataset.map(
        lambda examples: preprocess_function(
            examples, tokenizer, config.model.max_length
        ),
        batched=True,
        remove_columns=dataset.column_names,
        num_proc=32,
    )

    dataset = dataset.train_test_split(test_size=0.2)

    # initiate training
    training_args = TrainingArguments(**config.train)
    trainer = Trainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset[&quot;train&quot;],
        eval_dataset=dataset[&quot;test&quot;],
        args=training_args,
        # data_collator=data_collator,
    )

    with torch.autocast(&quot;cuda&quot;):
        trainer.train()

    del dataset
    del trainer

    gc.collect()
    deepspeed.runtime.utils.empty_cache()
    torch.cuda.empty_cache()


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<h2>train_base.yaml</h2>
<pre class=""lang-yaml prettyprint-override""><code>model:
  model: microsoft/Phi-3-mini-128k-instruct
  tokenizer: microsoft/Phi-3-mini-128k-instruct
  use_cache: False
  max_length: 512


train:
  output_dir: ./outputs
  evaluation_strategy: steps
  logging_strategy: steps
  save_strategy: steps
  learning_rate: 1e-6
  num_train_epochs: 3
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 256 # per_device_train_bath_size*gradient_accumulation_steps=256
  gradient_checkpointing: True
  weight_decay: 0.01
  warmup_ratio: 0.1
  optim: adamw_bnb_8bit # adamw_torch
  fp16: False
  bf16: True
  dataloader_num_workers: 1
  eval_steps: 50
  save_steps: 100
  logging_steps: 5
  run_name: test
  save_total_limit: 2
  save_on_each_node: False
  neftune_noise_alpha: 5 # NEFTTune
  # deepspeed: ./configs/deepspeed/ds_config_zero2.json
  report_to: wandb
  torch_compile: True
  logging_dir: ./outputs/log
  
seed: 42

dataset:
  path: hotchpotch/wikipedia-ja-20231030
  subset: chunked #!!null
  split: train
  cache_dir: /mnt/d/huggingface/datasets
</code></pre>
<h2>pyproject.toml</h2>
<pre class=""lang-ini prettyprint-override""><code>[tool.poetry]
name = &quot;continual-pretrain&quot;
version = &quot;0.1.0&quot;
description = &quot;&quot;
authors = [&quot;Carlos Luis Rivera&quot;]
license = &quot;MIT&quot;
readme = &quot;README.md&quot;

[tool.poetry.dependencies]
python = &quot;^3.11&quot;
fsspec = &quot;2024.3.1&quot;
datasets = &quot;^2.19.2&quot;
accelerate = &quot;^0.31.0&quot;
aiohttp = &quot;^3.9.5&quot;
aiosignal = &quot;^1.3.1&quot;
annotated-types = &quot;^0.7.0&quot;
appdirs = &quot;^1.4.4&quot;
async-timeout = &quot;^4.0.3&quot;
attrs = &quot;^23.2.0&quot;
bitsandbytes = &quot;^0.43.1&quot;
certifi = &quot;^2024.6.2&quot;
charset-normalizer = &quot;^3.3.2&quot;
click = &quot;^8.1.7&quot;
deepspeed = &quot;^0.14.2&quot;
dill = &quot;^0.3.8&quot;
docker-pycreds = &quot;^0.4.0&quot;
docstring-parser = &quot;^0.16&quot;
filelock = &quot;^3.14.0&quot;
frozenlist = &quot;^1.4.1&quot;
gitdb = &quot;^4.0.11&quot;
gitpython = &quot;^3.1.43&quot;
hjson = &quot;^3.1.0&quot;
huggingface-hub = &quot;^0.23.3&quot;
idna = &quot;^3.7&quot;
jinja2 = &quot;^3.1.4&quot;
markdown-it-py = &quot;^3.0.0&quot;
markupsafe = &quot;^2.1.5&quot;
mdurl = &quot;^0.1.2&quot;
mpmath = &quot;^1.3.0&quot;
multidict = &quot;^6.0.5&quot;
multiprocess = &quot;^0.70.16&quot;
networkx = &quot;^3.3&quot;
ninja = &quot;^1.11.1.1&quot;
numpy = &quot;^1.26.4&quot;
nvidia-ml-py = &quot;^12.555.43&quot;
packaging = &quot;^24.0&quot;
pandas = &quot;^2.2.2&quot;
peft = &quot;0.6.0&quot;
protobuf = &quot;&lt;5.0.0&quot;
psutil = &quot;^5.9.8&quot;
py-cpuinfo = &quot;^9.0.0&quot;
pyarrow = &quot;^16.1.0&quot;
pyarrow-hotfix = &quot;^0.6&quot;
pydantic = &quot;^2.7.3&quot;
pydantic-core = &quot;^2.18.4&quot;
pygments = &quot;^2.18.0&quot;
pynvml = &quot;^11.5.0&quot;
python-dateutil = &quot;^2.9.0.post0&quot;
pytz = &quot;^2024.1&quot;
pyyaml = &quot;^6.0.1&quot;
regex = &quot;^2024.5.15&quot;
requests = &quot;^2.32.3&quot;
rich = &quot;^13.7.1&quot;
safetensors = &quot;^0.4.3&quot;
scipy = &quot;^1.13.1&quot;
sentencepiece = &quot;^0.2.0&quot;
sentry-sdk = &quot;^2.5.1&quot;
setproctitle = &quot;^1.3.3&quot;
shtab = &quot;^1.7.1&quot;
six = &quot;^1.16.0&quot;
smmap = &quot;^5.0.1&quot;
sympy = &quot;^1.12.1&quot;
tokenizers = &quot;^0.19.1&quot;
tqdm = &quot;^4.66.4&quot;
transformers = &quot;^4.41.2&quot;
trl = &quot;^0.9.4&quot;
typing-extensions = &quot;^4.12.2&quot;
tyro = &quot;^0.8.4&quot;
tzdata = &quot;^2024.1&quot;
urllib3 = &quot;^2.2.1&quot;
wandb = &quot;^0.17.1&quot;
xxhash = &quot;^3.4.1&quot;
yarl = &quot;^1.9.4&quot;
omegaconf = &quot;^2.3.0&quot;
llama-cpp-python = { version = &quot;^0.2.77&quot;, source = &quot;llama_cpp_python_cu121&quot; }
torch = { version = &quot;^2.3.1+cu121&quot;, source = &quot;torch_cu121&quot; }
nvidia-cublas-cu12 = { version = &quot;^12.1.3.1&quot;, source = &quot;torch_cu121&quot; }
nvidia-cuda-cupti-cu12 = { version = &quot;^12.1.105&quot;, source = &quot;torch_cu121&quot; }
nvidia-cuda-nvrtc-cu12 = { version = &quot;^12.1.105&quot;, source = &quot;torch_cu121&quot; }
nvidia-cuda-runtime-cu12 = { version = &quot;^12.1.105&quot;, source = &quot;torch_cu121&quot; }
nvidia-cudnn-cu12 = { version = &quot;^8.9.2.26&quot;, source = &quot;torch_cu121&quot; }
nvidia-cufft-cu12 = { version = &quot;^11.0.2.54&quot;, source = &quot;torch_cu121&quot; }
nvidia-curand-cu12 = { version = &quot;^10.3.2.106&quot;, source = &quot;torch_cu121&quot; }
nvidia-cusolver-cu12 = { version = &quot;^11.4.5.107&quot;, source = &quot;torch_cu121&quot; }
nvidia-cusparse-cu12 = { version = &quot;^12.1.0.106&quot;, source = &quot;torch_cu121&quot; }
nvidia-nccl-cu12 = { version = &quot;^2.20.5&quot;, source = &quot;torch_cu121&quot; }
nvidia-nvtx-cu12 = { version = &quot;^12.1.105&quot;, source = &quot;torch_cu121&quot; }
optimum = &quot;^1.20.0&quot;
tensorboard = &quot;^2.17.0&quot;
wheel = &quot;^0.43.0&quot;
pytorch-triton = { version = &quot;^2.3.0&quot;, source = &quot;torch_cu121&quot; }

[tool.poetry.group.dev.dependencies]
black = &quot;^24.4.2&quot;
flake8 = &quot;^7.0.0&quot;
ipykernel = &quot;^6.29.4&quot;
ipywidgets = &quot;^8.1.3&quot;
seedir = &quot;^0.4.2&quot;
emoji = &quot;^2.12.1&quot;
nbformat = &quot;^5.10.4&quot;
nbclient = &quot;^0.10.0&quot;
nbconvert = &quot;^7.16.4&quot;


[[tool.poetry.source]]
name = &quot;torch_cu121&quot;
url = &quot;https://download.pytorch.org/whl/cu121&quot;
priority = &quot;explicit&quot;


[[tool.poetry.source]]
name = &quot;llama_cpp_python_cu121&quot;
url = &quot;https://abetlen.github.io/llama-cpp-python/whl/cu121&quot;
priority = &quot;explicit&quot;


[[tool.poetry.source]]
name = &quot;torch_nightly_cu121&quot;
url = &quot;https://download.pytorch.org/whl/nightly/cu121/&quot;
priority = &quot;explicit&quot;

[build-system]
requires = [&quot;poetry-core&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;
</code></pre>
",Training and Model Evaluation,necessary torch dtype loading model precision trainable weight different according comment huggingface peft package model loaded fp trainable weight must cast fp comment understand generally used loading model precision used training must different necessary change precision also doe principle apply fine tuning continual pretraining minimal working example attempting perform continual pretraining microsoft phi mini k instruct whose default bfloat loading model training commenced precision trainable weight wa set e different precision model loading trainable weight however precision trainable weight wa set e precision model loading trainable weight error occurred preventing start training execution environment wa nvidia rtx gb vram continual pretraining phi model set loading model trainable weight minimize vram usage instance model loaded precision trainable weight set model loaded precision trainable weight set would like know effective feasible combination mwe train deepspeed py train base yaml pyproject toml
Closest match for job titles,"<p>I have master list of known job titles and looking for ways to extract the same from the searched term. For example:</p>

<p>Searched job title: <strong>Senior Digital Marketing Specialist</strong><br>
Extracted to: <strong>Senior Digital Marketing</strong></p>

<p>Searched job title: <strong>Retail In-Store Sales Assistant; Full Time</strong><br>
Extracted to: <strong>Retail Sales Assistant</strong></p>

<p>So I tried to extract parameters that would be helpful for cleaning up the searched query. <br><br>
1) The occurrence of the 2 tokens in the db. (To get mathematical evaluation of how much are the terms related with each other)
Example: </p>

<pre><code> t01-&gt;t0 or t1        Senior || java---&gt;226374 
 t02-&gt;t0 or t2        Senior || software---&gt;2566450 
 t03-&gt;t0 or t3        Senior || engineer---&gt;7220787 
 t12-&gt;t1 or t2        java || software---&gt;315397
 t13-&gt;t1 or t3        java || engineer---&gt;407682
 t23-&gt;t2 or t3        software || engineer---&gt;11533495

 total =t01+t02+t03+t12+t13+t23
</code></pre>

<p><img src=""https://i.sstatic.net/o39hO.png"" alt=""enter image description here""></p>

<p>2) The occurrence of the token taken 1 at time in the entire db.
Example: </p>

<pre><code>t0-&gt;    Senior-----&gt;55042636  
t1-&gt;    java-----&gt;1655805
t2-&gt;    software-----&gt;26136204
t3-&gt;    engineer-----&gt;81574912
</code></pre>

<p>3) I took the sum of the related tokens and put a minimum threshold of 5% and that give me the following output, i.e (txy*100)/total > 5</p>

<p>My Output : <strong>Senior software engineer</strong><br>
Anyone have any experience with similar projects or ideas for further improvement ?</p>
",Training and Model Evaluation,closest match job title master list known job title looking way extract searched term example searched job title senior digital marketing specialist extracted senior digital marketing searched job title retail store sale assistant full time extracted retail sale assistant tried extract parameter would helpful cleaning searched query occurrence token db get mathematical evaluation much term related example occurrence token taken time entire db example took sum related token put minimum threshold give following output e txy total output senior software engineer anyone experience similar project idea improvement
Is there any way to flexibly change between VLM and LLM?,"<p>I want to train a VLM from LLM but when I inference, does the input have to contain image embedding and if I train with PEFT, will the other tasks of LLM be weakened?</p>
<p>My goal is that I want to keep the strengths of other tasks on LLM while it learns more about vision and when inferring the user can flexibly change between adding and not adding to the image. So is there a way?</p>
<p>Can I build something similar to BLIP2 but with an LLM of my own choosing?</p>
<p>I tried building a QFomer based layer with 2 images but the loss hardly decreases after each training, and I don't know where I'm going wrong.</p>
<p>This is my model:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoImageProcessor, AutoModel
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel, PretrainedConfig


class SelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super(SelfAttention, self).__init__()
        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads).to(torch.bfloat16)
    
    def forward(self, x):
        x = x.to(torch.bfloat16)
        attn_output, _ = self.self_attn(x, x, x)
        return attn_output

class CrossAttention(nn.Module):
    def __init__(self, query_dim, key_dim, hidden_dim, num_heads):
        super(CrossAttention, self).__init__()
        self.query_proj = nn.Linear(query_dim, hidden_dim).to(torch.bfloat16)
        self.key_proj = nn.Linear(key_dim, hidden_dim).to(torch.bfloat16)
        self.value_proj = nn.Linear(key_dim, hidden_dim).to(torch.bfloat16)
        self.multihead_attn = nn.MultiheadAttention(hidden_dim, num_heads).to(torch.bfloat16)
        self.output_proj = nn.Linear(hidden_dim, 1024).to(torch.bfloat16)
        
    def forward(self, query, key):
        query = query.to(torch.bfloat16)
        key = key.to(torch.bfloat16)
        value = self.value_proj(key)
        query = self.query_proj(query)
        key = self.key_proj(key)
        attn_output, _ = self.multihead_attn(query, key, value)
        output = self.output_proj(attn_output)
        return output
    
class QFormerLayer(nn.Module):
    def __init__(self, img_dim, hidden_dim, num_heads, feedforward_dim, text_dim):
        super(QFormerLayer, self).__init__()
        self.self_attn1 = SelfAttention(img_dim, num_heads)
        self.self_attn2 = SelfAttention(img_dim, num_heads)
        self.cross_attn_imgs = CrossAttention(img_dim, img_dim, hidden_dim, num_heads)
        self.cross_attn_text = CrossAttention(text_dim, img_dim, hidden_dim, num_heads)
        self.feedforward = nn.Sequential(
            nn.Linear(img_dim, feedforward_dim).to(torch.bfloat16),
            nn.ReLU(),
            nn.Linear(feedforward_dim, img_dim).to(torch.bfloat16)
        )
        
    def forward(self, text, img1, img2):
        img1_attn = self.self_attn1(img1)
        img2_attn = self.self_attn2(img2)
        
        img_combined = self.cross_attn_imgs(img1_attn, img2_attn)
        cross_attn_output = self.cross_attn_text(text, img_combined)
        output = self.feedforward(cross_attn_output)
        return output, img1, img2

class QFormer(nn.Module):
    def __init__(self, img_dim, hidden_dim, num_heads, feedforward_dim, text_dim, num_layers):
        super(QFormer, self).__init__()
        self.layers = nn.ModuleList([QFormerLayer(img_dim, hidden_dim, num_heads, feedforward_dim, text_dim) for _ in range(num_layers)])

    def forward(self, text, img1, img2):
        for layer in self.layers:
            output, img1, img2 = layer(text, img1, img2)
        return output

class PreTrainedModelForQFormer(PreTrainedModel):
    config_class = PretrainedConfig

    def __init__(self, config):
        super().__init__(config)
        self.text_dim = config.text_dim
        self.img_dim = config.img_dim
        self.hidden_dim = config.hidden_dim
        self.num_heads = config.num_heads
        self.feedforward_dim = config.feedforward_dim
        self.num_layers = config.num_layers
        self.qformer = QFormer(self.img_dim, self.hidden_dim, self.num_heads, self.feedforward_dim, self.text_dim, self.num_layers)
        self.output_proj = nn.Linear(self.img_dim, 2048).to(torch.bfloat16)
        self.vs = AutoModel.from_pretrained(config.vision_model_name, trust_remote_code=True, torch_dtype=torch.bfloat16).to('cuda')
        self.vs1 = AutoModel.from_pretrained(config.vision_model_name, trust_remote_code=True, torch_dtype=torch.bfloat16).to('cuda')
        self.lm = AutoModelForCausalLM.from_pretrained(config.lm_model_name).to(torch.bfloat16).to('cuda')
        self.tokenizer = AutoTokenizer.from_pretrained(config.lm_model_name)
        for param in self.lm.parameters():
            param.requires_grad = False
        for param in self.vs.parameters():
            param.requires_grad = False
        for param in self.vs1.parameters():
            param.requires_grad = True

    def forward(self, text, img1, img2, label):
        img1 = img1.to(torch.bfloat16)
        img2 = img2.to(torch.bfloat16)
        text, labels, attn_mask = collate_fn(self.tokenizer, text, label, device=&quot;cuda&quot;)
        img1 = self.vs(img1).pooler_output
        img2 = self.vs1(img2).pooler_output
        qformer_output = self.qformer(text, img1, img2)
        qformer_output = self.output_proj(qformer_output)
        text_embeds = self.lm.get_input_embeddings()(text)
        inputs_embeds = torch.cat([qformer_output.unsqueeze(1), text_embeds], dim=1)
        labels = torch.cat([torch.full((labels.size(0), 1), -100, dtype=labels.dtype, device=labels.device), labels], dim=1)
        attn_mask = torch.cat([torch.ones((attn_mask.size(0), 1), dtype=attn_mask.dtype, device=attn_mask.device), attn_mask], dim=1)
        return self.lm(inputs_embeds=inputs_embeds, labels=labels, attention_mask=attn_mask)
    
    def save_pretrained(self, save_directory):
        torch.save(self.qformer.state_dict(), f&quot;{save_directory}/qformer_weights.pth&quot;)
        torch.save(self.output_proj.state_dict(), f&quot;{save_directory}/output_proj_weights.pth&quot;)

    @classmethod
    def from_pretrained(cls, config, load_directory):
        model = cls(config)
        model.qformer.load_state_dict(torch.load(f&quot;{load_directory}/qformer_weights.pth&quot;, map_location=torch.device('cuda')))
        model.output_proj.load_state_dict(torch.load(f&quot;{load_directory}/output_proj_weights.pth&quot;, map_location=torch.device('cuda')))
        return model

class QFormerConfig(PretrainedConfig):
    def __init__(self, text_dim=512, img_dim=2048, hidden_dim=512, num_heads=8, feedforward_dim=1024, num_layers=6, lm_model_name=&quot;google/gemma-1.1-2b-it&quot;, vs_model_name='OpenGVLab/InternViT-300M-448px', **kwargs):
        super().__init__(**kwargs)
        self.text_dim = text_dim
        self.img_dim = img_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.feedforward_dim = feedforward_dim
        self.num_layers = num_layers
        self.lm_model_name = lm_model_name
        self.vision_model_name = vs_model_name

text_dim = 512
img_dim = 1024
hidden_dim = 1024
num_heads = 8
feedforward_dim = 2048
num_layers = 1
lm_model_name = &quot;google/gemma-1.1-2b-it&quot;
</code></pre>
",Training and Model Evaluation,way flexibly change vlm llm want train vlm llm inference doe input contain image embedding train peft task llm weakened goal want keep strength task llm learns vision inferring user flexibly change adding adding image way build something similar blip llm choosing tried building qfomer based layer image loss hardly decrease training know going wrong model
Issue with PyTorch&#39;s transformer Model repeating last token during inference,"<p>I’ve been trying to implement PyTorch’s nn.TransformerEncoder and nn.TransformerDecoder solutions into a simple model, but I’m running into an issue that I’m unable to resolve where during inference the model only produces the last token fed into it.</p>
<p>For example lets say I have a tensor [1,2,3,4,5] the model will continue the sequence with [1,2,3,4,5,5,5,5,5,5,…] or if I had [5,2,8,3] it would continue to produce [5,2,8,3,3,3,3,3,3,3,…] even when using training data as input although when using a new randomly initialized model it will produce diverse output although since not trained is useless.</p>
<p>Although it produces the above results, the loss continues to decrease as I train it further indicating that its managing to learn the dataset. Due to this I initially thought this was just a problem with the dataset where the target was the same as the input which would cause it to produce the same tokens, but after further testing I’m sure that the targets are definitely the next token in the sequence, for example the input would be [1,2,3,4] and the target would be [2,3,4,5].</p>
<p>This lead me to my current standing theory that there is something wrong with the seq2seq implementation but after much research and trying different implementations of the common components such as positional encoding, adjusting hyper-parameters and removing / adding masks to the encoder and decoder, but regardless still weeks later and I’m still zero progress towards identifying the issue.</p>
<p>For reference here is the model and training step I’m using:</p>
<pre><code>class TextEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embed_dim: int, padding_index: int):
        super(TextEmbedding, self).__init__()
        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim, padding_idx=padding_index)

    def forward(self, x):
        return self.embedding(x)

class TextTransformer(nn.Module):
    def __init__(self, vocab_size, embed_dim = 512, nhead = 8, num_encoder_layers = 6, num_decoder_layers = 6, max_length = 5000, padding_index = 0):
        super(TextTransformer, self).__init__()
        self.vocab_size = vocab_size
        self.max_length = max_length

        self.text_embedding = TextEmbedding(vocab_size, embed_dim, padding_index)
        self.positional_encoding = nn.Parameter(torch.zeros(1, max_length, embed_dim))

        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead, dim_feedforward=2048)
        self.encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=num_encoder_layers)

        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=nhead, dim_feedforward=2048)
        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=num_decoder_layers)

        self.fc = nn.Sequential(
            nn.Linear(embed_dim, vocab_size)
        )

    def forward(self, src, tgt, src_mask, tgt_mask):
        #Embedding + Positional Encoding
        src_embedding = self.text_embedding(src) + self.positional_encoding[:, :src.size(1), :]
        tgt_embedding = self.text_embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]

        tgt_square_mask = create_square_mask(tgt.size(1)).to(src.device)

        #Encoder
        memory = self.encoder(src_embedding.permute(1, 0, 2), src_key_padding_mask=src_mask)

        #Decoder
        decoder_out = self.decoder(tgt_embedding.permute(1, 0, 2), memory, tgt_mask=tgt_square_mask, tgt_key_padding_mask=tgt_mask)
        decoder_out = decoder_out.permute(1, 0, 2)

        #FC output
        output = self.fc(decoder_out)

        return output

    def seq2seq(self, src, src_mask, stop_token, max_length = 500):
        src_embedding = self.text_embedding(src) + self.positional_encoding[:, :src.size(1), :]

        memory = self.encoder(src_embedding.permute(1, 0, 2), src_key_padding_mask=src_mask)
        sequence = src
        stop = False

        while sequence.shape[1] &lt; min(self.max_length, max_length) and not stop:
            tgt_embedding = self.text_embedding(sequence) + self.positional_encoding[:, :sequence.size(1), :]

            tgt_square_mask = create_square_mask(sequence.size(1)).to(src.device)
            dec_output = self.decoder(tgt_embedding.permute(1, 0, 2), memory, tgt_mask=tgt_square_mask)
            dec_output = dec_output.permute(1, 0, 2)

            out = self.fc(dec_output)[:, -1, :]
            predicted = out.argmax(dim=1)
            
            if predicted.item() == stop_token:
                stop = True

            sequence = torch.cat((sequence, predicted.unsqueeze(dim=0)),dim=1)

        return sequence

    def create_square_mask(size):
        mask = torch.triu(torch.ones(size, size), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))
        return mask
</code></pre>
<pre><code>def train_step(model, dataloader, criterion, optimizer, device):
    avg_loss = 0
    model.train()
    for batch, (text_data, text_pad_mask) in enumerate(dataloader):
        text_data, text_pad_mask = text_data.to(device), text_pad_mask.to(device)

        #shift data so that the in_text is the initial tokens and that tgt_text is the next predicted token in the sequence
        in_text = text_data[:, :-1]
        in_mask = text_pad_mask[:, :-1]
        tgt_text = text_data[:, 1:]
        tgt_mask = text_pad_mask[:, 1:]


        out = model(in_text, tgt_text, in_mask, tgt_mask)

        outputs = out[:, :].reshape(-1, model.vocab_size)# Reshape to [batch_size * steps, vocab_size]
        targets = tgt_text[:, :].reshape(-1)# Reshape to [batch_size * steps]

        loss = criterion(outputs, targets)
        avg_loss += loss.item()

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    return avg_loss / len(dataloader)
</code></pre>
<p>The loss function is CrossEntropyLoss, the optimizer is AdamW and the dataloader returns tokenized texts in the shape of (batch, sequence). I think this is all that is necessary to try diagnose the issue as I’m 100% sure the tokenizer and data loader is working perfectly as I’ve done a lot of testing on them and don’t want to flood this post with too much code but I can provide the code for them upon request if it helps at all.</p>
",Training and Model Evaluation,issue pytorch transformer model repeating last token inference trying implement pytorch nn transformerencoder nn transformerdecoder solution simple model running issue unable resolve inference model produce last token fed example let say tensor model continue sequence would continue produce even using training data input although using new randomly initialized model produce diverse output although since trained useless although produce result loss continues decrease train indicating managing learn dataset due initially thought wa problem dataset target wa input would cause produce token testing sure target definitely next token sequence example input would target would lead current standing theory something wrong seq seq implementation much research trying different implementation common component positional encoding adjusting hyper parameter removing adding mask encoder decoder regardless still week later still zero progress towards identifying issue reference model training step using loss function crossentropyloss optimizer adamw dataloader return tokenized text shape batch sequence think necessary try diagnose issue sure tokenizer data loader working perfectly done lot testing want flood post much code provide code upon request help
Text summarization with deep learning,"<p>I'm finetuning the Mt5  model on the Arabic part of the Xl-sum data set
For ten epochs and the resulted manipulation model was stored in hugging face library, there were good results on the training and the validation loss as well as the Rouge measure was around 60, but when doing the testing  process, I surprised that the  results is a mixture of Arabic symbols and garbage, and it is not understandable.</p>
<p>i want to know , what is the error</p>
",Training and Model Evaluation,text summarization deep learning finetuning mt model arabic part xl sum data set ten epoch resulted manipulation model wa stored hugging face library good result training validation loss well rouge measure wa around testing process surprised result mixture arabic symbol garbage understandable want know error
Guidance on Extracting Compliance Items from PDF documents by fine-tuning a LLM,"<p>Need some guidance on extracting large compliance items from raw PDF documents. I have csv with these compliance items and I want to fine-tune a LLM such that if it reads any new PDF documents it can firstly identify the compliance items and extract them.</p>
<p>I have seen LLMs used for NER but this kinds of falls into Named Phrase Recognition(NPR - don't know if an acronym like this exists).</p>
<p>The PDFs are servicing guides.
Examples of them can be seen from the following site: <a href=""https://servicing-guide.fanniemae.com/"" rel=""nofollow noreferrer"">https://servicing-guide.fanniemae.com/</a></p>
<p>These are documents with lots of pages. They have compliance items inside them. I have a training document with these compliance documents.</p>
<p><strong>Compliance Items</strong>:</p>
<p>Compliance items are varied in nature. They can range from</p>
<ol>
<li>&quot;The servicer must document in the mortgage loan servicing file the date that the COVID - related hardship began and the date of the insured loss event.&quot;</li>
<li>An incentive fee payment for an eligible short sale is disbursed as outlined in the following table  2. Fannie Mae reviews eligibility for the short sale incentive fee and makes the determination based on information provided by the servicer through Fannie Maeâ€™s servicing solutions system.</li>
</ol>
<p>So, regex or anything is not suitable since there are various types of these items.</p>
<p>My goal is to train a LLM model on these compliance documents so that if I provide a new PDF it can predict/extract compliance items from it.</p>
<p>Can anyone guide me in which model will be better suited for this task, what tokenizers etc.</p>
",Training and Model Evaluation,guidance extracting compliance item pdf document fine tuning llm need guidance extracting large compliance item raw pdf document csv compliance item want fine tune llm read new pdf document firstly identify compliance item extract seen llm used ner kind fall named phrase recognition npr know acronym like exists pdfs servicing guide example seen following site document lot page compliance item inside training document compliance document compliance item compliance item varied nature range servicer must document mortgage loan servicing file date covid related hardship began date insured loss event incentive fee payment short sale disbursed outlined following table fannie mae review eligibility short sale incentive fee make determination based information provided servicer fannie mae servicing solution system regex anything suitable since various type item goal train llm model compliance document provide new pdf predict extract compliance item anyone guide model better suited task tokenizers etc
LDA is predicting same topics for all data,"<p>I'm using the German political <a href=""https://berd-platform.de/records/g3225-rba63"" rel=""nofollow noreferrer"">speech dataset</a> to train the LDA model. My goal here is to categorize each speech into some topics. But the problem is that the generated topics are too similar, and all speech prediction is predicting the same topic.</p>
<p>Tried to play with parameters. I don't see any changes.</p>
<p>preproessing : stopword used from <code>nltk</code>+ some generated by GPT + some common word of topic generated by lda (try and error)</p>
<pre><code>def basic_preprocess_text(text):
    # Lowercase the text
    text = text.lower()

    # Replace umlauts and ß
    text = text.replace('ä', 'ae').replace('ö', 'oe').replace('ü', 'ue').replace('ß', 'ss')

    return text

stopwords = set(basic_preprocess_text(word) for word in stopwords)

def regex_filter(text) :
      # Remove everything before &quot;:&quot; if the pattern matches
    text = re.sub(r'.*\[.*\]:\s*', '', text)

    # Replacing poilitical term F.D.P as FDP
    text = re.sub(r'\b([A-Z]{1})\.?([A-Z]{1})\.?([A-Z]{1})\b', r'\1\2\3', text)

    # replace occurrences of &quot;Nordrhein Westfalen&quot; with &quot;NRW&quot;
    # text = re.sub(r'nordrhein[- .]?westfalen', 'NRW', text, flags=re.IGNORECASE)
    text = re.sub(r'nordrhein[- .]?westfalen', '', text, flags=re.IGNORECASE)

    #removing special charater
    text = re.sub(r'[^a-zA-Zäöüß]', ' ', text)

    return text

# Preprocessing function
def preprocess_text(text):

    text = regex_filter(text)

    # Tokenize the text
    tokens = word_tokenize(text, language='german')

    # Lemmatize the text &amp; Basic preprocessing
    doc = nlp(' '.join(tokens))
    lemmatized_token = [basic_preprocess_text(token.lemma_) for token in doc if len(token.lemma_) &gt; 2]

    # Remove stopwords
    lemmatized_text = ' '.join([word for word in lemmatized_token if word not in stopwords])

    return lemmatized_text
    #return tokens
</code></pre>
<p>Ngram :</p>
<pre><code># Ensure 'Preprocessed_Speech' is treated as a list of tokens
df['Preprocessed_tokens'] = df['Preprocessed_Speech'].apply(lambda x: x.split())


# Create bigrams and trigrams
bigram = Phrases(df['Preprocessed_tokens'], min_count=3, threshold=5)
trigram = Phrases(bigram[df['Preprocessed_tokens']], min_count=3, threshold=5)

bigram_mod = Phraser(bigram)
trigram_mod = Phraser(trigram)

# Form Bigrams and Trigrams
texts = [trigram_mod[bigram_mod[text]] for text in df['Preprocessed_tokens']]

</code></pre>
<p>TF-IDF : i have tried to weight some word manually but opted for now</p>
<pre><code>
# Create TF-IDF matrix
texts_joined = [' '.join(text) for text in texts]
tfidf_vectorizer = TfidfVectorizer(max_df=0.50, min_df=5, ngram_range=(1, 2))
tfidf = tfidf_vectorizer.fit_transform(texts_joined)

# Convert TF-IDF matrix to a Gensim corpus
corpus = Sparse2Corpus(tfidf, documents_columns=False)

# Create the dictionary
id2word = {v: k for k, v in tfidf_vectorizer.vocabulary_.items()}

</code></pre>
<p>LDA train :</p>
<pre><code>
# Apply LDA with adjusted parameters
num_topics = 50  # Adjust number of topics for faster experimentation
passes = 20  # Increase number of passes
iterations = 1000  # Adjust number of iterations per pass
alpha = 'auto'  # Let gensim determine the optimal alpha
eta = 'auto'  # Let gensim determine the optimal eta

lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=id2word, passes=passes, iterations=iterations, alpha=alpha, eta=eta)
</code></pre>
<p>is there something I'm doing wrong? Or Is the dataset is not suitable for this model?</p>
",Training and Model Evaluation,lda predicting topic data using german political speech dataset train lda model goal categorize speech topic problem generated topic similar speech prediction predicting topic tried play parameter see change preproessing stopword used generated gpt common word topic generated lda try error ngram tf idf tried weight word manually opted lda train something wrong dataset suitable model
How do I evaluate a text summarization tool?,"<p>I have written a system that summarizes a long document containing thousands of words. Are there any norms on how such a system should be evaluated in the context of a user survey?</p>

<p>In short, is there a metric for evaluating the time that my tool has saved a human? Currently, I was thinking of using the (Time taken to read the original document/Time taken to read the summary) as a way of determining the time saved, but are there better metrics?</p>

<p>Currently, I am asking the user subjective questions about the accuracy of the summary.</p>
",Training and Model Evaluation,evaluate text summarization tool written system summarizes long document containing thousand word norm system evaluated context user survey short metric evaluating time tool ha saved human currently wa thinking using time taken read original document time taken read summary way determining time saved better metric currently asking user subjective question accuracy summary
Multi-classification of Text using NLP python - Recall is relatively very less for 2 categories out of total categories,"<p>I am having almost balanced dataset of 9 unique categories, each having almost 2200 rows with difference of +/-100 rows. To create model , i have used below mentioned urls approach but in each case my model accuracy is coming around 58% and precision/recall is also around 54%. Can you please let me know what wrong am I doing?</p>

<p><a href=""https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f"" rel=""nofollow noreferrer"">https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f</a>
<a href=""https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a"" rel=""nofollow noreferrer"">https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a</a></p>

<p><a href=""https://medium.com/@robert.salgado/multiclass-text-classification-from-start-to-finish-f616a8642538"" rel=""nofollow noreferrer"">https://medium.com/@robert.salgado/multiclass-text-classification-from-start-to-finish-f616a8642538</a></p>

<p>My dataset is having only 2 columns , 1 as feature and other as label.</p>

<pre><code>from pandas import ExcelFile

df = pd.read_excel('Prediction.xlsx', 
                   sheet_name='Sheet1')
df.head()
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))
import sys
!{sys.executable} -m pip install lxml

def clean_text(text):
    """"""
        text: a string

        return: modified initial string
    """"""
    text = BeautifulSoup(text, ""html.parser"").text # HTML decoding
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text
    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text
    return text

df['notes_issuedesc'] = df['notes_issuedesc'].apply(clean_text)
print_plot(10)
df['notes_issuedesc'].apply(lambda x: len(x.split(' '))).sum()
X = df.notes_issuedesc
y = df.final
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 42)
%%time
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer

nb = Pipeline([('vect', CountVectorizer()),
               ('tfidf', TfidfTransformer()),
               ('clf', MultinomialNB()),
              ])
nb.fit(X_train, y_train)

from sklearn.metrics import classification_report
y_pred = nb.predict(X_test)

print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred,target_names=my_tags))
</code></pre>
",Training and Model Evaluation,multi classification text using nlp python recall relatively le category total category almost balanced dataset unique category almost row difference row create model used mentioned url approach case model accuracy coming around precision recall also around please let know wrong dataset column feature label
The Impact of Pretraining on Fine-tuning and Inference,"<p>I am working on a binary prediction classification task, primarily focusing on fine-tuning a BERT model to learn the association between CVEs and CWEs. I've structured my task into three phases: first, using a large dataset of CVE and CWE for MLM pretraining; then using the pretrained model weights for fine-tuning; and finally, using the fine-tuned model with task-specific weights for inference. For inference, I randomly select 100 pairs of CVEs for the model to predict, only retaining the model weights and setting the model to `model.eval()`. However, I've encountered an issue where, after transferring the pretrained model to fine-tuning, the evaluation metrics significantly improve (accuracy, precision, recall, f1), and both training and validation loss decrease substantially. Yet, when I use the fine-tuned model weights for inference, the accuracy turns out to be worse than before.</p>
<p>What is the reason for this discrepancy?</p>
<p>Here are the hyperparameters used for pretraining and fine-tuning:</p>
<p>Pretrain Model:</p>
<pre><code>batch_size = 16

num_epochs = 10

learning_rate = 5e-4

eps = 1e-8

beta1 = 0.9

beta2 = 0.99

weight_decay = 0.01

total_steps = num_epochs \len(train_loader)

warmup_steps = total_steps // 10

early_stopping_patience = 2
</code></pre>
<pre><code>mask_prob = 0.15

replace_mask_prob = 0.8

random_replace_prob = 0.10

keep_original_prob = 0.10
</code></pre>
<p>Train Model:</p>
<pre><code>learning_rate = 2e-5

batch_size = 16

epoch = 5
</code></pre>
<p>I have experimented with various combinations of hyperparameters and found that the current settings are optimal for minimizing validation loss and maximizing accuracy.</p>
",Training and Model Evaluation,impact pretraining fine tuning inference working binary prediction classification task primarily focusing fine tuning bert model learn association cf cwes structured task three phase first using large dataset cve cwe mlm pretraining using pretrained model weight fine tuning finally using fine tuned model task specific weight inference inference randomly select pair cf model predict retaining model weight setting model however encountered issue transferring pretrained model fine tuning evaluation metric significantly improve accuracy precision recall f training validation loss decrease substantially yet use fine tuned model weight inference accuracy turn worse reason discrepancy hyperparameters used pretraining fine tuning pretrain model train model experimented various combination hyperparameters found current setting optimal minimizing validation loss maximizing accuracy
Feature Importance Chart in Keras API,"<p>I am implementing a series of three deep learning models (RNN, 1D-CNN, and custom transformer), all utilizing the Keras API, for an NLP binary classification problem. I would like to generate a feature importance chart, similar to the one shown in the question linked here: <a href=""https://stackoverflow.com/questions/45361559/feature-importance-chart-in-neural-network-using-keras-in-python"">Feature Importance Chart in neural network using Keras in Python</a></p>
<p>I have already run the models and have results, but I would like to show the feature importance chart as well. Here is the model architecture for the 1D-CNN model, which I will use as the example in this question:</p>
<pre class=""lang-py prettyprint-override""><code>from keras import layers
from keras.layers import Input, Embedding, Conv1D, Dropout, GlobalMaxPooling1D, Dense
from keras.optimizers import Adam
from keras.regularizers import L2

def create_model():
    embedding_dim = 500

    input = Input(shape = (maxlen,))
    x = Embedding(input_dim = vocab_size, 
                               output_dim = embedding_dim, input_shape = (1000,))(input)
    x = Conv1D(256, 7, padding = 'valid', activation = 'gelu', strides = 3, kernel_regularizer = L2(0.01))(x)
    x = Dropout(0.5)(x)
    x = Conv1D(256, 7, padding = 'valid', activation = 'gelu', strides = 3, kernel_regularizer = L2(0.01))(x)
    x = Dropout(0.5)(x)
    x = Conv1D(256, 7, padding = 'valid', activation = 'gelu', strides = 3, kernel_regularizer = L2(0.01))(x)
    x = GlobalMaxPooling1D()(x)
    x = Dense(256, activation = 'gelu')(x)
    x = Dropout(0.2)(x)
    x = Dense(128, activation = 'gelu')(x)
    x = Dropout(0.2)(x)
    x = Dense(64, activation = 'gelu')(x)
    x = Dropout(0.2)(x)
    x = Dense(32, activation = 'gelu')(x)
    x = Dropout(0.2)(x)
    
    class_1 = Dense(1, activation = 'sigmoid', name = 'class_1')(x)
    class_2 = Dense(1, activation = 'sigmoid', name = 'class_2')(x)
    class_3 = Dense(1, activation = 'sigmoid', name = 'class_3')(x)
    class_4 = Dense(1, activation = 'sigmoid', name = 'class_4')(x)
    class_5 = Dense(1, activation = 'sigmoid', name = 'class_5')(x)
    class_6 = Dense(1, activation = 'sigmoid', name = 'class_6')(x)
    
    opt = Adam(learning_rate = 0.001)
    
    model = keras.Model(
        inputs = [input],
        outputs = [class_1, class_2, class_3, class_4, class_5, class_6]
    )

    model.compile(optimizer = opt,
              loss = {'class_1' : 'binary_crossentropy', 'class_2' :  'binary_crossentropy', 'class_3' : 'binary_crossentropy', 'class_4' : 'binary_crossentropy', 'class_5' : 'binary_crossentropy', 'class_6' : 'binary_crossentropy'},
              metrics = ['accuracy', 'accuracy', 'accuracy', 'accuracy', 'accuracy', 'accuracy']
             )
    
    return model
</code></pre>
<p>Other Stack Overflow questions regarding this issue worked due to using a Sequential model, but I would like to continue using the Functional API. These questions are linked here:</p>
<p><a href=""https://stackoverflow.com/questions/77258642/feature-importance-keras-regressionmodel"">Feature Importance keras regressionmodel</a></p>
<p><a href=""https://stackoverflow.com/questions/45361559/feature-importance-chart-in-neural-network-using-keras-in-python"">Feature Importance Chart in neural network using Keras in Python</a></p>
<p><a href=""https://stackoverflow.com/questions/44119207/is-there-any-way-to-get-variable-importance-with-keras?noredirect=1&amp;lq=1"">Is there any way to get variable importance with Keras?</a></p>
<p><a href=""https://stackoverflow.com/questions/47713596/feature-importance-with-keras?noredirect=1&amp;lq=1"">Feature importance with keras</a></p>
<p>I have attempted to use eli5, but this requires a Keras wrapper for a sequential model. SHAP also requires me to re-train the model, which is not desired.</p>
",Training and Model Evaluation,feature importance chart kera api implementing series three deep learning model rnn cnn custom transformer utilizing kera api nlp binary classification problem would like generate feature importance chart similar one shown question linked stack overflow question regarding issue worked due using sequential model would like continue using functional api question linked attempted use eli requires kera wrapper sequential model shap also requires train model desired
Continual pre-training vs. Fine-tuning a language model with MLM,"<p>I have some custom data I want to use to <em><strong>further pre-train</strong></em> the BERT model. I’ve tried the two following approaches so far:</p>
<ol>
<li>Starting with a pre-trained BERT checkpoint and continuing the pre-training with Masked Language Modeling (<code>MLM</code>) + Next Sentence Prediction (<code>NSP</code>) heads (e.g. using <em><strong>BertForPreTraining</strong></em> model)</li>
<li>Starting with a pre-trained BERT model with the <code>MLM</code> objective (e.g. using the <em><strong>BertForMaskedLM</strong></em> model assuming we don’t need NSP for the pretraining part.)</li>
</ol>
<p>But I’m still confused that if using either <em>BertForPreTraining</em> or <em>BertForMaskedLM</em> actually does the continual pre-training on BERT or these are just two models for fine-tuning that use MLM+NSP and MLM for fine-tuning BERT, respectively. Is there even any difference between fine-tuning BERT with MLM+NSP or continually pre-train it using these two heads or this is something we need to test?</p>
<p>I've reviewed similar questions such as <a href=""https://stackoverflow.com/questions/65646925/how-to-train-bert-from-scratch-on-a-new-domain-for-both-mlm-and-nsp"">this one</a> but still, I want to make sure that whether technically there's a difference between continual pre-training a model from an initial checkpoint and fine-tuning it using the same objective/head.</p>
",Training and Model Evaluation,continual pre training v fine tuning language model mlm custom data want use pre train bert model tried two following approach far starting pre trained bert checkpoint continuing pre training masked language modeling next sentence prediction head e g using bertforpretraining model starting pre trained bert model objective e g using bertformaskedlm model assuming need nsp pretraining part still confused using either bertforpretraining bertformaskedlm actually doe continual pre training bert two model fine tuning use mlm nsp mlm fine tuning bert respectively even difference fine tuning bert mlm nsp continually pre train using two head something need test reviewed similar question href one still want make sure whether technically difference continual pre training model initial checkpoint fine tuning using objective head
Why charater based LSTM are taking more time than word based LSTM while next word prediction,"<p>I wanted to train LSTM Model for Next Word Prediction using word-based and character based. I used similar data processing technique for both character-based and word-based.</p>
<p>For Character,</p>
<pre><code>class TextDataset(Dataset):
    def __init__(self, text, sequence_length):
        self.text = text
        self.sequence_length = sequence_length
        self.text_length = len(text) - sequence_length

    def __len__(self):
        return self.text_length

    def __getitem__(self, idx):
        seq = self.text[idx: idx + self.sequence_length]
        next_char = self.text[idx + self.sequence_length]
        return torch.tensor(seq['tokens'], dtype=torch.long), torch.tensor(next_char['tokens'], dtype=torch.long)

sequence_length = 20
train_data = TextDataset(tokenized_dataset['train'], sequence_length)
valid_data = TextDataset(tokenized_dataset['valid'], sequence_length)
test_data = TextDataset(tokenized_dataset['test'], sequence_length)
train_dataloader = DataLoader(train_data, batch_size=1024, shuffle=False)
valid_dataloader = DataLoader(valid_data, batch_size=1024, shuffle=False)
test_dataloader = DataLoader(test_data, batch_size=1024, shuffle=False)
</code></pre>
<p>For Word-based, I took word instead of Character.</p>
<p>Here is LSTM model,</p>
<pre><code>class LSTMModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate):
        super(LSTMModel, self).__init__()
        self.num_layers = n_layers
        self.hidden_dim = hidden_dim
        self.embedding_dim = embedding_dim
        self.dropout = nn.Dropout(dropout_rate)
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=dropout_rate, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x, hidden):
        x = self.embedding(x)
        # x = x.view(self.sequence_length, -1, self.hidden_dim)
        # print(x.shape)
        out, hidden = self.lstm(x, hidden)
        out = self.dropout(out)
        out = self.fc(out[:,-1])
        return out, hidden

    # def init_hidden(self, batch_size):
    #     return (torch.zeros(1, batch_size, self.hidden_dim),
    #             torch.zeros(1, batch_size, self.hidden_dim))

    def init_hidden(self, batch_size, device):
        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)
        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)
        return hidden, cell

    def detach_hidden(self, hidden):
        hidden, cell = hidden
        hidden = hidden.detach()
        cell = cell.detach()
        return hidden, cell
</code></pre>
<p>And Training Step,</p>
<pre><code>for epoch in range(n_epochs):

    for inputs, targets in tqdm(train_dataloader):
        hidden = model.init_hidden(inputs.shape[0], device)
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        output, hidden = model(inputs, hidden)
        loss = criterion(output, targets)
        loss.backward()
        optimizer.step()
        model.detach_hidden(hidden)

    print(f&quot;Epoch {epoch+1}/{n_epochs}, Loss: {loss.item()}&quot;)
</code></pre>
<p><strong>Here, Character-based training takes much longer time than Word-based training. Why?</strong></p>
",Training and Model Evaluation,charater based lstm taking time word based lstm next word prediction wanted train lstm model next word prediction using word based character based used similar data processing technique character based word based character word based took word instead character lstm model training step character based training take much longer time word based training
GliNER finetuning - no validation loss is logging,"<p>I am trying to fine-tune using this notebook: <a href=""https://github.com/urchade/GLiNER/blob/main/examples/finetune.ipynb"" rel=""nofollow noreferrer"">GLiNER/examples/finetune.ipynb at main · urchade/GLiNER (github.com)</a></p>
<p>However, the logs only show 'loss' , which I assume is the training data set loss, but there is no recorded validation set loss</p>
<pre><code>training_args = TrainingArguments(
    output_dir=&quot;models&quot;,
    learning_rate=5e-6,
    weight_decay=0.01,
    others_lr=1e-5,
    others_weight_decay=0.01,
    lr_scheduler_type='linear', #&quot;cosine&quot;,
    warmup_ratio=0.1,
    per_device_train_batch_size=5,
    per_device_eval_batch_size=5,
    num_train_epochs=2,
    eval_strategy=&quot;epoch&quot;,
    save_steps = 100,
    save_total_limit=10,
    dataloader_num_workers = 1,
    use_cpu = True,
    disable_tqdm=False,
    logging_dir='logs',
    logging_steps=10,
    eval_steps=10,
    do_train=True,
    do_eval=True,
    seed=7,
    )

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=model.data_processor.transformer_tokenizer,
    data_collator=data_collator)
</code></pre>
<p>Where is validation loss computed?? I want to log both the training and validation sets' loss</p>
",Training and Model Evaluation,gliner finetuning validation loss logging trying fine tune using notebook gliner example finetune ipynb main urchade gliner github com however log show loss assume training data set loss recorded validation set loss validation loss computed want log training validation set loss
why softmax get small gradient when the value is large in paper &#39;Attention is all you need&#39;,"<p>This is the screen of the original paper: <a href=""https://i.sstatic.net/J9thT.png"" rel=""noreferrer"">the screen of the paper</a>. I understand the meaning of the paper is that when the value of dot-product is large, the gradient of softmax will get very small.
<br/>However, I tried to calculate the gradient of softmax with the cross entropy loss and found that the gradient of softmax is not directly related to value passed to softmax. <br/>Even the single value is large, it still can get a large gradient when ather values are large. (sorry about that I don't know how to pose the calculation process here)</p>
",Training and Model Evaluation,softmax get small gradient value large paper attention need screen original paper screen paper understand meaning paper value dot product large gradient softmax get small however tried calculate gradient softmax cross entropy loss found gradient softmax directly related value passed softmax even single value large still get large gradient ather value large sorry know pose calculation process
How to use Llama3?,"<p>I'm using Ollama and llama 3 to build a ChatBot. However, right now, it can't remember chat history. For example, if my first query is &quot;tell me about the theory of relativity,&quot; and if my next query is &quot;can you simplify it,&quot; it throws a message saying it can't recall earlier conversations.</p>
<p>Here is my current code snippet:</p>
<pre><code>import ollama
import streamlit as st

# function to get the llama's response
def get_llama_response(prompt):
    response = ollama.chat(
        model='llama3',
        messages= [
            {
                'role':'user',
                'content': prompt
            }
        ]
    )
    return response['message']['content']

# setting the title
st.title(&quot;Llama Knows All&quot;)

# loading message and we take prompt
prompt = st.chat_input(&quot;How can I help you today?&quot;)

# setting up a session to hold all messages during current interaction
if 'messages' not in st.session_state:
    st.session_state.messages = []

# displaying older messages
for message in st.session_state.messages:
    st.chat_message(message['role']).markdown(message['content'])

if prompt:
    # display the user message
    st.chat_message('user').markdown(prompt)
    st.session_state.messages.append({'role':'user', 'content': prompt})

    llama_response = get_llama_response(prompt)

    st.chat_message('llama').markdown(llama_response)
    st.session_state.messages.append({'role': 'llama', 'content': llama_response})
</code></pre>
",Training and Model Evaluation,use llama using ollama llama build chatbot however right remember chat history example first query tell theory relativity next query simplify throw message saying recall earlier conversation current code snippet
what is the difference between bigram and unigram text features extraction,"<p>I searched online to do bi-gram and unigram text features' extraction, but still didn't find something useful information, can someone tell me what is the difference between them?</p>

<p>For example, if I have a text ""I have a lovely dog""
what will happen if I use bi-gram way to do features extraction and to do unigram extraction?</p>
",Training and Model Evaluation,difference bigram unigram text feature extraction searched online bi gram unigram text feature extraction still find something useful information someone tell difference example text lovely dog happen use bi gram way feature extraction unigram extraction
Accuracy_score with same value in different classifiers methods,"<p>I'm doing a project, on Google Colab, for fake news classification with LIAR dataset. I am running with three differents features extractors (TF-IDF, DistilBERT and LLAMA 2) and seven classifiers (Logistic Regression, SVM, Naive Bayes, KNN, Random Forest, AdaBoost and XGBoost). However, many tests gave the same result, even though they used different algorithms. Below, the values ​​found for the average accuracy entered are listed with the standard deviation in parentheses. The cells in orange are those that showed the same result (64.15 (2.22 x 10^-16)).
<a href=""https://i.sstatic.net/jO4hk0Fd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jO4hk0Fd.png"" alt=""enter image description here"" /></a></p>
<p>This should not be happening, considering that, in addition to being different methods, I am running each experiment 30 times, varying the training and testing basis in each run. Below, an excerpt of the code, with Naive Bayes:</p>
<pre><code>model_nb = GaussianNB()
grid_nb = GridSearchCV(model_nb, parameters_nb)
for i in range(30):
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, stratify = Y)
    fit_model_nb = grid_nb.fit(X_train, y_train)
    pred_nb = fit_model_nb.predict(X_test)
    predictions_df = pd.DataFrame({'Predicted_Label': pred_nb, &quot;True&quot;: y_test})

    # Nomeie o arquivo com um identificador único (por exemplo, o número da iteração)
    filename = f'nb_predictions_{i}.csv'

    # Salve o DataFrame em um arquivo CSV com o nome único
    predictions_df.to_csv(filename, index=False)
</code></pre>
<p>I carried out other tests with other bases, such as FakeNewsNet and KaggleFN, and this problem did not occur there. One consideration to make is that I needed to adjust the LIAR labels to be a base of binary classes, so the six classes became just two (true or false), as follows:
'half-true'-&gt; 'false', 'barely-true'-&gt; 'false', 'pants-fire'-&gt; 'false', 'mostly-true'-&gt; 'true', 'true'-&gt; 'true', 'false'-&gt; 'false'
But I believe this should not impact the result.</p>
",Training and Model Evaluation,accuracy score value different classifier method project google colab fake news classification liar dataset running three differents feature extractor tf idf distilbert llama seven classifier logistic regression svm naive bayes knn random forest adaboost xgboost however many test gave result even though used different algorithm value found average accuracy entered listed standard deviation parenthesis cell orange showed result x happening considering addition different method running experiment time varying training testing basis run excerpt code naive bayes carried test base fakenewsnet kagglefn problem occur one consideration make needed adjust liar label base binary class six class became two true false follows half true false barely true false pant fire false mostly true true true true false false believe impact result
How to handle the loss function decrease at some points but bounce back to keep increasing when fine-tune LLaMA?,"<p><strong>Preface</strong></p>
<p>I am new to fine-tuning an LLM model on binary classification tasks. I have tried fine-tuning LLaMA 2 and 3 with the causal inference task (next token prediction) with PEFT LoRA and 4-bit quantization. The task that I have been trained on is using various prompts.</p>
<ol>
<li><code>${Text}. ${Question}. ${Answer (Positive class or negative class)}</code></li>
<li><code>${Some context}. ${Question}. ${Answer (Positive class or negative class)}</code></li>
</ol>
<p>The <a href=""https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments"" rel=""nofollow noreferrer"">training arguments</a> are here:</p>
<pre><code>per_device_train_batch_size = 8
gradient_accumulation_steps = 4
optim = &quot;paged_adamw_32bit&quot;
save_steps = 100
logging_steps = 10
learning_rate = 1e-4
max_grad_norm = 0.3 # I have tried to use default max_grad_norm
max_steps = 1000
warmup_ratio = 0.03 # I have tried to use default warmup_ratio
lr_scheduler_type = &quot;cosine_with_restarts&quot; # I have tried it using &quot;cosine&quot; too
</code></pre>
<p>Here is the <a href=""https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer"" rel=""nofollow noreferrer"">trainer</a>:</p>
<pre><code>trainer = SFTTrainer(
    model=model,
    args=training_arguments,
    train_dataset=dataset,
    packing=True,
    dataset_text_field=&quot;id&quot;,
    tokenizer=tokenizer,
    max_seq_length=1024,
    formatting_func=formatting_func,
)
</code></pre>
<p><strong>Issues and Questions</strong></p>
<p>The issue comes within the results. The loss function decreases after some steps, but after particular steps, the loss function keeps increasing (I supposed that LLaMA used cross entropy as the loss function). Is the issue within the LLM that I used (LLaMA) or what may cause the problem? and how to solve it.</p>
<p><a href=""https://i.sstatic.net/yrpuTu10m.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yrpuTu10m.png"" alt=""enter image description here"" /></a></p>
",Training and Model Evaluation,handle loss function decrease point bounce back keep increasing fine tune llama preface new fine tuning llm model binary classification task tried fine tuning llama causal inference task next token prediction peft lora bit quantization task trained using various prompt training argument trainer issue question issue come within result loss function decrease step particular step loss function keep increasing supposed llama used cross entropy loss function issue within llm used llama may cause problem solve
Understanding the results of Transformers Learn In Context with Gradient Descent,"<p>I'm trying to implement this paper:
<a href=""https://arxiv.org/pdf/2212.07677"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2212.07677</a></p>
<p>(Here's their code):
<a href=""https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd"" rel=""nofollow noreferrer"">https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd</a></p>
<p>I'm struggling to match their experimental results. Specifically, on their simplest GD model (a single layer with a single head and no softmax), they obtain a constant low loss of roughly 0.20 on their test data. I don't quite understand why this is the case, conceptually.</p>
<p>As I understand it, this model only does a single iteration of gradient descent on the data, so why would it reach such a low loss? And why would the loss be constant/near constant over training steps? Aren't we training the learning rate in the GD model?</p>
",Training and Model Evaluation,understanding result transformer learn context gradient descent trying implement paper code struggling match experimental result specifically simplest gd model single layer single head softmax obtain constant low loss roughly test data quite understand case conceptually understand model doe single iteration gradient descent data would reach low loss would loss constant near constant training step training learning rate gd model
Training Fastconformer-CTC on Kazakh Language,"<p>community. I just started working with NeMo. So, I want to train ASR based on Fastconformer-CTC with 100 hours of recorded calls in kazakh language. What type of training will be better for my situation, fine-tuning some pre-trained model or train it from scratch? Also can someone give info about the data processing and manifest?</p>
<p>Suggestions about Fastconformer-CTC</p>
",Training and Model Evaluation,training fastconformer ctc kazakh language community started working nemo want train asr based fastconformer ctc hour recorded call kazakh language type training better situation fine tuning pre trained model train scratch also someone give info data processing manifest suggestion fastconformer ctc
Text data labeling,"<p><strong>Task:</strong> Classify customer emails into relevant categories based on their content.</p>
<p><strong>Data:</strong> DataFrame containing customer emails.
I have a dataset of customer emails stored in a data frame. Each email pertains to a specific issue the customer encountered. My goal is to categorize these emails automatically. For example, an email about problems with food quality would be categorized as &quot;Food Quality Issue,&quot; while an email regarding payment difficulties would be categorized as &quot;Payment Issue.&quot;</p>
<p>The challenge lies in the unknown number of potential categories. There could be a vast range of issues customers might contact us about.</p>
<p>My plan is to address this challenge in two steps:</p>
<ol>
<li>Data Labeling: I need to  label a representative sample of emails from the data frame. This labeling process involves assigning each email a category that accurately reflects the customer's concern.</li>
<li>Classifier Model Training: Once I have a labeled dataset, I can use it to train a machine learning model. This model will then be able to automatically categorize new, unseen emails based on the patterns it learns from the labeled data.
I need answer for first part , how can i label data.</li>
</ol>
<p>My approach was to generate embeddings of email and then apply clustering,but result is not good.
Please help me to solve this problem and is any thing to apply before generating embeddings.</p>
",Training and Model Evaluation,text data labeling task classify customer email relevant category based content data dataframe containing customer email dataset customer email stored data frame email pertains specific issue customer encountered goal categorize email automatically example email problem food quality would categorized food quality issue email regarding payment difficulty would categorized payment issue challenge lie unknown number potential category could vast range issue customer might contact u plan address challenge two step data labeling need label representative sample email data frame labeling process involves assigning email category accurately reflects customer concern classifier model training labeled dataset use train machine learning model model able automatically categorize new unseen email based pattern learns labeled data need answer first part label data approach wa generate embeddings email apply clustering result good please help solve problem thing apply generating embeddings
Vosk Model for Speech to Text,"<p>I want to train vosk model vosk-model-en-us-0.22 from <a href=""https://alphacephei.com/vosk/models"" rel=""nofollow noreferrer"">https://alphacephei.com/vosk/models</a> with an addition data of my voice with transcript of 1 hour so that the model gets overfit with my voice and don't get problem recognizing my voice properly. So how can I train it if someone can give me roadmap how to train that as I am beginner in open source.</p>
<ol>
<li>How to record my voice in which type i.e. mp3, wav etc.</li>
<li>How to feed recording dataset and transcript to model.</li>
<li>What code I use for training it and testing it.</li>
<li>How to make it better.
And any other related query</li>
</ol>
<p>I tried reading <a href=""https://alphacephei.com/vosk/lm"" rel=""nofollow noreferrer"">https://alphacephei.com/vosk/lm</a> but could not help myself with it.</p>
",Training and Model Evaluation,vosk model speech text want train vosk model vosk model en u addition data voice transcript hour model get overfit voice get problem recognizing voice properly train someone give roadmap train beginner open source record voice type e mp wav etc feed recording dataset transcript model code use training testing make better related query tried reading could help
Out-of-memory problem when using dist.all_gather,"<p>I'm writing codes for multi-GPU training, and I need to gather embeddings from different gpus to calculate loss and then propagate the gradients back to different GPUs. However, when the programs runs to optimizer.step(), the memory usage increases dramatically and resulted in a out-of-memory problem. The code is as below, thanks!</p>
<pre><code>
def training_stage_2(model, optimizer, train_dataloader, val_dataloader, tokenizer, accelerator, epochs):
    # finetuning the model with query-doc pairs
    for epoch in range(epochs):
        model.train()
        for batch in tqdm(train_dataloader):
            query, doc = batch
            with accelerator.accumulate(model):
                # take the last hidden states of eos token as embeddings]
                query_inputs = tokenizer(query, return_tensors=&quot;pt&quot;, padding=True, truncation=True, max_length=512)
                query_embeds_ = model(**query_inputs).last_hidden_state[:,-1,:] # shape: (batch_size, hidden_size)
                doc_inputs = tokenizer(doc, return_tensors=&quot;pt&quot;, padding=True, truncation=True, max_length=512)
                doc_embeds_ = model(**doc_inputs).last_hidden_state[:,-1,:] # shape: (batch_size, hidden_size)
                # collect embeddings from all gpus
                query_embeds = torch.zeros((query_embeds_.shape[0] * accelerator.num_processes, query_embeds_.shape[1]), device=accelerator.device, dtype=query_embeds_.dtype)
                doc_embeds = torch.zeros((doc_embeds_.shape[0] * accelerator.num_processes, doc_embeds_.shape[1]), device=accelerator.device, dtype=doc_embeds_.dtype)
                dist.all_gather(list(query_embeds.chunk(accelerator.num_processes, dim=0)), query_embeds_.data)
                dist.all_gather(list(doc_embeds.chunk(accelerator.num_processes, dim=0)), doc_embeds_.data)
                # requires grad for embeddings
                query_embeds.requires_grad = True
                doc_embeds.requires_grad = True
                loss = classification_loss_single_vector(query_embeds, doc_embeds)
                accelerator.backward(loss)
                # scatter the gradients to all gpus
                if query_embeds.grad is not None:
                    query_embeds.grad.detach_()
                    doc_embeds.grad.detach_()
                query_grad = torch.zeros_like(query_embeds_)
                doc_grad = torch.zeros_like(doc_embeds_)
                # feature gradient all-reduce
                dist.reduce_scatter(query_grad, list(query_embeds.grad.chunk(accelerator.num_processes, dim=0)))
                dist.reduce_scatter(doc_grad, list(doc_embeds.grad.chunk(accelerator.num_processes, dim=0)))
                query_grad.mul_(accelerator.num_processes)
                doc_grad.mul_(accelerator.num_processes)
                # backward the model
                query_embeds_.backward(query_grad)
                doc_embeds_.backward(doc_grad)
                optimizer.step()
                optimizer.zero_grad()
</code></pre>
<p>I tried using accelerator.gather to replace dist.gather_all but it doesn't work.</p>
",Training and Model Evaluation,memory problem using dist gather writing code multi gpu training need gather embeddings different gpus calculate loss propagate gradient back different gpus however program run optimizer step memory usage increase dramatically resulted memory problem code thanks tried using accelerator gather replace dist gather work
Enhance model performance in text classification task,"<p>I tried to build a model for multi-label text classification task in chinese, but the performance of the model is not good enough (about 60% accuracy), and I come for help about how to enhance it.</p>
<p>I build a model based on a github project:</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F
from transformers import BertModel


class BertMultiLabelCls(nn.Module):
    def __init__(self, hidden_size, class_num, dropout=0.1):
        super(BertMultiLabelCls, self).__init__()
        self.fc = nn.Linear(hidden_size, class_num)
        self.drop = nn.Dropout(dropout)
        self.bert = BertModel.from_pretrained(&quot;bert-base-chinese&quot;)

    def forward(self, input_ids, attention_mask, token_type_ids):
        outputs = self.bert(input_ids, attention_mask, token_type_ids)
        cls = self.drop(outputs[1])
        out = F.sigmoid(self.fc(cls))
        return out
</code></pre>
<p>My dataset is 2000 query-tag pair, having 13 tags and query about the questions asked by the audience in the live commerce. I splited the dataset by 3:1:1 corresponding to train/test/val. My tags are NOT balanced and no up sample/down sample strategy is used.</p>
<p>Loss and accuracy during the training process, where the horizontal axis stands for the epoch:
<img src=""https://i.sstatic.net/VFqUaQth.png"" alt=""loss&amp;acc during the training process, where the horizontal axis stands for the epoch"" /></p>
<p>The vaildation accuracy stopped increasing near 60%, and same result for my test dataset.
I've tried various methods including adding more fully connection layer/adding residual connection, but the result remains the same.</p>
<p>Here are my training params if it helps:</p>
<pre><code>lr = 2e-5
batch_size = 128
max_len = 64
hidden_size = 768
epochs = 30
optimizer = AdamW(model.parameters(), lr=lr)
criterion = nn.BCELoss() # loss function
</code></pre>
<p>Any suggestions about how I improve my model besides the datasets? because that what I am doing paralleling and I know how to improve it. But I'm really newbie about the network itself.</p>
",Training and Model Evaluation,enhance model performance text classification task tried build model multi label text classification task chinese performance model good enough accuracy come help enhance build model based github project dataset query tag pair tag query question asked audience live commerce splited dataset corresponding train test val tag balanced sample sample strategy used loss accuracy training process horizontal axis stand epoch vaildation accuracy stopped increasing near result test dataset tried various method including adding fully connection layer adding residual connection result remains training params help suggestion improve model besides datasets paralleling know improve really newbie network
Audio Pronunciation Model Training using CNN Architecture,"<p>Hello currently im working on this web system prototype in which the system is able to predict the accuracy of user's pronunciation for certain words.</p>
<p>The model trained with the code below has the Final Model Accuracy of 0.9919999837875366 and Accuracy: 0.992, Precision: 0.9960762331838565 , F1 Score: 0.9932923420905534. Im using the CNN model architecture for this model training. Also, the dataset has a total of 500 audio recording for 10 different words.</p>
<p>Im using this trained model for my web server using Flask framework in which it will predict user's pronunciation accuracy of given word which are: (One Syllable) &quot;Sup&quot;, &quot;Bas&quot;, &quot;Jam&quot;, &quot;Wang&quot;, &quot;Sos&quot; and (Two Syllable) &quot;Roti&quot;, &quot;Ayam&quot;, &quot;Pintu&quot;, &quot;Kipas&quot;, &quot;Sampah&quot;. User need to click on the start recording then pronounce the word and click the stop recording button afterwards. The system will then predict the accuracy of the pronounced word by giving percentage value of the accuracy to the user as the feedback.</p>
<p>The problem is now, i dont know why when i use the trained model in the web, it seems like the system could not accurately predict the correctness of user's pronunciation. For example like if the given word is &quot;Sup&quot; and i pronounced as &quot;Batu&quot;, it should predict it as incorrect and give a low percentage of accuracy but instead, the system gives me a high percentage as the feedback (like 97.5% or 100%). Even if i tried to pronounce it in different ways (incorrect ways to pronounce the word) it will still gives me a range of high value percentages.</p>
<p>Below i provide the model training and the server code.</p>
<ol>
<li>Model Training (Model_Training.py)</li>
</ol>
<pre><code>import os
import numpy as np
import librosa
import seaborn as sns
import matplotlib.pyplot as plt
import joblib
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam

# Add noise with different types
def add_noise(audio_data, noise_level=0.005):
    noise = np.random.normal(0, 1, len(audio_data))
    scaled_noise = noise_level * noise
    return audio_data + scaled_noise

# Add reverberation
def add_reverb(audio_data, sr, intensity=0.5):
    return librosa.effects.preemphasis(audio_data, coef=intensity)

# Function to preprocess audio data
def preprocess_audio(file_path):
    audio_data, sr = librosa.load(file_path, sr=None)
    audio_data = librosa.effects.preemphasis(audio_data)
    audio_data = librosa.util.normalize(audio_data)
    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=20)
    return mfccs.T  # Transpose for Conv1D layer

# Function to augment audio data
def augment_audio(audio_data, sr):
    augmented_audios = []
    # Existing augmentations
    augmented_audios.append(audio_data + 0.005 * np.random.randn(len(audio_data)))  # Noise
    augmented_audios.append(np.roll(audio_data, int(np.random.uniform(-0.1, 0.1) * sr)))  # Time shifting
    augmented_audios.append(librosa.effects.pitch_shift(audio_data, sr=sr, n_steps=np.random.randint(-2, 2)))  # Pitch shifting
    augmented_audios.append(audio_data)  # Original audio
    
    # Advanced augmentations
    augmented_audios.append(add_noise(audio_data))
    augmented_audios.append(add_reverb(audio_data, sr))

    return augmented_audios

# Function to load and preprocess a dataset of audio files
def load_and_preprocess_dataset(dataset_folder):
    X, y = [], []
    max_length = 222
    for root, dirs, files in os.walk(dataset_folder):
        for directory in dirs:
            word_folder = os.path.join(root, directory)
            for participant_folder in os.listdir(word_folder):
                participant_folder_path = os.path.join(word_folder, participant_folder)
                if os.path.isdir(participant_folder_path):
                    for filename in os.listdir(participant_folder_path):
                        if filename.endswith(&quot;.wav&quot;):
                            file_path = os.path.join(participant_folder_path, filename)
                            audio_data, sr = librosa.load(file_path, sr=None)
                            augmented_audios = augment_audio(audio_data, sr)
                            for audio in augmented_audios:
                                mfccs_features = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20).T
                                max_length = max(max_length, mfccs_features.shape[0])
                                X.append(mfccs_features)
                                label = 1 if &quot;Correct&quot; in filename else 0
                                y.append(label)
    
    # Pad the sequences to have the same length
    X_padded = []
    for mfccs_features in X:
        if mfccs_features.shape[0] &lt; max_length:
            pad_width = max_length - mfccs_features.shape[0]
            mfccs_features = np.pad(mfccs_features, ((0, pad_width), (0, 0)), mode='constant')
        else:
            mfccs_features = mfccs_features[:max_length, :]
        X_padded.append(mfccs_features)
    
    X_reshaped = np.array(X_padded).reshape(-1, max_length * 20)  # matches your max_length * n_mfcc
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_reshaped)
    X_scaled = X_scaled.reshape(len(X_padded), max_length, 20)

    return X_scaled, np.array(y)

# Path to the dataset folder
dataset_folder = r'C:\Users\ACER\Desktop\Word_Dataset'

# Load and preprocess the dataset
X, y = load_and_preprocess_dataset(dataset_folder)
# Standardize the features
scaler = StandardScaler()
X_reshaped = X.reshape(-1, X.shape[1] * X.shape[2])
X_scaled = scaler.fit_transform(X_reshaped)
X_scaled = X_scaled.reshape(X.shape[0], X.shape[1], X.shape[2])

# Define a simple CNN model
def create_model(learning_rate=0.0005):
    model = Sequential()
    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_scaled.shape[1], X_scaled.shape[2])))
    model.add(BatchNormalization())
    model.add(MaxPooling1D(pool_size=2))
    model.add(Dropout(0.4))
    model.add(Conv1D(128, kernel_size=3, activation='relu'))
    model.add(BatchNormalization())
    model.add(MaxPooling1D(pool_size=2))
    model.add(Dropout(0.4))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))
    
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Cross-validation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
all_accuracies = []
for train_index, test_index in skf.split(X_scaled, y):
    X_train, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train, y_test = y[train_index], y[test_index]
    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
    class_weight_dict = dict(enumerate(class_weights))
    model = create_model(learning_rate=0.0005)
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)
    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr], class_weight=class_weight_dict)
    loss, accuracy = model.evaluate(X_test, y_test)
    all_accuracies.append(accuracy)
    print(f&quot;Fold Accuracy: {accuracy}&quot;)

# Final evaluation on the entire dataset
loss, accuracy = model.evaluate(X_scaled, y)
print(&quot;Final Model Accuracy:&quot;, accuracy)

# Save the trained model
MODEL_DIR = os.path.join(os.path.dirname(os.path.abspath(file)), '..', 'models')
os.makedirs(MODEL_DIR, exist_ok=True)
MODEL_PATH = os.path.join(MODEL_DIR, 'pronunciation_assessment_model_final4.h5')
model.save(MODEL_PATH)
print(f&quot;Model saved successfully at {MODEL_PATH}&quot;)

# Save the scaler
SCALER_PATH = os.path.join(MODEL_DIR, 'scaler.pkl')
joblib.dump(scaler, SCALER_PATH)
print(f&quot;Scaler saved successfully at {SCALER_PATH}&quot;)

# Predict labels for the dataset
y_pred = model.predict(X_scaled)
y_pred_classes = (y_pred &gt; 0.5).astype(&quot;int32&quot;)

# Calculate evaluation metrics
accuracy = accuracy_score(y, y_pred_classes)
precision = precision_score(y, y_pred_classes)
recall = recall_score(y, y_pred_classes)
f1 = f1_score(y, y_pred_classes)

print(f&quot;Accuracy: {accuracy}&quot;)
print(f&quot;Precision: {precision}&quot;)
print(f&quot;Recall: {recall}&quot;)
print(f&quot;F1 Score: {f1}&quot;)

# Generate confusion matrix
cm = confusion_matrix(y, y_pred_classes)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt=&quot;d&quot;, cmap=&quot;Blues&quot;, xticklabels=[&quot;Incorrect&quot;, &quot;Correct&quot;], yticklabels=[&quot;Incorrect&quot;, &quot;Correct&quot;])
plt.xlabel(&quot;Predicted labels&quot;)
plt.ylabel(&quot;True labels&quot;)
plt.title(&quot;Confusion Matrix&quot;)
plt.show()

# Print classification report
print(classification_report(y, y_pred_classes, target_names=[&quot;Incorrect&quot;, &quot;Correct&quot;]))
</code></pre>
<ol start=""2"">
<li>Server (Pronounce_System.py)</li>
</ol>
<pre><code>from flask import Flask, render_template, request, jsonify,session, redirect, url_for
from datetime import timedelta
import os
import librosa
import numpy as np
from tensorflow.keras.models import load_model
from sklearn.preprocessing import StandardScaler
import joblib  # Import joblib for saving and loading objects
import random
  
app = Flask(name, static_folder='../static', static_url_path='/static')
app.secret_key = 'f3cfe9ed8fae309f02079dbf'  # Set a secret key for session management
  
# Set session to expire after 30 minutes of inactivity
app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(minutes=30)
  
CURRENT_DIR = os.path.dirname(os.path.abspath(file))
TEMPLATES_DIR = os.path.join(CURRENT_DIR, '..', 'templates')
app.template_folder = TEMPLATES_DIR
  
MODEL_PATH = os.path.join(CURRENT_DIR, '..', 'models', 'pronunciation_assessment_model_final4.h5')
pronunciation_model = load_model(MODEL_PATH)
  
SCALER_PATH = os.path.join(CURRENT_DIR, '..', 'models', 'scaler.pkl')
scaler = joblib.load(SCALER_PATH)
  
# Function to preprocess user audio input
def preprocess_user_input(audio_data):
    audio_data = audio_data.astype(np.float32) / np.iinfo(np.int16).max
    audio_data = librosa.effects.preemphasis(audio_data)
    audio_data = librosa.util.normalize(audio_data)
    mfccs = librosa.feature.mfcc(y=audio_data, sr=22050, n_mfcc=20).T

    max_length = 222  # Adjust this to match training exactly
    print(&quot;MFCCs shape before padding/truncating:&quot;, mfccs.shape)  # Debugging

    if mfccs.shape[0] &lt; max_length:
        pad_width = max_length - mfccs.shape[0]
        mfccs = np.pad(mfccs, ((0, pad_width), (0, 0)), mode='constant')
    else:
        mfccs = mfccs[:max_length, :]

    print(&quot;MFCCs shape after padding/truncating:&quot;, mfccs.shape)  # Debugging

    # Flatten and reshape as done in training
    mfccs_flattened = mfccs.flatten()
    print(&quot;Features flattened shape:&quot;, mfccs_flattened.shape)  # Debugging
    mfccs_scaled = scaler.transform(mfccs_flattened.reshape(1, -1))
    mfccs_scaled = mfccs_scaled.reshape(1, max_length, 20)

    return mfccs_scaled

@app.route('/')
def index():
  return render_template('index.html')
  
# Words lists for different categories
WORDS_SATU = [&quot;Sup&quot;, &quot;Bas&quot;, &quot;Jam&quot;, &quot;Wang&quot;, &quot;Sos&quot;]
WORDS_DUA = [&quot;Roti&quot;, &quot;Ayam&quot;, &quot;Pintu&quot;, &quot;Kipas&quot;, &quot;Sampah&quot;]

feedback_phrases = {
    '0-10': [
        &quot;Jangan risau, cuba lagi. Awak pasti boleh lakukannya dengan lebih baik!&quot;,
        &quot;Tak mengapa, latihan membuat awak lebih bagus. Mari kita cuba lagi!&quot;
    ],
    '10-20': [
        &quot;Bagus usaha awak! Mari cuba sekali lagi, ya?&quot;,
        &quot;Sudah hampir betul, mari kita ulang semula&quot;
    ],
    '20-30': [
        &quot;Awak boleh melakukannya! Cuba sebut sekali lagi&quot;,
        &quot;Awak sudah mula faham. Teruskan berusaha!&quot;
    ],
    '30-40': [
        &quot;Bagus! Mari kita cuba lagi&quot;,
        &quot;Semakin baik! Sedikit lagi untuk sebutan yang betul&quot;
    ],
    '40-50': [
        &quot;Hebat! Kita cuba sekali lagi untuk sebutan lebih bagus&quot;,
        &quot;Awak semakin pandai! Teruskan berlatih&quot;
    ],
    '50-60': [
        &quot;Bagus! Teruskan usaha, kita cuba lagi sekali&quot;,
        &quot;Awak hampir berjaya! Sikit lagi pasti boleh&quot;
    ],
    '60-70': [
        &quot;Hebat! Sebutan awak makin bagus&quot;,
        &quot;Tahniah! Sebutan awak sudah bagus&quot;
    ],
    '70-80': [
        &quot;Bagus sekali! Kamu semakin pandai&quot;,
        &quot;Wow! Sebutan yang bagus. Teruskan usaha!&quot;
    ],
    '80-90': [
        &quot;Hebat! Sebutan awak sangat bagus&quot;,
        &quot;Sangat bagus! Sebutan awak sudah hampir sempurna&quot;
    ],
    '90-100': [
        &quot;Cemerlang! Sebutan awak sangat bagus!&quot;,
        &quot;Luar biasa! Awak sebut dengan betul&quot;
    ]
}

# Function to get random feedback phrase based on accuracy
def get_feedback_phrase(accuracy):
    if 0 &lt;= accuracy &lt; 10:
        return random.choice(feedback_phrases['0-10'])
    elif 10 &lt;= accuracy &lt; 20:
        return random.choice(feedback_phrases['10-20'])
    elif 20 &lt;= accuracy &lt; 30:
        return random.choice(feedback_phrases['20-30'])
    elif 30 &lt;= accuracy &lt; 40:
        return random.choice(feedback_phrases['30-40'])
    elif 40 &lt;= accuracy &lt; 50:
        return random.choice(feedback_phrases['40-50'])
    elif 50 &lt;= accuracy &lt; 60:
        return random.choice(feedback_phrases['50-60'])
    elif 60 &lt;= accuracy &lt; 70:
        return random.choice(feedback_phrases['60-70'])
    elif 70 &lt;= accuracy &lt; 80:
        return random.choice(feedback_phrases['70-80'])
    elif 80 &lt;= accuracy &lt; 90:
        return random.choice(feedback_phrases['80-90'])
    elif 90 &lt;= accuracy &lt;= 100:
        return random.choice(feedback_phrases['90-100'])
    else:
        return &quot;Error: Invalid accuracy value.&quot;

# Debug audio data processing
@app.route('/predict', methods=['POST'])
def predict():
    try:
        if 'audio' not in request.files:
            return &quot;No audio file found&quot;, 400
        audio_file = request.files['audio']
        user_audio = audio_file.read()

        # Check if the buffer size is a multiple of element size (2 bytes for np.int16)
        if len(user_audio) % 2 != 0:
            user_audio = user_audio[:-1] 
            
        if len(user_audio) == 0:
            return &quot;Empty audio file&quot;, 400

        user_audio_np = np.frombuffer(user_audio, dtype=np.int16)
        preprocessed_input = preprocess_user_input(user_audio_np)
        
        prediction = pronunciation_model.predict(preprocessed_input)[0][0]
        accuracy_percentage = float(round(prediction * 100, 2))
        is_correct = prediction &gt;= 0.5

        is_correct = bool(is_correct)
        feedback_phrase = get_feedback_phrase(accuracy_percentage)
        session['feedback_phrase'] = feedback_phrase

        session['end_of_list'] = session.get('end_of_list', False)
        session['prediction'] = is_correct
        session['accuracy_percentage'] = accuracy_percentage
        session['can_proceed'] = accuracy_percentage &gt;= 60.0

        return redirect(url_for('feedback'))

except Exception as e:
        print(&quot;Error during prediction: &quot;, str(e))
        return jsonify({'error': str(e)}), 500
  
# Select syllable page
@app.route('/select_syllable')
def select_syllable():
  return render_template('select_syllable.html')
  
# Pronunciation activity page
@app.route('/pronunciation_activity')
def pronunciation_activity():
  syllable = session.get('syllable')
  current_words = session.get('current_words')
  current_word_index = session.get('current_word_index', 0)
      
  if not current_words:  
      return 'No words available for this activity'
  
  current_word = current_words[current_word_index] if len(current_words) &gt; current_word_index else 'No word found'
      
  audio_path = url_for('static', filename=f'sound/{current_word}.mp3')
  
  if syllable == 'satu':
      return render_template('pronunciation_activity_satu.html', word=current_word, audio_path=audio_path)
  elif syllable == 'dua':
      return render_template('pronunciation_activity_dua.html', word=current_word, audio_path=audio_path)
  else:
      return 'Invalid or missing suku kata parameter'
  
@app.route('/start_activity/&lt;syllable&gt;')
def start_activity(syllable):
  if syllable == 'satu':
      session['current_words'] = WORDS_SATU
  elif syllable == 'dua':
      session['current_words'] = WORDS_DUA
  else:
      return 'Invalid syllable parameter', 400
  session['current_word_index'] = 0
  session['syllable'] = syllable
  session['current_word'] = session['current_words'][0]  # Set the first word
  session.modified = True
  return redirect(url_for('pronunciation_activity'))
  
@app.route('/repeat_word')
def repeat_word():
  # Simply redirect to the pronunciation activity which uses the current session word index
  return redirect(url_for('pronunciation_activity'))
  
@app.route('/next_word')
def next_word():
  current_index = session.get('current_word_index', 0) + 1
  words = session.get('current_words', [])
      
  if current_index &lt; len(words):
      session['current_word_index'] = current_index
      session['current_word'] = words[current_index]  # Update the current word
      return redirect(url_for('pronunciation_activity'))
  else:
      # Reset when reaching the end
      session.pop('current_word_index', None)
      session.pop('current_words', None)
      session.pop('syllable', None)
      session.pop('current_word', None)
      # Redirect to the end page
      return redirect(url_for('end_page'))
  
@app.route('/feedback')
def feedback():
  print(&quot;Can Proceed:&quot;, session.get('can_proceed', False))
  print(&quot;End of List:&quot;, session.get('end_of_list', False))
  
  prediction = session.get('prediction', False)
  accuracy = session.get('accuracy_percentage', 0)
  word = session.get('current_word', 'No word found')  # Ensure this is set during pronunciation
  can_proceed = session.get('can_proceed', False)
  end_of_list = session.get('end_of_list', False)  # Ensure this is checked and set
  
  return render_template('feedback.html', 
                          prediction=prediction,
                          accuracy=accuracy,
                          word=word,
                          can_proceed=can_proceed,
                          end_of_list=end_of_list)
  
@app.route('/end_page')
def end_page():
  return render_template('end_page.html')
      
if name == 'main':
  app.run(debug=True)

</code></pre>
<p>Im trying to make the system correctly predict the accuracy of the user's pronounced word.
If the current word is &quot;Sup&quot; and user pronounce it as &quot;Jam&quot; or &quot;Sip&quot; then the accuracy should be low and vice versa.
Please help me if you have any idea what are the problem that is causing this. Thank you in advance and have a great day!</p>
",Training and Model Evaluation,audio pronunciation model training using cnn architecture hello currently im working web system prototype system able predict accuracy user pronunciation certain word model trained code ha final model accuracy accuracy precision f score im using cnn model architecture model training also dataset ha total audio recording different word im using trained model web server using flask framework predict user pronunciation accuracy given word one syllable sup ba jam wang two syllable roti ayam pintu kipas sampah user need click start recording pronounce word click stop recording button afterwards system predict accuracy pronounced word giving percentage value accuracy user feedback problem dont know use trained model web seems like system could accurately predict correctness user pronunciation example like given word sup pronounced batu predict incorrect give low percentage accuracy instead system give high percentage feedback like even tried pronounce different way incorrect way pronounce word still give range high value percentage provide model training server code model training model training py server pronounce system py im trying make system correctly predict accuracy user pronounced word current word sup user pronounce jam sip accuracy low vice versa please help idea problem causing thank advance great day
Multitask learning with multi dataset,"<p>How can I train a model to perform multiple tasks simultaneously using multiple datasets, but only have labeled data for each task individually.
I combined two datasets into a comment format with a task type as a task requirement. During the training phase, I used this task type as a condition for the model to calculate the loss for each task and the total loss is the sum of the two losses. Is it ok, I tried it but not effective. Is this approach ok and reasonable???</p>
",Training and Model Evaluation,multitask learning multi dataset train model perform multiple task simultaneously using multiple datasets labeled data task individually combined two datasets comment format task type task requirement training phase used task type condition model calculate loss task total loss sum two loss ok tried effective approach ok reasonable
How to automatically choose the &quot;best&quot; K for a STM model based on multiple diagnostic variables,"<p>In structural topic modeling using the <code>stm</code> R package, the function <code>searchK</code> allows the user to run multiple models with different numbers of topics, and then returns the diagnostic properties for each model, so that the user can choose which performed better and therefore choose the ideal number of topics (K) for the data.</p>
<p>Some of these measures are <em>held-out likelihood</em> (higher is better), <em>residuals</em> (lower is better), and <em>semantic coherence</em> (higher is better).</p>
<p>The data looks like this, where each row is a model with a different amount of topics (K):</p>
<pre><code># K   semcoh    heldout  residual
# 10 -55.25328 -5.584059 1.127426
# 15 -54.84985 -5.532205 1.07168
# 17 -57.78122 -5.516972 1.056486
# 20 -56.90894 -5.493265 1.030628
# 22 -58.15011  -5.44756 1.015333
# 25 -59.86378 -5.431342 1.003179
</code></pre>
<p>Using <code>plot()</code>, the package returns the following graphic which allows the user to choose the best model:</p>
<p><a href=""https://i.sstatic.net/AJV0OjS8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AJV0OjS8.png"" alt=""enter image description here"" /></a>
In this case, its cleat that 25 is the best amount of topics, because it has higher held-out likelihood and lower residuals.</p>
<p>But in cases like the following the decision is not so clear, as the higher held-out likelihood is around 17, but the residuals are not lower at K = 17, so it's kind of a tradeoff: after K = 17, the higher K is, the lower the residuals are (good), but the held-out likelihood decreases (bad), but the better measure in residuals is actually a smaller change than the worsening measure in held-out likelihood, therefore in this case it's not worth it to go for lower residuals. Is there a way to automate these kind of decisions?</p>
<p>In other words, is there a way to find the optimal value for a variable by looking at diminishing returns on different variables, i.e., choose considering the next possibility can measure better on one variable, but also worse on other variable, balance the win vs. the loss, and settle on a value?</p>
<p>I tried doing a median-based approach using the <code>which</code> functions, but it doesn't always give the best K:</p>
<pre><code>median(c(
       which.max(results_a$semcoh),
       which.max(results_a$heldout),
       which.min(results_a$residual)
))
</code></pre>
<p><a href=""https://i.sstatic.net/WihPVKow.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WihPVKow.png"" alt=""enter image description here"" /></a></p>
<pre><code>results_a &lt;- structure(list(K = list(10, 15, 17, 20, 22, 25), semcoh = list(
    -55.2532813572245, -54.8498529671037, -57.7812176351009, 
    -56.9089355258198, -58.1501148277006, -59.8637782940342), 
    heldout = list(-5.58405885132492, -5.53220465315627, -5.51697205885505, 
        -5.49326482691615, -5.44755983789593, -5.43134154907285), 
    residual = list(1.12742587858013, 1.07167984608645, 1.05648599573627, 
        1.03062808239133, 1.01533334688186, 1.00317896181321)), row.names = c(NA, 
-6L), class = &quot;data.frame&quot;)

results_b &lt;- structure(list(K = list(10, 15, 17, 20, 22, 25), semcoh = list(
    -65.8929182884279, -69.437105852102, -73.1939358885751, -75.6839741459559, 
    -77.3398101594828, -77.3598106852508), heldout = list(-6.31404845126443, 
    -6.321083522624, -6.28269982323658, -6.30191607990617, -6.32046527332797, 
    -6.32299991122077), residual = list(1.34467069132499, 1.2780706228344, 
    1.23831374392626, 1.21041197190302, 1.19623018255656, 1.17377825556606)), row.names = c(NA, 
-6L), class = &quot;data.frame&quot;)
</code></pre>
",Training and Model Evaluation,automatically choose best k stm model based multiple diagnostic variable structural topic modeling using r package function allows user run multiple model different number topic return diagnostic property model user choose performed better therefore choose ideal number topic k data measure held likelihood higher better residual lower better semantic coherence higher better data look like row model different amount topic k using package return following graphic allows user choose best model case cleat best amount topic ha higher held likelihood lower residual case like following decision clear higher held likelihood around residual lower k kind tradeoff k higher k lower residual good held likelihood decrease bad better measure residual actually smaller change worsening measure held likelihood therefore case worth go lower residual way automate kind decision word way find optimal value variable looking diminishing return different variable e choose considering next possibility measure better one variable also worse variable balance win v loss settle value tried median based approach using function always give best k
Fine tuning T5 not converging,"<p>I am new in this world of transformers and NLP, and I am having a problem when fine tuning T5 for my specific use case.</p>
<p>What I want to achieve, is that the model receives an input text, and outputs a JSON (as a string) of the relevant information in the text.</p>
<p>There are 3 formats that the model can respond, below are some examples:
Input: Hey, can you give one hundred dollars to John?
Expected Output:  '{&quot;action&quot;: &quot;T&quot;, &quot;data&quot;: {&quot;name&quot;: &quot;John&quot;, &quot;amount&quot;: 100, &quot;currency&quot;: &quot;USD&quot;}}'</p>
<p>Input: I want to add Benjamin Franklin to my contacts. He has an account on citibank, with number 412389124.
Expected Output: '{&quot;action&quot;: &quot;A&quot;, &quot;data&quot;: {&quot;name&quot;: &quot;Benjamin Franklin&quot;, &quot;account_no&quot;: 412389124, &quot;entity&quot;: &quot;Citibank&quot;, &quot;id_num&quot;: null}'</p>
<p>Input: Hey, what's the weather gonna be tonight?
Expected Output:  '{&quot;accion&quot;: &quot;N&quot;, &quot;datos&quot;: {}}'</p>
<p>I've built a Python script to generate the inputs and labels as random as possible. With that python script, I generated 20000 data points (I can generate less or more of that).</p>
<p>Using T5 as my base model, I've trained it using the trainer from pytorch.</p>
<p>Below is my code:</p>
<pre><code>model_name_huggingface = &quot;google/t5-base&quot;

tokenizer = T5Tokenizer.from_pretrained(model_name_huggingface)
model = T5ForConditionalGeneration.from_pretrained(model_name_huggingface)
</code></pre>
<p>Then, after I tokenize my dataset.</p>
<pre><code>batch_size = 16

training_args = Seq2SeqTrainingArguments(
    output_dir=&quot;models/chimi-mt5-base&quot;,
    evaluation_strategy=&quot;steps&quot;,
    eval_steps=100,
    logging_strategy=&quot;steps&quot;,
    logging_steps=100,
    save_strategy=&quot;steps&quot;,
    save_steps=200,
    # learning_rate=1e-4,
    optim=&quot;adafactor&quot;,
    learning_rate=5e-4,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    predict_with_generate=True,
    weight_decay=0.05,
    save_total_limit=3,
    num_train_epochs=2,
    metric_for_best_model=&quot;exact_match&quot;,
    # greater_is_better=False,
    load_best_model_at_end=True
)
</code></pre>
<pre><code>data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=base_model)
cer = evaluate.load(&quot;cer&quot;, module_type=&quot;metric&quot;)
exact_match = evaluate.load(&quot;exact_match&quot;, module_type=&quot;metric&quot;)
</code></pre>
<pre><code>import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    result = {}

    # Compute CER
    result[&quot;cer&quot;] = cer.compute(predictions=decoded_preds, references=decoded_labels)

    # Compute Exact Match
    exact_match_res = exact_match.compute(predictions=decoded_preds, references=decoded_labels, ignore_case=True)
    result[&quot;exact_match&quot;] = exact_match_res[&quot;exact_match&quot;]

    return {k: round(v, 4) for k, v in result.items()}
</code></pre>
<pre><code>trainer = Seq2SeqTrainer(
    model=base_model,
    args=training_args,
    train_dataset=tokenized_chimi_dataset[&quot;train&quot;],
    eval_dataset=tokenized_chimi_dataset[&quot;validation&quot;],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
</code></pre>
<pre><code>result = trainer.train()
</code></pre>
<p>That's the current code I am using to fine tune T5.</p>
<p>The training loss goes down up to 0.054, and never improves.
The validation loss goes down up 0.034, and never improves.
The CER metric goes down up to 0.4875 and never improves after that. But, just to let you know, after the first 100 steps, it already has a CER of 0.583.
The Exact Match Metric goes up to 0.3089, and that already happens after the 600th step.</p>
<p>By testing, I see that it responds in the correct JSON format, and the action is responded correctly normally. But then, the data inside the JSON is not often correct.</p>
<p>What can I do to improve this?
I am stuck on this for a long time, and I am not really sure how to proceed. Any help is appreciated.</p>
<p>Thanks in advance!</p>
<p>I tried balancing my dataset, and tuning the hyperparameters, but it still didn't result in any relevant ups in performance.</p>
",Training and Model Evaluation,fine tuning converging new world transformer nlp problem fine tuning specific use case want achieve model receives input text output json string relevant information text format model respond example input hey give one hundred dollar john expected output action data name john amount currency usd input want add benjamin franklin contact ha account citibank number expected output action data name benjamin franklin account entity citibank id num null input hey weather gon na tonight expected output accion n datos built python script generate input label random possible python script generated data point generate le using base model trained using trainer pytorch code tokenize dataset current code using fine tune training loss go never improves validation loss go never improves cer metric go never improves let know first step already ha cer exact match metric go already happens th step testing see responds correct json format action correctly normally data inside json often correct improve stuck long time really sure proceed help appreciated thanks advance tried balancing dataset tuning hyperparameters still result relevant ups performance
Model prediction differed after one year with same config and dataset,"<p>I tried a TrOCR-Hugging Face model, with a training data (400k) and got around 83% of accuracy with the test data. After a year, I tried to train the same model, with same config and dataset. Now the model predicts only 69% of my test data only.</p>
<p>I used a pretrained model 'microsoft/trocr-base-handwritten' from Hugging Face. And the thing differed was GPU instances. Trained using data_parallel on dual gpus in first attempt (32*2 batch size) and 4 gpus on second attempt (16*4 batch_size).</p>
<p>Is there any other way to reproduce the same results?</p>
",Training and Model Evaluation,model prediction differed one year config dataset tried trocr hugging face model training data k got around accuracy test data year tried train model config dataset model predicts test data used pretrained model microsoft trocr base handwritten hugging face thing differed wa gpu instance trained using data parallel dual gpus first attempt batch size gpus second attempt batch size way reproduce result
ValueError: Expected input batch_size (2) to match target batch_size (4),"<p>Here is the code for a text classification task I am doing. The issue seems to lie here. This is a multi class problem. I have 3 labels. I tried several things. I changed the format of the labels to integers and tried looking into the loss function. I am not sure what parameter needs to be changed.</p>
<pre><code>def evaluate(model, dataloader_val):
    model.eval()
    model.train(False)
    
    loss_val_total = 0
    predictions, true_vals = [], []
    
    for batch in dataloader_val:
        
        batch = tuple(b.to(device) for b in batch)
        
        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }

        with torch.no_grad():        
            outputs = model(**inputs)
            
        loss = outputs[0]
        
        logits = outputs[1]
        loss_val_total += loss.item()

        probs = torch.argmax(logits, dim = 1).detach().cpu().numpy()
        label_ids = inputs['labels'].cpu().numpy()
        predictions.append(probs)
        true_vals.append(label_ids)
        
    loss_val_avg = loss_val_total/len(dataloader_val) 
    
    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)
    
    ### after evaluating we resume model training
    model.train(True)
    
    return loss_val_avg, predictions, true_vals
</code></pre>
<p>And this is the error I get</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-55-a095c6ad8f10&gt; in &lt;module&gt;
     44                      }       
     45 
---&gt; 46             outputs = model(**inputs)
ValueError: Expected input batch_size (2) to match target batch_size (4).
</code></pre>
",Training and Model Evaluation,valueerror expected input batch size match target batch size code text classification task issue seems lie multi class problem label tried several thing changed format label integer tried looking loss function sure parameter need changed error get
Text to Openpose and Weird RNN bugs,"<p>I want to create AI that generate openpose from textual description for example if input &quot;a man running&quot; output would be like the image I provided Is there any model architecture recommend for me?</p>
<p>my data condition is</p>
<ul>
<li>canvas_width: 900px</li>
<li>canvas_height: 300px</li>
<li>frames: 5 (5 person)</li>
</ul>
<p><a href=""https://i.sstatic.net/jt6VjR2F.png"" rel=""nofollow noreferrer"">expected output</a></p>
<p>I trying to train RNN for this task and I use sentence transformer for embedding text and then pass to RNN and the loss is look like image below</p>
<pre><code>from sentence_transformers import SentenceTransformer 
sentence_model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)
text = &quot;a man running&quot;
text_input = torch.tensor(sentence_model.encode(text), dtype=torch.float)
</code></pre>
<p><a href=""https://i.sstatic.net/f1sYGb6t.png"" rel=""nofollow noreferrer"">loss image with num_layers=3</a></p>
<p>My RNN setting</p>
<pre><code>embedding_dim = 384
hidden_dim = 512
num_layers = 3
output_dim = 180
num_epochs = 100
learning_rate = 0.001
rnn_model = RNN(embedding_dim, hidden_dim, num_layers, output_dim)
</code></pre>
<p>but the problem is whatever I input the output is the same everytime! but when I try changing num_layers to 1 and keep other setting the same like this</p>
<pre><code>embedding_dim = 384
hidden_dim = 512
num_layers = 1
output_dim = 180
num_epochs = 100
learning_rate = 0.001
rnn_model = RNN(embedding_dim, hidden_dim, num_layers, output_dim)
</code></pre>
<p>the loss now look like this
<a href=""https://i.sstatic.net/ACZ1Ma8J.png"" rel=""nofollow noreferrer"">loss image with num_layers=1</a>
and now the problem is gone !!</p>
<p>Also I try to check the cause of the &quot;output is the same everytime&quot; problem I check dataloader and other code but no problem was found only num_layers=3 that cause the problem num_layers=1 fixed it</p>
<p>This is my training loop</p>
<pre><code>criterion = nn.MSELoss()
optimizer = torch.optim.Adam(rnn_model.parameters(), lr=learning_rate)
</code></pre>
<pre><code>trainingEpoch_loss = []
validationEpoch_loss = []

for epoch in range(num_epochs):
    step_loss = []
    rnn_model.train()
    for idx, train_inputs in enumerate(train_dataloader):
        optimizer.zero_grad()
        outputs = rnn_model(torch.unsqueeze(train_inputs['text'], dim=0))
        training_loss = criterion(outputs, train_inputs['poses'])
        training_loss.backward()
        optimizer.step()
        step_loss.append(training_loss.item())

        if (idx+1) % 1 == 0: print (f'Epoch [{epoch+1}/{num_epochs}], Step [{idx+1}/{len(train_dataloader)}], Loss: {training_loss.item():.4f}')
    trainingEpoch_loss.append(np.array(step_loss).mean())

    rnn_model.eval()
    for idx, val_inputs in enumerate(val_dataloader):
      validationStep_loss = []
      outputs = rnn_model(torch.unsqueeze(val_inputs['text'], dim=0))
      val_loss = criterion(outputs, val_inputs['poses'])
      validationStep_loss.append(val_loss.item())
    validationEpoch_loss.append(np.array(validationStep_loss).mean())
</code></pre>
<p>This is my Inference</p>
<pre><code>text = &quot;a man running&quot;
processed_text = torch.tensor(sentence_model.encode(text), dtype=torch.float)
output_poses = rnn_model(processed_text.unsqueeze(0))
print(output_poses.shape) #shape=(1, 180) 1 person is 36 (original data for 1 person is 54 but I change to 36 because I want only x and y and not z so cut out the z axis) and there's 5 person so 5*36 = 180
</code></pre>
<p>My question is</p>
<ol>
<li>Is there any model architecture recommend for this task other than RNN?</li>
<li>Why whatever I input the output is the same everytime when num_layers=3 I'm very confused because the loss wouldn't go down if the model was giving the same output right? that's mean it give the same output in the Inference phase</li>
</ol>
<p>Expected Answer</p>
<ol>
<li>Model architecture that suit best for my task any papers or github repo related given would be appreciated</li>
<li>Answer why whatever I input the output is the same everytime when num_layers=3</li>
</ol>
",Training and Model Evaluation,text openpose weird rnn bug want create ai generate openpose textual description example input man running output would like image provided model architecture recommend data condition canvas width px canvas height px frame person expected output trying train rnn task use sentence transformer embedding text pas rnn loss look like image loss image num layer rnn setting problem whatever input output everytime try changing num layer keep setting like loss look like loss image num layer problem gone also try check cause output everytime problem check dataloader code problem wa found num layer cause problem num layer fixed training loop inference question model architecture recommend task rnn whatever input output everytime num layer confused loss go model wa giving output right mean give output inference phase expected answer model architecture suit best task paper github repo related given would appreciated answer whatever input output everytime num layer
Looking for the right framework to implement an enterprise document management and analysis system,"<p>I have already spent quite some time with literature reviews and Google searches, but I didn't find anything suitable, yet.</p>
<p>The task is to implement a flexible and scalable enterprise document management and analysis system. I guess that represents a prototypical use case for many businesses.</p>
<p>The perfect framework would allow on premises operation (only Azure would be an option) and provide a low-code platform that allows to receive, tag and register documents (PDFs, Word and Excel files, other text files), indexing and smart search within and across documents and document collections, plus an interface to implement NLP tasks with Python.</p>
<p>Moreover, it would be benefitial, if this framework also would allow to model meta data about documents and about the business processes they are embedded in (for example, to check and verify completeness of a set of necessary documents, before further processing gets triggered).</p>
<p>I thought about a combination of Elastic Search and a NoSql Database like Cassandra, but that would not fit the low-code requirement.</p>
<p>You might call me naive, but I supposed that there ought to be trillions of such frameworks, as this is such a typical use case in terms of business automation. But I did not find the right framework, yet. I hope someone can provide hints.</p>
<p>Summary:</p>
<p>A document management and analysis framework that features:</p>
<ul>
<li>Enterprise-ready (on premises or compatible with Microsoft Azure)</li>
<li>Low-code framework</li>
<li>Large-scale document management and analysis</li>
<li>Modular and extensible via Python and NLP models</li>
<li>Connectable to business logics (i.e. checks for completeness of document collections)</li>
<li>Allowing for meta data and smart search within and across documents</li>
</ul>
",Training and Model Evaluation,looking right framework implement enterprise document management analysis system already spent quite time literature review google search find anything suitable yet task implement flexible scalable enterprise document management analysis system guess represents prototypical use case many business perfect framework would allow premise operation azure would option provide low code platform allows receive tag register document pdfs word excel file text file indexing smart search within across document document collection plus interface implement nlp task python moreover would benefitial framework also would allow model meta data document business process embedded example check verify completeness set necessary document processing get triggered thought combination elastic search nosql database like cassandra would fit low code requirement might call naive supposed ought framework typical use case term business automation find right framework yet hope someone provide hint summary document management analysis framework feature enterprise ready premise compatible microsoft azure low code framework large scale document management analysis modular extensible via python nlp model connectable business logic e check completeness document collection allowing meta data smart search within across document
Key matrix redundant in Transformer language models?,"<p>Simple implementations of Transformer language models such as <a href=""https://www.youtube.com/watch?v=kCc8FmEb1nY"" rel=""nofollow noreferrer"">this one</a> define 3 matrices K,Q,V to compute keys, queries and values. However matrices K and Q are never used separately: all Transformer computations form their product <code>Q^t K</code>. So I wonder why not learn this product matrix directly instead of splitting it into 2 matrices K and Q.</p>
<p>Part of the answer may come from the size of K and Q, which is <code>d -&gt; n</code>, where d is the dimension of the token embeddings and n is the dimension of keys and queries. The size of <code>Q^t K</code> is <code>d -&gt; d</code>. So learning K and Q separately means optimizing <code>2*n*d</code> parameters, whereas learning the product <code>Q^t K</code> is <code>d*d</code> parameters. The only useful splitting I see is when <code>n &lt;= d/2</code>, because that's less parameters to optimize. But at the limit case <code>n = d/2</code>, the rank of the product matrix <code>Q^t K</code> is <code>d/2</code>, which is very degenerate. With the same number of parameters <code>d^2</code>, we could learn an unconstrained square matrix. That might learn more flexible and subtle patterns in the training data.</p>
<p>In the <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention is all you need</a> paper, base model page 9, we see d = 512 and n = 64, so the product matrix <code>Q^t K</code> does have degenerate rank. Is reducing the number of parameters the true and unique intent here? Is there a theoretical justification that these degenerate ranks help natural language processing?</p>
",Training and Model Evaluation,key matrix redundant transformer language model simple implementation transformer language model one define matrix k q v compute key query value however matrix k q never used separately transformer computation form product wonder learn product matrix directly instead splitting matrix k q part answer may come size k q dimension token embeddings n dimension key query size learning k q separately mean optimizing parameter whereas learning product parameter useful splitting see le parameter optimize limit case rank product matrix degenerate number parameter could learn unconstrained square matrix might learn flexible subtle pattern training data attention need paper base model page see n product matrix doe degenerate rank reducing number parameter true unique intent theoretical justification degenerate rank help natural language processing
Fine-tuning CodeBert for classification with more than 512 tokens,"<p>I have a dataset of SmartContracts source code written in Solidity labeled with one ore more vulnerabilities they contains.</p>
<p>I used the CodeBert model to classify them in a multilabel classification problem obtaining a 83% accuracy on test set.
The problem is that the CodeBert takes in input a maximum of 512 tokens so just a small initial portions of contracts, how can I input longer sequences to improve the results of my model?
I think that splitting a contract in substrings of lenght 512 is not the best solution since I'm training the model on wrong label for that subtring.</p>
",Training and Model Evaluation,fine tuning codebert classification token dataset smartcontracts source code written solidity labeled one ore vulnerability contains used codebert model classify multilabel classification problem obtaining accuracy test set problem codebert take input maximum token small initial portion contract input longer sequence improve result model think splitting contract substring lenght best solution since training model wrong label subtring
why gradient scaling used in mix-precision training could lead to bigger Learning Rate in float32 model?,"<p>Background:</p>
<p>Gradient Scaling original is used in mix-precision training like (part of model weights are float16 and part of them are float 32), which its purpose is to reduce underflow of small gradients stored with float16.
Recently, I was doing some experiments and found some result confussing me. In the case of whole model with float32 weights, if I update the model with Gradient Scaling, I could use much bigger LR (Learning Rate) for convergence compared to don't use it.</p>
<p>Scaled version updating code excerpt:</p>
<pre><code>loss_scaler = torch.cuda.amp.GradScaler()
loss_scaler.scale(loss).backward()
loss_scaler.unscale_(optimizer)
loss_scaler.step(optimizer)
loss_scaler.update()
</code></pre>
<p>unscaled version updating code excerpt:</p>
<pre><code>loss.backward()
optimizer.step()
</code></pre>
<p>Problem:
why would this happen? Should the scaled gradients be cancelled out tranformed to original gradients? What I expect is that thier LR should be the same for model training?</p>
<p>Here is the ChatGPT's explaination:
Even float32 could underflow, and the scale make these underflow gradients make contribute to the weights updating which make training more stable, so the lr could be bigger.</p>
<p>I wonder if this explaination is reasonable, cause the LR is much different. (1.5e-4 vs 1.5e-8)</p>
",Training and Model Evaluation,gradient scaling used mix precision training could lead bigger learning rate float model background gradient scaling original used mix precision training like part model weight float part float purpose reduce underflow small gradient stored float recently wa experiment found result confussing case whole model float weight update model gradient scaling could use much bigger lr learning rate convergence compared use scaled version updating code excerpt unscaled version updating code excerpt problem would happen scaled gradient cancelled tranformed original gradient expect thier lr model training chatgpt explaination even float could underflow scale make underflow gradient make contribute weight updating make training stable lr could bigger wonder explaination reasonable cause lr much different e v e
`index out of range in self` when I ran Dynamic Bernoulli Embeddings,"<p>I'm working on Dynamic Bernoulli Embeddings using Python, but I'm encountering an <code>IndexError: index out of range in self</code> when I attempt to train the model. I'm a beginner in NLP, so I struggle to understand and fix this problem.</p>
<p>Here's the relevant portion of my code:</p>
<pre class=""lang-py prettyprint-override""><code>import pickle
import re
import numpy as np
import pandas as pd
from dynamic_bernoulli_embeddings.analysis import DynamicEmbeddingAnalysis
from dynamic_bernoulli_embeddings.training import train_model
from nltk import word_tokenize as nltk_word_tokenize
from gensim.corpora import Dictionary
from tqdm.notebook import tqdm
tqdm.pandas()

# bow contains tokenized words
dataset = df2[['bow', 'time', 'year']]

# Create a Gensim dictionary
dictionary = Dictionary(dataset['bow'])
dictionary.filter_extremes(no_below=2, no_above=1.0)
dictionary.compactify()

# Training the Dynamic Bernoulli Embeddings model
model, loss_history = train_model(
    dataset, dictionary.token2id, validation=.1, num_epochs=5, k=50)

</code></pre>
<p>The following error occurs during the model training:</p>
<pre class=""lang-py prettyprint-override""><code>IndexError: index out of range in self

Traceback:
  File &quot;&lt;ipython-input-44-1d8d4321a42a&gt;&quot;, in &lt;cell line: 2&gt;
    model, loss_history = train_model(
      dataset, dictionary.token2id, validation=.1, num_epochs=5, k=50)

7 frames
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py&quot;, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)

</code></pre>
<ul>
<li>I'm using the Dynamic Bernoulli Embeddings library from this repository: <a href=""https://www.kaggle.com/code/llefebure/dynamic-bernoulli-embeddings#Dynamic-Bernoulli-Embeddings"" rel=""nofollow noreferrer"">Dynamic Bernoulli Embeddings</a></li>
</ul>
<p>What steps can I take to resolve it?</p>
",Training and Model Evaluation,ran dynamic bernoulli embeddings working dynamic bernoulli embeddings using python encountering attempt train model beginner nlp struggle understand fix problem relevant portion code following error occurs model training using dynamic bernoulli embeddings library repository dynamic bernoulli embeddings step take resolve
Python Libraries for Sentence Classification Based on Keyword,"<p>I want to separate a list of sentences into two lists based on whether they match the sentiment of a specific keyword. For example:</p>
<pre><code>valid_keyword = &quot;Guest Accepted&quot;

sentences = [
    &quot;Guest Allowed&quot;, &quot;Max Guest Allowed 1&quot;, &quot;Guest Not Allowed&quot;, 
    &quot;No Guest Allowed&quot;, &quot;Guest Restricted&quot;, &quot;Max Guest Allowed 0&quot;, 
    &quot;Guest Allowed on Request&quot;
]
</code></pre>
<p>I want to separate the list in the following way:</p>
<p>valid_sentences, which contains sentences that match the sentiment of the valid_keyword (&quot;Guest Accepted&quot;).</p>
<p>invalid_sentences, which contains sentences that do not match the sentiment of the valid_keyword.</p>
<p>The expected output would be:</p>
<pre><code>valid_sentences = [&quot;Guest Allowed&quot;, &quot;Max Guest Allowed 1&quot;, &quot;Guest Allowed on Request&quot;]
invalid_sentences = [&quot;Guest Not Allowed&quot;, &quot;No Guest Allowed&quot;, &quot;Guest Restricted&quot;, &quot;Max Guest Allowed 0&quot;]
</code></pre>
<p>I already tried <a href=""https://vadersentiment.readthedocs.io/en/latest/index.html#"" rel=""nofollow noreferrer"">VaderSentiment</a>, but didn't get what I expected.
What are the suitable Python libraries that I should go for this process that will give maximum accuracy?</p>
",Training and Model Evaluation,python library sentence classification based keyword want separate list sentence two list based whether match sentiment specific keyword example want separate list following way valid sentence contains sentence match sentiment valid keyword guest accepted invalid sentence contains sentence match sentiment valid keyword expected output would already tried vadersentiment get expected suitable python library go process give maximum accuracy
Fine-tuning T5 Model on a Book for Unsupervised Learning,"<p>I'm working on a project where I aim to fine-tune a T5 model or any other encoder-decoder model using an unsupervised learning approach to transfer knowledge from specific books.</p>
<p>My main goal is to train a model that becomes an expert on the book's topic. However, I'm uncertain about the specific fine-tuning process to follow and which approach would yield the best results.</p>
<p>Here are my specific questions:</p>
<p>Given that I'm using an encoder-decoder model, what fine-tuning pipeline should I choose?
I've heard that Masked Language Modeling (MLM) is effective for encoder models to learn knowledge. Is MLM suitable for an encoder-decoder architecture like T5, or should I consider other methods?
Are there any potential pitfalls associated with fine-tuning on a specific book that I should be aware of?
What suggestions do you have for optimizing the fine-tuning process to ensure the model becomes an expert on the book's topic?
Any advice or recommendations would be greatly appreciated.</p>
<p>Thanks for any advice!</p>
",Training and Model Evaluation,fine tuning model book unsupervised learning working project aim fine tune model encoder decoder model using unsupervised learning approach transfer knowledge specific book main goal train model becomes expert book topic however uncertain specific fine tuning process follow approach would yield best result specific question given using encoder decoder model fine tuning pipeline choose heard masked language modeling mlm effective encoder model learn knowledge mlm suitable encoder decoder architecture like consider method potential pitfall associated fine tuning specific book aware suggestion optimizing fine tuning process ensure model becomes expert book topic advice recommendation would greatly appreciated thanks advice
BERT MLM model fine-tuning bad results on new dataset,"<p>I'm trying to fine tune a MLM model on new kind of small data (train.csv 43285 lines, validation.csv 3597), my data looks like this:</p>
<pre><code>text
בראשית ברא אלהים את השמים ואת הארץ
והארץ היתה תהו ובהו וחשך על פני תהום ורוח אלהים מרחפת על פני המים
ויאמר אלהים יהי אור ויהי אור
וירא אלהים את האור כי טוב ויבדל אלהים בין האור ובין החשך
ויקרא אלהים לאור יום ולחשך קרא לילה ויהי ערב ויהי בקר יום אחד
ויאמר אלהים יהי רקיע בתוך המים ויהי מבדיל בין מים למים
ויעש אלהים את הרקיע ויבדל בין המים אשר מתחת לרקיע ובין המים אשר מעל לרקיע ויהי כן
</code></pre>
<p>My results are poor, on old data everything works as expected, on my new data not so much:</p>
<pre><code>PHRASE (old data): ומשה ואהרן עשו את כל המופתים האלה לפני פרעה ויחזק יהוה את לב פרעה ולא שלח את בני ישראל מארצו
PHRASE (with mask): ומשה ואהרן עשו את כל [MASK] האלה לפני פרעה ויחזק יהוה את לב פרעה ולא שלח את בני ישראל מארצו

&gt;&gt;&gt; ומשה ואהרן עשו את כל המופתים האלה לפני פרעה ויחזק יהוה את לב פרעה ולא שלח את בני ישראל מארצו
&gt;&gt;&gt; ומשה ואהרן עשו את כל האותות האלה לפני פרעה ויחזק יהוה את לב פרעה ולא שלח את בני ישראל מארצו
&gt;&gt;&gt; ומשה ואהרן עשו את כל המעשים האלה לפני פרעה ויחזק יהוה את לב פרעה ולא שלח את בני ישראל מארצו
&gt;&gt;&gt; ומשה ואהרן עשו את כל הדברים האלה לפני פרעה ויחזק יהוה את לב פרעה ולא שלח את בני ישראל מארצו
&gt;&gt;&gt; ומשה ואהרן עשו את כל השפטים האלה לפני פרעה ויחזק יהוה את לב פרעה ולא שלח את בני ישראל מארצו

PHRASE (new data): ומשה ואהרן עבדו ית כל פליאתה אלין לקדם פרעה ותקף יהוה ית לב פרעה ולא שלח ית בני ישראל מן ארעה
PHRASE (with mask): ומשה ואהרן עבדו ית כל [MASK] אלין לקדם פרעה ותקף יהוה ית לב פרעה ולא שלח ית בני ישראל מן ארעה

&gt;&gt;&gt; ומשה ואהרן עבדו ית כל פתגמיא אלין לקדם פרעה ותקף יהוה ית לב פרעה ולא שלח ית בני ישראל מן ארעה
&gt;&gt;&gt; ומשה ואהרן עבדו ית כל ממלל אלין לקדם פרעה ותקף יהוה ית לב פרעה ולא שלח ית בני ישראל מן ארעה
&gt;&gt;&gt; ומשה ואהרן עבדו ית כל עובדוי אלין לקדם פרעה ותקף יהוה ית לב פרעה ולא שלח ית בני ישראל מן ארעה
&gt;&gt;&gt; ומשה ואהרן עבדו ית כל מלי אלין לקדם פרעה ותקף יהוה ית לב פרעה ולא שלח ית בני ישראל מן ארעה
&gt;&gt;&gt; ומשה ואהרן עבדו ית כל אלין אלין לקדם פרעה ותקף יהוה ית לב פרעה ולא שלח ית בני ישראל מן ארעה

PHRASE (new data): ואמר משה אכהן אמר יהוה כפלגות ליליה אנה נפק בגו ארע מצרים
PHRASE (with mask): ואמר משה אכהן אמר יהוה כפלגות [MASK] אנה נפק בממצית ארע מצרים

&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות ##ה אנה נפק בממ ##צית ארע מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות הלא אנה נפק בממ ##צית ארע מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות ##ן אנה נפק בממ ##צית ארע מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות ##יה אנה נפק בממ ##צית ארע מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות משה אנה נפק בממ ##צית ארע מצרים

PHRASE (new data): ואמר משה אכהן אמר יהוה כפלגות ליליה אנא נפק בגו ארע מצרים
PHRASE (with mask): ואמר משה אכהן אמר יהוה כפלגות ליליה אנא נפק בגו [MASK] מצרים

&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו ארע מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו ארץ מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו בארע מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו משרית מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו נהר מצרים
</code></pre>
<p>I have trained the model with the script <code>run_mlm.py</code> from <code>transformers/examples/pytorch/language-modeling</code> as follows:</p>
<blockquote>
<p>python run_mlm.py --model_name_or_path dicta-il/BEREL_2.0 --train_file
./train.csv --validation_file ./validation.csv --line_by_line
--num_train_epochs 5 --warmup_steps 500 --gradient_accumulation_steps 16 --per_device_train_batch_size 4 --per_device_eval_batch_size 4
--learning_rate 1e-4 --optim adamw_torch --evaluation_strategy steps --load_best_model_at_end --do_train --do_eval --push_to_hub --output_dir ./BEREL_2.0-sam-finetune-v4</p>
</blockquote>
<p>My model: <code>johnlockejrr/BEREL_2.0-sam-finetune-v4</code>
My dataset: <code>johnlockejrr/samv2</code></p>
<p>What did I do wrong? In my dataset or my arguments to the trainer?</p>
<p>EDIT: maybe should I use &quot;Whole Word Masking&quot; to get the same results?</p>
<p>EDIT 2: I just trained the model with WWM, same problem...</p>
<pre><code>&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו ארע מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו ארץ מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו תחום מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו בתי מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו ארעא מצרים
</code></pre>
",Training and Model Evaluation,bert mlm model fine tuning bad result new dataset trying fine tune mlm model new kind small data train csv line validation csv data look like result poor old data everything work expected new data much trained model script follows python run mlm py model name path dictum il berel train file train csv validation file validation csv line line num train epoch warmup step gradient accumulation step per device train batch size per device eval batch size learning rate e optim adamw torch evaluation strategy step load best model end train eval push hub output dir berel sam finetune v model dataset wrong dataset argument trainer edit maybe use whole word masking get result edit trained model wwm problem
Training GPT-2 on simple arithmetic data shows decreasing loss but the final model is rubbish. Any explanations?,"<p>So, I'm training a GPT-2 model on a simple task like for instance addition using the hugging face transformers library. During training the model shows that the train and validation loss are both decreasing, but when I test the model on either the train or validation data the model generates outputs that are less than 50% accurate (accuracy).
<a href=""https://i.sstatic.net/lcBJt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lcBJt.png"" alt=""enter image description here"" /></a></p>
<p>No matter what I've tried it seems that it's not working on this simple task and I don't know why when it's supposed to be something easy to learn?</p>
",Training and Model Evaluation,training gpt simple arithmetic data show decreasing loss final model rubbish explanation training gpt model simple task like instance addition using hugging face transformer library training model show train validation loss decreasing test model either train validation data model generates output le accurate accuracy matter tried seems working simple task know supposed something easy learn
How to train Hugging Face Model On Multiple Datasets?,"<p>I am trying to fine tune a model based on two datasets, following the example on the Hugging Face website, I have my model training on the Yelp Review dataset, but I also want to train my model on the Short Jokes dataset.</p>
<p>These two datasets are picked just to eximplify that the datasets I would like to fine tune the model on are totally unrelated.</p>
<p>I have seen the <code>interleave_datasets</code> function, but im not sure if its exactly what i should be using.</p>
<p>I've tried this to train on one dataset:</p>
<pre><code>from datasets import load_dataset

yelp_dataset = load_dataset(&quot;yelp_review_full&quot;)
jokes_dataset = load_dataset(&quot;short-jokes&quot;)

from transformers import AutoTokenizer, Trainer

tokenizer = AutoTokenizer.from_pretrained(&quot;google-bert/bert-base-cased&quot;)

def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=True)

tokenized_datasets = yelp_dataset.map(tokenize_function, batched=True)

small_train_yelp_dataset = tokenized_datasets[&quot;train&quot;].shuffle(seed=42).select(range(1000))
small_eval_yelp_dataset = tokenized_datasets[&quot;test&quot;].shuffle(seed=42).select(range(1000))

# Where do these go?
small_train_short_jokes_dataset = jokes_dataset[&quot;train&quot;].shuffle(seed=42).select(range(1000))
small_eval_short_jokes_dataset = jokes_dataset[&quot;test&quot;].shuffle(seed=42).select(range(1000))

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(&quot;google-bert/bert-base-cased&quot;, num_labels=5)

from transformers import TrainingArguments

training_args = TrainingArguments(output_dir=&quot;test_trainer&quot;)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_yelp_dataset,
    eval_dataset=small_eval_yelp_dataset,

)

trainer.train()

</code></pre>
<p><strong>How would I train my model on two different sets at one time?</strong></p>
",Training and Model Evaluation,train hugging face model multiple datasets trying fine tune model based two datasets following example hugging face website model training yelp review dataset also want train model short joke dataset two datasets picked eximplify datasets would like fine tune model totally unrelated seen function im sure exactly using tried train one dataset would train model two different set one time
How to create an Embedding Model for Recipe Dataset Using Deep Metric Learning?,"<p>I want to create an <strong>embedding model</strong> for my own dataset of recipes. The goal is to create a neural network that takes each recipe, represented by a list of its ingredients, a list of recipe tags, and its title, as input. The network should use <strong>deep metric learning</strong> to create an embedding model that represents recipes in a way that distinguishes them well.</p>
<p>I have considered using a triplet network with the <strong>triplet loss function</strong>, trained on a dataset of triplets. Each triplet would consist of an anchor (the recipe itself), a positive sample (a similar recipe), and a negative sample (a very different recipe). The objective would be to minimize the distances between the anchor and positive samples while maximizing the distances between the anchor and negative samples.</p>
<p>Since I'm relatively new to neural networks and natural language processing, I would greatly appreciate your thoughts and insights on this approach. Do you think this method is suitable for my task? Are there any potential pitfalls or challenges that I should be aware of?
Additionally, I would be grateful if you could provide me with a guideline on how I can successfully execute this project.</p>
<p>Thank you.</p>
",Training and Model Evaluation,create embedding model recipe dataset using deep metric learning want create embedding model dataset recipe goal create neural network take recipe represented list ingredient list recipe tag title input network use deep metric learning create embedding model represents recipe way distinguishes well considered using triplet network triplet loss function trained dataset triplet triplet would consist anchor recipe positive sample similar recipe negative sample different recipe objective would minimize distance anchor positive sample maximizing distance anchor negative sample since relatively new neural network natural language processing would greatly appreciate thought insight approach think method suitable task potential pitfall challenge aware additionally would grateful could provide guideline successfully execute project thank
Error while installing medaCy package while following instructions from github repo,"<p><strong>I wanted to run this command to install medaCy's Prediction and Model Training (stable version)</strong> :</p>
<p>! pip install git+https://github.com/NLPatVCU/medaCy.git</p>
<p><strong>But I've encountered an error as shown below :</strong></p>
<p><a href=""https://i.sstatic.net/M81sz.png"" rel=""nofollow noreferrer"">img1</a></p>
<p><a href=""https://i.sstatic.net/pk76l.png"" rel=""nofollow noreferrer"">img2</a></p>
<p><a href=""https://i.sstatic.net/xAv29.png"" rel=""nofollow noreferrer"">img3</a></p>
<p><a href=""https://i.sstatic.net/WZxgl.png"" rel=""nofollow noreferrer"">img4</a></p>
<p>medaCy's github repo : <a href=""https://github.com/NLPatVCU/medaCy"" rel=""nofollow noreferrer"">https://github.com/NLPatVCU/medaCy</a></p>
<p>I wanted to run the command to install medaCy's Prediction and Model Training (stable version), and train the model on my own data.
expected work :</p>
<p><a href=""https://i.sstatic.net/brSrs.png"" rel=""nofollow noreferrer"">img5</a></p>
<p>But I've encountered an error while installing the medaCy model. Probably there is an issue with the version specifier format : <strong>spacy_lookups_data&gt;=0.0.5&lt;0.2.0</strong></p>
",Training and Model Evaluation,error installing medacy package following instruction github repo wanted run command install medacy prediction model training stable version pip install git encountered error shown img img img img medacy github repo wanted run command install medacy prediction model training stable version train model data expected work img encountered error installing medacy model probably issue version specifier format spacy lookup data
How to speed up computation time for stopword removal and lemmatization in NLP,"<p>As part of pre-processing for a text classification model, I have added stopword removal and lemmatization steps, using the NLTK library. The code is below:</p>
<pre><code>import pandas as pd
import nltk; nltk.download(&quot;all&quot;)
from nltk.corpus import stopwords; stop = set(stopwords.words('english'))
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

# Stopwords removal


def remove_stopwords(entry):
  sentence_list = [word for word in entry.split() if word not in stopwords.words(&quot;english&quot;)]
  return &quot; &quot;.join(sentence_list)

df[&quot;Description_no_stopwords&quot;] = df.loc[:, &quot;Description&quot;].apply(lambda x: remove_stopwords(x))

# Lemmatization

lemmatizer = WordNetLemmatizer()

def punct_strip(string):
  s = re.sub(r'[^\w\s]',' ',string)
  return s

def get_wordnet_pos(word):
    &quot;&quot;&quot;Map POS tag to first character lemmatize() accepts&quot;&quot;&quot;
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {&quot;J&quot;: wordnet.ADJ,
                &quot;N&quot;: wordnet.NOUN,
                &quot;V&quot;: wordnet.VERB,
                &quot;R&quot;: wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

def lemmatize_rows(entry):
  sentence_list = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in punct_strip(entry).split()]
  return &quot; &quot;.join(sentence_list)

df[&quot;Description - lemmatized&quot;] = df.loc[:, &quot;Description_no_stopwords&quot;].apply(lambda x: lemmatize_rows(x))

</code></pre>
<p>The problem is that, when I pre-process a dataset with 27k entries (my test set), it takes 40-45 seconds for stopwords removal and just as long for lemmatization. By contrast, model evaluation only takes 2-3 seconds.</p>
<p>How can I re-write the functions to optimise computation speed? I have read something about vectorization, but the example functions were much simpler than the ones that I have reported, and I wouldn't know how to do it in this case.</p>
",Training and Model Evaluation,speed computation time stopword removal lemmatization nlp part pre processing text classification model added stopword removal lemmatization step using nltk library code problem pre process dataset k entry test set take second stopwords removal long lemmatization contrast model evaluation take second write function optimise computation speed read something vectorization example function much simpler one reported know case
Swift Natural Language and CoreML: How to improve NLTagger to read Card Holder,"<p>I am using Natural Language framework to find personal name on credit card. 
Firstly I read credit card texts using Vision framework. Then I concatenate it. </p>

<p>So I have text that contains format similar to this: </p>

<pre><code>""Citi 6011 1111 1111 1117 07/25 ELON MUSK Discover Debit""
</code></pre>

<p>I have tired to simply find .personalName NLTags in such string
But it doesn't work perfect. </p>

<pre><code>let tagger = NLTagger(tagSchemes: [.nameType])
    tagger.string = text
    tagger.setLanguage(.english, range: range)

    var fullNames = [String]()

    // 1) personalName
    let options : NLTagger.Options = [.omitPunctuation, .omitWhitespace, .omitOther, .joinNames]
    let foundTags = tagger.tags(in: range, unit: .word, scheme: .nameType, options: options)

    foundTags.forEach { (tag, tokenRange) in
        if tag == .personalName {
            let name = text[tokenRange]
            fullNames.append(String(name))
        }
    }
</code></pre>

<p>So I am trying to use CreateML in macOS playeground to create CoreML model that will be used to tag custom CARDHOLDER tags in such string. </p>

<p><strong>How many example labeled data there is needed to train such model to be usable?</strong></p>

<p>Now I have such relatively simple case </p>

<pre><code>[
    {
        ""tokens"": [""Inteligo"", ""4242 4242 4242 4242"", ""09/21"", ""JOHN SMITH"", ""VISA"", ""Debit""],
        ""labels"": [""ORG"", ""NONE"", ""NONE"", ""CARDHOLDER"", ""ORG"", ""NONE""]
    },
    {
        ""tokens"": [""mBank"", ""4000 0566 5566 5556"", ""10/23"", ""STEVE JOBS"", ""VISA"", ""Debit""],
        ""labels"": [""ORG"", ""NONE"", ""NONE"", ""CARDHOLDER"", ""ORG"", ""NONE""]
    },
    {
        ""tokens"": [""ING"", ""5555 5555 5555 4444"", ""03/22"", ""EMMA WATSON"", ""mastercard"", ""Debit""],
        ""labels"": [""ORG"", ""NONE"", ""NONE"", ""CARDHOLDER"", ""ORG"", ""NONE""]
    },
    {
        ""tokens"": [""Bank of America"", ""3782 822463 10005"", ""05/24"", ""JULIA ROBERTS"", ""American Express"", ""Debit""],
        ""labels"": [""ORG"", ""NONE"", ""NONE"", ""CARDHOLDER"", ""ORG"", ""NONE""]
    },
    {
        ""tokens"": [""Citi"", ""6011 1111 1111 1117"", ""07/25"", ""ELON MUSK"", ""Discover"", ""Debit""],
        ""labels"": [""ORG"", ""NONE"", ""NONE"", ""CARDHOLDER"", ""ORG"", ""NONE""]
    }
]
</code></pre>

<p>Then I create CoreML model from this </p>

<pre><code>import Foundation
import CreateML

let trainingData = try MLDataTable(contentsOf:
    Bundle.main.url(forResource: ""data"", withExtension: ""json"")!)

let model = try MLWordTagger(trainingData: trainingData, tokenColumn: ""tokens"", labelColumn: ""labels"")

try model.write(to: URL(fileURLWithPath: ""#path/to/Desktop/creditcardtagger.mlmodel""))
</code></pre>

<p>Then I use it: </p>

<pre><code>let tagger = CreditCardTagger.shared
        tagger.string = text
        tagger.setLanguage(.english, range: range)
tagger.enumerateTags(in: range, unit: .word, scheme: CreditCardTagger.Scheme, options: options) { (tag, tokenRange) -&gt; Bool in

            if tag == CreditCardTagger.cardHolderTag {
                let cardHolder = text[tokenRange]
                print(""CARD HOLDER: \(cardHolder)"")
            }
            return true
        }
</code></pre>

<p>But I think that data I am using for training are insufficient. Do you know how much such records of data is needed to cover most of credit card cases?</p>
",Training and Model Evaluation,swift natural language coreml improve nltagger read card holder using natural language framework find personal name credit card firstly read credit card text using vision framework concatenate text contains format similar tired simply find personalname nltags string work perfect trying use createml macos playeground create coreml model used tag custom cardholder tag string many example labeled data needed train model usable relatively simple case create coreml model use think data using training insufficient know much record data needed cover credit card case
How to identify feature names from indices in a decision tree using scikit-learn’s CountVectorizer?,"<p>I have the following data for training a model to detect whether a sentence is about:</p>
<ul>
<li>a cat or dog</li>
<li>NOT about a cat or dog</li>
</ul>
<p><a href=""https://i.sstatic.net/gNnh6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gNnh6.png"" alt=""screenshot of data consisting of a text column and label column"" /></a></p>
<p>I ran the following code to train a <code>DecisionTreeClassifier()</code> model then view the tree visualisation:</p>
<pre><code>import numpy as np
from numpy.random import seed
import random as rn
import os
import pandas as pd
seed_num = 1
os.environ['PYTHONHASHSEED'] = '0'
np.random.seed(seed_num)
rn.seed(seed_num)

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

dummy_train = pd.read_csv('dummy_train.csv')

tree_clf = tree.DecisionTreeClassifier()

X_train = dummy_train[&quot;text&quot;]
y_train = dummy_train[&quot;label&quot;]

dt_tree_pipe = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),
                                                 binary=True)),
                     ('tfidf', TfidfTransformer(use_idf=False)),
                      ('clf', DecisionTreeClassifier(random_state=seed_num,
                                                 class_weight={0:1, 1:1})),
                   ])

tree_model_fold_1 = dt_tree_pipe.fit(X_train, y_train)

tree.plot_tree(dt_tree_pipe[&quot;clf&quot;])
</code></pre>
<p>...resulting in the following tree:</p>
<p><a href=""https://i.sstatic.net/4LBJ4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4LBJ4.png"" alt=""screenshot of decision tree visualisation"" /></a></p>
<p>The first node checks if <code>x[7]</code> is less than or equal to <code>0.177</code>. <strong>How do I find out which word <code>x[7]</code> represents?</strong></p>
<p>I tried the following code but the words returned in the output (&quot;describing&quot; and &quot;the&quot;) don't look correct. I would have thought <code>'cat'</code> and <code>'dog'</code> would be the two words used to split the data into the positive and negative class.</p>
<pre><code>vect_from_pipe = dt_tree_pipe[&quot;vect&quot;]
words = vect_from_pipe.vocabulary_.keys()
print(list(words)[7])
print(list(words)[5])
</code></pre>
<p><a href=""https://i.sstatic.net/qdCbc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qdCbc.png"" alt=""screenshot of the words 'describing' and 'the'"" /></a></p>
",Training and Model Evaluation,identify feature name index decision tree using scikit learn countvectorizer following data training model detect whether sentence cat dog cat dog ran following code train model view tree visualisation resulting following tree first node check le equal find word represents tried following code word returned output describing look correct would thought would two word used split data positive negative class
How to Train GloVe algorithm on my own corpus,"<p>I tried to follow <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">this.</a><br>
But some how I wasted a lot of time ending up with nothing useful.<br>
I just want to train a <code>GloVe</code> model on my own corpus (~900Mb corpus.txt file).
I downloaded the files provided in the link above and compiled it using <code>cygwin</code> (after editing the demo.sh file and changed it to <code>VOCAB_FILE=corpus.txt</code> . should I leave <code>CORPUS=text8</code> unchanged?)
the output was:  </p>

<ol>
<li>cooccurrence.bin </li>
<li>cooccurrence.shuf.bin  </li>
<li>text8</li>
<li>corpus.txt</li>
<li>vectors.txt</li>
</ol>

<p>How can I used those files to load it as a <code>GloVe</code> model on python?</p>
",Training and Model Evaluation,train glove algorithm corpus tried follow wasted lot time ending nothing useful want train model corpus mb corpus txt file downloaded file provided link compiled using editing demo sh file changed leave unchanged output wa cooccurrence bin cooccurrence shuf bin text corpus txt vector txt used file load model python
Not able to do grid search and train the model,"<p>I am working on a basic text classification problem, I want to use a stacking classifier along with some fine-tuning of the parameters of my base classifiers to get high-accuracy results.</p>
<p>My dataset has 8000 rows and 2 cols (text and class). The below piece of code seems to be stuck and I am not well versed in the field (beginner) to spot the problem.</p>
<pre><code>import pandas as pd
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import NuSVC
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import accuracy_score, log_loss, classification_report, confusion_matrix

# Define parameter grids for classifiers
param_grid_nusvc = {
    'nu': [0.1, 0.3, 0.5, 0.7, 0.9],
    'kernel': ['linear', 'rbf'],
}

param_grid_logreg = {
    'C': [0.1, 1, 10],
    'penalty': ['l1', 'l2'],
}

# Perform grid search for classifiers with improved clarity
nusvc_grid_search = GridSearchCV(NuSVC(probability=True), param_grid_nusvc, cv=2, scoring='accuracy')  # Use accuracy scoring
logreg_grid_search = GridSearchCV(LogisticRegression(), param_grid_logreg, cv=2, scoring='accuracy')

nusvc_grid_search.fit(X_train, y_train)
logreg_grid_search.fit(X_train, y_train)

# Get best parameters
best_params_nusvc = nusvc_grid_search.best_params_
best_params_logreg = logreg_grid_search.best_params_

# Set up base classifiers with best parameters
best_nusvc = NuSVC(probability=True, **best_params_nusvc)
best_logreg = LogisticRegression(**best_params_logreg)

# Setting up stacking classifier
sc = StackingClassifier(
    estimators=[
        ('NuSVC', best_nusvc),
        ('LDA', LinearDiscriminantAnalysis())
    ],
    final_estimator=best_logreg
)

sc.fit(X_train, y_train)

# Evaluate the combined classifiers
print('****Results****')
train_predictions = sc.predict(X_test)
acc = accuracy_score(y_test, train_predictions)
print(&quot;Accuracy: {:.4%}&quot;.format(acc))

train_predictions_proba = sc.predict_proba(X_test)
ll = log_loss(y_test, train_predictions_proba)
print(&quot;Log Loss: {}&quot;.format(ll))

# Print classification report (optional)
print('\nClassification Report:')
print(classification_report(y_test, train_predictions))

# Print confusion matrix (optional)
print('\nConfusion Matrix:')
print(confusion_matrix(y_test, train_predictions))
</code></pre>
<p>some changes in the above have been made from the advice from chatGPT to guide me on how to fine tune using grid search. The code seems to be stuck (about 20 mins). Without the grid search it seemed to run in around 2-3 mins easily.</p>
",Training and Model Evaluation,able grid search train model working basic text classification problem want use stacking classifier along fine tuning parameter base classifier get high accuracy result dataset ha row col text class piece code seems stuck well versed field beginner spot problem change made advice chatgpt guide fine tune using grid search code seems stuck min without grid search seemed run around min easily
Error When Fitting Model in TensorFlow: logits and labels must have the same first dimension,"<p>I am new to using TensorFlow, but I am trying to build an LSTM Model using tokenized strings as inputs.</p>
<pre><code># Define the model
def build_lstm_model():
    input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')
    
    # BERT embedding layer
    bert_model = TFBertModel.from_pretrained('bert-base-uncased')
    bert_output = bert_model(input_ids)[1]  # using the pooled output

    # LSTM layer
    lstm_output = tf.keras.layers.LSTM(64)(tf.expand_dims(bert_output, axis=1))  # Expand the dimensions for LSTM

    # Output layer
    output = tf.keras.layers.Dense(1, activation='softmax')(lstm_output)

    model = tf.keras.Model(inputs=input_ids, outputs=output)
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Train the model
model = build_lstm_model()
#model.summary()

model.fit(train_dataset, validation_data=val_dataset, epochs=3)
</code></pre>
<p>Trying to fit this model returns the error:</p>
<blockquote>
<p>logits and labels must have the same first dimension, got logits shape [8,1] and labels shape [1024]</p>
</blockquote>
<p>More notes:
'train_dataset' and 'val_dataset' are both TensorFlow datasets with shape=(None,128).</p>
<p>Here is the output of <strong>model.summary()</strong>:</p>
<pre><code>Model: &quot;model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_ids (InputLayer)      [(None, 128)]             0         
                                                                 
 tf_bert_model (TFBertModel  TFBaseModelOutputWithPo   109482240 
 )                           olingAndCrossAttentions             
                             (last_hidden_state=(Non             
                             e, 128, 768),                       
                              pooler_output=(None, 7             
                             68),                                
                              past_key_values=None,              
                             hidden_states=None, att             
                             entions=None, cross_att             
                             entions=None)                       
                                                                 
 tf.expand_dims (TFOpLambda  (None, 1, 768)            0         
 )                                                               
                                                                 
 lstm (LSTM)                 (None, 64)                213248    
                                                                 
 dense (Dense)               (None, 1)                 65        
                                                                 
=================================================================
Total params: 109695553 (418.46 MB)
Trainable params: 109695553 (418.46 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</code></pre>
<p>I can't figure out how to get the dimensions to match so I can start training my model.</p>
<p>I tried the following solutions after reading other StackOverflow posts:</p>
<ul>
<li>Flattening the LSTM layer</li>
</ul>
<pre><code>lstm_output = tf.keras.layers.Flatten()(lstm_output)
</code></pre>
<ul>
<li>Changing the output shape of the LSTM layer and Dense layer.</li>
<li>Changing the loss function to 'categorical_crossentropy'.</li>
<li>Changing the output layer activation to 'sigmoid'.</li>
</ul>
",Training and Model Evaluation,error fitting model tensorflow logits label must first dimension new using tensorflow trying build lstm model using tokenized string input trying fit model return error logits label must first dimension got logits shape label shape note train dataset val dataset tensorflow datasets shape none output model summary figure get dimension match start training model tried following solution reading stackoverflow post flattening lstm layer changing output shape lstm layer dense layer changing loss function categorical crossentropy changing output layer activation sigmoid
Pre-training or using the existing model of FastText?,"<p>I am planning to create a classification model. Instead of using traditional models, I decided to use a new technique of creating word embeddings, clustering them using k-means, then use the mean of each cluster for comparision with the input(s). I decided to use fasttext as it supports subwords. I also have a large unsupervised text data. I would like to know if I should train the fasttext model with the data I have or I can go with the pre-trained model. If I should train, what are the benefits? Can someone explain me please</p>
",Training and Model Evaluation,pre training using existing model fasttext planning create classification model instead using traditional model decided use new technique creating word embeddings clustering using k mean use mean cluster comparision input decided use fasttext support subwords also large unsupervised text data would like know train fasttext model data go pre trained model train benefit someone explain please
Fine tuning Sentence transformers for semantic product search task,"<p>Problem I have at hand is to build a product suggestion model which suggest products based on the context of the search query of a user. My plan is to get a pre-trained model from the sentence-transformers pre-trained models and embed product descriptions using that and then get a list of top k most semantically similar products to the encoded query which user typed using the same model. I want to know how should I prepare my dataset to finetune the model to the product domain (e-commerce). Data set I have is called the amazon-esci dataset which has products relevant to search queries by users, labelled.</p>
<p>I tried to directly use the pre-trained model. But it suggest irrelevant products. I would like to know how to fine-tune the model.</p>
",Training and Model Evaluation,fine tuning sentence transformer semantic product search task problem hand build product suggestion model suggest product based context search query user plan get pre trained model sentence transformer pre trained model embed product description using get list top k semantically similar product encoded query user typed using model want know prepare dataset finetune model product domain e commerce data set called amazon esci dataset ha product relevant search query user labelled tried directly use pre trained model suggest irrelevant product would like know fine tune model
How to apply a linear layer atop a sentence transformer,"<p>Hey there I am trying to create a basic Sentence Transformer model for few shot learning, however while fitting I observed that the changes made to the model are miniscule because the model has been trained on 1B+ pairs whereas I train it on around 40 pairs per epochs, to deal with this problem I decided to apply a linear layer on top of the sentence transformer in order to learn the embeddings corresponding to a specific data set.
However there seems to be no forward function for the sentence transformers. Their is an alternative with the model.encode() method but it does not change the model parameters.
So summarizing I want to create a network that does a forward pass on the sentence transformer, then on the linear layer and then finally get a loss which can be used across the model.
Any help would be useful.
Thank you.</p>
",Training and Model Evaluation,apply linear layer atop sentence transformer hey trying create basic sentence transformer model shot learning however fitting observed change made model miniscule model ha trained b pair whereas train around pair per epoch deal problem decided apply linear layer top sentence transformer order learn embeddings corresponding specific data set however seems forward function sentence transformer alternative model encode method doe change model parameter summarizing want create network doe forward pas sentence transformer linear layer finally get loss used across model help would useful thank
How to Handle Imbalance Dataset in NER?,"<p>I'm now doing information extraction using NER. My dataset domain (mostly) in computer science. It contains label/tag: &quot;TUJUAN&quot;, &quot;METODE&quot;, and &quot;TEMUAN&quot;. The problem is almost 80-90% data are labeled O which means it has no meaningful tag. The precision and recall from the model is 0, while the accuracy is about 0.78. I use IndoBERT as model for NER task.</p>
<p><a href=""https://i.sstatic.net/ThRbD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ThRbD.png"" alt=""enter image description here"" /></a></p>
<p>I suspect this happens because my dataset is extremely unbalanced. At first, I want to modify the loss function based on BertForTokenClassification <a href=""https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#bertfortokenclassification"" rel=""nofollow noreferrer"">documentation</a> to Dice Loss or Focal Loss as it mentioned <a href=""https://www.reddit.com/r/learnmachinelearning/comments/uulc5q/imbalanced_dataset_during_named_entity_recognition/"" rel=""nofollow noreferrer"">here</a> but I don't know how since my Python knowledge is still very weak.</p>
<pre><code>class BertForTokenClassification(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels

        self.bert = BertModel(config, add_pooling_layer=False)
        classifier_dropout = (
            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob
        )
        self.dropout = nn.Dropout(classifier_dropout)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

        # Initialize weights and apply final processing
        self.post_init()

    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(&quot;batch_size, sequence_length&quot;))
    @add_code_sample_docstrings(
        checkpoint=_CHECKPOINT_FOR_TOKEN_CLASSIFICATION,
        output_type=TokenClassifierOutput,
        config_class=_CONFIG_FOR_DOC,
        expected_output=_TOKEN_CLASS_EXPECTED_OUTPUT,
        expected_loss=_TOKEN_CLASS_EXPECTED_LOSS,
    )
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        token_type_ids: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        inputs_embeds: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -&gt; Union[Tuple[torch.Tensor], TokenClassifierOutput]:
        r&quot;&quot;&quot;
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.
        &quot;&quot;&quot;
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        sequence_output = outputs[0]

        sequence_output = self.dropout(sequence_output)
        logits = self.classifier(sequence_output)

        loss = None
        if labels is not None:
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

        if not return_dict:
            output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return TokenClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

</code></pre>
<p>My full code is <a href=""https://colab.research.google.com/drive/1yUGdIiPuB0-JnojCxbLnrRPAsv4HjZ2O?usp=sharing"" rel=""nofollow noreferrer"">here</a></p>
<p>Can I get any help how to handle my imbalance dataset based on my problems?</p>
",Training and Model Evaluation,handle imbalance dataset ner information extraction using ner dataset domain mostly computer science contains label tag tujuan metode temuan problem almost data labeled mean ha meaningful tag precision recall model accuracy use indobert model ner task suspect happens dataset extremely unbalanced first want modify loss function based bertfortokenclassification documentation dice loss focal loss mentioned know since python knowledge still weak full code get help handle imbalance dataset based problem
How to return history of validation loss in Keras,"<p>Using Anaconda Python 2.7 Windows 10.</p>

<p>I am training a language model using the Keras exmaple:</p>

<pre><code>print('Build model...')
model = Sequential()
model.add(GRU(512, return_sequences=True, input_shape=(maxlen, len(chars))))
model.add(Dropout(0.2))
model.add(GRU(512, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(len(chars)))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

def sample(a, temperature=1.0):
    # helper function to sample an index from a probability array
    a = np.log(a) / temperature
    a = np.exp(a) / np.sum(np.exp(a))
    return np.argmax(np.random.multinomial(1, a, 1))


# train the model, output generated text after each iteration
for iteration in range(1, 3):
    print()
    print('-' * 50)
    print('Iteration', iteration)
    model.fit(X, y, batch_size=128, nb_epoch=1)
    start_index = random.randint(0, len(text) - maxlen - 1)

    for diversity in [0.2, 0.5, 1.0, 1.2]:
        print()
        print('----- diversity:', diversity)

        generated = ''
        sentence = text[start_index: start_index + maxlen]
        generated += sentence
        print('----- Generating with seed: ""' + sentence + '""')
        sys.stdout.write(generated)

        for i in range(400):
            x = np.zeros((1, maxlen, len(chars)))
            for t, char in enumerate(sentence):
                x[0, t, char_indices[char]] = 1.

            preds = model.predict(x, verbose=0)[0]
            next_index = sample(preds, diversity)
            next_char = indices_char[next_index]

            generated += next_char
            sentence = sentence[1:] + next_char

            sys.stdout.write(next_char)
            sys.stdout.flush()
        print()
</code></pre>

<p>According to Keras documentation, the <code>model.fit</code> method returns a History callback, which has a history attribute containing the lists of successive losses and other metrics.</p>

<pre><code>hist = model.fit(X, y, validation_split=0.2)
print(hist.history)
</code></pre>

<p>After training my model, if I run <code>print(model.history)</code> I get the error:</p>

<pre><code> AttributeError: 'Sequential' object has no attribute 'history'
</code></pre>

<p>How do I return my model history after training my model with the above code?</p>

<p><strong>UPDATE</strong></p>

<p>The issue was that:</p>

<p>The following had to first be defined:</p>

<pre><code>from keras.callbacks import History 
history = History()
</code></pre>

<p>The callbacks option had to be called</p>

<pre><code>model.fit(X_train, Y_train, nb_epoch=5, batch_size=16, callbacks=[history])
</code></pre>

<p>But now if I print</p>

<pre><code>print(history.History)
</code></pre>

<p>it returns</p>

<pre><code>{}
</code></pre>

<p>even though I ran an iteration. </p>
",Training and Model Evaluation,return history validation loss kera using anaconda python window training language model using kera exmaple according kera documentation method return history callback ha history attribute containing list successive loss metric training model run get error return model history training model code update issue wa following first defined callback option called print return even though ran iteration
What are the obstacles in today&#39;s object detection?,"<p>I have read papers about faster RCNN and RFCN, also read YOLO. It seems the biggest problem is the speed? And all of them use image data data only. Are there any models that combines text and image data? Which means we can use the information from text to help detection when the training data is small. For example, when the training data is small, the model cannot tell dogs and cats clearly, but the model could tell there is a bone near that object, and the model gets some information from text that the object near a bone is most likely a dog, thus the model now could tell what the object is. Does this kind of algorithm exist?</p>
",Training and Model Evaluation,obstacle today object detection read paper faster rcnn rfcn also read yolo seems biggest problem speed use image data data model combine text image data mean use information text help detection training data small example training data small model tell dog cat clearly model could tell bone near object model get information text object near bone likely dog thus model could tell object doe kind algorithm exist
Weird results in the test of Token Classification model - CamemBERT,"<p>I am working on a NER task using the <a href=""https://huggingface.co/camembert-base"" rel=""nofollow noreferrer"">camembert-base</a> model and Pytorch for the fine-tuning, the obtained model is not giving good results for now but at least the labels of some sentences from the training corpus are predicted correctly.</p>
<p>The training is done using this <a href=""https://drive.google.com/file/d/1Odq6eTexLg9ZXCjbWbnZiWMGAqg9Ut45/view?usp=sharing"" rel=""nofollow noreferrer"">dataset</a>, more details can be found in this <a href=""https://colab.research.google.com/drive/1I7cFPaQNWjV_jT3_FJ9LtcxEfAwNO5a3?usp=sharing"" rel=""nofollow noreferrer"">notebook</a>.</p>
<p>The weird thing about this model is that in the test phase, some labels got a null precision, recall, and f1 score! This is pretty weird as it I think it means that the model didn’t manage to predict any entities for that label, a thing that I think is not possible as a model doing random shots can predict at least one label correctly.
I tried to test the model on the training data, and got the following results, I don't understand why this is happening with data that the model has already seen.</p>
<pre><code>{'age': {'f1': 0.9339622641509434,
         'number': 420,
         'precision': 0.9252336448598131,
         'recall': 0.9428571428571428},
 'anatomie': {'f1': 0.0, 'number': 1070, 'precision': 0.0, 'recall': 0.0},
 'date': {'f1': 0.0, 'number': 38, 'precision': 0.0, 'recall': 0.0},
 'dose': {'f1': 0.0, 'number': 102, 'precision': 0.0, 'recall': 0.0},
 'duree': {'f1': 0.0, 'number': 105, 'precision': 0.0, 'recall': 0.0},
 'examen': {'f1': 0.0, 'number': 721, 'precision': 0.0, 'recall': 0.0},
 'frequence': {'f1': 0.0, 'number': 77, 'precision': 0.0, 'recall': 0.0},
 'genre': {'f1': 0.5926748057713652,
           'number': 426,
           'precision': 0.5621052631578948,
           'recall': 0.6267605633802817},
 'issue': {'f1': 0.18621973929236502,
           'number': 285,
           'precision': 0.1984126984126984,
           'recall': 0.17543859649122806},
 'mode': {'f1': 0.0, 'number': 77, 'precision': 0.0, 'recall': 0.0},
 'moment': {'f1': 0.0, 'number': 174, 'precision': 0.0, 'recall': 0.0},
 'origine': {'f1': 0.49336283185840707,
             'number': 426,
             'precision': 0.4665271966527197,
             'recall': 0.5234741784037559},
 'overall_accuracy': 0.8862863686790967,
 'overall_f1': 0.21902729417050434,
 'overall_precision': 0.37072243346007605,
 'overall_recall': 0.15542802486848398,
 'pathologie': {'f1': 0.0, 'number': 211, 'precision': 0.0, 'recall': 0.0},
 'sosy': {'f1': 0.03614457831325302,
          'number': 1161,
          'precision': 0.03911735205616851,
          'recall': 0.03359173126614987},
 'substance': {'f1': 0.0, 'number': 371, 'precision': 0.0, 'recall': 0.0},
 'traitement': {'f1': 0.0, 'number': 254, 'precision': 0.0, 'recall': 0.0},
 'valeur': {'f1': 0.0, 'number': 355, 'precision': 0.0, 'recall': 0.0}}
</code></pre>
<p>Any clues to fix this issue, is it possible that this problem comes from the frequency of the labels used in the dataset? or could it be something else? And what can I do to fix?</p>
",Training and Model Evaluation,weird result test token classification model camembert working ner task using camembert base model pytorch fine tuning obtained model giving good result least label sentence training corpus predicted correctly training done using dataset detail found notebook weird thing model test phase label got null precision recall f score pretty weird think mean model manage predict entity label thing think possible model random shot predict least one label correctly tried test model training data got following result understand happening data model ha already seen clue fix issue possible problem come frequency label used dataset could something else fix
BERT encoding layer produces same output for all inputs during evaluation (PyTorch),"<p>I don't understand why my BERT model returns the same output during evaluation. The output of my model during training seems correct, as the values were different, but is totally the same during evaluation.
<a href=""https://i.sstatic.net/Ry8SY.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ry8SY.jpg"" alt=""Output during evaluation""></a></p>

<p>Here is my BERT model class </p>

<pre><code>class BERTBaseUncased(nn.Module):
    def __init__(self):
        super(BERTBaseUncased, self).__init__()
        self.bert = BertModel.from_pretrained(""bert-base-uncased"")
        self.bert_drop = nn.Dropout(0.3)
        self.out = nn.Linear(768, 4)

    def forward(self, ids, mask, token_type_ids):
        _, o2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids) # Use one of the outputs
        bo = self.bert_drop(o2)
        return self.out(bo)
</code></pre>

<p>My dataset class</p>

<pre><code>class BERTDataset:
    def __init__(self, review, target, tokenizer, classes=4):
        self.review = review
        self.target = target
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.classes = classes

    def __len__(self):
        return len(self.review)

    def __getitem__(self, item):
        review = str(self.review)
        review = "" "".join(review.split())

        inputs = self.tokenizer.encode_plus(review, None, add_special_tokens=True, max_length= self.max_len,
                                            pad_to_max_length=True, return_token_type_ids=True,
                                            return_attention_masks=True)

        ids = inputs[""input_ids""]
        mask = inputs[""attention_mask""]
        token_type_ids = inputs[""token_type_ids""]

        return {
            'ids': torch.tensor(ids, dtype=torch.long),
            'mask': torch.tensor(mask, dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
            'targets': torch.tensor(to_categorical(self.target[item], self.classes), dtype=torch.float)
        }
</code></pre>

<p>My evaluation function</p>

<pre><code>def eval_fn(data_loader, model, device):
    model.eval()

    total_loss = 0.0

    with torch.no_grad():
        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):
            ids = d['ids']
            token_type_ids = d['token_type_ids']
            mask = d['mask']
            targets = d['targets']

            ids = ids.to(device, dtype=torch.long)
            token_type_ids = token_type_ids.to(device, dtype=torch.long)
            mask = mask.to(device, dtype=torch.long)
            targets = targets.to(device, dtype=torch.float)

            outputs = model(
                ids=ids,
                mask=mask,
                token_type_ids=token_type_ids
            )

            loss = loss_fn(outputs, targets)
            total_loss += loss.item()
</code></pre>

<p>And my training function</p>

<pre><code>def train_fn(data_loader, model, optimizer, device, scheduler):
    model.train()

    total_loss = 0.0

    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):
        ids = d['ids']
        token_type_ids = d['token_type_ids']
        mask = d['mask']
        targets = d['targets']

        ids = ids.to(device, dtype=torch.long)
        token_type_ids = token_type_ids.to(device, dtype=torch.long)
        mask = mask.to(device, dtype=torch.long)
        targets = targets.to(device, dtype=torch.float)

        optimizer.zero_grad()

        outputs = model(
            ids=ids,
            mask=mask,
            token_type_ids=token_type_ids
        )

        loss = loss_fn(outputs, targets)
        total_loss += loss.item()
        loss.backward()

        optimizer.step()
        scheduler.step()

    return total_loss/len(data_loader)
</code></pre>

<p>Thanks!</p>
",Training and Model Evaluation,bert encoding layer produce output input evaluation pytorch understand bert model return output evaluation output model training seems correct value different totally evaluation bert model class dataset class evaluation function training function thanks
How to Export Gensim Word2Vec Model with Ngram Weights for DL4J?,"<p>I'm quite new to nlp. I'm trying to use a model trained with gensim in dl4j. I'm saving the model with</p>
<pre><code>w2v_model.wv.save_word2vec_format(&quot;path/to/w2v_model.bin&quot;, binary=True)
</code></pre>
<p>and afterwards I'm loading it with</p>
<pre><code>Word2Vec w2vModel = WordVectorSerializer.readWord2VecModel(&quot;path/to/w2v_model.bin&quot;); 
</code></pre>
<p>The model works well except for the handling of out-of-vocabulary (OOV) words. In Gensim, it seems to calculate vectors for OOV words based on the word's n-grams, but in DL4J, it provides an empty vector for them.</p>
<p>My questions are:</p>
<ol>
<li>Is there a way to export the n-gram weights along with the model from Gensim so that DL4J can use them?</li>
<li>If exporting the n-gram weights is not possible, is there a method to reconstruct them on the DL4J side to achieve similar results for OOV words as in Gensim?</li>
</ol>
<p>Any guidance or suggestions would be greatly appreciated</p>
",Training and Model Evaluation,export gensim word vec model ngram weight dl j quite new nlp trying use model trained gensim dl j saving model afterwards loading model work well except handling vocabulary oov word gensim seems calculate vector oov word based word n gram dl j provides empty vector question way export n gram weight along model gensim dl j use exporting n gram weight possible method reconstruct dl j side achieve similar result oov word gensim guidance suggestion would greatly appreciated
Implementing tensorflow 1d cross-correlation similarity loss,"<p>I’m looking to add a loss to my NLP model which regularizes it such that the output confidences over the vocabulary are structurally similar to the input one-hot sequences, but allowing for re-arrangement of the position of the tokens.
Can anyone recommend the right way to implement this using tensorflow’s vectorized convolution ops?</p>
<p>pseudocode looks something like this:</p>
<pre><code>originals = Tensor(shape=(batch_size, sequence_length, vocabulary_size) #one-hot inputs
confidences = Tensor(shape=(batch_size, sequence_length, vocabulary_size) #output from model

#join sequence into one dimension
#reshape to (batch_size, sequence_length * vocabulary-size)
originals_flat = tf.reshape(originals, (batch_size, -1)
confidences_flat = tf.reshape(confidences, (batch_size, -1)

similarity_scores = []
for one_hot, conf in zip(originals_flat, confidences_flat):
# “sliding window” comparison, see cross-correlation
cross_corr = np.correlate(conf, one_hot, mode=“full”)

max_corr = np.max(cross_corr)

# Normalize the maximum correlation value
similarity = (
    2
    * max_corr
    / (np.sum(one_hot) + np.sum(conf) + tf.keras.backend.epsilon())
)

similarity_scores.append(similarity)
</code></pre>
<p>Which gets me the correct result i’m looking for. however, I’d like to remove the loop and instead implement it on the original batched tensor using tf.nn.convolution (or conv1d, conv2d).Unfortunately, no amount of fiddling with the shapes or strides manages to get me the values or even output shapes i’m looking for when a batch dimension is included.
I managed to get it working by using tf.map_fn, but this in essence is just re-creating the loop I’m trying to pass down to the actual conv1d vectorized code. The revised tensorflow version is actually slower than the numpy version with the for loop.</p>
<pre><code>
originals = tf.constant([[[0, 1, 0], [0, 1, 0], [1, 0, 0]],
[[1, 0, 0], [0, 0, 1], [0, 1, 0]]], dtype=tf.float32)
reconstructions = tf.constant([[[0, 1, 0], [0, 1, 0], [1, 0, 0]],
[[1, 0.5, 0], [0, 0.5, 0.5], [1, 0, 0]]], dtype=tf.float32)

reshape from (batch_size, seq_len, vocab_size) to (batch_size, seq_len * vocab_size)
confidences_flat = tf.reshape(reconstructions, (reconstructions.shape[0], -1))
one_hot_sequences_flat = tf.reshape(originals, (originals.shape[0], -1))

@tf.function()
def _tf_1d_cross_correlation(args):
originals, reconstructions = args

data = tf.reshape(originals, [1, -1, 1], name='data')
kernel = tf.reshape(reconstructions, [-1, 1, 1], name='kernel')

#cross-correlate
res = tf.squeeze(tf.nn.conv1d(data, kernel, 1, 'VALID'))

#normalize
similarity = 2 * res / (tf.reduce_sum(originals) + tf.reduce_sum(reconstructions) + tf.keras.backend.epsilon())

return similarity
Apply the function to each pair of sequences
similarities = tf.map_fn(_tf_1d_cross_correlation, (one_hot_sequences_flat, confidences_flat), dtype=tf.float32)

print(similarities)

</code></pre>
<p>result: tf.Tensor([1. 0.46153846], shape=(2,), dtype=float32)</p>
",Training and Model Evaluation,implementing tensorflow cross correlation similarity loss looking add loss nlp model regularizes output confidence vocabulary structurally similar input one hot sequence allowing arrangement position token anyone recommend right way implement using tensorflow vectorized convolution ops pseudocode look something like get correct result looking however like remove loop instead implement original batched tensor using tf nn convolution conv conv unfortunately amount fiddling shape stride manages get value even output shape looking batch dimension included managed get working using tf map fn essence creating loop trying pas actual conv vectorized code revised tensorflow version actually slower numpy version loop result tf tensor shape dtype float
How to deal with large vocab_size when training a Language Model in Keras?,"<p>I want to train a language model in Keras, by this tutorial:
<a href=""https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/"" rel=""nofollow noreferrer"">https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/</a></p>

<p>My input is composed of:
lines num: 4823744
maximum line: 20
Vocabulary Size: 790609
Total Sequences: 2172328
Max Sequence Length: 11</p>

<p>As you can see by this lines: </p>

<pre><code>num_words = 50
tokenizer = Tokenizer(num_words=num_words, lower=True)
tokenizer.fit_on_texts([data])
# determine the vocabulary size
vocab_size = len(tokenizer.word_index) + 1
</code></pre>

<p>I'm using the tokenizer with num_words=50. 
The vocab_size is taken from the tokenizer, but it's still the bigger size (790K).</p>

<p>Therefore this line:</p>

<pre><code>y = to_categorical(y, num_classes=vocab_size)
</code></pre>

<p>Causes a memory error. </p>

<p>This is the model definition: </p>

<pre><code>model = Sequential()
model.add(Embedding(vocab_size, 10, input_length=max_length-1))
model.add(LSTM(50))
model.add(Dense(vocab_size, activation='softmax'))
</code></pre>

<p>How can I deal with it?</p>

<p>I do want to have word-level model and not char-level. 
And I do want to take at least 10K of the most common words. </p>

<p>I thought about filtering words before hand, but it may cause the language model to learn false sequences. </p>

<p>How can I solve it? </p>

<p>Thanks</p>
",Training and Model Evaluation,deal large vocab size training language model kera want train language model kera tutorial input composed line num maximum line vocabulary size total sequence max sequence length see line using tokenizer num word vocab size taken tokenizer still bigger size k therefore line cause memory error model definition deal want word level model char level want take least k common word thought filtering word hand may cause language model learn false sequence solve thanks
Stanford Stanza sometimes splits a sentence into two sentences,"<p>I am using <a href=""https://github.com/stanfordnlp/stanza"" rel=""nofollow noreferrer"">stanza</a> 1.6.1. I have been experimenting with Stanza's constituency parser.</p>
<p>In certain cases it splits a sentence into 2 <strong>Sentence</strong> objects. For example, take this sentence : <em>Pull up Field with low precision</em>.</p>
<p>It splits it into 2 sentences internally (<strong>Pull up</strong> and <strong>Field with low precision</strong>) and so the constituency parser output comes out as 2 trees (one for each sentence).</p>
<p>Changing &quot;Field&quot; to lowercase in above sentence makes Stanza treat it as one sentence and I get one tree representation (as expected) as constituency output.</p>
<p>Is there some way to make Stanza consider this as one sentence apart from string manipulation techniques like converting to lowercase?
Or is there a case insensitive model that I could use?</p>
",Training and Model Evaluation,stanford stanza sometimes split sentence two sentence using stanza experimenting stanza constituency parser certain case split sentence sentence object example take sentence pull field low precision split sentence internally pull field low precision constituency parser output come tree one sentence changing field lowercase sentence make stanza treat one sentence get one tree representation expected constituency output way make stanza consider one sentence apart string manipulation technique like converting lowercase case insensitive model could use
What is the best way to scale up Gensim Doc2Vec training?,"<p>I have one million documents.  I chunk those documents up into 20 million paragraphs (each paragraph is about 50 words) to train a Doc2Vec model with.  It currently takes about 20 hours to train the model with these paragraphs and I would like to speed this up (it only takes 15 mins to build the vocabulary).  Gensim never uses more than 4 cores during training (no matter how many cores I have available).  I know there are issues with the Python GIL that may be the cause of this.</p>
<p>Here's the code I use to train the Doc2Vec model:</p>
<pre><code>model = Doc2Vec(vector_size=50, min_count=3, epochs=40, workers=24, dm=0, window=3, dbow_words=0, sample=0.00010, negative=15, ns_exponent=0.75)
model.build_vocab(training_corpus)
model.train(training_corpus, total_examples=model.corpus_count, epochs=40)
</code></pre>
<p>Is there a way to distribute this across multiple machines so I can train the model more quickly?</p>
",Training and Model Evaluation,best way scale gensim doc vec training one million document chunk document million paragraph paragraph word train doc vec model currently take hour train model paragraph would like speed take min build vocabulary gensim never us core training matter many core available know issue python gil may cause code use train doc vec model way distribute across multiple machine train model quickly
How to reduce Computational Time for a small dataset?,"<p>I've been working on text detection as part of my research, focusing on various characteristics. I'm encountering a significant issue with computational time. My dataset consists of 3400 rows and 2 columns. I'm using a BERT model to train and evaluate the dataset. Currently, I'm utilizing Colab Pro as I don't have access to a GPU. Initially, for 50 epochs, it took approximately 65 hours for computational processing. I then reduced it to 20 epochs, but it's still taking an unexpectedly long time—around 24 hours—for a dataset containing only 3400 texts</p>
<p>Training Arguments:</p>
<pre><code>training_args = TrainingArguments(
    output_dir='./output',
    do_train=True,
    do_eval=True,
    num_train_epochs=20,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=16,
    warmup_steps=100,
    weight_decay=0.05,
    logging_strategy='steps',
    logging_dir='./multi-class-logs',
    logging_steps=50,
    evaluation_strategy=&quot;steps&quot;,
    eval_steps=50,
    save_strategy=&quot;steps&quot;,
    load_best_model_at_end=True
)
</code></pre>
<p>Is there anything here that if changed then my computation might get faster?
I am expecting changes in my code that will faster the computational time.</p>
",Training and Model Evaluation,reduce computational time small dataset working text detection part research focusing various characteristic encountering significant issue computational time dataset consists row column using bert model train evaluate dataset currently utilizing colab pro access gpu initially epoch took approximately hour computational processing reduced epoch still taking unexpectedly long time around hour dataset containing text training argument anything changed computation might get faster expecting change code faster computational time
Use nn.transformerEncoder for context-free grammar parsing (sequence classification),"<p>I want to use a transformer to do context-free grammar parsing (to classify whether a sequence is in the grammar or not). The input are sequences like &quot;abbaba&quot;, the output is 0 or 1. Here's my transformer architecture:</p>
<pre><code>class PositionalEncoding(nn.Module):

    def __init__(self, d_model, max_len=200, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float()
            * (-math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer(&quot;pe&quot;, pe)

    def forward(self, x):
        # print(self.pe.shape, x.shape)
        x = x + self.pe[:, : x.size(1), :]
        return self.dropout(x)


class TransformerClassifier(nn.Module):
    def __init__(self, input_size, d_model, num_classes, num_layers, nhead, dim_feedforward, dropout):
        super(TransformerClassifier, self).__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(input_size, d_model)

        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)

        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)

        self.pos_encoder = PositionalEncoding(d_model=d_model, dropout=dropout)
        
        self.linear = nn.Linear(d_model, num_classes)

    def forward(self, x):
        x = self.embedding(x) * math.sqrt(self.d_model)
        x = self.pos_encoder(x)
        x = self.transformer_encoder(x)
        x = x.mean(dim=1)  # Aggregate across the sequence dimension
        x = self.linear(x)
        return x
</code></pre>
<p>I set the hyperparameters to:</p>
<pre><code>input_size = 27 # 26 letters + &lt;pad&gt;
d_model = 50
dim_feedforward = 50
nhead = 5
num_classes = 2  # Binary classification (0 or 1)
num_layers = 4
dropout = 0.1
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
</code></pre>
<p>The lr was set to 1e-4, batch size was 20.
The training loop:</p>
<pre><code>epochs = 100
epoch_losses = []
for epoch in range(epochs):
    batch_loss = []
    for inputs, targets in tqdm(train_dataloader):
        inputs = inputs.to(device)
        targets = targets.to(device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        batch_loss.append(loss.item())

    epoch_loss = sum(batch_loss) / len(batch_loss)
    epoch_losses.append(sum(batch_loss) / len(batch_loss))
    print(f&quot;Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}&quot;)
</code></pre>
<p>However the result seems to be abnormal.
The training curve seems to be a bit noisy:
<a href=""https://i.sstatic.net/ABcNy.png"" rel=""nofollow noreferrer""></a>
Also, even if I use a very large transformer (large values for d_model, nhead, num_layer, etc), the accuracy on the training set is about 0.7-0.8 and don't go higher. Also, sometimes the accuracy on the test set was even higher than the training set. Additionally, I want to test how the transformer perform on OOD data (I set OOD here to be longer sequences, and only train the model with shorter sequences), I found that the accuracy on OOD test set was sometimes higher than ID (in distribution) test set, and sometimes even higher than training set.
<a href=""https://i.sstatic.net/AKEyA.png"" rel=""nofollow noreferrer""></a>
I was very confused and was wondering what's wrong.</p>
<p>I want to know if there's anything wrong with my codes or are there any other mistakes.</p>
",Training and Model Evaluation,use nn transformerencoder context free grammar parsing sequence classification want use transformer context free grammar parsing classify whether sequence grammar input sequence like abbaba output transformer architecture set hyperparameters lr wa set e batch size wa training loop however result seems abnormal training curve seems bit noisy also even use large transformer large value model nhead num layer etc accuracy training set go higher also sometimes accuracy test set wa even higher training set additionally want test transformer perform ood data set ood longer sequence train model shorter sequence found accuracy ood test set wa sometimes higher id distribution test set sometimes even higher training set wa confused wa wondering wrong want know anything wrong code mistake
Incorrect Predictions from Tensorflow Spelling Correction Model,"<p>I trained a Tensorflow model for spelling correction. I trained for &gt;60 epochs and achieved an accuracy of ~82.2% with a loss of 0.3032. When I try to make predictions with the model, it didn't provide any correct predictions. The model is trained with &gt;100k sentences of which about 20% have spelling errors. The output is binary data (not one-hot encoded). The model didn't provide correct predictions even with some of the training data. The model is as follows:</p>
<pre><code>def create_model():

  tf.random.set_seed(42)

  # input_layer = Input(shape=(x_train.shape[1],1),name='input_layer')#input layer for use without embedding
  input_layer = Input(shape=(x_train.shape[1]),name='input_layer')#input layer for use with embedding
  sp_embedding_layer = Embedding(len(char2idx),EMBEDDING_DIM,embeddings_initializer=initializers.Constant(embed_matrix),trainable=False)(input_layer)#embeddings_initializer=initializers.Constant(embed_matrix),

  x = Bidirectional(LSTM(1024,return_sequences = False))(sp_embedding_layer)
  x = Dense(1920, activation = 'relu',name='Dense_1')(x)
  x = Dense(2048, activation = 'relu',name='Dense_2')(x)
  x = Dense(2048, activation = 'relu',name='Dense_3')(x)
  x = Dense(2048, activation = 'relu',name='Dense_4')(x)
  x = Dense(2048, activation = 'relu',name='Dense_5')(x)
  x = Dense(1024, activation = 'relu',name='Dense_6')(x)
  x = Dense(1024, activation = 'relu',name='Dense_7')(x)
  x = Dense(y_train.shape[1],name='Output',activation = 'sigmoid')(x)
  model = models.Model(inputs = input_layer, outputs = x)

  return model
</code></pre>
<p>Optimiser and callbacks are as follows:</p>
<pre><code>sf = 4

spb = x_train.shape[0]/BATCH_SIZE #steps per batch
sf_epoch = spb * sf


myoptimizer = optimizers.Adam(learning_rate=0.000001)


filepath=&quot;/content/drive/MyDrive/NLP/Models/SCModels/weights.{epoch:02d}.tf&quot;
checkpoint = EpochModelCheckpoint(filepath,frequency=sf,save_weights_only=True)
# ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True,
#                              mode='max',save_freq = sf)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,verbose=1,
                              patience=2)

callbacks_list = [checkpoint,reduce_lr]
</code></pre>
<p>Training code:</p>
<pre><code>history = n_model.fit(x_train,y_train,validation_data=(x_val,y_val),epochs=60,
                    callbacks=callbacks_list,batch_size=BATCH_SIZE)
</code></pre>
<p>The prediction code is as follows:</p>
<pre><code>test_model = create_model()
test_model.load_weights(selected_model)
test_optimizer = optimizers.Adam(learning_rate=0.0983)
test_model.compile(optimizer=test_optimizer,loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

print('input:',sample_sent)
input = np.array(encode(sample_sent,MAX_SENT_LEN+10)).reshape((1,-1))
answer = test_model.predict(input)
answer_dec = np.where(answer&lt;0.5,0,1)
print(answer_dec[0].tolist())
print(decode(recover_int_seq(answer_dec[0].tolist())))
</code></pre>
<p>Why are the predictions off? I'd appreciate any help with this. Thanks.</p>
",Training and Model Evaluation,incorrect prediction tensorflow spelling correction model trained tensorflow model spelling correction trained epoch achieved accuracy loss try make prediction model provide correct prediction model trained k sentence spelling error output binary data one hot encoded model provide correct prediction even training data model follows optimiser callback follows training code prediction code follows prediction appreciate help thanks
Understanding the Calculation Process of BERT Model Loss in Sequence Classification,"<p>I am seeking clarification on the process flow for calculating the loss in a BERT model for sequence classification. I have tokenized data and a code snippet as follows:</p>
<pre><code>tokenized_data1= {'input_ids': array([[  101, 19278, 15091, ...,     0,     0,     0],
        [  101, 19278, 15091, ...,     0,     0,     0],
        [  101,  8084, 11318, ...,     0,     0,     0],
        ...,
        [  101,  7708, 14381, ...,     0,     0,     0],
        [  101,  5379,   787, ...,     0,     0,     0],
        [  101, 22327,  1987, ...,     0,     0,     0]]),
 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]]),
 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        ...,
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0]])}


labels1=[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
       0, 0, 0, 0, 0, 1, 0, 0, 0, 0]

from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;)
model.compile(optimizer=Adam(3e-5))
model.fit(tokenized_data1, labels1)

1/1 [==============================] - 40s 40s/step - loss: 0.5368
</code></pre>
<p>In the above, I am using a pretrained transformer model. The tokenized data has input_ids of shape 32, so while fitting the model, only one run gets executed, and the default batch size is 32.</p>
<p>During training, the loss value is reported as 0.5368. I am attempting to understand the steps leading to this specific loss value. My understanding so far involves obtaining the logits from the model and applying a cross-entropy loss function. However, my attempt yielded a different loss value (0.6769). Here is the snippet of my attempt:</p>
<pre><code># Obtaining logits
outputs = model(input_ids=tf.constant(tokenized_data1['input_ids']),
                attention_mask=tf.constant(tokenized_data1['attention_mask']))
logits = torch.Tensor(outputs.logits.numpy())
labels1=torch.tensor(labels1)

# Calculating loss using cross-entropy loss function
loss_fct = CrossEntropyLoss()
loss = loss_fct(logits.view(-1, 2), torch.tensor(labels1).view(-1))
</code></pre>
<p>The computed loss (0.6769) differs from the reported loss (0.5368).</p>
<p>I need guidance on understanding the correct process flow for obtaining the reported loss value and identifying any errors in my approach. Thank you.</p>
",Training and Model Evaluation,understanding calculation process bert model loss sequence classification seeking clarification process flow calculating loss bert model sequence classification tokenized data code snippet follows using pretrained transformer model tokenized data ha input id shape fitting model one run get executed default batch size training loss value reported attempting understand step leading specific loss value understanding far involves obtaining logits model applying cross entropy loss function however attempt yielded different loss value snippet attempt computed loss differs reported loss need guidance understanding correct process flow obtaining reported loss value identifying error approach thank
Conversational dialogue data,"<p>I'm trying out some ideas around the <a href=""https://arxiv.org/abs/1506.05869"" rel=""nofollow noreferrer"">Neural Conversational Model</a> and looking for some actual dialogue data as my training data to play around. But it seems such data is not easy to find on the Web, compared to text corpus or part-of-speech tagged data. Are there any public dialogue datasets out there that would be useful as the training data.</p>
<p>It possible, I'd like to use dialogue data in the context of banking, finance, or anything related, but any general dialogue data will be nice.</p>
",Training and Model Evaluation,conversational dialogue data trying idea around neural conversational model looking actual dialogue data training data play around seems data easy find web compared text corpus part speech tagged data public dialogue datasets would useful training data possible like use dialogue data context banking finance anything related general dialogue data nice
Logistic Regression gradually tends to predict all 0s while training on mini batches,"<p>I am using mini-batches to train my model which is as follows</p>
<pre><code>SimpleModel(
  (embed): Embedding(vocab_size, embedding_size, max_norm=2)
  (model): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=in_features, out_features=1, bias=True)
  )
  (sig): Sigmoid()
)
</code></pre>
<p>these are the specifics of the model and upon training through mini-batches, after 2-3 minibatches all the outputs become <code>0.</code> training function looks like this while the training loop is usual</p>
<pre class=""lang-py prettyprint-override""><code>def trainer(train_loader, model, optimizer, criterion):
    model.train()
    it_loss = 0
    counter = 0
    for data in train_loader:
        optimizer.zero_grad()
        msgs = data['msg']
        targets = data['target']
        out = model(msgs)
        print(out)
        loss = criterion(out, targets)
        loss.backward()
        optimizer.step()
        
        it_loss+=loss.item()*msgs.shape[0]
        counter+=msgs.shape[0]
        
    return it_loss/counter
</code></pre>
<p>I have tried using various optimizer and all those things, data is not imbalanced as shown</p>
<pre><code>0    3900
1    1896
Name: count, dtype: int64
</code></pre>
<p>what could be the possible reason and how can I solve it</p>
<p>Edit :</p>
<p>The output of first mini batch looks like</p>
<pre><code>tensor([[0.4578],
        [0.4569],
        [0.4686],
            .
            .
            .
        [0.4602],
        [0.4674],
        [0.4398]], grad_fn=&lt;SigmoidBackward0&gt;)
</code></pre>
<p>while output of 4th or 5th mini batch looks like</p>
<pre><code>tensor([[0.0057],
        [0.0058],
        [0.0058],
            .
            .
            .
        [0.0058],
        [0.0057],
        [0.0059]], grad_fn=&lt;SigmoidBackward0&gt;)
</code></pre>
<p>And furthermore it gradually becomes exact <code>0.</code></p>
",Training and Model Evaluation,logistic regression gradually tends predict training mini batch using mini batch train model follows specific model upon training mini batch minibatches output become training function look like training loop usual tried using various optimizer thing data imbalanced shown could possible reason solve edit output first mini batch look like output th th mini batch look like furthermore gradually becomes exact
LSTM for One Line Poem Generation from a Single Word? Help for Input Manipulation and Model Creation,"<pre><code>X = []
Y = []

for line in document:
  words = line.split()
  line_length = len(words)
  if line_length &gt; 1:  # Lines with 1 word or less are excluded
      input_sequence = [word_to_index.get(words[0], 0)]  # First word as input
      output_sequence = [word_to_index.get(word, 0) for word in words[1:max_sequence_length]]  # Remaining words as output
      # Pad shorter sequences with zeros
      while len(output_sequence) &lt; max_sequence_length - 1:
        output_sequence.append(0)
      X.append(input_sequence)
      Y.append(output_sequence)
</code></pre>
<p>Is this the correct way or? I just want my model to be able to create a poem of less than 10-12 words as each element in posted list of string, and I am trying to use NLP for it.
word_to_index is just assigning numbers to each words in document.
How can I do this? When I proceeded with this, it showed some error in dimension in loss computation.</p>
<p>Any further tips for this? How should I move forward to get the poem generator? <br />
<a href=""https://i.sstatic.net/ufG3x.png"" rel=""nofollow noreferrer"">The dataset looks like this</a> <br />
<a href=""https://i.sstatic.net/kNLiG.png"" rel=""nofollow noreferrer"">The X and Y train are of the shapes as this</a></p>
<p>This gives the error</p>
<pre><code> File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py&quot;, line 1377, in train_function  *
        return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py&quot;, line 1360, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py&quot;, line 1349, in run_step  **
        outputs = model.train_step(data)
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py&quot;, line 1127, in train_step
        loss = self.compute_loss(x, y, y_pred, sample_weight)
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py&quot;, line 1185, in compute_loss
        return self.compiled_loss(
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py&quot;, line 277, in __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/losses.py&quot;, line 143, in __call__
        losses = call_fn(y_true, y_pred)
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/losses.py&quot;, line 270, in call  **
        return ag_fn(y_true, y_pred, **self._fn_kwargs)
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/losses.py&quot;, line 2221, in categorical_crossentropy
        return backend.categorical_crossentropy(
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/backend.py&quot;, line 5575, in categorical_crossentropy
        target.shape.assert_is_compatible_with(output.shape)

    ValueError: Shapes (None, 16) and (None, 1, 16) are incompatible
</code></pre>
<p>I just want my model to take a word input and create a one line output poem of 5-7-5 syllables count or just the appropriate one liner poem for now that can be about 10 to 15 words around. How should I approach this?</p>
<p>I tried the approach but got errors in the shape. I am finding it hard to modify it as I think I am doing something wrong already!
Any help is appreciated!</p>
",Training and Model Evaluation,lstm one line poem generation single word help input manipulation model creation correct way want model able create poem le word element posted list string trying use nlp word index assigning number word document proceeded showed error dimension loss computation tip move forward get poem generator dataset look like x train shape give error want model take word input create one line output poem syllable count appropriate one liner poem word around approach tried approach got error shape finding hard modify think something wrong already help appreciated
Pretrained model with stride doesn’t predict long text,"<p>My objective is to annotate long documents with bioformer-8L. I have been said to use stride and truncation so I don’t have to split my documents in chunks of 512 tokens.</p>
<p>In the training phase, I called the tokenizer like this:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, stride = 128, return_overflowing_tokens=True, model_max_length=512, truncation=True, is_split_into_words=True)
</code></pre>
<p>Then I train my model and at this stage I don't see any parameter that could help me in my task.</p>
<p>With my trained model I do this for the predictions:</p>
<pre><code>model = AutoModelForTokenClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path, stride = 128, return_overflowing_tokens=True, model_max_length=512, truncation=True, is_split_into_words=True)
ner = pipeline(“token-classification”, model=model, tokenizer=tokenizer, aggregation_strategy=“first”)
</code></pre>
<p>But it does not work, the model stops providing annotations in the middle of the text. For the test</p>
",Training and Model Evaluation,pretrained model stride predict long text objective annotate long document bioformer l said use stride truncation split document chunk token training phase called tokenizer like train model stage see parameter could help task trained model prediction doe work model stop providing annotation middle text test
Why building a new scorer outputs an empty string for deepspeech 0.9.3,"<p>I am trying to create a limited corpus and train a language model to use for a deepspeech scorer.</p>
<p>I have followed the information provided in <a href=""https://deepspeech.readthedocs.io/en/r0.9/Scorer.html"" rel=""nofollow noreferrer"">the docs here</a></p>
<p>I read a helpful guide posted for an older version of deepspeech for generating a language model <a href=""https://discourse.mozilla.org/t/tune-moziiladeepspeech-to-recognize-specific-sentences/41350/22"" rel=""nofollow noreferrer"">here</a></p>
<p>And I have read <a href=""https://mozilla.github.io/deepspeech-playbook/"" rel=""nofollow noreferrer"">the playbook here</a>,</p>
<p>It seems that this has been encountered before, but <a href=""https://github.com/coqui-ai/STT/discussions/1472"" rel=""nofollow noreferrer"">no answer was given there</a></p>
<p>I have set up the docker environment for training and followed the docs to the letter.</p>
<p>I can train a model, and then convert this to a .scorer file, so the whole process is working.</p>
<p>The steps I take from inside the docker container are:</p>
<ul>
<li>create a vocab.txt file with my input sentences and store it in the deepspeech-data-input folder.</li>
<li>run this script to build the model in the output<code>python3 generate_lm.py --input_txt ../../deepspeech-data/input/vocab.txt --output_dir ../../deepspeech-data/output --top_k 100 --kenlm_bins /DeepSpeech/native_client/kenlm/build/bin/ --arpa_order 5 --max_arpa_memory &quot;85%&quot; --arpa_prune &quot;0|0|0|0&quot; --binary_a_bits 255 --binary_q_bits 8 --binary_type trie --discount_fallback </code></li>
<li>run this script to generate the scorer: <code>./generate_scorer_package --alphabet ../../deepspeech-data/input/alphabet.txt --lm ../../deepspeech-data/output/lm.binary --vocab ../../deepspeech-data/output/vocab-100.txt --package ../../deepspeech-data/output/deepspeech-0.9.3-models.scorer --default_alpha 0.9 --default_beta 0.9 --force_bytes_output_mode 1 </code></li>
<li>replace the default scorer with my one.</li>
<li>Run deepspeech</li>
</ul>
<p>Everything seems to work as it should, no errors or anything, but when doing this deepspeech just detects an empty string. If I use the default scorer I have it working fine, but I need to restrict the vocabulary so that I can just detect a few commands.</p>
<p>I have tried adjusting some of the flags, but I always get the same result.</p>
<p>I am using the <code>--discount_fallback</code> flag as suggested as it is a small corpus</p>
<p>So my question is this. Why would a deepspeech language model/scrorer output an empty string and how can I fix it?</p>
<p>I am running this inside the NodeJS example on github but testing against any of them would work to reproduce. <a href=""https://github.com/mozilla/DeepSpeech-examples"" rel=""nofollow noreferrer"">github examples</a></p>
",Training and Model Evaluation,building new scorer output empty string deepspeech trying create limited corpus train language model use deepspeech scorer followed information provided doc read helpful guide posted older version deepspeech generating language model read playbook seems ha encountered answer wa given set docker environment training followed doc letter train model convert scorer file whole process working step take inside docker container create vocab txt file input sentence store deepspeech data input folder run script build model output run script generate scorer replace default scorer one run deepspeech everything seems work error anything deepspeech detects empty string use default scorer working fine need restrict vocabulary detect command tried adjusting flag always get result using flag suggested small corpus question would deepspeech language model scrorer output empty string fix running inside nodejs example github testing would work reproduce github example
Incremental Inverse Document Frequency without storing the past information,"<p>I compute the <code>tf-idf</code> everyday in my pipeline using pyspark to evaluate the significance of a keyword in a specific document. This enables me to generate a summary for utilization in my machine learning model. Although the documents in my pipeline change daily, many keywords persist. Storing the historical information of document frequency for each keyword is impractical and not possible.
How can I approximate or incrementally calculate the IDF score for a given keyword in this scenario?</p>
<p>IDF calculation:
<code>idf(t) = log(D / (d: t in d))</code></p>
",Training and Model Evaluation,incremental inverse document frequency without storing past information compute everyday pipeline using pyspark evaluate significance keyword specific document enables generate summary utilization machine learning model although document pipeline change daily many keywords persist storing historical information document frequency keyword impractical possible approximate incrementally calculate idf score given keyword scenario idf calculation
Fairseq Custom Model Training Error: Issues Running fairseq-train with Simple LSTM Architecture,"<p>I am trying to train a custom sequence-to-sequence model using Fairseq's <strong><code>fairseq-train</code></strong> command. I've implemented my own SimpleLSTM architecture in Google Collab, and although Fairseq seems to detect the model correctly, it keeps throwing errors during training.</p>
<h4>Data Preparation Commands:</h4>
<pre><code>!pip install fairseq
</code></pre>
<p>!git clone <a href=""https://github.com/pytorch/fairseq.git"" rel=""nofollow noreferrer"">https://github.com/pytorch/fairseq.git</a></p>
<p>cd /content/fairseq/examples/translation</p>
<p>!chmod +x /content/fairseq/examples/translation/prepare-iwslt14.sh</p>
<p>!/content/fairseq/examples/translation/prepare-iwslt14.sh</p>
<h4>Training Command:</h4>
<pre><code>!fairseq-train /content/fairseq/examples/translation/iwslt14.tokenized.de-en \
  --arch=tutorial_simple_lstm \
  --encoder-dropout=0.2 \
  --decoder-dropout=0.2 \
  --optimizer=adam \
  --lr=0.005 \
  --lr-shrink=0.5 \
  --max-tokens=12000
</code></pre>
<p><strong>My model:</strong></p>
<p>I've seen in other threads that the problem arose from not placing the model in the Fairseq models folder, but I have already done that with the following:</p>
<pre><code>%%writefile /content/fairseq/fairseq/models/tutorial_simple_lstm.py

import torch.nn as nn
from fairseq import utils
from fairseq.models import FairseqEncoder

class SimpleLSTMEncoder(FairseqEncoder):
    def __init__(self, args, dictionary, embed_dim=128, hidden_dim=128, dropout=0.1):
        super().__init__(dictionary)
        self.args = args
        self.embed_tokens = nn.Embedding(len(dictionary), embed_dim, padding_idx=dictionary.pad())
        self.dropout = nn.Dropout(p=dropout)
        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=False, batch_first=True)

    def forward(self, src_tokens, src_lengths):
        if self.args.left_pad_source:
            src_tokens = utils.convert_padding_direction(src_tokens, padding_idx=self.dictionary.pad(), left_to_right=True)
        x = self.embed_tokens(src_tokens)
        x = self.dropout(x)
        x = nn.utils.rnn.pack_padded_sequence(x, src_lengths, batch_first=True)
        _outputs, (final_hidden, _final_cell) = self.lstm(x)
        return {'final_hidden': final_hidden.squeeze(0)}

    def reorder_encoder_out(self, encoder_out, new_order):
        pass
import torch.nn as nn
from fairseq import utils
from fairseq.models import FairseqEncoder, FairseqDecoder
import torch

class SimpleLSTMEncoder(FairseqEncoder):
    def __init__(self, args, dictionary, embed_dim=128, hidden_dim=128, dropout=0.1):
        super().__init__(dictionary)
        self.args = args
        self.embed_tokens = nn.Embedding(len(dictionary), embed_dim, padding_idx=dictionary.pad())
        self.dropout = nn.Dropout(p=dropout)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=1, bidirectional=False, batch_first=True)

    def forward(self, src_tokens, src_lengths):
        if self.args.left_pad_source:
            src_tokens = utils.convert_padding_direction(src_tokens, padding_idx=self.dictionary.pad(), left_to_right=True)
        x = self.embed_tokens(src_tokens)
        x = self.dropout(x)
        x = nn.utils.rnn.pack_padded_sequence(x, src_lengths, batch_first=True)
        _outputs, (final_hidden, _final_cell) = self.lstm(x)
        return {'final_hidden': final_hidden.squeeze(0)}

    def reorder_encoder_out(self, encoder_out, new_order):
        final_hidden = encoder_out['final_hidden']
        return {'final_hidden': final_hidden.index_select(0, new_order)}


class SimpleLSTMDecoder(FairseqDecoder):
    def __init__(self, dictionary, encoder_hidden_dim=128, embed_dim=128, hidden_dim=128, dropout=0.1):
        super().__init__(dictionary)
        self.embed_tokens = nn.Embedding(len(dictionary), embed_dim, padding_idx=dictionary.pad())
        self.dropout = nn.Dropout(p=dropout)
        self.lstm = nn.LSTM(encoder_hidden_dim + embed_dim, hidden_dim, num_layers=1, bidirectional=False)
        self.output_projection = nn.Linear(hidden_dim, len(dictionary))

    def forward(self, prev_output_tokens, encoder_out):
        bsz, tgt_len = prev_output_tokens.size()
        final_encoder_hidden = encoder_out['final_hidden']
        x = self.embed_tokens(prev_output_tokens)
        x = self.dropout(x)
        x = torch.cat([x, final_encoder_hidden.unsqueeze(1).expand(bsz, tgt_len, -1)], dim=2)
        initial_state = (final_encoder_hidden.unsqueeze(0), torch.zeros_like(final_encoder_hidden).unsqueeze(0))
        output, _ = self.lstm(x.transpose(0, 1), initial_state)
        x = output.transpose(0, 1)
        x = self.output_projection(x)
        return x, None

from fairseq.models import FairseqEncoderDecoderModel, register_model

@register_model('simple_lstm')
class SimpleLSTMModel(FairseqEncoderDecoderModel):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--encoder-embed-dim', type=int, metavar='N')
        parser.add_argument('--encoder-hidden-dim', type=int, metavar='N')
        parser.add_argument('--encoder-dropout', type=float, default=0.1)
        parser.add_argument('--decoder-embed-dim', type=int, metavar='N')
        parser.add_argument('--decoder-hidden-dim', type=int, metavar='N')
        parser.add_argument('--decoder-dropout', type=float, default=0.1)

    @classmethod
    def build_model(cls, args, task):
        encoder = SimpleLSTMEncoder(
            args=args,
            dictionary=task.source_dictionary,
            embed_dim=args.encoder_embed_dim,
            hidden_dim=args.encoder_hidden_dim,
            dropout=args.encoder_dropout,
        )
        decoder = SimpleLSTMDecoder(
            dictionary=task.target_dictionary,
            encoder_hidden_dim=args.encoder_hidden_dim,
            embed_dim=args.decoder_embed_dim,
            hidden_dim=args.decoder_hidden_dim,
            dropout=args.decoder_dropout,
        )
        model = SimpleLSTMModel(encoder, decoder)
        print(model)
        return model

from fairseq.models import register_model_architecture

@register_model_architecture('simple_lstm', 'tutorial_simple_lstm')
def tutorial_simple_lstm(args):
    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)
    args.encoder_hidden_dim = getattr(args, 'encoder_hidden_dim', 256)
    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)
    args.decoder_hidden_dim = getattr(args, 'decoder_hidden_dim', 256)
</code></pre>
<p><strong>The Error:</strong></p>
<pre><code>2023-09-16 11:37:17.444106: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-16 11:37:18.331492: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-16 11:37:19 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.
2023-09-16 11:37:20 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
usage: fairseq-train
       [-h]
       [--no-progress-bar]
       [--log-interval LOG_INTERVAL]
       [--log-format {json,none,simple,tqdm}]
       [--log-file LOG_FILE]
       [--aim-repo AIM_REPO]
       [--aim-run-hash AIM_RUN_HASH]
       [--tensorboard-logdir TENSORBOARD_LOGDIR]
       [--wandb-project WANDB_PROJECT]
       [--azureml-logging]
       [--seed SEED]
       [--cpu]
       [--tpu]
       [--bf16]
       [--memory-efficient-bf16]
       [--fp16]
       [--memory-efficient-fp16]
       [--fp16-no-flatten-grads]
       [--fp16-init-scale FP16_INIT_SCALE]
       [--fp16-scale-window FP16_SCALE_WINDOW]
       [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]
       [--on-cpu-convert-precision]
       [--min-loss-scale MIN_LOSS_SCALE]
       [--threshold-loss-scale THRESHOLD_LOSS_SCALE]
       [--amp]
       [--amp-batch-retries AMP_BATCH_RETRIES]
       [--amp-init-scale AMP_INIT_SCALE]
       [--amp-scale-window AMP_SCALE_WINDOW]
       [--user-dir USER_DIR]
       [--empty-cache-freq EMPTY_CACHE_FREQ]
       [--all-gather-list-size ALL_GATHER_LIST_SIZE]
       [--model-parallel-size MODEL_PARALLEL_SIZE]
       [--quantization-config-path QUANTIZATION_CONFIG_PATH]
       [--profile]
       [--reset-logging]
       [--suppress-crashes]
       [--use-plasma-view]
       [--plasma-path PLASMA_PATH]
       [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]
       [--tokenizer {moses,nltk,space}]
       [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]
       [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]
       [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]
       [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}]
       [--task TASK]
       [--num-workers NUM_WORKERS]
       [--skip-invalid-size-inputs-valid-test]
       [--max-tokens MAX_TOKENS]
       [--batch-size BATCH_SIZE]
       [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]
       [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]
       [--dataset-impl {raw,lazy,cached,mmap,fasta,huffman}]
       [--data-buffer-size DATA_BUFFER_SIZE]
       [--train-subset TRAIN_SUBSET]
       [--valid-subset VALID_SUBSET]
       [--combine-valid-subsets]
       [--ignore-unused-valid-subsets]
       [--validate-interval VALIDATE_INTERVAL]
       [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]
       [--validate-after-updates VALIDATE_AFTER_UPDATES]
       [--fixed-validation-seed FIXED_VALIDATION_SEED]
       [--disable-validation]
       [--max-tokens-valid MAX_TOKENS_VALID]
       [--batch-size-valid BATCH_SIZE_VALID]
       [--max-valid-steps MAX_VALID_STEPS]
       [--curriculum CURRICULUM]
       [--gen-subset GEN_SUBSET]
       [--num-shards NUM_SHARDS]
       [--shard-id SHARD_ID]
       [--grouped-shuffling]
       [--update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR]
       [--update-ordered-indices-seed]
       [--distributed-world-size DISTRIBUTED_WORLD_SIZE]
       [--distributed-num-procs DISTRIBUTED_NUM_PROCS]
       [--distributed-rank DISTRIBUTED_RANK]
       [--distributed-backend DISTRIBUTED_BACKEND]
       [--distributed-init-method DISTRIBUTED_INIT_METHOD]
       [--distributed-port DISTRIBUTED_PORT]
       [--device-id DEVICE_ID]
       [--distributed-no-spawn]
       [--ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}]
       [--ddp-comm-hook {none,fp16}]
       [--bucket-cap-mb BUCKET_CAP_MB]
       [--fix-batches-to-gpus]
       [--find-unused-parameters]
       [--gradient-as-bucket-view]
       [--fast-stat-sync]
       [--heartbeat-timeout HEARTBEAT_TIMEOUT]
       [--broadcast-buffers]
       [--slowmo-momentum SLOWMO_MOMENTUM]
       [--slowmo-base-algorithm SLOWMO_BASE_ALGORITHM]
       [--localsgd-frequency LOCALSGD_FREQUENCY]
       [--nprocs-per-node NPROCS_PER_NODE]
       [--pipeline-model-parallel]
       [--pipeline-balance PIPELINE_BALANCE]
       [--pipeline-devices PIPELINE_DEVICES]
       [--pipeline-chunks PIPELINE_CHUNKS]
       [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]
       [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]
       [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]
       [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]
       [--pipeline-checkpoint {always,never,except_last}]
       [--zero-sharding {none,os}]
       [--no-reshard-after-forward]
       [--fp32-reduce-scatter]
       [--cpu-offload]
       [--use-sharded-state]
       [--not-fsdp-flatten-parameters]
       [--arch ARCH]
       [--max-epoch MAX_EPOCH]
       [--max-update MAX_UPDATE]
       [--stop-time-hours STOP_TIME_HOURS]
       [--clip-norm CLIP_NORM]
       [--sentence-avg]
       [--update-freq UPDATE_FREQ]
       [--lr LR]
       [--stop-min-lr STOP_MIN_LR]
       [--use-bmuf]
       [--skip-remainder-batch]
       [--save-dir SAVE_DIR]
       [--restore-file RESTORE_FILE]
       [--continue-once CONTINUE_ONCE]
       [--finetune-from-model FINETUNE_FROM_MODEL]
       [--reset-dataloader]
       [--reset-lr-scheduler]
       [--reset-meters]
       [--reset-optimizer]
       [--optimizer-overrides OPTIMIZER_OVERRIDES]
       [--save-interval SAVE_INTERVAL]
       [--save-interval-updates SAVE_INTERVAL_UPDATES]
       [--keep-interval-updates KEEP_INTERVAL_UPDATES]
       [--keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN]
       [--keep-last-epochs KEEP_LAST_EPOCHS]
       [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS]
       [--no-save]
       [--no-epoch-checkpoints]
       [--no-last-checkpoints]
       [--no-save-optimizer-state]
       [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]
       [--maximize-best-checkpoint-metric]
       [--patience PATIENCE]
       [--checkpoint-suffix CHECKPOINT_SUFFIX]
       [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]
       [--load-checkpoint-on-all-dp-ranks]
       [--write-checkpoints-asynchronously]
       [--store-ema]
       [--ema-decay EMA_DECAY]
       [--ema-start-update EMA_START_UPDATE]
       [--ema-seed-model EMA_SEED_MODEL]
       [--ema-update-freq EMA_UPDATE_FREQ]
       [--ema-fp32]
fairseq-train: error: argument --arch/-a: invalid choice: 'tutorial_simple_lstm' (choose from 's2t_berard', 's2t_berard_256_3_3', 's2t_berard_512_3_2', 's2t_berard_512_5_3', 'transformer_tiny', 'transformer', 'transformer_iwslt_de_en', 'transformer_wmt_en_de', 'transformer_vaswani_wmt_en_de_big', 'transformer_vaswani_wmt_en_fr_big', 'transformer_wmt_en_de_big', 'transformer_wmt_en_de_big_t2t', 'convtransformer', 'convtransformer_espnet', 's2t_transformer', 's2t_transformer_s', 's2t_transformer_xs', 's2t_transformer_sp', 's2t_transformer_m', 's2t_transformer_mp', 's2t_transformer_l', 's2t_transformer_lp', 'wav2vec', 'wav2vec2', 'wav2vec_ctc', 'wav2vec_seq2seq', 'xm_transformer', 's2t_conformer', 'fconv', 'fconv_iwslt_de_en', 'fconv_wmt_en_ro', 'fconv_wmt_en_de', 'fconv_wmt_en_fr', 'tacotron_2', 'tts_transformer', 'fastspeech2', 'lstm', 'lstm_wiseman_iwslt_de_en', 'lstm_luong_wmt_en_de', 'lstm_lm', 'fconv_lm', 'fconv_lm_dauphin_wikitext103', 'fconv_lm_dauphin_gbw', 'hubert', 'hubert_ctc', 'lightconv', 'lightconv_iwslt_de_en', 'lightconv_wmt_en_de', 'lightconv_wmt_en_de_big', 'lightconv_wmt_en_fr_big', 'lightconv_wmt_zh_en_big', 'lightconv_lm', 'lightconv_lm_gbw', 'fconv_self_att', 'fconv_self_att_wp', 'nonautoregressive_transformer', 'nonautoregressive_transformer_wmt_en_de', 'nacrf_transformer', 'iterative_nonautoregressive_transformer', 'iterative_nonautoregressive_transformer_wmt_en_de', 'cmlm_transformer', 'cmlm_transformer_wmt_en_de', 'levenshtein_transformer', 'levenshtein_transformer_wmt_en_de', 'levenshtein_transformer_vaswani_wmt_en_de_big', 'levenshtein_transformer_wmt_en_de_big', 'insertion_transformer', 'transformer_lm', 'transformer_lm_big', 'transformer_lm_baevski_wiki103', 'transformer_lm_wiki103', 'transformer_lm_baevski_gbw', 'transformer_lm_gbw', 'transformer_lm_gpt', 'transformer_lm_gpt2_small', 'transformer_lm_gpt2_tiny', 'transformer_lm_gpt2_medium', 'transformer_lm_gpt2_big', 'transformer_lm_gpt2_big_wide', 'transformer_lm_gpt2_bigger', 'transformer_lm_gpt3_small', 'transformer_lm_gpt3_medium', 'transformer_lm_gpt3_large', 'transformer_lm_gpt3_xl', 'transformer_lm_gpt3_2_7', 'transformer_lm_gpt3_6_7', 'transformer_lm_gpt3_13', 'transformer_lm_gpt3_175', 'transformer_ulm', 'transformer_ulm_big', 'transformer_ulm_tiny', 'roberta', 'roberta_prenorm', 'roberta_base', 'roberta_large', 'xlm', 'roberta_enc_dec', 'transformer_from_pretrained_xlm', 'transformer_align', 'transformer_wmt_en_de_big_align', 'xmod_base_13', 'xmod_base_30', 'xmod_base_60', 'xmod_base_75', 'xmod_base', 'xmod_large_prenorm', 'bart_large', 'bart_base', 'mbart_large', 'mbart_base', 'mbart_base_wmt20', 's2ut_transformer', 's2ut_transformer_fisher', 's2spect_transformer', 's2spect_transformer_fisher', 's2ut_conformer', 'masked_lm', 'bert_base', 'bert_large', 'xlm_base', 'multilingual_transformer', 'multilingual_transformer_iwslt_de_en', 'hf_gpt2', 'hf_gpt2_medium', 'hf_gpt2_large', 'hf_gpt2_xl', 'dummy_model', 'model_parallel_roberta', 'model_parallel_roberta_v1', 'model_parallel_roberta_postnorm', 'model_parallel_roberta_base', 'model_parallel_roberta_large', 'transformer_iwslt_de_en_pipeline_parallel', 'transformer_wmt_en_de_big_pipeline_parallel', 'transformer_lm_megatron', 'transformer_lm_megatron_11b')
</code></pre>
<p>I believe the error could either be in the execution environment since it's Google Colab, in the installed libraries, or in the model code itself which may not be well implemented.</p>
",Training and Model Evaluation,fairseq custom model training error issue running fairseq train simple lstm architecture trying train custom sequence sequence model using fairseq command implemented simplelstm architecture google collab although fairseq seems detect model correctly keep throwing error training data preparation command git clone cd content fairseq example translation chmod x content fairseq example translation prepare iwslt sh content fairseq example translation prepare iwslt sh training command model seen thread problem arose placing model fairseq model folder already done following error believe error could either execution environment since google colab installed library model code may well implemented
"Can the total Entropy of all clusters be greater than 1, after classification?","<p>After doing k-means classification on a dataset (value of k = 3), I tried to find out the total entropy of all the clusters. (Total number of datapoints, or, the total length of the dataset was : 500)</p>
<p><strong>My classification results:</strong></p>
<p><strong>Cluster 1:</strong><br />
Class: neutral, Count: 64, Pr(neutral): 0.30769<br />
Class: positive, Count: 85, Pr(positive): 0.40865<br />
Class: negative, Count: 59, Pr(negative): 0.28365</p>
<p>Entropy of Cluster: 1.566429</p>
<p>Cluster size: 208</p>
<p><strong>Cluster 2:</strong><br />
Class: neutral, Count: 65, Pr(neutral): 0.363128<br />
Class: positive, Count: 36, Pr(positive): 0.2011173<br />
Class: negative, Count: 78, Pr(negative): 0.4357541</p>
<p>Entropy of Cluster: 1.5182706</p>
<p>Cluster size: 179</p>
<p><strong>Cluster 3:</strong><br />
Class: neutral, Count: 39, Pr(neutral): 0.345132<br />
Class: positive, Count: 30, Pr(positive): 0.265486<br />
Class: negative, Count: 44, Pr(negative): 0.389380</p>
<p>Entropy of Cluster: 1.56750289</p>
<p>Cluster size: 113</p>
<p><em>Total Entropy: 1.549431124 (which is &gt; 1)</em></p>
<p>It means, that the 1st cluster contains 3 different types (classes) of datapoints in it, (whereas, for a perfect cluster, it should have contained only 1 type of class) namely, in the 1st cluster, there were a total 208 data points, out of which, 64 of them belongs to the neutral class, 85 belongs to the positive and 59 belongs to the negative class, and so on for the other 2 clusters</p>
<p><strong>I used the formula</strong>:</p>
<p><strong>Entropy of a single Cluster</strong></p>
<p><a href=""https://i.sstatic.net/13773.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/13773.gif"" alt=""enter image description here"" /></a></p>
<p>where: <strong>c</strong> is a classification in the set <strong>C</strong> of all classifications
<strong>P(w_c)</strong> is probability of a data point being classified as <strong>c</strong> in cluster <strong>w</strong>.</p>
<p><a href=""https://i.sstatic.net/7JwaI.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7JwaI.gif"" alt=""enter image description here"" /></a></p>
<p>where:
<strong>|w_c|</strong> is the count of points classified as <strong>c</strong> in cluster <strong>w</strong>
<strong>n_w</strong> is the count of points in cluster <strong>w</strong></p>
<p><strong>Total Entropy of a clustering</strong></p>
<p><a href=""https://i.sstatic.net/wyv9H.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wyv9H.gif"" alt=""enter image description here"" /></a></p>
<p>where:</p>
<p><a href=""https://i.sstatic.net/dD6ki.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dD6ki.gif"" alt=""enter image description here"" /></a></p>
<p>is the set of clusters.
<strong>H(w)</strong> is a single clusters entropy
<strong>N_w</strong> is the number of points in cluster <strong>w</strong>
<strong>N</strong> is the total number of points.</p>
<p>I used the above formula to calculate the total entropy of a clustering, the result I got was a value &gt; 1. I thought entropies are supposed to lie between 0 and 1, still I got something &gt; 1, I could not understand my fault here, was my calculation wrong ? (but I have used the formula as was supposed to be used), or I missed something in the formula, or such (you might as well check the results after a manual calculation yourselves)</p>
",Training and Model Evaluation,total entropy cluster greater classification k mean classification dataset value k tried find total entropy cluster total number datapoints total length dataset wa classification result cluster class neutral count pr neutral class positive count pr positive class negative count pr negative entropy cluster cluster size cluster class neutral count pr neutral class positive count pr positive class negative count pr negative entropy cluster cluster size cluster class neutral count pr neutral class positive count pr positive class negative count pr negative entropy cluster cluster size total entropy mean st cluster contains different type class datapoints whereas perfect cluster contained type class namely st cluster total data point belongs neutral class belongs positive belongs negative class cluster used formula entropy single cluster c classification set c classification p w c probability data point classified c cluster w w c count point classified c cluster w n w count point cluster w total entropy clustering set cluster h w single cluster entropy n w number point cluster w n total number point used formula calculate total entropy clustering result got wa value thought entropy supposed lie still got something could understand fault wa calculation wrong used formula wa supposed used missed something formula might well check result manual calculation
Not having the train model in Generic Assistant,"<p>In book <code>GenericAssistant</code>, when we enter the data from file <code>json</code>, we have to press <code>train_modle</code>, but when I press <code>train_modle</code>, it says there is no such module!</p>
<pre><code>import sys
import threading
import tkinter as tk

import speech_recognition as spr
import pyttsx3 as tts

from neuralintents import GenericAssistant.train_modle()

class ass:
    def __init__(self):
        
        
        self.rcgr = spr.Recognizer()
        self.speaker = tts.init()
        self.speaker.setProperty(&quot;rate&quot; , 150)
         
        self.ass = GenericAssistant(&quot;intense.json&quot;)
        &gt; ***** self.ass.train_modle() ***** 

        self.root = tk.Tk()
        self.lable = tk.Label(text=&quot;🤖&quot; , font=(&quot;Arial&quot; , 120 , &quot;bold&quot;))
        self.lable.pack()

        threading.Thread(target=self.run_ass).start()

        self.root.mainloop()

    def run_ass(self):
        while True:
            try:
                with spr.Microphone() as mic:

                    self.rcgr.adjust_for_ambient_noise(mic , duration= 0.2)
                    audio = self.rcgr.listen(mic)

                    text = self.rcgr.recognize_sphinx(audio)
                    text = text.lower()
                    
                    if &quot;hey jake&quot; in text:
                        self.lable.config(fg=&quot;red&quot;)

                        audio = self.rcgr.listen(mic)
                        text = self.rcgr.recognize_sphinx(audio)
                        text = text.lower()

                        if text == &quot;stop&quot;:
                            self.speaker.say(&quot;Good Bye&quot;)
                            self.speaker.runAndWait()
                            
                            self.speaker.stop()
                            self.root.destroy()
                            sys.exit()

                        else:
                            if text is not None :
                                rsp = self.ass.requste(text)
                                if rsp is not None:
                                    self.speaker.say(rsp)
                                    self.speaker.runAndWait()

                            self.lable.config(fg=&quot;black&quot;)

            except:
                self.lable.config(fg=&quot;black&quot;)
                continue


ass()
</code></pre>
<p>If you think of a method, I would be grateful if you could share it with me.</p>
",Training and Model Evaluation,train model generic assistant book enter data file press press say module think method would grateful could share
How to handle tokens that don&#39;t have a label in an NLP task?,"<p>I'm working on training an NLP model to detect sensitive information in documents. There are 15 categories of sensitive information I'm attempting to predict. It seemed like adding another category for nonsensitive data would be a good idea so that the model could distinguish between sensitive and non-sensitive.</p>
<p>However, this has lead to the dataset to be very skewed and the model to be very accurate at predicting the non-sensitive data. I'm unsure how to approach the task becuase if I mask the nonsensitive data, then I don't believe the model would learn to distinguish between sensitive and nonsensitive very well.</p>
<p>Right now there is so much nonsensitive data that I'm considering doing a heavy amount of undersampling to shrink the dataset.</p>
<p>After training the model with the current data, the metrics are great for the Non-Sensitive Data, however most of the other categories aren't predicted very well.</p>
<p><a href=""https://i.sstatic.net/8PVdW.png"" rel=""nofollow noreferrer"">Model training results</a></p>
<h2>Data Overview</h2>
<p>The data for this problem is a dataset of medical notes that have fake sensitive information included in them. Don't believe the dataset is very large compared to other NLP datasets, but I don't have much experience with NLP problems. It is 26MB when formatted and placed in Excell files as Train, Validation, and Test.</p>
<p>When splitting the data, I set Training as 50% of the data Validation as 25%, and Test as 25%. I did this so that I could ensure test had enough of different categories of sensitive info in it.</p>
<h2>Data Specifics</h2>
<p>The data is being classified in context of each sentence. So the data is formatted into 38 token sequences to be used as input. Each token has been assigned a label, including padding. However, I have a mask on my model which ignores the padding and the labels for it. All of the labels are encoded into a range of numbers between 0 and 15. Padding is '0' and Nonsensitive information is '1'.</p>
<p>An example sequence of input_ids and matching labels:</p>
<ul>
<li>Tokens:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2501, 3058, 1024, 12875, 2575, 1011, 6185, 1011, 2260, 6063, 2030, 2705, 24174, 2594, 9228, 1018, 2081, 2527, 4418, 18168, 4817, 1010, 11721, 22955, 2581, 2475]</li>
<li>Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 10, 10, 10, 10, 10, 10, 9, 1, 1, 1, 1, 1, 14, 14, 14, 14, 12, 12, 1, 5, 11, 11, 1]</li>
</ul>
<p>I will also add, that the sentences of the data have been tokenized by the <code>bert-base-uncased</code> tokenizer.</p>
<h2>The model</h2>
<p>Because I believed the dataset was small, I've been using a GRU model rather than an LSTM or trying to train a Transformer.</p>
<p>Here is my code for the model:</p>
<pre><code>#Dataset statistics
numb_classes = 17 #Number of classes in the NLP problem
vocab_size = 30522 #The vocab size of the bert-base-uncased model
out_dim = 100
input_len = 38

callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)

model_gru = Sequential()
model_gru.add(Embedding(input_dim=vocab_size, output_dim=out_dim, input_length=input_len, mask_zero=True))
model_gru.add(GRU(64, return_sequences=True, activation='tanh'))
model_gru.add(Dropout(.25))
model_gru.add(GRU(32, return_sequences=True, activation='tanh'))
model_gru.add(Dropout(.25))
model_gru.add(Dense(numb_classes, activation='softmax'))

model_gru.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])
history_gru = model_gru.fit(train_X_gru, train_y_gru, epochs=10, batch_size=70,
                        validation_data=(val_X_gru, val_y_gru))
</code></pre>
<p>The model is very simplistic right now, because it seems to be overfitting to the data, and I've added dropout to help combat that.</p>
",Training and Model Evaluation,handle token label nlp task working training nlp model detect sensitive information document category sensitive information attempting predict seemed like adding another category nonsensitive data would good idea model could distinguish sensitive non sensitive however ha lead dataset skewed model accurate predicting non sensitive data unsure approach task becuase mask nonsensitive data believe model would learn distinguish sensitive nonsensitive well right much nonsensitive data considering heavy amount undersampling shrink dataset training model current data metric great non sensitive data however category predicted well model training result data overview data problem dataset medical note fake sensitive information included believe dataset large compared nlp datasets much experience nlp problem mb formatted placed excell file train validation test splitting data set training data validation test could ensure test enough different category sensitive info data specific data classified context sentence data formatted token sequence used input token ha assigned label including padding however mask model ignores padding label label encoded range number padding nonsensitive information example sequence input id matching label token label also add sentence data tokenized tokenizer model believed dataset wa small using gru model rather lstm trying train transformer code model model simplistic right seems overfitting data added dropout help combat
How to train doc2vec with pre-built vocab in gensim,"<p>I have 1000 documents.</p>
<p>For some purpose I need to keep specific words in the vocab. I tokenize the 1000 documents and I design a word_freq dict. e.g. {&quot;word1&quot;:100, &quot;word2&quot;: 2000, ...}</p>
<p>Now I want to build a doc2vec model using this word_freq.</p>
<pre><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument

model = Doc2Vec(vector_size= 300,
                window=300,
                min_count=0, 
                alpha=0.01, 
                min_alpha=0.0007,
                sample=1e-4,
                negative=5,
                dm=1,
                epochs=20,
                workers=16)

model.build_vocab_from_freq(word_freq=tf_idf_vocab, keep_raw_vocab=False, corpus_count=1000, update=False)

N = 942100020 # is the total number of words in the whole 1000 docs.
model.train(corpus_file=train_data,
                total_examples=model.corpus_count,
                total_words=N,
                epochs=model.epochs)
</code></pre>
<p>To boost the training time, I used corpus file (SentenceLine) for training where each document is a line (document' words are separated by space).</p>
<p>Each document is expected to be tagged with its number in the corpus file (i.e. numeric tag.)</p>
<p>As a test, I trained the model for few epochs. To get the most similar word to a given document with e.g. tag=0, I use:</p>
<pre><code>doc_vector = model_copy.dv[tag]

sims = model_copy.wv.most_similar([doc_vector], topn=20)
</code></pre>
<p><strong>I got an error in <code>doc_vector = model_copy.dv[tag]</code> said that tag=0 does not exist!</strong>
I debug and it seems that model.dv is empty!</p>
<pre><code>model.dv.expandos # {}
</code></pre>
<p>I checked the code of <code>build_vocab()</code>, at some point it call _scan_vocab() where it set the <code>model.dv</code> with tags.</p>
<p>However, in <code>build_vocab_from_freq()</code> it does not call _scan_vocab() and there is no tagging!?</p>
<pre><code>def _scan_vocab(...):
    ....
    for t, dt in doctags_lookup.items():
            self.dv.key_to_index[t] = dt.index
            self.dv.set_vecattr(t, 'word_count', dt.word_count)
            self.dv.set_vecattr(t, 'doc_count', dt.doc_count)
</code></pre>
<p>Note that when I used <code>model.build_vocab(corpus_file=train_data, progress_per=1000)</code> to build the vocab internally, the documents are tagged with numeric numbers as I explained above!</p>
",Training and Model Evaluation,train doc vec pre built vocab gensim document purpose need keep specific word vocab tokenize document design word freq dict e g word word want build doc vec model using word freq boost training time used corpus file sentenceline training document line document word separated space document expected tagged number corpus file e numeric tag test trained model epoch get similar word given document e g tag use got error said tag doe exist debug seems model dv empty checked code point call scan vocab set tag however doe call scan vocab tagging note used build vocab internally document tagged numeric number explained
Can I use lora to just reduce the size and run inference?,"<p>So, lora basically can make finetune a model really easy right, but I want just to test a language model, in my case Flan-t5 , can I use lora to make it small so it can fit in my gpu ? , I’ve seen tutorials that train the model with HF but I just want it to run as inference, how can I do that, I was just trying with hugging face</p>
<blockquote>
<p>peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM,
inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)
model_name_or_path = “google/flan-t5-xl”</p>
<p>model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path,device_map=‘auto’)</p>
<p>model = get_peft_model(model, peft_config)</p>
</blockquote>
<p>to then just save it , but Im not sure if this is the right thing
Thanks</p>
",Training and Model Evaluation,use lora reduce size run inference lora basically make finetune model really easy right want test language model case flan use lora make small fit gpu seen tutorial train model hf want run inference wa trying hugging face peft config loraconfig task type tasktype seq seq lm inference mode false r lora alpha lora dropout model name path google flan xl model automodelforseq seqlm pretrained model name path device map auto model get peft model model peft config save im sure right thing thanks
model did not return a loss / BertForQuestionAnswering.forward() got an unexpected keyword argument &#39;labels&#39;,"<p>I have this data:</p>
<p><code>intents.json</code>:</p>
<pre><code>{&quot;version&quot;: &quot;0.1.0&quot;,
    &quot;data&quot;:
 [
        {&quot;id&quot;: &quot;hi&quot;, 
        &quot;question&quot;: [&quot;hi&quot;, &quot;how are you&quot;],
        &quot;answers&quot;: [&quot;hi!&quot;, &quot;how can i help you?&quot;],
        &quot;context&quot;: &quot;&quot;
        },
        
        {&quot;id&quot;: &quot;bye&quot;, 
        &quot;question&quot;: [&quot;Bye&quot;, &quot;good bye&quot;, &quot;see you&quot;],
        &quot;answers&quot;: [&quot;see you later&quot;, &quot;have a nice day&quot;, &quot;bye&quot;, &quot;thanks for visiting&quot;],
        &quot;context&quot;: &quot;&quot;
        },

        {&quot;id&quot;: &quot;weather&quot;, 
        &quot;question&quot;: [&quot;how is the weather&quot;, &quot;weather forecast&quot;, &quot;weather&quot;],
        &quot;answers&quot;: [&quot;weather is good&quot;, &quot;we have 25 degrees&quot;],
        &quot;context&quot;: &quot;&quot;
        }

    ]
}
</code></pre>
<p>and I am trying to build a question answer bot.</p>
<p>I am using this code:</p>
<pre><code>from datasets import load_dataset
import datasets
from transformers import AutoTokenizer,  AutoModel, TrainingArguments,\
    Trainer, AutoModelForQuestionAnswering, DefaultDataCollator, \
        DataCollatorForLanguageModeling

MAX_LENGTH = 128

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

def preprocess_func(x): 
    return tokenizer(x[&quot;id&quot;],
                     padding='max_length',
                     truncation=True,
                     max_length=MAX_LENGTH)

train = load_dataset('json', data_files='intents.json', field='data', split='train[:80%]')
test = load_dataset('json', data_files='intents.json', field='data', split='train[80%:]')
 
data = datasets.DatasetDict({&quot;train&quot;:train, &quot;test&quot;: test})

tokenized = data.map(preprocess_func, batched=True)

#data_collator = DefaultDataCollator()
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True
)

device = &quot;cpu&quot;
model = AutoModelForQuestionAnswering.from_pretrained('bert-base-uncased')
model = model.to(device)

training_args = TrainingArguments(
  output_dir=&quot;./results&quot;,
  evaluation_strategy=&quot;epoch&quot;,
  learning_rate=2e-5,
  per_device_train_batch_size=2,
  num_train_epochs=2,
  weight_decay=0.01,
)

trainer = Trainer(
  model=model,
  args=training_args,
  train_dataset=tokenized[&quot;train&quot;],
  tokenizer=tokenizer,
  data_collator=data_collator,
)

trainer.train()
</code></pre>
<p>and I am receiving:</p>
<p><code>BertForQuestionAnswering.forward() got an unexpected keyword argument 'labels'</code></p>
<p>but I don't have any labels in the data:</p>
<pre><code>tokenized

DatasetDict({
    train: Dataset({
        features: ['context', 'id', 'question', 'answers', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 2
    })
    test: Dataset({
        features: ['context', 'id', 'question', 'answers', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 1
    })
})
</code></pre>
<p>If I use :</p>
<p><code>DefaultDataCollator()</code> instead of <code>DataCollatorForLanguageModeling</code>, I receive:</p>
<p><code>The model did not return a loss from the inputs, only the following keys: start_logits,end_logits</code></p>
<p>I am not sure if the <code>preprocess_func</code> needs more things to do.</p>
<p>Like, for example <a href=""https://medium.datadriveninvestor.com/lets-build-an-ai-powered-question-answering-system-with-huggingface-transformers-2622d8de18e9"" rel=""nofollow noreferrer"">here</a></p>
<pre><code>def preprocess_function(examples):
    questions = [q.strip() for q in examples[&quot;question&quot;]]
    inputs = tokenizer(
        questions,
        examples[&quot;context&quot;],
        max_length=512,
        truncation=&quot;only_second&quot;,
        return_offsets_mapping=True,
        padding=&quot;max_length&quot;,
    )

    offset_mapping = inputs.pop(&quot;offset_mapping&quot;)
    answers = examples[&quot;answers&quot;]
    start_positions = []
    end_positions = []

    for i, offset in enumerate(offset_mapping):
        answer = answers[i]
        start_char = answer[&quot;answer_start&quot;][0]
        end_char = answer[&quot;answer_start&quot;][0] + len(answer[&quot;text&quot;][0])
        sequence_ids = inputs.sequence_ids(i)

        # Find the start and end of the context
        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        # If the answer is not fully inside the context, label it (0, 0)
        if offset[context_start][0] &gt; end_char or offset[context_end][1] &lt; start_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            # Otherwise it's the start and end token positions
            idx = context_start
            while idx &lt;= context_end and offset[idx][0] &lt;= start_char:
                idx += 1
            start_positions.append(idx - 1)

            idx = context_end
            while idx &gt;= context_start and offset[idx][1] &gt;= end_char:
                idx -= 1
            end_positions.append(idx + 1)

    inputs[&quot;start_positions&quot;] = start_positions
    inputs[&quot;end_positions&quot;] = end_positions
    return inputs
</code></pre>
",Training and Model Evaluation,model return loss bertforquestionanswering forward got unexpected keyword argument label data trying build question answer bot using code receiving label data use instead receive sure need thing like example
Does Mistral-7B-Instruct use RLHF?,"<p>In the paper it says</p>
<blockquote>
<p>To evaluate the generalization capabilities of
Mistral 7B, we fine-tuned it on instruction datasets
publicly available on the Hugging Face repository.
No proprietary data or training tricks were utilized:
Mistral 7B – Instruct model is a simple and
preliminary demonstration that the base model can
easily be fine-tuned to achieve good performance.</p>
</blockquote>
<p>However they don't specify if they used RLHF or not.</p>
",Training and Model Evaluation,doe mistral b instruct use rlhf paper say evaluate generalization capability mistral b fine tuned instruction datasets publicly available hugging face repository proprietary data training trick utilized mistral b instruct model simple preliminary demonstration base model easily fine tuned achieve good performance however specify used rlhf
Bleu score in python from scratch,"<p>After watching Andrew Ng's video about <a href=""https://www.youtube.com/watch?v=DejHQYAGb7Q"" rel=""nofollow noreferrer"">Bleu score</a> I wanted to implement one from scratch in python. I wrote the code full in python with numpy sparingly. This is the full code</p>

<pre><code>import numpy as np

def n_gram_generator(sentence,n= 2,n_gram= False):
    '''
    N-Gram generator with parameters sentence
    n is for number of n_grams
    The n_gram parameter removes repeating n_grams 
    '''
    sentence = sentence.lower() # converting to lower case
    sent_arr = np.array(sentence.split()) # split to string arrays
    length = len(sent_arr)

    word_list = []
    for i in range(length+1):
        if i &lt; n:
            continue
        word_range = list(range(i-n,i))
        s_list = sent_arr[word_range]
        string = ' '.join(s_list) # converting list to strings
        word_list.append(string) # append to word_list
        if n_gram:
            word_list = list(set(word_list))
    return word_list

def bleu_score(original,machine_translated):
    '''
    Bleu score function given a orginal and a machine translated sentences
    '''
    mt_length = len(machine_translated.split())
    o_length = len(original.split())

    # Brevity Penalty 
    if mt_length&gt;o_length:
        BP=1
    else:
        penality=1-(mt_length/o_length)
        BP=np.exp(penality)

    # calculating precision
    precision_score = []
    for i in range(mt_length):
        original_n_gram = n_gram_generator(original,i)
        machine_n_gram = n_gram_generator(machine_translated,i)
        n_gram_list = list(set(machine_n_gram)) # removes repeating strings

        # counting number of occurence 
        machine_score = 0
        original_score = 0
        for j in n_gram_list:
            machine_count = machine_n_gram.count(j)
            original_count = original_n_gram.count(j)
            machine_score = machine_score+machine_count
            original_score = original_score+original_count

        precision = original_score/machine_score
        precision_score.append(precision)
    precisions_sum = np.array(precision_score).sum()
    avg_precisions_sum=precisions_sum/mt_length
    bleu=BP*np.exp(avg_precisions_sum)
    return bleu

if __name__ == ""__main__"":
    original = ""this is a test""
    bs=bleu_score(original,original)
    print(""Bleu Score Original"",bs)
</code></pre>

<p>I tried to test my score with nltk's </p>

<pre><code>from nltk.translate.bleu_score import sentence_bleu
reference = [['this', 'is', 'a', 'test']]
candidate = ['this', 'is', 'a', 'test']
score = sentence_bleu(reference, candidate)
print(score)
</code></pre>

<p>The problem is my bleu score is about <code>2.718281</code> and nltk's is <code>1</code>. What am I doing wrong? </p>

<p>Here are some possible reason's:</p>

<p>1) I calculated ngrams with respect to the length of the machine translated sentence. Here from 1 to 4</p>

<p>2) <code>n_gram_generator</code> function which I wrote myself and not sure about its accuracy</p>

<p>3) Some how I used wrong function or miscalculated bleu score</p>

<p>Can some one look my code up and tell me where I did the mistake?</p>
",Training and Model Evaluation,bleu score python scratch watching andrew ng video bleu score wanted implement one scratch python wrote code full python numpy sparingly full code tried test score nltk problem bleu score nltk wrong possible reason calculated ngrams respect length machine translated sentence function wrote sure accuracy used wrong function miscalculated bleu score one look code tell mistake
Correctly submit triplets to a BERT model with triplet loss,"<p>I'm working on a model consisting in taking the elements of a triplet (consisting in an anchor, a positive example and a negative example), pass them through a BERT model and use them to calculate the triplet loss. Here is the code I have for the moment:</p>
<pre><code>import tensorflow as tf
import pandas as pd
from transformers import DistilBertTokenizer, TFDistilBertModel
from tensorflow import keras

model_name = &quot;distilbert-base-multilingual-cased&quot;
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
model = TFDistilBertModel.from_pretrained(model_name)

## retrieve data that finally is a data frame with 3 columns (anchor, positive, negative):
test_data = test_data[['anchor', 'match', 'non_match']]
sample_size = int(0.8 * len(test_data))
train, validation = test[:sample_size], test[sample_size:]

def triplet_loss(y_true, y_pred):
    &quot;&quot;&quot; ignore y_true
    &quot;&quot;&quot;
    anchor = y_pred[:, :VECTOR_SIZE]
    positive = y_pred[:, VECTOR_SIZE:2*VECTOR_SIZE]
    negative = y_pred[:, 2*VECTOR_SIZE:]
    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1) 
    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)
    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), 0.1)
    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)
    return loss

def create_model(model):
    input_anchor = tf.keras.layers.Input(shape=(max_length,), name='input_anchor', dtype='int32')
    input_positive = tf.keras.layers.Input(shape=(max_length,), name='input_positive', dtype='int32')
    input_negative = tf.keras.layers.Input(shape=(max_length,), name='input_negative', dtype='int32')
    anchor_output = model(input_anchor)[0]
    positive_output = model(input_positive)[0]
    negative_output = model(input_negative)[0]
    merged_output = tf.keras.layers.concatenate([anchor_output, positive_output, negative_output])
    model = tf.keras.Model(inputs=[input_anchor, input_positive, input_negative], outputs=merged_output) 
    model.compile(optimizer='Adam', loss = triplet_loss, metrics=['accuracy'])
    return model
</code></pre>
<p>Now I can create the model:</p>
<pre><code>triplet_model = create_model(model)
triplet_model.summary()

Model: &quot;model&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_anchor (InputLayer)      [(None, 64)]         0           []                               
                                                                                                  
 input_positive (InputLayer)    [(None, 64)]         0           []                               
                                                                                                  
 input_negative (InputLayer)    [(None, 64)]         0           []                               
                                                                                                  
 tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_anchor[0][0]',           
 BertModel)                     ast_hidden_state=(N               'input_positive[0][0]',         
                                one, 64, 768),                    'input_negative[0][0]']         
                                 hidden_states=None                                               
                                , attentions=None)                                                
                                                                                                  
 concatenate (Concatenate)      (None, 64, 2304)     0           ['tf_distil_bert_model[0][0]',   
                                                                  'tf_distil_bert_model[1][0]',   
                                                                  'tf_distil_bert_model[2][0]']   
                                                                                                  
==================================================================================================
Total params: 134,734,080
Trainable params: 134,734,080
Non-trainable params: 0
</code></pre>
<p>When I try to fit the model:</p>
<pre><code>history = triplet_model.fit(
    x=[train['anchor'], train['match'], train['non_match']],
    y=None,
    batch_size=32,
    epochs=5,
    steps_per_epoch=2
)
</code></pre>
<p>it returns me this error:</p>
<pre><code>ValueError: Target data is missing. Your model was compiled with loss=&lt;function triplet_loss at 0x7f6161a1f9a0&gt;, and therefore expects target data to be provided in `fit()
</code></pre>
<p>What am I missing?? As far as i know theres no need to provide y when your loss function is a triplet loss</p>
<p>Any help is highly appreciated</p>
",Training and Model Evaluation,correctly submit triplet bert model triplet loss working model consisting taking element triplet consisting anchor positive example negative example pas bert model use calculate triplet loss code moment create model try fit model return error missing far know need provide loss function triplet loss help highly appreciated
Create a model to classificy a sentence logical or not,"<p>How could I train a model to classify following sentences are logical or illogical?</p>
<p>“He has two legs”–logical
“He has six legs”–illogical</p>
<p>Solution I tried:</p>
<p>1 : Train the classifier by cnn</p>
<p>I have done it before, it works very well if you have enough of data. Problem is I do not have a huge data set which comes with “logical” or “illogical” labels for this case.</p>
<p>2 : Use language model</p>
<p>Train a language model introduced by gluonnlp on some data set like wiki, use it to find out the probability of the sentences. If the probability of the sentences are high, mark it as logical and vice versa. Problem is the results not good.</p>
<p>The way I estimate the probability</p>
<pre><code>def __predict(self):
    lines = self.__text_edit_input.toPlainText().split(&quot;\n&quot;)
    result = &quot;&quot;
    for line in lines:
        result += str(self.__sentence_prob(line, 10)) + &quot;\n&quot;

    self.__text_edit_output.setPlainText(result)

def __prepare_sentence(self, text, max_len):
    result = mx.nd.zeros([max_len, 1], dtype='float32')
    max_len = min(len(text), max_len)
    i = max(max_len - len(text), 0)
    j = 0
    for index in range(i, max_len):
        result[index][0] = self.__vocab[text[j]]
        j = j + 1
    return result

def __sentence_prob(self, text, max_len):
    hiddens = self.__model.begin_state(1, func=mx.nd.zeros, ctx=self.__context)
    tokens = self.__tokenizer(text)
    data = self.__prepare_sentence(tokens, max_len)
    output, _ = self.__model(data, hiddens)
    prob = 0
    for i in range(max_len):
        total_prob = mx.nd.softmax(output[i][0])
        prob += total_prob[self.__vocab[i]].asscalar()

    return prob / max_len
</code></pre>
<p>Possible issues of language models:</p>
<pre><code>1. Do not use correct way to split the sentences(I am using jieba to split the Chinese senteces)
2. Number of vocab is too small/big(test 10000, 15000 and 30000)
3. Loss too high(ppl around 190) after 50 epochs?
4. Number of sentences length should be larger/smaller(tried 10,20,35)
5. The data I use do not meet my requirements(not every sentences are logical) 
6. Language model is not appropriate for this task?
</code></pre>
<p>Any suggestions?</p>
",Training and Model Evaluation,create model classificy sentence logical could train model classify following sentence logical illogical ha two leg logical ha six leg illogical solution tried train classifier cnn done work well enough data problem huge data set come logical illogical label case use language model train language model introduced gluonnlp data set like wiki use find probability sentence probability sentence high mark logical vice versa problem result good way estimate probability possible issue language model suggestion
How to get the mask average for multi-token masking?,"<p>Following <a href=""https://aclanthology.org/2021.eacl-main.284/"" rel=""nofollow noreferrer"">this paper</a>, I'm trying to implement how they calculated the average of the log probabilities for each entity (Section 3.3). More specifically, the score for each entity is calculated as the average of the log probabilities across its tokens.</p>
<p>I have a list of entities and some prompts:</p>
<pre><code>my_entities = ['one', 'two', 'penicillin', 'chocolate', 'chocolates', 'potato', 'potatoes', 'kid', 'kiddies', 'alberta']
prompts = ['I like to eat [MASK]', '[MASK] are really good at playing soccer', '[MASK] is a drug']
</code></pre>
<p>The task is to find the <code>top_k</code> (e.g., top 3) entities that should fit instead of <code>[MASK]</code> for each prompt. The issue is that some entities are composed of multiple tokens:</p>
<pre><code>import torch
from transformers import BertTokenizer, BertForMaskedLM
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
model.eval()
for word in my_entities:
    word_id = tokenizer.encode(word, add_special_tokens=False)
    print(tokenizer.convert_ids_to_tokens(word_id))
    print(word_id)
    print()
&gt;&gt;&gt; 
['one']
[2028]

['two']
[2048]

['pen', '##ici', '##llin']
[7279, 28775, 21202]
[...]
</code></pre>
<p>If I understand correctly, in the paper, for each prompt they repeated the <code>[MASK]</code> token <code>k</code> times, where <code>k</code> is the number of tokens the model split some entity into: &quot;We consider multitoken objects by including multiple [MASK] tokens in the templates&quot;:</p>
<pre><code>for prompt in prompts:
    print('orignal prompt: ', prompt)
    for word in my_entities:
        word_id = tokenizer.encode(word, add_special_tokens=False)
        word_id_length = len(word_id)
        new_mask_string = '[MASK] ' * word_id_length
        modified_prompt = prompt.replace('[MASK]', new_mask_string).replace('  ', ' ')
        print('modified_prompt', modified_prompt)
&gt;&gt;&gt;
orignal prompt:  I like to eat [MASK]
modified_prompt I like to eat [MASK] 
modified_prompt I like to eat [MASK] 
modified_prompt I like to eat [MASK] [MASK] [MASK] 
</code></pre>
<p>But how can I get the average of the log probabilities for each of my entities so that I can take the top_k results?</p>
<p>Say I have one such modified prompt. I can get the model's hidden states at the masked position as follows:</p>
<pre><code>sentence = '[MASK] [MASK] [MASK] are really good at playing soccer'
token_ids = tokenizer.encode(sentence, return_tensors='pt')
token_ids_tk = tokenizer.tokenize(sentence)
masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero()
masked_pos = [mask.item() for mask in masked_position ]
with torch.no_grad():
    output = model(token_ids)
last_hidden_state = output[0].squeeze()
for mask_index in masked_pos:
    mask_hidden_state = last_hidden_state[mask_index]
    print(mask_hidden_state)

&gt;&gt;&gt;
tensor([-9.3608, -9.2616, -9.2870,  ..., -8.5086, -8.2262, -8.5502])
tensor([-9.6831, -9.7911, -9.5848,  ..., -8.3435, -8.0961, -8.0973])
tensor([-6.2620, -6.3202, -6.1112,  ..., -5.9973, -5.6018, -5.8678])
</code></pre>
<p>But I'm not sure how to continue. The output I'm eventually looking for</p>
<pre><code>my_entities = ['one', 'two', 'penicillin', 'chocolate', 'chocolates', 'potato', 'potatoes', 'kid', 'kiddies', 'alberta']
prompts = ['I like to eat [MASK]', '[MASK] are really good at playing soccer', '[MASK] is a drug']
scores = [[0.1, 0.3, 0.01...], [0.12, 0.23, 0.21...], [0.41, 0.3, 0.01...]]
</code></pre>
<p>Where <code>scores</code> is a list of length <code>len(prompts)</code>, where each sublist is of length <code>len(my_entities)</code>. That is, each sublist scores each entity for its fit in each modified prompt. Unless there's a more efficient approach to get the top_k entities for each prompt (e.g., instead of duplicating each prompt N times where N is the number of entities).</p>
<p><strong>Update: Adding an answer, but it's VERY slow</strong></p>
<p>I think I finally figured it out, but this is very slow and doesn't really use GPU as it iterates through each prompt one by one. I will accept an answer that can optimize this because at the current state I can't really use this code (too slow):</p>
<pre><code>import torch
from transformers import AutoModelForMaskedLM, AutoTokenizer
import torch.nn as nn 

softmax = nn.Softmax(dim=0)
device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = AutoModelForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;).to(device)
model.eval()

tokenized_masked_token = tokenizer.mask_token #&quot;[MASK]&quot;
entities = [&quot;one&quot;, &quot;two&quot;, &quot;penicillin&quot;, &quot;chocolate&quot;, &quot;chocolates&quot;, &quot;potato&quot;, &quot;potatoes&quot;, &quot;kids&quot;, &quot;kiddies&quot;, &quot;alberta&quot;, &quot;alberta&quot;, &quot;alberta&quot;, &quot;chemistry&quot;, &quot;absd&quot;] * 1000
original_prompts = [&quot;I like to eat [MASK]&quot;, &quot;[MASK] are really good at playing soccer&quot;, &quot;[MASK] is a drug&quot;] * 3000
correct_answers = [&quot;potatoes&quot;, &quot;kids&quot;, &quot;penicillin&quot;] * 3000

top_k_value = 10
top_k_score = {'1':0, '5':0, '10':0} # dictionary to hold the results per model
prompts = [p.replace('[MASK]', tokenized_masked_token) for p in original_prompts] # replace the masked token from the original prompt with the corresponding tokenizer's masked token

ent_ids_lists = [tokenizer.encode(ent, add_special_tokens=False) for ent in entities] # get the ids for each entity
entity_lengths = [len(ent_ids_list) for ent_ids_list in ent_ids_lists] # get the length of each entity (number of tokens)
unique_entity_lengths = list(set(entity_lengths)) # get the set of different lengths
calculated_prompts_counter = 0

for idx, prompt in enumerate(prompts):
    if idx % 10 == 0:
        print(f'idx: {idx} out of {len(prompts)}')
    correct_answer = correct_answers[idx]
    probabilities_per_ent_length_dict = {} # a dictionary to hold the predicted probabilities per unique entity length -- so I won't need to duplicate each prompt len(entities) times, and just len(unique_entity_lengths) times   

    if prompt.count(tokenized_masked_token) != 1: # dont use prompts with more than 1 masking token because they break the prediction since we duplicate the masking token
        continue
    calculated_prompts_counter += 1 # count how many prompts we went over (not counting those with more than 1 masking token)

    # get length of each entity
    duplicated_prompt_list = [[prompt] * len(unique_entity_lengths)] # duplicate the prompt len(unique_entity_lengths) times (number of different entities length) -- JUST NEED TO DUPLICATE IT K TIMES, WHERE K IS THE DIFFERENT NUMBER OF ENTITY LENGTHS. SINCE THE PROBABILITIES WILL BE THE SAME FOR THE SAME PROMPT (THEY WILL ONLY DIFFER IF THERE ARE DIFFERENT NUMBER OF MASKED TOKENS)
    duplicated_prompt_flatten = [item for sublist in duplicated_prompt_list for item in sublist] # flatten
    masked_duplicated_prompts = [p.replace(tokenized_masked_token, tokenized_masked_token * l).replace('][', '] [') for p,l in zip(duplicated_prompt_flatten, unique_entity_lengths)] # replace each prompt masked token with the corresponding number of masked tokens
    
    for masked_duplicated_prompt, unique_entity_length in zip(masked_duplicated_prompts, unique_entity_lengths): 
        mask_hidden_states_lists = [] # a list to hold the hidden states of each masked token for each duplicated prompt
        token_ids = tokenizer.encode(masked_duplicated_prompt, return_tensors='pt') # tokenize the masked_duplicated_prompt
        if len(token_ids[0]) &gt; 512: # truncate sequences that are longer than the context length
            token_ids = token_ids[0][:512].unsqueeze(0) # truncate

        token_ids_tk = tokenizer.tokenize(masked_duplicated_prompt)
        masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero() # get positions of masked tokens
        masked_pos = [mask.item() for mask in masked_position] # get positions of masked tokens
        with torch.no_grad():
            output = model(token_ids.to(device)) # send input ids to the model to get predictions
        last_hidden_state = output[0].squeeze() # get last hidden states
        for mask_index in masked_pos: # masked_pos has a length of K, where K is the number of tokens the entity is split into
            mask_hidden_state = last_hidden_state[mask_index] # get the logits at the masked position
            mask_hidden_states_lists.append(softmax(mask_hidden_state)) # softmax to get probs from logits
        probabilities_per_ent_length_dict[unique_entity_length] = mask_hidden_states_lists # add the predicted probabilities for the specific entity length (each is a list of length K, where K is the number of tokens the entity is split into)
    
    entity_scores = [] # list to hold the scores for each entity -- we take the top_k from here. Each score is an average of the log of the probs
    entities_probs = [probabilities_per_ent_length_dict[ent_length] for ent_length in entity_lengths] # get probabilities per entity length (different length will have different number of mask tokens which will have different probabilities)
    for ent_ids_sublist_idx, ent_ids_sublist in enumerate(ent_ids_lists):
        tmp_score_list = []
        ent_id_sublist_len = len(ent_ids_sublist)
        for ent_len_idx in range(ent_id_sublist_len):
            ent_log_prob = torch.log(entities_probs[ent_ids_sublist_idx][ent_len_idx][ent_ids_sublist[ent_len_idx]])
            tmp_score_list.append(ent_log_prob)
        entity_scores.append(torch.tensor(tmp_score_list).mean())
    topk_entity_sublist_indices = torch.topk(torch.tensor(entity_scores), top_k_value).indices
    top_k_entity_ids = [ent_ids_lists[idx] for idx in topk_entity_sublist_indices] #torch.index_select(torch.tensor(ent_ids_lists), 0, topk_entity_sublist_indices)
    top_k_entity_strings = tokenizer.batch_decode(top_k_entity_ids)
    
    if correct_answer in top_k_entity_strings[:1]:
        top_k_score['1'] += 1
    if correct_answer in top_k_entity_strings[:5]:
        top_k_score['5'] += 1
    if correct_answer in top_k_entity_strings:
        top_k_score['10'] += 1                

print(f'top 1 accuracy: {top_k_score[&quot;1&quot;]/calculated_prompts_counter}')
print(f'top 5 accuracy: {top_k_score[&quot;5&quot;]/calculated_prompts_counter}')
print(f'top 10 accuracy: {top_k_score[&quot;10&quot;]/calculated_prompts_counter}')    
</code></pre>
",Training and Model Evaluation,get mask average multi token masking following paper trying implement calculated average log probability entity section specifically score entity calculated average log probability across token list entity prompt task find e g top entity fit instead prompt issue entity composed multiple token understand correctly paper prompt repeated token time number token model split entity consider multitoken object including multiple mask token template get average log probability entity take top k result say one modified prompt get model hidden state masked position follows sure continue output eventually looking list length sublist length sublist score entity fit modified prompt unless efficient approach get top k entity prompt e g instead duplicating prompt n time n number entity update adding answer slow think finally figured slow really use gpu iterates prompt one one accept answer optimize current state really use code slow
Low Tensorflow Model Test Accuracy,"<p>My model takes binary input which is from binary encoded text (not one-hot encoded). I achieve a binary accuracy of 99.5% and accuracy of 85%. Immediately after training, I achieve a test result of about 75%. However, if I restart the kernel and load the model (using the make_model function below) followed by load*_*weights, I only get an accuracy of 35%. Why the huge difference?</p>
<pre><code># start span identification model definition - increased parameters
def make_model(learn_rate: float):
    inputs_sp_can = Input(shape=(MAX_BIN_SPAN_LEN,), name=&quot;Candidate Span&quot;)
    sp_embedding_layer = Embedding(
        len(char2idx),
        EMBEDDING_DIM,
        embeddings_initializer=keras.initializers.Constant(embed_matrix),
        trainable=False,
    )(inputs_sp_can)
    x = Bidirectional(LSTM(960))(sp_embedding_layer)
    # x = BatchNormalization()(x)
    x = Dense(1920,activation = 'selu')(x)
    x = Dense(640, activation='selu')(x)
    x = Dropout(0.5)(x)
    x = Dense(1280, activation='selu')(x)
    x = models.Model(inputs=inputs_sp_can, outputs=x)

    inputs_sent = Input(shape=(MAX_BIN_SENT_LEN,), name=&quot;Input Sentences&quot;)
    sent_embedding_layer = Embedding(
        len(char2idx),
        EMBEDDING_DIM,
        embeddings_initializer=keras.initializers.Constant(embed_matrix),
        trainable=False,
    )(inputs_sent)
    y = Bidirectional(LSTM(1800))(sent_embedding_layer)
    # y = BatchNormalization()(y)
    y = Dense(1920,activation = 'selu')(y)
    y = Dense(640, activation='selu')(y)
    y = Dropout(0.5)(y)
    y = Dense(1280, activation='selu')(y)
    y = models.Model(inputs=inputs_sent, outputs=y)

    combined_layer = Concatenate(axis=1)(
        [x.output, y.output]
    )  # ([x.output,w.output,y.output])
    z = Dense(1024, activation='selu')(combined_layer)
    z = Dense(512, activation='selu')(z)
    z = Dense(512, activation='selu')(z)
    # z = Dropout(0.5)(z)
    # z = Dense(1024, activation='selu')(z)

    pre_span_label = Dense(128, activation=&quot;selu&quot;, name=&quot;pre_span_label&quot;)(z)
    span_label = Dense(len(label_idx), activation=&quot;softmax&quot;, name=&quot;span_label&quot;)(
        pre_span_label
    )

    model = models.Model(
        inputs=[x.input, y.input], outputs=[span_label]
    )  # [x.input,w.input,y.input], outputs = [span_check,span_label])

    myoptimizer = optimizers.Adam(learning_rate=learn_rate, clipnorm=0.1)

    model.compile(
        optimizer=myoptimizer,
        loss=&quot;categorical_crossentropy&quot;,
        metrics=[&quot;binary_accuracy&quot;, &quot;accuracy&quot;],
    )

    return model 
</code></pre>
<p>I use a learning rate of 0.00015. The fit function is below:</p>
<pre><code>filepath = save_path + &quot;Char_Tok_Models/Models/weights.{epoch:02d}.hdf5&quot;
checkpoint = ModelCheckpoint(
    filepath,
    monitor=&quot;val_loss&quot;,
    verbose=1,
    save_best_only=False,
    mode=&quot;max&quot;,
    period=5
)

callbacks_list = [checkpoint]

history = model.fit(
    [x_span_train, corr_sent_train],
    corr_label_train,
    validation_data=([x_span_val, corr_sent_val], [corr_label_val]),
    epochs=25,
    callbacks=callbacks_list,
    batch_size=64,
)  #
</code></pre>
<p>The code for loading the weights and evaluating the model is as follows:</p>
<pre><code>model = make_model(0.0002)
model.load_weights(save_path + &quot;Char_Tok_Models/Models_Upd/weights.40.hdf5&quot;)

model.evaluate([x_span_test, corr_sent_test], corr_label_test)
</code></pre>
<p>I'll appreciate the help. Thanks.</p>
<p>I have varied the learning rates and tried using the 'sigmoid' activation in the last layer. My expectation is that the inference should have at least 80% accuracy.</p>
",Training and Model Evaluation,low tensorflow model test accuracy model take binary input binary encoded text one hot encoded achieve binary accuracy accuracy immediately training achieve test result however restart kernel load model using make model function followed load weight get accuracy huge difference use learning rate fit function code loading weight evaluating model follows appreciate help thanks varied learning rate tried using sigmoid activation last layer expectation inference least accuracy
Arbitrary threshold for sigmoid activation function for CNN binary classification?,"<p>I am classifying sentiment of reviews - <code>0</code> or <code>1</code> - using <code>gensim</code> Doc2Vec and CNN in <code>Tensorflow 2.2.0</code>:</p>
<pre><code>model = tf.keras.Sequential([
      tf.keras.layers.Embedding(vocab_size, embedding_dim, 
                                input_length=maxlen, 
                                embeddings_initializer=Constant(embedding), trainable=False),
      tf.keras.layers.Conv1D(128, 5, activation='relu'),
      tf.keras.layers.GlobalMaxPooling1D(),
      tf.keras.layers.Dense(10, activation='relu'),
      tf.keras.layers.Dense(1, activation='sigmoid')
    ])

model.compile(loss='binary_crossentropy',
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['accuracy'])

history = model.fit(X_train, y_train,
                    epochs=8,
                    validation_split=0.3,
                    batch_size=10)
</code></pre>
<p>I then make predictions and convert my sigmoid probability to <code>0</code> or <code>1</code> using <code>np.round()</code>:</p>
<pre><code>predicted = model.predict(X_test)
predicted = np.round(predicted,1).astype(np.int32)
</code></pre>
<p>I get great results (~96% accuracy) indicating that the threshold of <code>0.5</code> is working as expected...</p>
<p>However, when I try to predict on a set of new data, the model seems to separate bad reviews from good ones but across approx <code>0.0</code>:</p>
<pre><code># Example sigmoid outputs for new test reviews:
good_review_1: 0.000052
good_review_2: 0.000098

bad_review_1: 0.112334
bad_review_2: 0.214934
</code></pre>
<p>Mind you, the model never saw <code>X_test</code> during training and it is able to predict just fine. It's only when I introduce a new set of review text strings, I run into incorrect predictions. For new reviews, the only preprocessing that I do before calling <code>model.predict()</code> is feeding them through the same tokenizer used for model training:</p>
<pre><code>s = 'This is a sample bad review.'
tokenizer.texts_to_sequences(pd.Series(s))
s = pad_sequences(s, maxlen=maxlen, padding='pre', truncating='pre')

model.predict(s)
</code></pre>
<p>I've been trying to make sense of this conundrum but I'm making little progress. I ran into <a href=""https://stackoverflow.com/questions/12405473/does-a-neural-network-with-sigmoid-activation-use-thresholds/12406063#12406063"">post</a> and it indicates</p>
<blockquote>
<p>Some sigmoid functions will have this at 0, while some will have it set to a different 'threshold'.</p>
</blockquote>
<p>But this still doesn't explain why my model was able to predict on <code>np.round()</code>'s <code>0.5</code> threshold for <code>X_test</code> dataset (which the model never learned on) and then unable to predict on new dataset at the same <code>0.5</code> threshold...</p>
",Training and Model Evaluation,arbitrary threshold sigmoid activation function cnn binary classification classifying sentiment review using doc vec cnn make prediction convert sigmoid probability using get great result accuracy indicating threshold working expected however try predict set new data model seems separate bad review good one across approx mind model never saw training able predict fine introduce new set review text string run incorrect prediction new review preprocessing calling feeding tokenizer used model training trying make sense conundrum making little progress ran href indicates p sigmoid function set different threshold still explain model wa able predict threshold dataset model never learned unable predict new dataset threshold
Training loss remained almost unchanged during training,"<p>I am training a model.
This is my code to train the model.</p>
<pre><code>optimizer = optim.SGD(model.parameters(), lr=args.learning_rate)
criterion = InfoNCELoss(temperature=0.1)
scheduler = optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)

log_iter = 100

for e in range(1, args.epoch + 1):
    model.train()
    t0 = time.time()
    total_train_loss = 0

    for i, batch in enumerate(train_dataloader):

        optimizer.zero_grad()
        sentence1_embedding, sentence2_embedding = model(batch[&quot;sentence1&quot;], batch[&quot;sentence2&quot;])
        train_batch_loss = criterion(sentence1_embedding, sentence2_embedding, batch[&quot;sentence1_label&quot;], batch[&quot;sentence2_label&quot;])

        total_train_loss += train_batch_loss.item()
        train_batch_loss.backward()
        optimizer.step()
        scheduler.step()
</code></pre>
<p>This is my training loss:</p>
<p><a href=""https://i.sstatic.net/wSJLX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wSJLX.png"" alt=""enter image description here"" /></a></p>
<p>Training loss remained almost unchanged during training.</p>
<p>Please help me to solve this.</p>
<p>Thank!</p>
",Training and Model Evaluation,training loss remained almost unchanged training training model code train model training loss training loss remained almost unchanged training please help solve thank
"BERT NLP classification model, how to combine word embeddings","<p>I am using BERT embeddings to train a classification model.</p>
<p>For each sentence in my training data I extract the BERT embeddings for two or more words and then use these to predict the relationship in the sentence. My data looks like this</p>
<p><strong>sentence</strong>: &quot;Robert is the brother of John&quot; <strong>class</strong>: &quot;FAMILY_RELATIONSHIP&quot;</p>
<p><strong>sentence</strong>: &quot;Constantinople is the former name of Istanbul&quot; <strong>class</strong>: &quot;HISTORICAL_RELATIONSHIP&quot;</p>
<p><strong>sentence</strong>: &quot;Stacy is the daughter of Henry and Alice&quot; <strong>class</strong>: &quot;FAMILY_RELATIONSHIP&quot;</p>
<p>In the first example, I get the embeddings for <strong>Robert</strong> and <strong>John</strong>, call these h1 and h2, and concatenate these, h = concat(h1, h2), and feed h into a final dense layer to predict the class.</p>
<p>However, in sentence 3 I have three people (<strong>Stacy</strong>, <strong>Henry</strong> and <strong>Alice</strong>) so h = concat(h1, h2, h3) which will be longer than for sentence 1.</p>
<p>What is the best way of combining the embeddings so that I have the same length input to the final dense layer?</p>
<p>Does it make sense to max pool the embeddings even if some sentences have 2, 3, 4, 5 or more embeddings?</p>
<p>Is there any other way to pass this data to the final dense layer?</p>
<p>EDIT</p>
<p>I will try and provide some more details.</p>
<p>I am trying to classify the relationship encoded in a sentence. The sentence may contain two or more words of interest. I know these words in advance and I can use a mask to collect the correct word embedding from BERT.</p>
<p>Some pseudo code might look like this:</p>
<pre><code># Here I have a pretrained bert model. I pass the input for each
# sentence to bert to get the word embeddings
bert_inputs = [token_ids, token_type_ids]
seq_out = bert_layer(bert_inputs)

# for each sentence, I have a mask so that I can 
# extract the embedding for the words of interest, that I know in advance 
h1 = seq_out[h1_mask] 
h2 = seq_out[h2_mask]
h3 = seq_out[h3_mask] # if there is no third word, this is empty/zero/nothing

# however, depending on the sentence I may need to concatenate 2 embeddings,
# or 3 or 4,  etc.
dense_input = concat(h1, h2) # two words case
dense_input = concat(h1, h2, h3) # three words case

# finally I pass dense_input to a standard dense layer
# but how can I do this when the length of dense_input can vary?
pred_layer = tf.keras.layers.Dense(n=10) # 10 classes to predict
pred_layer(dense_input) # predict class
</code></pre>
",Training and Model Evaluation,bert nlp classification model combine word embeddings using bert embeddings train classification model sentence training data extract bert embeddings two word use predict relationship sentence data look like sentence robert brother john class family relationship sentence constantinople former name istanbul class historical relationship sentence stacy daughter henry alice class family relationship first example get embeddings robert john call h h concatenate h concat h h feed h final dense layer predict class however sentence three people stacy henry alice h concat h h h longer sentence best way combining embeddings length input final dense layer doe make sense max pool embeddings even sentence embeddings way pas data final dense layer edit try provide detail trying classify relationship encoded sentence sentence may contain two word interest know word advance use mask collect correct word embedding bert pseudo code might look like
Training difficulties on Transformer seq2seq task using pytorch,"<p>I am currently employing a seq2seq task using the vanilla <code>torch.nn.Transformer</code>.
My implementation is provided below (SimpleTransformer). I just seem to not be able to make my model output non-trivial sequences and also loss doesn't seem to shrink after flattening. Maybe some more experienced ML researcher could tell me what else to try and what their opinions on those results seem to be?</p>
<h1>Task:</h1>
<p>The task is the following:</p>
<p>Given a sentence belonging to a defined formal grammar give me the parse tree representation of that sentence. E.g:</p>
<ul>
<li>I like apples. -&gt; (S (NP <strong>I</strong>) (VP <strong>like apples</strong>))</li>
<li>John says he likes apples -&gt; (S (NP <strong>John</strong>) (VP <strong>says</strong> (S (NP <strong>he</strong>) (VP <strong>likes apples</strong>))))</li>
</ul>
<p>Brackets will refer to a certain Non-Terminal rule (i.e. a grammatical object / POS).</p>
<p>Each opening and closing bracket type will be a single token. (e.g. S-opening, S-closing, NP-opening, NP-closing, ..., will be seperate tokens). The text will be tokenized using Byte-Pair-Encoding.</p>
<h1>Training params</h1>
<p>I trained my model using Cross-Entropy-Loss, AdamW (0.9, 0.98), learning rates of magnitued e-4 and e-5, with a max token size of 512, a batch size of 4 (as GPU memory wasn't big enough for more), with an data-set of ~70 000 example sentences over 3-4 epochs.</p>
<h1>Results</h1>
<ul>
<li><a href=""https://api.wandb.ai/links/whatfuckingever/275k4oi0"" rel=""nofollow noreferrer"">My loss curve looks like this.</a></li>
<li><a href=""https://wandb.ai/whatfuckingever/Tree%20Bracketing/reports/f1-23-09-08-16-30-50---Vmlldzo1MzQ1MTA5"" rel=""nofollow noreferrer"">My F1 scores during training
like this.</a></li>
<li><a href=""https://api.wandb.ai/links/whatfuckingever/mv6t7cwc"" rel=""nofollow noreferrer"">My evaluation loss is much higher (Note that
evaluation examples will be generally longer than training set and
will be sorted by length)</a></li>
</ul>
<h1>My model:</h1>
<pre><code>class SimpleTransformer(nn.Module):
def __init__(self, vocab_size: int, ntokens=512, d_model=512, num_layers=6, bidirectional=False, device=&quot;cpu&quot;):
    super().__init__()
    self.d_model = d_model
    self.src_embed = nn.Embedding(vocab_size, self.d_model)
    self.tgt_embed = nn.Embedding(vocab_size, self.d_model)
    self.positional_encoder = PositionalEncoding(d_model=self.d_model, max_len=ntokens)
    self.model = Transformer(d_model=self.d_model, batch_first=True, num_encoder_layers=num_layers,
                             num_decoder_layers=num_layers)
    self.bidirectional = bidirectional
    self.generator = Generator(hidden_size=self.d_model, vocab_size=vocab_size) # Just a fc layer
    self.device = device

def forward(self, in_ids, l_ids, in_masks, l_masks):
    in_ids = self.src_embed(in_ids.long()) * math.sqrt(self.d_model)  # scale by sqrt of dmodel
    in_ids = self.positional_encoder(in_ids)

    l_ids = self.tgt_embed(l_ids.long()) * math.sqrt(self.d_model)
    l_ids = self.positional_encoder(l_ids)

    # Create Masks
    src_seq_len = in_ids.size(1)
    tgt_seq_len = l_ids.size(1)
    src_mask = torch.zeros(src_seq_len, src_seq_len, device=self.device).type(torch.bool)
    if not self.bidirectional:
        tgt_mask = torch.triu(torch.full((tgt_seq_len, tgt_seq_len), float('-inf'), device=self.device), diagonal=1)
    else:
        tgt_mask = torch.zeros(tgt_seq_len, tgt_seq_len, device=self.device).type(torch.bool)
    in_masks = in_masks == 0.0 # in_masks will mask special pad_tokens
    l_masks = l_masks == 0.0 # l_masks will mask special pad_tokens

    out = self.model(src=in_ids, tgt=l_ids,
                     src_mask=src_mask, tgt_mask=tgt_mask,
                     src_key_padding_mask=in_masks,
                     tgt_key_padding_mask=l_masks)
    return self.generator(out)
</code></pre>
<p>I tried with</p>
<ul>
<li>num_layers = 6, 3, 1,</li>
<li>d_model = 512</li>
<li>feed-forward dim: 2048 (default)</li>
<li>PositionalEncoding: either sin/cos or none</li>
</ul>
<p>I saw that the outputs will be like:</p>
<pre><code>(s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s ...
</code></pre>
<p>No matter the input... I just don't know whether the task is to difficult to handle or whether I made a mistake somewhere in the process. I can't get my model to output non-trivial patterns. I'm quite new in the machine learning business so it could certainly be that it might be a stupid mistake somewhere. Maybe I stopped too early with the training or some hyper-parameters are wrong?</p>
<h1>Comparative Attempts</h1>
<p>Additionally, I used the <code>facebook/fairseq</code> toolkit to train a model on the same task. It's performance was better with loss decreasing significantly:</p>
<ul>
<li><a href=""https://api.wandb.ai/links/whatfuckingever/nl2qwp1t"" rel=""nofollow noreferrer"">Train loss</a></li>
<li><a href=""https://api.wandb.ai/links/whatfuckingever/hl6l0p4i"" rel=""nofollow noreferrer"">Validation Perplexity</a></li>
<li><a href=""https://api.wandb.ai/links/whatfuckingever/q1jwji58"" rel=""nofollow noreferrer"">Validation loss</a></li>
</ul>
<p>I also trained the pre-trained <code>bart-base</code> Model from the <code>huggingface</code> library with the identical training script and training data. It's performce was way better than both of the previous models.</p>
<ul>
<li><a href=""https://api.wandb.ai/links/whatfuckingever/4zpkleru"" rel=""nofollow noreferrer"">Train loss</a> (Valid loss is also around 0.025)</li>
<li><a href=""https://api.wandb.ai/links/whatfuckingever/ro3fjk2b"" rel=""nofollow noreferrer"">Train F1</a></li>
<li><a href=""https://api.wandb.ai/links/whatfuckingever/3avqmhy4"" rel=""nofollow noreferrer"">Eval F1</a></li>
</ul>
",Training and Model Evaluation,training difficulty transformer seq seq task using pytorch currently employing seq seq task using vanilla implementation provided simpletransformer seem able make model output non trivial sequence also loss seem shrink flattening maybe experienced ml researcher could tell else try opinion result seem task task following given sentence belonging defined formal grammar give parse tree representation sentence e g like apple np vp like apple john say like apple np john vp say np vp like apple bracket refer certain non terminal rule e grammatical object po opening closing bracket type single token e g opening closing np opening np closing seperate token text tokenized using byte pair encoding training params trained model using cross entropy loss adamw learning rate magnitued e e max token size batch size gpu memory big enough data set example sentence epoch result loss curve look like f score training like evaluation loss much higher note evaluation example generally longer training set sorted length model tried num layer model feed forward dim default positionalencoding either sin co none saw output like matter input know whether task difficult handle whether made mistake somewhere process get model output non trivial pattern quite new machine learning business could certainly might stupid mistake somewhere maybe stopped early training hyper parameter wrong comparative attempt additionally used toolkit train model task performance wa better loss decreasing significantly train loss validation perplexity validation loss also trained pre trained model library identical training script training data performce wa way better previous model train loss valid loss also around train f eval f
LoRA vs QLoRA finetuning performance on llama2,"<p>I am finetuning llama2 uusing <strong>LoRA</strong> and <strong>QLoRA</strong> to see the differences in both. I first trained on loRA with special end token <strong>&lt;|end|&gt;</strong> so that the model knows when to stop. With loRA fintuning it works fine and model also predicts the <strong>&lt;|end|&gt;</strong> token. keeping the trainings configuration  same apart form 4 bit quantization with QLoRA, I see the model cannot predict the <strong>&lt;|end|&gt;</strong>.</p>
<p>Also when I prepare the peft model, I do load the model using <strong>prepare_model_for_kbit_training</strong> and then do <strong>get_peft_model</strong>. Do I need to do <strong>prepare_model_for_kbit_training</strong>  when I do 4 bit quantization in QLoRA.  Becuase If I don't do that then it CUDA OOM. Every thing is kept same like batch size and all other params for loRA and QLoRA.</p>
<p>What could be the reason for less accuracy with QLoRA. If I understood it decreases the less GPU utilizattion but does it affect the model performance.</p>
",Training and Model Evaluation,lora v qlora finetuning performance llama finetuning llama uusing lora qlora see difference first trained lora special end token end model know stop lora fintuning work fine model also predicts end token keeping training configuration apart form bit quantization qlora see model predict end also prepare peft model load model using prepare model kbit training get peft model need prepare model kbit training bit quantization qlora becuase cuda oom every thing kept like batch size params lora qlora could reason le accuracy qlora understood decrease le gpu utilizattion doe affect model performance
Which co-occurrence matrix generation method is better for a list of words?,"<p>I am working on a project where I need to generate a co-occurrence matrix for a given list of words. I have found two different methods for generating this matrix, but I'm not sure which one would be more suitable for my task.</p>
<p>The list of words I am working with is quite extensive (provided below an example), and I need to make sure that the method I choose is efficient and accurate for generating the co-occurrence matrix.</p>
<pre><code>word_list = ['geben', 'interessieren', 'bringen', 'lassen', 'stellen', 'sehen', 'hand', 'erfahren', 'nehmen', 'steigen', 'super', 'kandidatinnen', 'veroffentlicht', 'weiß', 'kampfen', 'stehen', 'wort', 'stark', 'schließlich', 'einfach', 'sogar', 'weg', 'bleiben', 'direkt', 'gehort', 'zeigen', 'madels', 'clip_s', 'schaffen', 'begeistern', 'erklart', 'schwierig', 'kandidatin', 'wohnen', 'fragen', 'sagen', 'topmodel', 'letzt', 'backstage', 'meinung', 'erfahrung', 'selbstbewusst', 'sache', 'wichtig', 'germany', 'emotional', 'aussehen', 'fuhlt', 'freuen', 'familie', 'nathalie', 'std', 'umstyling_std', 'justine', 'casting_edition', 'tamara', 'theresia', 'maribel', 'untertitel', 'enisa', 'edition', 'modelloft', 'transformation', 'lijana', 'simone', 'tatjana', 'thoma', 'maureen', 'sayana', 'staffel_episode', 'gebardensprache', 'sendung', 'nacktshooting', 'linda', 'social', 'streit', 'sender', 'high_fashion', 'madchen', 'duellwoche', 'michael', 'jasmin', 'preview', 'makeover', 'team', 'kostenlos', 'dream_edition', 'sixx', 'bodypainting', 'nackt', 'anmelden', 'nachher', 'episode', 'celine', 'sedcard', 'anna', 'schuh', 'wasser', 'live', 'jacky', 'kostenlos', 'sender', 'anmelden', 'edition', 'high_fashion', 'transformation', 'duellwoche', 'dream_edition', 'casting_edition', 'gebardensprache', 'untertitel', 'film', 'modelloft', 'std', 'umstyling_std', 'nacktshooting', 'cover', 'ander', 'erhalten', 'hollywood', 'berlin', 'social', 'show', 'entdecken', 'fashion_week', 'tv', 'einzug', 'live', 'nachwuchsmodels', 'video', 'pose', 'shooting', 'catwalk', 'halbfinale', 'episode', 'bodypainting', 'juror', 'ansehen', 'sedcard', 'schnell', 'highlights', 'zuschauer', 'heidi_klum', 'gastjurorinnen', 'trailer', 'gntm', 'gewinnerinnen', 'madchen', 'staffel_episode', 'gast']
</code></pre>
<p>Here are the two methods I've come across:</p>
<p>Method 1: Custom Python code</p>
<pre><code>
    def _co_occurrence_n_gram(self, word_list, window_size):
        co_occurence_counts = defaultdict(int)
        vocab = set()

        # Iterate over the words in the word list
        for word in word_list:
            # iterate over tokens in the word
            for i, token in enumerate(word):
                vocab.add(token)  # add the token to the vocab

                # Get the next tokens within the window size
                next_tokens = word[i + 1 : i + 1 + window_size]
                for next_token in next_tokens:
                    # Create a tuple of the sorted tokens and increment the co-occurrence count
                    co_occurrence_key = tuple(sorted([next_token, token]))
                    co_occurence_counts[co_occurrence_key] += 1

        # Create a DataFrame to represent the co-occurrence counts
        vocab = sorted(vocab)  # sort eh vocab
        data_frame = pd.DataFrame(
            data=np.zeros((len(vocab), len(vocab)), dtype=np.int16), index=vocab, columns=vocab
        )

        # populate the DataFrame with the co-occurrence counte
        for co_occurrence_key, count in co_occurence_counts.items():
            data_frame.at[co_occurrence_key[0], co_occurrence_key[1]] = count
            data_frame.at[co_occurrence_key[1], co_occurrence_key[0]] = count

        return data_frame
</code></pre>
<p>Method 2: Using scikit-learn's CountVectorizer</p>
<pre><code>    def _co_occurrence_n_gram(self, word_list):
        # Flatten the list of words
        all_words = &quot; &quot;.join(word_list)

        # Create a CountVectorizer with ngram_range=(1, 1) and stop_words='english'
        cv = CountVectorizer(ngram_range=(1, 1))

        # Convert the list of words into a matrix of token counts
        X = cv.fit_transform([all_words])

        # Calculate the co-occurrence matrix
        Xc = X.T * X
        Xc.setdiag(0)  # Set the diagonal elements to zero

        # Get the feature names (tokens)
        names = cv.get_feature_names_out()

        # Create a DataFrame to represent the co-occurrence matrix
        data_frame = pd.DataFrame(data=Xc.toarray(), columns=names, index=names)

        return data_frame
</code></pre>
<p>I want to know the differences between these two methods and which one is more suitable for generating a co-occurrence matrix for a large list of words. Specifically, I'm interested in factors like efficiency, accuracy, and ease of use.</p>
<p>Could someone please provide insights or recommendations on which method to choose and why? Any examples or comparisons would be greatly appreciated. Thank you!</p>
",Training and Model Evaluation,co occurrence matrix generation method better list word working project need generate co occurrence matrix given list word found two different method generating matrix sure one would suitable task list word working quite extensive provided example need make sure method choose efficient accurate generating co occurrence matrix two method come across method custom python code method using scikit learn countvectorizer want know difference two method one suitable generating co occurrence matrix large list word specifically interested factor like efficiency accuracy ease use could someone please provide insight recommendation method choose example comparison would greatly appreciated thank
Why does my model perform better when I use a Spacy pretrained pipeline without word vectors?,"<p>My model performs better (higher accuracy, precision, recall) when I train it using Spacy's smallest pretrained pipeline, <code>en_core_web_sm</code>. When I train it using Spacy's <code>en_core_web_md</code> or <code>en_core_web_lg</code>, it struggles to reach a measly 70% accuracy. With <code>en_core_web_sm</code>, it reaches 91% accuracy with much smaller loss scores.</p>
<p>I modified training data several times to improve accuracy and reliability. I am confident that my training data does not contain any outliers or incorrectly labelled entries. Regardless of the state of my training data and the initial learning rate, the only way I could improve my accuracy score was to revert to the <code>en_core_web_sm</code> pipeline.</p>
<p>My training data is syntactically simple. It is not verbose but does contain out-of-vocabulary words that are unique to my data.</p>
",Training and Model Evaluation,doe model perform better use spacy pretrained pipeline without word vector model performs better higher accuracy precision recall train using spacy smallest pretrained pipeline train using spacy struggle reach measly accuracy reach accuracy much smaller loss score modified training data several time improve accuracy reliability confident training data doe contain outlier incorrectly labelled entry regardless state training data initial learning rate way could improve accuracy score wa revert pipeline training data syntactically simple verbose doe contain vocabulary word unique data
Applying SMOTE to a feature-set with both text and numerical,"<p>I am trying to train a RF Classifier on a dataset that has both a textual description feature and other categorical features. There is heavy class imbalance, so a need for SMOTE Upsampling prior to the fitting of the model. As SMOTE cannot be applied to text, I would need to vectorize this one feature before upsampling, which works fine by itself, but I am not sure how to incorporate the other features as well.</p>
<p>I tried taking the ColumnTransformer and Pipeline approach:</p>
<pre><code>column_transformer = ColumnTransformer(
[('tfidf', tfidf_vectorizer, 'description')],
remainder='passthrough')

pipeline = ImbPipeline([
('column_transformer', column_transformer),
('smote', smote),
('model', model)
])
</code></pre>
<p>That fails, as smote cannot be applied before the transformation; and the other way around with the CT before does not fail but seems like it doesn't apply SMOTE at all, getting the same results as without it. Any help ?</p>
",Training and Model Evaluation,applying smote feature set text numerical trying train rf classifier dataset ha textual description feature categorical feature heavy class imbalance need smote upsampling prior fitting model smote applied text would need vectorize one feature upsampling work fine sure incorporate feature well tried taking columntransformer pipeline approach fails smote applied transformation way around ct doe fail seems like apply smote getting result without help
Training a gensim Word2Vec model after saving on a Hindi dataset,"<p>I am trying to re-train my word2vec model on a bigger dataset, I've saved the model using <code>model.save('ikshan_word2vec.bin')</code>, and now I am loading this .bin file in my colab, where I have the dataset stored in corpus (Already pre-processed).</p>
<p>When I try to retrain my model using <code>update=True</code>, and I print my vocabulary size before training and after training, it shows no change in the length.</p>
<p>Here is a snippet of my new corpus as well to ensure that its of right form.</p>
<p><a href=""https://i.sstatic.net/xj3ms.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xj3ms.png"" alt=""image"" /></a></p>
<p>This is what I am trying to do, also this code executed in 0.0 seconds.</p>
<pre><code>from gensim.models import KeyedVectors

# Load the pre-saved Word2Vec model
pretrained_model = KeyedVectors.load('/content/ikshan_word2vec.bin')

# Check the initial vocabulary size
print(len(pretrained_model.wv.key_to_index))

# New set of sentences to train on
new_sentences = corpus

# Update the vocabulary with new sentences
pretrained_model.build_vocab(new_sentences, update=True)

# Train the model on the new sentences
pretrained_model.train(new_sentences, total_examples=len(new_sentences), epochs=pretrained_model.epochs)

# Check the updated vocabulary size
print(len(pretrained_model.wv.key_to_index))
</code></pre>
",Training and Model Evaluation,training gensim word vec model saving hindi dataset trying train word vec model bigger dataset saved model using loading bin file colab dataset stored corpus already pre processed try retrain model using print vocabulary size training training show change length snippet new corpus well ensure right form trying also code executed second
Backpropagation / minibatching in training large language models (LLMs),"<p>I am struggling to understand how backprop works for transformer-based LLMs.</p>
<p>Here is my <em>guess</em> of how this process works. Given a sequence of tokens with length 64, we process the sequence in parallel using teacher forcing (i.e., for each  ACTUAL consecutive subsequence starting from the first token we PREDICT the next token and calculate a loss based on the new predicted token and the actual next token, therefore creating 63 cross-entropy loss values).</p>
<p>We do this for many (let's say, batch size 8192) sequences at a time, in one minibatch, during pretraining. We then take a backpropagation step through the network and adjust weights - till now we've only done a single step. We then move on to the next batch of size 8192 sequences.</p>
<ol>
<li>Is this understanding correct?</li>
<li>If so, do we average the 63 losses for a single sequence?</li>
<li>Do we average the losses across the 8192 sequences?</li>
<li>If not averaging, how are the losses accumulated to backpropagate for a single minibatch, and why?</li>
</ol>
<p>Tried searching for papers to explain this process in great detail for language models, but couldn't seem to find any - most were for neural networks generally and did not clarify some of these questions I have about language sequences.</p>
",Training and Model Evaluation,backpropagation minibatching training large language model llm struggling understand backprop work transformer based llm guess process work given sequence token length process sequence parallel using teacher forcing e actual consecutive subsequence starting first token predict next token calculate loss based new predicted token actual next token therefore creating cross entropy loss value many let say batch size sequence time one minibatch pretraining take backpropagation step network adjust weight till done single step move next batch size sequence understanding correct average loss single sequence average loss across sequence averaging loss accumulated backpropagate single minibatch tried searching paper explain process great detail language model seem find neural network generally clarify question language sequence
evaluation NLP classifier with annotated data,"<p>if we want to evaluate a classifier of NLP application with data that are annotated with two annotators, and they are not completely agreed on the annotation, how is the procedure?
That is, if we should compare the classifier output with just the portion of data that annotators agreed on? or just one of the annotator data? or the both of them separately and then compute the average?</p>
",Training and Model Evaluation,evaluation nlp classifier annotated data want evaluate classifier nlp application data annotated two annotator completely agreed annotation procedure compare classifier output portion data annotator agreed one annotator data separately compute average
Is there a search engine that will give a direct answer?,"<p>I've been wondering about this for a while and I can't see why Google haven't tried it yet - or maybe they have and I just don't know about it.</p>

<p>Is there a search engine that you can type a question into which will give you a single answer rather than a list of results which you then have to trawl through yourself to find what you want to know?</p>

<p>For example, this is how I would design the system:</p>

<p>User’s input: “Where do you go to get your eyes tested?”</p>

<p>System output: “Opticians. Certainty: 95%”</p>

<p>This would be calculated as follows:</p>

<ol>
<li>The input is parsed from natural language into a simple search string, probably something like “eye testing” in this case.  The term “Where do you go” would also be interpreted by the system and used when comparing results.</li>
<li>The search string would be fed into a search engine.</li>
<li>The system would then compare the contents of the results to find matching words or phrases taking note of what the question is asking (i.e. what, where, who, how etc.)</li>
<li>Once a suitable answer is determined, the system displays it to the user along with a measure of how sure it is that the answer is correct.</li>
</ol>

<p>Due to the dispersed nature of the Internet, a correct answer is likely to appear multiple times, especially for simple questions.  For this particular example, it wouldn’t be too hard for the system to recognise that this word keeps cropping up in the results and that it is almost certainly the answer being searched for.</p>

<p>For more complicated questions, a lower certainty would be shown, and possibly multiple results with different levels of certainty.  The user would also be offered the chance to see the sources which the system calculated the results from.</p>

<p>The point of this system is that it simplifies searching.  Many times when we use a search engine, we’re just looking for something really simple or trivial.  Returning a long list of results doesn’t seem like the most efficient way of answering the question, even though the answer is almost certainly hidden away in those results.  </p>

<p>Just take a look at the Google results for the above question to see my point:
<a href=""http://www.google.co.uk/webhp?sourceid=chrome-instant&amp;ie=UTF-8&amp;ion=1&amp;nord=1#sclient=psy&amp;hl=en&amp;safe=off&amp;nord=1&amp;site=webhp&amp;source=hp&amp;q=Where%20do%20you%20go%20to%20get%20your%20eyes%20tested%3F&amp;aq=&amp;aqi=&amp;aql=&amp;oq=&amp;pbx=1&amp;fp=72566eb257565894&amp;fp=72566eb257565894&amp;ion=1"" rel=""nofollow noreferrer"">http://www.google.co.uk/webhp?sourceid=chrome-instant&amp;ie=UTF-8&amp;ion=1&amp;nord=1#sclient=psy&amp;hl=en&amp;safe=off&amp;nord=1&amp;site=webhp&amp;source=hp&amp;q=Where%20do%20you%20go%20to%20get%20your%20eyes%20tested%3F&amp;aq=&amp;aqi=&amp;aql=&amp;oq=&amp;pbx=1&amp;fp=72566eb257565894&amp;fp=72566eb257565894&amp;ion=1</a></p>

<p>The results given don't immediately answer the question - they need to be searched through by the user before the answer they really want is found.  Search engines are great directories.  They're really good for giving you more information about a subject, or telling you where to find a service, but they're not so good at answering direct questions.</p>

<p>There are many aspects that would have to be considered when creating the system – for example a website’s accuracy would have to be taken into account when calculating results.</p>

<p>Although the system should work well for simple questions, it may be quite a task to make it work for more complicated ones.  For example, common misconceptions would need to be handled as a special case.  If the system finds evidence that the user’s question has a common misconception as an answer, it should either point this out when providing the answer, or even simply disregard the most common answer in favour of the one provided by the website that points out that it is a common misconception.  This would all have to be weighed up by comparing the accuracy and quality of conflicting sources.</p>

<p>It's an interesting question and would involve a lot of research, but surely it would be worth the time and effort?  It wouldn't always be right, but it would make simple queries a lot quicker for the user.</p>
",Training and Model Evaluation,search engine give direct answer wondering see google tried yet maybe know search engine type question give single answer rather list result trawl find want know example would design system user input go get eye tested system output optician certainty would calculated follows input parsed natural language simple search string probably something like eye testing case term go would also interpreted system used comparing result search string would fed search engine system would compare content result find matching word phrase taking note question asking e etc suitable answer determined system display user along measure sure answer correct due dispersed nature internet correct answer likely appear multiple time especially simple question particular example hard system recognise word keep cropping result almost certainly answer searched complicated question lower certainty would shown possibly multiple result different level certainty user would also offered chance see source system calculated result point system simplifies searching many time use search engine looking something really simple trivial returning long list result seem like efficient way answering question even though answer almost certainly hidden away result take look google result question see point result given immediately answer question need searched user answer really want found search engine great directory really good giving information subject telling find service good answering direct question many aspect would considered creating system example website accuracy would taken account calculating result although system work well simple question may quite task make work complicated one example common misconception would need handled special case system find evidence user question ha common misconception answer either point providing answer even simply disregard common answer favour one provided website point common misconception would weighed comparing accuracy quality conflicting source interesting question would involve lot research surely would worth time effort always right would make simple query lot quicker user
Is there an easy way to identify course codes and course names from university&#39;s UI pages,"<p>I need to pull course codes and course names from university course catalogs. However, I need to do this for all universities and writing codes for each page of every university is a daunting task. One solution is to get the raw text from the html pages and then extract the course codes and course names from this raw text. However, the format of course codes and course names keep changing and this cannot be done via regex. I thought of using NLP to train models but again this would need a lot of training data where I would need to manually identify the course codes and course names. Is there a package or method I could use to get these course codes and course names easily from raw text?</p>
",Training and Model Evaluation,easy way identify course code course name university ui page need pull course code course name university course catalog however need university writing code page every university daunting task one solution get raw text html page extract course code course name raw text however format course code course name keep changing done via regex thought using nlp train model would need lot training data would need manually identify course code course name package method could use get course code course name easily raw text
Selecting only ngrams based on the first word in rstudio,"<p>I'm currently working on a nlp-project. As a training data set I'm using the Bible. You can easily create a random corpus with, if you want to try it yourself:</p>
<pre><code>rcorpus(nwords = 50, alphabet = letters, minwordlen = 1, maxwordlen = 6)
</code></pre>
<p>After processing the text file, I'm dividing the corpus into n-grams with ngram-package</p>
<pre><code>library(ngram)
# this is a preprocessed Corpus I have created earlier

bible_corpus&lt;- Corpus(DirSource(&quot;C:/Users/XYZ/XYZ/&quot;))
</code></pre>
<p>Now I'm processing the corpus with a function, I have set up earlier.</p>
<pre><code>corpus_sentences &lt;- Text_To_Clean_Sentences(paste(bible_corpus, collapse=&quot; &quot;))
</code></pre>
<p>Next step is to make a function for splitting our corpus into ngram</p>
<pre><code># function for getting n-grams
Get_Ngrams &lt;- function(sentence_splits, ngram_size=2) {
ngrams &lt;- c()
for (sentence in sentence_splits) {
sentence &lt;- Trim(sentence)
if ((nchar(sentence) &gt; 0) &amp;&amp; (sapply(gregexpr(&quot;\\W+&quot;, sentence), length) &gt;= 
ngram_size)) {
    ngs &lt;- ngram(sentence , n=ngram_size)
    ngrams &lt;- c(ngrams, get.ngrams(ngs))
     }
}
 return (ngrams)
}

# making n-grams based on Get_Ngrams
n2 &lt;- Get_Ngrams(corpus_sentences, ngram_size=2)   
n3 &lt;- Get_Ngrams(corpus_sentences, ngram_size=3)
n4 &lt;- Get_Ngrams(corpus_sentences, ngram_size=4)
n5 &lt;- Get_Ngrams(corpus_sentences, ngram_size=5)

# collect all n-grams
n_all &lt;- c(n5,n4,n3,n2)
</code></pre>
<p>Time to enter a search term</p>
<pre><code># enter SEARCH Word
word &lt;- 'good '

#
matches &lt;- c()
for (sentence in n_all) {
    # find exact match with double backslash and escape
    if (grepl(paste0('\\&lt;',word), sentence)) {
        print(sentence)
        matches &lt;- c(matches, sentence)
    }
}

# find highest probability word
precision_match &lt;- c()
for (a_match in matches) {
    # how many spaces in from of search word
    precision_match &lt;- c(precision_match,nchar(strsplit(x = a_match, split = word)[[1]][[1]]))
}
</code></pre>
<p>The last step returns all ngrams, which contain our search word from line 29.</p>
<p>Now I want to remove all sentence which don't start with search word we have entered.</p>
<p>For example &quot;precision_match&quot; returns:</p>
<pre><code>[1] search_word wordX wordY wordZ
[2] search_word wordY wordX wordZ
[3] wordY search_word wordX wordZ
[4] wordY wordX wordZ search_word
</code></pre>
<p>Of course I could manually select [1] und [2] since I can see that these two lines start with our search_word. But this isn't practical with a big number of matches. So how can I extract the n-grams starting with our search_word?</p>
",Training and Model Evaluation,selecting ngrams based first word rstudio currently working nlp project training data set using bible easily create random corpus want try processing text file dividing corpus n gram ngram package processing corpus function set earlier next step make function splitting corpus ngram time enter search term last step return ngrams contain search word line want remove sentence start search word entered example precision match return course could manually select und since see two line start search word practical big number match extract n gram starting search word
0.0 Accuracy when using KNN on extracted text features by camemBERT,"<p>I'm trying to use camembert model to just to extract text features. After that, I'm trying to use a KNN classifier to classify the feature vectors as inputs.</p>
<p>This is the code I wrote</p>
<pre><code>import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, CamembertModel
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import torch

# Initialize the Camembert tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(&quot;camembert-base&quot;)
model = CamembertModel.from_pretrained(&quot;camembert-base&quot;)

# Select the relevant columns from the DataFrame
cols = [&quot;Intitulé (Ce champ doit respecter la nomenclature suivante : Code action – Libellé)_y&quot;, &quot;Domaine sou domaine &quot;]
df = df[cols]

# Convert DataFrame to a dictionary with row indices as keys and cell values as values
data = df.to_dict(orient='split')
data = dict(zip(data['index'], data['data']))

# Collect all the input texts into a list of strings
input_texts = [str(text) for text in data.values()]

# Set the batch size for processing
batch_size = 8

# Initialize lists to store the input features and labels
input_features_list = []
input_labels_list = []

# Process the data in batches
for i in range(0, len(input_texts), batch_size):
    batch_texts = input_texts[i:i + batch_size]

    # Tokenize the batch of texts
    inputs = tokenizer(batch_texts, return_tensors=&quot;pt&quot;, padding=True, truncation=True)

    # Get the model outputs for the batch
    with torch.no_grad():
        outputs = model(**inputs)

    # Extract the last hidden states and convert them to a numpy array
    last_hidden_states = outputs.last_hidden_state
    input_features = last_hidden_states[:, 0, :].numpy()

    # Add the input features and labels to the corresponding lists
    input_features_list.append(input_features)
    input_labels_list.extend(list(data.keys())[i:i + batch_size])

# Concatenate the input features and labels for all batches
input_features = np.concatenate(input_features_list)
input_labels = input_labels_list

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(input_features, input_labels, test_size=0.2, random_state=42)

# Initialize and train the KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = neigh.predict(X_test)

# Calculate the accuracy
accuracy = accuracy_score(y_test, y_pred)

print(&quot;Accuracy:&quot;, accuracy)

</code></pre>
<p>The data I'm using in the dictionary has this form:</p>
<pre><code>{
    'index': [row_index_1, row_index_2, ...],
    'columns': [column_name_1, column_name_2, ...],
    'data': [
        [cell_value_row_1_col_1, cell_value_row_1_col_2, ...],
        [cell_value_row_2_col_1, cell_value_row_2_col_2, ...],
        ...
    ]
}
</code></pre>
<p>When executing the code, it runs for 70 minutes then I get 0.0 Accuracy. Please let me know if want me to provide more details.</p>
",Training and Model Evaluation,accuracy using knn extracted text feature camembert trying use camembert model extract text feature trying use knn classifier classify feature vector input code wrote data using dictionary ha form executing code run minute get accuracy please let know want provide detail
Facing a GPU memory leak in the training of multi-head attention to generate sentences in an autoregressive way,"<p>I am working on time series forecasting using GPT-like model. The idea is similar to train a language model that, given the beginning part of a sentence, the model can generate the rest of that sentence.</p>
<p>“I’m hungry, I want a hamburger and a cup of cola.”</p>
<p>Input: I’m hungry</p>
<p>Predict: I want a hamburger and a cup of cola.</p>
<p>An autoregressive language model will generate words step by step.</p>
<p>I’m hungry, I
I’m hungry, I want
I’m hungry, I want a
……
I’m hungry, I want a hamburger and a cup of cola.</p>
<p>That is, the newly generated word will be appended to the end of the previous input sequence to construct the new input sequence. During training, I will calculate the loss on the generated content “I want a hamburger and a cup of cola” and use back-propagation to update model parameters. The generation process can be implemented through a for-loop and a “decoder-only” module.</p>
<p>However, the GPU memory usage always spikes in this for-loop.</p>
<p>Do you have any suggestions to optimize the implementation?</p>
<p>Do you think my implementation is the right way to generate word sequences?</p>
<p>My time series forecasting sequence contains around 100 elements, that is, the for-loop repeat operation 100 times.</p>
",Training and Model Evaluation,facing gpu memory leak training multi head attention generate sentence autoregressive way working time series forecasting using gpt like model idea similar train language model given beginning part sentence model generate rest sentence hungry want hamburger cup cola input hungry predict want hamburger cup cola autoregressive language model generate word step step hungry hungry want hungry want hungry want hamburger cup cola newly generated word appended end previous input sequence construct new input sequence training calculate loss generated content want hamburger cup cola use back propagation update model parameter generation process implemented loop decoder module however gpu memory usage always spike loop suggestion optimize implementation think implementation right way generate word sequence time series forecasting sequence contains around element loop repeat operation time
Supervised fine tuning in pre-trained language model,"<p>Supervised find turning adds a extra output layer to the pre-trained model.</p>
<p>Does this extra layer alter the probability of words that are not related to the fine tune data?</p>
",Training and Model Evaluation,supervised fine tuning pre trained language model supervised find turning add extra output layer pre trained model doe extra layer alter probability word related fine tune data
classification report for adapters with transformers,"<p>I used this code, but I want to calculate classification report especially f1 score but I donnot kow how todo that</p>
<pre><code>import numpy as np
from transformers import TrainingArguments, AdapterTrainer, EvalPrediction

training_args = TrainingArguments(
    learning_rate=3e-4,
    max_steps=80000,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    logging_steps=1000,
    output_dir=&quot;adapter-roberta-base-amazon-polarity&quot;,
    overwrite_output_dir=True,
    remove_unused_columns=False,
)
def compute_accuracy(eval_pred):
  preds = np.argmax(eval_pred.predictions, axis=1)
  return {&quot;acc&quot;: (preds == eval_pred.label_ids).mean(),&quot;f1&quot;:f1_score(preds == eval_pred.label_ids)}
trainer = AdapterTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset[&quot;train&quot;],
    eval_dataset=dataset[&quot;test&quot;],
    compute_metrics=compute_accuracy,
)
</code></pre>
<p>I used previous compute accuracy to calculate f1_score but only accuracy return. Any suggestions for return predictions and labels and calculate classification report?</p>
",Training and Model Evaluation,classification report adapter transformer used code want calculate classification report especially f score donnot kow todo used previous compute accuracy calculate f score accuracy return suggestion return prediction label calculate classification report
“element 0 of tensors does not require grad and does not have a grad_fn”,"<p>I am facing an issue while training my deep learning model using PyTorch Lightning. During the training process, I encountered the following error:</p>
<pre><code>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>Context: I am working on a text classification task and have implemented a custom dataset class UCC_Dataset, as well as a LightningDataModule subclass UCC_Data_Module. My model is defined as a subclass of pl.LightningModule called UCC_Comment_Classifier. The error occurs when calling trainer.fit(model, ucc_data_module).</p>
<p>Here is the relevant code snippet where the error occurs:</p>
<pre><code>class UCC_Comment_Classifier(pl.LightningModule):

  def __init__(self):
    super().__init__()
    self.config = config
    self.pretrained_model = AutoModel.from_pretrained(config['model_name'], return_dict = True)
    self.hidden = torch.nn.Linear(self.pretrained_model.config.hidden_size, self.pretrained_model.config.hidden_size)
    self.classifier = torch.nn.Linear(self.pretrained_model.config.hidden_size, self.config['n_labels'])
    torch.nn.init.xavier_uniform_(self.classifier.weight)
    self.loss_func = nn.BCEWithLogitsLoss(reduction='mean')
    self.dropout = nn.Dropout()
    
  # Activer les gradients pour tous les paramètres du modèle
    for param in self.parameters():
        param.requires_grad = True
    
  def forward(self, input_ids, attention_mask, labels=None):
    # roberta layer
    output = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask)
    pooled_output = torch.mean(output.last_hidden_state, 1)
    # final logits
    pooled_output = self.dropout(pooled_output)
    pooled_output = self.hidden(pooled_output)
    pooled_output = F.relu(pooled_output)
    pooled_output = self.dropout(pooled_output)
    logits = self.classifier(pooled_output)
    # calculate loss
    loss = None
    if labels is not None:
      loss = self.loss_func(logits.view(-1, self.config['n_labels']), labels.view(-1, self.config['n_labels']))
    return loss, logits

  def training_step(self, batch, batch_index):
    loss, outputs = self(**batch)
    self.log(&quot;train loss &quot;, loss, prog_bar = True, logger=True)
    return {&quot;loss&quot;:loss, &quot;predictions&quot;:outputs, &quot;labels&quot;: batch[&quot;labels&quot;]}

  def validation_step(self, batch, batch_index):
    loss, outputs = self(**batch)
    self.log(&quot;validation loss &quot;, loss, prog_bar = True, logger=True)
    return {&quot;val_loss&quot;: loss, &quot;predictions&quot;:outputs, &quot;labels&quot;: batch[&quot;labels&quot;]}

  def predict_step(self, batch, batch_index , dataloader_idx: int = None):
    loss, outputs = self(**batch)
    return outputs



  def configure_optimizers(self):
    optimizer = AdamW(self.parameters(), lr=self.config['lr'], weight_decay=self.config['weight_decay'], no_deprecation_warning=True)
    total_steps = config['train_size'] * self.config['n_epochs']
    warmup_steps = math.floor(total_steps * self.config['warmup'])

    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)
    return [optimizer],[scheduler]```
</code></pre>
<p>I have verified that all the model parameters have requires_grad set to True, and I am unsure why this error is happening.</p>
",Training and Model Evaluation,element tensor doe require grad doe grad fn facing issue training deep learning model using pytorch lightning training process encountered following error context working text classification task implemented custom dataset class ucc dataset well lightningdatamodule subclass ucc data module model defined subclass pl lightningmodule called ucc comment classifier error occurs calling trainer fit model ucc data module relevant code snippet error occurs verified model parameter requires grad set true unsure error happening
Why do you need to re-upcast the norm layers of HF falcon to 32 floating point (fb32) when the code use floating point 16 (fb16)?,"<p>I saw these lines:</p>
<pre><code>bnb_4bit_compute_dtype=torch.float16,
...
optim = &quot;paged_adamw_32bit&quot;
...
for name, module in trainer.model.named_modules():
    if &quot;norm&quot; in name:
        module = module.to(torch.float32)
</code></pre>
<p>in the falcon tutorial. These are confusing me. Based on the original QLoRA tutorial, they use 4 bit model + during training they use 16 brain float (not normal float nor float 32). See equation 5:</p>
<pre><code>YBF16 = X_BF16 * doubleDequant(c_FP32_1, c_k_bit_2, W_NF4) + X_BF16 * L_BF16_1 * L_BF16_2
W_BF16 = doubleDequant(c_FP32_1, c_k-bit_2, W_k-bit) = dequant(dequant(c_FP32_1, c_k-bit_2), W_4bit)
</code></pre>
<p>which says the computation is done in brain float 16 (bf16):</p>
<p><a href=""https://i.sstatic.net/aCuJz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/aCuJz.png"" alt=""enter image description here"" /></a></p>
<p>I'm puzzled. Why are we:</p>
<ol>
<li>Using an optimizer in 32 floating point when the paper (and the bnb code too, <code>bnb_4bit_compute_dtype=torch.float16,</code> ok normal float but still 16, that's likely a bug) says brain float 16 for computation?</li>
<li>Why is the norm layers up casted to float point 32 when the paper and bnb code use brain float 16 (again code likely has a bug uses fb16 not bf16)?</li>
<li>The model is loaded from a 16 brain float, so why upcasting the norm layer to 32 floating point? Shouldn't there be a datatype error anyway?</li>
</ol>
<h2>I'm puzzled what is going on and why the code doesn't just crash at runtime anyway.</h2>
<p>Ref: <a href=""https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing</a></p>
<hr />
<h1>All Code</h1>
<pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;Falcon-Guanaco.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o

## Finetune Falcon-7b on a Google colab

Welcome to this Google Colab notebook that shows how to fine-tune the recent Falcon-7b model on a single Google colab and turn it into a chatbot

We will leverage PEFT library from Hugging Face ecosystem, as well as QLoRA for more memory efficient finetuning

## Setup

Run the cells below to setup and install the required libraries. For our experiment we will need `accelerate`, `peft`, `transformers`, `datasets` and TRL to leverage the recent [`SFTTrainer`](https://huggingface.co/docs/trl/main/en/sft_trainer). We will use `bitsandbytes` to [quantize the base model into 4bit](https://huggingface.co/blog/4bit-transformers-bitsandbytes). We will also install `einops` as it is a requirement to load Falcon models.
&quot;&quot;&quot;

!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git
!pip install -q datasets bitsandbytes einops wandb

&quot;&quot;&quot;## Dataset

For our experiment, we will use the Guanaco dataset, which is a clean subset of the OpenAssistant dataset adapted to train general purpose chatbots.

The dataset can be found [here](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)
&quot;&quot;&quot;

from datasets import load_dataset

dataset_name = &quot;timdettmers/openassistant-guanaco&quot;
dataset = load_dataset(dataset_name, split=&quot;train&quot;)

&quot;&quot;&quot;## Loading the model

In this section we will load the [Falcon 7B model](https://huggingface.co/tiiuae/falcon-7b), quantize it in 4bit and attach LoRA adapters on it. Let's get started!
&quot;&quot;&quot;

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer

model_name = &quot;ybelkada/falcon-7b-sharded-bf16&quot;

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    trust_remote_code=True
)
model.config.use_cache = False

&quot;&quot;&quot;Let's also load the tokenizer below&quot;&quot;&quot;

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

&quot;&quot;&quot;Below we will load the configuration file in order to create the LoRA model. According to QLoRA paper, it is important to consider all linear layers in the transformer block for maximum performance. Therefore we will add `dense`, `dense_h_to_4_h` and `dense_4h_to_h` layers in the target modules in addition to the mixed query key value layer.&quot;&quot;&quot;

from peft import LoraConfig

lora_alpha = 16
lora_dropout = 0.1
lora_r = 64

peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;,
    target_modules=[
        &quot;query_key_value&quot;,
        &quot;dense&quot;,
        &quot;dense_h_to_4h&quot;,
        &quot;dense_4h_to_h&quot;,
    ]
)

&quot;&quot;&quot;## Loading the trainer

Here we will use the [`SFTTrainer` from TRL library](https://huggingface.co/docs/trl/main/en/sft_trainer) that gives a wrapper around transformers `Trainer` to easily fine-tune models on instruction based datasets using PEFT adapters. Let's first load the training arguments below.
&quot;&quot;&quot;

from transformers import TrainingArguments

output_dir = &quot;./results&quot;
per_device_train_batch_size = 4
gradient_accumulation_steps = 4
optim = &quot;paged_adamw_32bit&quot;
save_steps = 10
logging_steps = 10
learning_rate = 2e-4
max_grad_norm = 0.3
max_steps = 500
warmup_ratio = 0.03
lr_scheduler_type = &quot;constant&quot;

training_arguments = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    fp16=True,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=True,
    lr_scheduler_type=lr_scheduler_type,
)

&quot;&quot;&quot;Then finally pass everthing to the trainer&quot;&quot;&quot;

from trl import SFTTrainer

max_seq_length = 512

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field=&quot;text&quot;,
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
)

&quot;&quot;&quot;We will also pre-process the model by upcasting the layer norms in float 32 for more stable training&quot;&quot;&quot;

for name, module in trainer.model.named_modules():
    if &quot;norm&quot; in name:
        module = module.to(torch.float32)

&quot;&quot;&quot;## Train the model

Now let's train the model! Simply call `trainer.train()`
&quot;&quot;&quot;

trainer.train()

&quot;&quot;&quot;During training, the model should converge nicely as follows:

![image](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/loss-falcon-7b.png)

The `SFTTrainer` also takes care of properly saving only the adapters during training instead of saving the entire model.
&quot;&quot;&quot;
</code></pre>
<hr />
<h1>How are they doing if they aren't (it seems) using mixed precision training?</h1>
<p>In the falcon qlora fine-tuning tutorial they upcast the norm layers to fp32 (and the paged optimizer is also 32fp). But the precision during compute they use is fp16 for the demo (or I use bf16 whenever I can). I found this puzzling because in their arguments they aren't (obviously to me at least) using mixed precision training. Do you know what might be going on? ref colab they give: <a href=""https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing</a></p>
<hr />
<p>cross:</p>
<ul>
<li>hf: <a href=""https://discuss.huggingface.co/t/why-do-you-need-to-re-upcast-the-norm-layers-of-hf-falcon-to-fb32/46139"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/why-do-you-need-to-re-upcast-the-norm-layers-of-hf-falcon-to-fb32/46139</a></li>
<li>discord hf: <a href=""https://discord.com/channels/879548962464493619/1019883044724822016/threads/1127484258852802660"" rel=""nofollow noreferrer"">https://discord.com/channels/879548962464493619/1019883044724822016/threads/1127484258852802660</a></li>
<li>so: <a href=""https://stackoverflow.com/questions/76646136/why-do-you-need-to-re-upcast-the-norm-layers-of-hf-falcon-to-32-floating-point"">Why do you need to re-upcast the norm layers of HF falcon to 32 floating point (fb32) when the code use floating point 16 (fb16)?</a></li>
</ul>
",Training and Model Evaluation,need upcast norm layer hf falcon floating point fb code use floating point fb saw line falcon tutorial confusing based original qlora tutorial use bit model training use brain float normal float float see equation say computation done brain float bf puzzled using optimizer floating point paper bnb code ok normal float still likely bug say brain float computation norm layer casted float point paper bnb code use brain float code likely ha bug us fb bf model loaded brain float upcasting norm layer floating point datatype error anyway puzzled going code crash runtime anyway ref code seems using mixed precision training falcon qlora fine tuning tutorial upcast norm layer fp paged optimizer also fp precision compute use fp demo use bf whenever found puzzling argument obviously least using mixed precision training know might going ref colab give cross hf discord hf href need upcast norm layer hf falcon floating point fb code use floating point fb
How should data be formatted to train the Huggingface DPR model?,"<p>I am new to machine learning, so maybe I have completely overlooked something, but I am trying to finetune the DPR models from the Huggingface transformers model using a dataset I am building (<a href=""https://huggingface.co/docs/transformers/model_doc/dpr"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/model_doc/dpr</a>). In the documentation on the Huggingface website, the area explaining how the model expects to be fed data is blank. How should I format my question / answer pairs to train the model? I am using pytorch.</p>
<p>I know DPR uses in-batch negatives, but some resources I have found suggest manually writing negatives and hard negatives, and other resources say the model automatically pulls negatives from other positive pairs in the batch. I can't find which is the case.</p>
<p>I read the documentation from Huggingface.com (see above). The explanation section is blank, and there are no examples that I could find.</p>
<p>I then went through the github page (<a href=""https://github.com/facebookresearch/DPR"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/DPR</a>) In the readme file there is a section on retriever data formatting. I am skeptical of this for a couple reasons 1) Huggingface calls all models context encoders and question encoders, not retrievers, so I am not sure that these are referencing the same models. 2) Providing every question its own set of negative answers seems computationally inefficient and doesn't allow for effective batching. 3) The model expects a json file? So during training we constantly have to write and retrieve json files? I have uploaded the pretrained model and called embeddings without json files so that doesn't seem to track
I started reading all of the .py files, trying to parse the actual formatting aspect of data preprocessing, and quickly got lost.</p>
<p>I have read the original DPR paper, but they are training their own model, with their own training data, and that model is different than the one on Huggingface.</p>
",Training and Model Evaluation,data formatted train huggingface dpr model new machine learning maybe completely overlooked something trying finetune dpr model huggingface transformer model using dataset building documentation huggingface website area explaining model expects fed data blank format question answer pair train model using pytorch know dpr us batch negative resource found suggest manually writing negative hard negative resource say model automatically pull negative positive pair batch find case read documentation huggingface com see explanation section blank example could find went github page readme file section retriever data formatting skeptical couple reason huggingface call model context encoders question encoders retriever sure referencing model providing every question set negative answer seems computationally inefficient allow effective batching model expects json file training constantly write retrieve json file uploaded pretrained model called embeddings without json file seem track started reading py file trying parse actual formatting aspect data preprocessing quickly got lost read original dpr paper training model training data model different one huggingface
"GPT2 model training gives, Loss nan during training","<p>I am following <a href=""https://discuss.huggingface.co/t/fine-tuning-gpt2-for-question-answering/31895"" rel=""nofollow noreferrer"">this</a> tutorial of huggingface library. Since I have a large dataset so I have used Dataloader and created batches. But am facing this error. Here is the process of creating batches,</p>
<pre><code> class FeedbackEssentials(Dataset):
def __init__(self, qa_pairs, tokenizer, max_length):
    self.qa_pairs = qa_pairs
    self.tokenizer = tokenizer
    self.max_length = max_length

def __len__(self):
    return len(self.qa_pairs)

def __getitem__(self, idx):
    question = self.qa_pairs[idx][0]
    text = f&quot;{question} {self.tokenizer.eos_token}&quot;
    input_ids = self.tokenizer.encode(text, add_special_tokens=True, max_length=self.max_length, padding='max_length', truncation=True)
    attention_mask = [1] * len(input_ids)  # Assuming all tokens should be attended to

    return {
        'input_ids': torch.tensor(input_ids),
        'attention_mask': torch.tensor(attention_mask)
    }



def text_manipulation(train_dataset):
column1_values = train_dataset['Total Marks'].values
column2_values = train_dataset['Coding'].values
listOfLists = [[pair[0], pair[1]] for pair in zip(column1_values, column2_values)]

text = &quot;&quot;
for feedback in listOfLists:
    text += f&quot;{feedback[0]} {feedback[1]} {tokenizer.eos_token}&quot;
return text

training_dataset = text_manipulation(dataset)
max_length_training = max(len(tokenizer.encode(qa_pair[0],add_special_tokens=True)) for qa_pair in training_dataset)
dataset_training = FeedbackEssentials(training_dataset, tokenizer, max_length_training)
</code></pre>
<p>Here is data loader and batching</p>
<pre><code>batch_size = 4
dataloader = DataLoader(dataset_training, batch_size=batch_size, shuffle=True)

optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)
</code></pre>
<p>I have studied the other deeplearning answers as well. but everything seems on its right place</p>
",Training and Model Evaluation,gpt model training give loss nan training following tutorial huggingface library since large dataset used dataloader created batch facing error process creating batch data loader batching studied deeplearning answer well everything seems right place
Loss function negative log likelihood giving loss despite perfect accuracy,"<p>I am debugging a sequence-to-sequence model and purposely tried to perfectly overfit a small dataset of ~200 samples (sentence pairs of length between 5-50). I am using negative log-likelihood loss in pytorch. I get low loss (~1e^-5), but the accuracy on the same dataset is only 33%.</p>
<p>I trained the model on 3 samples as well and obtained 100% accuracy, yet during training I had loss. I was under the impression that negative log-likelihood only gives loss (loss is in the same region of ~1e^-5) if there is a mismatch between predicted and target label?</p>
<p>Is a bug in my code likely?</p>
",Training and Model Evaluation,loss function negative log likelihood giving loss despite perfect accuracy debugging sequence sequence model purposely tried perfectly overfit small dataset sample sentence pair length using negative log likelihood loss pytorch get low loss e accuracy dataset trained model sample well obtained accuracy yet training loss wa impression negative log likelihood give loss loss region e mismatch predicted target label bug code likely
How to input a list into a Keras Model,"<p>I am pretty new to using tensowflow and keras.
I have an x_train and y_train set of the form</p>
<pre><code>x_train = [[ 21   0   0 ...   0   0   0]
 [ 22   0   0 ...   0   0   0]
 [ 23   0   0 ...   0   0   0]
 ...
 [255   0   0 ...   0   0   0]
 [256   0   0 ...   0   0   0]
 [257   0   0 ...   0   0   0]]

y_train = [4 1 1 1 4 1 3 1 2 4 4 1 1 4 4 4 1 1 1 4 4 1 4 4 1 1 4 1 4 1 3 1 1 1 1 1 4
3 4 1 1 4 3 1 4 4 1 4 1 3 1 1 1 1 4 1 1 1 4 4 1 1 1 4 1 3 4 4 1 4 1 2 1 4
4 1 1 4 1 4 4 3 4 1 3 1 1 3 1 4 2 4 2 2 1 4 3 1 1 4 2 1 3 1 4 4 1 3 4 4 1
1 1 3 3 4 1 4 1 1 4 4 1 4 4 4 1 1 3 4 1 2 4 1 4 4 1 4 4 1 1 4 1 4 4 4 4 1
1 4 4 1 1 1 1 1 1 4 4 1 4 4 1 1 4 1 1 1 4 4 1 4 4 1 4 1 1 2 1 1 1 1 4 1 1
1 4 1 1 4 1 1 4 4 1 3 4 3 4 1 1 1 1 1 4 4 1 4 4 4 4 1 1 1 4 3 4 2 1 4 4 1
4 1 1 4 4 1 4 4 1 4 4 4 1 4 1 2 4 1 1 1 1 4 1 4 4 4 4 1 1 1 4 1 4 4 4 1 4
1 1 4 4]
</code></pre>
<p>How do I feed this into a model.</p>
<p>I have tried this after following a tutorial online. But it doesnt seem to take in list input</p>
<pre><code>model = keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10)
])
model.compile(optimizer='adam', loss = 'binary_crossentropy',metrics=['accuracy'])
model.fit(x_train, y_train, epochs=10, batch_size=32)
</code></pre>
",Training and Model Evaluation,input list kera model pretty new using tensowflow kera x train train set form feed model tried following tutorial online doesnt seem take list input
How to create a dataset for a model like Falcon-7b/40b?,"<p>I am having the data as docx files and I want to use them to fine-tune the Falcon model. From what I see the data used to train the model was in json format. How can I convert my data in a format to be useful for the model?</p>
<p>Currently I am trying to convert my data in the json format, but it's a tedious work to do by hand.</p>
",Training and Model Evaluation,create dataset model like falcon b b data docx file want use fine tune falcon model see data used train model wa json format convert data format useful model currently trying convert data json format tedious work hand
Transfer Learning Distillation Model Loss Not Decreasing,"<p>Currently I'm trying to reproduce paper <a href=""https://aclanthology.org/2022.lrec-1.330/"" rel=""nofollow noreferrer"">&quot;A Deep Transfer Learning Method for Cross-Lingual Natural Language Inference&quot; (Bandyopadhyay et al., LREC 2022)</a> for Cross-Lingual Natural Language Inference task. But, the model I'm trying to reproduce is not learning any parameters which demonstrated by the model's loss not decreasing.</p>
<p>The dataset I'm using is IndoNLI with hypothesis sentences translated into Javanese. But, as you can read on the paper, you can also use XNLI for this task.</p>
<p>For this experiment, I'm using <code>Pytorch</code>, <code>Huggingface Transformers</code>, <code>Pandas</code>, <code>Numpy</code>, and <code>Wandb</code> for logging.</p>
<p>First, I construct my dataset as follows:</p>
<pre><code>class CompDataset(Dataset):
    def __init__(self, df_teacher, df_student):
        self.df_data_teacher = df_teacher
        self.df_data_student = df_student
        
    def __getitem__(self, index):
        # Teacher
        sentence_teacher_1 = self.df_data_teacher.loc[index, 'premise']
        sentence_teacher_2 = self.df_data_teacher.loc[index, 'hypothesis']
        
        encoded_dict_teacher = tokenizer.encode_plus(
            sentence_teacher_1,
            sentence_teacher_2,
            add_special_tokens = True,
            max_length = MAX_LEN,
            truncation='longest_first',
            padding = 'max_length',
            return_attention_mask = True,
            return_tensors = 'pt'
        )
        
        padded_token_list_teacher = encoded_dict_teacher['input_ids'][0]
        att_mask_teacher = encoded_dict_teacher['attention_mask'][0]
        tok_type_id_teacher = encoded_dict_teacher['token_type_ids'][0]
        
        target_teacher = torch.tensor([self.df_data_teacher.loc[index, 'label']])
        lt_target_teacher = torch.LongTensor(target_teacher)
        onehot_encoded_lbl_teacher = F.one_hot(lt_target_teacher, num_classes=3) # 3 classes: entails, neutral, contradict
        
        # Student
        sentence_student_1 = self.df_data_student.loc[index, 'premise']
        sentence_student_2 = self.df_data_student.loc[index, 'hypothesis']
        
        encoded_dict_student = tokenizer.encode_plus(
            sentence_student_1,
            sentence_student_2,
            add_special_tokens = True,
            max_length = MAX_LEN,
            truncation='longest_first',
            padding = 'max_length',
            return_attention_mask = True,
            return_tensors = 'pt'
        )
        
        padded_token_list_student = encoded_dict_student['input_ids'][0]
        att_mask_student = encoded_dict_student['attention_mask'][0]
        tok_type_id_student = encoded_dict_student['token_type_ids'][0]
        
        target_student = torch.tensor([self.df_data_student.loc[index, 'label']])
        lt_target_student = torch.LongTensor(target_student)
        onehot_encoded_lbl_student = F.one_hot(lt_target_student, num_classes=3) # 3 classes: entails, neutral, contradict
        
        output = {
            &quot;input_ids_teacher&quot;: padded_token_list_teacher, 
            &quot;attention_mask_teacher&quot;: att_mask_teacher,
            &quot;token_type_ids_teacher&quot;: tok_type_id_teacher,
            &quot;lbl_teacher&quot;: onehot_encoded_lbl_teacher,
            &quot;input_ids_student&quot;: padded_token_list_student, 
            &quot;attention_mask_student&quot;: att_mask_student,
            &quot;token_type_ids_student&quot;: tok_type_id_student,
            &quot;lbl_student&quot;: onehot_encoded_lbl_student
        }
        
        return output
    
    def __len__(self):
        return len(self.df_data_teacher)
</code></pre>
<p>Then, I build the transformers' dataset &amp; dataloader. The df_train_t and df_train_student being dataframe for teacher dataset (Indonesian premise-Indonesian hypothesis) and student dataset (Indonesian premise-Javanese hypothesis).</p>
<pre><code>train_data_cmp = CompDataset(df_train_t, df_train_student)
valid_data_cmp = CompDataset(df_valid_t, df_valid_student)
test_data_cmp = CompDataset(df_test_t, df_test_student)

train_dataloader = DataLoader(train_data_cmp, batch_size = BATCH_SIZE)
valid_dataloader = DataLoader(valid_data_cmp, batch_size = BATCH_SIZE)
test_dataloader = DataLoader(test_data_cmp, batch_size = BATCH_SIZE)
</code></pre>
<p>After that, I try to build the model using the schematic and algorithm of transfer learning method provided on the paper. As you can see on the code below, I tried to freeze the mBERT model for teacher, and update only the student model parameters.</p>
<pre><code>class TransferLearningPaper(PreTrainedModel):
    def __init__(self, config, lambda_kld, learningrate_student, batchnorm_epsilon = 1e-5):
        super(TransferLearningPaper, self).__init__(config)
        
        self.bert_model_teacher = BertModel.from_pretrained(
            MODEL_TEACHER_TYPE, # using already pretrained mBERT in INA language
            num_labels = 3,
            output_hidden_states=True
        )
        
        # Freeze teacher mBERT parameters
        for params_teacher in self.bert_model_teacher.parameters():
            params_teacher.requires_grad = False
    
        self.bert_model_student = BertModel.from_pretrained(
            MBERT_TYPE,
            num_labels = 3,
            output_hidden_states=True
        )
        
        self.optimizer_student = AdamW(
            self.bert_model_student.parameters(), 
            lr=learningrate_student
        )
        
        self.linear = nn.Linear(config.hidden_size, 3)  # Linear layer
        self.batchnorm = nn.BatchNorm1d(config.hidden_size, eps=batchnorm_epsilon)
        self.softmax = nn.Softmax(dim=1)  # Softmax activation
        
        self.cross_entropy = nn.CrossEntropyLoss()
        self.kld = nn.KLDivLoss(reduction='batchmean')
        
        # Initialize the weights of the linear layer
        self.linear.weight.data.normal_(mean=0.0, std=0.02)
        self.linear.bias.data.zero_()
        
        self.lambda_kld = lambda_kld
    
    def forward(self, input_ids_teacher, attention_mask_teacher, token_type_ids_teacher, lbl_teacher, input_ids_student, attention_mask_student, token_type_ids_student, lbl_student):
        # assume the label is already one-hot encoded
        
        self.bert_model_teacher.eval()
        self.bert_model_student.eval()
        
        with torch.no_grad():
            outputs_teacher = self.bert_model_teacher(
                input_ids=input_ids_teacher, 
                attention_mask=attention_mask_teacher, 
                token_type_ids=token_type_ids_teacher
            )
            outputs_student = self.bert_model_student(
                input_ids=input_ids_student, 
                attention_mask=attention_mask_student, 
                token_type_ids=token_type_ids_student
            )
        
            # take CLS token of the last hidden state
            pooled_output_teacher = outputs_teacher[0][:, 0, :]
            pooled_output_student = outputs_student[0][:, 0, :]
        
        batchnormed_logits = self.batchnorm(pooled_output_student)
        linear_output = self.linear(batchnormed_logits) # the output's logits
        softmax_linear_output = F.log_softmax(linear_output, dim=1)
        
        lbl_student = lbl_student[:,0,:].float()
        lbl_teacher = lbl_teacher[:,0,:].float()
        softmax_linear_output = softmax_linear_output.float()
        
        cross_entropy_loss = self.cross_entropy(softmax_linear_output, lbl_student)
        total_kld = self.kld(F.log_softmax(pooled_output_student, dim=1), F.softmax(pooled_output_teacher, dim=1))
        
        joint_loss = cross_entropy_loss + (self.lambda_kld * total_kld )
        
        return {&quot;loss&quot;: joint_loss, &quot;logits&quot;: softmax_linear_output}
    
    def update_param_student_model(self, loss):
        # Doing customized backpropagation for student's model
        self.bert_model_student.train()
        
        self.optimizer_student.zero_grad()
        loss.backward()
        self.optimizer_student.step()
</code></pre>
<p>Then, I instantiate the model and its configurations and hyperparameters:</p>
<pre><code>config = PretrainedConfig(
    problem_type = &quot;single_label_classification&quot;,
    id2label = {
        &quot;0&quot;: &quot;ENTAIL&quot;,
        &quot;1&quot;: &quot;NEUTRAL&quot;,
        &quot;2&quot;: &quot;CONTRADICTION&quot;
    },
    label2id = {
        &quot;ENTAIL&quot;: 0,
        &quot;NEUTRAL&quot;: 1,
        &quot;CONTRADICTION&quot;: 2
    },
    num_labels = 3,
    hidden_size = 768,
    name_or_path = &quot;indojavanesenli-transfer-learning&quot;,
    finetuning_task = &quot;indonesian-javanese natural language inference&quot;
)
print(config)
transferlearning_model = TransferLearningPaper(
    config = config,
    lambda_kld = 0.011, # antara 0.01-0.5
    learningrate_student = STUDENT_LRATE,
    batchnorm_epsilon = BATCH_NORM_EPSILON
)
transferlearning_model = transferlearning_model.to(device)
</code></pre>
<p>After that, I create functions to train and validate my model:</p>
<pre><code>def train(the_model, train_data):
    the_model.train()
    
    batch_loss = 0
    
    for batch, data in enumerate(train_data):
        input_ids_teacher = data[&quot;input_ids_teacher&quot;].to(device)
        attention_mask_teacher = data[&quot;attention_mask_teacher&quot;].to(device)
        token_type_ids_teacher = data[&quot;token_type_ids_teacher&quot;].to(device)
        lbl_teacher = data[&quot;lbl_teacher&quot;].to(device)
        input_ids_student = data[&quot;input_ids_student&quot;].to(device)
        attention_mask_student = data[&quot;attention_mask_student&quot;].to(device)
        token_type_ids_student = data[&quot;token_type_ids_student&quot;].to(device)
        lbl_student = data[&quot;lbl_student&quot;].to(device)
        
        output = the_model(
            input_ids_teacher = input_ids_teacher, 
            attention_mask_teacher = attention_mask_teacher, 
            token_type_ids_teacher = token_type_ids_teacher, 
            lbl_teacher = lbl_teacher, 
            input_ids_student = input_ids_student, 
            attention_mask_student = attention_mask_student, 
            token_type_ids_student = token_type_ids_student, 
            lbl_student = lbl_student
        )
        
        loss_model = output[&quot;loss&quot;]
        batch_loss += loss_model
        wandb.log({&quot;train/loss&quot;: loss_model})
        
        # Backpropagation
        the_model.update_param_student_model(loss_model)
    
    training_loss = batch_loss / BATCH_SIZE
    
    return training_loss

def validate(the_model, valid_data):
    the_model.eval()
    
    batch_loss = 0
    
    with torch.no_grad():
        for batch, data in enumerate(valid_data):
            input_ids_teacher = data[&quot;input_ids_teacher&quot;].to(device)
            attention_mask_teacher = data[&quot;attention_mask_teacher&quot;].to(device)
            token_type_ids_teacher = data[&quot;token_type_ids_teacher&quot;].to(device)
            lbl_teacher = data[&quot;lbl_teacher&quot;].to(device)
            input_ids_student = data[&quot;input_ids_student&quot;].to(device)
            attention_mask_student = data[&quot;attention_mask_student&quot;].to(device)
            token_type_ids_student = data[&quot;token_type_ids_student&quot;].to(device)
            lbl_student = data[&quot;lbl_student&quot;].to(device)

            output = the_model(
                input_ids_teacher = input_ids_teacher, 
                attention_mask_teacher = attention_mask_teacher, 
                token_type_ids_teacher = token_type_ids_teacher, 
                lbl_teacher = lbl_teacher, 
                input_ids_student = input_ids_student, 
                attention_mask_student = attention_mask_student, 
                token_type_ids_student = token_type_ids_student, 
                lbl_student = lbl_student
            )

            logits = output[&quot;logits&quot;].cpu().detach().numpy()
            packed_val = logits, lbl_student.cpu().detach().numpy()
            metrics = compute_metrics(packed_val)

            loss_model = output[&quot;loss&quot;]
            batch_loss += loss_model
            wandb.log({
                &quot;eval/loss&quot;: loss_model, 
                &quot;eval/f1_score&quot;: metrics[&quot;f1_score&quot;], 
                &quot;eval/accuracy&quot;: metrics[&quot;accuracy&quot;],
                &quot;eval/precision&quot;: metrics[&quot;precision&quot;],
                &quot;eval/recall&quot;: metrics[&quot;recall&quot;]
            })
    
        eval_loss = batch_loss / BATCH_SIZE
    
    return eval_loss, metrics

def training_sequence(the_model, train_data, valid_data, epochs):
    track_train_loss = []
    track_val_loss = []
    
    t = trange(epochs, colour=&quot;green&quot;, position=0, leave=True)
    for ep in t:
        training_loss = train(the_model, train_data)
        valid_loss, _ = validate(the_model, valid_data)
        
        track_train_loss.append(training_loss)
        track_val_loss.append(valid_loss)
        
        t.set_description(f&quot;Epoch [{ep + 1}/{epochs}] - Train loss: {training_loss:.2f} Valid loss: {valid_loss:.2f}&quot;)
        
        if valid_loss &lt; min(track_val_loss) or ep + 1 == 1:
            the_model.save_pretrained(
                save_directory = MODEL_PATH + &quot;indojavanesenli-transfer-learning&quot;
            )
            
        wandb.log({
            &quot;train_loss/epoch&quot;: training_loss,
            &quot;validation_loss/epoch&quot;: valid_loss
        })
        
    return {
        &quot;training_loss&quot;: track_train_loss,
        &quot;validation_loss&quot;: track_val_loss
    }
</code></pre>
<p>Finally, I train my model by using:</p>
<pre><code>training_result = training_sequence(transferlearning_model, train_dataloader, valid_dataloader, NUM_EPOCHS)
</code></pre>
<p>But the problem is, during training, the model not updating the student's model parameters as you can see on Fig.1 below.</p>
<p><a href=""https://i.sstatic.net/M9QWG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M9QWG.png"" alt=""Figure 1. Model loss not decreasing"" /></a></p>
<p>Figure 1. Model loss not decreasing</p>
<p>FYI, this is the configuration variable I use for the code above:</p>
<pre><code>TOKENIZER_TYPE = 'bert-base-multilingual-cased'
MBERT_TYPE = 'bert-base-multilingual-cased'
MODEL_TEACHER_TYPE = 'jalaluddin94/nli_mbert' # This is an already fine-tuned mBERT on the Indonesian language
MODEL_PATH = 'D:/Training/Machine Learning/NLP/NLI/Indo-Javanese-NLI/ResearchedModels/'

STUDENT_LRATE = 2e-5
MAX_LEN = 512
NUM_EPOCHS = 25
BATCH_SIZE = 12
BATCH_NORM_EPSILON = 1e-5
LAMBDA_L2 = 3e-5
</code></pre>
",Training and Model Evaluation,transfer learning distillation model loss decreasing currently trying reproduce paper deep transfer learning method cross lingual natural language inference bandyopadhyay et al lrec cross lingual natural language inference task model trying reproduce learning parameter demonstrated model loss decreasing dataset using indonli hypothesis sentence translated javanese read paper also use xnli task experiment using logging first construct dataset follows build transformer dataset dataloader df train df train student dataframe teacher dataset indonesian premise indonesian hypothesis student dataset indonesian premise javanese hypothesis try build model using schematic algorithm transfer learning method provided paper see code tried freeze mbert model teacher update student model parameter instantiate model configuration hyperparameters create function train validate model finally train model using problem training model updating student model parameter see fig figure model loss decreasing fyi configuration variable use code
"InvalidArgumentError: Graph execution error, &#39;logits shape and labels shape must have the same dimension&#39;","<p>I am trying to solve an aspect based sentiment analysis problem.</p>
<p>I have constructed an NN architecture which I believe is logical but had problems in finding the right loss function. or maybe my architecture is wrong and you may help me with that. but I mainly write here for the error that my code has.  Here is the set up.</p>
<p>I have a training set of 980 rows which has different (turkish) sentences in each line. It also contains labels as one or more aspects for each sentence and the sentiments for each of these aspects. so one sentence can have more than one aspect.</p>
<p>There are 910 different aspects in the training set.</p>
<p>I use lstm architecture by devising the problem as a multi label multi class problem.
for each sentence input, there will be 910 neurons at the output layer for each aspect and the nuerons will have 4 different classes(1 if the sentiment is positive 2 if negative 3 if neutral and 0 if the aspect doesnt exist for this sentence)</p>
<p>so it esssentially looks like a softmax -categorical cross entropy problem
but the problem is the classes are not binary. they can take 4 different values.</p>
<p>that is why a softmax activation function with categorical cross entropy as loss does not work.</p>
<p>so I use softmax as activation and sparsecategoricalentropy as loss function. but it raises an error.</p>
<p>for reference here is my code for the model</p>
<pre><code>model = Sequential()

model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_sequence_length))

model.add(Bidirectional(LSTM(128, return_sequences=True)))

model.add(Bidirectional(LSTM(128)))

model.add(Dense(512, activation=‘relu’))

model.add(Dense(512, activation=‘relu’))

model.add(Dense(910, activation=‘softmax’))

model.compile(loss=‘SparseCategoricalCrossentropy’, optimizer=‘adam’, metrics=[‘categorical_accuracy’])

batch_size = 64

epochs = 100

model.fit(X_train, y_train, batch_size=2, epochs=epochs)
</code></pre>
<p>when I run the model it gives the following error</p>
<p>InvalidArgumentError: Graph execution error:
logits and labels must have the same first dimension, got logits shape [64,910] and labels shape [58240]
[[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_135707]
[[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_11542]</p>
<p>the interesting thing is that when I use ‘categoricalcrossentropy’ it does not raise error but when I change it to sparsecategoricalentropy, it raises the above error. It says 'logits and labels must have the same first dimension, got logits shape [64,910] and labels shape [58240]'
I have no idea where 58240 comes from.
what do you guys think?</p>
<p>thank you in advance for your help,</p>
<p>mehmet</p>
",Training and Model Evaluation,invalidargumenterror graph execution error logits shape label shape must dimension trying solve aspect based sentiment analysis problem constructed nn architecture believe logical problem finding right loss function maybe architecture wrong may help mainly write error code ha set training set row ha different turkish sentence line also contains label one aspect sentence sentiment aspect one sentence one aspect different aspect training set use lstm architecture devising problem multi label multi class problem sentence input neuron output layer aspect nuerons different class sentiment positive negative neutral aspect doesnt exist sentence esssentially look like softmax categorical cross entropy problem problem class binary take different value softmax activation function categorical cross entropy loss doe work use softmax activation sparsecategoricalentropy loss function raise error reference code model run model give following error invalidargumenterror graph execution error logits label must first dimension got logits shape label shape node sparse categorical crossentropy sparsesoftmaxcrossentropywithlogits sparsesoftmaxcrossentropywithlogits op inference train function node sparse categorical crossentropy sparsesoftmaxcrossentropywithlogits sparsesoftmaxcrossentropywithlogits op inference train function interesting thing use categoricalcrossentropy doe raise error change sparsecategoricalentropy raise error say logits label must first dimension got logits shape label shape idea come guy think thank advance help mehmet
Why are my `RoBERTa` model outputs the same every time?,"<p>I made my model for assing score about corpus based on RoBERTa(fine-tuning).</p>
<pre class=""lang-py prettyprint-override""><code>class TF_RoBERTa_VAD_Classification(tf.keras.Model):
    def __init__(self, model_name):
        super(TF_RoBERTa_VAD_Classification, self).__init__()

        self.model_name = model_name
        self.roberta = TFRobertaModel.from_pretrained(model_name, from_pt=True)

        self.output_layer = tf.keras.layers.Dense(1, kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02), activation=&quot;linear&quot;, name=&quot;Linear_Output&quot;) # Initializer function test
    
    def call(self, inputs):
        input_ids, attention_mask = inputs

        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        cls_token = outputs[1]

        outputs = self.output_layer(cls_token)

        return outputs

    def get_config(self):
        config = super().get_config()
        config.update({
            &quot;model_name&quot;: self.model_name,
        })
        return config

    @classmethod
    def from_config(cls, config):
        model = cls(config[&quot;model_name&quot;])
        return model
</code></pre>
<p>This is my model's code.</p>
<p>When I trained my model, the loss(MSE) was changing but val_loss(MSE) doesn't change from 1epoch to 10~15epochs.</p>
<p>I tried to add a &quot;gelu&quot; layer before linear output layer but it still shows the same value.</p>
<p>How to resolve this?</p>
",Training and Model Evaluation,model output every time made model assing score corpus based roberta fine tuning model code trained model loss mse wa changing val loss mse change epoch epoch tried add gelu layer linear output layer still show value resolve
HF push_to_hub API through Google Colab,"<p>I am using Google Colab to push my fine tuned model to the Hub. When I run the model.fit() function, a output directory was created on the Colab my drive and the model(Bert-base-uncased) begins to train on my datasets(glue- sst2) for 2 epochs. As the 2nd epoch progress bar run to completion, the training keeps running for hours and eventually crashed and the whole process failed, then Colab displays can't connect to GPU as backend for that day. I have tried this twice but keeps getting the same results. Could it be the Colab GPU isn’t enough to push the model to the Hub or am doing something wrong?</p>
<pre><code>from datasets import load_dataset
raw_datasets = load_dataset('glue', 'sst2')

checkpoint = &quot;bert-base-uncased&quot;
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

def tokenizer_function(example):
  return tokenizer(example['sentence'], truncation=True)

tokenized_datasets = raw_datasets.map(tokenizer_function, batched= True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=&quot;tf&quot;)

tf_train_dataset = tokenized_datasets[&quot;train&quot;].to_tf_dataset(
    columns=[&quot;attention_mask&quot;, &quot;input_ids&quot;,&quot;token_type_ids&quot;],
    label_cols=['labels'],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=30,
)

tf_validation_dataset = tokenized_datasets[&quot;validation&quot;].to_tf_dataset(
    columns=[&quot;attention_mask&quot;, &quot;input_ids&quot;,&quot;token_type_ids&quot;],
    label_cols=['labels'],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=30,
)

num_epochs = 2
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=2e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
opt = Adam(learning_rate=lr_scheduler)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(loss=loss, optimizer=opt, metrics= ['accuracy'])

callbacks = PushToHubCallback(
    &quot;bert-fine-tuned_sst2&quot;, save_strategy=&quot;epoch&quot;, tokenizer=tokenizer
)

model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=1, callbacks=callbacks)  #this is where am having issues
</code></pre>
<p>The notebook crashed and model failed to push to the hugging face hub</p>
",Training and Model Evaluation,hf push hub api google colab using google colab push fine tuned model hub run model fit function output directory wa created colab drive model bert base uncased begin train datasets glue sst epoch nd epoch progress bar run completion training keep running hour eventually crashed whole process failed colab display connect gpu backend day tried twice keep getting result could colab gpu enough push model hub something wrong notebook crashed model failed push hugging face hub
"When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1","<p>I'm traing neural network model to text summarization. i'm using <a href=""https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp"" rel=""nofollow noreferrer"">this</a> dataset for model training. after load the train model this issues show up.</p>
<pre><code>ValueError: Exception encountered when calling layer 'text_vectorization_1' (type TextVectorization).
    
    When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, 7) with rank=2
    
    Call arguments received by layer 'text_vectorization_1' (type TextVectorization):
      • inputs=tf.Tensor(shape=(None, 7), dtype=string)
</code></pre>
<p>This is my code so far</p>
<pre><code>text_vectorizer = tf.keras.layers.TextVectorization(
    max_tokens=12000,
    output_mode='int',
    output_sequence_length=15
)

text_vectorizer.adapt(train_sentences)

embedding_2 = tf.keras.layers.Embedding(
    input_dim=len(text_vectorizer.get_vocabulary()),
    output_dim=128,
    embeddings_initializer='uniform',
    input_length=15,
    name='embedding_2'
) 

inputs = tf.keras.layers.Input(shape=(1,), dtype='string')

x = text_vectorizer(inputs)
x = embedding_2(x)
x = tf.keras.layers.LSTM(64)(x)

outputs = tf.keras.layers.Dense(6, activation='softmax')(x)

model_2 = tf.keras.Model(inputs, outputs, name='model_2_simple_lstm')

model_2.compile(
    optimizer='Adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model_2_history = model_2.fit(
    train_sentences,
    train_labels,
    epochs=5,
    validation_data=(val_sentences, val_labels)
)

model_2.save('sentiment_model_LTSM', save_format='tf')
</code></pre>
<p>This is the summarization code block using save model</p>
<pre><code> Load and tokenize the text
def preprocess_text(text):
    sentences = sent_tokenize(text)
    return sentences


# Load the trained RNN model
def load_model():
    # Assuming you have already trained and saved your RNN model
    model = tf.keras.models.load_model('/content/sentiment_model101')
    #model = keras.models.load_model('/content/sentiment_model.h5')
    return model


# Perform sentiment analysis using the loaded RNN model
def perform_sentiment_analysis(sentences, model):
    tokenizer = keras.preprocessing.text.Tokenizer()
    tokenizer.fit_on_texts(sentences)
    sequences = tokenizer.texts_to_sequences(sentences)
    max_sequence_length = max([len(seq) for seq in sequences])
    padded_sequences = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_sequence_length)

    sentiment_scores = model.predict(padded_sequences)

    return sentiment_scores


# Map sentiment scores to emotions
def map_to_emotion(sentiment_scores):
    max_score = np.max(sentiment_scores)
    if max_score &gt;= 0.5:
        return 'joyful'
    elif max_score &lt;= -0.5:
        return 'angry'
    elif max_score &lt;= -0.2:
        return 'sad'
    else:
        return 'neutral'


# Generate the text summary based on emotions
def generate_summary(text):
    sentences = preprocess_text(text)

    model = load_model()

    sentiment_scores = perform_sentiment_analysis(sentences, model)

    emotion_trigger = map_to_emotion(sentiment_scores)

    max_score = np.max(sentiment_scores)
    max_sentence = sentences[np.argmax(sentiment_scores)]

    summary = f&quot;The text evokes a {emotion_trigger} emotion. The key sentence is: '{max_sentence}'&quot;

    return summary
</code></pre>
",Training and Model Evaluation,using tokenize string input rank must last shape dimension must traing neural network model text summarization using dataset model training load train model issue show code far summarization code block using save model
What does the loss calculation from spacy textcat tells me?,"<p>I am new to NLP and use of spacy. What does the 'loss' in spacy textcat mean? I get value of 0.0000 for all iterations during constructing a custom text classifier using spacy textcat model training. I'm wondering if there is some problem with my model training.</p>
<pre><code>        for i in range(n_iter):
            losses = {}
            # batch up the examples using spaCy's minibatch
            batches = minibatch(train_data, size=compounding(4., 32., 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                #print('texts: '+str(texts))
                #print('annotations: '+str(annotations))
                nlp.update(texts, annotations, sgd=optimizer, drop=0.2,losses=losses)
            # with textcat.model.use_params(optimizer.averages):
                # evaluate on the dev data split off in load_data()

            print('{0:.3f}'  # print a simple table
                  .format(losses['textcat']))

</code></pre>
<p>What does the code below on output_dir mean?</p>
<pre><code>    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.to_disk(output_dir)
        print(&quot;Saved model to&quot;, output_dir)

        # test the saved model
        print(&quot;Loading from&quot;, output_dir)
        nlp2 = spacy.load(output_dir)
        doc2 = nlp2(test_text)
        print(test_text, doc2.cats)

</code></pre>
",Training and Model Evaluation,doe loss calculation spacy textcat tell new nlp use spacy doe loss spacy textcat mean get value iteration constructing custom text classifier using spacy textcat model training wondering problem model training doe code output dir mean
How to speed up Gensim Word2vec model load time?,"<p>I'm building a chatbot so I need to vectorize the user's input using Word2Vec. </p>

<p>I'm using a pre-trained model with 3 million words by Google (GoogleNews-vectors-negative300).</p>

<p>So I load the model using Gensim:</p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>The problem is that it takes about 2 minutes to load the model. I can't let the user wait that long.</p>

<p>So what can I do to speed up the load time?</p>

<p>I thought about putting each of the 3 million words and their corresponding vector into a MongoDB database. That would certainly speed things up but intuition tells me it's not a good idea.</p>
",Training and Model Evaluation,speed gensim word vec model load time building chatbot need vectorize user input using word vec using pre trained model million word google googlenews vector negative load model using gensim problem take minute load model let user wait long speed load time thought putting million word corresponding vector mongodb database would certainly speed thing intuition tell good idea
expected scalar type Long but found Float,"<p>This code uses pytorch. It is a NLP machine learning model. I am facing problem with the model part.</p>
<pre><code>16     labels=labels.type(torch.LongTensor)
     17     print(labels.type())
---&gt; 18     outputs= model(labels)
     19     loss=criterion(outputs.type(torch.LongTensor), labels)
     20     optimizer.zero_grad()

3 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)
    112 
    113     def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 114         return F.linear(input, self.weight, self.bias)
    115 
    116     def extra_repr(self) -&gt; str:

RuntimeError: expected scalar type Long but found Float
</code></pre>
<p>I tried converting label to LongTensor. But that did not fix, the error. This is my code:</p>
<pre><code>
learning_rate=0.01
output_size=len(products)
num_epochs=5
hidden_size=50
input_size=len(all_words)

criterion=nn.CrossEntropyLoss()
model=NeuralNet(input_size,hidden_size,output_size)
optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)
#since it is multiclass

for epoch in range(num_epochs):
  for (words, i) in xy:
    labels=bag_of_words(words,all_words)
    labels=torch.from_numpy(labels)
    labels=labels.type(torch.LongTensor)
    print(labels.type())
    outputs= model(labels)
    loss=criterion(outputs.type(torch.LongTensor), labels)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
</code></pre>
<p>Kindly help me out with what I can do to fix this error. Which variable is the error referring to? Will there be a problem with the loss function also?</p>
",Training and Model Evaluation,expected scalar type long found float code us pytorch nlp machine learning model facing problem model part tried converting label longtensor fix error code kindly help fix error variable error referring problem loss function also
How do I fine tune BERT&#39;s self attention mechanism?,"<p>My goal is to fine tune BERT's self attention so that I can see to what extent two random sentences in a document (with positional encoding) rely on each other contextually.</p>
<p>Many explanations and article that I see talk about the implementation of self-attention but do not mention how to further train self attention.</p>
<p>Here is what I'm thinking of doing to train BERT's self attention:</p>
<ol>
<li><p>Use some kind of word to vector algorithm and vectorize all the words in the article.</p>
</li>
<li><p>Add positional encoding to each sentence [where the sentence is an array of vectors (words)] using a sinusoidal function.</p>
</li>
<li><p>Make matrix of each sentence concatenated with every other sentence</p>
</li>
<li><p>For each sentence-sentence pair, iterate through each words masking them.  The model must guess the word based on context; back prop is based on accuracy of the guess.</p>
</li>
<li><p>The finished model should be able to take in an arbitrary sentence-sentence pair and output an attention matrix.</p>
</li>
</ol>
<p>I'm not sure if such a method is the right one for fine tuning (or if this even counts as continuing pre-training) a self attention mechanism, or if BERT is even the best model to train a self attention function on.</p>
<p>I'm obviously very new to fine tuning LLMs, so any guidance would be greatly appreciated!</p>
",Training and Model Evaluation,fine tune bert self attention mechanism goal fine tune bert self attention see extent two random sentence document positional encoding rely contextually many explanation article see talk implementation self attention mention train self attention thinking train bert self attention use kind word vector algorithm vectorize word article add positional encoding sentence sentence array vector word using sinusoidal function make matrix sentence concatenated every sentence sentence sentence pair iterate word masking model must guess word based context back prop based accuracy guess finished model able take arbitrary sentence sentence pair output attention matrix sure method right one fine tuning even count continuing pre training self attention mechanism bert even best model train self attention function obviously new fine tuning llm guidance would greatly appreciated
Incorporating validation data correctly in model.fit with Keras and DistilBERT,"<p>I'm new to NLP and am trying to do some binary classification with DistilBERT on a Kaggle dataset (<a href=""https://www.kaggle.com/competitions/nlp-getting-started/data"" rel=""nofollow noreferrer"">https://www.kaggle.com/competitions/nlp-getting-started/data</a>). Everything is actually going fine, but I'm having trouble incorporating my validation data into model.fit. I just can't figure out why val_accuracy and val_loss don't appear in my epoch values. My code is below.</p>
<pre><code>#import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
import tensorflow as tf

#import tokenizer and pre-trained model
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)
model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')

#random seed
random_seed = 42

#set random seed in tensorflow
tf.random.set_seed(random_seed)

#set random seed in numpy
np.random.seed(random_seed)

#load the raw training data
df_raw_train = pd.read_csv(&quot;data/train.csv&quot;)
#make a copy of df_raw_train
df_train = df_raw_train.copy(deep=True)

#load the raw test data
df_raw_test = pd.read_csv(&quot;data/test.csv&quot;)
#make a copy of df_raw_test
df_test = df_raw_test.copy(deep=True)

#get target name
target = 'target'

#drop columns
df_train.drop(['id','keyword','location'],axis=1,inplace=True)
df_test.drop(['id','keyword','location'],axis=1,inplace=True)

#training (80%) and validation (20%) data split

df_train, df_val = train_test_split(df_train, train_size=0.8, random_state=random_seed)

#reset index
df_train, df_val = df_train.reset_index(drop=True), df_val.reset_index(drop=True)

#batch tokenize our tweet field
X_train = tokenizer.batch_encode_plus(df_train.text, pad_to_max_length=True, return_tensors=&quot;tf&quot;)
X_val = tokenizer.batch_encode_plus(df_val.text, pad_to_max_length=True, return_tensors=&quot;tf&quot;)
X_test = tokenizer.batch_encode_plus(df_test.text, pad_to_max_length=True, return_tensors=&quot;tf&quot;)

#obtain target
y_train = df_train['target'].to_numpy()
y_val = df_val['target'].to_numpy()

#optimize model
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
bce = tf.keras.losses.BinaryCrossentropy()
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])
model.fit(x=X_train['input_ids'], y=y_train, epochs=2, batch_size=15, verbose=2, validation_data=(X_val, y_val))
</code></pre>
<p>The epoch output looks like this:</p>
<pre><code>Epoch 1/2
343/508 [===================&gt;..........] - ETA: 6:30 - loss: 0.4728 - accuracy: 0.7811
</code></pre>
<p>I'd really appreciate some pointers here. Thank you, all.</p>
",Training and Model Evaluation,incorporating validation data correctly model fit kera distilbert new nlp trying binary classification distilbert kaggle dataset everything actually going fine trouble incorporating validation data model fit figure val accuracy val loss appear epoch value code epoch output look like really appreciate pointer thank
Why accuracy is 0%,"<p><a href=""https://github.com/Saranja-Navaneethakumar/WSD_Skipgram/blob/main/skipgram.py"" rel=""nofollow noreferrer"">https://github.com/Saranja-Navaneethakumar/WSD_Skipgram/blob/main/skipgram.py</a></p>
<p>I'm doing word sense disambiguation with word2vec skipgaram model and train &amp; test datasets - 2017 SemEval benchmark dataset but I tried above code but it fails and accuracy becomes 0%
I need to increase accuracy Is there any way to increase it or any other mistakes in my code?.
Can I use any other version of benchmark dataset?</p>
",Training and Model Evaluation,accuracy word sense disambiguation word vec skipgaram model train test datasets semeval benchmark dataset tried code fails accuracy becomes need increase accuracy way increase mistake code use version benchmark dataset
Balancing a multilabel dataset,"<p>I have a dataset that contains email texts and their corresponding labels. Each email can have multiple labels, making it a multilabel problem. I used multi-hot encoding for labels, so they look like [1, 0, 1, 0, 0], where 1 indicates that an email belongs to that class. However, my data is imbalanced, and some label combinations barely occur in the dataset, which makes my LSTM biased towards the majority classes.</p>
<p>I tried a weighted loss function by specifying pos_weight for BCEWithLogitsLoss, but that doesn’t seem to help. What are some balancing techniques that I could use?</p>
",Training and Model Evaluation,balancing multilabel dataset dataset contains email text corresponding label email multiple label making multilabel problem used multi hot encoding label look like indicates email belongs class however data imbalanced label combination barely occur dataset make lstm biased towards majority class tried weighted loss function specifying po weight bcewithlogitsloss seem help balancing technique could use
Is it valid to evaluate a flan-t5 model on sequences longer than it&#39;s max_length of 2048 tokens (assuming I have enough memory)?,"<p>I am evaluating the different flan-t5 models with few-shot chain of thought prompts which can go over the 2048 maximum token length. I am under the impression that because T5 uses relative position encoding, that it would be valid (make sense) to do zero shot on sequences longer than 2048, provided that I can handle the quadratic memory scaling, but wanted to double check if that is indeed the case.</p>
<p>Way I see it, the linear mappings only learn relative dependencies from the relative positional encodings, so evaluation should still be valid on longer sequences even if it was not actually trained on sequences of that length. The only issue I think of would be of that it would not learn a pattern for relative dependencies longer than 2048.</p>
",Training and Model Evaluation,valid evaluate flan model sequence longer max length token assuming enough memory evaluating different flan model shot chain thought prompt go maximum token length impression us relative position encoding would valid make sense zero shot sequence longer provided handle quadratic memory scaling wanted double check indeed case way see linear mapping learn relative dependency relative positional encoding evaluation still valid longer sequence even wa actually trained sequence length issue think would would learn pattern relative dependency longer
Unable to train model using gpu on M1 Mac,"<p>How to reproduce the behaviour</p>
<p>install space for apple , select model training option and follow the on screen instructions.</p>
<p>generate config files for model training.</p>
<p>Declare your training and testing corpus in base_config file then auto fill the to generate the final config file.</p>
<p>python -m spacy init fill-config base_config.cfg config.cfg</p>
<p>python -m spacy train config/config.cfg -g 0 --output trf_model-2</p>
<p>Your Environment</p>
<p>Operating System: MacOS 12.4
Python Version Used: 3.10
spaCy Version Used: 3.4.0
Environment Information: -
spaCy version: 3.4.0
Platform: macOS-12.4-arm64-arm-64bit
Python version: 3.10.5
Pipelines: en_core_web_trf (3.4.0)
While I'm trying to train the model using gpu on m1 Mac got this error</p>
<p>RuntimeError: invalid gradient at index 0 - expected device cpu but got mps:0</p>
",Training and Model Evaluation,unable train model using gpu mac reproduce behaviour install space apple select model training option follow screen instruction generate config file model training declare training testing corpus base config file auto fill generate final config file python spacy init fill config base config cfg config cfg python spacy train config config cfg g output trf model environment operating system macos python version used spacy version used environment information spacy version platform macos arm arm bit python version pipeline en core web trf trying train model using gpu mac got error runtimeerror invalid gradient index expected device cpu got mp
"Expected input batch_size (28) to match target batch_size (456), Changing batch size increase the target batch size with GPT2 model","<p>I was practising fine-tuning a gpt2 model on a simple question-answer dataset when I encountered this error. I have studied other answers, but my input dataset shapes look fine.</p>
<pre><code>def tokenize_data(total_marks, coding_feeddback):
    inputs = tokenizer(total_marks, truncation=True, padding=True, 
                                                     return_tensors=&quot;pt&quot;)
    labels = tokenizer(coding_feeddback, truncation=True, padding=True, 
                                                    return_tensors=&quot;pt&quot;)['input_ids']
return inputs, labels

*# Prepare the training and validation datasets*

 train_inputs, train_labels = tokenize_data(train_df['Question'].tolist(), 
 train_df['ans'].tolist())
 val_inputs, val_labels = tokenize_data(val_df['Question'].tolist(), 
 val_df['ans'].tolist())

 train_dataset = TensorDataset(train_inputs['input_ids'], train_labels)
 val_dataset = TensorDataset(val_inputs['input_ids'], val_labels)
</code></pre>
<blockquote>
<p><em>Here is the size of the train and validation dataset</em></p>
</blockquote>
<pre><code>print('train input shape:',train_inputs['input_ids'].shape)
print('train label shape: ',train_labels.shape)
print('validation input shape: ',val_inputs['input_ids'].shape)
print('validation label shape: ',val_labels.shape)
</code></pre>
<blockquote>
<p><em>The output of the above lines is as follows.</em></p>
</blockquote>
<pre><code>train input shape: torch.Size([76, 8])
train label shape:  torch.Size([76, 115])
validation input shape:  torch.Size([20, 8])
validation label shape:  torch.Size([20, 98])
</code></pre>
<blockquote>
<p><em>This is how I am using Dataloader.</em></p>
</blockquote>
<pre><code>batch_size = 4
train_dataloader = DataLoader(train_dataset, 
                   batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size)
</code></pre>
<blockquote>
<p>Here is the model.</p>
</blockquote>
<h1>Training loop</h1>
<pre><code>model.train()
for epoch in range(num_epochs):
 for batch in train_dataloader:
    batch = [item.to(device) for item in batch]
    input_ids, labels = batch

    optimizer.zero_grad()
    
    print(&quot;indputIds:&quot;,len(input_ids))
    print(&quot;lebels:&quot;,len(labels))

    outputs = model(input_ids=input_ids, labels=labels)
    loss = outputs.loss
    logits = outputs.logits

    loss.backward()
    optimizer.step()

# Validation
with torch.no_grad():
    model.eval()
    val_loss = 0.0
    for val_batch in val_dataloader:
        val_batch = [item.to(device) for item in val_batch]
        val_input_ids, val_labels = val_batch

        val_outputs = model(input_ids=val_input_ids, labels=val_labels)
        val_loss += val_outputs.loss.item()

    average_val_loss = val_loss / len(val_dataloader)
    print(f&quot;Epoch: {epoch+1}, Validation Loss: {average_val_loss:.4f}&quot;)

model.train()
</code></pre>
<blockquote>
<p>Even the dimension in each batch is the same In train and validation data loader For example:</p>
</blockquote>
<pre><code>Batch 19
Inputs:
  torch.Size([4, 8])
Targets:
  torch.Size([4, 115])
</code></pre>
<blockquote>
<p>Same for validation except in validation target size is [4,8] and [8,98].</p>
</blockquote>
",Training and Model Evaluation,expected input batch size match target batch size changing batch size increase target batch size gpt model wa practising fine tuning gpt model simple question answer dataset encountered error studied answer input dataset shape look fine size train validation dataset output line follows using dataloader model training loop even dimension batch train validation data loader example validation except validation target size
Bert NER model start and end position None after fine-tuning,"<p>I have fine-tuned a BERT NER model to my dataset. The base model that I am fine-tuning is “dslim/bert-base-NER”. I have been successfully able to train the model using the following script as refrence:
<a href=""https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb#scrollTo=zPDla1mmZiax"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb#scrollTo=zPDla1mmZiax</a></p>
<p>The code which does the prediction:</p>
<pre><code>from transformers import pipeline, BertTokenizer

tokenizer = BertTokenizer.from_pretrained('dslim/bert-base-NER', return_offsets_mapping=True, is_split_into_words=True)
model = BertForTokenClassification.from_pretrained('dslim/bert-base-NER')

pipe = pipeline(task=&quot;ner&quot;, model=model.to(&quot;cpu&quot;), tokenizer=tokenizer, grouped_entities=True)
pipe(&quot;this is a Abc Corp. Ltd&quot;)
</code></pre>
<p>The prediction form the base model contained the start and end position of the word in the original text like:</p>
<pre><code>{‘entity_group’: ‘ORG’, ‘score’: 0.9992545247077942, ‘word’: ‘A’, ‘start’: 10, ‘end’: 11}
{‘entity_group’: ‘ORG’, ‘score’: 0.998507097363472, ‘word’: ‘##bc Corp Ltd’, ‘start’: 11, ‘end’: 22}
</code></pre>
<p>While the prediction from the re-trained model is:</p>
<pre><code>{‘entity_group’: ‘ORG’, ‘score’: 0.747031033039093, ‘word’: ‘##7’, ‘start’: None, ‘end’: None},
{‘entity_group’: ‘ORG’, ‘score’: 0.9055356582005819, ‘word’: ‘Games , Inc’, ‘start’: None, ‘end’: None}
</code></pre>
<p>I am passing the <em><strong>position ids</strong></em> to the model during the training process. I looked at the model training parameters but, could not find a way to pass <em><strong>start and end position</strong></em> of the words to model training process. I have the start and end position of the tokenized words.</p>
",Training and Model Evaluation,bert ner model start end position none fine tuning fine tuned bert ner model dataset base model fine tuning dslim bert base ner successfully able train model using following script refrence code doe prediction prediction form base model contained start end position word original text like prediction trained model passing position id model training process looked model training parameter could find way pas start end position word model training process start end position tokenized word
Fine-tune T5 pre-trained model on a specific domain for question answering,"<p>I need to build a question-answering system on a specific domain of Finance, I have documents data containing all the information about the field,</p>
<p>Can I fine-tune T5 pre-trained model (large) unsupervised training on the documents so it can answer related questions based on my documents corpus?<br />
The documents corpus I have is quite large, so I cannot just use it as a context in the current QA within T5,</p>
<p>I am open to your suggestions!</p>
",Training and Model Evaluation,fine tune pre trained model specific domain question answering need build question answering system specific domain finance document data containing information field fine tune pre trained model large unsupervised training document answer related question based document corpus document corpus quite large use context current qa within open suggestion
How to create a dataset object with for multiple input of texts to the SetFit model?,"<p>The <code>Setfit</code> library accept two inputs : &quot;text&quot; and &quot;label&quot;, <a href=""https://huggingface.co/blog/setfit"" rel=""nofollow noreferrer"">https://huggingface.co/blog/setfit</a></p>
<p>My goals is to train Setfit using two similarity input with binary label (similar or not similar). (&quot;text1&quot;,&quot;text2&quot;,&quot;similiar/not&quot;)</p>
<p>The example of dataset look like this (<a href=""https://huggingface.co/datasets/SetFit/mnli"" rel=""nofollow noreferrer"">setfit/mnli</a>) dataset:</p>
<pre><code>&gt;&gt;&gt; dataset = load_dataset('setfit/mnli')
&gt;&gt;&gt; dataset

DatasetDict({
    train: Dataset({
        features: ['text1', 'text2', 'label', 'idx', 'label_text'],
        num_rows: 392702
    })
    test: Dataset({
        features: ['text1', 'text2', 'label', 'idx', 'label_text'],
        num_rows: 9796
    })
    validation: Dataset({
        features: ['text1', 'text2', 'label', 'idx', 'label_text'],
        num_rows: 9815
    })
})
</code></pre>
<p>I tried:</p>
<pre><code>trainer = SetFitTrainer(
model=model,
train_dataset=train_dataset,
eval_dataset=eval_dataset,
loss_class=CosineSimilarityLoss,
metric=&quot;accuracy&quot;,
column_mapping={&quot;text1&quot;: &quot;text&quot;,&quot;text2&quot;: &quot;text&quot;, &quot;label&quot;: &quot;label&quot;} 
</code></pre>
<p>)</p>
<p>But fitting the raw Dataset with <code>text1</code> and <code>text2</code> doesn't work. <strong>Is there any way I could train with those kind of dataset of input?</strong></p>
",Training and Model Evaluation,create dataset object multiple input text setfit model library accept two input text label goal train setfit using two similarity input binary label similar similar text text similiar example dataset look like setfit mnli dataset tried fitting raw dataset work way could train kind dataset input
Loss function does not train,"<p>We are training a QuestionAnswering model for the SQUAD v2 dataset.</p>
<p>A RoBERTa encoder, with a classifier on top. Predicting the answer span works perfectly. However, we wanted to add a front classifier to predict the answerability of a question (as suggested in the paper &quot;Retrospective Reader for Machine Reading Comprehension&quot;).</p>
<p>Using the model below, the start and end loss are decreasing, however the answerable_loss does not train.</p>
<p>What we intend to do is:</p>
<ul>
<li>Get the first element of the encoded input</li>
<li>Predict (answerable, unanswerable) on this element</li>
<li>softmax(answerable, unanswerable) to get a certainty percentage</li>
<li>calculate the loss using cross entropy</li>
</ul>
<p>(PS: notice some tricky dimension stuff due to batches)</p>
<p>And to the best of my knowledge this is what we should do, and are doing in the code. But obviously it does not work...</p>
<p>I can't seem to find my error. Why might this be the case?</p>
<pre><code>class LSTMFrontVerifier(nn.Module):
    name = &quot;lstm-front-verifier&quot;
    
    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder
    
        self.answerable = nn.Linear(encoder.config.hidden_size, 2)
        self.classifier = nn.Linear(encoder.config.hidden_size, 2)
    
    def forward(self, input_ids, attention_mask=None, start_positions=None, end_positions=None):
        outputs = self.encoder(input_ids, attention_mask=attention_mask)
        lstm_output = outputs.last_hidden_state
    
        logits = self.classifier(lstm_output)
    
        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits, end_logits = start_logits.squeeze(-1), end_logits.squeeze(-1)
    
        # Given answer-ability
        unanswerabe = torch.logical_and(start_positions == 0, end_positions == 0).float()
    
        start_loss = F.cross_entropy(start_logits, start_positions)
        end_loss = F.cross_entropy(end_logits, end_positions)
    
        # Predict answerability
        pred_answerable = self.answerable(lstm_output[:, 0])
        answerable_pred = F.softmax(pred_answerable, dim=1)
        answerable_loss = F.cross_entropy(answerable_pred[:, 0], 1-unanswerabe) + \
                          F.cross_entropy(answerable_pred[:, 1], unanswerabe)
    
        print((start_loss + end_loss).item(), answerable_loss.item())
        loss = start_loss + end_loss + answerable_loss
    
        return loss

    def forward_ex(self, example):
        input_ids = example[&quot;input_ids&quot;].to(device)
        start_positions = example[&quot;start_positions&quot;].to(device) if &quot;start_positions&quot; in example else None
        end_positions = example[&quot;end_positions&quot;].to(device) if &quot;end_positions&quot; in example else None
        attention_mask = example[&quot;attention_mask&quot;].to(device) if &quot;attention_mask&quot; in example else None
        return self.forward(input_ids, attention_mask, start_positions, end_positions)
</code></pre>
<p>For reproducability purpouses here a minimal code example:</p>
<pre><code>import torch
from transformers import AutoTokenizer
from datasets import load_dataset
from torch.utils.data.dataloader import DataLoader
import torch.nn.functional as F
import torch.nn as nn
from transformers import AutoModel

max_train_examples = 1000
max_length = 384
stride = 128
device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;

tokenizer = AutoTokenizer.from_pretrained(&quot;roberta-base&quot;)

def preprocess_examples(examples):
    questions = [q.strip() for q in examples[&quot;question&quot;]]

    inputs = tokenizer(
        questions,
        examples[&quot;context&quot;],
        max_length=max_length,
        truncation=&quot;only_second&quot;,
        stride=stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding=&quot;max_length&quot;,
    )

    offset_mapping = inputs.pop(&quot;offset_mapping&quot;)
    sample_map = inputs.pop(&quot;overflow_to_sample_mapping&quot;)
    answers = examples[&quot;answers&quot;]
    start_positions = []
    end_positions = []

    for i, offset in enumerate(offset_mapping):
        sample_idx = sample_map[i]
        answer = answers[sample_idx]
        if not answer[&quot;answer_start&quot;]:
            start_positions.append(0)
            end_positions.append(0)
            continue

        start_char = answer[&quot;answer_start&quot;][0]
        end_char = answer[&quot;answer_start&quot;][0] + len(answer[&quot;text&quot;][0])
        sequence_ids = inputs.sequence_ids(i)

        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        if offset[context_start][0] &gt; start_char or offset[context_end][1] &lt; end_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            idx = context_start
            while idx &lt;= context_end and offset[idx][0] &lt;= start_char:
                idx += 1
            start_positions.append(idx - 1)

            idx = context_end
            while idx &gt;= context_start and offset[idx][1] &gt;= end_char:
                idx -= 1
            end_positions.append(idx + 1)

    inputs[&quot;start_positions&quot;] = start_positions
    inputs[&quot;end_positions&quot;] = end_positions
    return inputs


def convert_to_tensors(examples):
    return {k: torch.tensor([x[k] for x in examples]) for k in examples[0]}


squad = load_dataset('squad_v2')

squad[&quot;train&quot;] = squad[&quot;train&quot;].select(range(max_train_examples))
tokenized_datasets = squad.map(
    preprocess_examples,
    batched=True,
    remove_columns=squad[&quot;train&quot;].column_names,
)

train_dataloader = DataLoader(tokenized_datasets[&quot;train&quot;], batch_size=8, collate_fn=convert_to_tensors, shuffle=True)    

roberta = AutoModel.from_pretrained(&quot;roberta-base&quot;).to(device)
model = LSTMFrontVerifier(roberta).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-3)

model.train()
optimizer.zero_grad()  # Reset gradients tensors

for batch, x in enumerate(train_dataloader):
    # Compute prediction error
    loss = model.forward_ex(x)

    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
</code></pre>
<p>Packages:</p>
<ol>
<li>Install pytorch with gpu: <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/locally/</a></li>
<li>pip install transformers datasets</li>
</ol>
<p>Some prints of the print((start_loss + end_loss), answerable_loss):</p>
<pre><code>11.970989227294922 16.621864318847656
11.742887496948242 16.640228271484375
11.448827743530273 16.66130828857422
11.063633918762207 16.665538787841797
10.49462890625 16.663175582885742
10.176043510437012 16.661867141723633
10.13321590423584 16.646312713623047
10.05152702331543 16.64898681640625
9.76708698272705 16.648393630981445
9.409551620483398 16.700483322143555
8.939659118652344 16.641441345214844
9.03275203704834 16.647899627685547
8.160870552062988 16.63787078857422
8.27975845336914 16.641223907470703
7.900142669677734 16.64410972595215
6.427922248840332 16.644954681396484
6.4332380294799805 16.643535614013672
6.626171112060547 16.642642974853516
4.79502010345459 16.640335083007812
6.948017120361328 16.641925811767578
5.472411632537842 16.642606735229492
6.458420753479004 16.63710594177246
6.552549362182617 16.637182235717773
4.95197868347168 16.637977600097656
5.235410690307617 16.643829345703125
4.700412750244141 16.63840675354004
4.11396598815918 16.646831512451172
5.13016414642334 16.643505096435547
3.7867109775543213 16.63637924194336
5.582259178161621 16.643115997314453
5.7655229568481445 16.64023208618164
5.085046768188477 16.63158416748047
4.153951644897461 16.63810920715332
4.100613594055176 16.644237518310547
4.206878662109375 16.636249542236328
3.450410842895508 16.635835647583008
4.827783584594727 16.63633918762207
2.2874913215637207 16.644474029541016
2.3297667503356934 16.647319793701172
3.4870200157165527 16.652259826660156
3.31907320022583 16.6363582611084
4.377845764160156 16.637659072875977
3.427989959716797 16.635705947875977
4.224106311798096 16.640310287475586
</code></pre>
",Training and Model Evaluation,loss function doe train training questionanswering model squad v dataset roberta encoder classifier top predicting answer span work perfectly however wanted add front classifier predict answerability question suggested paper retrospective reader machine reading comprehension using model start end loss decreasing however answerable loss doe train intend get first element encoded input predict answerable unanswerable element softmax answerable unanswerable get certainty percentage calculate loss using cross entropy p notice tricky dimension stuff due batch best knowledge code obviously doe work seem find error might case reproducability purpouses minimal code example package install pytorch gpu pip install transformer datasets print print start loss end loss answerable loss
Is the Word2Vec Spark implementation distributed?,"<p>I'm relatively new to Spark and having some difficulty understanding Spark ML.</p>
<p>The problem I have is that I have 3TB of text, which I want to train a Word2Vec model on. The server I'm running on has around 1TB of ram and so I can't save the file temporarily.</p>
<p>The file is saved as a parquet that I import into Spark. The question I have is does the Spark ML library distribute the Word2Vec training? If so is there anything I need to worried about while processing such a large text file? If not, is there anyway to stream this data while training Word2Vec?</p>
",Training and Model Evaluation,word vec spark implementation distributed relatively new spark difficulty understanding spark ml problem tb text want train word vec model server running ha around tb ram save file temporarily file saved parquet import spark question doe spark ml library distribute word vec training anything need worried processing large text file anyway stream data training word vec
How can I handle overflowing tokens in Huggingface Transformer model?,"<p>I am training a XLM-RoBERTa model for token classification using Huggingface Transformers. My maximum token length of the already fine-tuned model is 166. I truncated longer and padded shorter sequences in the training data. Now, during inference/prediction time I would like to predict all tokens, even in sequences longer than 166. But if I read the documentation correctly, overflowing tokens get thrown away. Is that correct? I am not completely sure what the &quot;return_overflowing_tokens&quot; and stride parameters do. Could they be used to split sequences that are too long into two or more shorter sequences?</p>
<p>I have already tried to split my text data into sentences to have smaller chunks, but some of them still exceed the max token length. It would be ideal, if overflowing tokens would be automatically added to an additional sequence.</p>
",Training and Model Evaluation,handle overflowing token huggingface transformer model training xlm roberta model token classification using huggingface transformer maximum token length already fine tuned model truncated longer padded shorter sequence training data inference prediction time would like predict token even sequence longer read documentation correctly overflowing token get thrown away correct completely sure return overflowing token stride parameter could used split sequence long two shorter sequence already tried split text data sentence smaller chunk still exceed max token length would ideal overflowing token would automatically added additional sequence
How to properly prompt the decoder of a Transformer model?,"<p>I am using Hugging Face Transformers. I have a pretrained Encoder + Decoder model (Pegasus), and want to fine-tune it as described in <a href=""https://arxiv.org/abs/2104.07606"" rel=""nofollow noreferrer"">this article</a>.</p>
<p>Specifically, they use the following process:</p>
<p><a href=""https://i.sstatic.net/JZgtt.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JZgtt.jpg"" alt=""Summary generation using entity prompts"" /></a></p>
<p>In other words, they prepend a <em>manual</em> prompt to the generation of the model itself.</p>
<p>My question relates to the Decoder input. Specifically, I want to fine tune the model so that it takes the prompt (entity chain), and generates a summary from that point onwards.</p>
<p>For instance:</p>
<pre><code>&lt;s&gt; [ENTITYCHAIN] Frozen | Disney [SUMMARY] $tok_1 $tok_2 $tok_3 ...
=========================================== ^^^^^^ ^^^^^^ ^^^^^^
This is not generated                       Generate from here
</code></pre>
<p>However, as you would expect, the model is generating predictions for each token in the entity chain, which I do not need. But most importantly, the <strong>loss</strong> is being computed by also factoring in the predictions related to the entity chain. This clearly undermines the purpose of training, since it confuses the model, because it should learn to <strong>only generate the summary</strong>, and not the entity chain (which is already given as a prompt).</p>
<p>As I was saying, what I want is to give a prompt (entity chain) to the decoder, and make it generate a summary, while being able to attend to the extra information from the prompt. Of course, the loss should only be computed among the generated tokens, excluding the prompt tokens.</p>
<p>By looking into the <a href=""https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"" rel=""nofollow noreferrer"">model documentation</a>, I don't seem to find an option to do this. Any ideas? :)</p>
",Training and Model Evaluation,properly prompt decoder transformer model using hugging face transformer pretrained encoder decoder model pegasus want fine tune described article specifically use following process word prepend manual prompt generation model question relates decoder input specifically want fine tune model take prompt entity chain generates summary point onwards instance however would expect model generating prediction token entity chain need importantly loss computed also factoring prediction related entity chain clearly undermines purpose training since confuses model learn generate summary entity chain already given prompt wa saying want give prompt entity chain decoder make generate summary able attend extra information prompt course loss computed among generated token excluding prompt token looking model documentation seem find option idea
Can I add configuration of &#39;dropout_rate&#39; to Seq2SeqTrainer?,"<p>I'am trying to train T5 model using Seq2SeqTrainer.
I found out that the Config of T5 model is like below.</p>
<pre><code>T5Config {
  &quot;_name_or_path&quot;: &quot;allenai/tk-instruct-base-def-pos&quot;,
  &quot;architectures&quot;: [
    &quot;T5ForConditionalGeneration&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.5,
  &quot;d_ff&quot;: 2048,
  &quot;d_kv&quot;: 64,
  &quot;d_model&quot;: 768,
  &quot;decoder_start_token_id&quot;: 0,
  &quot;dense_act_fn&quot;: &quot;gelu_new&quot;,
  &quot;dropout_rate&quot;: 0.1,
  &quot;eos_token_id&quot;: 1,
  &quot;feed_forward_proj&quot;: &quot;gated-gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.5,
  &quot;initializer_factor&quot;: 1.0,
  &quot;is_encoder_decoder&quot;: true,
  &quot;is_gated_act&quot;: true,
  &quot;layer_norm_epsilon&quot;: 1e-06,
  &quot;model_type&quot;: &quot;t5&quot;,
  &quot;num_decoder_layers&quot;: 12,
  &quot;num_heads&quot;: 12,
  &quot;num_layers&quot;: 12,
  &quot;output_past&quot;: true,
  &quot;pad_token_id&quot;: 0,
  &quot;relative_attention_max_distance&quot;: 128,
  &quot;relative_attention_num_buckets&quot;: 32,
  &quot;tie_word_embeddings&quot;: false,
  &quot;torch_dtype&quot;: &quot;bfloat16&quot;,
  &quot;transformers_version&quot;: &quot;4.28.0&quot;,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 32100
}
</code></pre>
<p>But there is no configuration 'dropout_rate' in the Seq2SeqTrainer.
I wrote codes of training T5Generator like below.</p>
<pre><code>def train(self, tokenized_datasets, **kwargs):
        &quot;&quot;&quot;
        Train the generative model.
        &quot;&quot;&quot;
        #Set training arguments
        args = Seq2SeqTrainingArguments(
            **kwargs
        )

        # Define trainer object
        trainer = Seq2SeqTrainer(
            self.model,
            args, ...)
</code></pre>
<p>Is there a way to change configuration about dropout?
Or could you tell me some configurations that can make similar effects?</p>
<p>Here's what I've tried:</p>
<pre><code>
training_args = {
  'output_dir':model_out_path,
  'evaluation_strategy':&quot;epoch&quot;,
  'learning_rate':5e-5,
  'lr_scheduler_type':'cosine',
  'per_device_train_batch_size':8,
  'per_device_eval_batch_size':16,
  'num_train_epochs':4,
  'weight_decay':0.01,
  'warmup_ratio':0.1,
  'save_strategy':'no',
  'load_best_model_at_end':False,
  'push_to_hub':False,
  'eval_accumulation_steps':1,
  'predict_with_generate':True,
  'use_mps_device':use_mps_,
  &quot;dropout_rate&quot;: 0.3
  }
</code></pre>
<p>And I also tried to use this solution, <a href=""https://stackoverflow.com/questions/64947064/transformers-pretrained-model-with-dropout-setting"">Transformers pretrained model with dropout setting</a>
but, I couldn't understand where I should attach that codes...</p>
",Training and Model Evaluation,add configuration dropout rate seq seqtrainer trying train model using seq seqtrainer found config model like configuration dropout rate seq seqtrainer wrote code training generator like way change configuration dropout could tell configuration make similar effect tried also tried use solution href pretrained model dropout setting understand attach code
Sugestions on the best way to work with NLP mixed some numerical and categorical features,"<p>I'm working with a <strong>dataset of medicinal products across different countries</strong>, with each country having it's own data source. This results in the data not always being quite 'standardized' (for a lack of a better word), so one of the problems I'm trying to solve is to have the dosage in the same format across all countries. I've been doing it 'manually' for each country using <code>regex</code>, while having into account some criteria that I want to use as features in the model. For example: the <strong>number of active substances</strong> of the product, the <strong>pharmaceutical form</strong> and if some <strong>specific active substance is present in the product</strong>. By doing this 'manually' for like 1/3 of the countries, I've got a reasonable amount of records to train a model.</p>
<pre><code>Name   ActiveSubstances   NumberOfActSubst   PharmaceuticalForm   Dosage        DosageFinal

X      ['Y','Z']          2                  Tablet               '20mg/5mg'    '20 mg + 5 mg'

A      ['B']              1                  Tablet               '(50 microg+10mg)/ml''50 µg/ml + 10mg/ml'
</code></pre>
<p>I want this DosageFinal field to be filled automatically. What would be the best way to approach this task? <strong>I looked into parallel networks and the idea would be to use one NN to get the embeddings of the text variables, and another NN to collect the embeddings of the only numeric feature and later concatenate the embeddings.</strong> Am I overcomplicating it?</p>
",Training and Model Evaluation,sugestions best way work nlp mixed numerical categorical feature working dataset medicinal product across different country country data source result data always quite standardized lack better word one problem trying solve dosage format across country manually country using account criterion want use feature model example number active substance product pharmaceutical form specific active substance present product manually like country got reasonable amount record train model want dosagefinal field filled automatically would best way approach task looked parallel network idea would use one nn get embeddings text variable another nn collect embeddings numeric feature later concatenate embeddings overcomplicating
"Transformers from scratch - shape &#39;[1, 40, 64]&#39; is invalid for input of size when passing input from encoder to decoder","<h3>I'm trying to build a transformer model from scratch:</h3>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

class TransformerEncoder(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_heads, ff_size, dropout):
        super(TransformerEncoder, self).__init__()
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.pos_embedding = nn.Embedding(1000, hidden_size)  # Positional embedding
        self.dropout = nn.Dropout(dropout)
        
        self.self_attention = nn.MultiheadAttention(hidden_size, num_heads)
        self.layer_norm1 = nn.LayerNorm(hidden_size)
        
        self.feed_forward = nn.Sequential(
            nn.Linear(hidden_size, ff_size),
            nn.ReLU(),
            nn.Linear(ff_size, hidden_size)
        )
        self.layer_norm2 = nn.LayerNorm(hidden_size)
        
        self.num_layers = num_layers

    def forward(self, input_seq):
        seq_len, batch_size = input_seq.size()

        # Embedding and positional encoding
        embedded = self.embedding(input_seq)
        pos_ids = torch.arange(seq_len, device=input_seq.device).unsqueeze(0).expand_as(input_seq)
        pos_embedded = self.pos_embedding(pos_ids)
        encoded = self.dropout(embedded + pos_embedded)

        # Transformer encoder layers
        for _ in range(self.num_layers):
            # Self-attention
            attention_output, _ = self.self_attention(encoded, encoded, encoded)
            attention_output = self.layer_norm1(encoded + self.dropout(attention_output))
            
            # Feed-forward
            ff_output = self.feed_forward(attention_output)
            ff_output = self.layer_norm2(attention_output + self.dropout(ff_output))

            encoded = ff_output

        return encoded


class TransformerDecoder(nn.Module):
    def __init__(self, output_size, hidden_size, num_layers, num_heads, ff_size, dropout):
        super(TransformerDecoder, self).__init__()
        self.embedding = nn.Embedding(output_size, hidden_size)
        self.pos_embedding = nn.Embedding(1000, hidden_size)  # Positional embedding
        self.dropout = nn.Dropout(dropout)
        
        self.self_attention = nn.MultiheadAttention(hidden_size, num_heads)
        self.layer_norm1 = nn.LayerNorm(hidden_size)
        
        self.encoder_attention = nn.MultiheadAttention(hidden_size, num_heads)
        self.layer_norm2 = nn.LayerNorm(hidden_size)
        
        self.feed_forward = nn.Sequential(
            nn.Linear(hidden_size, ff_size),
            nn.ReLU(),
            nn.Linear(ff_size, hidden_size)
        )
        self.layer_norm3 = nn.LayerNorm(hidden_size)
        
        self.fc = nn.Linear(hidden_size, output_size)

        self.num_layers = num_layers

    def forward(self, input_seq, encoder_output):
        seq_len, batch_size = input_seq.size()

        # Embedding and positional encoding
        embedded = self.embedding(input_seq)
        pos_ids = torch.arange(seq_len, device=input_seq.device).unsqueeze(0).expand_as(input_seq)
        pos_embedded = self.pos_embedding(pos_ids)
        encoded = self.dropout(embedded + pos_embedded)

        # Transformer decoder layers
        for _ in range(self.num_layers):
            # Self-attention
            self_attention_output, _ = self.self_attention(encoded, encoded, encoded)
            self_attention_output = self.layer_norm1(encoded + self.dropout(self_attention_output))
            
            # Encoder-decoder attention
            encoder_attention_output, _ = self.encoder_attention(self_attention_output, encoder_output, encoder_output)
            encoder_attention_output = self.layer_norm2(self_attention_output + self.dropout(encoder_attention_output))
            
            # Feed-forward
            ff_output = self.feed_forward(encoder_attention_output)
            ff_output = self.layer_norm3(encoder_attention_output + self.dropout(ff_output))

            encoded = ff_output

        output = self.fc(encoded)
        return output


class Transformer(nn.Module):
    def __init__(self, input_size, output_size, hidden_size, num_layers, num_heads, ff_size, dropout):
        super(Transformer, self).__init__()
        self.encoder = TransformerEncoder(input_size, hidden_size, num_layers, num_heads, ff_size, dropout)
        self.decoder = TransformerDecoder(output_size, hidden_size, num_layers, num_heads, ff_size, dropout)

    def forward(self, input_seq, target_seq):
        encoder_output = self.encoder(input_seq)
        output = self.decoder(target_seq, encoder_output)
        return output


# Example parameters
input_size = 50265  # Vocabulary size
output_size = 50265  # Vocabulary size
hidden_size = 256  # Hidden state size of the transformer layers
num_layers = 2  # Number of transformer layers
num_heads = 4  # Number of attention heads
ff_size = 1024  # Feed-forward layer size
dropout = 0.1  # Dropout rate

# Example training loop
input_text = &quot;Hello, how are you?&quot;
output_text = &quot;I am doing well, thank you.&quot;

# Tokenization using Transformers
tokenizer = BartTokenizer.from_pretrained('bart-large-cnn')
tokenizer.add_tokens([&quot;&lt;custom_token_1&gt;&quot;, &quot;&lt;custom_token_2&gt;&quot;])

# Tokenize input and output text
input_tokens = tokenizer.encode(input_text, add_special_tokens=True, truncation=True, padding=True)
output_tokens = tokenizer.encode(output_text, add_special_tokens=True, truncation=True, padding=True)

# Convert tokens to tensors
input_tensor = torch.tensor(input_tokens).unsqueeze(0)  # Add batch dimension
output_tensor = torch.tensor(output_tokens).unsqueeze(0)  # Add batch dimension

# Initialize the transformer model
model = Transformer(input_size, output_size, hidden_size, num_layers, num_heads, ff_size, dropout)

# Example training loop
num_epochs = 10
learning_rate = 0.001
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

for epoch in range(num_epochs):
    # Perform forward pass
    output = model(input_tensor, output_tensor)
    
    # Compute loss
    loss = F.cross_entropy(output.view(-1, output_size), output_tensor.view(-1))
    
    # Backpropagation and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # Print the loss for monitoring
    print(f&quot;Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}&quot;)

# Test the trained model with input text
test_input_text = &quot;How's the weather today?&quot;
test_input_tokens = tokenizer.encode(test_input_text, add_special_tokens=True, truncation=True, padding=True)
test_input_tensor = torch.tensor(test_input_tokens).unsqueeze(0)  # Add batch dimension

# Set the model to evaluation mode
model.eval()

# Perform forward pass with the test input
test_output = model(test_input_tensor, torch.zeros(1, 1).long())

# Get the predicted class
_, predicted_classes = torch.max(test_output, dim=2)
predicted_classes = predicted_classes.squeeze(0).tolist()

# Convert the predicted class tokens to text
predicted_text = tokenizer.decode(predicted_classes, skip_special_tokens=True)

# Print the predicted output text
print(f&quot;Input: {test_input_text}&quot;)
print(f&quot;Predicted Output: {predicted_text}&quot;)
</code></pre>
<p>The code imports the required libraries and defines a Transformer model with an Encoder and a Decoder. The Encoder and the Decoder consist of multiple transformer layers. The Transformer model is trained using the Adam optimizer and Cross-Entropy loss function. The trained model is then used to predict the output for a test input.</p>
<p>The code also uses the <code>BartTokenizer</code> from the transformers library to tokenize the input and output text.</p>
<h2>Errors</h2>
<p>The following errors are encountered while running the code:</p>
<pre><code>RuntimeError: shape '[1, 40, 64]' is invalid for input of size 2048

</code></pre>
",Training and Model Evaluation,transformer scratch shape invalid input size passing input encoder decoder trying build transformer model scratch code import required library defines transformer model encoder decoder encoder decoder consist multiple transformer layer transformer model trained using adam optimizer cross entropy loss function trained model used predict output test input code also us transformer library tokenize input output text error following error encountered running code
Default optimizer and loss of transformers.Seq2SeqTrainer?,"<p>What is the default optimizer and loss of <code>transformers.Seq2SeqTrainer</code>? I checked here <a href=""https://huggingface.co/docs/transformers/main_classes/trainer"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/main_classes/trainer</a> but did not see any information.</p>
",Training and Model Evaluation,default optimizer loss transformer seq seqtrainer default optimizer loss checked see information
Prompt tuning for Dolly-v2-7b model for Question and Answer giving gibberish output?,"<p>I am following this page for <code>Prompt tuning for Dolly-v2-7b model for Question and Answer</code>: <a href=""https://huggingface.co/docs/peft/task_guides/clm-prompt-tuning"" rel=""nofollow noreferrer"">https://huggingface.co/docs/peft/task_guides/clm-prompt-tuning</a></p>
<p>Instead of doing the training in old <code>pytorch way</code>. I am doing the training using <code>Trainer api</code>. Also in this link
<a href=""https://huggingface.co/stevhliu/bloomz-560m_PROMPT_TUNING_CAUSAL_LM/tree/main"" rel=""nofollow noreferrer"">https://huggingface.co/stevhliu/bloomz-560m_PROMPT_TUNING_CAUSAL_LM/tree/main</a> , I see 2 files <code>adapter_config.json</code> and <code>adapter_model.bin</code>.</p>
<p>But when I save the model using Trainer api I do not see any <code>config file</code>.  Also model size is bigger than what is shown in above link.</p>
<p>Is this correct way to <strong>train</strong>, <strong>save</strong> and <strong>load</strong> model for Prompt Tuning. ?</p>
<p><strong>The inference take lot of time to generate. and gives some gibberish output</strong></p>
<p>Here is my code:
The use-case is:</p>
<p>I have <code>Context</code> which has lot of paragraphs and then <code>Question</code> , the model has to <code>answer</code> the <code>Question</code> based on <code>Context</code> in a professional manner. Also can it classify the <code>Question</code> as <strong>relevant</strong> if answer is present in <code>Context</code> and <strong>irrelevant</strong> if <code>answer</code> is not in <code>Context</code></p>
<p>The code that I have written is:</p>
<pre><code>    task_type=TaskType.CAUSAL_LM,
    prompt_tuning_init=PromptTuningInit.TEXT,
    num_virtual_tokens=30,
    prompt_tuning_init_text=&quot;Answer the question as truthfully as possible using and only using the provided context and if the answer is not contained within the context/text, say Irrelevant&quot;,
    tokenizer_name_or_path=&quot;dolly-v2-7b&quot;
)
</code></pre>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;dolly-v2-7b&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;dolly-v2-7b&quot;,load_in_8bit=True,device_map='auto') #,load_in_8bit=True
</code></pre>
<p><code>model = get_peft_model(model, peft_config)</code></p>
<pre><code>train_data = [
    {
        &quot;Context&quot;: &quot;How to Link Credit Card to ICICI Bank Account Step 1: Login to ICICIBank.com using your existing internet banking credentials. Step 2: Go to the 'Service Request' section. Step 3: Visit the 'Customer Service' option. Step 4: Select the Link Accounts/ Policy option to link your credit card to the existing user ID.&quot;,
        &quot;Question&quot;: &quot;How to add card?&quot;,
        &quot;Answer&quot;: &quot;Relevant. To add your card you can follow these steps: Step 1: Login to ICICIBank.com using your existing internet banking credentials. Step 2: Go to the 'Service Request' section. Step 3: Visit the 'Customer Service' option. Step 4: Select the Link Accounts/ Policy option to link your credit card to the existing user ID.&quot;
    },
    {
        &quot;Context&quot;: &quot;The python programming language is used in many different fields including web development, data analysis, artificial intelligence and scientific computing. It is a high-level language that is easy to learn and has a large community of users who can provide support and advice. &quot;,
        &quot;Question&quot;: &quot;What is Python used for?&quot;,
        &quot;Answer&quot;: &quot;Relevant. Python is used in many different fields including web development, data analysis, artificial intelligence and scientific computing.&quot;
    }
]
</code></pre>
<h1>Define a function to map examples to inputs and targets</h1>
<pre><code>def preprocess_function(examples):
    tokenized_examples = tokenizer(
        examples[&quot;Question&quot;][0],
        examples[&quot;Context&quot;][0],
        truncation=True,
        max_length=1024,
        padding=&quot;max_length&quot;
    )
    tokenized_examples['labels']=tokenizer(
        examples[&quot;Answer&quot;],
        truncation=True,
        max_length=1024,
        padding=&quot;max_length&quot;,
        return_tensors=&quot;pt&quot;)['input_ids'][0]
        
    return tokenized_examples
</code></pre>
<p><code>tokenized_train_data = [preprocess_function(example) for example in train_data]</code></p>
<pre><code>
class DemoDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data[idx]
        
        item = {k: torch.tensor(v) for k, v in sample.items()}
        return item
</code></pre>
<p><code>dataset = DemoDataset(tokenized_train_data)</code></p>
<pre><code>training_args = TrainingArguments(
    output_dir=&quot;results&quot;,
    learning_rate=1e-5,
    per_device_train_batch_size=1,
    num_train_epochs=10,
    weight_decay=0.01,
    logging_steps=1,
    save_steps=1,
    logging_dir=&quot;logs&quot;
)
</code></pre>
<pre><code>trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    # data_collator=data_collator,
    tokenizer=tokenizer
)
trainer.train()
</code></pre>
<p><strong>Is this correct way to save?</strong></p>
<p><code>trainer.save_model(&quot;dolly3b_demo_model&quot;)</code></p>
<p><strong>Inference</strong></p>
<p><strong>Is this correct way to do inference</strong></p>
<pre><code>from peft import PeftModel, PeftConfig
tokenizer = AutoTokenizer.from_pretrained(&quot;dolly-v2-3b&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;dolly3b_demo_model&quot;)
model = get_peft_model(model, peft_config)
</code></pre>
<pre><code>
# Define example
context = &quot;How to Link Credit Card to ICICI Bank Account Step 1: Login to ICICIBank.com using your existing internet banking credentials. Step 2: Go to the 'Service Request' section. Step 3: Visit the 'Customer Service' option. Step 4: Select the Link Accounts/ Policy option to link your credit card to the existing user ID.&quot;
question = &quot;How to add card?&quot;

# Encode inputs with prompt and tokenize
inputs = [f&quot;{context} {question}&quot;]
inputs_encoded = tokenizer(inputs, padding=True, truncation=True, max_length=1024, return_tensors=&quot;pt&quot;)
</code></pre>
<pre><code>outputs = model.generate(input_ids=inputs_encoded[&quot;input_ids&quot;], attention_mask=inputs_encoded[&quot;attention_mask&quot;], max_new_tokens=200,)
print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))
</code></pre>
",Training and Model Evaluation,prompt tuning dolly v b model question answer giving gibberish output following page instead training old training using also link see file save model using trainer api see also model size bigger shown link correct way train save load model prompt tuning inference take lot time generate give gibberish output code use case ha lot paragraph model ha based professional manner also classify relevant answer present irrelevant code written define function map example input target correct way save inference correct way inference
Issue with adding evaluation dataset with T5Model Training,"<p>I am training a T5 model from simpletransformers. I am getting an error in the following line -
<strong>model.train_model(train, eval_data = eval_data)</strong></p>
<p>The ERROR is as follows -</p>
<pre><code>*/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(&quot;Detected call of `lr_scheduler.step()` before `optimizer.step()`. &quot;
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-16-89940dd49e1d&gt; in &lt;cell line: 1&gt;()
----&gt; 1 model.train_model(train, eval_data = eval_data)

4 frames
/usr/local/lib/python3.10/dist-packages/simpletransformers/t5/t5_model.py in train_model(self, train_data, output_dir, show_running_loss, args, eval_data, verbose, **kwargs)
    227         os.makedirs(output_dir, exist_ok=True)
    228 
--&gt; 229         global_step, training_details = self.train(
    230             train_dataset,
    231             output_dir,

/usr/local/lib/python3.10/dist-packages/simpletransformers/t5/t5_model.py in train(self, train_dataset, output_dir, show_running_loss, eval_data, verbose, **kwargs)
    756 
    757             if args.evaluate_during_training and args.evaluate_each_epoch:
--&gt; 758                 results = self.eval_model(
    759                     eval_data,
    760                     verbose=verbose and args.evaluate_during_training_verbose,

/usr/local/lib/python3.10/dist-packages/simpletransformers/t5/t5_model.py in eval_model(self, eval_data, output_dir, verbose, silent, **kwargs)
    912         self._move_model_to_device()
    913 
--&gt; 914         eval_dataset = self.load_and_cache_examples(
    915             eval_data, evaluate=True, verbose=verbose, silent=silent
    916         )

/usr/local/lib/python3.10/dist-packages/simpletransformers/t5/t5_model.py in load_and_cache_examples(self, data, evaluate, no_cache, verbose, silent)
   1181             return CustomDataset(tokenizer, args, data, mode)
   1182         else:
-&gt; 1183             return T5Dataset(
   1184                 tokenizer,
   1185                 self.args,

/usr/local/lib/python3.10/dist-packages/simpletransformers/t5/t5_utils.py in __init__(self, tokenizer, args, data, mode)
    161                 (prefix, input_text, target_text, tokenizer, args)
    162                 for prefix, input_text, target_text in zip(
--&gt; 163                     data[&quot;prefix&quot;], data[&quot;input_text&quot;], data[&quot;target_text&quot;]
    164                 )
    165             ]

TypeError: list indices must be integers or slices, not str*
</code></pre>
<p>Please note that both the train, and evaluation datasets have a prefix, input_text, and target_text columns.
No idea where <em>TypeError: list indices must be integers or slices, not str</em> is coming from</p>
<p>I am using Google Colab to train. I tried opening the internal t5_model.py file, adding print statements, and changing here and there to get a hang of what the issue is, but I don't think my changes are getting reflected because it is stuck to throwing an error at line 929.
The same code was working a few days back.</p>
",Training and Model Evaluation,issue adding evaluation dataset model training training model simpletransformers getting error following line model train model train eval data eval data error follows please note train evaluation datasets prefix input text target text column idea typeerror list index must integer slice str coming using google colab train tried opening internal model py file adding print statement changing get hang issue think change getting reflected stuck throwing error line code wa working day back
How can I check a confusion_matrix after fine-tuning with custom datasets?,"<p>This question is the same with <a href=""https://datascience.stackexchange.com/questions/99815/how-can-i-check-a-confusion-matrix-after-fine-tuning-with-custom-datasets"">How can I check a confusion_matrix after fine-tuning with custom datasets?</a>, on Data Science Stack Exchange.</p>
<h2>Background</h2>
<p>I would like to check a confusion_matrix, including precision, recall, and f1-score like below after fine-tuning with custom datasets.</p>
<p>Fine tuning process and the task are <a href=""https://huggingface.co/transformers/custom_datasets.html#sequence-classification-with-imdb-reviews"" rel=""noreferrer"">Sequence Classification with IMDb Reviews</a> on the <a href=""https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-trainer"" rel=""noreferrer"">Fine-tuning with custom datasets tutorial on Hugging face</a>.</p>
<p>After finishing the fine-tune with Trainer, how can I check a confusion_matrix in this case?</p>
<p>An image of confusion_matrix, including precision, recall, and f1-score <a href=""http://www.renom.jp/notebooks/product/renom_dl/trainer/notebook.html"" rel=""noreferrer"">original site</a>: just for example output image</p>
<pre><code>predictions = np.argmax(trainer.test(test_x), axis=1)

# Confusion matrix and classification report.
print(classification_report(test_y, predictions))

            precision    recall  f1-score   support

          0       0.75      0.79      0.77      1000
          1       0.81      0.87      0.84      1000
          2       0.63      0.61      0.62      1000
          3       0.55      0.47      0.50      1000
          4       0.66      0.66      0.66      1000
          5       0.62      0.64      0.63      1000
          6       0.74      0.83      0.78      1000
          7       0.80      0.74      0.77      1000
          8       0.85      0.81      0.83      1000
          9       0.79      0.80      0.80      1000

avg / total       0.72      0.72      0.72     10000
</code></pre>
<h2>Code</h2>
<pre class=""lang-py prettyprint-override""><code>from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
)

model = DistilBertForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased&quot;)

trainer = Trainer(
    model=model,                         # the instantiated 🤗 Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset             # evaluation dataset
)

trainer.train()
</code></pre>
<h2>What I did so far</h2>
<p>Data set Preparation for <a href=""https://huggingface.co/transformers/custom_datasets.html#sequence-classification-with-imdb-reviews"" rel=""noreferrer"">Sequence Classification with IMDb Reviews</a>, and I'm fine-tuning with Trainer.</p>
<pre><code>from pathlib import Path

def read_imdb_split(split_dir):
    split_dir = Path(split_dir)
    texts = []
    labels = []
    for label_dir in [&quot;pos&quot;, &quot;neg&quot;]:
        for text_file in (split_dir/label_dir).iterdir():
            texts.append(text_file.read_text())
            labels.append(0 if label_dir is &quot;neg&quot; else 1)

    return texts, labels

train_texts, train_labels = read_imdb_split('aclImdb/train')
test_texts, test_labels = read_imdb_split('aclImdb/test')

from sklearn.model_selection import train_test_split
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)

from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)

import torch

class IMDbDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = IMDbDataset(train_encodings, train_labels)
val_dataset = IMDbDataset(val_encodings, val_labels)
test_dataset = IMDbDataset(test_encodings, test_labels)
</code></pre>
",Training and Model Evaluation,check confusion matrix fine tuning custom datasets question sequence classification imdb review fine tuning custom datasets tutorial hugging face finishing fine tune trainer check confusion matrix case image confusion matrix including precision recall f score original site example output image code far data set preparation sequence classification imdb review fine tuning trainer
Find similarity in a very precise numerical environment,"<p>I have a list of 100+ sentences, and I need to find which is the closest to a user prompt.
The thing is that we are dealing with very precise, nuanced prompt, because we analyze numeric data.
Eaxmple:</p>
<pre><code>1. Did variable x changed at least 5% in the past week ?
2. show me variable x change in the past week
</code></pre>
<p>In this example, sentence 1 and 2 are <strong>totally different in the context of a chart but similar in the global context</strong>, but most simple models like <code>spaCY</code> will rate them as very similar (0.9+) because they have many similar words.</p>
<p>What is the way to go to be able to train a model or to use a trained model, to find similarity in a very precise numerical environment like this, where sentences have many similar words but have total different meaning ?</p>
<p>I used this <code>spaCY</code> model:</p>
<pre><code>prompt_doc = nlp(user_prompt)
similarities = []

 
for sentence in sentences:
    sentence_doc = nlp(sentence)
    similarity = prompt_doc.similarity(sentence_doc)
    similarities.append(similarity)
    print(&quot;Sentence:&quot;, sentence)
    print(&quot;Similarity rating:&quot;, similarity)
    print()
</code></pre>
<p>The result for 100 sentences like the above, was that all of them have around 0.8-0.9 similarity. Which is very wrong.</p>
",Training and Model Evaluation,find similarity precise numerical environment list sentence need find closest user prompt thing dealing precise nuanced prompt analyze numeric data eaxmple example sentence totally different context chart similar global context simple model like rate similar many similar word way go able train model use trained model find similarity precise numerical environment like sentence many similar word total different meaning used model result sentence like wa around similarity wrong
"Error : Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 2])) while training a binary classifier deepset/gbert-base","<p>I am aware of most of the solutions which are discussed here previously regarding the same problem but still I had no luck with those solutions.</p>
<p>I’m trying to implement a <strong>binary classifier</strong>. I’m using is a <strong>customized dataset</strong> and having one text column with german text data and the label column has two classes either 0 or 1.</p>
<p>I’m using here the <strong>deepset/gbert-base</strong> model and number of labels as 2.
I have followed the official tutorial of hugging face <a href=""https://huggingface.co/learn/nlp-course/chapter3/4"" rel=""nofollow noreferrer"">https://huggingface.co/learn/nlp-course/chapter3/4</a>
I’m getting everything similar till the step:</p>
<pre><code>outputs = model(**batch)
</code></pre>
<p>I have tried the following work arounds suggested in this forum and other coding forums. Which are mentioned below:</p>
<ol>
<li><p>I checked the pytorch version(Suggested by online forums : to update the pytorch version which are below verison 2) and I’m using the following:<code>2.0.0+cu118</code></p>
</li>
<li><p>The labels are of the float type and does not contain any null value (Suggested by online forums : to check if the data type of labels is float as the model expect it in that format)</p>
</li>
<li><p>Also tried to change the label shape from [0] and [1] to [1,0] for class zero and [0,1] for class 1 because the error says the input from the model to the loss function is of size [16,2] and the target size which are labels here are of size [16] . But changing the shape from [0] and [1] to [1,0] for class zero and [0,1] for class 1 also did not solve the problem.</p>
</li>
<li><p>I also tried to implement through Trainer API following the official tutorial of hugging face <a href=""https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt"" rel=""nofollow noreferrer"">https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt</a> and tried to customize the loss function from binary_cross_entropy_with_logits to nn.CrossEntropyLoss() . Just tried to change the loss function to see if the code runs but ended up with the same error.</p>
</li>
<li><p>Also tried using different models apart from the above mentioned model. which are:</p>
</li>
</ol>
<p>nlptown/bert-base-multilingual-uncased-sentiment
papluca/xlm-roberta-base-language-detection
oliverguhr/german-sentiment-bert</p>
<p>But getting the same error.</p>
<p>Code:</p>
<pre><code>from transformers import AutoTokenizer, DataCollatorWithPadding
tokenizer = AutoTokenizer.from_pretrained(&quot;deepset/gbert-base&quot;)
 
def tokenize_function(examples):
    return tokenizer(examples[&quot;text1&quot;], truncation=True)
 
tokenized_datasets = final_dataset_dict.map(tokenize_function, batched=True)
data_collator= DataCollatorWithPadding(tokenizer)
tokenized_datasets = tokenized_datasets.remove_columns([&quot;text1&quot;])
tokenized_datasets[&quot;train&quot;].column_names
tokenized_datasets.set_format(&quot;torch&quot;)
 
from torch.utils.data import DataLoader
 
train_dataloader = DataLoader(tokenized_datasets[&quot;train&quot;], shuffle = True, batch_size = 8, collate_fn = data_collator)
eval_dataloader = DataLoader(tokenized_datasets[&quot;unsupervised&quot;], batch_size = 8, collate_fn = data_collator)
 
for batch in train_dataloader:
  break
print({k: v.shape for k, v in batch.items()})
#print(batch)
 
from transformers import AutoModelForSequenceClassification
checkpoint = &quot;deepset/gbert-base&quot;
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels =2)
 
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
</code></pre>
<p><strong>After tokenization my data looks like this :</strong></p>
<blockquote>
<p>DatasetDict({
train: Dataset({
features: ['text1', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],
num_rows: 2512
})
test: Dataset({
features: ['text1', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],
num_rows: 1255
})
validation: Dataset({
features: ['text1', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],
num_rows: 1255
})
})
<strong>The batch items in the train_dataloader looks like this.</strong>
{'labels': torch.Size([8]), 'input_ids': torch.Size([8, 69]), 'token_type_ids': torch.Size([8, 69]), 'attention_mask': torch.Size([8, 69])}
<strong>The detailed error is as follows:</strong></p>
</blockquote>
<pre><code> ---------------------------------------------------------------------------
 ValueError                                Traceback (most recent call last)
 &lt;ipython-input-36-b84c8f6552ab&gt; in &lt;cell line: 1&gt;()
 ----&gt; 1 outputs = model(**batch)
       2 #print(outputs.shape)
       3 print(outputs.loss, outputs.logits.shape)
 
 4 frames
 /usr/local/lib/python3.9/dist-packages/torch/nn/functional.py in binary_cross_entropy_with_logits(input, target, weight, size_average, reduce, reduction, pos_weight)
    3161 
    3162     if not (target.size() == input.size()):
 -&gt; 3163         raise ValueError(&quot;Target size ({}) must be the same as input size ({})&quot;.format(target.size(), input.size()))
    3164 
    3165     return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)
 
 ValueError: Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 2]))
</code></pre>
<p>Any lead from this problem will be very much appreciated.</p>
<p>I expect the output like :
<a href=""https://i.sstatic.net/ouIPC.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",Training and Model Evaluation,error target size torch size must input size torch size training binary classifier deepset gbert base aware solution discussed previously regarding problem still luck solution trying implement binary classifier using customized dataset one text column german text data label column ha two class either using deepset gbert base model number label followed official tutorial hugging face getting everything similar till step tried following work arounds suggested forum coding forum mentioned checked pytorch version suggested online forum update pytorch version verison using following label float type doe contain null value suggested online forum check data type label float model expect format also tried change label shape class zero class error say input model loss function size target size label size changing shape class zero class also solve problem also tried implement trainer api following official tutorial hugging face tried customize loss function binary cross entropy logits nn crossentropyloss tried change loss function see code run ended error also tried using different model apart mentioned model nlptown bert base multilingual uncased sentiment papluca xlm roberta base language detection oliverguhr german sentiment bert getting error code tokenization data look like datasetdict train dataset feature text label input id token type id attention mask num row test dataset feature text label input id token type id attention mask num row validation dataset feature text label input id token type id attention mask num row batch item train dataloader look like label torch size input id torch size token type id torch size attention mask torch size detailed error follows lead problem much appreciated expect output like enter image description
Difficulty in Training a Question-Answering Model,"<p>I'm trying to train a simple deep learning model for question-answering, but I'm not getting good results. As you can see in the code below, the model contains:</p>
<ul>
<li>An embedding layer that generates a token and positional embedding for the src input.</li>
<li>A transformer encoder.</li>
<li>A decoder in the form of a
linear layer, which takes the output of the encoder and generate
logits from it.</li>
</ul>
<p>The model takes in input a batch of tokenized sentences of shape (batch_size, max_length). The embedding and the encoder's output are of shape (batch_size, max_length, vocab_size) and the generated tokens are of shape (batch_size, max_length). The model is then trained with cross entropy loss.</p>
<p>For the tokenization, I don't include  and , there is only  token for padding, and  for unknown words. That's because I think the model will learn to generate a padded sequence, so there's no need to use them, plus it always generates a sequence of length max_length.</p>
<pre><code>class Model(nn.Module):

    def __init__(self,
                 vocab_size,
                 hidden_size,
                 max_length,
                 pad_id,
                 num_encoder_layers: int = 2,
                 layer_norm_eps: float = 1e-5):
        super(Model, self).__init__()

        self.pad_id = pad_id

        self.embeddings = Embedding(
            vocab_size=vocab_size,
            hidden_size=hidden_size,
            max_position_embeddings=max_length,
            pad_token_id=pad_id
        )

        encoder_layer = TransformerEncoderLayer(hidden_size, num_encoder_layers)
        encoder_norm = LayerNorm(hidden_size, eps=layer_norm_eps)
        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)

        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, src, trg=None):
        src_embeddings = self.embeddings(src)

        output = self.encoder(src_embeddings)
        logits = self.fc(output)

        if trg is not None:
            loss = F.cross_entropy(
                input=logits.view(-1, logits.size(-1)),
                target=trg.view(-1),
            )
        else:
            loss = None

        predicted_ids = torch.argmax(logits, dim=-1)

        return predicted_ids, loss
</code></pre>
<p>What I'm doing wrong? The generated sequences are too bad, even when the validation loss is low. (If you decided to answer, please provide a detailed answer. I'm having a hard time detecting what's wrong)</p>
<p>Note:</p>
<ul>
<li>I think the data is enough to train this model.</li>
<li>The model is big enough to computer pattern's in the data.</li>
<li>I toyed with different hyperparams.</li>
<li>The loss curves don't show any overfitting.</li>
</ul>
",Training and Model Evaluation,difficulty training question answering model trying train simple deep learning model question answering getting good result see code model contains embedding layer generates token positional embedding src input transformer encoder decoder form linear layer take output encoder generate logits model take input batch tokenized sentence shape batch size max length embedding encoder output shape batch size max length vocab size generated token shape batch size max length model trained cross entropy loss tokenization include token padding unknown word think model learn generate padded sequence need use plus always generates sequence length max length wrong generated sequence bad even validation loss low decided answer please provide detailed answer hard time detecting wrong note think data enough train model model big enough computer pattern data toyed different hyperparams loss curve show overfitting
Roberta with GRU is not training,"<p>I'm trying to fine-tune RoBERTa and integrate external knowledge via a BiGRU block. But the model is not learning (the train loss is around 0.8 and is not decreasing). There is no problem with the data, I tried some other RoBERTa-based models on the same dataset and it worked fine.</p>
<p>Here is the architecture:</p>
<pre><code>class CustomRoberta(PreTrainedModel):

    def __init__(self, config, num_labels, max_em_len_1, max_em_len_2, no_bert_layers=2):
        super(CustomRoberta, self).__init__(config)
        self.num_labels = num_labels
        self.bert = RobertaModel.from_pretrained(&quot;roberta-large&quot;)
        self.hidden_size = self.config.hidden_size
        self.emotion_embeddings = nn.Embedding(max_em_len_1, self.config.hidden_size)
        self.opinion_embeddings = nn.Embedding(max_em_len_2, self.config.hidden_size)
        self.dropout = nn.Dropout(0.1)
        self.dense = nn.Linear(self.config.hidden_size * 3 * 3 * 2, num_labels) # avd_pool max_pool, last hidden state
        self.apply(self.init_bert_weights)
        self.linear = nn.Linear(self.config.hidden_size, num_labels)

        self.gru = nn.GRU(self.config.hidden_size * 3, self.config.hidden_size * 3, bidirectional=True, batch_first=True)


    def init_bert_weights(self, module):
        &quot;&quot;&quot; Initialize the weights.&quot;&quot;&quot;
        if isinstance(module, (nn.Linear, nn.Embedding)):
            nn.init.xavier_uniform_(module.weight)
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
        if isinstance(module, nn.Linear) and module.bias is not None:
            module.bias.data.zero_()

    def get_att(self, hiddes, emotion_embd):

        concat = torch.cat([hiddes, emotion_embd], -1)
        g = self.att_lin(concat)
        alpha = F.softmax(g, dim=0)
        att_hidden = alpha * hiddes
        return att_hidden

    def forward(self, input_ids, opinion_ids, attention_mask, emotion_ids, return_indices=False):
        bert_encoded_layers_raw = self.bert(input_ids, attention_mask).last_hidden_state 
        
        bert_encoded_layers = self.dropout(bert_encoded_layers_raw)

        
        emotion_embeddings = self.emotion_embeddings(emotion_ids)
        opinion_embeddings = self.opinion_embeddings(opinion_ids)
        eks = torch.cat((opinion_embeddings,  emotion_embeddings), -1)
        concat = torch.cat((bert_encoded_layers, eks), -1)
        gru_all_hidden, gru_last_hidden = self.gru(concat, torch.zeros(2, concat.shape[0], self.config.hidden_size * 3).to(device))

        gru_last_hidden_dir0 = gru_last_hidden[0, :, :]
        gru_last_hidden_dir1 = gru_last_hidden[1, :, :]


        gru_last_hidden_stacked = torch.cat(
            (gru_last_hidden_dir0, gru_last_hidden_dir1), dim=1
        )

        gru_avg = torch.mean(gru_all_hidden, dim=1)
        gru_max, _ = torch.max(gru_all_hidden, dim=1)
        gru_complete_concatted = torch.cat(
            (gru_last_hidden_stacked, gru_avg, gru_max), dim=1
        )
        logits = self.dense(gru_complete_concatted)
        

        return logits
</code></pre>
<p>Here is the training code:</p>
<pre><code>roberta = CustomRoberta(config=config, num_labels=3, max_em_len_1=3, max_em_len_2=4)
roberta = roberta.to(device)
param_optimizer = list(roberta.named_parameters())
no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.1},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]


batch_size = 16
epochs = 5
optimizer = AdamW(optimizer_grouped_parameters, lr=5e-6)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*total_steps, num_training_steps=total_steps)
loss_fn = torch.nn.CrossEntropyLoss()


for epoch in range(epochs):
            loss_epoch = []
            for step, batch in enumerate(train_dataloader):
                model.train()
                input_ids, opinion_ids, input_mask, emotion_ids, label_ids = batch
                logits = model(input_ids=input_ids.to(device), opinion_ids=opinion_ids.to(device), attention_mask=input_mask.to(device), emotion_ids=emotion_ids.to(device))
                loss = loss_fn(logits, label_ids.to(device))
                loss.backward()
                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                loss_epoch.append(loss.item())

                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
        
</code></pre>
<p>What I've already checked and double-checked:</p>
<ul>
<li>if the weights are changing during backpropagation (they are)</li>
<li>if the DataLoader works as expected and shuffle=True</li>
<li>to overfit the model on a tiny subset and many epochs (it can't overfit at all)</li>
<li>decreasing and increasing learning rate</li>
<li>deleting the entire GRU part and only training RoBERTa and the final dense layer</li>
</ul>
<p>None of these worked. Please help!!</p>
",Training and Model Evaluation,roberta gru training trying fine tune roberta integrate external knowledge via bigru block model learning train loss around decreasing problem data tried roberta based model dataset worked fine architecture training code already checked double checked weight changing backpropagation dataloader work expected shuffle true overfit model tiny subset many epoch overfit decreasing increasing learning rate deleting entire gru part training roberta final dense layer none worked please help
Extract relationship concepts from sentences,"<p>Is there a current model or how could I train a model that takes a sentence involving two subjects like: </p>

<blockquote>
  <p>[Meiosis] is a type of [cell division]...</p>
</blockquote>

<p>and decides if one is the child or parent concept of the other? In this case, cell division is the parent of meiosis.</p>
",Training and Model Evaluation,extract relationship concept sentence current model could train model take sentence involving two subject like meiosis type cell division decides one child parent concept case cell division parent meiosis
sentence transformer use of evaluator,"<p>I came across <a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/sts/training_stsbenchmark_continue_training.py"" rel=""nofollow noreferrer"">this script</a> which is second link on <a href=""https://www.sbert.net/examples/training/sts/README.html"" rel=""nofollow noreferrer"">this page</a>  and <a href=""https://www.sbert.net/docs/package_reference/SentenceTransformer.html"" rel=""nofollow noreferrer"">this explanation</a>
I am using <code>all-mpnet-base-v2</code> (<a href=""https://huggingface.co/sentence-transformers/all-mpnet-base-v2"" rel=""nofollow noreferrer"">link</a>) and I am using my custom data</p>
<p>I am having hard time understanding use of</p>
<pre><code>evaluator = EmbeddingSimilarityEvaluator.from_input_examples(
    dev_samples, name='sts-dev')
</code></pre>
<p>The documentation says:</p>
<blockquote>
<p>evaluator – An evaluator (sentence_transformers.evaluation) evaluates the model performance during training on held-out dev data. It is used to determine the best model that is saved to disc.</p>
</blockquote>
<p>But in this case, as we are fine tuning on our own examples, <code>train_dataloader</code>has <code>train_samples</code> which has our model sentences and scores.</p>
<p><strong>Q1. How is <code>train_samples</code> different than <code>dev_samples</code>?</strong></p>
<p><strong>Q2a: If the model is going to print performance against <code>dev_samples</code> then how is it going to help &quot;<em>to determine the best model that is saved to disc</em>&quot;?</strong></p>
<p><strong>Q2b: Are we required to run <code>dev_samples</code> against the model saved on the disc and then compare scores?</strong></p>
<p><strong>Q3. If my goal is to take a single model and then fine tune it, is it okay to skip parameters <code>evaluator</code> and <code>evaluation_steps</code>?</strong></p>
<p><strong>Q4. How to determine total steps in the model? Do I need to set <code>evaluation_steps</code>?</strong></p>
<hr />
<h3>Updated</h3>
<p>I followed the answer provided by Kyle and have below follow up questions</p>
<p>In the <code>fit</code> method I used the <code>evaluator</code> and below data was written to a file
<a href=""https://i.sstatic.net/9QhYz.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9QhYz.jpg"" alt=""enter image description here"" /></a></p>
<p><strong>Q5. which metric is used to select the best epoch? is it <code>cosine_pearson</code>?</strong></p>
<p><strong>Q6: why steps are <code>-1</code> in the above output?</strong></p>
<p><strong>Q7a: how to find steps based upon size of my data, batch size etc.</strong></p>
<p>Currently i have kept them to 1000. But not sure if that it is too much. I am running for 10 epochs, i have 2509 examples in the training data and batch size is 64.</p>
<p><strong>Q7b: are my steps going to be 2509/64?</strong> if yes then 1000 seems to be too high number</p>
",Training and Model Evaluation,sentence transformer use evaluator came across script second link page explanation using link using custom data hard time understanding use documentation say evaluator evaluator sentence transformer evaluation evaluates model performance training held dev data used determine best model saved disc case fine tuning example ha ha model sentence score q different q model going print performance going help determine best model saved disc q b required run model saved disc compare score q goal take single model fine tune okay skip parameter q determine total step model need set updated followed answer provided kyle follow question method used data wa written file q metric used select best epoch q step output q find step based upon size data batch size etc currently kept sure much running epoch example training data batch size q b step going yes seems high number
Determining tense of a sentence Python,"<p>Following several other posts, [e.g. <a href=""https://stackoverflow.com/questions/3434144/detect-english-verb-tenses-using-nltk"">Detect English verb tenses using NLTK</a> , <a href=""https://stackoverflow.com/questions/19966345/identifying-verb-tenses-in-python"">Identifying verb tenses in python</a>, <a href=""https://stackoverflow.com/questions/2539782/python-nltk-figure-out-tense"">Python NLTK figure out tense</a> ] I wrote the following code to determine tense of a sentence in Python using POS tagging: </p>

<pre><code>from nltk import word_tokenize, pos_tag

def determine_tense_input(sentence):
    text = word_tokenize(sentence)
    tagged = pos_tag(text)

    tense = {}
    tense[""future""] = len([word for word in tagged if word[1] == ""MD""])
    tense[""present""] = len([word for word in tagged if word[1] in [""VBP"", ""VBZ"",""VBG""]])
    tense[""past""] = len([word for word in tagged if word[1] in [""VBD"", ""VBN""]]) 
    return(tense)
</code></pre>

<p>This returns a value for the usage of past/present/future verbs, which I typically then take the max value of as the tense of the sentence. The accuracy is moderately decent, but I am wondering if there is a better way of doing this.</p>

<p>For example, is there now by-chance a package written which is more dedicated to extracting the tense of a sentence? [note - 2 of the 3 stack-overflow posts are 4-years old, so things may have now changed]. Or alternatively, should I be using a different parser from within nltk to increase accuracy? If not, hope the above code may help someone else!</p>
",Training and Model Evaluation,determining tense sentence python following several post e g href nltk figure tense wrote following code determine tense sentence python using po tagging return value usage past present future verb typically take max value tense sentence accuracy moderately decent wondering better way example chance package written dedicated extracting tense sentence note stack overflow post year old thing may changed alternatively using different parser within nltk increase accuracy hope code may help someone else
How to specify document language while importing a dataset in Google Cloud AutoML?,"<p>I am trying to train a model for text classification in VertexAI AutoML (Google Cloud) using documents in Spanish. I imported the documents as JSON lines and tried specifying the language of each document as follows:</p>
<pre><code>{&quot;textContent&quot;:&quot;Esto está escrito en español&quot;,&quot;languageCode&quot;:&quot;es-ES&quot;,&quot;classificationAnnotations&quot;:[{&quot;displayName&quot;:&quot;Class A&quot;},{&quot;displayName&quot;:&quot;Class B&quot;}]} 
</code></pre>
<p>According to the <a href=""https://storage.cloud.google.com/google-cloud-aiplatform/schema/dataset/ioformat/text_classification_multi_label_io_format_1.0.0.yaml"" rel=""nofollow noreferrer"">schema file</a> in the Vertex AI documentation on <a href=""https://cloud.google.com/vertex-ai/docs/datasets/prepare-text#multi-label-classification"" rel=""nofollow noreferrer"">how to prepare the training data</a>, the line above should work. However I could not find a way to check whether the language was imported correctly, and if I export the dataset back the <code>languageCode</code> field has an empty string as value.</p>
<p><strong>What is the correct way to specify language of a document while importing it into a dataset? Is there any way to check that the language was imported correctly?</strong></p>
",Training and Model Evaluation,specify document language importing dataset google cloud automl trying train model text classification vertexai automl google cloud using document spanish imported document json line tried specifying language document follows according schema file vertex ai documentation prepare training data line work however could find way check whether language wa imported correctly export dataset back field ha empty string value correct way specify language document importing dataset way check language wa imported correctly
Why do people set label token&#39;s value that has &lt;pad&gt; token to -100 when Language model is pre-trained?,"<p><a href=""https://i.sstatic.net/nvsUD.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Why do people set label token's value that has  token to -100 when Language model is pre-trained? For example, values with pad token in the following figure, are converted to -100.</p>
<p>I think this is for ignoring influence of pad tokens. Then I wonder how the values change when cross-entropy loss is calculated.</p>
",Training and Model Evaluation,people set label token value ha pad token language model pre trained enter image description people set label token value ha token language model pre trained example value pad token following figure converted think ignoring influence pad token wonder value change cross entropy loss calculated
Why does the loss of Gensim Word2Vec model deteriorate in every epoch?,"<p>I'm training a Word2vec model using Gensim Word2Vec on twitter data. The loss of the model deteriorates in every epoch. The first epoch gives the lowest loss. Why is it so? Code is shared below:</p>
<pre><code>loss_list = []
class callback(CallbackAny2Vec):
     
    def __init__(self):
        self.epoch = 0
          
    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        loss_list.append(loss)
        print('Loss after epoch {}: {}'.format(self.epoch, loss))
        self.epoch = self.epoch + 1

model = Word2Vec(df['tweet_text'], vector_size=300, window=10, epochs=30, hs=0, negative = 1, compute_loss=True, callbacks=[callback()])
embedding_size = model.wv.vectors.shape[1]
print(&quot;embedding size---&gt;&quot;, embedding_size)
vocab = model.wv.index_to_key
print(&quot;minimum loss {} at epoch {}&quot;.format(min(loss_list), loss_list.index(min(loss_list))))
</code></pre>
<p>The output is:</p>
<pre><code>Loss after epoch 0: 527066.375
Loss after epoch 1: 1038087.0625
Loss after epoch 2: 1510719.75
Loss after epoch 3: 1936163.875
Loss after epoch 4: 2364015.5
Loss after epoch 5: 2779299.75
Loss after epoch 6: 3183956.25
Loss after epoch 7: 3570054.5
Loss after epoch 8: 3966524.75
Loss after epoch 9: 4335994.5
Loss after epoch 10: 4706316.0
Loss after epoch 11: 5046213.0
Loss after epoch 12: 5410604.5
Loss after epoch 13: 5754962.0
Loss after epoch 14: 6080469.0
Loss after epoch 15: 6428622.5
Loss after epoch 16: 6771707.0
Loss after epoch 17: 7105302.0
Loss after epoch 18: 7400089.0
Loss after epoch 19: 7732032.0
Loss after epoch 20: 8059942.5
Loss after epoch 21: 8408386.0
Loss after epoch 22: 8685176.0
Loss after epoch 23: 8959723.0
Loss after epoch 24: 9242788.0
Loss after epoch 25: 9506676.0
Loss after epoch 26: 9752588.0
Loss after epoch 27: 10013168.0
Loss after epoch 28: 10288152.0
Loss after epoch 29: 10550915.0
embedding size---&gt; 300
minimum loss 527066.375 at epoch 0
</code></pre>
",Training and Model Evaluation,doe loss gensim word vec model deteriorate every epoch training word vec model using gensim word vec twitter data loss model deteriorates every epoch first epoch give lowest loss code shared output
How can I keep track of the number of epochs completed while training a Word2Vec model?,"<p>I'm training my Word2Vec model for more than 12 hours for a corpus of more than 90k tweets (samples), ~10k unique words in the dictionary for a number of 5 epochs with my 8gb RAM laptop. Is it normal?</p>
<p>I want to track the progress of the training process which is why I want to keep track of the number of epochs completed while training. How can I do that? My code given below:</p>
<pre><code>model = Word2Vec(df['tweet_text'], window=10, vector_size=300, hs=0, negative=1) 
model.train([df['tweet_text']], total_examples=len(df['tweet_text']), epochs=5) 
</code></pre>
",Training and Model Evaluation,keep track number epoch completed training word vec model training word vec model hour corpus k tweet sample k unique word dictionary number epoch gb ram laptop normal want track progress training process want keep track number epoch completed training code given
How to compute sentence level perplexity from hugging face language models?,"<p>I have a large collection of documents each consisting of ~ 10 sentences. For each document, I wish to find the sentence that maximises perplexity, or equivalently the loss from a fine-tuned causal LM. I have decided to use Hugging Face and the <code>distilgpt2</code> model for this purpose. I have 2 problems when trying to do in an efficient (vectorized) fashion:</p>
<ol>
<li><p>The tokenizer required padding to work in batch mode, but when computing the loss on padded <code>input_ids</code> those pad tokens are contributing to the loss. So the loss of a given sentence depends on the length of the longest sentence in the batch which is clearly wrong.</p>
</li>
<li><p>When I pass a batch of input IDs to the model and compute the loss, I get a scalar as it (mean?) pools across the batch. I instead need the loss per item, not the pooled one.</p>
</li>
</ol>
<p>I made a version that operates on a sentence by sentence basis and while correct, it is extremely slow (I want to process ~ 25m sentences total). Any advice?</p>
<p>Minimal example below:</p>
<pre><code># Init
tokenizer = AutoTokenizer.from_pretrained(&quot;distilgpt2&quot;)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(&quot;clm-gpu/checkpoint-138000&quot;)
segmenter = spacy.load('en_core_web_sm')

# That's the part I need to vectorise, surely within a document (bsize ~ 10)
# and ideally across documents (bsize as big as my GPU can handle)
def select_sentence(sentences):
    &quot;&quot;&quot;We pick the sentence that maximizes perplexity&quot;&quot;&quot;
    max_loss, best_index = 0, 0
    for i, sentence in enumerate(sentences):
        encodings = tokenizer(sentence, return_tensors=&quot;pt&quot;)
        input_ids = encodings.input_ids
        loss = lm(input_ids, labels=input_ids).loss.item()
        if loss &gt; max_loss:
            max_loss = loss
            best_index = i

    return sentences[best_index]

for document in documents:
    sentences = [sentence.text.strip() for sentence in segmenter(document).sents]
    best_sentence = select_sentence(sentences)
    write(best_sentence)

</code></pre>
",Training and Model Evaluation,compute sentence level perplexity hugging face language model large collection document consisting sentence document wish find sentence maximises perplexity equivalently loss fine tuned causal lm decided use hugging face model purpose problem trying efficient vectorized fashion tokenizer required padding work batch mode computing loss padded pad token contributing loss loss given sentence depends length longest sentence batch clearly wrong pas batch input id model compute loss get scalar mean pool across batch instead need loss per item pooled one made version operates sentence sentence basis correct extremely slow want process sentence total advice minimal example
Retraining an existing machine learning model with new data,"<p>I have a ML model which is trained on a million data set (supervised classification on text) , however I want the <strong>same model</strong> to get trained again as soon as a new data comes in (training data).</p>
<p>This process is continuous and I <strong>don't</strong> want to <strong>loose</strong> the power of the <strong>model's prediction</strong> every time it receives a new data set. I don't want to merge the new data with my history data (~1 million samples) to train again.</p>
<p>So the ideal would be for this model to grow up gradually training on all data over a period of time and preserving the intelligence of the model every time it receives a new training set data. What is the best way to <strong>avoid retraining all historical data</strong>? A Code sample would help me.</p>
",Training and Model Evaluation,retraining existing machine learning model new data ml model trained million data set supervised classification text however want model get trained soon new data come training data process continuous want loose power model prediction every time receives new data set want merge new data history data million sample train ideal would model grow gradually training data period time preserving intelligence model every time receives new training set data best way avoid retraining historical data code sample would help
How to effectively tune the hyper-parameters of Gensim Doc2Vec to achieve maximum accuracy in Document Similarity problem?,"<p>I have around 20k documents with 60 - 150 words. Out of these 20K documents, there are 400 documents for which the similar document are known. These 400 documents serve as my test data.</p>

<p>At present I am removing those 400 documents and using remaining 19600 documents for training the doc2vec. Then I extract the vectors of train and test data. Now for each test data document, I find it's cosine distance with all the 19600 train documents and select the top 5 with least cosine distance. If the similar document marked is present in these top 5 then take it to be accurate. Accuracy% = No. of Accurate records / Total number of Records.</p>

<p>The other way I find similar documents is by using the doc2Vec most similiar method. Then calculate accuracy using the above formula.</p>

<p>The above two accuracy doesn't match. With each epoch one increases other decreases.</p>

<p>I am using the code given here: <a href=""https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e"" rel=""nofollow noreferrer"">https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e</a>. For training the Doc2Vec.</p>

<p>I would like to know how to tune the hyperparameters so that I can get making accuracy by using above-mentioned formula. Should I use cosine distance to find the most similar documents or shall I use the gensim's most similar function?</p>
",Training and Model Evaluation,effectively tune hyper parameter gensim doc vec achieve maximum accuracy document similarity problem around k document word k document document similar document known document serve test data present removing document using remaining document training doc vec extract vector train test data test data document find cosine distance train document select top least cosine distance similar document marked present top take accurate accuracy accurate record total number record way find similar document using doc vec similiar method calculate accuracy using formula two accuracy match epoch one increase decrease using code given training doc vec would like know tune hyperparameters get making accuracy using mentioned formula use cosine distance find similar document shall use gensim similar function
A language model with only one embedding layer in both encode and decode only predict &lt;eos&gt;,"<p>I'm trying to make the model predict a word from a sentence using pretrained Huggingface's BERT as feature extractor. The model look like this</p>
<pre><code>class BertAutoEncoder(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        decoder_layer = nn.TransformerDecoderLayer(768, 2, 1024, dropout=0.1)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, 2)
        self.fc = nn.Linear(768, vocab_size)

    def forward(self, memory, embedded_word):
        output = self.transformer_decoder(embedded_word, memory)
        output = self.fc(output)
        return output
</code></pre>
<p>And when train/evaluate I call the model like this</p>
<pre><code>bert = BertModel.from_pretrained('bert-base-uncased')
bert.requires_grad_(False)
...
memory = bert(**src).last_hidden_state.transpose(0, 1)
embeded_word = bert.embeddings(trg.data['input_ids'][:, :-1], token_type_ids=trg.data['token_type_ids'][:, :-1]).transpose(0, 1)
output = model(memory, embeded_word)
</code></pre>
<p>The loss reduced nicely but turned out the model only predict <code>&lt;eos&gt;</code> token.</p>
<p>I tried train the model with 1 batch of 32 samples and it did work when loss reduced pass <code>8e-6</code> but when I trained it with all data the loss could go way beyond that but none of the saved models work. Even the one with eval or train loss around <code>4e-6</code> - <code>8e-6</code>.</p>
<p>Surprisingly the model would work if I use a separate decoder's Embedding like this</p>
<pre><code>class BertAutoEncoderOld(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        decoder_layer = nn.TransformerDecoderLayer(768, 2, 1024, dropout=0.1)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, 2)
        self.decoder = nn.Embedding(vocab_size, 768)
        self.pos_decoder = PositionalEncoding(768, 0.5)
        self.fc = nn.Linear(768, vocab_size)

    def forward(self, memory, word):
        tgt = self.decoder(word.data['input_ids'][:, :-1].transpose(0, 1))
        tgt = self.pos_decoder(tgt)
        output = self.transformer_decoder(tgt, memory)
        output = self.fc(output)
        return output
</code></pre>
<p>But I was asked to make it work with one Embedding and I have no idea how.</p>
<p>I tried</p>
<ul>
<li>Reduce/increase batch from 32 to 8-64</li>
<li>Also tried 2 and 1024 batch size</li>
<li>Remove <code>&lt;eos&gt;</code> token and change it's attention mask to 0</li>
</ul>
<p>But none of those work.</p>
<p>What did I do wrong and how to fix it?</p>
<p>Thanks</p>
<h1>Edit per @emily qeustion</h1>
<p>I change the data itself in collate function</p>
<pre><code>text.data['attention_mask'][text.data['input_ids'] == 102] = 0
text.data['input_ids'][text.data['input_ids'] == 102] = 0
word.data['attention_mask'][word.data['input_ids'] == 102] = 0
word.data['input_ids'][word.data['input_ids'] == 102] = 0
</code></pre>
<p>It only used in Bert though.</p>
",Training and Model Evaluation,language model one embedding layer encode decode predict eos trying make model predict word sentence using pretrained huggingface bert feature extractor model look like train evaluate call model like loss reduced nicely turned model predict token tried train model batch sample work loss reduced pas trained data loss could go way beyond none saved model work even one eval train loss around surprisingly model would work use separate decoder embedding like wa asked make work one embedding idea tried reduce increase batch also tried batch size remove token change attention mask none work wrong fix thanks edit per emily qeustion change data collate function used bert though
Is there anything incorrect about my implementation of my Naive Bayes Classifier?,"<p>I'm building a text classifier using Naive Bayes for a school project. My accuracy on the testing set that my professor provided us is 89.4% which is reasonably high but my professor said that if I correctly implemented Naive Bayes that it should be around a point higher. Is there anything incorrect with my implementation of Naive Bayes or if there's maybe any normalization technique that I should try applying to the documents? Below are the two main functions for my classifier. There are other functions and without them the program won't run but they're mostly related to tuning the model. Please let me know if you need me to clarify anything else or if you want the full version of the code.</p>
<p>Also the training and testing set consists of two columns with the first column being the file path to the document and the second one the class that that document belongs to.</p>
<pre><code>def getStatistics(trainSet):
  #here we're going to calculate the probability of a document being in a certain
  #category by first looking at the fraction of documents in the training set that
  #are in that category

  probCateg = trainSet[&quot;Category&quot;].value_counts()

  probCateg = probCateg/probCateg.sum()

  #we create a dictionary that maps a category to the probability of a document being
  #in that category

  probCateg = probCateg.to_dict()


  #to calculate p(t|c) we need to find the probability of a term occuring in 
  #a category

  #So to quickly get the probability of a word appearing in a document of a certain category
  #I used a dictionary which maps the word to another dictionary which in turn maps the category
  #to &quot;C(t|c)&quot;. This might not be as efficient as having the dictionary map to a list of 
  #prob values (lookup is still O(1) but hashing the key is probably more computationally expensive than retrieving 
  #element index) but in my opinion it makes the data structure work very well with pandas 
  #so that training my classifier only takes a few lines of code.
  wordFreq = {}


  lemmatizer = WordNetLemmatizer()

  #for each data point in our training set
  for index, row in trainSet.iterrows():

      #we read in the document
      with open(row[&quot;Filename&quot;], 'r') as f:
          text = f.read()

          f.close()

      #and split the document into tokens
      #then we're going to reduce all of the words in the document
      #to their base form and then make all of the letters lower case. 
      #This helps make our assumption that the probabilities of each individual
      #word appearing in the document are independent of one another more valid
      #as if you see the word robbery in a document then you're also likely to see
      #words like robbed, robbing, and rob in the document. Additionally, this will
      #make training and classification of our model much faster as there are significantly 
      #less words that it has to calculate probabilities for

      #I also removed stop words and punctuation from the documents since they only seemed
      #to hurt the model by adding noise and it will also make training and classification
      #faster as well

      tokens = nltk.word_tokenize(text)

      #I found that for the third corpus, counting the number of times the word appears in a document
      #instead of just if it occurs in the document signifcantly increases the accuracy by around 4%
      
      tokens = list(set(tokens))
      

      tokens = [token.lower() for token in tokens]
      tokens = [token for token in tokens if (token not in punctuation) and (token not in stopwords)]
      tokens = [(lemmatizer.lemmatize(word)) for word in tokens]

      #we then convert the tokens to a set and then back to a list so that we only 
      #have a list of unique words in the document
      

      tokens = list(set(tokens))
        
      #for each word
      for word in tokens:
        #if the word isn't yet in the dictionary 
        if not (word in wordFreq):
        
          #initialize an entry in the dictionary that maps that word
          #to a dictionary that maps the names of the categories to 0s
          #since so far none of the categories have had that word
          wordFreq[word] = dict(zip(probCateg.keys(), [0]*len(probCateg.keys())))
        
        
        #then if the word is in that document. We increment the # of occurences of that term
        #in this category by 1
        wordFreq[word][row[&quot;Category&quot;]] += 1

      

  return wordFreq, probCateg
#in this function, we classify the document
#the function takes in the name of the file in which the document is in
#and a tunable paramter that's used to account for words that don't
#appear in the training set but are present in the testing set
#The function returns the category that the document is most likely
#to be
def classifyDoc(wordFreq, probCateg, filename, eps, trainSize):

  #open and read in the file
  with open(filename, 'r') as f:
        text = f.read()

        f.close()
  
  #and we do the same text processing that we did for the documents in the training set
  lemmatizer = WordNetLemmatizer()

  tokens = nltk.word_tokenize(text)


      
  tokens = list(set(tokens))

  tokens = [token.lower() for token in tokens]
  tokens = [token for token in tokens if (token not in punctuation) and (token not in stopwords)]
  tokens = [(lemmatizer.lemmatize(word)) for word in tokens]


  #we then convert the tokens to a set and then back to a list so that we only 
  #have a list of unique words in the document

  tokens = list(set(tokens))
        
  #since we're going to multiply several very small numbers together, we run 
  #the risk of the value rounding to 0. To avoid this, we take the log probability
  #instead

  #we're going to calculate the log probability for each category
  #so we can hold these values in a dictionary that maps the categories to 
  #their probabilities for that word
  logProb = dict(zip(probCateg.keys(), [math.log(probCateg[key]) for key in probCateg.keys()]))
  
  secCorp = &quot;O&quot; in probCateg.keys()

  #for each category
  for categ in probCateg.keys():
    #to get p(t|c) we divide by total number of documents that are of that category

    denom = trainSize*probCateg[categ]

    #for the second corpus i found that the full form for laplace smoothing gives better results
    if secCorp:
      denom += len(wordFreq)*eps

    #for each word in the document
    for word in tokens:
      
      #if we encountered the word before
      if word in wordFreq:
        #we get C(t|c) from the dictionary and add the log of it to the corresponding
        #log probability plus the smoothing parameter.
        logProb[categ] += math.log(wordFreq[word][categ] + eps)
        
              
      #However, if the word isn't in that category, that would mean that p(t|c) = 0
      #which would mean that p(c|d) = 0 which is unreasonable. Instead,
      #we add a small constant, eps, to all of the term frequencies for all of the categories
      else:

        logProb[categ] += math.log(eps)

      #and then here we divide by denom to get p(t|c) which for log prob is the same as subtracting
      #log(denom)
      logProb[categ] -= math.log(denom)
        


  #after we have calculated the log probabilities we return the category with 
  #the highest probability
  return max(logProb, key=logProb.get) 

</code></pre>
",Training and Model Evaluation,anything incorrect implementation naive bayes classifier building text classifier using naive bayes school project accuracy testing set professor provided u reasonably high professor said correctly implemented naive bayes around point higher anything incorrect implementation naive bayes maybe normalization technique try applying document two main function classifier function without program run mostly related tuning model please let know need clarify anything else want full version code also training testing set consists two column first column file path document second one class document belongs
FastText difference between saving directly and saving after build_vocab and training with epochs,"<p>I've been building FastText model and saving as following:</p>
<pre><code>model = FastText(#some parameters)
model.build_vocab(corpus_strings)
model.train(corpus_iterable=corpus_strings, total_examples=len(corpus_strings), epochs=model.epochs)
model.save(filename)
</code></pre>
<p>After a while, I tried to save without training because I've seen that someone using FastText in that way:</p>
<pre><code>model = FastText(corpus_strings, #some parameters)
model.save(filename)
</code></pre>
<p>I've seen that the generated vectors by those 2 models are not same for same strings. I'm thinking that it could be because of epochs but is there any other impact that comes from build_vocab and train?</p>
<p>On the other hand, I want also to know that what is the difference between those 2 ways, which one should better in which case?</p>
<p>The vector generated by 2 different models (first has been built by build_vocab, train and save and the second saved directly after calling model = FastText(#some parameters)) are not. I was expecting same vectors. And I want to know why it is the case.</p>
",Training and Model Evaluation,fasttext difference saving directly saving build vocab training epoch building fasttext model saving following tried save without training seen someone using fasttext way seen generated vector model string thinking could epoch impact come build vocab train hand want also know difference way one better case vector generated different model first ha built build vocab train save second saved directly calling model fasttext parameter wa expecting vector want know case
Why do we need to write a function to &quot;Compute Metrics&quot; with Huggingface Question Answering Trainer when evaluating SQuAD?,"<p>Currently, I'm trying to build a Extractive QA pipeline, following the <a href=""https://huggingface.co/course/chapter7/7?fw=pt#question-answering"" rel=""nofollow noreferrer"">Huggingface Course on the matter</a>. There, they show how to create a compute_metrics() function to evaluate the model after training. However, I was wondering if there's a way to obtain those metrics on training, and pass the compute_metrics() function directly to the trainer. They are training using only the training loss, and I would like to have the evaluation f1 score on training.</p>
<p>But, as I see it, it might be a little bit tricky, because they need the original spans to calculate the squad metrics, but you don't get those original spans passed on your tokenized training dataset.</p>
<pre><code>predicted_answer = {'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}
theoretical_answer = {'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}

metric.compute(predictions=predicted_answers, references=theoretical_answers)
</code></pre>
<p>That's why they make the whole compute_metrics() function, taking a few extra parameters than the prediction outputted in the evaluation loop, as they need to rebuild those spans.</p>
<h3>Q: How do I make the squad metric outputs F1 and accuracy scores from evaluate? How do I use the squad metric with the <code>Trainer</code> object?</h3>
",Training and Model Evaluation,need write function compute metric huggingface question answering trainer evaluating squad currently trying build extractive qa pipeline following huggingface course matter show create compute metric function evaluate model training however wa wondering way obtain metric training pas compute metric function directly trainer training using training loss would like evaluation f score training see might little bit tricky need original span calculate squad metric get original span passed tokenized training dataset make whole compute metric function taking extra parameter prediction outputted evaluation loop need rebuild span q make squad metric output f accuracy score evaluate use squad metric object
Predicting word vectors instead of words (Natural Language Processing),"<p>I wonder whether there are any attempts to predict <em>word embedding vectors</em> as targets in neural networks architectures (like Transformers, Sequence-to-Sequence-Models or simple RNNs) using for example <em>mean squared errors</em> as a loss-function instead of predicting words as categories with <em>softmax</em>. Couldn’t find any articles on that topic. Does someone know any papers on that or if not a reason why that‘s not being done?</p>
<p><em><strong>For clarification</strong></em>:</p>
<p>Take for example a simple Encoder-Decoder-Architecture as proposed by Cho et al. 2014 (<a href=""https://arxiv.org/abs/1406.1078"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1406.1078</a>). In such models (where sequences of words a produced), a softmax activation function is commonly used in the output layer in order to predict target words. If you have 30,000 words in your vocabulary, you need 30,000 neurons in the output layer.</p>
<p><em>My question is now</em>: Instead of predicting words in the output layer, why not represent target words as vectors? Practically this should be easy to accomplish when using pretrained vector representations like <em>fastText</em> or <em>vec2words</em>. Such vectors spaces describe a word using for example 300 float values. One could first look up every target word in the vector space and then use the floats describing the word instead of the word itself (as a category) as a target. The model will then predict the float values of the vector representation. When using such a model to predict a word (or a sequence of words), we would simply use the predicted values to look up the closest word in the vector space.</p>
<p>The <em>advantage</em> of this procedure should be twofold. <em>First</em>, it would significantly reduce the number of neurons in the output layer (from 30,000 to 300). <em>Second</em>, training such a model would – from a theoretical point of view – lead to a better underlying „language understanding“ of the model since the vector representations are semantically and grammatically rich while the words are just monolytic categories.</p>
",Training and Model Evaluation,predicting word vector instead word natural language processing wonder whether attempt predict word embedding vector target neural network architecture like transformer sequence sequence model simple rnns using example mean squared error loss function instead predicting word category softmax find article topic doe someone know paper reason done clarification take example simple encoder decoder architecture proposed cho et al model sequence word produced softmax activation function commonly used output layer order predict target word word vocabulary need neuron output layer question instead predicting word output layer represent target word vector practically easy accomplish using pretrained vector representation like fasttext vec word vector space describe word using example float value one could first look every target word vector space use float describing word instead word category target model predict float value vector representation using model predict word sequence word would simply use predicted value look closest word vector space advantage procedure first would significantly reduce number neuron output layer second training model would theoretical point view lead better underlying language understanding model since vector representation semantically grammatically rich word monolytic category
Predicting word vectors instead of words (Natural Language Processing),"<p>I wonder whether there are any attempts to predict <em>word embedding vectors</em> as targets in neural networks architectures (like Transformers, Sequence-to-Sequence-Models or simple RNNs) using for example <em>mean squared errors</em> as a loss-function instead of predicting words as categories with <em>softmax</em>. Couldn’t find any articles on that topic. Does someone know any papers on that or if not a reason why that‘s not being done?</p>
<p><em><strong>For clarification</strong></em>:</p>
<p>Take for example a simple Encoder-Decoder-Architecture as proposed by Cho et al. 2014 (<a href=""https://arxiv.org/abs/1406.1078"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1406.1078</a>). In such models (where sequences of words a produced), a softmax activation function is commonly used in the output layer in order to predict target words. If you have 30,000 words in your vocabulary, you need 30,000 neurons in the output layer.</p>
<p><em>My question is now</em>: Instead of predicting words in the output layer, why not represent target words as vectors? Practically this should be easy to accomplish when using pretrained vector representations like <em>fastText</em> or <em>vec2words</em>. Such vectors spaces describe a word using for example 300 float values. One could first look up every target word in the vector space and then use the floats describing the word instead of the word itself (as a category) as a target. The model will then predict the float values of the vector representation. When using such a model to predict a word (or a sequence of words), we would simply use the predicted values to look up the closest word in the vector space.</p>
<p>The <em>advantage</em> of this procedure should be twofold. <em>First</em>, it would significantly reduce the number of neurons in the output layer (from 30,000 to 300). <em>Second</em>, training such a model would – from a theoretical point of view – lead to a better underlying „language understanding“ of the model since the vector representations are semantically and grammatically rich while the words are just monolytic categories.</p>
",Training and Model Evaluation,predicting word vector instead word natural language processing wonder whether attempt predict word embedding vector target neural network architecture like transformer sequence sequence model simple rnns using example mean squared error loss function instead predicting word category softmax find article topic doe someone know paper reason done clarification take example simple encoder decoder architecture proposed cho et al model sequence word produced softmax activation function commonly used output layer order predict target word word vocabulary need neuron output layer question instead predicting word output layer represent target word vector practically easy accomplish using pretrained vector representation like fasttext vec word vector space describe word using example float value one could first look every target word vector space use float describing word instead word category target model predict float value vector representation using model predict word sequence word would simply use predicted value look closest word vector space advantage procedure first would significantly reduce number neuron output layer second training model would theoretical point view lead better underlying language understanding model since vector representation semantically grammatically rich word monolytic category
Training data for question-answering with model from huggingface or any other open-source,"<p>I am relatively new in AI using this model.
<a href=""https://huggingface.co/MaRiOrOsSi/t5-base-finetuned-question-answering"" rel=""nofollow noreferrer"">https://huggingface.co/MaRiOrOsSi/t5-base-finetuned-question-answering</a></p>
<p>I have code like this.</p>
<pre><code>model_name = &quot;MaRiOrOsSi/t5-base-finetuned-question-answering&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelWithLMHead.from_pretrained(model_name)
question = &quot;If I need to give one banana to each person, how many people can I give?&quot;
context = &quot;There are two banana and one orange.&quot;
input = f&quot;question: {question} context: {context}&quot;
encoded_input = tokenizer([input],
                         return_tensors='pt',
                         max_length=512,
                         truncation=True)
output = model.generate(input_ids = encoded_input.input_ids,
                        attention_mask = encoded_input.attention_mask)
output = tokenizer.decode(output[0], skip_special_tokens=True)
print(output)
</code></pre>
<p>It could give answer as &quot;two&quot; and it is still okay. But if I change the question and context like this, it is not okay.</p>
<pre><code>question = &quot;Did the user answer all requested information?&quot;
context = &quot;Agent: Could you please provide us user id and postal code? User: Here is my postal code, 11111111&quot;
</code></pre>
<p>It is not okay. I know that there are limitation on this model. My question is</p>
<ol>
<li>Is there a better model to handle this kind of question-answer? (Except for ChatGPT. I have tested on ChatGPT and it can handle correctly.)</li>
<li>Is there a way I should ask? Or do I need to train more for this model?</li>
</ol>
<p>Thank you.</p>
",Training and Model Evaluation,training data question answering model huggingface open source relatively new ai using model code like could give answer two still okay change question context like okay okay know limitation model question better model handle kind question answer except chatgpt tested chatgpt handle correctly way ask need train model thank
Split text into logical blocks,"<p>I have an array of (insurance) contracts (in .docx format) processing of which I'm trying to automate.</p>

<p>Current task at hand is to split every contract into so called clauses - parts of contract which describe some specific risk or exclusion from cover.</p>

<p>For example, it can be just one sentence – “This contract covers loss or damage due to fire” or several paragraphs of text that give more details and explain what type of fire this contract covers and what damage is reimbursed.</p>

<p>Good thing is that usually contracts are formatted in some way or another. In best possible scenario, whole contract is a numbered list with items and sub items and we simply can split it by certain level of list hierarchy.</p>

<p>Bad thing is that this is not always the case and the list can be not numbered, but alphabetical or not list at all in word terms: each line starts with a number or a letter user typed in manually. Or it can be not letters or numbers at all, but some amount of spaces or tabs. Or clauses can be separated by their titles that are typed in ALL CAPS.</p>

<p>So the visual representation of structure varies from contract to contract.</p>

<p>So my question is what is the best approach to this task? Regexp? Some ML algo? Maybe there are open source scripts out there that were written to deal with this or similar tasks? Any help will be most welcome!</p>

<p><strong>EDIT (24.12.2019):</strong></p>

<p>Found this repo on github: <a href=""https://github.com/bmmidei/SliceCast"" rel=""nofollow noreferrer"">https://github.com/bmmidei/SliceCast</a></p>

<p>Form its description: ""This repository explores a neural network approach to segment podcasts based on topic of discussion. We model the problem as a binary classification task where each sentence is either labeled as the first sentence of a new segment or a continuation of the current segment. We embed sentences using the Universal Sentence Encoder and use an LSTM-based classification network to obtain the cutoff probabilities. Our results indicate that neural network models are indeed suitable for topical segmentation on long, conversational texts, but larger datasets are needed for a truly viable product.</p>

<p>Read the full report for this work here: <a href=""https://github.com/bmmidei/SliceCast/blob/master/Neural_Text_Segmentation_on_Podcast_Transcripts.pdf"" rel=""nofollow noreferrer"">Neural Text Segmentation on Podcast Transcripts</a>""</p>
",Training and Model Evaluation,split text logical block array insurance contract docx format processing trying automate current task hand split every contract called clause part contract describe specific risk exclusion cover example one sentence contract cover loss damage due fire several paragraph text give detail explain type fire contract cover damage reimbursed good thing usually contract formatted way another best possible scenario whole contract numbered list item sub item simply split certain level list hierarchy bad thing always case list numbered alphabetical list word term line start number letter user typed manually letter number amount space tab clause separated title typed cap visual representation structure varies contract contract question best approach task regexp ml algo maybe open source script written deal similar task help welcome edit found repo github form description repository explores neural network approach segment podcasts based topic discussion model problem binary classification task sentence either labeled first sentence new segment continuation current segment embed sentence using universal sentence encoder use lstm based classification network obtain cutoff probability result indicate neural network model indeed suitable topical segmentation long conversational text larger datasets needed truly viable product read full report work neural text segmentation podcast transcript
Debugging python running unreasonably slow when adding numbers,"<p>I'm working on an NLP and I got bitten by an unreasonably slow behaviour in Python while operating with small amounts of numbers.</p>
<p>I have the following code:</p>
<pre class=""lang-py prettyprint-override""><code>import random, time
from functools import reduce

def trainPerceptron(perceptron, data):
  learningRate = 0.002
  weights = perceptron['weights']
  error = 0
  for chunk in data:
      input = chunk['input']
      output = chunk['output']

      # 12x slower than equivalent JS
      sum_ = 0
      for key in input:
          v = weights[key]
          sum_ += v

      # 20x slower than equivalent JS
      #sum_ = reduce(lambda acc, key: acc + weights[key], input)

      actualOutput = sum_ if sum_ &gt; 0 else 0

      expectedOutput = 1 if output == perceptron['id'] else 0
      currentError = expectedOutput - actualOutput
      if currentError:
          error += currentError ** 2
          change = currentError * learningRate
          for key in input:
              weights[key] += change

  return error

# Build mock data structure
data = [{
   'input': random.sample(range(0, 5146), 10),
   'output': 0
} for _ in range(11514)]
perceptrons = [{
   'id': i,
   'weights': [0.0] * 5147,
} for i in range(60)] # simulate 60 perceptrons

# Simulate NLU
for i in range(151): # 150 iterations
  hrstart = time.perf_counter()
  for perceptron in perceptrons:
    trainPerceptron(perceptron, data)
  hrend = time.perf_counter()
  print(f'Epoch {i} - Time for training: {int((hrend - hrstart) * 1000)}ms')
</code></pre>
<p>Running it on my M1 MBP I get the following numbers.</p>
<pre><code>Epoch 0 - Time for training: 199ms
Epoch 1 - Time for training: 198ms
Epoch 2 - Time for training: 199ms
Epoch 3 - Time for training: 197ms
Epoch 4 - Time for training: 199ms
...
Epoch 146 - Time for training: 198ms
Epoch 147 - Time for training: 200ms
Epoch 148 - Time for training: 198ms
Epoch 149 - Time for training: 198ms
Epoch 150 - Time for training: 198ms
</code></pre>
<p>Each epoch is taking around 200ms, which is unreasonably slow given the small amount of numbers that are being processed. I profiled the code with <code>cProfile</code> in order to find out what is going on:</p>
<pre><code>         655306 function calls (655274 primitive calls) in 59.972 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      3/1    0.000    0.000   59.972   59.972 {built-in method builtins.exec}
        1    0.005    0.005   59.972   59.972 poc-small.py:1(&lt;module&gt;)
     9060   59.850    0.007   59.850    0.007 poc-small.py:4(trainPerceptron)
        1    0.006    0.006    0.112    0.112 poc-small.py:34(&lt;listcomp&gt;)
    11514    0.039    0.000    0.106    0.000 random.py:382(sample)
   115232    0.034    0.000    0.047    0.000 random.py:235(_randbelow_with_getrandbits)
    11548    0.002    0.000    0.012    0.000 {built-in method builtins.isinstance}
    11514    0.002    0.000    0.010    0.000 &lt;frozen abc&gt;:117(__instancecheck__)
   183616    0.010    0.000    0.010    0.000 {method 'getrandbits' of '_random.Random' objects}
    11514    0.002    0.000    0.008    0.000 {built-in method _abc._abc_instancecheck}
    11514    0.002    0.000    0.006    0.000 &lt;frozen abc&gt;:121(__subclasscheck__)
   115140    0.005    0.000    0.005    0.000 {method 'add' of 'set' objects}
    11514    0.003    0.000    0.004    0.000 {built-in method _abc._abc_subclasscheck}
   115232    0.004    0.000    0.004    0.000 {method 'bit_length' of 'int' objects}
      151    0.003    0.000    0.003    0.000 {built-in method builtins.print}
</code></pre>
<p>This wasn't too helpful, so I tried with <a href=""https://github.com/pyutils/line_profiler"" rel=""nofollow noreferrer"">line_profiler</a>:</p>
<pre><code>Timer unit: 1e-06 s

Total time: 55.2079 s
File: poc-small.py
Function: trainPerceptron at line 4

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     4                                           @profile
     5                                           def trainPerceptron(perceptron, data):
     6      1214        301.0      0.2      0.0    learningRate = 0.002
     7      1214        255.0      0.2      0.0    weights = perceptron['weights']
     8      1214        114.0      0.1      0.0    error = 0
     9  13973840    1742427.0      0.1      3.2    for chunk in data:
    10  13973840    1655043.0      0.1      3.0        input = chunk['input']
    11  13973840    1487543.0      0.1      2.7        output = chunk['output']
    12                                           
    13                                                 # 12x slower than equivalent JS
    14  13973840    1210755.0      0.1      2.2        sum_ = 0
    15 139738397   13821056.0      0.1     25.0        for key in input:
    16 139738397   13794656.0      0.1     25.0            v = weights[key]
    17 139738396   14942692.0      0.1     27.1            sum_ += v
    18                                           
    19                                                 # 20x slower than equivalent JS
    20                                                 #sum_ = reduce(lambda acc, key: acc + weights[key], input)
    21                                           
    22  13973839    1618273.0      0.1      2.9        actualOutput = sum_ if sum_ &gt; 0 else 0
    23                                           
    24  13973839    1689194.0      0.1      3.1        expectedOutput = 1 if output == perceptron['id'] else 0
    25  13973839    1365346.0      0.1      2.5        currentError = expectedOutput - actualOutput
    26  13732045    1211916.0      0.1      2.2        if currentError:
    27    241794      38375.0      0.2      0.1            error += currentError ** 2
    28    241794      25377.0      0.1      0.0            change = currentError * learningRate
    29   2417940     271237.0      0.1      0.5            for key in input:
    30   2417940     332890.0      0.1      0.6                weights[key] += change
    31                                           
    32      1213        405.0      0.3      0.0    return error
</code></pre>
<p>This shows that these 3 lines (that are adding the numbers) are taking the entire runtime budget:</p>
<pre class=""lang-py prettyprint-override""><code>for key in input:
   v = weights[key]
   sum_ += v
</code></pre>
<p>I thought throwing numpy at the problem in order to speed it up, but <code>input</code> has a very small size, which means that the overhead of calling numpy will hurt the performance more than the gains obtained by making the math operations with it.</p>
<p>Anyhow, adding numbers shouldn't be that slow, which makes me believe something weird is going on with Python. In order to confirm my theory, I ported the code to Javascript. This is the result:</p>
<pre class=""lang-js prettyprint-override""><code>function trainPerceptron(perceptron, data) {
  const learningRate = 0.002;
  const weights = perceptron['weights'];
  let error = 0;
  for (const chunk of data) {
    const input = chunk['input'];
    const output = chunk['output'];

    const sum = input.reduce((acc, key) =&gt; acc + weights[key], 0);
    const actualOutput = sum &gt; 0 ? sum : 0;

    const expectedOutput = output === perceptron['id'] ? 1 : 0;
    const currentError = expectedOutput - actualOutput;
    if (currentError) {
      error += currentError ** 2;
      const change = currentError * learningRate;
      for (const key in input) {
        weights[key] += change;
      }
    }
  }
  return error;
}

// Build mock data structure
const data = new Array(11514);
for (let i = 0; i &lt; data.length; i++) {
  const inputSet = new Set();
  while (inputSet.size &lt; 10) {
    inputSet.add(Math.floor(Math.random() * 5146));
  }
  const input = Array.from(inputSet);
  data[i] = { input: input, output: 0 };
}

const perceptrons = Array.from({ length: 60 }, (_, i) =&gt; ({
  id: i,
  weights: Array.from({ length: 5147 }, () =&gt; 0.0),
})); // simulate 60 perceptrons

// Simulate NLU
for (let i = 0; i &lt; 151; i++) { // 150 iterations
  const hrstart = performance.now();
  for (const perceptron of perceptrons) {
    trainPerceptron(perceptron, data);
  }
  const hrend = performance.now();
  console.log(`Epoch ${i} - Time for training: ${Math.floor(hrend - hrstart)}ms`);
}
</code></pre>
<p>When I run the JS code I get the following numbers:</p>
<pre><code>Epoch 0 - Time for training: 30ms
Epoch 1 - Time for training: 18ms
Epoch 2 - Time for training: 17ms
Epoch 3 - Time for training: 17ms
Epoch 4 - Time for training: 17ms
...
Epoch 147 - Time for training: 17ms
Epoch 148 - Time for training: 17ms
Epoch 149 - Time for training: 17ms
Epoch 150 - Time for training: 17ms
</code></pre>
<p>These numbers confirm my theory. Python is being unreasonably slow. Any idea why or what exactly is making it perform so poorly?</p>
<p>Runtime details:</p>
<p>MacOS Ventura 13.2.1 (22D68)
Macbook Pro M1 Pro 32GB
Python 3.11.0 (native Apple Silicon)</p>
",Training and Model Evaluation,debugging python running unreasonably slow adding number working nlp got bitten unreasonably slow behaviour python operating small amount number following code running mbp get following number epoch taking around unreasonably slow given small amount number processed profiled code order find going helpful tried line profiler show line adding number taking entire runtime budget thought throwing numpy problem order speed ha small size mean overhead calling numpy hurt performance gain obtained making math operation anyhow adding number slow make believe something weird going python order confirm theory ported code javascript result run j code get following number number confirm theory python unreasonably slow idea exactly making perform poorly runtime detail macos ventura macbook pro pro gb python native apple silicon
MarkupLM model applied to html longer than 512,"<p>I’m using markupLM transformer for token classsification with finance articles longer than 512 max length i’m asking if there is a solution to fine tune MarkupLM with inputs longer than 512 .</p>
<p>I was trying to chunk tensors but this method increase information loss i'm looking for another solution</p>
",Training and Model Evaluation,markuplm model applied html longer using markuplm transformer token classsification finance article longer max length asking solution fine tune markuplm input longer wa trying chunk tensor method increase information loss looking another solution
After fine tuning model with model.train it gives different predictions for same text,"<p>This is how I fine tuned the model:</p>
<pre class=""lang-py prettyprint-override""><code>    input_ids=tokenizer(str(parseddata), padding=True, truncation=True, max_length=500, 
    return_tensors=&quot;pt&quot;)
    labels = torch.tensor([0])

    lr_scheduler = get_scheduler(
        name=&quot;linear&quot;, optimizer=optimizer, num_warmup_steps=0, num_training_steps=2
    )

    # del banmodel

    model.train(mode=True)
    for i in range(2):
        outputs = model(input_ids, labels=labels)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
</code></pre>
<p>This is how I predicted it:</p>
<pre class=""lang-py prettyprint-override""><code>def predict_label(text):
    # input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)
    input_ids=tokenizer(text, padding=True, truncation=True, max_length=500, return_tensors=&quot;pt&quot;)
    logits = model(**input_ids)[0]
    probs = torch.nn.functional.softmax(logits, dim=1)
    
    return probs
</code></pre>
<p>Only after training it model gives different answers for the same text input. However, when I close the entire process and turn it on again it works and gives me the same prediction. Any help would be extremely appreciated thanks.</p>
",Training and Model Evaluation,fine tuning model model train give different prediction text fine tuned model predicted training model give different answer text input however close entire process turn work give prediction help would extremely appreciated thanks
Save paragraphs containing keywords into txt file,"<p>Recently I have an ongoing research project that requires me to only keep paragraphs containing keywords of each txt file. Does there have any way to do that?</p>
<p>keywords=[&quot;cryptocurren&quot;,&quot;virtual curren&quot;,&quot;digital curren&quot;]</p>
<p>txt sample</p>
<blockquote>
<p>The widespread adoption of new technologies, including internet services, <strong>cryptocurrencies</strong> and payment systems, could require substantial expenditures to modify or adapt our existing products and services as we grow and develop our internet banking and mobile banking channel strategies in addition to remote connectivity solutions.</p>
</blockquote>
<blockquote>
<p>A significant natural disaster, such as a tornado, hurricane, earthquake, fire or flood, could have a material adverse impact on our ability to conduct business, and our insurance coverage may be insufficient to compensate for losses that may occur.  Acts of terrorism, war, civil unrest, or pandemics could cause disruptions to our business or the economy as a whole.  While we have established and regularly test disaster recovery procedures, the occurrence of any such event could have a material adverse effect on our business, operations and financial condition.</p>
</blockquote>
<p>As the text showed above, only the first paragraph contains the keyword of the keyword list. Thus, I only want the txt file contain the 1st paragraph.</p>
<p>Thank you in advance!</p>
<p>I hope to find a way to only keep paragraphs that contain the keywords of the txt file.</p>
",Training and Model Evaluation,save paragraph containing keywords txt file recently ongoing research project requires keep paragraph containing keywords txt file doe way keywords cryptocurren virtual curren digital curren txt sample widespread adoption new technology including internet service cryptocurrencies payment system could require substantial expenditure modify adapt existing product service grow develop internet banking mobile banking channel strategy addition remote connectivity solution significant natural disaster tornado hurricane earthquake fire flood could material adverse impact ability conduct business insurance coverage may insufficient compensate loss may occur act terrorism war civil unrest pandemic could cause disruption business economy whole established regularly test disaster recovery procedure occurrence event could material adverse effect business operation financial condition text showed first paragraph contains keyword keyword list thus want txt file contain st paragraph thank advance hope find way keep paragraph contain keywords txt file
Python NLP error can anyone explain more to me of why this error is occuring due to inputs i assume but where am i wrong,"<p>This is my code I am able to execute all lines till the model. fit(X_train, y_train, epochs = 5, validation_data = (X_test, y_test)). I am just wondering if someone knows why and explain to me in detail I assume that my input variables in the line is the problem that I am having but I dint understand why</p>
<pre><code>

from tensorflow.keras.preprocessing.text import one_hot,Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Flatten,Embedding,Activation,Dropout
from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D
import numpy as np
from numpy import array
import pandas as pd

from sklearn.model_selection import train_test_split

df = pd.read_csv('IMDB Dataset.csv')
df.head()

df['sentiment'].value_counts()

text = df['review'].tolist()
text

y = df['sentiment']

token = Tokenizer()
token.fit_on_texts(text)

token

token.word_index

vocab_size = len(token.word_index) + 1
vocab_size

encoded_text = token.texts_to_sequences(text)

encoded_text

max_length = 120
X = pad_sequences(encoded_text, maxlen = max_length , padding ='post')

X.shape

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42,test_size = 0.2,stratify = y)


vec_size = 300

model = Sequential()
model.add(Embedding(vocab_size, vec_size, input_length = max_length))

model.add(Conv1D(64, 8, activation = 'relu'))
model.add(MaxPooling1D(2))
model.add(Dropout(0.2))

model.add(Dense(32, activation = 'relu'))
model.add(Dropout(0.5))

model.add(Dense(16, activation = 'relu'))

model.add(GlobalMaxPooling1D())

model.add(Dense(1, activation = 'sigmoid'))

model.compile(optimizer='adam',loss = 'binary_crossentropy', metrics = ['accuracy'])

%%time
model.fit(X_train, y_train, epochs = 5, validation_data = (X_test, y_test))
</code></pre>
<p>from this line
''' model.fit(X_train, y_train, epochs = 5, validation_data = (X_test, y_test))'''</p>
<pre><code>Epoch 1/5
</code></pre>
<pre><code>---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_23044\3427946316.py in &lt;module&gt;
      1 '%%time'
----&gt; 2 model.fit(X_train, y_train, epochs = 5, validation_data = (X_test, y_test))

~\anaconda3\lib\site-packages\keras\utils\traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---&gt; 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

~\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50   try:
     51     ctx.ensure_initialized()
---&gt; 52     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                         inputs, attrs, num_outputs)
     54   except core._NotOkStatusException as e:

UnimplementedError: Graph execution error:

Detected at node 'binary_crossentropy/Cast' defined at (most recent call last):
    File &quot;C:\Users\Steven\anaconda3\lib\runpy.py&quot;, line 197, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File &quot;C:\Users\Steven\anaconda3\lib\runpy.py&quot;, line 87, in _run_code
      exec(code, run_globals)
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\ipykernel_launcher.py&quot;, line 17, in &lt;module&gt;
      app.launch_new_instance()
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\traitlets\config\application.py&quot;, line 992, in launch_instance
      app.start()
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\ipykernel\kernelapp.py&quot;, line 711, in start
      self.io_loop.start()
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\tornado\platform\asyncio.py&quot;, line 215, in start
      self.asyncio_loop.run_forever()
    File &quot;C:\Users\Steven\anaconda3\lib\asyncio\base_events.py&quot;, line 601, in run_forever
      self._run_once()
    File &quot;C:\Users\Steven\anaconda3\lib\asyncio\base_events.py&quot;, line 1905, in _run_once
      handle._run()
    File &quot;C:\Users\Steven\anaconda3\lib\asyncio\events.py&quot;, line 80, in _run
      self._context.run(self._callback, *self._args)
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\ipykernel\kernelbase.py&quot;, line 510, in dispatch_queue
      await self.process_one()
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\ipykernel\kernelbase.py&quot;, line 499, in process_one
      await dispatch(*args)
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\ipykernel\kernelbase.py&quot;, line 406, in dispatch_shell
      await result
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\ipykernel\kernelbase.py&quot;, line 729, in execute_request
      reply_content = await reply_content
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\ipykernel\ipkernel.py&quot;, line 418, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\ipykernel\zmqshell.py&quot;, line 531, in run_cell
      return super().run_cell(*args, **kwargs)
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\IPython\core\interactiveshell.py&quot;, line 2914, in run_cell
      result = self._run_cell(
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\IPython\core\interactiveshell.py&quot;, line 2960, in _run_cell
      return runner(coro)
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\IPython\core\async_helpers.py&quot;, line 78, in _pseudo_sync_runner
      coro.send(None)
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\IPython\core\interactiveshell.py&quot;, line 3185, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\IPython\core\interactiveshell.py&quot;, line 3377, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\IPython\core\interactiveshell.py&quot;, line 3457, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File &quot;C:\Users\Steven\AppData\Local\Temp\ipykernel_23044\3427946316.py&quot;, line 2, in &lt;module&gt;
      model.fit(X_train, y_train, epochs = 5, validation_data = (X_test, y_test))
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 65, in error_handler
      return fn(*args, **kwargs)
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1650, in fit
      tmp_logs = self.train_function(iterator)
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1249, in train_function
      return step_function(self, iterator)
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1233, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1222, in run_step
      outputs = model.train_step(data)
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1024, in train_step
      loss = self.compute_loss(x, y, y_pred, sample_weight)
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1082, in compute_loss
      return self.compiled_loss(
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\keras\engine\compile_utils.py&quot;, line 265, in __call__
      loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\keras\losses.py&quot;, line 152, in __call__
      losses = call_fn(y_true, y_pred)
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\keras\losses.py&quot;, line 284, in call
      return ag_fn(y_true, y_pred, **self._fn_kwargs)
    File &quot;C:\Users\Steven\anaconda3\lib\site-packages\keras\losses.py&quot;, line 2165, in binary_crossentropy
      y_true = tf.cast(y_true, y_pred.dtype)
Node: 'binary_crossentropy/Cast'
Cast string to float is not supported
     [[{{node binary_crossentropy/Cast}}]] [Op:__inference_train_function_23546]
</code></pre>
",Training and Model Evaluation,python nlp error anyone explain error occuring due input assume wrong code able execute line till model fit x train train epoch validation data x test test wondering someone know explain detail assume input variable line problem dint understand line model fit x train train epoch validation data x test test
seq2seq inference outputs wrong results despite high accuracy,"<p>I am training a seq2seq model following Keras tutorial <a href=""https://keras.io/examples/nlp/lstm_seq2seq/"" rel=""nofollow noreferrer"">https://keras.io/examples/nlp/lstm_seq2seq/</a>, the same code but a different dataset.
Here is the main model code for reference:</p>
<p>Code snippet for data preparation:</p>
<pre><code>for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):
    for t, char in enumerate(input_text):
        encoder_input_data[i, t, input_token_index[char]] = 1.0
    encoder_input_data[i, t + 1 :, input_token_index[&quot; &quot;]] = 1.0
    for t, char in enumerate(target_text):
        # decoder_target_data is ahead of decoder_input_data by one timestep
        decoder_input_data[i, t, target_token_index[char]] = 1.0
        if t &gt; 0:
            # decoder_target_data will be ahead by one timestep
            # and will not include the start character.
            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0
    decoder_input_data[i, t + 1 :, target_token_index[&quot; &quot;]] = 1.0
    decoder_target_data[i, t:, target_token_index[&quot; &quot;]] = 1.0
</code></pre>
<p>For training:</p>
<pre><code># Define an input sequence and process it.
encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))
encoder = keras.layers.LSTM(latent_dim, return_state=True)
encoder_outputs, state_h, state_c = encoder(encoder_inputs)

# We discard `encoder_outputs` and only keep the states.
encoder_states = [state_h, state_c]

# Set up the decoder, using `encoder_states` as initial state.
decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))

# We set up our decoder to return full output sequences,
# and to return internal states as well. We don't use the
# return states in the training model, but we will use them in inference.
decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=&quot;softmax&quot;)
decoder_outputs = decoder_dense(decoder_outputs)

# Define the model that will turn
# `encoder_input_data` &amp; `decoder_input_data` into `decoder_target_data`
model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.summary()
</code></pre>
<p>Here is the accuracy I got:</p>
<pre><code>Epoch 1/5
1920/1920 [==============================] - 818s 426ms/step - loss: 0.2335 - accuracy: 0.9319 - val_loss: 0.2244 - val_accuracy: 0.9350
Epoch 2/5
1920/1920 [==============================] - 947s 493ms/step - loss: 0.2032 - accuracy: 0.9410 - val_loss: 0.1976 - val_accuracy: 0.9430
Epoch 3/5
1920/1920 [==============================] - 879s 458ms/step - loss: 0.1799 - accuracy: 0.9482 - val_loss: 0.1807 - val_accuracy: 0.9483
Epoch 4/5
1920/1920 [==============================] - 832s 433ms/step - loss: 0.1599 - accuracy: 0.9545 - val_loss: 0.1570 - val_accuracy: 0.9562
Epoch 5/5
1920/1920 [==============================] - 774s 403ms/step - loss: 0.1442 - accuracy: 0.9594 - val_loss: 0.1580 - val_accuracy: 0.9548
</code></pre>
<p>Here is the inference model:</p>
<pre><code>encoder_inputs = model.input[0]  # input_1
encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1
encoder_states = [state_h_enc, state_c_enc]
encoder_model = keras.Model(encoder_inputs, encoder_states)

decoder_inputs = model.input[1]  # input_2
decoder_state_input_h = keras.Input(shape=(latent_dim,))
decoder_state_input_c = keras.Input(shape=(latent_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
decoder_lstm = model.layers[3]
decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(
    decoder_inputs, initial_state=decoder_states_inputs
)
decoder_states = [state_h_dec, state_c_dec]
decoder_dense = model.layers[4]
decoder_outputs = decoder_dense(decoder_outputs)
decoder_model = keras.Model(
    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states
)
def decode_sequence(input_seq):
    # Encode the input as state vectors.
    states_value = encoder_model.predict(input_seq)

    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1, 1, num_decoder_tokens))
    # Populate the first character of target sequence with the start character.
    target_seq[0, 0, target_token_index[&quot;\t&quot;]] = 1.0

    # Sampling loop for a batch of sequences
    # (to simplify, here we assume a batch of size 1).
    stop_condition = False
    decoded_sentence = &quot;&quot;
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :]) #greedy approach 
        sampled_char = reverse_target_char_index[sampled_token_index]
        decoded_sentence += sampled_char

        # Exit condition: either hit max length
        # or find stop character.
        if sampled_char == &quot;\n&quot; or len(decoded_sentence) &gt; max_decoder_seq_length:
            stop_condition = True

        # Update the target sequence (of length 1).
        target_seq = np.zeros((1, 1, num_decoder_tokens))
        target_seq[0, 0, sampled_token_index] = 1.0

        # Update states
        states_value = [h, c]
    return decoded_sentence
for seq_index in range(5):
    # Take one sequence (part of the training set)
    # for trying out decoding.
    input_seq = X_test[seq_index : seq_index + 1]
    decoded_sentence = decode_sequence(input_seq)
    print(&quot;-&quot;)
    print(&quot;Input sentence:&quot;, input_texts[seq_index])
    print(&quot;Decoded sentence:&quot;, decoded_sentence)
</code></pre>
<p>However, the output I am getting is almost random. What could be the reason behind that? the accuracy is increasing in the training, and the loss is decreasing as well.</p>
",Training and Model Evaluation,seq seq inference output wrong result despite high accuracy training seq seq model following kera tutorial code different dataset main model code reference code snippet data preparation training accuracy got inference model however output getting almost random could reason behind accuracy increasing training loss decreasing well
Is GPT-3 a model or a framework?,"<p>We all hear GPT-3 being called a large language model (LLM), but is it really more of a framework since you can use GPT-3 with your own dataset, to train your own version of a GPT-3 model?</p>
<p>My understanding is that a model is the result of training, and you can use one of many frameworks/libraries to train the model (ex: tensor flow).  If GPT-3 was just a model, you wouldn't be able to train with your own data on it, right?  So that makes GPT-3 a framework?</p>
<p>Can anyone help me to better understand the AI terminology for this?</p>
",Training and Model Evaluation,gpt model framework hear gpt called large language model llm really framework since use gpt dataset train version gpt model understanding model result training use one many framework library train model ex tensor flow gpt wa model able train data right make gpt framework anyone help better understand ai terminology
does mlm loss calculate non masked token&#39;s loss too?,"<p>In BERT, I understand what the Masked Language Model(MLM) pretraining task does, but when calculating the loss for this task, how is it exactly calculated?</p>
<p>It is obvious that the loss(e.g. cross entropy loss) for the masked tokens will be included in the final loss.</p>
<p>But what about the other tokens which aren't masked? Is loss calculated for these tokens and included in the final loss as well?</p>
",Training and Model Evaluation,doe mlm loss calculate non masked token loss bert understand masked language model mlm pretraining task doe calculating loss task exactly calculated obvious loss e g cross entropy loss masked token included final loss token masked loss calculated token included final loss well
Find if one column of dataframe contains text from column of another dataframe and add a third column where there is a match,"<p>I am working in an NLP problem statement in python. I have two dataframes -</p>
<p>DF1 -</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Problem</th>
<th>Region</th>
</tr>
</thead>
<tbody>
<tr>
<td>I have wrong product</td>
<td>A</td>
</tr>
<tr>
<td>I have excess payment</td>
<td>A</td>
</tr>
<tr>
<td>address problem</td>
<td>B</td>
</tr>
<tr>
<td>I have delayed delivery</td>
<td>C</td>
</tr>
</tbody>
</table>
</div>
<p>DF2 -</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Key</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td>wrong</td>
<td>Accuracy</td>
</tr>
<tr>
<td>pay</td>
<td>Pay related</td>
</tr>
<tr>
<td>delay</td>
<td>Delay related</td>
</tr>
</tbody>
</table>
</div>
<p>I need a final dataframe that checks if 'Problem' contains 'Key'. eg. &quot;pay&quot; exists in &quot;Excess Payment&quot;
If yes then, 'Category' is assigned. So the resultant dataframe will be -</p>
<p>DF3 -</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Problem</th>
<th>Region</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td>I have wrong product</td>
<td>A</td>
<td>Accuracy</td>
</tr>
<tr>
<td>I have excess payment</td>
<td>A</td>
<td>Pay related</td>
</tr>
<tr>
<td>address problem</td>
<td>B</td>
<td></td>
</tr>
<tr>
<td>I have delayed delivery</td>
<td>C</td>
<td>Delay related</td>
</tr>
</tbody>
</table>
</div>
<p>Have found solutions where index needs to match but that is not the case here. Another vague solution is writing multiple str.contains statement but then that is not scalable. Any leads how to solve this?</p>
",Training and Model Evaluation,find one column dataframe contains text column another dataframe add third column match working nlp problem statement python two dataframes df problem region wrong product excess payment address problem b delayed delivery c df key category wrong accuracy pay pay related delay delay related need final dataframe check problem contains key eg pay exists excess payment yes category assigned resultant dataframe df problem region category wrong product accuracy excess payment pay related address problem b delayed delivery c delay related found solution index need match case another vague solution writing multiple str contains statement scalable lead solve
"In the original T5 paper, what does &#39;step&#39; mean?","<p>I have been reading the original T5 paper 'Exploring the limits of transfer learning with a unified text-to-text transformer.' On page 11, it says &quot;We pre-train each model for 2^19=524,288 <strong>steps</strong> on C4 before fine-tuning.&quot;</p>
<p>I am not sure what the '<strong>steps</strong>' mean. Is it the same as epochs? Or the number of iterations per epoch?</p>
<p>I guess 'steps'='iterations' in a single epoch.</p>
",Training and Model Evaluation,original paper doe step mean reading original paper exploring limit transfer learning unified text text transformer page say pre train model step c fine tuning sure step mean epoch number iteration per epoch guess step iteration single epoch
SetFit Model training on a custom dataset,"<p>I've a dataframe with 3 columns and I'm trying to perform few shot text classification using SetFit model</p>
<p>Dataframe (df)</p>
<pre><code>           A                   B                      C
0    Lorem ipsum ta      lorem ipsum                  Yes
1    Excepteur sint      occaecat excepteur           No
2    Duis aute irure     aute irure                   Yes
</code></pre>
<p>The traditional SetFit model accepts two inputs i.e. Text and label</p>
<p>I want to train the model with an extra input as well i.e. 2 inputs (A,B) to predict the label (C).</p>
<p>And I'm not sure how to apply the SetFit model from Transformers library to do this</p>
<p>I am trying to search how to do this and trying different code pieces but in the meanwhile I would appreciate any guidance, Thank you in advance!</p>
",Training and Model Evaluation,setfit model training custom dataset dataframe column trying perform shot text classification using setfit model dataframe df traditional setfit model accepts two input e text label want train model extra input well e input b predict label c sure apply setfit model transformer library trying search trying different code piece meanwhile would appreciate guidance thank advance
WER for wav2vec2-base model remains as 1 throughout the whole training process,"<p>I am trying to run the <strong>wav2vec2</strong> speech recognition model as shared in <a href=""https://huggingface.co/docs/transformers/tasks/asr"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/tasks/asr</a></p>
<p>This is the <a href=""https://i.sstatic.net/mOA0V.png"" rel=""nofollow noreferrer"">loss and WER</a> during the training process, whereby the validation loss is reducing significantly, whereas the WER remains as 1.</p>
<p>I tried to print out the predicted and label values and this is what I got for the last 3 <a href=""https://i.sstatic.net/RAaaZ.png"" rel=""nofollow noreferrer"">outputs</a>, which results in the WER = 1.</p>
<p>This is the set of parameters of the model. <a href=""https://i.sstatic.net/Rk6ge.png"" rel=""nofollow noreferrer"">model param</a>.</p>
<p>What may actually go wrong here? Please help.. Thanks!</p>
<p>I have tried tuning the hyperparameters and hoping to reduce the WER.</p>
",Training and Model Evaluation,wer wav vec base model remains throughout whole training process trying run wav vec speech recognition model shared loss wer training process whereby validation loss reducing significantly whereas wer remains tried print predicted label value got last output result wer set parameter model model param may actually go wrong please help thanks tried tuning hyperparameters hoping reduce wer
UnimplementedError: Graph execution error during NLP model training,"<p>I am working on an NLP sentiment analysis model to classify the sentiment (neutral, positive, negative) of a tweet based on the content of the tweet on Google Colab. I have prepped the test_x and train_x data into sequences of ints using the Tokenizer module. I followed the Tokenizer tutorial on the official TensorFlow Youtube channel so there should be nothing wrong with that part.</p>
<p>However, when beginning to train the model, I run into UnimplementedError: Graph Execution.</p>
<p>I tried changing the layers of the model and decreasing the size of my data sets but the same error still popped up every time.</p>
<p>Could anyone clarify what this error means and is trying to say and also point out what is wrong with my code? Thanks!</p>
<pre><code>import os
import sys
import tensorflow as tf
import numpy as np
import pandas as pd
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

device_name = tf.test.gpu_device_name()
if len(device_name) &gt; 0:
    print(&quot;Found GPU at: {}&quot;.format(device_name))
else:
    device_name = &quot;/device:CPU:0&quot;
    print(&quot;No GPU, using {}.&quot;.format(device_name))
</code></pre>
<pre><code># Load dataset into a dataframe
train_data_path = &quot;/content/drive/MyDrive/ML Datasets/tweet_sentiment_analysis/train.csv&quot;
test_data_path = &quot;/content/drive/MyDrive/ML Datasets/tweet_sentiment_analysis/test.csv&quot;

train_df = pd.read_csv(train_data_path, encoding='unicode_escape')
test_df = pd.read_csv(test_data_path, encoding='unicode_escape')


train_df.head()
</code></pre>
<pre><code># Function to convert df into a list of strings
def convert_to_list(df, x):
  selected_text_list = []
  labels = []

  for index, row in df.iterrows():
    selected_text_list.append(str(row[x]))
    labels.append(str(row['sentiment']))
  
  return np.array(selected_text_list), np.array(labels)


train_sentences, train_labels = convert_to_list(train_df, 'selected_text')
test_sentences, test_labels = convert_to_list(test_df, 'text')

print(train_sentences)
print(train_labels)
</code></pre>
<pre><code># Instantiate tokenizer and create word_index
tokenizer = Tokenizer(num_words=1000, oov_token='&lt;oov&gt;')
tokenizer.fit_on_texts(train_sentences)
word_index = tokenizer.word_index

# Convert sentences into a sequence 
train_sequence = tokenizer.texts_to_sequences(train_sentences)
test_sequence = tokenizer.texts_to_sequences(test_sentences)

# Padding sequences 
pad_test_seq = pad_sequences(test_sequence, padding='post')
max_len = pad_test_seq[0].size
pad_train_seq = pad_sequences(train_sequence, padding='post', maxlen=max_len)
</code></pre>
<pre><code>model = tf.keras.Sequential([
    tf.keras.layers.Embedding(10000, 24, input_length=max_len),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

with tf.device(device_name):
  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
</code></pre>
<pre><code>num_epochs = 20

with tf.device(device_name):
  history = model.fit(pad_train_seq, train_labels, epochs=num_epochs, validation_data=(pad_test_seq, test_labels), verbose=2)
</code></pre>
<p>Here is a screenshot of the error:
<a href=""https://i.sstatic.net/PcNmt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PcNmt.png"" alt=""enter image description here"" /></a></p>
",Training and Model Evaluation,unimplementederror graph execution error nlp model training working nlp sentiment analysis model classify sentiment neutral positive negative tweet based content tweet google colab prepped test x train x data sequence ints using tokenizer module followed tokenizer tutorial official tensorflow youtube channel nothing wrong part however beginning train model run unimplementederror graph execution tried changing layer model decreasing size data set error still popped every time could anyone clarify error mean trying say also point wrong code thanks screenshot error
Updating model parameters of two models in one optimizer optimizes just one neural network,"<p>I'm trying to train two sequential neural networks in one optimizer. I read that this can be done by defining the optimizer as follows:</p>
<pre><code>optimizer_domain = torch.optim.SGD(list(sentences_model.parameters()) + list(words_model.parameters()),lr=0.001)
</code></pre>
<p>This also works for simple models after testing. However, my model is a bit more complex. A schematic overview is shown below:</p>
<p><a href=""https://i.sstatic.net/7Z3Jn.png"" rel=""nofollow noreferrer"">Model schematic overview</a></p>
<p>The code for the model is found below:</p>
<pre><code>criterion = nn.MSELoss()
optimizer_domain = torch.optim.SGD(list(words_model.parameters()) + list(sentences_model.parameters()),lr=0.001)
epochs = 2
costval_shared_d = []
data = Data(df_xy, yi_train)
train_loader=DataLoader(data,batch_size=batch_size_Tanh,shuffle=True)

for j in range(epochs):
    for x,y in train_loader:
        words_model_output = words_model(x,y)

        if (all words of a sentence have passed):
            adjusted_sentence = some_functions(words_model_output)
            train_loader_sentences = DataLoader(adjusted_sentences,batch_size=batch_size_sentences,shuffle=True)
            for x_sentence,y_sentence in train_loader_sentences:
                output_sentences = sentences_model(x_sentence)

                #calculating loss
                cost_shared_d = criterion(y_sentence,y_real)

                print(words_model.param1.grad,sentences_model.grad) #Here the gradients of all parameters of words_model are None
                
                optimizer_domain.zero_grad()
                
                cost_shared_d.backward()
                
                optimizer_domain.step()

                costval_shared_d.append(cost_shared_d)
</code></pre>
<p>As you can see in the code above I'm first loading a dataset of all (numerical representations of) words and I'm passing them through words_model to decide if the words should be masked in the sentence. Then, if it has been decided which words in the sentence should be masked, the new sentence with masked words is passed on through some functions to obtain the representation of the sentence. Then the sentence is passed onto a train_loader that feeds the sentence to sentences_model, which is the second neural network. This model tries to classify the domain of the sentence.</p>
<p>Then the loss is calculated and the optimizer makes a step. But then I see that only the parameters of the sentences_model are updated and not the parameters of the words_model where the gradient is zero.</p>
<p>I'm not sure how I can also train the first neural network, the words_model, at the same time. Because the input of the second neural network is the output of the first neural network and the sequential network is trained on a classification task that only uses the output of the second network, it is important that those networks are updated simultaneously.</p>
<p>Any thoughts or help would be greatly appreciated.</p>
",Training and Model Evaluation,updating model parameter two model one optimizer optimizes one neural network trying train two sequential neural network one optimizer read done defining optimizer follows also work simple model testing however model bit complex schematic overview shown model schematic overview code model found see code first loading dataset numerical representation word passing word model decide word masked sentence ha decided word sentence masked new sentence masked word passed function obtain representation sentence sentence passed onto train loader feed sentence sentence model second neural network model try classify domain sentence loss calculated optimizer make step see parameter sentence model updated parameter word model gradient zero sure also train first neural network word model time input second neural network output first neural network sequential network trained classification task us output second network important network updated simultaneously thought help would greatly appreciated
Gensim Word2Vec produces different most_similar results through final epoch than end of training,"<p>I'm using gensim's Word2Vec for a recommendation-like task with part of my evaluation being the use of callbacks and the <code>most_similar()</code> method. However, I am noticing a huge disparity between the final few epoch callbacks and that of immediately post-training. In fact, the last epoch callback may often appear worthless, while the post training result is as best as could be desired.</p>
<p>My during-training tracking of most similar entries utilizes gensim's <code>CallbackAny2Vec</code> class. It follows the <a href=""https://radimrehurek.com/gensim/models/callbacks.html"" rel=""nofollow noreferrer"">doc example</a> fairly directly and roughly looks like:</p>
<pre><code>class EpochTracker(CallbackAny2Vec):

  def __init__(self):
    self.epoch = 0

  def on_epoch_begin(self, model):
    print(&quot;Epoch #{} start&quot;.format(self.epoch))

  def on_epoch_end(self, model):
    
    print('Some diagnostics')
    # Multiple terms used in the below
    e = model.wv
    print(e.most_similar(positive=['some term'])[0:3]) # grab the top 3 examples for some term

    print(&quot;Epoch #{} end&quot;.format(self.epoch))
    self.epoch += 1
</code></pre>
<p>As the epochs progress, the <code>most_similar()</code> results given by the callbacks to not seem to indicate an advancement of learning and seem erratic. In fact, often the callback from the first epoch shows the best result.</p>
<p>Counterintuitively, I also have an additional process (not shown) built into the callback that does indicate gradual learning. Following the similarity step, I take the current model's vectors and evaluate them against a down-stream task. In brief, this process is a sklearn <code>GridSearchCV</code> logistic regression check against some known labels.</p>
<p>I find that often the last <code>on_epoch_end</code> callback appears to be garbage. Or perhaps some multi-threading shenanigans. However, if directly after training the model I try the similarity call again:</p>
<pre><code>e = e_model.wv # e_model was the variable assignment of the model overall
print(e.most_similar(positive=['some term'])[0:3])
</code></pre>
<p>I tend to get beautiful results that are in agreement with the downstream evaluation task also used in the callbacks, or are at least vastly different than that of the final epoch end.</p>
<p>I suspect I am missing something painfully apparent or <code>most_similar()</code> has an unusual behavior with epoch-end callbacks. Is this a known issue or is my approach flawed?</p>
",Training and Model Evaluation,gensim word vec produce different similar result final epoch end training using gensim word vec recommendation like task part evaluation use callback method however noticing huge disparity final epoch callback immediately post training fact last epoch callback may often appear worthless post training result best could desired training tracking similar entry utilizes gensim class follows doc example fairly directly roughly look like epoch progress result given callback seem indicate advancement learning seem erratic fact often callback first epoch show best result counterintuitively also additional process shown built callback doe indicate gradual learning following similarity step take current model vector evaluate stream task brief process sklearn logistic regression check known label find often last callback appears garbage perhaps multi threading shenanigan however directly training model try similarity call tend get beautiful result agreement downstream evaluation task also used callback least different final epoch end suspect missing something painfully apparent ha unusual behavior epoch end callback known issue approach flawed
Confusion Matrix interpretation data perfectly balanced,"<p>I have trained a transformer based classifier with 2 classes (0,1) reaching a 91 % accuracy on a perfectly balanced dataset. I printed out the confusion matrix on validation data after had tuned the threshold on them and those are the results but they are perfectly balanced. Makes sense in your opinion?</p>
<pre><code>09:29:30 root INFO:*** EVALUATION ON VALIDATION DATA ***
09:29:30 root INFO:AUC: 0.9708
09:29:30 root INFO:Tuned Threshold: 0.3104
09:29:31 root INFO:Matthews Correlation Coefficient computed after applying the tuned/selected threshold : 0.8230210619188743
09:29:31 root INFO:Accuracy: 91.15%
09:29:32 root INFO:--Classification report for VAL DATA--
09:29:32 root INFO:              precision    recall  f1-score   support

          0       0.91      0.91      0.91     88406
          1       0.91      0.91      0.91     88406

   accuracy                           0.91    176812
  macro avg       0.91      0.91      0.91    176812
weighted avg       0.91      0.91      0.91    176812

        pred:0  pred:1
true:0   80583    7823
true:1    7823   80583
</code></pre>
<p>Thanks for the advice.</p>
<p>UPDATE:</p>
<p>confusion matrix on test set using the same threshold:</p>
<pre><code>        pred:0  pred:1
true:0   81714    9968
true:1    9612   82070
</code></pre>
",Training and Model Evaluation,confusion matrix interpretation data perfectly balanced trained transformer based classifier class reaching accuracy perfectly balanced dataset printed confusion matrix validation data tuned threshold result perfectly balanced make sense opinion thanks advice update confusion matrix test set using threshold
Evaluation of gensim Doc2Vec model for Recommendations,"<p>I have developed a pipeline to extract text from documents, preprocess the text, and train a gensim Doc2vec model on given documents. Given a document in my corpus, I would like to recommend other documents in the corpus.</p>
<p>I want to know how I can evaluate my model without having a pre-defined list of &quot;good&quot; recommendations. Any ideas?</p>
",Training and Model Evaluation,evaluation gensim doc vec model recommendation developed pipeline extract text document preprocess text train gensim doc vec model given document given document corpus would like recommend document corpus want know evaluate model without pre defined list good recommendation idea
how to implement trained nlp spacy model in a front end application,"<p>I am new to ML and DS. I have done many projects on Kaggle or Colab but this is my first time doing an NLP project on Resume Parsing using spacy.</p>
<p>I have trained my model using data from 200 resumes and now i have a model best and a model last</p>
<p>i tried this model on the test data and it works almost flawlessly by returning the label from the text.</p>
<p>I want guidance on how i can use this model and create either a web app or a mobile app</p>
<p>I have some level of knowledge on how to make a material app using flutter and also java mobile apps</p>
<p>but i jus dont know how i can proceed from here</p>
<p>my project is just a python notebook on google colab and everytime i wanna use it i have to train the model for 2 hours</p>
<p>The model just returns labelled data like name, college name, degree, companies worked at and so on from the txt data that i extracted using PyMupdf</p>
<p><a href=""https://github.com/simpostor/resumeparse"" rel=""nofollow noreferrer"">github repo</a></p>
<p>Tbh i didnt try much because i just dont know how to approach this</p>
",Training and Model Evaluation,implement trained nlp spacy model front end application new ml done many project kaggle colab first time nlp project resume parsing using spacy trained model using data resume model best model last tried model test data work almost flawlessly returning label text want guidance use model create either web app mobile app level knowledge make material app using flutter also java mobile apps jus dont know proceed project python notebook google colab everytime wan na use train model hour model return labelled data like name college name degree company worked txt data extracted using pymupdf github repo tbh didnt try much dont know approach
the right way to make prediction using Spacy word vectors,"<p>Im learning how to convert text into numbers for NLP problems and following a course Im learning about word vectors provided by Spacy package. the code works all fine from learning and evaluation but I have some problems regarding:</p>
<ol>
<li><p>making prediction for new sentences, I cannot seems to make it work and most examples just fit the model then use X_test set for evaluation. ( Code below)</p>
</li>
<li><p>The person explaining stated that its bad( won't give good results) if I used</p>
</li>
</ol>
<p>&quot;&quot;
doc.vector   over doc.vector.values</p>
<p>&quot;&quot;</p>
<p>when trying both I don't see a difference, what is the difference between the two?</p>
<p>the example is to classify news title between fake and real</p>
<pre><code>import spacy
import pandas as pd

df= pd.read_csv('Fake_Real_Data.csv')

print(df.head())
print(f&quot;shape is: {df.shape}&quot;)


print(&quot;checking the impalance: \n &quot;, df.label.value_counts())



df['label_No'] = df['label'].map({'Fake': 0, 'Real': 1})
print(df.head())




nlp= spacy.load('en_core_web_lg') # only large and medium model have word vectors


df['Text_vector'] = df['Text'].apply(lambda x: nlp(x).vector) #apply the function to EACH element in the column
print(df.head(5))



from sklearn.model_selection import train_test_split

X_train, X_test, y_train,y_test= train_test_split(df.Text_vector.values, df.label_No, test_size=0.2, random_state=2022)




x_train_2D= np.stack(X_train)
x_test_2D= np.stack(X_test)



from sklearn.naive_bayes import MultinomialNB

clf=MultinomialNB()

from sklearn.preprocessing import MinMaxScaler

scaler= MinMaxScaler()

scaled_train_2d= scaler.fit_transform(x_train_2D)
scaled_test_2d= scaler.transform(x_test_2D) 

clf.fit(scaled_train_2d, y_train)

from sklearn.metrics import classification_report

y_pred=clf.predict(scaled_test_2d)

print(classification_report(y_test, y_pred))




</code></pre>
",Training and Model Evaluation,right way make prediction using spacy word vector im learning convert text number nlp problem following course im learning word vector provided spacy package code work fine learning evaluation problem regarding making prediction new sentence seems make work example fit model use x test set evaluation code person explaining stated bad give good result used doc vector doc vector value trying see difference difference two example classify news title fake real
Algorithm for Negating Sentences,"<p>I was wondering if anyone was familiar with any attempts at algorithmic sentence negation.</p>

<p>For example, given a sentence like ""This book is good"" provide any number of alternative sentences meaning the opposite like ""This book is not good"" or even ""This book is bad"".</p>

<p>Obviously, accomplishing this with a high degree of accuracy would probably be beyond the scope of current NLP, but I'm sure there has been some work on the subject.  If anybody knows of any work, care to point me to some papers?</p>
",Training and Model Evaluation,algorithm negating sentence wa wondering anyone wa familiar attempt algorithmic sentence negation example given sentence like book good provide number alternative sentence meaning opposite like book good even book bad obviously accomplishing high degree accuracy would probably beyond scope current nlp sure ha work subject anybody know work care point paper
Fine-tuning distilbert takes hours,"<p>I am fine tuning the distilbert pretrained model for sentiment analysis (multilabel with 6 labels) using Huggingface emotion dataset. I am new to this, but 1 epoch, 250 steps takes around 2 hours to train on Google Colab notebook, is this normal? The train dataset has 16.000 twitter text data which of course affects the performance but isn't this too long? What is the reason behind this?</p>
<p>Also after 3 epochs, the accuracy started to drop. What could be the reason for this?</p>
",Training and Model Evaluation,fine tuning distilbert take hour fine tuning distilbert pretrained model sentiment analysis multilabel label using huggingface emotion dataset new epoch step take around hour train google colab notebook normal train dataset ha twitter text data course affect performance long reason behind also epoch accuracy started drop could reason
Can anyone explain how to get BIDMach&#39;s Word2vec to work?,"<p>In a paper titled, ""<a href=""http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363760"" rel=""noreferrer"">Machine Learning at the Limit</a>,"" Canny, et. al. report substantial <a href=""https://code.google.com/archive/p/word2vec/"" rel=""noreferrer"">word2vec</a> processing speed improvements. </p>

<p>I'm working with the <a href=""https://github.com/BIDData/BIDMach"" rel=""noreferrer"">BIDMach</a> library used in this paper, and cannot find any resource that explains how Word2Vec is implemented or how it should be used within this framework.</p>

<p>There are several scripts in the repo:</p>

<ul>
<li><a href=""https://github.com/BIDData/BIDMach/blob/master/scripts/getw2vdata.sh"" rel=""noreferrer"">getw2vdata.sh</a></li>
<li><a href=""https://github.com/BIDData/BIDMach/blob/master/scripts/getw2vdata.ssc"" rel=""noreferrer"">getwv2data.ssc</a></li>
</ul>

<p>I've tried running them (after building the referenced <code>tparse2.exe</code> file) with no success.</p>

<p>I've tried modifying them to get them to run but have nothing but errors come back.</p>

<p>I emailed the author, and posted <a href=""https://github.com/BIDData/BIDMach/issues/96"" rel=""noreferrer"">an issue on the github repo</a>, but have gotten nothing back. I only got somebody else having the same troubles, who says he got it to run but at much slower speeds than reported on newer GPU hardware.</p>

<p>I've searched all over trying to find anyone that has used this library to achieve these speeds with no luck. There are multiple references floating around that point to this library as the fastest implementation out there, and cite the numbers in the paper:</p>

<ul>
<li><a href=""https://pdfs.semanticscholar.org/cced/c38f68ffaf51cf8c31cd6c6b5c2cf033f91a.pdf"" rel=""noreferrer"">Intel research references the reported numbers without running the code on GPU (they cite numbers reported in the original paper)</a></li>
<li><a href=""https://www.reddit.com/r/MachineLearning/comments/4p3enc/advice_library_for_training_word2vec_on_gpu/"" rel=""noreferrer"">old reddit post pointing to BIDMach as the best</a> (but the OP says ""I haven't tested BIDMach myself yet"")</li>
<li><a href=""https://stackoverflow.com/questions/30573873/how-to-train-word2vec-on-very-large-datasets"">SO post citing BIDMach as the best</a> (OP doesn't actually run the library to make this claim...)</li>
<li>many more not worth listing citing BIDMach as the best/fastest without example or claims of ""I haven't tested myself...""</li>
</ul>

<p>When I search for a similiar library (gensim), and the <code>import</code> code required to run it, <a href=""https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=%22from+gensim.models+import+Word2Vec%22&amp;*"" rel=""noreferrer"">I find thousands of results and tutorials</a> but a similar search for the BIDMach code <a href=""https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=%22import+BIDMach.networks.Word2Vec%22&amp;*"" rel=""noreferrer"">yields only the BIDMach repo</a>.</p>

<p>This BIDMach implementation certainly carries the reputation for being the best, but <strong>can anyone out there tell me how to use it</strong>?</p>

<p>All I want to do is run a simple training process to compare it to a handful of other implementations on my own hardware.</p>

<p>Every other implementation of this concept I can find either has works with the <a href=""https://github.com/svn2github/word2vec/blob/master/demo-word.sh"" rel=""noreferrer"">original shell script test file</a>, <a href=""https://github.com/yindlib/cuda-word2vec"" rel=""noreferrer"">provides actual instructions</a>, or <a href=""https://github.com/IntelLabs/pWord2Vec/tree/master/sandbox"" rel=""noreferrer"">provides shell scripts of their own</a> to <a href=""https://github.com/facebookresearch/fastText/blob/master/word-vector-example.sh"" rel=""noreferrer"">test</a>.</p>

<hr>

<p>UPDATE:
The author of the library has added additional shell scripts to get the previously mentioned scripts running, but exactly what they mean or how they work is still a total mystery and I can't understand how to get the word2vec training procedure to run on my own data.</p>

<hr>

<p><strong>EDIT (for bounty)</strong></p>

<p>I'll give out the bounty to anywone that can explain how I'd use my own corpus (text8 would be great), and then train a model, and then save the ouput vectors and the vocabulary to files that can be read by <a href=""https://bitbucket.org/omerlevy/hyperwords"" rel=""noreferrer"">Omar Levy's Hyperwords</a>.</p>

<p>This is exactly what the original C implementation would do with arguments <code>-binary 1 -output vectors.bin -save-vocab vocab.txt</code></p>

<p>This is also what Intel's implementation does, and other CUDA implementations, etc, so this is a great way to generate something that can be easily compared with other versions...</p>

<hr>

<p><strong>UPDATE (bounty expired without answer)</strong>
John Canny has updated a few scripts in the repo and added a <code>fmt.txt</code> file, thus making it possible to run test scripts that are package in the repo.</p>

<p>However, my attempt to run this with the <strong>text8</strong> corpus yields near 0% accuracy on they hyperwords test.</p>

<p>Running the training process on the billion word benchmark (which is what the repo scripts now do) also yields well-below-average accuracy on the hyperwords test.</p>

<p>So, either the library never yielded accuracy on these tests, or I'm still missing something in my setup. </p>

<p><a href=""https://github.com/BIDData/BIDMach/issues/96"" rel=""noreferrer"">The issue remains open on github</a>.</p>
",Training and Model Evaluation,anyone explain get bidmach word vec work paper titled machine learning limit canny et al report substantial word vec processing speed improvement working bidmach library used paper find resource explains word vec implemented used within framework several script repo getw vdata sh getwv data ssc tried running building referenced file success tried modifying get run nothing error come back emailed author posted issue github repo gotten nothing back got somebody else trouble say got run much slower speed reported newer gpu hardware searched trying find anyone ha used library achieve speed luck multiple reference floating around point library fastest implementation cite number paper intel research reference reported number without running code gpu cite number reported original paper old reddit post pointing bidmach best op say tested bidmach yet search similiar library gensim code required run find thousand result tutorial similar search bidmach code yield bidmach repo bidmach implementation certainly carry reputation best anyone tell use want run simple training process compare handful implementation hardware every implementation concept find either ha work original shell script test file provides actual instruction provides shell script test update author library ha added additional shell script get previously mentioned script running exactly mean work still total mystery understand get word vec training procedure run data edit bounty give bounty anywone explain use corpus text would great train model save ouput vector vocabulary file read omar levy hyperwords exactly original c implementation would argument also intel implementation doe cuda implementation etc great way generate something easily compared version update bounty expired without answer john canny ha updated script repo added file thus making possible run test script package repo however attempt run text corpus yield near accuracy hyperwords test running training process billion word benchmark repo script also yield well average accuracy hyperwords test either library never yielded accuracy test still missing something setup issue remains open github
How to get word timestamp given an audio and its transcript?,"<p>Hi I am working on timestamp generation for my transcript. I have already had the transcript.</p>
<p>I am wondering if there is more accurate way to get timestamp besides algorithms like Montreal-forced-alignment. They have some accuracy issue to me.</p>
",Training and Model Evaluation,get word timestamp given audio transcript hi working timestamp generation transcript already transcript wondering accurate way get timestamp besides algorithm like montreal forced alignment accuracy issue
improve gensim most_similar() return values by using wordnet hypernyms,"<pre><code>import gensim.downloader as api
glove = api.load('glove-wiki-gigaword-200')
</code></pre>
<p>I first ran this code to download the pre-trained model.</p>
<pre><code>glove.most_similar(positive=['sushi', 'uae'], negative=['japan'])
</code></pre>
<p>would then result in:</p>
<pre><code>[('nahyan', 0.5181387066841125),
 ('caviar', 0.4778318405151367),
 ('paella', 0.4497394263744354),
 ('nahayan', 0.44313961267471313),
 ('zayed', 0.4321245849132538),
 ('omani', 0.4285220503807068),
 ('seafood', 0.4279175102710724),
 ('saif', 0.426000714302063),
 ('dirham', 0.4214130640029907),
 ('sashimi', 0.4165934920310974)]
</code></pre>
<p>and in this example, we can see that the method failed to capture the 'type' or 'category' of the query. 'zayed', 'nahyan' are not actually of 'type' food and rather they represent person name.</p>
<p>The approach suggested by my professor is to use wordnet hypernyms to find the 'type'.</p>
<p>With much research, the closest solution I found is to somehow incorporate
<code>lowest_common_hypernyms()</code> that will give the lowest common hypernym between two synsets and use it to filter the results of <code>most_similar()</code>.</p>
<p>I am not sure if my idea make sense and would like the community feedback on this.</p>
<p>My idea is compute the hypernym of, e.g. 'sushi' and the hypernyms of all the similar words returned by <code>most_similar()</code> and only choose the word with 'longest' lowest common hypernym path. I expect this should return the word that best matches the 'type'</p>
<p>Not sure if it makes sense...</p>
",Training and Model Evaluation,improve gensim similar return value using wordnet hypernym first ran code download pre trained model would result example see method failed capture type category query zayed nahyan actually type food rather represent person name approach suggested professor use wordnet hypernym find type much research closest solution found somehow incorporate give lowest common hypernym two synset use filter result sure idea make sense would like community feedback idea compute hypernym e g sushi hypernym similar word returned choose word longest lowest common hypernym path expect return word best match type sure make sense
Text classification with a Language Model (LM) with class labels existing in text tokens,"<p>I have a multi-label text classification task. The train data labels are categories that might exist as tokens in the training data texts. For instance, some observations look like the following:</p>
<pre><code>Train=[[&quot;input&quot;: &quot;Dogs are animals. Dogs  ove humans.&quot;, class: [&quot;dog&quot;]],
       [&quot;input&quot;: &quot;Cats are running in the street.&quot;, class: [&quot;cat&quot;]],
       [&quot;input&quot;: &quot;Cats and dogs live with humans.&quot;, class: [&quot;cat&quot;, &quot;dog&quot;]],
       [&quot;input&quot;: &quot;These animals don't each chocolate.&quot;, class: [&quot;dog&quot;]]]
</code></pre>
<p>I want to train a classifier by fine-tuning a language model using  Pytorch. My question is if I must ensure that the class labels are masked in the training input text? If not, will the classifier overfit or lose generalizability?</p>
<p>If I must mask the labels in the inputs, how can I do it using Pytorch?</p>
",Training and Model Evaluation,text classification language model lm class label existing text token multi label text classification task train data label category might exist token training data text instance observation look like following want train classifier fine tuning language model using pytorch question must ensure class label masked training input text classifier overfit lose generalizability must mask label input using pytorch
generating multi classifier training data from document,"<p>I am looking for guidance to generate multiple classifier training data from document. e.g. if particular document has three sections with each 10 pages in each sections. (total 30 pages)</p>
<p>I am looking for open source library, where I can pass on document (explicitly specifying section 1, section 2 and section 3 pages) then it can give me list of important words to be used as training data to identify &quot;section 1&quot; vs &quot;section 2&quot; vs &quot;section 3&quot;. (multiple classification)</p>
",Training and Model Evaluation,generating multi classifier training data document looking guidance generate multiple classifier training data document e g particular document ha three section page section total page looking open source library pas document explicitly specifying section section section page give list important word used training data identify section v section v section multiple classification
HuggingFace hate detection model,"<p>I am a beginner in NLP and have undertaken a challenge.</p>
<p>I am trying to train and evaluate a hate detection model using the <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">HuggingFace Transformers library</a> and this <a href=""https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data"" rel=""nofollow noreferrer"">dataset</a>. Model performance is secondary, just trying to get it going. I have preprocessed the data and tokenised it as shown below:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
from numpy.random import RandomState
import re
import preprocessor as p
from transformers import AutoTokenizer

# Loading raw data
original_data = pd.read_csv('../data/data.csv')

# Make a random test and train split
rng = RandomState()
train = original_data.sample(frac=0.7, random_state=rng)
test = original_data.loc[~df.index.isin(train.index)]

# Preprocessing: remove special characters using RegEx
REPLACE_NO_SPACE = re.compile(&quot;(\.)|(\;)|(\:)|(\!)|(\')|(\?)|(\,)|(\&quot;)|(\|)|(\()|(\))|(\[)|(\])|(\%)|(\$)|(\&gt;)|(\&lt;)|(\{)|(\})&quot;)
REPLACE_WITH_SPACE = re.compile(&quot;(&lt;br\s/&gt;&lt;br\s/?)|(-)|(/)|(:).&quot;)

# Custom function to clean the datasets
def clean_tweets(df):
  tempArr = []
  for line in df:
    # send to tweet_processor
    tmpL = p.clean(line)
    # remove puctuation
    tmpL = REPLACE_NO_SPACE.sub(&quot;&quot;, tmpL.lower()) # convert all tweets to lower cases
    tmpL = REPLACE_WITH_SPACE.sub(&quot; &quot;, tmpL)
    tempArr.append(tmpL)
  return tempArr

# clean training data
train_tweet = clean_tweets(train[&quot;tweet&quot;])
train_tweet = pd.DataFrame(train_tweet)
# append cleaned tweets to the testing data
train[&quot;clean_tweet&quot;] = train_tweet

# clean the test data 
test_tweet = clean_tweets(test[&quot;tweet&quot;])
test_tweet = pd.DataFrame(test_tweet)
# append cleaned tweets to the training data
test[&quot;clean_tweet&quot;] = test_tweet

# Tokenisation so the inputs are ready for the model
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
</code></pre>
<p>The above code generates a table for the test data as:
<a href=""https://i.sstatic.net/dMzOi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dMzOi.png"" alt=""enter image description here"" /></a></p>
<p>As I understand, the next part should be the model training part and extracting the % of hate tweets. Any suggestions on implementation?</p>
",Training and Model Evaluation,huggingface hate detection model beginner nlp undertaken challenge trying train evaluate hate detection model using huggingface transformer library dataset model performance secondary trying get going preprocessed data tokenised shown code generates table test data understand next part model training part extracting hate tweet suggestion implementation
Possible to do incremental training with AWS comprehend?,"<p>I am looking at AWS Comprehend for a text classification task which will involve an active learning component. I am trying to understand if it's possible to incrementally train a custom comprehend model using batches of newly annotated data, or if it only supports training from scratch. In this <a href=""https://aws.amazon.com/blogs/machine-learning/active-learning-workflow-for-amazon-comprehend-custom-classification-part-2/"" rel=""nofollow noreferrer"">blog post</a> it sounds like they are stitching the annotated data back together with the original training data (i.e. retraining from screatch each time), but I don't see the mentioned cloudformation template (part 1 has the template for training/deployment, but part 2 seems to be talking about another template).</p>
<p>Is it possible to do incremental training with Comprehend? Or would I need to use a custom text classification model through SageMaker and then do incremental training that way? I am attempting to do the following</p>
<ol>
<li>Get a pretrained model</li>
<li>Fine tune on own classification data</li>
<li>Incrementally train on annotated low confidence preditions</li>
</ol>
<p>1 and 2 can be done with AWS Comprehend, but not sure about 3. Thanks</p>
",Training and Model Evaluation,possible incremental training aws comprehend looking aws comprehend text classification task involve active learning component trying understand possible incrementally train custom comprehend model using batch newly annotated data support training scratch blog post sound like stitching annotated data back together original training data e retraining screatch time see mentioned cloudformation template part ha template training deployment part seems talking another template possible incremental training comprehend would need use custom text classification model sagemaker incremental training way attempting following get pretrained model fine tune classification data incrementally train annotated low confidence preditions done aws comprehend sure thanks
Training an Hugginface model without n_epochs,"<p>I would like to train from scratch a   <code>RobertaForMaskedLM</code> in Hugginface.
However I would like to not specify any stopping time, but to stop only when there is no more improvement in the training. There is a way to do that? I know that the n_epochs in the  <code>Trainer </code>  is default 3.</p>
<p>Thanks</p>
",Training and Model Evaluation,training hugginface model without n epoch would like train scratch hugginface however would like specify stopping time stop improvement training way know n epoch default thanks
Search through GPT-3&#39;s training data,"<p>I'm using GPT-3 for some experiments where I prompt the language model with tests from cognitive science. The tests have the form of short text snippets. Now I'd like to check whether GPT-3 has already encountered these text snippets during training. Hence my question: Is there any way to sift through GPT-3's training text corpora? Can one find out whether a certain string is part of these text corpora?</p>
<p>Thanks for your help!</p>
",Training and Model Evaluation,search gpt training data using gpt experiment prompt language model test cognitive science test form short text snippet like check whether gpt ha already encountered text snippet training hence question way sift gpt training text corpus one find whether certain string part text corpus thanks help
Warnings while saving model,"<p>I am using the following model for machine translation using transformer with KerasNLP:
`</p>
<pre><code># Encoder
encoder_inputs = keras.Input(shape=(None,), dtype=&quot;int64&quot;, name=&quot;encoder_inputs&quot;)

x = keras_nlp.layers.TokenAndPositionEmbedding(
    vocabulary_size=ENG_VOCAB_SIZE,
    sequence_length=MAX_SEQUENCE_LENGTH,
    embedding_dim=EMBED_DIM,
    mask_zero=True,
)(encoder_inputs)

encoder_outputs = keras_nlp.layers.TransformerEncoder(
    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS
)(inputs=x)
encoder = keras.Model(encoder_inputs, encoder_outputs)


# Decoder
decoder_inputs = keras.Input(shape=(None,), dtype=&quot;int64&quot;, name=&quot;decoder_inputs&quot;)
encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=&quot;decoder_state_inputs&quot;)

x = keras_nlp.layers.TokenAndPositionEmbedding(
    vocabulary_size=SND_VOCAB_SIZE,
    sequence_length=MAX_SEQUENCE_LENGTH,
    embedding_dim=EMBED_DIM,
    mask_zero=True,
)(decoder_inputs)

x = keras_nlp.layers.TransformerDecoder(
    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS
)(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)
x = keras.layers.Dropout(0.5)(x)
decoder_outputs = keras.layers.Dense(SND_VOCAB_SIZE, activation=&quot;softmax&quot;)(x)
decoder = keras.Model(
    [
        decoder_inputs,
        encoded_seq_inputs,
    ],
    decoder_outputs,
)
decoder_outputs = decoder([decoder_inputs, encoder_outputs])

transformer = keras.Model(
    [encoder_inputs, decoder_inputs],
    decoder_outputs,
    name=&quot;transformer&quot;,
)
</code></pre>
<p>`</p>
<p>`</p>
<pre><code>transformer.summary()
transformer.compile(
    &quot;rmsprop&quot;, loss=&quot;sparse_categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;]
)
hist = transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)
</code></pre>
<p>`</p>
<p>When I am trying to save the model it is giving me warnings and it is not predicting correctly:</p>
<p>`</p>
<pre><code>with open('model.pkl', 'wb') as file:
  pickle.dump(transformer, file)
</code></pre>
<p>`</p>
<p>WARNING:absl:Found untraced functions such as token_embedding1_layer_call_fn, token_embedding1_layer_call_and_return_conditional_losses, position_embedding1_layer_call_fn, position_embedding1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 78). These functions will not be directly callable after loading.</p>
",Training and Model Evaluation,warning saving model using following model machine translation using transformer kerasnlp trying save model giving warning predicting correctly warning absl found untraced function token embedding layer call fn token embedding layer call return conditional loss position embedding layer call fn position embedding layer call return conditional loss multi head attention layer call fn saving showing function directly callable loading
Having trouble training Word2Vec iteratively on Gensim,"<p>I'm attempting to train multiple texts supplied by myself iteratively. However, I keep running into  an issue when I train the model more than once:</p>
<blockquote>
<p>ValueError: You must specify either total_examples or total_words, for proper learning-rate and progress calculations. If you've just built the vocabulary using the same corpus, using the count cached in the model is sufficient: total_examples=model.corpus_count.</p>
</blockquote>
<p>I'm currently initiating my model like this:</p>
<pre class=""lang-py prettyprint-override""><code>model = Word2Vec(sentences, min_count=0, workers=cpu_count())
model.build_vocab(sentences, update=False)
model.save('firstmodel.model')

model = Word2Vec.load('firstmodel.model')
</code></pre>
<p>and subsequently training it iteratively like this:</p>
<pre class=""lang-py prettyprint-override""><code>model.build_vocab(sentences, update = True)
model.train(sentences, totalexamples=model.corpus_count, epochs=model.epochs)
</code></pre>
<p>What am I missing here?</p>
<p>Somehow, it worked when I just trained one other model, so not sure why it doesn't work beyond two models...</p>
",Training and Model Evaluation,trouble training word vec iteratively gensim attempting train multiple text supplied iteratively however keep running issue train model valueerror must specify either total example total word proper learning rate progress calculation built vocabulary using corpus using count cached model sufficient total example model corpus count currently initiating model like subsequently training iteratively like missing somehow worked trained one model sure work beyond two model
Pytorch LSTM and cross entropy,"<p>I am working on sentiment analysis, I want to classify the output into 4 classes. For <strong>loss</strong> I am using <strong>cross-entropy</strong>.</p>
<p>The problem is PyTorch cross-entropy needs the input of <strong>(batch_size, output)</strong> which is am having trouble with.</p>
<p>I am taking a <strong>batch size of 12</strong> and <strong>sequence size is 32</strong></p>
<pre><code>import torch.nn as nn


class RNN(nn.Module):
    def __init__(self, hidden_dim = 256, input_size = 32 , num_layers = 1, num_classes=4, vocab_size = len(vocab_to_int)+1, embedding_dim=100):
        super().__init__()
        self.input_size = input_size
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.num_classes = num_classes
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers)
        self.fc1 = nn.Linear(hidden_dim, 50)
        self.fc2 = nn.Linear(50, 4)

    
    def forward(self, x, hidden):
        x = self.embedding(x)
        x = x.view(32, 12, 100)
        x, hidden = self.lstm(x, hidden)
        x = x.contiguous().view(-1, 256)
        x = self.fc1(x) # output shape ([384, 50])
        x = self.fc2(x) # output shape [384, 4]
        return x, hidden
    
    def init_hidden(self, batch_size=12):
        weight = next(self.parameters()).data
        hidden = (weight.new(self.num_layers, 12, self.hidden_dim).zero_().cuda(), weight.new(self.num_layers, 12, self.hidden_dim).zero_().cuda())
        return hidden
</code></pre>
",Training and Model Evaluation,pytorch lstm cross entropy working sentiment analysis want classify output class loss using cross entropy problem pytorch cross entropy need input batch size output trouble taking batch size sequence size
NLG | T5 Paraphrase results are not creative enough,"<p>I've tried to use pre-trained model to create paraphrases of sentences,
Even after trying to correlate top_p and top_k the result paraphrases were almost the same as the original sentence.
I would like to get results that look completely different (eg. third person talking about this topic or to point an event that illustrates this topic.)</p>
<p>Below is an example of three sentences that express the same idea in different words:</p>
<ol>
<li>Looking within is the best way to start solving the challenges in our lives.</li>
<li>People who are looking for solutions to their problems in the world around them will always continue to look for solutions.</li>
<li>The first step in healing our pain lies in our ability to &quot;look in the mirror&quot;.
Do you think it possible is to achieve those results with more effort on fine tuning or correlation?</li>
</ol>
<p>models I've tried:</p>
<p>&quot;ramsrigouthamg/t5_paraphraser&quot;: <a href=""https://huggingface.co/ramsrigouthamg/t5_paraphraser"" rel=""nofollow noreferrer"">https://huggingface.co/ramsrigouthamg/t5_paraphraser</a></p>
<p>&quot;Vamsi/T5_Paraphrase_Paws&quot;: <a href=""https://huggingface.co/Vamsi/T5_Paraphrase_Paws"" rel=""nofollow noreferrer"">https://huggingface.co/Vamsi/T5_Paraphrase_Paws</a></p>
",Training and Model Evaluation,nlg paraphrase result creative enough tried use pre trained model create paraphrase sentence even trying correlate top p top k result paraphrase almost original sentence would like get result look completely different eg third person talking topic point event illustrates topic example three sentence express idea different word looking within best way start solving challenge life people looking solution problem world around always continue look solution first step healing pain lie ability look mirror think possible achieve result effort fine tuning correlation model tried ramsrigouthamg paraphraser vamsi paraphrase paw
How to measure the accuracy of cosine similarity,"<p>I'm doing an NLP project that use cosine similarity between n-documents, and i build 2 model. The first model is using TF-IDF &amp; cosine_sim and the second model is using FastText &amp; cosine_sim.</p>
<p>I want to know which model is more accurate, i've been searching for the accuracy metrics but didn't find the answer that i want.</p>
",Training and Model Evaluation,measure accuracy cosine similarity nlp project use cosine similarity n document build model first model using tf idf cosine sim second model using fasttext cosine sim want know model accurate searching accuracy metric find answer want
How to stop data shuffling while training the HuggingFace BERT model?,"<p>I want to train a BERT transformer model using the <code>HuggingFace</code> implementation/library. During training, <code>HuggingFace</code> shuffles the training data for each epoch, but I don't want to shuffle the data. For example, if I have 5 training data and the batch size = 2, then I want the training data to be presented as [1, 2], [2, 3], [3, 4] and [4, 5]. I cannot find any resources that show how to disable the default shuffling.</p>
",Training and Model Evaluation,stop data shuffling training huggingface bert model want train bert transformer model using implementation library training shuffle training data epoch want shuffle data example training data batch size want training data presented find resource show disable default shuffling
Model is stuck at 0.51-0.52 accuracy and not improving,"<p>This is for a kaggle competition wherein I have essays which I have to grade/predict on 6 parameters (vocabulary, cohesion, conventions, grammar, phraseology, syntax).
I have implemented RandomSearch hypermodel to get better results but the model is peaking at 0.52 accuracy and is giving terrible predictions.</p>
<p><a href=""https://i.sstatic.net/tmh4i.png"" rel=""nofollow noreferrer"">Following is the X and y for the model:  (y=1:7, X=7:</a></p>
<p>Following is the model_builder:</p>
<pre><code>def model_builder(hp):
  model = tf.keras.Sequential()
#   model_input = tf.keras.layers.Input(shape=(6,))
    
  model.add(Dense(6,input_shape=(6,), activation = &quot;relu&quot;))
#   hp_units = random.randrange(32,512,32)

  units1 = random.randrange(32,512,32)  
  model.add(tf.keras.layers.Dense(units=units1, activation='relu'))
  model.add(tf.keras.layers.Dropout(0.3))
  
  units2 = random.randrange(32,512,32)  
  model.add(tf.keras.layers.Dense(units=units2, activation='relu'))
  model.add(tf.keras.layers.Dropout(0.3))

  units3 = random.randrange(32,512,32)  
  model.add(tf.keras.layers.Dense(units=units3, activation='relu'))
  model.add(tf.keras.layers.Dropout(0.4))

#   units4 = random.randrange(32,512,32)  
#   model.add(tf.keras.layers.Dense(units=units4, activation='LeakyReLU'))  
#   model.add(tf.keras.layers.Dropout(0.4))

#   units5 = random.randrange(32,512,32)  
#   model.add(tf.keras.layers.Dense(units=units5, activation='LeakyReLU'))  
#   model.add(tf.keras.layers.Dropout(0.5))

#   units6 = random.randrange(32,512,32)  
#   model.add(tf.keras.layers.Dense(units=units6, activation='LeakyReLU'))  
#   model.add(tf.keras.layers.Dropout(0.4))
    
  units7 = random.randrange(32,512,32)  
  model.add(tf.keras.layers.Dense(units=units7, activation='relu'))  
  model.add(tf.keras.layers.Dropout(0.3))

  units8 = random.randrange(32,512,32)  
  model.add(tf.keras.layers.Dense(units=units8, activation='relu'))  
  model.add(tf.keras.layers.Dropout(0.2))
    
  model.add(tf.keras.layers.Dense(6))

  # Tune the learning rate for the optimizer
  # Choose an optimal value from 0.01, 0.001, or 0.0001
  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
  hp_momentum = hp.Choice('momentum', values = [1e-1, 3e-1, 5e-1, 7e-1, 9e-1])

  model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=hp_learning_rate,
                                                   momentum=hp_momentum,
                                                  ),
                loss=tf.keras.losses.MeanSquaredError(),
                metrics=['accuracy'])

  return model


#Following is the tuner and search code:


tuner = kt.RandomSearch(
        hypermodel=model_builder,
        objective='val_accuracy',
        max_trials=10,
    #     seed=None,
    #     hyperparameters=None,
    #     tune_new_entries=True,
    #     allow_new_entries=True,
    #     **kwargs
    )

early_stopping_monitor = EarlyStopping(monitor='val_accuracy', patience = 10, restore_best_weights=True)

tuner.search(X_train,
             y_train, 
             epochs=80, 
             batch_size=16,
             validation_data=(X_test,y_test), 
             callbacks=[early_stopping_monitor])

# Get the optimal hyperparameters
best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

print(f&quot;&quot;&quot;
The hyperparameter search is complete. The optimal learning rate for the optimizer
is {best_hps.get('learning_rate')}.
&quot;&quot;&quot;)

</code></pre>
<p><a href=""https://i.sstatic.net/froOp.png"" rel=""nofollow noreferrer"">History Plot:</a></p>
<p><a href=""https://i.sstatic.net/HCBxZ.png"" rel=""nofollow noreferrer"">Test vs train accuracy plot:</a></p>
<p>I have tried feature extraction for better correlation but it's not great.
I tried to use different optimizers, activations, etc but none prevailed.</p>
<p>My last shot at this is probably to scale the data to 0-1 and use sigmoid on it.</p>
",Training and Model Evaluation,model stuck accuracy improving kaggle competition wherein essay grade predict parameter vocabulary cohesion convention grammar phraseology syntax implemented randomsearch hypermodel get better result model peaking accuracy giving terrible prediction following x model x following model builder history plot test v train accuracy plot tried feature extraction better correlation great tried use different optimizers activation etc none prevailed last shot probably scale data use sigmoid
Python Regex to match a colon either side (left and right) of a word,"<p>At a complete loss here - trying to match a a colon either side of any given word in a passage of text.</p>
<p>For example:</p>
<pre><code>:wave: Hello guys! :partyface: another huge win for us all to celebrate!
</code></pre>
<p>An appropriate regex that would match:</p>
<pre><code>:wave:
:partyface:
</code></pre>
<p>Really appreciate your help!</p>
<pre><code>\w*:\b
</code></pre>
",Training and Model Evaluation,python regex match colon either side left right word complete loss trying match colon either side given word passage text example appropriate regex would match really appreciate help
Unable to create tensor,"<p>I am trying to train an NLP model for MLM problem, but the trainer.train function is throwing:</p>
<blockquote>
<p>Unable to create tensor, you should probably activate truncation
and/or padding with 'padding=True' 'truncation=True' to have batched
tensors with the same length. Perhaps your features (<code>input_ids</code> in
this case) have excessive nesting (inputs type <code>list</code> where type <code>int</code>
is expected).</p>
</blockquote>
<p>I really don't know what's going on because I followed the hugging-face tutorials.</p>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer,AutoModelForMaskedLM
cp= &quot;tau/tavbert-he&quot;
model=AutoModelForMaskedLM.from_pretrained(cp)
tokenizer=AutoTokenizer.from_pretrained(cp)

import datasets
ds=datasets.load_dataset(&quot;csv&quot;, data_files='/content/drive/Shareddrives/Embible/data.csv')
ds=ds['train'].train_test_split(train_size=0.8, seed=42)


def tokenize_function(dataset):
  return tokenizer(str(dataset[&quot;verse&quot;]),truncation=True,padding=True ,
                       max_length=512, return_overflowing_tokens=True)

tokenized_ds=ds.map(tokenize_function)

from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
tokenized_ds=tokenized_ds.remove_columns(ds[&quot;train&quot;].column_names)

from transformers import TrainingArguments
from transformers import Trainer


training_args = TrainingArguments(&quot;test-trainer&quot;)
trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_ds['train'],
    eval_dataset=tokenized_ds[&quot;test&quot;],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
</code></pre>
",Training and Model Evaluation,unable create tensor trying train nlp model mlm problem trainer train function throwing unable create tensor probably activate truncation padding padding true truncation true batched tensor length perhaps feature case excessive nesting input type type expected really know going followed hugging face tutorial code
How to set up compute_metric function for Donut Transformer using trainer?,"<p><a href=""https://i.sstatic.net/mLR6B.png"" rel=""nofollow noreferrer"">This image is what I got from the trainer.train()</a>I am trying to fine-tune a Donut Transformer using trainer(). And I am wondering what kind of compute_metric function should I use?</p>
<p>Here is what I tried.</p>
<pre><code>def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }
</code></pre>
<p>But it is not returning other metrics accuracy, f1, etc.</p>
",Training and Model Evaluation,set compute metric function donut transformer using trainer image got trainer train trying fine tune donut transformer using trainer wondering kind compute metric function use tried returning metric accuracy f etc
Sequential Keras model is able to predict different size of input?,"<p>I am new to recurrent networks and nlp. I try to create a text generation model. First I padded my sequences in order to prevent shape mismatch during training:</p>
<pre><code>[1505,  422,   63,  324],
[   7,   63,    7,    0],
[ 201,   16,   63,  201],
</code></pre>
<p>My model is:</p>
<pre><code>    model3 = Sequential()
#model3.add(Input(3,))
model3.add(Embedding(1614, 20, input_length=3))
model3.add(LSTM(50,return_sequences=True))
model3.add(LSTM(50))
model3.add(Dense(1614,activation='sigmoid'))
model3.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])

model3ecod.fit(x_cod,y_cod, epochs =20,batch_size=10)
</code></pre>
<p>After training, I try to predict next word of given vector, for example,:</p>
<p>np.argsort(model3.predict( np.array([[63,7,63]])))[0][-1:]</p>
<p>returns,0 as expected;</p>
<p>However I noticed that I can even predict [63,7,63,0,124,.... ]] or even single sequences [[63,]]. The accuracy seems low, but however It looks strange to me and made me suspect if my model is wrong? The LSTM layers should only take fixed size of input isn't it? Then how the model is able to predict different size of input than training data?</p>
<p>Thank you so much.</p>
",Training and Model Evaluation,sequential kera model able predict different size input new recurrent network nlp try create text generation model first padded sequence order prevent shape mismatch training model training try predict next word given vector example np argsort model predict np array return expected however noticed even predict even single sequence accuracy seems low however look strange made suspect model wrong lstm layer take fixed size input model able predict different size input training data thank much
Map BERTopic topic IDs back to the training dataframe,"<p>I have trained a BERTopic model on a dataframe of length of 400k. I want to map the topics of each document in a new column inside the dataframe. I could do that by running a for loop on all the documents and do <code>topic_model.transform(doc)</code> on them. The only problem is, it takes more than a second to transform each document into its topic and it would take days for the whole dataset.</p>
<p>Is there a way to achieve this faster since I want to map the topics on the training data.</p>
<p>I tried:</p>
<pre><code>topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs)
topic_model.reduce_topics(docs, nr_topics=200)

topics = []
for text in df.texts:
    tops = topic_model.transform(text)
    topics.append(tops)
df['topics'] = topics
</code></pre>
",Training and Model Evaluation,map bertopic topic id back training dataframe trained bertopic model dataframe length k want map topic document new column inside dataframe could running loop document problem take second transform document topic would take day whole dataset way achieve faster since want map topic training data tried
How can i know if it is a random word(or string),"<p>I want to know which given string is random.</p>
<p>For example, If there was given words (A)&quot;protectsky&quot; and (B)&quot;ptctpkysui&quot;, A human would recognize (A) as a meaningful string and (B) as a meaningless string.</p>
<p>In this way, is there any way for the computer to differentiate between any given string in the same way as above?</p>
<p>Machine learning, dictionary matching, entropy calculation, etc.. any method or idea is fine.</p>
<p>It is also okay if the result is a probability value or a specific threshold value.</p>
<p>If there is a better way, please let me know.</p>
",Training and Model Evaluation,know random word string want know given string random example wa given word protectsky b ptctpkysui human would recognize meaningful string b meaningless string way way computer differentiate given string way machine learning dictionary matching entropy calculation etc method idea fine also okay result probability value specific threshold value better way please let know
Output multiple possible tags with spaCy spancat,"<p>The problem I'm working on involves span categorisation with spaCy, however some of the tags are ambiguous, e.g. span1 =&gt; 60% tag1, 40% tag2</p>
<p>I'm trying to figure out if there is a way to get Spacy's output to capture this ambiguity.</p>
<p>I have been looking at the model's confidence levels and at the moment Spacy's outputs are pretty much binary, e.g. span1 =&gt; 99% tag1, 1% tag2, which I guess makes sense in terms of its loss function.</p>
<p>Is there a way to modify the loss function so e.g. it only looks at recall rather than precision,  or is there any other way I could approach this?</p>
<p>Thanks!</p>
",Training and Model Evaluation,output multiple possible tag spacy spancat problem working involves span categorisation spacy however tag ambiguous e g span tag tag trying figure way get spacy output capture ambiguity looking model confidence level moment spacy output pretty much binary e g span tag tag guess make sense term loss function way modify loss function e g look recall rather precision way could approach thanks
Fine-tuning SentenceTransformer on text classification task,"<p>Wish to fine-tune SentenceTransformer model with multi-class labeled dataset for text classification.
Tutorials seen so far need a specific format as a training data, such as list of positive triplets such as (senetnce1, sentence2, 1) and list of negative triplets such as (senetnce1, senetnce3, 0).
A typical classification dataset is not like that. Its a list of (senetnce1, class1), (senetnce2, class2), (senetence3, class1), (senetnce4, class3), etc.</p>
<p>Is there any ready logic/code/tutorial which will demonstrate, given a typical classification dataset, generate necessary triplet lists, by permutations and combinations? and then train SentenceTransformer successfully, and hopefully with better accuracy?</p>
",Training and Model Evaluation,fine tuning sentencetransformer text classification task wish fine tune sentencetransformer model multi class labeled dataset text classification tutorial seen far need specific format training data list positive triplet senetnce sentence list negative triplet senetnce senetnce typical classification dataset like list senetnce class senetnce class senetence class senetnce class etc ready logic code tutorial demonstrate given typical classification dataset generate necessary triplet list permutation combination train sentencetransformer successfully hopefully better accuracy
How to predict new data set using trained classifier,"<p>I have trained a model using Gaussian classifier and my model has 63% accuracy. Now I need to use this model for predicting data in a different file. How can I do this?</p>

<p>This is the code I have done.</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
dataset = pd.read_csv('fno.tsv', delimiter = '\t', quoting = 3)
import re
from sklearn.externals import joblib
import phrasemachine as pm
import nltk 
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer 
from nltk.util import ngrams
corpus = []

for j in range(0, 400):
    review = re.sub('[^a-zA-Z]', ' ', dataset['Final Narrative'][j])
    review = review.lower()
    review = review.split()
    ps = PorterStemmer()
    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] 
    review = ' '.join(review)
    corpus.append(review)
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer() 
X = cv.fit_transform(corpus).toarray()
y = dataset.iloc[:, 17].values
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, random_state = 0)
from sklearn.naive_bayes import GaussianNB
from sklearn import metrics
model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

from sklearn.feature_extraction.text import TfidfVectorizer
tf=TfidfVectorizer()
text_tf= tf.fit_transform(dataset['Final Narrative'])

from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    text_tf, dataset['Source of Hazard'], test_size=0.3, random_state=123)
#Accuracy Check
from sklearn.naive_bayes import GaussianNB
from sklearn import metrics

clf = GaussianNB().fit(X_train.toarray(), y_train)
predicted= clf.predict(X_test.toarray())
print(""MultinomialNB Accuracy:"",metrics.accuracy_score(y_test, predicted))
</code></pre>

<p>Now I have another file called data which has only data (X) to predict without Y. How I use the above classifier to predict this new data set?</p>
",Training and Model Evaluation,predict new data set using trained classifier trained model using gaussian classifier model ha accuracy need use model predicting data different file code done another file called data ha data x predict without use classifier predict new data set
Giving less weight to data coming from another dataset that is noisy,"<p>I have two datasets, one with clean data and one with dirty data. I train a Roberta model on the clean dataset and then get predictions for the dirty dataset. Those predictions with a probability greater than 0.9 go to the clean dataset. I then retrain the Roberta model with this new dataset (clean + dirty moving to clean).</p>
<p>For the retraining I am using the MAE loss function (more robust to noisy labels) and I use weights to give less value to the data that passes from the dirty to the clean dataset, as follows:</p>
<pre><code>loss = torch.mean(torch.abs(y_true - y_pred) * weights)
</code></pre>
<p>Initially I am using an arbitrary weight of 0.5 for all the dirty data that gets passed into the clean dataset. However, I would like to assign them a weight in a more academic way, not so arbitrary.</p>
<p>How can I do that?</p>
",Training and Model Evaluation,giving le weight data coming another dataset noisy two datasets one clean data one dirty data train roberta model clean dataset get prediction dirty dataset prediction probability greater go clean dataset retrain roberta model new dataset clean dirty moving clean retraining using mae loss function robust noisy label use weight give le value data pass dirty clean dataset follows initially using arbitrary weight dirty data get passed clean dataset however would like assign weight academic way arbitrary
Why isn&#39;t my Gensim fastText model continuing to train on a new corpus?,"<p>I am trying to continue training a fastText model with Gensim, using my own corpus of text.</p>
<p>I've followed along with the documentation here:
<a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html</a></p>
<p>And I have written the following code:</p>
<p>First, create a small corpus:</p>
<pre><code>corpus = [
    &quot;The brown dog jumps over the kangaroo&quot;,
    &quot;I want to ride my bicycle to Mount Everest&quot;,
    &quot;What a lovely day it is&quot;,
    &quot;When I Wagagamagga, everybody stops to listen&quot;
]

corpus = [sentence.split() for sentence in corpus]
</code></pre>
<p>And then load a testing model:</p>
<pre><code>from gensim.models.fasttext import load_facebook_model
from gensim.test.utils import datapath

model = load_facebook_model(datapath(&quot;crime-and-punishment.bin&quot;))
</code></pre>
<p>Then I do a check to see if the model knows my weird new word in the corpus:</p>
<pre><code>'Wagagamagga' in model.wv.key_to_index
</code></pre>
<p>Which returns False.</p>
<p>Then I try to continue the training:</p>
<pre><code>model.build_vocab(corpus, update=True)
model.train(corpus, total_examples=len(corpus), epochs=model.epochs)
</code></pre>
<p>The model should know about my weird new word now, but this returns False, when I am expecting it to return True:</p>
<pre><code>'Wagagamagga' in model.wv.key_to_index
</code></pre>
<p>What have I missed?</p>
",Training and Model Evaluation,gensim fasttext model continuing train new corpus trying continue training fasttext model gensim using corpus text followed along documentation written following code first create small corpus load testing model check see model know weird new word corpus return false try continue training model know weird new word return false expecting return true missed
Training spaCy - NameError,"<p>I need to train a spaCy model to improve the accuracy to identify products. I'm struggling with training my spacy model. I have the following code:</p>
<pre><code>TRAIN_DATA = [('..., {'entities': [(36,55,'PRODUCT')]})]
nlp = spacy.load(&quot;en_core_web_lg&quot;)
ner = nlp.get_pipe(&quot;ner&quot;)

optimizer = nlp.create_optimizer()
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != &quot;ner&quot;]

with nlp.disable_pipes(*other_pipes): # only train NER
    for itn in range(50):
        random.shuffle(TRAIN_DATA)
        losses = {}
        for text, annotations in TRAIN_DATA:
            doc = nlp.make_doc(text)
            example = Example.from_dict(doc, annotations)
            nlp.update([example], drop=0.25, sgd=optimizer, losses=losses)
</code></pre>
<p>but it's failing due to:</p>
<pre><code>NameError                                 Traceback (most recent call last)
&lt;ipython-input-4-903f2be7114f&gt; in &lt;module&gt;
     15         for text, annotations in TRAIN_DATA:
     16             doc = nlp.make_doc(text)
---&gt; 17             example = Example.from_dict(doc, annotations)
     18             nlp.update([example], drop=0.25, sgd=optimizer, losses=losses)
     19 print(losses)

NameError: name 'Example' is not defined
</code></pre>
<p>How do I need to define <code>Example</code>?</p>
",Training and Model Evaluation,training spacy nameerror need train spacy model improve accuracy identify product struggling training spacy model following code failing due need define
Hugging Face T5 model that is not pre-trained and training it,"<p>I want to use the Hugging Face T5 model to do summarization but I want to train the model with my own dataset.</p>
<p>How can I get the T5 model such that it has not been trained yet? And what steps do I need to take to train it?</p>
<p>Currently I am look at this tutorial:
<a href=""https://huggingface.co/docs/transformers/tasks/summarization"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/tasks/summarization</a></p>
<p>I also found this code which allows me to get a T5 model without weights but how can I train it with my own data?
<a href=""https://stackoverflow.com/questions/73700165/how-to-use-architecture-of-t5-without-pretrained-model-hugging-face"">How to use architecture of T5 without pretrained model (Hugging face)</a></p>
",Training and Model Evaluation,hugging face model pre trained training want use hugging face model summarization want train model dataset get model ha trained yet step need take train currently look tutorial also found code allows get model without weight train data href use architecture without pretrained model hugging face
output dimension for language model,"<p>I am working on language model. In the final layer I am using dense layer with output size equals to vocab size and this layers contribute huge parameters to train.</p>
<p>How can I reduce the size of the final layer in the language model? Does the final layer's size should be equal to vocab size, if yes then will the model not be large if we have large vocab?</p>
",Training and Model Evaluation,output dimension language model working language model final layer using dense layer output size equal vocab size layer contribute huge parameter train reduce size final layer language model doe final layer size equal vocab size yes model large large vocab
How should I approach classifying these text fields into numeric?,"<p>I have data that has a bunch of text entries that look like this: 7-8 business days, 10 days, 10-12 days, 2-3 weeks, 1 year, 8-12 days, 2 weeks, etc.</p>
<p>I want to train a model to convert these text entries to numeric. I assume this is a fairly easy problem for an NLP model to handle, but I am not too confident about which models to use.</p>
<p>I would like to take the larger of the two numbers if a range is specified. For instance, 2-3 weeks should become 21 and 8-10 days should become 10.</p>
<p>I figure I can manually code around 100 records to train the model. Can someone recommend an NLP model to use or even a script that I can edit? If this isn't a good use case for NLP, please also advise.</p>
<p>(I am only practiced at the R programming language, but I can tinker around in Python if needed).</p>
<p>If all else fails is two break the data into separate columns using the hyphen as a delimiter using <code>strsplit(as.character(df$column), &quot;-&quot;)</code>, and then evaluating it using if-statements: for example:</p>
<pre><code>date_conversion = if_else(is.numeric(columnA) &amp; columnB == &quot;weeks&quot;, columnA*7, if_else(is.numeric(columnA) &amp; columnB == &quot;days&quot;, columnA, NA)
</code></pre>
<p>But ideally, I would like to train a model since I have a lot of data.</p>
<p>Some data using dput:</p>
<pre><code>text_fields &lt;- c(&quot;10 - 14 days&quot;, &quot;2-3 Weeks&quot;, &quot;10-12 days&quot;, &quot;8 days&quot;, &quot;8-12 days&quot;, 
                 &quot;10 days&quot;, &quot;7-10 days&quot;, &quot;5 days&quot;, &quot;7-10 days&quot;, &quot;5-7 days&quot;, &quot;10 days&quot;, 
                 &quot;7 days&quot;, &quot;7 - 10 days&quot;, &quot;1 week&quot;, &quot;5-7 days&quot;, &quot;2 weeks&quot;, &quot;2-4 weeks&quot;, 
                 &quot;10 days&quot;, &quot;7-10 days&quot;, &quot;8-10 days&quot;, &quot;1 week&quot;, &quot;10 days&quot;, &quot;8-10 days&quot;, 
                 &quot;2 weeks&quot;, &quot;10-12 days&quot;, &quot;7-10 days&quot;, &quot;2-3 weeks&quot;, &quot;7-10 days&quot;, 
                 &quot;10 days&quot;, &quot;2 weeks&quot;, &quot;8-12 days&quot;, &quot;12 days&quot;, &quot;10 Days&quot;, &quot;7 Days&quot;, 
                 &quot;2 weeks&quot;, &quot;5-8 days&quot;, &quot;8-12 days&quot;, &quot;8-12 days&quot;, &quot;10 days&quot;, &quot;12-14 days&quot;, 
                 &quot;10-12 days&quot;, &quot;7 days&quot;, &quot;5-7 days&quot;, &quot;2 weeks&quot;, &quot;2-3 weeks&quot;, &quot;5-7 days&quot;, 
                 &quot;5-7 business days&quot;, &quot;5-7 days&quot;, &quot;5-7 business days&quot;, &quot;7 days&quot;, 
                 &quot;2-3 weeks&quot;, &quot;7-10 days&quot;, &quot;8-12 Days&quot;, &quot;10 days&quot;, &quot;10 days&quot;, 
                 &quot;10 days&quot;, &quot;10 days&quot;, &quot;10 days&quot;, &quot;14&quot;, &quot;2 weeks&quot;, &quot;10 business days&quot;, 
                 &quot;2-3 weeks&quot;, &quot;4 days&quot;, &quot;1 month&quot;, &quot;7-10 days&quot;, &quot;8-12 days&quot;, &quot;2-3 weeks&quot;, 
                 &quot;3-5 days&quot;, &quot;10 days&quot;, &quot;3-5 days&quot;, &quot;2-3 days&quot;, &quot;2-3 days&quot;, &quot;3-5&quot;, 
                 &quot;5-7 days&quot;, &quot;7-10 days&quot;, &quot;5-7 days&quot;, &quot;8-12 days&quot;, &quot;7-10 days&quot;, 
                 &quot;7-10 days&quot;, &quot;7-10 days&quot;, &quot;2.5 weeks&quot;, &quot;2 Weeks&quot;, &quot;10-12 days&quot;, 
                 &quot;10-12 days&quot;, &quot;7-10 days&quot;, &quot;7-10 days&quot;, &quot;7-10 days&quot;, &quot;7-10 days&quot;, 
                 &quot;7-10 days&quot;, &quot;7-10 days&quot;, &quot;2 weeks&quot;, &quot;1 month&quot;, &quot;1 month&quot;, &quot;1 week&quot;
)
</code></pre>
",Training and Model Evaluation,approach classifying text field numeric data ha bunch text entry look like business day day day week year day week etc want train model convert text entry numeric assume fairly easy problem nlp model handle confident model use would like take larger two number range specified instance week become day become figure manually code around record train model someone recommend nlp model use even script edit good use case nlp please also advise practiced r programming language tinker around python needed else fails two break data separate column using hyphen delimiter using evaluating using statement example ideally would like train model since lot data data using dput
Accuracy is getting worse after text pre processing,"<p>I'm working a multi-class text classification project.</p>
<p>After splitting the dataset into train and test datasets, I've applied the below function on the train dataset (AKA pre processing):</p>
<pre><code>STOPWORDS = set(stopwords.words('english'))

def clean_text(text):   
    # lowercase text
    text = text.lower() 
    
    # delete bad symbols
    text = re.sub(r&quot;(@\[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?&quot;, &quot;&quot;, text)  
  
    # delete stopwords from text
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) 

    # Stemming the words
    text = ' '.join([stemmer.stem(word) for word in text.split()])
    
    return text
</code></pre>
<p>To my surprise, I've got much worst results (i.e. va_accuracy) applying on the train dataset rather than just &quot;do nothing&quot; (59% vs 69%)</p>
<p>I've literally commented out the apply line in the below section:</p>
<pre><code>all_data = dataset.sample(frac=1).reset_index(drop=True)
train_df, valid = train_test_split(all_data, test_size=0.2)

train_df['text'] = train_df['text'].apply(clean_text)

</code></pre>
<p>What am I missing?
How can it be that pre processing steps decreased accuracy?</p>
<p><strong>A bit more info</strong></p>
<p>I forgot to mention I'm using the below to tokenize the text:</p>
<pre><code>X_train = train.iloc[:, :-1]
y_train = train.iloc[:, -1:]
X_test = valid.iloc[:, :-1]
y_test = valid.iloc[:, -1:]

weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), 
                                            y=y_train.values.reshape(-1))
le = LabelEncoder()
le.fit(weights)
class_weights_dict = dict(zip(le.transform(list(le.classes_)), weights))


tokenizer = Tokenizer(num_words=vocab_size, oov_token='&lt;OOV&gt;')
tokenizer.fit_on_texts(X_train['text'])

train_seq = tokenizer.texts_to_sequences(X_train['text'])
train_padded = pad_sequences(train_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)

validation_seq = tokenizer.texts_to_sequences(X_test['text'])
validation_padded = pad_sequences(validation_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)
</code></pre>
<p>Later on I'm fitting all into the model as follows:</p>
<pre><code>model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=train_padded.shape[1]))

model.add(Conv1D(48, len(GROUPS), activation='relu', padding='valid'))
model.add(GlobalMaxPooling1D())
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dropout(0.5))

model.add(Dense(len(GROUPS), activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

epochs = 100
batch_size = 32

history = model.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size,
                    class_weight=class_weights_dict,
                    validation_data=(validation_padded, validation_labels),
                    callbacks=[ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001), 
                               EarlyStopping(monitor='val_loss', mode='min', patience=2, verbose=1),
                               EarlyStopping(monitor='val_accuracy', mode='max', patience=5, verbose=1)])
</code></pre>
",Training and Model Evaluation,accuracy getting worse text pre processing working multi class text classification project splitting dataset train test datasets applied function train dataset aka pre processing surprise got much worst result e va accuracy applying train dataset rather nothing v literally commented apply line section missing pre processing step decreased accuracy bit info forgot mention using tokenize text later fitting model follows
Can I use ml/nlp to determine pattern in the type of usernames are generated?,"<p>I have a dataset which has first and last names along with their respective email ids. Some of the email ids follow a certain pattern such as:</p>
<p>Fn1 = John , Ln1 = Jacobs, eid1= jj@xyz.com</p>
<p>Fn2 = Emily , Ln2 = Queens, eid2= eq@pqr.com</p>
<p>Fn3 = Harry , Ln3 = Smith, eid3= hsm@abc.com</p>
<p>The content after @ has no importance for finding the pattern. I want to find out how many people follow a certain pattern and what is that pattern. Is it possible to do so using nlp and python?</p>
<p>EXTRA: To know what kind of pattern is for a some number of people we can store examples of that pattern along with its count in an excel sheet?!</p>
",Training and Model Evaluation,use ml nlp determine pattern type usernames generated dataset ha first last name along respective email id email id follow certain pattern fn john ln jacob eid jj xyz com fn emily ln queen eid eq pqr com fn harry ln smith eid hsm abc com content ha importance finding pattern want find many people follow certain pattern pattern possible using nlp python extra know kind pattern number people store example pattern along count excel sheet
How to use CTC Loss Seq2Seq correctly?,"<p>I am trying to create ASR model by myself and learn how to use CTC loss.</p>
<p>I test and I see this:</p>
<pre class=""lang-py prettyprint-override""><code>ctc_loss = nn.CTCLoss(blank=95)
</code></pre>
<pre><code>output: tensor([[63,  8,  1, 38, 29, 14, 41, 71, 14, 29, 45, 41, 3]]): torch.Size([1, 13]); output_size: tensor([13])

input1: torch.Size([167, 1, 96]); input1_size: tensor([167])
</code></pre>
<p>After applying the <code>argmax</code> on this input (= prediction of phonems)</p>
<pre class=""lang-py prettyprint-override""><code>torch.argmax(input1, dim=2)
</code></pre>
<p>I get a series of symbols:</p>
<pre><code>tensor([[63, 63, 63, 63, 63, 63, 95, 95, 63, 63, 95, 95,  8,  8,  8, 95,  8, 95,
           8,  8, 95, 95, 95,  1,  1, 95,  1, 95,  1,  1, 95, 95, 38, 95, 95, 38,
          38, 38, 38, 38, 29, 29, 29, 29, 29, 29, 29, 95, 29, 29, 95, 95, 95, 95,
          95, 95, 95, 95, 95, 95, 14, 95, 14, 95, 95, 95, 95, 14, 95, 14, 41, 41,
          41, 95, 41, 41, 41, 41, 41, 41, 71, 71, 71, 95, 71, 71, 71, 71, 71, 95,
          95, 14, 14, 95, 14, 14, 95, 14, 14, 95, 29, 29, 95, 29, 29, 29, 29, 29,
          29, 29, 45, 95, 95, 45, 45, 95, 45, 45, 45, 45, 41, 95, 41, 41, 95, 95,
          95, 41, 41, 41,  3,  3,  3,  3,  3, 95,  3,  3,  3, 95, 95, 95, 95, 95,
          95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,
          95, 95, 95, 95, 95]])
</code></pre>
<p>and the following loss values.</p>
<pre class=""lang-py prettyprint-override""><code>ctc_loss(input1, output, input_size, output_size)
# Returns 222.8446
</code></pre>
<p>With a different input:</p>
<pre><code>input2: torch.Size([167, 1, 96]) input2_size: tensor([167])
</code></pre>
<pre class=""lang-py prettyprint-override""><code>torch.argmax(input2, dim=2)
</code></pre>
<p>the prediction is just a sequence of blank symbols.</p>
<pre><code>tensor([[95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,
          95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,
          95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,
          95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,
          95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,
          95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,
          95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,
          95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,
          95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,
          95, 95, 95, 95, 95]]) 
</code></pre>
<p>However, the loss value with the same desired output is much lower.</p>
<pre class=""lang-py prettyprint-override""><code>ctc_loss(input2, output, input_size, output_size)
# Returns 3.7955
</code></pre>
<p>I don't know why <code>input1</code> is better than <code>input2</code> but the loss of <code>input1</code> is higher than <code>input2</code>? Can someone explain that?</p>
",Training and Model Evaluation,use ctc loss seq seq correctly trying create asr model learn use ctc loss test see applying input prediction phonems get series symbol following loss value different input prediction sequence blank symbol however loss value desired output much lower know better loss higher someone explain
Inaccurate similarities results by doc2vec using gensim library,"<p>I am working with Gensim library to train some data files using doc2vec,  while trying to test the similarity of one of the files using the method <code>model.docvecs.most_similar(&quot;file&quot;)</code> , I always get all the results above 91% with almost no difference between them (which is not logic), because the files do not have similarities between them. so the results are inaccurate.<br/><br/>
<strong>Here is the code for training the model</strong><br/></p>
<pre><code>model = gensim.models.Doc2Vec(vector_size=300, min_count=0, alpha=0.025, min_alpha=0.00025,dm=1)
model.build_vocab(it)
for epoch in range(100):
    model.train(it,epochs=model.iter, total_examples=model.corpus_count)
    model.alpha -= 0.0002
    model.min_alpha = model.alpha
model.save('doc2vecs.model')
model_d2v = gensim.models.doc2vec.Doc2Vec.load('doc2vecs.model')
sim = model_d2v.docvecs.most_similar('file1.txt')
print sim
</code></pre>
<br/> 
**this is the output result**
<blockquote>
<p>
[('file2.txt', 0.9279470443725586), ('file6.txt', 0.9258157014846802), ('file3.txt', 0.92499840259552), ('file5.txt', 0.9209873676300049), ('file4.txt', 0.9180108308792114), ('file7.txt', 0.9141069650650024)]
</p>
</blockquote>
<p>what am I doing wrong ? how could I improve the accuracy of results ?</p>
",Training and Model Evaluation,inaccurate similarity result doc vec using gensim library working gensim library train data file using doc vec trying test similarity one file using method always get result almost difference logic file similarity result inaccurate code training model output result file txt file txt file txt file txt file txt file txt wrong could improve accuracy result
Can gensim Doc2Vec be used to compare a novel document to a trained model?,"<p>I have a set of documents that all fit a pre-defined category and have successfully trained a model off of those documents.</p>

<p>The question is, if I have a novel document, how can I calculate how closely this new document lines up with my trained model?</p>

<p>My current solution:</p>

<pre><code>novel_vector = model.infer_vector(novel_doc_words, steps = 20)
similarity_scores = model.docvecs.most_similar([novel_vector])
average = 0
for score in similarity_scores:
  average += score[1]
overall_similarity = average/len(similarity_scores)
</code></pre>

<p>I was unable to find any convenience methods in the documentation</p>
",Training and Model Evaluation,gensim doc vec used compare novel document trained model set document fit pre defined category successfully trained model document question novel document calculate closely new document line trained model current solution wa unable find convenience method documentation
Better way to perform NLP sentence topic classification?,"<p>For one of my projects I have been classifying sentences according to predefined topics/labels.
This sentences are provided through long texts with different paragraphs.</p>
<p><strong>Current Status</strong>
For this task I have perform as follows</p>
<ol>
<li>Training:</li>
</ol>
<ul>
<li>I label thousands of sentences related with the task</li>
<li>I balance the training dataset</li>
<li>I finetuned Bert (tried deberta but for me is not performing better and seems to be slower)</li>
</ul>
<ol start=""2"">
<li>Use:</li>
</ol>
<ul>
<li>I segment the text into sentences</li>
<li>I apply the NLP finetuned model (Bert)</li>
</ul>
<p><strong>Question</strong></p>
<p>The accuracy is good, but since I segment the text, some times there are sentences that have no topic out of the full context. Nevertheless, if you read the text you know that they are talking about topic X. After doing some research I couldn't come with a direct solution, therefore my question would be:</p>
<ul>
<li><p>There is some better way to do?
For example, can I train some model that reads the full text and then provides me the topic for each part of the text (as a mask for example). Then I can take the parts that talk about each topic and separate them in sentences.</p>
</li>
<li><p>Would this incur a decrease in the general performance of the topic labelling?</p>
</li>
</ul>
<p>Thank you very much for your time and support.</p>
",Training and Model Evaluation,better way perform nlp sentence topic classification one project classifying sentence according predefined topic label sentence provided long text different paragraph current status task perform follows training label thousand sentence related task balance training dataset finetuned bert tried deberta performing better seems slower use segment text sentence apply nlp finetuned model bert question accuracy good since segment text time sentence topic full context nevertheless read text know talking topic x research come direct solution therefore question would better way example train model read full text provides topic part text mask example take part talk topic separate sentence would incur decrease general performance topic labelling thank much time support
how to evaluate an AutoModelForQuestionAnswering?,"<p>I'm using this AutoModelForQuestionAnswering from the transformers for semantic search. Therefore i could not find a way to evaluate it knowing that i'm do not have a predicted values and i don't have a train and test data.Here's the code.</p>
<pre><code> def load_model():
    model_name = &quot;camembert-base&quot;
    tokenizer = CamembertTokenizer.from_pretrained(model_name)
   model = AutoModelForQuestionAnswering.from_pretrained(model_name)
    return model, tokenizer

    model,tokenizer= load_model()
    #input_ids are the indices corresponding to each token in the sentence.
    #input_ids = tokenizer.encode(search, text)
    encoded_output = tokenizer.encode_plus(text=search, text_pair=text, padding=True)
    input_ids = torch.tensor(encoded_output['input_ids'])
    attention_mask = torch.tensor(encoded_output['attention_mask'])
    token_type_ids = torch.tensor(0)
    tokens = tokenizer.convert_ids_to_tokens(input_ids)

    # Batch size of 1
    input_ids = input_ids.unsqueeze(0)
    token_type_ids = token_type_ids.unsqueeze(0)
    attention_mask = attention_mask.unsqueeze(0)

    print(&quot;this is the encode output&quot;, encoded_output)
    print(&quot;this is token_types_ids&quot;,token_type_ids)
    print(&quot;this is attention mask&quot;,attention_mask)
    print(&quot;this is input_ids&quot;,input_ids)
    print(&quot;this is tokens&quot;, tokens)

    output = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)

    print( model.eval() )

    # 3. GET THE ANSWER SPAN
    # once we have the most likely start and end tokens, we grab all the tokens between them
    # and convert tokens back to words!

    # tokens with highest start and end scores
    answer_start = torch.argmax(output.start_logits)  # get the most likely beginning of answer with the argmax of the score
    answer_end = torch.argmax(output.end_logits)
    if answer_end &gt;= answer_start:
        answer = &quot; &quot;.join(tokens[answer_start:answer_end + 1])
    else:
        answer = &quot;No Answer&quot;
    return answer
</code></pre>
<p>'''</p>
",Training and Model Evaluation,evaluate automodelforquestionanswering using automodelforquestionanswering transformer semantic search therefore could find way evaluate knowing predicted value train test data code
Bigrams Modelling: Text Length,"<ul>
<li><p>I find myself in the process of extracting bigrams (sequences of two characters) from text data to perform an analysis of frequencies and develop a model</p>
</li>
<li><p>Now, the frequencies and related quintiles are of course affected by the text length, which can vary both in the train set and in the future score sets</p>
</li>
<li><p>I searched some proper approach to follow to avoid the variance in text length to affect the model performances.</p>
</li>
</ul>
<p>It seems that one approach is the following:</p>
<p>“ Recall that for batching, we need to have all the sequences in a given batch be of uniform length ( N of words).</p>
<p>To do that, we either:</p>
<p>•    (1) pad the sequences that are shorter than a given length or</p>
<p>•    (2) truncate the sequences that are bigger than the given length.</p>
<p>•   The question is how do we decide this length? We have several options:</p>
<p>We decide on a global maximum sequence length based on the sequence length characteristics of the training data. “</p>
<p>Meaning that:</p>
<ol>
<li>It is identified in the training set, within the distribution of N of words composing a text,  the maximum value or 75% quantile , and use it as upper limit</li>
<li>Based on this value, all the texts with greater length are truncated</li>
</ol>
<p>The approach seems sound to me in avoiding that text length will affect the bigrams distributions and model performance on the score set</p>
<p>Still, I would know if someone has managed similar tasks before and if any relevant material can be suggested</p>
",Training and Model Evaluation,bigram modelling text length find process extracting bigram sequence two character text data perform analysis frequency develop model frequency related quintiles course affected text length vary train set future score set searched proper approach follow avoid variance text length affect model performance seems one approach following recall batching need sequence given batch uniform length n word either pad sequence shorter given length truncate sequence bigger given length question decide length several option decide global maximum sequence length based sequence length characteristic training data meaning identified training set within distribution n word composing text maximum value quantile use upper limit based value text greater length truncated approach seems sound avoiding text length affect bigram distribution model performance score set still would know someone ha managed similar task relevant material suggested
Keras Model fit throws shape mismatch error,"<p>I am building a Siamese network using Keras(TensorFlow) where the target is a binary column, i.e., match or mismatch(1 or 0). But the model fit method throws an error saying that the <code>y_pred is not compatible with the y_true shape</code>. I am using the <code>binary_crossentropy</code> loss function.</p>
<p>Here is the error I see:</p>
<p><a href=""https://i.sstatic.net/xMPD6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xMPD6.png"" alt=""enter image description here"" /></a></p>
<p>Here is the code I am using:</p>
<pre><code>model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[tf.keras.metrics.Recall()])
history = model.fit([X_train_entity_1.todense(),X_train_entity_2.todense()],np.array(y_train),
                    epochs=2, 
                    batch_size=32,
                    verbose=2,
                    shuffle=True)
</code></pre>
<p>My Input data shapes are as follows:</p>
<pre><code>Inputs:
X_train_entity_1.shape is (700,2822)
X_train_entity_2.shape is (700,2822)

Target:
y_train.shape is (700,1)
</code></pre>
<p>In the error it throws, <code>y_pred</code> is the variable which was created internally. What is <code>y_pred</code> dimension is 2822 when I am having a binary target. And 2822 dimension actually matches the input size, but how do I understand this?</p>
<p>Here is the model I created:</p>
<pre><code>in_layers = []
out_layers = []
for i in range(2):
  input_layer = Input(shape=(1,))
  embedding_layer = Embedding(embed_input_size+1, embed_output_size)(input_layer)
  lstm_layer_1 = Bidirectional(LSTM(1024, return_sequences=True,recurrent_dropout=0.2, dropout=0.2))(embedding_layer)
  lstm_layer_2 = Bidirectional(LSTM(512, return_sequences=True,recurrent_dropout=0.2, dropout=0.2))(lstm_layer_1)

  in_layers.append(input_layer)
  out_layers.append(lstm_layer_2)

merge = concatenate(out_layers)
dense1 = Dense(256, activation='relu', kernel_initializer='he_normal', name='data_embed')(merge)
drp1 = Dropout(0.4)(dense1)
btch_norm1 = BatchNormalization()(drp1)
dense2 = Dense(32, activation='relu', kernel_initializer='he_normal')(btch_norm1)
drp2 = Dropout(0.4)(dense2)
btch_norm2 = BatchNormalization()(drp2)
output = Dense(1, activation='sigmoid')(btch_norm2)
model = Model(inputs=in_layers, outputs=output)
model.summary()
</code></pre>
<p>Since my data is very sparse, I used <strong>todense</strong>. And there the type is as follows:</p>
<pre><code>type(X_train_entity_1) is scipy.sparse.csr.csr_matrix
type(X_train_entity_1.todense()) is numpy.matrix
type(X_train_entity_2) is scipy.sparse.csr.csr_matrix
type(X_train_entity_2.todense()) is numpy.matrix
</code></pre>
<p>Summary of last few layers as follows:
<a href=""https://i.sstatic.net/lGr2q.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lGr2q.png"" alt=""enter image description here"" /></a></p>
",Training and Model Evaluation,kera model fit throw shape mismatch error building siamese network using kera tensorflow target binary column e match mismatch model fit method throw error saying using loss function error see code using input data shape follows error throw variable wa created internally dimension binary target dimension actually match input size understand model created since data sparse used todense type follows summary last layer follows
Transformers training vs fine-tuning for a specific task,"<p>I am looking at the below tensorflow transformers implementation.</p>
<p><a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a></p>
<p>I am not sure I understood correctly. When initialising a transformers model it need to be trained on a lot of raw text in an unsupervised way so that it learns the language and then you can fit it to a particular task.</p>
<p>In this example, I am not sure if the training data is used to train the transformers model itself? It look like there is only one &quot;fitting&quot; procedure. Is this correct?</p>
",Training and Model Evaluation,transformer training v fine tuning specific task looking tensorflow transformer implementation sure understood correctly initialising transformer model need trained lot raw text unsupervised way learns language fit particular task example sure training data used train transformer model look like one fitting procedure correct
"ValueError: Shapes (426530, 2) and (1930, 2) are incompatible. Shape of one element of the dataset used for prediction is (221,)","<p>I am working on the DistillBert project for binary classification. I am trying to run the following code using the Spam SMS data set (You can also use the IMDB dataset, it is also giving the same issue), I am trying to find out the recall, precision, and AUC score. However, I am getting a Value error. The error is occurring at the end of the code given below i.e. after this code block.</p>
<pre><code>m = tf.keras.metrics.Recall()
m.update_state(y_test_encoded, rounded_predictions)
m.result().numpy()
</code></pre>
<p>The error can be generated using the code below and the SMS spam data split into train and test set which can be found here - <a href=""https://drive.google.com/drive/folders/1De_2FL2HSWcrXQFcnTnkB0xIMTml1rmE?usp=sharing"" rel=""nofollow noreferrer"">SMS spam dataset</a></p>
<p>Here I am using the <code>BinaryCrossentropy</code> loss function and <code>Adam</code> optimizer.</p>
<p>Dataset: The dataset used here is the spam SMS dataset which has binary labels 0 for standard SMS and 1 for spam SMS. The same error can be reproduced using the IMDB data set for this code.</p>
<p>Error traceback:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/scratch/local/46806264/ipykernel_20971/1328487267.py in &lt;module&gt;
      1 m = tf.keras.metrics.Recall()
----&gt; 2 m.update_state(y_test_encoded, y_pred)
      3 m.result().numpy()

/apps/tensorflow/2.4.1.cuda11/lib/python3.8/site-packages/tensorflow/python/keras/utils/metrics_utils.py in decorated(metric_obj, *args, **kwargs)
     88 
     89     with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):
---&gt; 90       update_op = update_state_fn(*args, **kwargs)
     91     if update_op is not None:  # update_op will be None in eager execution.
     92       metric_obj.add_update(update_op)

/apps/tensorflow/2.4.1.cuda11/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py in update_state_fn(*args, **kwargs)
    175         control_status = ag_ctx.control_status_ctx()
    176         ag_update_state = autograph.tf_convert(obj_update_state, control_status)
--&gt; 177         return ag_update_state(*args, **kwargs)
    178     else:
    179       if isinstance(obj.update_state, def_function.Function):

/apps/tensorflow/2.4.1.cuda11/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    665       try:
    666         with conversion_ctx:
--&gt; 667           return converted_call(f, args, kwargs, options=options)
    668       except Exception as e:  # pylint:disable=broad-except
    669         if hasattr(e, 'ag_error_metadata'):

/apps/tensorflow/2.4.1.cuda11/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, args, kwargs, caller_fn_scope, options)
    348   if conversion.is_in_allowlist_cache(f, options):
    349     logging.log(2, 'Allowlisted %s: from cache', f)
--&gt; 350     return _call_unconverted(f, args, kwargs, options, False)
    351 
    352   if ag_ctx.control_status_ctx().status == ag_ctx.Status.DISABLED:

/apps/tensorflow/2.4.1.cuda11/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs, options, update_cache)
    476 
    477   if kwargs is not None:
--&gt; 478     return f(*args, **kwargs)
    479   return f(*args)
    480 

/apps/tensorflow/2.4.1.cuda11/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py in update_state(self, y_true, y_pred, sample_weight)
   1404       Update op.
   1405     &quot;&quot;&quot;
-&gt; 1406     return metrics_utils.update_confusion_matrix_variables(
   1407         {
   1408             metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,

/apps/tensorflow/2.4.1.cuda11/lib/python3.8/site-packages/tensorflow/python/keras/utils/metrics_utils.py in update_confusion_matrix_variables(variables_to_update, y_true, y_pred, thresholds, top_k, class_id, sample_weight, multi_label, label_weights)
    352           losses_utils.squeeze_or_expand_dimensions(
    353               y_pred, y_true, sample_weight=sample_weight))
--&gt; 354   y_pred.shape.assert_is_compatible_with(y_true.shape)
    355 
    356   if top_k is not None:

/apps/tensorflow/2.4.1.cuda11/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py in assert_is_compatible_with(self, other)
   1132     &quot;&quot;&quot;
   1133     if not self.is_compatible_with(other):
-&gt; 1134       raise ValueError(&quot;Shapes %s and %s are incompatible&quot; % (self, other))
   1135 
   1136   def most_specific_compatible_shape(self, other):

ValueError: Shapes (426530, 2) and (1930, 2) are incompatible
</code></pre>
<p>The shape of the ground truth label is (1930,2) and that of the predicted label is (426530,2). This is because the shape of the <code>input_ids</code> and <code>attention_mask</code> of one element in the <code>test_dataset</code> is (221,). 1930x221=426530. I have given the values and shapes of the <code>test_dataset</code> below. I am using <code>test_dataset</code> for prediction. I don't understand how is this happening. How is the model making predictions for all the elements in the tokenized dataset? How can I fix this to get my model's matrix?</p>
<p>Value of test_dataset:<code>test_dataset</code></p>
<p><code>&lt;TensorSliceDataset shapes: ({input_ids: (221,), attention_mask: (221,)}, (2,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.float32)&gt;</code></p>
<p>1st element of test_dataset: On running command-</p>
<pre><code>for x, y in test_dataset.take(1):
    print(x)
</code></pre>
<p>output-</p>
<pre><code>array([  101,  2272,  2000, 14163,  1010,  2057,  1005,  2128, 22210,
        2041,  2256, 27290,  3663,   102,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0], dtype=int32)&gt;, 'attention_mask': &lt;tf.Tensor: shape=(221,), dtype=int32, numpy=
array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0], dtype=int32)&gt;}
</code></pre>
<p>Code:</p>
<pre><code>import pandas as pd
import tensorflow as tf
import transformers
from transformers import DistilBertTokenizer
from transformers import TFAutoModelForSequenceClassification
pd.set_option('display.max_colwidth', None)
MODEL_NAME = 'distilbert-base-uncased'
BATCH_SIZE = 8
N_EPOCHS = 3

train = pd.read_csv(&quot;train_set.csv&quot;, error_bad_lines=False)
test = pd.read_csv(&quot;test_set.csv&quot;, error_bad_lines=False)

X_train = train.text
X_test = test.text
y_train = train.label
y_test = test.label

#One-hot encoding of labels
y_train_encoded = tf.one_hot(y_train.values, 2)
y_test_encoded = tf.one_hot(y_test.values, 2)

tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)

train_encodings = tokenizer(list(X_train.values),
                        truncation=True, 
                        padding=True)
test_encodings = tokenizer(list(X_test.values),
                       truncation=True, 
                       padding=True)

train_dataset = 
tf.data.Dataset.from_tensor_slices((dict(train_encodings),list(y_train_encoded)))

test_dataset = 
tf.data.Dataset.from_tensor_slices((dict(test_encodings),list(y_test_encoded)))
test_dataset2 = test_dataset.shuffle(buffer_size=1024).take(1000).batch(16)

model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

optimizerr = tf.keras.optimizers.Adam(learning_rate=5e-5)

losss = tf.keras.losses.BinaryCrossentropy((from_logits=True)

model.compile(optimizer=optimizerr,
          loss=losss,
          metrics=['accuracy'])

print(&quot;Evaluate Base model on test data&quot;)
results = model.evaluate(test_dataset2)
print(&quot;test loss, test acc:&quot;, results)

model.fit(train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE),
      epochs=N_EPOCHS,
      batch_size=BATCH_SIZE)

predictions = model.predict(test_dataset)


y_pred = tf.round(tf.nn.sigmoid(predictions.logits))

m = tf.keras.metrics.Recall()
m.update_state(y_test_encoded, rounded_predictions)
m.result().numpy()
#the above-mentioned error is occurring here. 
</code></pre>
<p>How can I fix the error and get the recall, precision and AUC scores?</p>
",Training and Model Evaluation,valueerror shape incompatible shape one element dataset used prediction working distillbert project binary classification trying run following code using spam sm data set also use imdb dataset also giving issue trying find recall precision auc score however getting value error error occurring end code given e code block error generated using code sm spam data split train test set found sm spam dataset using loss function optimizer dataset dataset used spam sm dataset ha binary label standard sm spam sm error reproduced using imdb data set code error traceback shape ground truth label predicted label shape one element x given value shape using prediction understand happening model making prediction element tokenized dataset fix get model matrix value test dataset st element test dataset running command output code fix error get recall precision auc score
Config for pytesseract (Urdu language),"<p>I am having some problems with pytesseract. With this line of code pytesseract works poorly with Urdu language:</p>
<p><code>text = pytesseract.image_to_string(img, lang=&quot;urd&quot;)</code></p>
<p>What configuration should I use to improve the accuracy for Urdu language? And what kind of pre-processing can I do on the image?</p>
<p>I am using this kind of image: <a href=""https://i.sstatic.net/FEsbZ.png"" rel=""nofollow noreferrer"">TestFile</a></p>
<p>For the image attached the output should be:</p>
<p>بعد نجی ٹی وی سے گفتگو کرتے ہوئے وزیر خارجہ شاہ محمود قریشی نے بتایا کہ ملاقات</p>
<p>But the output I am getting is:</p>
<p>٦ری‏ وی سے کلوکرتے ہونے وز خارمہ اہ مود رٹ نے نال لات</p>
<p>Images are in these fonts: Pak Nastaleeq, Alvi Nastaleeq, Jameel Noori Nastaleeq, Nafees Nastaleeq.</p>
",Training and Model Evaluation,config pytesseract urdu language problem pytesseract line code pytesseract work poorly urdu language configuration use improve accuracy urdu language kind pre processing image using kind image testfile image attached output output getting image font pak nastaleeq alvi nastaleeq jameel noori nastaleeq nafees nastaleeq
"Why am I getting the prediction value like [[ 8.45632 , -8.409305], [-8.977011, 8.996431],...] for a binary classification in Tersorflow?","<p>I am working on the DistillBert project for binary classification. I am trying to run the following code using the Spam SMS data set (You can also use the IMDB dataset, it is also giving the same issue), I am trying to find out the recall, precision, and AUC score. However, I am getting an Invalid Argument error.</p>
<p>Also, the prediction is like</p>
<pre><code>TFSequenceClassifierOutput(loss=None, logits=array([[ 8.45632 , -8.409305],
       [-8.977011,  8.996431],
       [-8.982615,  9.00266 ],
       ...,
       [ 8.369539, -8.322055],
       [-8.935638,  8.940543],
       [ 8.457567, -8.396695]], dtype=float32), hidden_states=None, attentions=None) 
</code></pre>
<p>Why is there a negative value here when it is the probability of the class? The original labels are 0 and 1.</p>
<p>Here I am using the <code>BinaryCrossentropy</code> loss function and <code>Adam</code> optimizer.</p>
<p>Dataset: The dataset used here is the spam SMS dataset which has binary labels 0 for normal SMS and 1 for spam SMS. The same error can be reproduced using the IMDB data set for this code.</p>
<p>Error:</p>
<pre><code>InvalidArgumentError: predictions must be &gt;= 0
Condition x &gt;= y did not hold.
First 3 elements of x:
[-9.  9. -9.]
First 1 elements of y:
[0.]
</code></pre>
<p>Code:</p>
<pre><code>import pandas as pd
import tensorflow as tf
import transformers
from transformers import DistilBertTokenizer
from transformers import TFAutoModelForSequenceClassification
pd.set_option('display.max_colwidth', None)
MODEL_NAME = 'distilbert-base-uncased'
BATCH_SIZE = 8
N_EPOCHS = 3

train = pd.read_csv(&quot;train_set.csv&quot;, error_bad_lines=False)
test = pd.read_csv(&quot;test_set.csv&quot;, error_bad_lines=False)

X_train = train.text
X_test = test.text
y_train = train.label
y_test = test.label

#One-hot encoding of labels
y_train_encoded = tf.one_hot(y_train.values, 2)
y_test_encoded = tf.one_hot(y_test.values, 2)

tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)

train_encodings = tokenizer(list(X_train.values),
                        truncation=True, 
                        padding=True)
test_encodings = tokenizer(list(X_test.values),
                       truncation=True, 
                       padding=True)

train_dataset = 
tf.data.Dataset.from_tensor_slices((dict(train_encodings),list(y_train_encoded)))

test_dataset = 
tf.data.Dataset.from_tensor_slices((dict(test_encodings),list(y_test_encoded)))
test_dataset2 = test_dataset.shuffle(buffer_size=1024).take(1000).batch(16)

model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

optimizerr = tf.keras.optimizers.Adam(learning_rate=5e-5)

losss = tf.keras.losses.BinaryCrossentropy((from_logits=True)

model.compile(optimizer=optimizerr,
          loss=losss,
          metrics=['accuracy'])

print(&quot;Evaluate Base model on test data&quot;)
results = model.evaluate(test_dataset2)
print(&quot;test loss, test acc:&quot;, results)

model.fit(train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE),
      epochs=N_EPOCHS,
      batch_size=BATCH_SIZE)

predictions = model.predict(test_dataset2)
# predictions = TFSequenceClassifierOutput(loss=None, logits=array([[-8.96754 ,  8.966875],....[-8.970767,  8.975923]], dtype=float32), hidden_states=None, attentions=None)

import numpy as np
rounded_predictions = np.rint(predictions.logits)
# rounded_predictions = array([[-9.,  9.],....[-9.,  9.]], dtype=float32)

y_test_encoded
#y_test_encoded = &lt;tf.Tensor: shape=(1930, 2), dtype=float32, numpy=array([[1., 0.],....[0., 1.]], dtype=float32)&gt;

m = tf.keras.metrics.Recall()
m.update_state(y_test_encoded, rounded_predictions)
m.result().numpy()
# This is where I getting the above mentioned error.
</code></pre>
<p>How can I fix the error and get the recall, precision and AUC scores? And also can anyone please explain to me why the prediction is like that?</p>
",Training and Model Evaluation,getting prediction value like binary classification tersorflow working distillbert project binary classification trying run following code using spam sm data set also use imdb dataset also giving issue trying find recall precision auc score however getting invalid argument error also prediction like negative value probability class original label using loss function optimizer dataset dataset used spam sm dataset ha binary label normal sm spam sm error reproduced using imdb data set code error code fix error get recall precision auc score also anyone please explain prediction like
Getting Invalid argument error for Binary classification,"<p>I am working on the DistillBert project for binary classification. I am trying to run the following code using the Spam SMS data set (You can also use the IMDB dataset, it is also giving the same issue), I am trying to find out the recall, precision, and AUC score. However, I am getting an Invalid Argument error.</p>
<p>Also, the prediction is like <code>[-9.,  9.] </code>. Why is there a negative value here when it is the probability of the class?</p>
<p>Here I am using the <code>BinaryCrossentropy</code> loss function and <code>Adam</code> optimizer.</p>
<p>Dataset: The dataset used here is the spam SMS dataset which has binary labels 0 for normal SMS and 1 for spam SMS. The same error can be reproduced using the IMDB data set for this code.</p>
<p>Error:</p>
<pre><code>InvalidArgumentError: predictions must be &gt;= 0
Condition x &gt;= y did not hold.
First 3 elements of x:
[-9.  9. -9.]
First 1 elements of y:
[0.]
</code></pre>
<p>Code:</p>
<pre><code>import pandas as pd
import tensorflow as tf
import transformers
from transformers import DistilBertTokenizer
from transformers import TFAutoModelForSequenceClassification
pd.set_option('display.max_colwidth', None)
MODEL_NAME = 'distilbert-base-uncased'
BATCH_SIZE = 8
N_EPOCHS = 3

train = pd.read_csv(&quot;train_set.csv&quot;, error_bad_lines=False)
test = pd.read_csv(&quot;test_set.csv&quot;, error_bad_lines=False)

X_train = train.text
X_test = test.text
y_train = train.label
y_test = test.label

#One-hot encoding of labels
y_train_encoded = tf.one_hot(y_train.values, 2)
y_test_encoded = tf.one_hot(y_test.values, 2)

tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)

train_encodings = tokenizer(list(X_train.values),
                        truncation=True, 
                        padding=True)
test_encodings = tokenizer(list(X_test.values),
                       truncation=True, 
                       padding=True)

train_dataset = 
tf.data.Dataset.from_tensor_slices((dict(train_encodings),list(y_train_encoded)))

test_dataset = 
tf.data.Dataset.from_tensor_slices((dict(test_encodings),list(y_test_encoded)))
test_dataset2 = test_dataset.shuffle(buffer_size=1024).take(1000).batch(16)

model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

optimizerr = tf.keras.optimizers.Adam(learning_rate=5e-5)

losss = tf.keras.losses.BinaryCrossentropy((from_logits=True)

model.compile(optimizer=optimizerr,
          loss=losss,
          metrics=['accuracy'])

print(&quot;Evaluate Base model on test data&quot;)
results = model.evaluate(test_dataset2)
print(&quot;test loss, test acc:&quot;, results)

model.fit(train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE),
      epochs=N_EPOCHS,
      batch_size=BATCH_SIZE)

predictions = model.predict(test_dataset2)
# predictions = TFSequenceClassifierOutput(loss=None, logits=array([[-8.96754 ,  8.966875],....[-8.970767,  8.975923]], dtype=float32), hidden_states=None, attentions=None)

import numpy as np
rounded_predictions = np.rint(predictions.logits)
# rounded_predictions = array([[-9.,  9.],....[-9.,  9.]], dtype=float32)

y_test_encoded
#y_test_encoded = &lt;tf.Tensor: shape=(1930, 2), dtype=float32, numpy=array([[1., 0.],....[0., 1.]], dtype=float32)&gt;

m = tf.keras.metrics.Recall()
m.update_state(y_test_encoded, rounded_predictions)
m.result().numpy()
# This is where I getting the above mentioned error.
</code></pre>
<p>How can I fix the error and get the recall, precision and AUC scores?</p>
",Training and Model Evaluation,getting invalid argument error binary classification working distillbert project binary classification trying run following code using spam sm data set also use imdb dataset also giving issue trying find recall precision auc score however getting invalid argument error also prediction like negative value probability class using loss function optimizer dataset dataset used spam sm dataset ha binary label normal sm spam sm error reproduced using imdb data set code error code fix error get recall precision auc score
How to test masked language model after training it?,"<p>I have followed this tutorial for masked language modelling from Hugging Face using BERT, but I am unsure how to actually deploy the model.</p>
<p>Tutorial: <a href=""https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb"" rel=""noreferrer"">https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb</a></p>
<p>I have trained the model using my own dataset, which has worked fine, but I don't know how to actually use the model, as the notebook does not include an example on how to do this, sadly.</p>
<p><a href=""https://i.sstatic.net/dLyex.png"" rel=""noreferrer"">Example of what I want to do with my trained model</a></p>
<p>On the Hugging Face website, this is the code used in the example; hence, I want to do this exact thing but with my model:</p>
<pre><code>&gt;&gt;&gt; from transformers import pipeline
&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='bert-base-uncased')
&gt;&gt;&gt; unmasker(&quot;Hello I'm a [MASK] model.&quot;)

[{'sequence': &quot;[CLS] hello i'm a fashion model. [SEP]&quot;,
  'score': 0.1073106899857521,
  'token': 4827,
  'token_str': 'fashion'},
 {'sequence': &quot;[CLS] hello i'm a role model. [SEP]&quot;,
  'score': 0.08774490654468536,
  'token': 2535,
  'token_str': 'role'},
 {'sequence': &quot;[CLS] hello i'm a new model. [SEP]&quot;,
  'score': 0.05338378623127937,
  'token': 2047,
  'token_str': 'new'},
 {'sequence': &quot;[CLS] hello i'm a super model. [SEP]&quot;,
  'score': 0.04667217284440994,
  'token': 3565,
  'token_str': 'super'},
 {'sequence': &quot;[CLS] hello i'm a fine model. [SEP]&quot;,
  'score': 0.027095865458250046,
  'token': 2986,
  'token_str': 'fine'}
</code></pre>
<p>Any help on how to do this would be great.</p>
",Training and Model Evaluation,test masked language model training followed tutorial masked language modelling hugging face using bert unsure actually deploy model tutorial trained model using dataset ha worked fine know actually use model notebook doe include example sadly example want trained model hugging face website code used example hence want exact thing model help would great
Confidence score of answer extracted using ELMo BiDAF model and AllenNLP,"<p>I'm working on a Deep Learning project where I use a bidirectional attention flow model (allennlp pretrained model)to make a question answering system.It uses squad dataset.The bidaf model extracts the answer span from paragraph.Is there any way to determine the confidence score(accuracy)or any other metrics of the answer extracted by the model?
I have used the subcommand evaluate from the allennlp package but it determines only score of the model after testing.I was hoping there is a much easier way to solve the issue using other such command.
Attaching the code and the terminal output below.</p>

<pre><code>from rake_nltk import Rake
from string import punctuation
from nltk.corpus import stopwords
from allennlp.predictors.predictor import Predictor
import spacy
import wikipedia
import re
import requests
from requests_html import HTMLSession
from bs4 import BeautifulSoup
import traceback
from nltk.stem import SnowballStemmer
from nltk.util import ngrams
from math import log10
from flask import Flask, request, jsonify, render_template
from gevent.pywsgi import WSGIServer
import time
import multiprocessing as mp
from gtts import gTTS 
import os 

NLP = spacy.load('en_core_web_md')
stop = stopwords.words('english')
symbol = r""""""!#$%^&amp;*();:\n\t\\\""!\{\}\[\]&lt;&gt;-\?""""""
stemmer = SnowballStemmer('english')
wikipedia.set_rate_limiting(True)
session = HTMLSession()
results = 5
try:
    predictor = Predictor.from_path(""bidaf-model-2017.09.15-charpad.tar.gz"")
except:
    predictor = Predictor.from_path(""https://storage.googleapis.com/allennlp-public-models/bidaf-elmo-model-2018.11.30-charpad.tar.gz"")
try:
    srl = Predictor.from_path('srl-model-2018.05.25.tar.gz')
except:
    srl = Predictor.from_path('https://s3-us-west-2.amazonaws.com/allennlp/models/bert-base-srl-2019.06.17.tar.gz')
key = Rake(min_length=1, stopwords=stop, punctuations=punctuation, max_length=6)
wh_words = ""who|what|how|where|when|why|which|whom|whose|explain"".split('|')
stop.extend(wh_words)
session = HTMLSession()
output = mp.Queue()

def termFrequency(term, doc):
    normalizeTermFreq = re.sub('[\[\]\{\}\(\)]', '', doc.lower()).split()
    normalizeTermFreq = [stemmer.stem(i) for i in normalizeTermFreq]
    dl = len(normalizeTermFreq)
    normalizeTermFreq = ' '.join(normalizeTermFreq)
    term_in_document = normalizeTermFreq.count(term)
    #len_of_document = len(normalizeTermFreq )
    #normalized_tf = term_in_document / len_of_document
    normalized_tf = term_in_document
    return normalized_tf, normalizeTermFreq, dl#, n_unique_term

def inverseDocumentFrequency(term, allDocs):
    num_docs_with_given_term = 0
    for doc in allDocs:
        if term in doc:
            num_docs_with_given_term += 1
    if num_docs_with_given_term &gt; 0:
        total_num_docs = len(allDocs)
        idf_val = log10(((total_num_docs+1) / num_docs_with_given_term))
        term_split = term.split()
        if len(term_split) == 3:
            if len([term_split[i] for i in [0, 2] if term_split[i] not in stop]) == 2:
                return idf_val*1.5
            return idf_val
        return idf_val
    else:
        return 0
def sent_formation(question, answer):
    tags_doc = NLP(question)
    tags_doc_cased = NLP(question.title())
    tags_dict_cased = {i.lower_:i.pos_ for i in tags_doc_cased}
    tags_dict = {i.lower_:i.pos_ for i in tags_doc}
    question_cased = []
    for i in question[:-1].split():
        if tags_dict[i] == 'PROPN' or tags_dict[i] == 'NOUN':
            question_cased.append(i.title())
        else:
            question_cased.append(i.lower())
    question_cased.append('?')
    question_cased = ' '.join(question_cased)
    #del tags_dict,tags_doc, tags_doc_cased
    pre = srl.predict(question_cased)
    verbs = []
    arg1 = []
    for i in pre['verbs']:
        verbs.append(i['verb'])
        if 'B-ARG1' in i['tags']:
            arg1.append((i['tags'].index('B-ARG1'), i['tags'].count('I-ARG1'))\
                if not pre['words'][i['tags'].index('B-ARG1')].lower() in wh_words else \
                    (i['tags'].index('B-ARG2'), i['tags'].count('I-ARG2')))
    arg1 = arg1[0] if arg1 else []
    if not arg1:
        verb_idx = pre['verbs'][0]['tags'].index('B-V')
        verb = pre['words'][verb_idx] if pre['words'][verb_idx] != answer.split()[0].lower() else ''
        subj_uncased = pre['words'][verb_idx+1:] if pre['words'][-1]  not in symbol else \
                        pre['words'][verb_idx+1:-1]
    else:
        verb = ' '.join(verbs)
        subj_uncased = pre['words'][arg1[0]:arg1[0]+arg1[1]+1]
    conj = ''
    if question.split()[0].lower() == 'when':
        conj = ' on' if len(answer.split()) &gt; 1 else ' in'
    subj = []
    for n, i in enumerate(subj_uncased):
        if tags_dict_cased[i.lower()] == 'PROPN' and tags_dict[i.lower()] != 'VERB' or n == 0:
            subj.append(i.title())
        else:
            subj.append(i.lower())
    subj[0] = subj[0].title()
    print(subj)
    print(pre)
    subj = ' '.join(subj)
    sent = ""{} {}{} {}."".format(subj, verb, conj, answer if answer[-1] != '.' else answer[:-1])
    return sent

class extractAnswer:
    def __init__(self):
        self.wiki_error = (wikipedia.exceptions.DisambiguationError,
                           wikipedia.exceptions.HTTPTimeoutError,
                           wikipedia.exceptions.WikipediaException)
        self.article_title = None
#        symbol = """"""!#$%^&amp;*();:\n\t\\\""!\{\}\[\]&lt;&gt;-\?""""""
    def extractAnswer_model(self, passage, question, s=0.4, e=0.3, wiki=False):
        if type(passage) == list:
            passage = "" "".join(passage)
        if not question[-1] == '?':
            question = question+'?'
        pre = predictor.predict(passage=passage, question=question)
        if wiki:
            if max(pre['span_end_probs']) &gt; 0.5:
                s = 0.12
            elif max(pre['span_end_probs']) &gt; 0.4:
                s = 0.13
            elif max(pre['span_end_probs']) &gt; 0.35:
                s = 0.14
            if max(pre['span_start_probs']) &gt; 0.5:
                e = 0.12
            elif max(pre['span_start_probs']) &gt; 0.4:
                e = 0.14
            elif max(pre['span_start_probs']) &gt; 0.3:
                e = 0.15
        if max(pre['span_start_probs']) &gt; s and max(pre['span_end_probs']) &gt; e:
            key.extract_keywords_from_text(question)
            ques_key = [stemmer.stem(i) for i in ' '.join(key.get_ranked_phrases())]
            key.extract_keywords_from_text(passage)
            pass_key = [stemmer.stem(i) for i in ' '.join(key.get_ranked_phrases())]
            l = len(ques_key)
            c = 0
            for i in ques_key:
                if i in pass_key:
                    c += 1
            if c &gt;= l/2:
                print(max(pre['span_start_probs']),
                      max(pre['span_end_probs']))
                if wiki:
                    return pre['best_span_str'], max(pre['span_start_probs']) + max(pre['span_end_probs'])
                try:
                    ans = sent_formation(question, pre['best_span_str'])
                except:
                    ans = pre['best_span_str']
                    print(traceback.format_exc())
                return ans
            print(ques_key, c, l)
            print(max(pre['span_start_probs']), max(pre['span_end_probs']))
            return 0, 0
        else:
            print(max(pre['span_start_probs']), max(pre['span_end_probs']), pre['best_span_str'])
            return 0, 0

    def wiki_search_api(self, query):
        article_list = []
        try:
            article_list.extend(wikipedia.search(query, results=results))
            print(article_list)
            return article_list
        except self.wiki_error:
            params = {'search': query, 'profile': 'engine_autoselect',
                      'format': 'json', 'limit': results}
            article_list.extend(requests.get('https://en.wikipedia.org/w/api.php?action=opensearch',
                                             params=params).json()[1])
            return article_list
        except:
            print('Wikipedia search error!')
            print(traceback.format_exc())
            return 0
    def wiki_passage_api(self, article_title, article_list, output):
#        Disambiguation_title = {}
        try:
            passage = wikipedia.summary(article_title)
            output.put((article_title, self.passage_pre(passage)))
        except wikipedia.exceptions.DisambiguationError as e:
            print(e.options[0], e.options)
            Disambiguation_pass = {}
            for p in range(2 if len(e.options) &gt; 1 else len(e.options)):
                params = {'search':e.options[p], 'profile':'engine_autoselect', 'format':'json'}
                article_url = requests.get('https://en.wikipedia.org/w/api.php?action=opensearch',
                                           params=params).json()
                if not article_url[3]:
                    continue
                article_url = article_url[3][0]
                r = session.get(article_url)
                soup = BeautifulSoup(r.html.raw_html)
                print(soup.title.string)
                article_title_dis = soup.title.string.rsplit('-')[0].strip()
                if article_title_dis in article_list:
                    print('continue')
                    continue
                try:
                    url = ""https://en.wikipedia.org/w/api.php?format=json&amp;action=query&amp;prop=extracts&amp;exintro&amp;explaintext&amp;redirects=1&amp;titles={}"".format(article_title_dis)
                    passage = requests.get(url).json()['query']['pages']
                    for i in passage.keys():
                        if 'extract' in passage[i]:
                            Disambiguation_pass[article_title_dis] = self.passage_pre(passage[i]['extract'])
                except wikipedia.exceptions.HTTPTimeoutError:
                    passage = wikipedia.summary(article_title_dis)
                    Disambiguation_pass[article_title_dis] = self.passage_pre(passage)
                except:
                    Disambiguation_pass[article_title_dis] = ''
                    continue
            output.put((article_title, Disambiguation_pass))
        except:
            output.put((article_title, ''))
            print(traceback.format_exc())
    def sorting(self, article, question, topic):
        processes = [mp.Process(target=self.wiki_passage_api, args=(article[x], article, output))\
                     for x in range(len(article))]
        for p in processes:
            p.start()
        for p in processes:
            p.join(timeout=3)
        results_p = [output.get() for p in processes]
        article_list = []
        passage_list = []
        for i, j in results_p:
            if type(j) != dict and j:
                article_list.append(i)
                passage_list.append(j)
            elif type(j) == dict and j:
                for k, l in j.items():
                    if l:
                        article_list.append(k)
                        passage_list.append(l)
        normalize_passage_list = []
        start = time.time()
        keywords = "" "".join(self.noun+self.ques_key+[topic.lower()])
        keywords = re.sub('[{0}]'.format(symbol), ' ', keywords).split()
        question = question+' '+topic
        ques_tokens = [stemmer.stem(i.lower()) for i in question.split() \
                       if i.lower() not in wh_words]
        print(ques_tokens)
        keywords_bigram = [' '.join(i) for i in list(ngrams(ques_tokens, 2)) \
                           if i[0] not in stop and i[1] not in stop]
        if len(ques_tokens) &gt; 3:
            keywords_trigram = [' '.join(i) for i in list(ngrams(ques_tokens, 3)) \
                                if (i[0] in stop) + (i[2] in stop) + (i[1] in stop) &lt; 3]
        else:
            keywords_trigram = []
        if len(ques_tokens) &gt; 5:
            keywords_4gram = [' '.join(i) for i in list(ngrams(ques_tokens, 4)) \
                              if (i[0] in stop) + (i[2] in stop) +(i[1] in stop)+(i[3] in stop) &lt; 4]
        else:
            keywords_4gram = []
        keywords_unigram = list(set([stemmer.stem(i.lower()) for i in keywords \
                                     if i.lower() not in stop]))
        keywords = keywords_unigram+list(set(keywords_bigram))+keywords_trigram+keywords_4gram
        tf = []
        if not passage_list:
            return 0
        pass_len = []
        #n_u_t=[]
        #key_dict = {i: keywords.count(i) for i in keywords}
        print('Extraction complete')
        #remove_pass={}
        #for n,i in enumerate(passage_list):
            #if len(i)&lt;200 or not i:
                #remove_pass[article_list[n]]=i
                #print(n, article_list[n])
        #passage_list=[i for i in passage_list if i not in remove_pass.values()]
        #article_list=[i for i in article_list if i not in remove_pass.keys()]
        passage_list_copy = passage_list.copy()
        article_list_copy = article_list.copy()
        for i in range(len(passage_list_copy)):
            if passage_list.count(passage_list_copy[i]) &gt; 1:
                passage_list.remove(passage_list_copy[i])
                article_list.remove(article_list_copy[i])
                print('Copy:', article_list_copy[i])
        del passage_list_copy
        del article_list_copy
        for n, i in enumerate(passage_list):
            temp_tf = {}
            c = 0
            for j in keywords:
                temp_tf[j], temp_pass, temp_len = termFrequency(j, i + ' ' + article_list[n])
                if temp_tf[j]:
                    c += 1
            normalize_passage_list.append(temp_pass)
            pass_len.append(temp_len)
            temp_tf['key_match'] = c
            tf.append(temp_tf)
        print(pass_len)
        print(keywords)
        idf = {}
        for i in keywords:
            idf[i] = inverseDocumentFrequency(i, normalize_passage_list)
        #print(tf, idf)
        tfidf = []
        #b=0.333 #for PLN
        b, k = 0.75, 1.2 #for BM25
        avg_pass_len = sum(pass_len)/len(pass_len)
        #pivot=sum(n_u_t)/len(n_u_t)
        for n, i in enumerate(tf):
            tf_idf = 0
            #avg_tf=sum(i.values())/len(i)
            key_match_ratio = i['key_match']/len(keywords)
            for j in keywords:
                #tf_idf+=idf[j]*((log(1+log(1+i[j])))/(1-b+(b*pass_len[n]/avg_pass_len))) #PLN
                tf_idf += idf[j]*(((k+1)*i[j])/(i[j]+k*(1-b+(b*pass_len[n]/avg_pass_len)))) #BM25
            tfidf.append(tf_idf*key_match_ratio)
        tfidf = [i/sum(tfidf)*100 for i in tfidf if any(tfidf)]
        if not tfidf:
            return 0, 0, 0, 0, 0
        print(tfidf)
        print(article_list, len(passage_list))
        if len(passage_list) &gt; 1:
            sorted_tfidf = sorted(tfidf, reverse=1)
            idx1 = tfidf.index(sorted_tfidf[0])
            passage1 = passage_list[idx1]
            #article_title=
            tfidf1 = sorted_tfidf[0]
            idx2 = tfidf.index(sorted_tfidf[1])
            passage2 = passage_list[idx2]
            article_title = (article_list[idx1], article_list[idx2])
            tfidf2 = sorted_tfidf[1]
        else:
            article_title = 0
            tfidf2 = 0
            if passage_list:
                passage1 = passage_list[0]
                tfidf1 = tfidf[0]
                passage2 = 0
            else:
                passage1 = 0
                passage2 = 0
                tfidf1, tfidf2 = 0, 0
        end = time.time()
        print('TFIDF time:', end-start)
        return passage1, passage2, article_title, tfidf1, tfidf2

    def passage_pre(self, passage):
        #passage=re.findall(""[\da-zA-z\.\,\'\-\/\–\(\)]*"", passage)
        passage = re.sub('\n', ' ', passage)
        passage = re.sub('\[[^\]]+\]', '', passage)
        passage = re.sub('pronunciation', '', passage)
        passage = re.sub('\\\\.+\\\\', '', passage)
        passage = re.sub('{.+}', '', passage)
        passage = re.sub(' +', ' ', passage)
        return passage
    def wiki(self, question, topic=''):
        if not question:
            return 0
        question = re.sub(' +', ' ', question)
        question = question.title()
        key.extract_keywords_from_text(question)
        self.ques_key = key.get_ranked_phrases()
        doc = NLP(question)
        self.noun = [str(i).lower() for i in doc.noun_chunks if str(i).lower() not in wh_words]
        print(self.ques_key, self.noun)
        question = re.sub('[{0}]'.format(symbol), ' ', question)
        if not self.noun + self.ques_key:
            return 0
        article_list = None
        question = question.lower()
        if self.noun:
            if len(self.noun) == 2 and len("" "".join(self.noun).split()) &lt; 6:
                #question1=question
                self.noun = "" "".join(self.noun).split()
                if self.noun[0] in stop:
                    self.noun.pop(0)
                self.noun = question[question.index(self.noun[0]):question.index(self.noun[-1]) \
                                     +len(self.noun[-1])+1].split()
                #del question1
                print(self.noun)
            article_list = self.wiki_search_api(' '.join(self.noun))
        if self.ques_key and not article_list:
            article_list = self.wiki_search_api(self.ques_key[0])
        if not article_list:
            article_list = self.wiki_search_api(' '.join(self.ques_key))
        if not article_list:
            print('Article not found on wikipedia.')
            return 0, 0
        article_list = list(set(article_list))
        passage1, passage2, article_title, tfidf1, tfidf2 = self.sorting(article_list,
                                                                         question, topic)
        if passage1:
            ans1, conf1 = self.extractAnswer_model(passage1, question, s=0.20, e=0.20, wiki=True)
        else:
            ans1, conf1 = 0, 0
        if ans1:
            conf2 = 0
            if len(ans1) &gt; 600:
                print(ans1)
                print('Repeat')
                ans1, conf1 = self.extractAnswer_model(ans1, question, s=0.20, e=0.20, wiki=True)
        threshhold = 0.3 if not ((tfidf1- tfidf2) &lt;= 10) else 0.2
        if round(tfidf1- tfidf2) &lt; 5:
            threshhold = 0
        if (tfidf1- tfidf2) &gt; 20:
            threshhold = 0.35
        if (tfidf1- tfidf2) &gt; 50:
            threshhold = 1
        if (passage2 and conf1 &lt; 1.5) or (tfidf1 - tfidf2) &lt; 10:
            ans2, conf2 = self.extractAnswer_model(passage2, question, s=0.20, e=0.20,
                                                   wiki=True) if passage2 else (0, 0)
        title = 0
        if round(conf1, 2) &gt; round(conf2, 2) - threshhold:
            print('ans1')
            ans = ans1
            title = article_title[0] if article_title else 0
        else:
            print('ans2')
            title = article_title[1] if article_title else 0
            ans = ans2
        if not question[-1] == '?':
            question = question+'?'
        try:
            ans = sent_formation(question, ans)
        except:
            print(traceback.format_exc())
        print(ans, '\n', '\n', article_title)
        return ans, title

extractor = extractAnswer()
app = Flask(__name__)
@app.route(""/"", methods=[""POST"", ""get""])
@app.route(""/ans"")


def ans():
    start = time.time()
    question = request.args.get('question')
    topic = request.args.get('topic')
    passage = request.args.get('passage')
    if not question:
        return render_template('p.html')
    if not topic:
        topic = ''
    if passage:
        answer = extractor.extractAnswer_model(passage, question)
    else:
        answer, title = extractor.wiki(question, topic)
    end = time.time()
    if answer:
        mytext = str(answer)
        language = 'en'
        myobj = gTTS(text=mytext, lang=language, slow=False)
        myobj.save(""welcome.mp3"")
       # prevName = 'welcome.mp3'
        #newName = 'static/welcome.mp3'
        #os.rename(prevName,newName)
        return render_template('pro.html', answer=answer)   

    else:
        return jsonify(Status='E', Answer=answer, Time=end-start)
@app.route(""/audio_del/"", methods=[""POST"", ""get""])
def audio_del():
    return render_template('p.html');

@app.route(""/audio_play/"", methods=[""POST"", ""get""])
def audio_play():
    os.system(""mpg321 welcome.mp3"")
    return render_template('white.html')

if __name__ == ""__main__"":
    PORT = 7091
    HTTP_SERVER = WSGIServer(('0.0.0.0', PORT), app)
    print('Running on',PORT, '...')
    HTTP_SERVER.serve_forever()



![Output in the terminal for a question I've asked](https://i.sstatic.net/6pyv5.jpg)
</code></pre>
",Training and Model Evaluation,confidence score answer extracted using elmo bidaf model allennlp working deep learning project use bidirectional attention flow model allennlp pretrained model make question answering system us squad dataset bidaf model extract answer span paragraph way determine confidence score accuracy metric answer extracted model used subcommand evaluate allennlp package determines score model testing wa hoping much easier way solve issue using command attaching code terminal output
loss value deep learning model is inf,"<p>I train a RNN deep learning model as bellow:</p>
<pre><code>model = Sequential()
initializer = tf.keras.initializers.RandomNormal(mean=.5, stddev=1)
model.add(LSTM(512, return_sequences=True, dropout=0.2,input_shape=trainX.shape[1:] ,recurrent_dropout=0.2))
model.add(Dense(256,activation='linear',kernel_initializer='random_normal',bias_initializer=initializer))

model.add(LSTM(128 ,return_sequences=False, dropout=0.1, recurrent_dropout=0.1))
model.add(Dense(128, activation='tanh',kernel_initializer='random_normal',bias_initializer=initializer))

model.add(Dense(64, activation='tanh',kernel_initializer='random_normal'))

model.add(Dense(1,kernel_initializer='random_normal', activation='linear'))

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.000001),
              loss='mean_absolute_error',
              metrics=['mean_squared_error'])

history = model.fit(trainX, y, validation_split = 0.3, epochs=500, batch_size=30)
</code></pre>
<p>the output of my model is:
<a href=""https://i.sstatic.net/7eFLN.jpg"" rel=""nofollow noreferrer"">the loss validation value is decreased </a></p>
<p>I do normalization on my input data and there is no 'nan' or 'zero' in input data. My label is the Jaccard similarity on the word2vec input data and it is so simple label!!!
but I don't know, how can fix it?</p>
",Training and Model Evaluation,loss value deep learning model inf train rnn deep learning model bellow output model loss validation value decreased normalization input data nan zero input data label jaccard similarity word vec input data simple label know fix
Looking for a model on how to match a keyword / part(s) of word into a full name,"<p>Our data set is a lot of ship movements. A captain usually enters destination in form of a short code like: &quot;<em>CHN</em>&quot;, &quot;<em>Singapore</em>&quot;, &quot;<em>XIAO HU</em>&quot; etc.
These keywords are pretty short, can be somewhat misspelled and can be a code for a port, name of the city or just random gibberish depending on user's mood :)</p>
<p>We have a large dictionary of destination words and want to try and match the code into a real destination (Like Singapore Port X).</p>
<p>So, basically it's a word =&gt; words predictor, depending on earlier history (ie. same vessels usually goes on same route).</p>
<p>Is there a good NLP model to use to implement these kind of predictions? Right now we're using regular text/pattern matching and it WORKS, but thinking this could be solved better by ML.</p>
<p>Since we also have the actual trip data, we <strong>do</strong> know the actual destination afterwards, so it's possible to have a decent training data set. Unfortunately the captain might be different for same vessel on different trips, so the &quot;style&quot; of codes might not be consistent.</p>
",Training and Model Evaluation,looking model match keyword part word full name data set lot ship movement captain usually enters destination form short code like chn singapore xiao hu etc keywords pretty short somewhat misspelled code port name city random gibberish depending user mood large dictionary destination word want try match code real destination like singapore port x basically word word predictor depending earlier history ie vessel usually go route good nlp model use implement kind prediction right using regular text pattern matching work thinking could solved better ml since also actual trip data know actual destination afterwards possible decent training data set unfortunately captain might different vessel different trip style code might consistent
Which Machine Learning classification to use?,"<p>I have a  master dataset lets call it &quot;train&quot; with approx 7000 rows and 2 columns of text data. There is a column name &quot;Item Description&quot; with unique text description and another column called &quot;Code&quot; with unique values (see the below table). Please note the codes don't repeat and it is unique , also the &quot;Item Description&quot; will also be unique. Also the word count of the &quot;Item Description&quot; varies from short text to lengthy text .</p>
<p>Master Data  - I plan to use this full dataset as a training data</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Item Description</th>
<th>Code</th>
</tr>
</thead>
<tbody>
<tr>
<td>I love cats</td>
<td>000</td>
</tr>
<tr>
<td>I love dogs and puppies</td>
<td>001</td>
</tr>
<tr>
<td>Dogs chase Cats</td>
<td>002</td>
</tr>
<tr>
<td>Cat chase Mouse</td>
<td>003</td>
</tr>
<tr>
<td>Dogs chase both mouse and cat</td>
<td>004</td>
</tr>
</tbody>
</table>
</div>
<p>I have another separate dataset lets call it &quot;test&quot; with 500 rows with Item Descriptions and codes and I need to check if the Item Description of the below table is mapped correctly using the above training master table as a reference. Please note this is not a text matching exercise, I need an ML / NLP  solution for matching</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Item Description</th>
<th>Code</th>
</tr>
</thead>
<tbody>
<tr>
<td>dogs and puppies are cute</td>
<td>000</td>
</tr>
<tr>
<td>humans like cats</td>
<td>001</td>
</tr>
<tr>
<td>Cats run away from dogs</td>
<td>002</td>
</tr>
<tr>
<td>Rats dont like cats</td>
<td>003</td>
</tr>
<tr>
<td>Cats dont like mice</td>
<td>004</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>What type of classification does this fall  -  Multi Class Classification or Multilabel classification ?</li>
<li>What similarity measures do I need to use ?</li>
<li>How can I do a train test split - Is it possible since I have two separate dataset and each rows are unique in the master data ?</li>
</ul>
<p>I am bit confused here... Please give me some tips to start this project and some useful weblinks for reference ?</p>
<p>I have done some coding but it is not working.</p>
<pre><code>x_train=train['Item Description'].values
y_train = train['Code'].values

x_test = test['Item Description'].values
y_test = test['Code'].values
 
transformer = TfidfTransformer(smooth_idf=False)
count_vectorizer = CountVectorizer(ngram_range=(1, 2))

# fit train data to the count vectorizer
x_train = count_vectorizer.fit_transform(x_train)
y_train = count_vectorizer.fit_transform(y_train)


from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(C=1e5)
logreg.fit(X_train, y_train)
</code></pre>
<p>I get</p>
<pre><code> ValueError: X has 25118 features, but LogisticRegression is expecting 26991 features as 
 input.
</code></pre>
",Training and Model Evaluation,machine learning classification use master dataset let call train approx row column text data column name item description unique text description another column called code unique value see table please note code repeat unique also item description also unique also word count item description varies short text lengthy text master data plan use full dataset training data item description code love cat love dog puppy dog chase cat cat chase mouse dog chase mouse cat another separate dataset let call test row item description code need check item description table mapped correctly using training master table reference please note text matching exercise need ml nlp solution matching item description code dog puppy cute human like cat cat run away dog rat dont like cat cat dont like mouse type classification doe fall multi class classification multilabel classification similarity measure need use train test split possible since two separate dataset row unique master data bit confused please give tip start project useful weblinks reference done coding working get
Transformer summariser pipeline giving different results on same model with fixed seed,"<p>I am using a HuggingFace summariser pipeline and I noticed that if I train a model for 3 epochs and then at the end run evaluation on all 3 epochs with fixed random seeds, I get a different results based on whether I restart the python console 3 times or whether I load the different model (one for every epoch) on the same summariser object in a loop, and I would like to understand why we have this strange behaviour.</p>
<p>While my results are based on ROUGE score on a large dataset, I have made this small reproducible example to show this issue. Instead of using the weights of the same model at different training epochs, I decided to demonstrate using two different summarization models, but the effect is the same. Grateful for any help.</p>
<p>Notice how in the first run I firstly use the <code>facebook/bart-large-cnn</code> model and then the <code>lidiya/bart-large-xsum-samsum</code> model without shutting the python terminal. In the second run I only use <code>lidiya/bart-large-xsum-samsum</code> model and get different output (which should not be the case).</p>
<p>NOTE: this reproducible example won't work on a CPU machine as it doesn't seem sensitive to <code>torch.use_deterministic_algorithms(True)</code> and it might give different results every time when run on a CPU, so should be reproduced on a GPU.</p>
<p>FIRST RUN</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer,  AutoModelForSeq2SeqLM, pipeline
import torch

# random text taken from UK news website
text = &quot;&quot;&quot;
The veteran retailer Stuart Rose has urged the government to do more to shield the poorest from double-digit inflation, describing the lack of action as “horrifying”, with a prime minister “on shore leave” leaving a situation where “nobody is in charge”.
Responding to July’s 10.1% headline rate, the Conservative peer and Asda chair said: “We have been very, very slow in recognising this train coming down the tunnel and it’s run quite a lot of people over and we now have to deal with the aftermath.”
Attacking a lack of leadership while Boris Johnson is away on holiday, he said: “We’ve got to have some action. The captain of the ship is on shore leave, right, nobody’s in charge at the moment.”
Lord Rose, who is a former boss of Marks &amp; Spencer, said action was needed to kill “pernicious” inflation, which he said “erodes wealth over time”. He dismissed claims by the Tory leadership candidate Liz Truss’s camp that it would be possible for the UK to grow its way out of the crisis.
&quot;&quot;&quot;

seed = 42
torch.cuda.manual_seed_all(seed)
torch.use_deterministic_algorithms(True)
tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/bart-large-cnn&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;facebook/bart-large-cnn&quot;)
model.eval()
summarizer = pipeline(
    &quot;summarization&quot;, model=model, tokenizer=tokenizer, 
    num_beams=5, do_sample=True, no_repeat_ngram_size=3, device=0
)

output = summarizer(text, truncation=True)

tokenizer = AutoTokenizer.from_pretrained(&quot;lidiya/bart-large-xsum-samsum&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;lidiya/bart-large-xsum-samsum&quot;)
model.eval()
summarizer = pipeline(
    &quot;summarization&quot;, model=model, tokenizer=tokenizer, 
    num_beams=5, do_sample=True, no_repeat_ngram_size=3, device=0
)

output = summarizer(text, truncation=True)
print(output)
</code></pre>
<p>output from <code>lidiya/bart-large-xsum-samsum</code> model should be</p>
<pre><code>[{'summary_text': 'The UK economy is in crisis because of inflation. The government has been slow to react to it. Boris Johnson is on holiday.'}]
</code></pre>
<p>SECOND RUN (you must restart python to conduct the experiment)</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer,  AutoModelForSeq2SeqLM, pipeline
import torch

text = &quot;&quot;&quot;
The veteran retailer Stuart Rose has urged the government to do more to shield the poorest from double-digit inflation, describing the lack of action as “horrifying”, with a prime minister “on shore leave” leaving a situation where “nobody is in charge”.
Responding to July’s 10.1% headline rate, the Conservative peer and Asda chair said: “We have been very, very slow in recognising this train coming down the tunnel and it’s run quite a lot of people over and we now have to deal with the aftermath.”
Attacking a lack of leadership while Boris Johnson is away on holiday, he said: “We’ve got to have some action. The captain of the ship is on shore leave, right, nobody’s in charge at the moment.”
Lord Rose, who is a former boss of Marks &amp; Spencer, said action was needed to kill “pernicious” inflation, which he said “erodes wealth over time”. He dismissed claims by the Tory leadership candidate Liz Truss’s camp that it would be possible for the UK to grow its way out of the crisis.
&quot;&quot;&quot;

seed = 42
torch.cuda.manual_seed_all(seed)
torch.use_deterministic_algorithms(True)

tokenizer = AutoTokenizer.from_pretrained(&quot;lidiya/bart-large-xsum-samsum&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;lidiya/bart-large-xsum-samsum&quot;)
model.eval()
summarizer = pipeline(
    &quot;summarization&quot;, model=model, tokenizer=tokenizer, 
    num_beams=5, do_sample=True, no_repeat_ngram_size=3, device=0
)

output = summarizer(text, truncation=True)
print(output)
</code></pre>
<p>output should be</p>
<pre><code>[{'summary_text': 'The government has been slow to deal with inflation. Stuart Rose has urged the government to do more to shield the poorest from double-digit inflation.'}]
</code></pre>
<p>Why is the first output different from the second one?</p>
",Training and Model Evaluation,transformer summariser pipeline giving different result model fixed seed using huggingface summariser pipeline noticed train model epoch end run evaluation epoch fixed random seed get different result based whether restart python console time whether load different model one every epoch summariser object loop would like understand strange behaviour result based rouge score large dataset made small reproducible example show issue instead using weight model different training epoch decided demonstrate using two different summarization model effect grateful help notice first run firstly use model model without shutting python terminal second run use model get different output case note reproducible example work cpu machine seem sensitive might give different result every time run cpu reproduced gpu first run output model second run must restart python conduct experiment output first output different second one
Hierarchical LSTM autoencoder - model not training,"<p>I'm trying to reconstruct <a href=""https://arxiv.org/pdf/1506.01057.pdf"" rel=""nofollow noreferrer"">this paper</a> about hierarchical autoencoder for paragraphs.</p>
<p>The idea is: Break a paragraph into sentences, then encode each sentence using an LSTM, and then using these encoding as an input for another LSTM that encode the entire paragraph.</p>
<p>Then, using a mirror decoder, decode the encoded paragraph using an LSTM into multiple sentences, and then use another LSTM to decode each word, with a linear layer on top and predicts the word.</p>
<p>The objective is to try to predict the original paragraph.</p>
<p>I've done some preprocessing, and right now I save each paragraph as a tensor of <code>(maxSentence,maxWordsPerSentence,VocabSize)</code>, using one hot encoding.</p>
<p>My problem is, there model is not learning. The loss stays exactly the same and it doesn't seem as anything is happening.. I wasn't sure on how to calculate the loss (I've ran a batch all together and decoded it into multiple paragraphs, and then calculated the loss against the entire batch predictions, my train function is added below. I don't know if that is the problem (maybe I should calculate loss sentence by sentence instead the entire paragraph?) or maybe I have a problem in my model.</p>
<p>Encoder code:</p>
<pre><code>class Encoder(nn.Module):
def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):
    super().__init__()
    
    #self.embedding = nn.Embedding(input_dim, emb_dim)
    self.rnn_sent = nn.GRU(input_dim, enc_hid_dim, bidirectional = True)
    self.rnn_par = nn.GRU(enc_hid_dim*2, dec_hid_dim, bidirectional = True)

    
def forward(self, src):
    
    outputs, hidden = self.rnn_sent(src[:,0,0])
    total_out = outputs.unsqueeze(0).permute(1,0,2)
    for i in range(1,src.shape[1]):
      for j in range(src.shape[2]):
        outputs, hidden = self.rnn_sent(src[:,i,j],hidden)
      total_out = torch.cat((total_out,outputs.unsqueeze(0).permute(1,0,2)),dim=1)  

    outputs_par, hidden_par = self.rnn_par(total_out[:,0])
    
    for i in range(total_out.shape[1]):
        outputs_par, hidden_par = self.rnn_par(total_out[:,i],hidden_par)

    return outputs_par, hidden_par
</code></pre>
<p>Decoder code:</p>
<pre><code>class Decoder(nn.Module):
def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):
    super().__init__()

    self.output_dim = output_dim
    self.attention = attention
    #self.embedding = nn.Embedding(output_dim, emb_dim)
    self.rnn_par = nn.GRU((enc_hid_dim * 2), dec_hid_dim*2)
    self.rnn_sen = nn.GRU(output_dim, dec_hid_dim*2)
    self.fc_out = nn.Linear(dec_hid_dim*2, output_dim)
    self.dropout = nn.Dropout(dropout)
    
def forward(self, input, hidden, encoder_outputs):
    output, hidden = self.rnn_par(encoder_outputs)
    all_par = output.unsqueeze(0).permute(1,0,2)
    for i in range(1,max_par_len):
      output,hidden = self.rnn_par(output,hidden)
      all_par = torch.cat((all_par,output.unsqueeze(0).permute(1,0,2)),dim=1)


    for i in range(max_par_len):
      output_arg = self.fc_out(all_par[:,i])
      #output_argmax = F.one_hot(output_arg.argmax(dim = 1), self.output_dim).to(torch.float)
      output_argmax = torch.softmax(output_arg,dim=1)
      output_sen, hidden_sen = self.rnn_sen(output_argmax)
      all_par_sen = output_argmax.unsqueeze(0).permute(1,0,2)
      for j in range(max_sen_len - 1):
        output_sen,hidden_sen = self.rnn_sen(output_argmax,hidden_sen)
        output_arg = self.fc_out(output_sen)
        output_argmax = torch.softmax(output_arg,dim=1)

        all_par_sen = torch.cat((all_par_sen,output_argmax.unsqueeze(0).permute(1,0,2)),dim=1)
      if i == 0:
        all_doc = all_par_sen.unsqueeze(0).permute(1,0,2,3)
      else:
        all_doc = torch.cat((all_doc,all_par_sen.unsqueeze(0).permute(1,0,2,3)),dim=1)

      i+=1
    return all_doc  ,hidden_sen
</code></pre>
<p>And my train function:</p>
<pre><code>def train(model, iterator, optimizer, criterion, clip, epoch):

model.train()

epoch_loss = 0
data = tqdm(iterator)
for i, batch in enumerate(data):
    src = batch[0].to(device)#.to(torch.long)#.reshape(batch[0].shape[0],-1)
    trg = batch[0].to(device)#.to(torch.long)#.reshape(batch[0].shape[0],-1)
    target = torch.argmax(trg,dim=3).view(-1)
    print(target)
    optimizer.zero_grad()
    output = model(src, trg).view(-1,OUTPUT_DIM)
    loss = criterion(output, target)        
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
    optimizer.step()
    epoch_loss += loss.item()


N_EPOCHS = 20
CLIP = 1
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
criterion = nn.CrossEntropyLoss(ignore_index = vocabulary['&lt;pad&gt;'])
best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):

    start_time = time.time()
    train_loader, valid_loader = data_loaders['train_loader'], data_loaders['test_loader']
    train_loss = train(model, train_loader, optimizer, criterion, CLIP,f'{epoch+1}/{N_EPOCHS}')
    #valid_loss = evaluate(model, valid_loader, criterion)

    end_time = time.time()

    epoch_mins, epoch_secs = epoch_time(start_time, end_time)

    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
</code></pre>
",Training and Model Evaluation,hierarchical lstm autoencoder model training trying reconstruct paper hierarchical autoencoder paragraph idea break paragraph sentence encode sentence using lstm using encoding input another lstm encode entire paragraph using mirror decoder decode encoded paragraph using lstm multiple sentence use another lstm decode word linear layer top predicts word objective try predict original paragraph done preprocessing right save paragraph tensor using one hot encoding problem model learning loss stay exactly seem anything happening sure calculate loss ran batch together decoded multiple paragraph calculated loss entire batch prediction train function added know problem maybe calculate loss sentence sentence instead entire paragraph maybe problem model encoder code decoder code train function
Do I need training data in multiple languages for a multilingual transformer?,"<p>I am attempting to train a transformer which can categorize sentences into one of <em>n</em> categories. This model should be able to work with a number of different languages - English and Arabic in my case.</p>
<p>Do I need to have labelled training data in both English and Arabic to fine tune a pretrained transformer, such as <a href=""https://huggingface.co/docs/transformers/model_doc/bloom"" rel=""nofollow noreferrer"">BLOOM</a>, or can I fine tune the model using only English samples, and then the model should also work well on Arabic samples for my classificaiton task, since the fine tuning only trained the classification head?</p>
<p>My thoughts are that the pretraining of this model should allow it to transform the same input texts in English and Arabic to the same (or similar) embedding, which the classification head would have learned to then predict these embeddings accurately through the fine tuning.</p>
",Training and Model Evaluation,need training data multiple language multilingual transformer attempting train transformer categorize sentence one n category model able work number different language english arabic case need labelled training data english arabic fine tune pretrained transformer bloom fine tune model using english sample model also work well arabic sample classificaiton task since fine tuning trained classification head thought pretraining model allow transform input text english arabic similar embedding classification head would learned predict embeddings accurately fine tuning
Re-Train huggingface pre trained model properly,"<p>I have a hugginface model, I am re-training it after adding a few layers of my own.</p>
<pre><code>embedding = model(input_ids, attention_mask=input_mask)[0]

embedding = tf.layers.Embedding(tokenizer.vocab_size, 256, input_length=max_len)(embedding)

out = tf.layers.LSTM(50,return_sequences=True)(embedding)
out = tf.layers.Dropout(.2)(out)

out = tf.layers.Bidirectional(tf.layers.LSTM(10,return_sequences=True))(out)
out = tf.layers.Dropout(.2)(out)

out = tf.layers.Dense(20, activation='relu')(out)
out = tf.layers.Dropout(.2)(out)

out = tf.layers.Dense(3, activation='softmax')(out)

new_model = tf.Model(inputs=[input_ids,input_mask], outputs=out)

optimizer = tf.optimizers.Adam(
learning_rate=.01,
epsilon=.01,
decay=.1,
clipnorm=1.0
)
loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.metrics.SparseCategoricalAccuracy('accuracy')

new_model.compile(optimizer=optimizer,loss=loss,metrics=metric)
</code></pre>
<p>Now when I run model.fit with training and validation data i get this runtime error</p>
<pre><code>WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/word_embeddings/weight:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/token_type_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/position_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/classifier/dense/kernel:0', 'tf_roberta_for_sequence_classification/classifier/dense/bias:0', 'tf_roberta_for_sequence_classification/classifier/out_proj/kernel:0', 'tf_roberta_for_sequence_classification/classifier/out_proj/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?
</code></pre>
<p>What am I doing wrong. Also can someone explain what does this error actually means to say?</p>
<p>Also even after 3 epochs the accuracy is not improving.</p>
<p>Please help me on how to re-train a pre trained model with transfer learning I am very new to this.</p>
",Training and Model Evaluation,train huggingface pre trained model properly hugginface model training adding layer run model fit training validation data get runtime error wrong also someone explain doe error actually mean say also even epoch accuracy improving please help train pre trained model transfer learning new
Save best model with train_step without model.fit(),"<p>Hello I'm new in machine learning, so I'm trying to save the best model weights out of 30 epochs. Now I can only save all 30 models using this code</p>
<pre><code>train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
    name='train_accuracy')

transformer = Transformer(num_layer, d_model, num_heads, dff, row_size, col_size, target_vocab_size,
                          max_pos_encoding=target_vocab_size, rate=dropout_rate)

@tf.function
def train_step(img_tensor, tar):
    tar_inp = tar[:, :-1]
    tar_real = tar[:, 1:]

    dec_mask = create_masks_decoder(tar_inp)

    with tf.GradientTape() as tape:
        predictions, _ = transformer(img_tensor, tar_inp,
                                     True,
                                     dec_mask)
        loss = loss_function(tar_real, predictions)

    gradients = tape.gradient(loss, transformer.trainable_variables)
    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))

    train_loss(loss)
    train_accuracy(tar_real, predictions)

for epoch in range(30):
    start = time.time()

    train_loss.reset_states()
    train_accuracy.reset_states()

    for (batch, (img_tensor, tar)) in enumerate(dataset):
        train_step(img_tensor, tar)

        if batch % 50 == 0:
            print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(
                epoch + 1, batch, train_loss.result(), train_accuracy.result()))

    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,
                                                        train_loss.result(),
                                                        train_accuracy.result()))

    print('Time taken for 1 epoch: {} secs\n'.format(time.time() - start))
    model_name = 'image_caption_transformer_' + str(epoch + 1) + '.h5'
    transformer.save_weights(model_name)
</code></pre>
<p>I wanted to try using ModelCheckpoint from keras but I don't know how to implement it without model.fit(), any solutions to save the best model with the code above or change the code above to use model.fit()?</p>
",Training and Model Evaluation,save best model train step without model fit hello new machine learning trying save best model weight epoch save model using code wanted try using modelcheckpoint kera know implement without model fit solution save best model code change code use model fit
How to check the balance accuracy between two texts?,"<p>I tested some audio files for speech-to-text on IBM's Watson API and My own API.</p>
<p>for example,
text 1: got from IBM's Watson API.
text 2: got from own API.</p>
<p>I want to check the accuracy of text2 Relatively to text1.</p>
",Training and Model Evaluation,check balance accuracy two text tested audio file speech text ibm watson api api example text got ibm watson api text got api want check accuracy text relatively text
How to create a abstractive summary using supervised Machine learning?,"<p>I want to summarise my document to about 7-8% of total words in document.The summary should be abstractive not extractive. I've referred some of the previous abstractive summarisation strategies however they're using deep learning models like seq2seq, lstm, etc. And I would like to do this task using some basic supervised Machine learning algorithms like svm, logistic regression, etc. The accuracy is not a concern for me. I did a lot of research but was not able to get something relevant.</p>
",Training and Model Evaluation,create abstractive summary using supervised machine learning want summarise document total word document summary abstractive extractive referred previous abstractive summarisation strategy however using deep learning model like seq seq lstm etc would like task using basic supervised machine learning algorithm like svm logistic regression etc accuracy concern lot research wa able get something relevant
Huggingface Trainer load_best_model f1 score vs. loss and overfitting,"<p>I have trained a roberta-large and specified <code>load_best_model_at_end=True</code> and <code>metric_for_best_model=f1</code>. During training, I can see overfitting after the 6th epoch, which is the sweetspot. In Epoch 8, which is the next one to evaluate due to gradient accumulation, we can see that train loss decreases and eval_loss increases. Thus, overfitting starts. The transformers trainer in the end loads the model from epoch 8, checkpoint <code>-14928</code>, as the f1 score is a bit highea. I was wondering, in theory, wouldn't be the model from epoch 6 be better suited, as it did not overfit? Or does one really go for the f1 metric here even though the model did overfit? (the eval loss decreased in epochs &lt;6 constantly).</p>
<p>The test_loss from the second checkpoint, which is then loaded as the &quot;best&quot;, is 0.128. Is it possible to lower that using the first checkpoint which should be the better model anyway?</p>
<pre><code>checkpoint-11196:
{'loss': 0.0638, 'learning_rate': 8.666799323450404e-06, 'epoch': 6.0}

{'eval_loss': 0.09599845856428146, 'eval_accuracy': 0.9749235986101227, 'eval_precision': 0.9648319293367138, 'eval_recall': 0.9858766505097777, 'eval_f1': 0.9752407721241682, 'eval_runtime': 282.2294, 'eval_samples_per_second': 84.637, 'eval_steps_per_second': 2.647, 'epoch': 6.0}

VS.

checkpoint-14928:
{'loss': 0.0312, 'learning_rate': 7.4291115311909265e-06, 'epoch': 8.0}

{'eval_loss': 0.12377820163965225, 'eval_accuracy': 0.976305103194206, 'eval_precision': 0.9719324391455539, 'eval_recall': 0.9810295838208257, 'eval_f1': 0.9764598236566295, 'eval_runtime': 276.7619, 'eval_samples_per_second': 86.309, 'eval_steps_per_second': 2.699, 'epoch': 8.0}
</code></pre>
",Training and Model Evaluation,huggingface trainer load best model f score v loss overfitting trained roberta large specified training see overfitting th epoch sweetspot epoch next one evaluate due gradient accumulation see train loss decrease eval loss increase thus overfitting start transformer trainer end load model epoch checkpoint f score bit highea wa wondering theory model epoch better suited overfit doe one really go f metric even though model overfit eval loss decreased epoch constantly test loss second checkpoint loaded best possible lower using first checkpoint better model anyway
Discrepancy between results reported by TensorFlow model.evaluate and model.predict,"<p>I've been back and forth with this for ages, but without being able to find a solution so far anywhere. So, I have a HuggingFace model ('bert-base-cased') that I'm using with TensorFlow and a custom dataset. I've: (1) tokenized my data (2) split the data; (3) converted the data to TF dataset format; (4) instantiated, compiled and fit the model.</p>
<p>During training, it behaves as you'd expect: training and validation accuracy go up. But when I evaluate the model on the test dataset using TF's model.evaluate and model.predict, the results are very different. The accuracy as reported by model.evaluate is higher (and more or less in line with the validation accuracy); the accuracy as reported by model.predict is about 10% lower. (Maybe it's just a coincidence, but it's similar to the reported training accuracy after the single epoch of fine-tuning.)</p>
<p>Can anyone figure out what's causing this? I include snippets of my code below.</p>
<pre><code># tokenize the dataset
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=&quot;bert-base-cased&quot;,use_fast=False)

def tokenize_function(examples):
  return tokenizer(examples['text'], padding = &quot;max_length&quot;, truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# splitting dataset
trainSize = 0.7
valTestSize = 1 - trainSize
train_testvalid = tokenized_datasets.train_test_split(test_size=valTestSize,stratify_by_column='class')
valid_test = train_testvalid['test'].train_test_split(test_size=0.5,stratify_by_column='class')

# renaming each of the datasets for convenience
train_set = train_testvalid['train']
val_set = valid_test['train']
test_set = valid_test['test']

# converting the tokenized datasets to TensorFlow datasets
data_collator = DefaultDataCollator(return_tensors=&quot;tf&quot;)
tf_train_dataset = train_set.to_tf_dataset(
    columns=[&quot;attention_mask&quot;, &quot;input_ids&quot;, &quot;token_type_ids&quot;],
    label_cols=['class'],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8)
tf_validation_dataset = val_set.to_tf_dataset(
    columns=[&quot;attention_mask&quot;, &quot;input_ids&quot;, &quot;token_type_ids&quot;],
    label_cols=['class'],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8)
tf_test_dataset = test_set.to_tf_dataset(
    columns=[&quot;attention_mask&quot;, &quot;input_ids&quot;, &quot;token_type_ids&quot;],
    label_cols=['class'],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8)

# loading tensorflow model
model = TFAutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;, num_labels=1)

# compiling the model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-6),
    loss=tf.keras.losses.BinaryCrossentropy(),
    metrics=tf.metrics.BinaryAccuracy())

# fitting model
history = model.fit(tf_train_dataset,
          validation_data=tf_validation_dataset,
          epochs=1)

# Evaluating the model on the test data using `evaluate`
results = model.evaluate(x=tf_test_dataset,verbose=2) # reports binary_accuracy: 0.9152

# first attempt at using model.predict method
hits = 0
misses = 0
for x, y in tf_test_dataset:
  logits = tf.keras.backend.get_value(model(x, training=False).logits)
  labels = tf.keras.backend.get_value(y)
  for i in range(len(logits)):
    if logits[i][0] &lt; 0:
      z = 0
    else:
      z = 1
    if z == labels[i]:
      hits += 1
    else:
      misses += 1
print(hits/(hits+misses)) # reports binary_accuracy: 0.8187

# second attempt at using model.predict method
modelPredictions = model.predict(tf_test_dataset).logits
testDataLabels = np.concatenate([y for x, y in tf_test_dataset], axis=0)
hits = 0
misses = 0
for i in range(len(modelPredictions)):
  if modelPredictions[i][0] &gt;= 0:
    z = 1
  else:
    z = 0
  if z == testDataLabels[i]:
    hits += 1
  else:
    misses += 1

print(hits/(hits+misses)) # reports binary_accuracy: 0.8187
</code></pre>
<p>Things I've tried include:</p>
<ol>
<li><p>different loss functions (it's a binary classification problem with the label column of the dataset filled with either a zero or a one for each row);</p>
</li>
<li><p>different ways of unpacking the test dataset and feeding it to model.predict;</p>
</li>
<li><p>altering the 'num_labels' parameter between 1 and 2.</p>
</li>
</ol>
",Training and Model Evaluation,discrepancy result reported tensorflow model evaluate model predict back forth age without able find solution far anywhere huggingface model bert base cased using tensorflow custom dataset tokenized data split data converted data tf dataset format instantiated compiled fit model training behaves expect training validation accuracy go evaluate model test dataset using tf model evaluate model predict result different accuracy reported model evaluate higher le line validation accuracy accuracy reported model predict lower maybe coincidence similar reported training accuracy single epoch fine tuning anyone figure causing include snippet code thing tried include different loss function binary classification problem label column dataset filled either zero one row different way unpacking test dataset feeding model predict altering num label parameter
PyTorch Custom LSTM architecture not learning,"<p>I am building a model to classify news (AG news dataset). The vocab size ~33k with custom embedding layer. I have run this for 20 epochs but the loss and accuracy (1.3 and 26% respec.) is almost constant even at the end of 20th epoch. Can someone please help me with this? Also, am I feeding the correct input to the fc layer? I am using CrossEntropyLoss as the loss function.</p>
<p>Here is my model class:</p>
<pre><code>class NewsClassifier(nn.Module):
  def __init__(self, vocab_weights = None, rnn_type = 'LSTM', vocab_size = len(vocab.vocab), n_classes = 4, embed_size = 300, rnn_units = 512, \
               n_layers = 2, bi_dir = True, rnn_drop = 0.0, padding_index = vocab['&lt;unk&gt;']):
    super().__init__()
    self.rnn_units = rnn_units
    self.n_classes = n_classes
    self.rnn_type = rnn_type
    if vocab_weights:
      self.embedding = nn.Embedding.from_pretrained(torch.as_tensor(vocab_weights))
    else:
      self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx = padding_index)
    if rnn_type == 'LSTM':
      self.rnn = nn.LSTM(embed_size, rnn_units, num_layers = n_layers, bidirectional = bi_dir, dropout = rnn_drop)
    elif rnn_type == 'GRU':
      self.rnn = nn.GRU(embed_size, rnn_units, num_layers = n_layers, bidirectional = bi_dir, dropout = rnn_drop)
    else:
      raise NotImplementError
    self.fc = nn.Linear(2 * rnn_units if bi_dir else rnn_units, self.n_classes)
  
  def forward(self, data, lens):
    x_embed = self.embedding(data) # (padded_lens, batch_size, embed_dim)
    x_packed = pack_padded_sequence(x_embed, lens.cpu(), enforce_sorted = False) #packing sequences and passing to RNN unit
    if self.rnn_type == 'LSTM':
      output_packed, (hidden,cell) = self.rnn(x_packed) #output is packed and cannot be fed to linear layers
    else:
      output_packed, hidden = self.rnn(x_packed) #For GRU there is only hidden state
    #Though n number of layers are stacked the output is always 1
    output_padded, _ = pad_packed_sequence(output_packed) #output is padded to be fed to linear layer (padded_lens, batch size, hidden_units)
    #Picking only the last output --&gt; equivalent to reutrn_sequences = False in Keras
    out_reduced = torch.cat((output_padded[-1, :, : self.rnn_units], output_padded[-1, :, self.rnn_units :]), 1) 
    return self.fc(out_reduced)

model = NewsClassifier()
print(f'The total number of trainable parameters are : {sum(p.numel() for p in model.parameters() if p.requires_grad)}')
</code></pre>
<p>My training function is:</p>
<pre><code>def train(model, iterator = trainDataloader, optimizer = optimizer, loss_fn = criterion):
  e_loss = e_acc = i = 0
  model.train()
  for inputs, leng, labels in iterator:
    inputs, leng, labels = inputs.to(device), leng.to(device), labels.to(device)
    optimizer.zero_grad()
    preds = model(inputs, leng).squeeze(1)
    loss = loss_fn(preds, labels.long())
    acc = accuracy(preds, labels)
    loss.backward()
    optimizer.step()
    e_loss += loss.item()
    e_acc += acc.item()
    i += 1
  return e_loss/i, e_acc/i

def predict(model, iterator = testDataloader, loss_fn = criterion):
  e_loss = e_acc = i = 0
  model.eval()
  with torch.no_grad():
    for inputs, leng, labels in iterator:
      inputs, leng, labels = inputs.to(device), leng.to(device), labels.to(device)
      preds = model(inputs, leng).squeeze(1)
      loss = loss_fn(preds, labels.long())
      acc = accuracy(preds, labels)
      e_loss += loss.item()
      e_acc += acc.item()
      i += 1
  return e_loss/i, e_acc/i

N_EPOCHS = 20

best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):

    start_time = time.time()
    
    train_loss, train_acc = train(model)
    valid_loss, valid_acc = predict(model)
    
    end_time = time.time()

    epoch_mins, epoch_secs = epoch_time(start_time, end_time)
    
    if valid_loss &lt; best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'tut1-model.pt')
    
    print(f'Epoch: {epoch+1:02} / {N_EPOCHS} | Epoch Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')
</code></pre>
",Training and Model Evaluation,pytorch custom lstm architecture learning building model classify news ag news dataset vocab size k custom embedding layer run epoch loss accuracy respec almost constant even end th epoch someone please help also feeding correct input fc layer using crossentropyloss loss function model class training function
How to train a LM model with whole word masking using Pytorch Trainer API,"<p>I am thinking of fine tuning model by training Language Model from scratch. I have couple of basic questions related to this:</p>
<p>I wanted to use whole-word-masking in training LM from scratch. I could not have found how to apply this option using Trainer.</p>
<p>Here is my data-set and code:</p>
<pre><code>text=['I am huggingface fan', 'I love huggingface', ....]
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

trainer = tr.Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_data
)

trainer.train()
</code></pre>
<p>But it doesn’t take into account whole word masking.</p>
<p>How can I use this function to train LM on whole word masking using Pytorch Trainer?</p>
<p>How can I train on larger sequences which are greater than models max-length using Pytorch Trainer?</p>
",Training and Model Evaluation,train lm model whole word masking using pytorch trainer api thinking fine tuning model training language model scratch couple basic question related wanted use whole word masking training lm scratch could found apply option using trainer data set code take account whole word masking use function train lm whole word masking using pytorch trainer train larger sequence greater model max length using pytorch trainer
Tensor Flow Error: required broadcastable shapes when training Variable Auto Encoder for Text Posts,"<p>Good morning,</p>
<p>I'm attempting apply and adapt a variational auto encoder that I found <a href=""https://github.com/NicGian/text_VAE/blob/master/text_VAE_v18.ipynb"" rel=""nofollow noreferrer"">here</a> to a dataset consisting of news headlines. The data will feed into the neural network, but the neural network will not train. I believe the error has to do with the way the loss function is written, but I cannot figure it out. My full repository is available <a href=""https://github.com/PaulKMandal/News-Headline-GAN/blob/main/VAE1.ipynb"" rel=""nofollow noreferrer"">here</a></p>
<pre><code>#Much thanks to NicGian for his VAE code!
#NicGian's Repo can be found here https://github.com/NicGian/text_VAE/blob/master/text_VAE_v18.ipynb
import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow import keras
from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout
#from keras.preprocessing.sequence import pad_sequences
from keras.layers import ELU
from keras.preprocessing.text import Tokenizer
from keras.callbacks import ModelCheckpoint
from keras.optimizers import Adam
from keras import backend as K
from keras.models import Model
from scipy import spatial
import tensorflow as tf
import pandas as pd
import numpy as np
import codecs
import csv
import os

from tensorflow.python.framework.ops import disable_eager_execution
disable_eager_execution()

max_len = 20
batch_size = 100
emb_dim = EMBEDDING_DIM
latent_dim = 32
intermediate_dim = 16
epsilon_std = 1.0
kl_weight = 0.01
num_sampled=500
act = ELU()


x = Input(shape=(max_len,))
#x = tf.random.normal(shape=(max_len,))
x_embed = Embedding(NB_WORDS, emb_dim, weights=[glove_embedding_matrix],
                            input_length=max_len, trainable=False)(x)
h = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat')(x_embed)
#h = Bidirectional(LSTM(intermediate_dim, return_sequences=False), merge_mode='concat')(h)
#h = Dropout(0.2)(h)
#h = Dense(intermediate_dim, activation='linear')(h)
#h = act(h)
#h = Dropout(0.2)(h)
z_mean = Dense(latent_dim)(h)
z_log_var = Dense(latent_dim)(h)

def sampling(args):
    z_mean, z_log_var = args
    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,
                              stddev=epsilon_std)
    return z_mean + K.exp(z_log_var / 2) * epsilon

# note that &quot;output_shape&quot; isn't necessary with the TensorFlow backend
z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])
# we instantiate these layers separately so as to reuse them later
repeated_context = RepeatVector(max_len)
decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)
decoder_mean = Dense(NB_WORDS, activation='linear')#softmax is applied in the seq2seqloss by tf #TimeDistributed()
h_decoded = decoder_h(repeated_context(z))
x_decoded_mean = decoder_mean(h_decoded)


# placeholder loss
def zero_loss(y_true, y_pred):
    return K.zeros_like(y_pred)

# Custom loss layer
class CustomVariationalLayer(Layer):
    def __init__(self, **kwargs):
        self.is_placeholder = True
        super(CustomVariationalLayer, self).__init__(**kwargs)
        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)

    def vae_loss(self, x, x_decoded_mean):
        #xent_loss = K.sum(metrics.categorical_crossentropy(x, x_decoded_mean), axis=-1)
        labels = tf.cast(x, tf.int32)
        xent_loss = K.sum(tfa.seq2seq.sequence_loss(x_decoded_mean, labels, 
                                                     weights=self.target_weights,
                                                     average_across_timesteps=False,
                                                     average_across_batch=False), axis=-1)#,
                                                     #softmax_loss_function=softmax_loss_f), axis=-1)#,
        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
        xent_loss = K.mean(xent_loss)
        kl_loss = K.mean(kl_loss)
        return K.mean(xent_loss + kl_weight * kl_loss)

    def call(self, inputs):
        x = inputs[0]
        x_decoded_mean = inputs[1]
        print(x.shape, x_decoded_mean.shape)
        loss = self.vae_loss(x, x_decoded_mean)
        self.add_loss(loss, inputs=inputs)
        # we don't use this output, but it has to have the correct shape:
        return K.ones_like(x)
    
def kl_loss(x, x_decoded_mean):
    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
    kl_loss = kl_weight * kl_loss
    return kl_loss

loss_layer = CustomVariationalLayer()([x, x_decoded_mean])
vae = Model(x, [loss_layer])
opt = Adam(learning_rate=0.01) 
vae.compile(optimizer='adam', loss=[zero_loss], metrics=[kl_loss])
vae.summary()
</code></pre>
<p>This code then gives the following output:</p>
<pre><code>WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
(None, 20) (100, 20, 10000)
Model: &quot;model_1&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_2 (InputLayer)           [(None, 20)]         0           []                               
                                                                                                  
 embedding_1 (Embedding)        (None, 20, 100)      1000000     ['input_2[0][0]']                
                                                                                                  
 bidirectional_1 (Bidirectional  (None, 32)          14976       ['embedding_1[0][0]']            
 )                                                                                                
                                                                                                  
 dense_3 (Dense)                (None, 32)           1056        ['bidirectional_1[0][0]']        
                                                                                                  
 dense_4 (Dense)                (None, 32)           1056        ['bidirectional_1[0][0]']        
                                                                                                  
 lambda_1 (Lambda)              (100, 32)            0           ['dense_3[0][0]',                
                                                                  'dense_4[0][0]']                
                                                                                                  
 repeat_vector_1 (RepeatVector)  (100, 20, 32)       0           ['lambda_1[0][0]']               
                                                                                                  
 lstm_3 (LSTM)                  (100, 20, 16)        3136        ['repeat_vector_1[0][0]']        
                                                                                                  
 dense_5 (Dense)                (100, 20, 10000)     170000      ['lstm_3[0][0]']                 
                                                                                                  
 custom_variational_layer_1 (Cu  (None, 20)          0           ['input_2[0][0]',                
 stomVariationalLayer)                                            'dense_5[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,190,224
Trainable params: 190,224
Non-trainable params: 1,000,000
__________________________________________________________________________________________________
</code></pre>
<p>When I attempt to train the model, I use the following code:</p>
<pre><code>def create_model_checkpoint(dir, model_name):
    filepath = dir + '/' + model_name + &quot;.h5&quot; 
    directory = os.path.dirname(filepath)
    try:
        os.stat(directory)
    except:
        os.mkdir(directory)
    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)
    return checkpointer

checkpointer = create_model_checkpoint('models', 'vae_seq2seq_test_very_high_std')



vae.fit(x_partial_train, x_partial_train,
     shuffle=True,
     epochs=100,
     batch_size=batch_size,
     validation_data=(x_val, x_val), callbacks=[checkpointer])

#print(K.eval(vae.optimizer.lr))
#K.set_value(vae.optimizer.lr, 0.01)

vae.save('models/vae_lstm.h5')
#vae.load_weights('models/vae_seq2seq_test.h5')
</code></pre>
<p>And get the following error:</p>
<pre><code>Train on 11985 samples, validate on 1500 samples
Epoch 1/100
11800/11985 [============================&gt;.] - ETA: 0s - loss: nan - kl_loss: nan

2022-07-28 11:01:06.923587: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: required broadcastable shapes

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Input In [27], in &lt;cell line: 15&gt;()
      9     return checkpointer
     11 checkpointer = create_model_checkpoint('models', 'vae_seq2seq_test_very_high_std')
---&gt; 15 vae.fit(x_partial_train, x_partial_train,
     16      shuffle=True,
     17      epochs=100,
     18      batch_size=batch_size,
     19      validation_data=(x_val, x_val), callbacks=[checkpointer])
     21 #print(K.eval(vae.optimizer.lr))
     22 #K.set_value(vae.optimizer.lr, 0.01)
     24 vae.save('models/vae_lstm.h5')

File ~/.local/lib/python3.10/site-packages/keras/engine/training_v1.py:776, in Model.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    773 self._check_call_args('fit')
    775 func = self._select_training_loop(x)
--&gt; 776 return func.fit(
    777     self,
    778     x=x,
    779     y=y,
    780     batch_size=batch_size,
    781     epochs=epochs,
    782     verbose=verbose,
    783     callbacks=callbacks,
    784     validation_split=validation_split,
    785     validation_data=validation_data,
    786     shuffle=shuffle,
    787     class_weight=class_weight,
    788     sample_weight=sample_weight,
    789     initial_epoch=initial_epoch,
    790     steps_per_epoch=steps_per_epoch,
    791     validation_steps=validation_steps,
    792     validation_freq=validation_freq,
    793     max_queue_size=max_queue_size,
    794     workers=workers,
    795     use_multiprocessing=use_multiprocessing)

File ~/.local/lib/python3.10/site-packages/keras/engine/training_arrays_v1.py:641, in ArrayLikeTrainingLoop.fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    637     raise ValueError('`validation_steps` should not be specified if '
    638                      '`validation_data` is None.')
    639   val_x, val_y, val_sample_weights = None, None, None
--&gt; 641 return fit_loop(
    642     model,
    643     inputs=x,
    644     targets=y,
    645     sample_weights=sample_weights,
    646     batch_size=batch_size,
    647     epochs=epochs,
    648     verbose=verbose,
    649     callbacks=callbacks,
    650     val_inputs=val_x,
    651     val_targets=val_y,
    652     val_sample_weights=val_sample_weights,
    653     shuffle=shuffle,
    654     initial_epoch=initial_epoch,
    655     steps_per_epoch=steps_per_epoch,
    656     validation_steps=validation_steps,
    657     validation_freq=validation_freq,
    658     steps_name='steps_per_epoch')

File ~/.local/lib/python3.10/site-packages/keras/engine/training_arrays_v1.py:377, in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    374 callbacks._call_batch_hook(mode, 'begin', batch_index, batch_logs)
    376 # Get outputs.
--&gt; 377 batch_outs = f(ins_batch)
    378 if not isinstance(batch_outs, list):
    379   batch_outs = [batch_outs]

File ~/.local/lib/python3.10/site-packages/keras/backend.py:4284, in GraphExecutionFunction.__call__(self, inputs)
   4278 if (self._callable_fn is None or feed_arrays != self._feed_arrays or
   4279     symbol_vals != self._symbol_vals or
   4280     feed_symbols != self._feed_symbols or self.fetches != self._fetches or
   4281     session != self._session):
   4282   self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)
-&gt; 4284 fetched = self._callable_fn(*array_vals,
   4285                             run_metadata=self.run_metadata)
   4286 self._call_fetch_callbacks(fetched[-len(self._fetches):])
   4287 output_structure = tf.nest.pack_sequence_as(
   4288     self._outputs_structure,
   4289     fetched[:len(self.outputs)],
   4290     expand_composites=True)

File ~/.local/lib/python3.10/site-packages/tensorflow/python/client/session.py:1480, in BaseSession._Callable.__call__(self, *args, **kwargs)
   1478 try:
   1479   run_metadata_ptr = tf_session.TF_NewBuffer() if run_metadata else None
-&gt; 1480   ret = tf_session.TF_SessionRunCallable(self._session._session,
   1481                                          self._handle, args,
   1482                                          run_metadata_ptr)
   1483   if run_metadata:
   1484     proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

InvalidArgumentError: 2 root error(s) found.
  (0) INVALID_ARGUMENT: required broadcastable shapes
     [[{{node lambda_1/mul}}]]
     [[metrics_2/kl_loss/Identity/_267]]
  (1) INVALID_ARGUMENT: required broadcastable shapes
     [[{{node lambda_1/mul}}]]
0 successful operations.
0 derived errors ignored.
</code></pre>
<p>I'm honestly at a loss here and have been trying to fix this for the past week, so any help would be much appreciated. Thanks!</p>
",Training and Model Evaluation,tensor flow error required broadcastable shape training variable auto encoder text post good morning attempting apply adapt variational auto encoder found dataset consisting news headline data feed neural network neural network train believe error ha way loss function written figure full repository available code give following output attempt train model use following code get following error honestly loss trying fix past week help would much appreciated thanks
What is the actual use of num_words parameter in keras Tokenizer? How much overall does it affect the accuracy of my model,"<p>In the given line of code <code>tokenizer=Tokenizer(num_words=, oov_token= '&lt;OOV&gt;')</code>, what does the num_words parameter actually do and what to take into consideration before determining the value to assign to it. What will be the effect of assigning a very high value to it and a very low one.</p>
",Training and Model Evaluation,actual use num word parameter kera tokenizer much overall doe affect accuracy model given line code doe num word parameter actually take consideration determining value assign effect assigning high value low one
How to understand the answer_start parameter of Squad dataset for training BERT-QA model + practical implications for creating custom dataset?,"<p>I am in the process of creating a custom dataset to benchmark the accuracy of the <a href=""https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad"" rel=""nofollow noreferrer"">'bert-large-uncased-whole-word-masking-finetuned-squad'</a> model for my domain, to understand if I need to fine-tune further, etc.</p>
<p>When looking at the different Question Answering datasets on the Hugging Face site (<a href=""https://huggingface.co/datasets/squad"" rel=""nofollow noreferrer"">squad</a>, <a href=""https://huggingface.co/datasets/adversarial_qa"" rel=""nofollow noreferrer"">adversarial_qa</a>, etc. ), I see that the answer is commonly formatted as a dictionary with keys: answer (the text) and answer_start (char index where answer starts).</p>
<p>I'm trying to understand:</p>
<ul>
<li>The intuition behind how the model uses the answer_start when calculating the loss, accuracy, etc.</li>
<li>If I need to go through the process of adding this to my custom dataset (easier to run model evaluation code, etc?)</li>
<li>If so, is there a programmatic way to do this to avoid manual effort?</li>
</ul>
<p>Any help or direction would be greatly appreciated!</p>
<p><strong>Code example to show format:</strong></p>
<pre><code>import datasets
ds = datasets.load_dataset('squad')
train = ds['train']
print('Example: \n')
print(train['answers'][0])
</code></pre>
",Training and Model Evaluation,understand answer start parameter squad dataset training bert qa model practical implication creating custom dataset process creating custom dataset benchmark accuracy bert large uncased whole word masking finetuned squad model domain understand need fine tune etc looking different question answering datasets hugging face site squad adversarial qa etc see answer commonly formatted dictionary key answer text answer start char index answer start trying understand intuition behind model us answer start calculating loss accuracy etc need go process adding custom dataset easier run model evaluation code etc programmatic way avoid manual effort help direction would greatly appreciated code example show format
Pinyin packages: accuracy and efficiency,"<p>I am looking to get the pinyin of Simplified Mandarin characters, and have come across two packages:</p>
<ul>
<li><a href=""https://pypi.org/project/pinyin/"" rel=""nofollow noreferrer"">pinyin 0.4.0</a> which is 6 years old (<a href=""https://github.com/lxyu/pinyin"" rel=""nofollow noreferrer"">GitHub repo here</a>)</li>
<li><a href=""https://pypi.org/project/pinyin_jyutping_sentence/"" rel=""nofollow noreferrer"">pinyin_jyutping_sentence</a> which is 2&gt; years old. (<a href=""https://github.com/Language-Tools/python-pinyin-jyutping-sentence"" rel=""nofollow noreferrer"">GitHub repo here</a>)</li>
</ul>
<p>Both offer similar features in terms of the ability to print character pinyin with and without the diacritics, but I am curious if one is more efficient than the other.</p>
<p>Right off the bat, I noticed that on the first <code>import pinyin_jyutping_sentence</code> that the package builds out a <code>Prefix dict</code>:</p>
<pre><code>import pinyin_jyutping_sentence as pnyn
Building prefix dict from Path\to\python\lib\site-packages\pinyin_jyutping_sentence\dict.txt.big ...
Dumping model to file cache Path\to\AppData\Local\Temp\jieba.ue5a383df573783d4e379d21ab891d92a.cache
Loading model cost 0.793 seconds.
Prefix dict has been built successfully.
</code></pre>
<p>Whereas running <code>import pinyin</code> did <em>not</em> result in the creation of any kind of a dictionary.</p>
<p>Is there a difference between the two packages in speed and accuracy?</p>
",Training and Model Evaluation,pinyin package accuracy efficiency looking get pinyin simplified mandarin character come across two package pinyin year old github repo pinyin jyutping sentence year old github repo offer similar feature term ability print character pinyin without diacritic curious one efficient right bat noticed first package build whereas running result creation kind dictionary difference two package speed accuracy
IndicBART seq2seq model gives entirely blank predictions,"<p>I am using IndicBART for sequence-to-sequence prediction on tamil sentences.</p>
<p>I have trained the model on 100,000 samples of tamil data for 30 epochs, then tested predictions on a few sentences. These are the predictions given by the model- I have presented both the token IDs output by the model and the tokens decoded from the token IDs:</p>
<pre><code>source: 'bullet es'
predicted: ''
predicted token IDs: [64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 2]
gold: 'bullet e s'

source: 'இங்க பழச த்துலேந்து பேசுறங்க' 
predicted: ''
predicted token IDs: [64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 2]
gold: 'இங்க palladam துல இருந்து பேசறங்க'
</code></pre>
<p>And so on. All the predictions are blank lines.</p>
<p>Untrained IndicBART is not producing blank predictions, only the trained checkpoint.</p>
<p>Is this a feature of IndicBART, or am I doing something wrong?</p>
",Training and Model Evaluation,indicbart seq seq model give entirely blank prediction using indicbart sequence sequence prediction tamil sentence trained model sample tamil data epoch tested prediction sentence prediction given model presented token id output model token decoded token id prediction blank line untrained indicbart producing blank prediction trained checkpoint feature indicbart something wrong
Overfitting on LSTM text classification using Keras,"<p>I am trying to develop an LSTM model using Keras, following <a href=""https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17"" rel=""nofollow noreferrer"">this tutorial</a>. However, I am implementing it with a different dataset of U.S. political news articles with the aim of classifying them based on a political bias (labels: Left, Centre and Right). I have gotten a model to run with the tutorial, but the loss and accuracy would look very off, like this:</p>
<p><a href=""https://i.sstatic.net/Lx90r.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Lx90r.png"" alt=""enter image description here"" /></a></p>
<p>I tried to play around with different DropOut probabilities (i.e. 0.5 instead of 0.2), adding/removing hidden layers (and making them less dense), and decreasing/increasing the max number of words and max sequence length.</p>
<p>I have managed to get the graphs to align a bit more, however, that has led to the model having less accuracy with the training data (and the problem of overfitting is still bad):
<a href=""https://i.sstatic.net/BifzB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BifzB.png"" alt=""enter image description here"" /></a></p>
<p>Additionally, I am not sure why the validation accuracy always seems to be higher than the model accuracy in the first epoch (shouldn't it usually be lower)?</p>
<p>Here is some code that is being used when tokenizing, padding, and initializing variables:</p>
<pre class=""lang-py prettyprint-override""><code># The maximum number of words to be used. (most frequent)
MAX_NB_WORDS = 500

# Max number of words in each news article
MAX_SEQUENCE_LENGTH = 100 # I am aware this may be too small

# This is fixed.
EMBEDDING_DIM = 64

tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~', 
lower=True)
tokenizer.fit_on_texts(df_raw['titletext'].values)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))


X = tokenizer.texts_to_sequences(df_raw['titletext'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)

Y = pd.get_dummies(df_raw['label']).values
print('Shape of label tensor:', Y.shape)

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

X_train.view()
</code></pre>
<p>When I look at what is shown when <code>X_train.view()</code> is executed, I am also not sure why all the arrays start with zeros like this:</p>
<p><a href=""https://i.sstatic.net/CBZ2v.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CBZ2v.png"" alt=""enter image description here"" /></a></p>
<p>I also did a third attempt that was just a second attempt with the number of epochs increased, it looks like this:</p>
<p><a href=""https://i.sstatic.net/1Iyjp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1Iyjp.png"" alt=""enter image description here"" /></a></p>
<p>Here is the code of the actual model:</p>
<pre><code>model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
# model.add(SpatialDropout1D(0.2)) ---&gt; commented out
# model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) ---&gt; commented out
model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))
model.add(Dropout(0.5))
model.add(Dense(8))
model.add(Dropout(0.5))
model.add(Dense(3, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

epochs = 25
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, 
batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])
</code></pre>
<p>Here is the <a href=""https://colab.research.google.com/drive/1wG00R9Futu3gYedZQP7Eorc-9lnuVGXr?usp=sharing"" rel=""nofollow noreferrer"">link</a> to the full code, including the dataset</p>
<p>Any help would be greatly appreciated!</p>
",Training and Model Evaluation,overfitting lstm text classification using kera trying develop lstm model using kera following tutorial however implementing different dataset u political news article aim classifying based political bias label left centre right gotten model run tutorial loss accuracy would look like tried play around different dropout probability e instead adding removing hidden layer making le dense decreasing increasing max number word max sequence length managed get graph align bit however ha led model le accuracy training data problem overfitting still bad additionally sure validation accuracy always seems higher model accuracy first epoch usually lower code used tokenizing padding initializing variable look shown executed also sure array start zero like also third attempt wa second attempt number epoch increased look like code actual model link full code including dataset help would greatly appreciated
How to train a spacy model by using streaming data?,"<p>I have created a spacy model. But I need to retrain it until it reaches it maximum level. I need to train this model and retrain the model using the streaming data. I have seen that we can train some machine learning model using stream data. Is it possible to do the same to NLP models?</p>
",Training and Model Evaluation,train spacy model using streaming data created spacy model need retrain reach maximum level need train model retrain model using streaming data seen train machine learning model using stream data possible nlp model
How to write a config.json to train a Language model,"<p>Looking at NLP models on Huggingface I can see that each of those has a so called <code>config.json</code> file. Now I want to train a <code>BART</code> mode from scratch along <a href=""https://github.com/octaviaguo/Constrained-Labeled-Data-Generation/tree/main/pipeline_scripts"" rel=""nofollow noreferrer"">this</a> repo. To do so, I have to &quot;write&quot; a <code>config.json</code> myself. Now I can not find any <strong>documentation</strong> what attributes this <code>config.json</code> needs to contain.</p>
<p>The training script from the linked <a href=""https://github.com/octaviaguo/Constrained-Labeled-Data-Generation/tree/main/pipeline_scripts"" rel=""nofollow noreferrer"">repo</a> (./t5_train/t5_train.py) does fail if the provided config does not contain certain attributes. That way I found out what is missing but without docs I am clueless with what I have to set them.</p>
<p>Example what is among the expected attributes that I dont find a documentation for:</p>
<pre><code>training:{
  device: ...,
  optimizer: ...,
  type: ...,
  noise: ...,
  noise_vocab: ...,
}
model:{
  src_lang: ...,
  trg_lang: ...,
  seq2seq: ...,
  dim_word_src: ...,
  n_layers_src: ...,
  bidirectional: ...,
}
</code></pre>
<hr />
<p><strong>Does somebody know where I can find a detailed documentation for how to write a <code>config.json</code> in particular and a <code>config.json</code> for BART in detail?</strong></p>
<hr />
<p>I allready took a look at <a href=""https://huggingface.co/docs/transformers/main_classes/configuration"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/main_classes/configuration</a> but I could not find the attributes that the script expects.</p>
",Training and Model Evaluation,write config json train language model looking nlp model huggingface see ha called file want train mode scratch along repo write find documentation attribute need contain training script linked repo train train py doe fail provided config doe contain certain attribute way found missing without doc clueless set example among expected attribute dont find documentation doe somebody know find detailed documentation write particular bart detail allready took look could find attribute script expects
How to order sentences based on pairwise probabilities?,"<p>I'm generating pairwise sentence order probabilities in the following way:</p>
<pre><code>import itertools
import random

import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertForNextSentencePrediction

np.set_printoptions(suppress=True)
cache_dir = '/path/to/cache/dir'
pretrained_weights = 'bert-base-multilingual-cased'
tokenizer = BertTokenizer.from_pretrained(
pretrained_weights,
cache_dir=cache_dir,
)
model = TFBertForNextSentencePrediction.from_pretrained(
    pretrained_weights,
    cache_dir=cache_dir,
)
sentences = &quot;&quot;&quot;
In “The Necklace” by Guy de Maupassant, the main character, Mathilde, has always dreamed of 
being an aristocrat but lives in poverty. 
Embarrassed about her lack of fine possessions, she borrows a necklace from a wealthy 
friend but loses it. 
The story is known for its subversive and influential twist ending
&quot;&quot;&quot;
sentences = [s.strip() for s in sentences.strip().split('.')]
random.shuffle(sentences)
print(sentences)
pairs = itertools.permutations(sentences, 2)
encoded = tokenizer.batch_encode_plus(pairs, return_tensors='np', padding=True)
outputs = model(encoded)
probs = tf.keras.activations.softmax(outputs[0])
for i, s in enumerate(sentences, 1):
    print(f's{i}: {s}')
for s, prob in zip(itertools.permutations(['s1', 's2', 's3'], 2), probs):
    print(s, prob)
</code></pre>
<p>I'm not sure how to interpret the resulting probabilities to generate the ordered sentences.</p>
<pre><code>s1: Embarrassed about her lack of fine possessions, she borrows a necklace from a wealthy 
friend but loses it
s2: In “The Necklace” by Guy de Maupassant, the main character, Mathilde, has always dreamed of 
being an aristocrat but lives in poverty
s3: The story is known for its subversive and influential twist ending
('s1', 's2') tf.Tensor([0.9987061  0.00129389], shape=(2,), dtype=float32)
('s1', 's3') tf.Tensor([0.9514299  0.04857007], shape=(2,), dtype=float32)
('s2', 's1') tf.Tensor([0.9994491  0.00055089], shape=(2,), dtype=float32)
('s2', 's3') tf.Tensor([0.94130975 0.05869029], shape=(2,), dtype=float32)
('s3', 's1') tf.Tensor([0.15520796 0.84479207], shape=(2,), dtype=float32)
('s3', 's2') tf.Tensor([0.98460925 0.01539072], shape=(2,), dtype=float32)
</code></pre>
<p><strong>Update:</strong> I created this hacky solution based on the one I found <a href=""https://github.com/gioelecrispo/text-reorderer"" rel=""nofollow noreferrer"">here</a> (not sure about its accuracy) which orders sentences however, due to calculating the probabilities for the cartesian product of sentences, the total items to predict increases by a square factor ex: for a set of 88 sentences the total pairs would be 88 * 88 = 7744 which won't scale nicely. The inference speed wouldn't be much of a problem when done on gpu but still, better suggestions of achieving the same result are welcome.</p>
<pre><code>class HashableDict(dict):  #  Passed to `tf.keras.Model.predict` to enable batching
    def __hash__(self):
        return hash(tuple(self.items()))


def create_correlation_matrix(sentences, tokenizer, model, **kwargs):
    np.set_printoptions(suppress=True)
    pairs = itertools.product(sentences, repeat=2)
    encoded = tokenizer.batch_encode_plus(pairs, return_tensors='np', padding=True)
    logits = model.predict(HashableDict(**encoded), **kwargs)
    probs = tf.keras.activations.softmax(tf.convert_to_tensor(logits[0]))
    size = len(sentences)
    return probs[:, 0].numpy().reshape(size, size)


def reorder_sentences(sentences, tokenizer, model, **kwargs):
    ordered = []
    correlation_matrix = create_correlation_matrix(
        sentences, tokenizer, model, **kwargs
    )
    idx = np.unravel_index(
        np.argmax(correlation_matrix, axis=None), correlation_matrix.shape
    )
    while correlation_matrix.any():
        x_idx = idx[1]
        correlation_matrix[idx[0], :] = 0
        correlation_matrix[:, idx[0]] = 0
        ordered.append(idx[0])
        idx = np.unravel_index(
            np.argmax(correlation_matrix[x_idx, :], axis=None),
            correlation_matrix[x_idx, :].shape,
        )
        idx = (x_idx, idx[0])
    return ordered


if __name__ == '__main__':
    cache_dir = '/path/to/cache/dir'
    pretrained_weights = 'bert-base-multilingual-cased'
    tok = BertTokenizer.from_pretrained(
        pretrained_weights,
        cache_dir=cache_dir,
    )
    m = TFBertForNextSentencePrediction.from_pretrained(
        pretrained_weights,
        cache_dir=cache_dir,
    )
    s = &quot;&quot;&quot;
    In “The Necklace” by Guy de Maupassant, the main character, Mathilde, has always dreamed of 
    being an aristocrat but lives in poverty. 
    Embarrassed about her lack of fine possessions, she borrows a necklace from a wealthy 
    friend but loses it. 
    The story is known for its subversive and influential twist ending
    &quot;&quot;&quot;
    s = [ss.strip() for ss in s.strip().split('.')]
    print(s)
    random.shuffle(s)
    print(s)
    ordering = reorder_sentences(s, tok, m, verbose=True, batch_size=8)
    reordered_sentences = [s[idx] for idx in ordering]
    print(ordering, reordered_sentences)
</code></pre>
<p>results in:</p>
<pre><code>2/2 [==============================] - 9s 198ms/step
[1, 0, 2] ['In “The Necklace” by Guy de Maupassant, the main character, Mathilde, has always dreamed of \n    being an aristocrat but lives in poverty', 'Embarrassed about her lack of fine possessions, she borrows a necklace from a wealthy \n    friend but loses it', 'The story is known for its subversive and influential twist ending']
</code></pre>
",Training and Model Evaluation,order sentence based pairwise probability generating pairwise sentence order probability following way sure interpret resulting probability generate ordered sentence update created hacky solution based one found sure accuracy order sentence however due calculating probability cartesian product sentence total item predict increase square factor ex set sentence total pair would scale nicely inference speed much problem done gpu still better suggestion achieving result welcome result
When should I train my own models and when should I use pretrained models?,"<p>Is it recommended to train my own models for things like sentiment analysis, despite only having a very small dataset (5000 reviews), or is it best to use pretrained models which were trained on way larger datasets, however aren't &quot;specialized&quot; on my data.</p>
<p>Also, how could I train my model on my data and then later use it on it too? I was thinking of an iterative approach where the training data would be randomly selected subset of my total data for each learning epoch.</p>
",Training and Model Evaluation,train model use pretrained model recommended train model thing like sentiment analysis despite small dataset review best use pretrained model trained way larger datasets however specialized data also could train model data later use wa thinking iterative approach training data would randomly selected subset total data learning epoch
Rouge scores are different when using package &quot;datasets&quot; and &quot;rouge_score&quot;,"<p>I use 2 packages, &quot;datasets&quot; and &quot;rouge_score&quot; to get the rouge-1 scores. However, the precision and recall are different. I wonder which package produces the correct scores?</p>
<pre><code>from rouge_score import rouge_scorer
import datasets

hyp = ['I have no car.']
ref = ['I want to buy a car.']

scorer1 = datasets.load_metric('rouge')
scorer2 = rouge_scorer.RougeScorer(['rouge1'])

results = {'precision_rouge_score': [], 'recall_rouge_score': [], 'fmeasure_rouge_score': [], \
           'precision_datasets': [], 'recall_datasets': [], 'fmeasure_datasets': []}

for (h, r) in zip(hyp, ref):

    precision, recall, fmeasure = scorer2.score(h, r)['rouge1']
    results['precision_rouge_score'].append(precision)
    results['recall_rouge_score'].append(recall)
    results['fmeasure_rouge_score'].append(fmeasure)

    output = scorer1.compute(predictions=[h], references=[r])
    results['precision_datasets'].append(output['rouge1'].mid.precision)
    results['recall_datasets'].append(output['rouge1'].mid.recall)
    results['fmeasure_datasets'].append(output['rouge1'].mid.fmeasure)

print('results: ', results)
</code></pre>
<p>The results are:</p>
<pre><code>{'precision_rouge_score': [0.3333333333333333], 'recall_rouge_score': [0.5], 
'fmeasure_rouge_score': [0.4],
'precision_datasets': [0.5], 'recall_datasets': [0.3333333333333333],
'fmeasure_datasets': [0.4]}
</code></pre>
",Training and Model Evaluation,rouge score different using package datasets rouge score use package datasets rouge score get rouge score however precision recall different wonder package produce correct score result
Analysing words in dataset based on training data,"<p>I have a training dataset for eg.</p>
<pre><code>Letter    Word
A         Apple
B         Bat
C         Cat
D         Dog
E         Elephant
</code></pre>
<p>and I need to check the dataframe such as
AD    Apple Dog
AE    Applet Elephant
DC    Dog Cow
EB    Elephant Bag</p>
<p>the instances <code>AD,AE,EB</code> are almost accurate (Apple and Applet are considered closer to each other, similar for Bat and Bag) but <code>DC</code> doesn't match.</p>
<p>Output Required:</p>
<pre><code>Letters    Words               Status
AD         Apple Dog           Accept
AE         Applet Elephant     Accept
DC         Dog Cow             Reject
EB         Elephant Bag        Accept
</code></pre>
<p>How can I find these matches using Python.</p>
",Training and Model Evaluation,analysing word dataset based training data training dataset eg need check dataframe ad apple dog ae applet elephant dc dog cow eb elephant bag instance almost accurate apple applet considered closer similar bat bag match output required find match using python
Detect yes/no answers using a pre-trained NLP model,"<p>I'm making a chatbot where at the end of the process, it asks the user for confirmation, and the user would supposedly reply with yes/no-like responses (&quot;Sure&quot;, &quot;Why not?&quot;, &quot;Nope&quot;, &quot;Let's do it&quot;, ...etc).</p>
<p>I'm looking for a pre-trained model to do that for me as I don't want to train my own for various reasons.</p>
<p>Do you have any suggestions ? Is there a library of pre-trained models that I can browse for this ?</p>
",Training and Model Evaluation,detect yes answer using pre trained nlp model making chatbot end process asks user confirmation user would supposedly reply yes like response sure nope let etc looking pre trained model want train various reason suggestion library pre trained model browse
Multiple BERT binary classifications on a single graph to save on inference time,"<p>I have five classes and I want to compare four of them against <strong>one and the same class</strong>. This isn't a One vs Rest classifier, as for each output I want to score them against one base class.</p>
<p>The four outputs should be: <code>base class vs classA</code>, <code>base class vs classB</code>, etc.</p>
<p>I could do this by having multiple binary classification tasks, but that's wasting computation time if the first layers are BERT preprocessing + pretrained BERT layers, and the only differences between the four classifiers are the last few layers of BERT (finetuned ones) and the Dense layer.</p>
<p><strong>So why not merge the graphs for more performance?</strong></p>
<p><a href=""https://i.sstatic.net/7nflM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7nflM.png"" alt="""" /></a></p>
<p>My inputs are four different datasets, each annotated with true/false for each class.</p>
<p><strong>As I understand it, I can re-use most of the pipeline (BERT preprocessing and the first layers of BERT)</strong>, as those have shared weights. I should then be able to train the last few layers of BERT and the Dense layer on top differently depending on the branch of the classifier (maybe using something like keras.switch?).</p>
<p>I have tried many alternative options including multi-class and multi-label classifiers, with actual and generated (eg, machine-annotated) labels in the case of multiple input labels, different activation and loss functions, but none of the results were acceptable to me (none were as good as the four separate models).</p>
<p>Is there a solution for merging the four different models for more performance, or am I stuck with using 4x binary classifiers?</p>
",Training and Model Evaluation,multiple bert binary classification single graph save inference time five class want compare four one class one v rest classifier output want score one base class four output etc could multiple binary classification task computation time first layer bert preprocessing pretrained bert layer difference four classifier last layer bert finetuned one dense layer merge graph performance input four different datasets annotated true false class understand use pipeline bert preprocessing first layer bert shared weight able train last layer bert dense layer top differently depending branch classifier maybe using something like kera switch tried many alternative option including multi class multi label classifier actual generated eg machine annotated label case multiple input label different activation loss function none result acceptable none good four separate model solution merging four different model performance stuck using x binary classifier
pytorch nllloss function target shape mismatch,"<p>I'm training a LSTM model using pytorch with batch size of 256 and NLLLoss() as loss function.
The loss function is having problem with the data shape.</p>

<p>The softmax output from the forward passing has shape of <code>torch.Size([256, 4, 1181])</code> where 256 is batch size, 4 is sequence length, and 1181 is vocab size.</p>

<p>The target is in the shape of <code>torch.Size([256, 4])</code> where 256 is batch size and 4 is the output sequence length.</p>

<p>When I was testing earlier with batch size of 1, the model works fine but when I add batch size, it is breaking. I read that NLLLoss() can take class target as input instead of one hot encoded target.</p>

<p>Am I misunderstanding it? Or did I not format the shape of the target correctly?</p>

<pre><code>class LSTM(nn.Module):

    def __init__(self, embed_size=100, hidden_size=100, vocab_size=1181, embedding_matrix=...):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.word_embeddings = nn.Embedding(vocab_size, embed_size)
        self.word_embeddings.load_state_dict({'weight': torch.Tensor(embedding_matrix)})
        self.word_embeddings.weight.requires_grad = False
        self.lstm = nn.LSTM(embed_size, hidden_size)
        self.hidden2out = nn.Linear(hidden_size, vocab_size)


    def forward(self, tokens):
        batch_size, num_steps = tokens.shape
        embeds = self.word_embeddings(tokens)
        lstm_out, _ = self.lstm(embeds.view(batch_size, num_steps, -1))
        out_space = self.hidden2out(lstm_out.view(batch_size, num_steps, -1))
        out_scores = F.log_softmax(out_space, dim=1)
        return out_scores

model = LSTM(self.config.embed_size, self.config.hidden_size, self.config.vocab_size, self.embedding_matrix)
loss_function = nn.NLLLoss()
optimizer = optim.Adam(model.parameters(), lr=self.config.lr)
</code></pre>

<p>Error:</p>

<pre><code>~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)
   1846         if target.size()[1:] != input.size()[2:]:
   1847             raise ValueError('Expected target size {}, got {}'.format(
-&gt; 1848                 out_size, target.size()))
   1849         input = input.contiguous().view(n, c, 1, -1)
   1850         target = target.contiguous().view(n, 1, -1)

ValueError: Expected target size (256, 554), got torch.Size([256, 4])
</code></pre>
",Training and Model Evaluation,pytorch nllloss function target shape mismatch training lstm model using pytorch batch size nllloss loss function loss function problem data shape softmax output forward passing ha shape batch size sequence length vocab size target shape batch size output sequence length wa testing earlier batch size model work fine add batch size breaking read nllloss take class target input instead one hot encoded target misunderstanding format shape target correctly error
Is there a machine learning or NLP model to separate questions and answers in raw text?,"<p>I have some raw text that has questions and answers in it.  I would like to identify which parts of the text are questions and which parts are the answers.  This seems like it would be easy, but the questions aren't necessarily terminated with question marks. The only thing I know for sure is that after a question is over the answer begins, and after the answer is over another question begins, but there is no consistent format on how many \n are included in the answers.  A question is definitely its own paragraph though.</p>
<p>I'm hoping for some sort of pre-trained model for this?</p>
<p>One possibility would be to take some existing data, manually tag each paragraph as q vs a and then use google's universal sentence encoder for each paragraph to get the 512 dimension output and then use that as the input to train a neural net or some other classification model on the labeled data.  I'm hoping to avoid this path because I don't want to manually tag a few thousand paragraphs, and after all that work, who knows if the model will have a decent classification error.</p>
<p>Another possibility is to use something like gpt3: feed it the entire text and just ask it what are the questions/requests.  The problem with this is that the gpt3 api is still a bit sandboxed.  I tried a sample on the gpt3 playground and it only identified 80% of the questions.</p>
<p>Any other suggestions?</p>
<p>To give you an idea, the text may look like this:</p>
<p>What is the name of the company?</p>
<p>We are Acme Inc.</p>
<p>How many employees are there.</p>
<p>There are 50 employees.</p>
<p>Describe a day in the life of an employee.</p>
<p>An employee arrives at 9am.</p>
<p>Then they go to the factory and make widgets for 4 hours.  After making widgets they eat lunch and then go to the QA engineer to make sure their widgets are good enough.</p>
<p>After QA, they write a report about how many widgets they made.</p>
<p>Most employees leave around 5pm.</p>
<p>List the pay range of your employees.</p>
<p>The starting salary is $22/hours.</p>
<p>After 1 year pay increases to $25 an hour and then increases 3% per year.</p>
<p>Contact information:</p>
<p>Acme Inc</p>
<p>123 Main Street</p>
<p>Anyplace, USA</p>
",Training and Model Evaluation,machine learning nlp model separate question answer raw text raw text ha question answer would like identify part text question part answer seems like would easy question necessarily terminated question mark thing know sure question answer begin answer another question begin consistent format many n included answer question definitely paragraph though hoping sort pre trained model one possibility would take existing data manually tag paragraph q v use google universal sentence encoder paragraph get dimension output use input train neural net classification model labeled data hoping avoid path want manually tag thousand paragraph work know model decent classification error another possibility use something like gpt feed entire text ask question request problem gpt api still bit sandboxed tried sample gpt playground identified question suggestion give idea text may look like name company acme inc many employee employee describe day life employee employee arrives go factory make widget hour making widget eat lunch go qa engineer make sure widget good enough qa write report many widget made employee leave around pm list pay range employee starting salary hour year pay increase hour increase per year contact information acme inc main street anyplace usa
Model not training and generations messed up,"<p>I am trying to train an LSTM Encoder-Decoder model for paraphrase generation. My model is as follows:</p>
<pre><code>StackedResidualLSTM(
  (encoder): RecurrentEncoder(
    (embed_tokens): Embedding(30522, 256)
    (dropout): Dropout(p=0.5, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.5)
  )
  (decoder): RecurrentDecoder(
    (embed_tokens): Embedding(30522, 128)
    (dropout_in_module): Dropout(p=0.5, inplace=False)
    (dropout_out_module): Dropout(p=0.1, inplace=False)
    (layers): ModuleList(
      (0): LSTMCell(384, 256)
      (1): LSTMCell(256, 256)
    )
    (fc_out): Linear(in_features=256, out_features=30522, bias=True)
  )
)
</code></pre>
<p>Following is a print of the source sentence, the sentence fed to the decoder (shifted right), the predictions, and the true sentence (labels):</p>
<blockquote>
<p>Source: [CLS] where can i get quality services in brisbane for plaster
and drywall repair? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
[PAD] [PAD] [PAD] [PAD]</p>
<p>Decoder Input: [CLS] [CLS] where can i get
quality services for plaster and drywall repairs in brisbane? [SEP]
[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]</p>
<p>Preds:
[CLS] the? [SEP]? [SEP]? [SEP]? [SEP]? [SEP]? [SEP]? [SEP]? [SEP]?
[SEP]</p>
<p>Target: [CLS] where can i get quality services for plaster and
drywall repairs in brisbane? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
[PAD] [PAD] [PAD] [PAD] [PAD]</p>
</blockquote>
<p>My loss function is a CrossEntropy between the output and labels (the padding token is switched with -100 to ignore). Something like:</p>
<pre><code>loss_fct = CrossEntropyLoss()
loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))
</code></pre>
<p>I am using Torch-Ignite to handle the training loop. It goes as follows:</p>
<pre><code>model.train()
accumulation_steps = cfg.train.get(&quot;accumulation_steps&quot;, 1)

if batch[&quot;labels&quot;].device != device:
      batch = {k: v.to(device) for (k, v) in batch.items()}

      input_ids = batch[&quot;input_ids&quot;]
      labels = batch[&quot;labels&quot;]

      loss = model(input_ids=input_ids, labels=labels)[0]
      loss /= accumulation_steps

      scaler.scale(loss).backward()
      if engine.state.iteration % accumulation_steps == 0:
          scaler.step(optimizer)
          scaler.update()
          optimizer.zero_grad()

      return {&quot;batch loss&quot;: loss.item()}
</code></pre>
<p>The loss is computed inside the model class as described above.</p>
<p>There are two problems occurring:</p>
<ul>
<li>the loss does not go down</li>
<li>the generations are all the same for every entry of the same epoch (after weight updating the generations might be different than the ones from the previous epoch, but remain the same for every entry of the new epoch)</li>
</ul>
<p>Do you have any idea what might I try to fix the issue? Thanks in advance for any help you can provide. The whole code can be found in my <a href=""https://github.com/AfonsoSalgadoSousa/catbird"" rel=""nofollow noreferrer"">GitHub repository</a>.</p>
",Training and Model Evaluation,model training generation messed trying train lstm encoder decoder model paraphrase generation model follows following print source sentence sentence fed decoder shifted right prediction true sentence label source cl get quality service brisbane plaster drywall repair sep pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad decoder input cl cl get quality service plaster drywall repair brisbane sep pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad preds cl sep sep sep sep sep sep sep sep sep target cl get quality service plaster drywall repair brisbane sep pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad loss function crossentropy output label padding token switched ignore something like using torch ignite handle training loop go follows loss computed inside model class described two problem occurring loss doe go generation every entry epoch weight updating generation might different one previous epoch remain every entry new epoch idea might try fix issue thanks advance help provide whole code found github repository
How to train spacy trainned model,"<p>Is it possible to train spacy trained model such as model-best, model-last. If it is possible please tell me how to pretrain my model. Thank you</p>
",Training and Model Evaluation,train spacy trainned model possible train spacy trained model model best model last possible please tell pretrain model thank
NLTK language modeling confusion,"<p>I want to train a language model using NLTK in python but I got into several problems.
first of all, I don't know why my words turn into just characters as I write something like this :</p>

<pre><code>s = ""Natural-language processing (NLP) is an area of computer science "" \
""and artificial intelligence concerned with the interactions "" \
""between computers and human (natural) languages.""
s = s.lower();


paddedLine = pad_both_ends(word_tokenize(s),n=2);

train, vocab = padded_everygram_pipeline(2, paddedLine)
print(list(vocab))
lm = MLE(2);
lm.fit(train,vocab)
</code></pre>

<p>and the printed vocab is something like this that is clearly not correct(i don't want to work with characters!),this is part of output.:</p>

<pre><code>&lt;s&gt;', '&lt;', 's', '&gt;', '&lt;/s&gt;', '&lt;s&gt;', 'n', 'a', 't', 'u', 'r', 'a', 'l', '-', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', '&lt;/s&gt;', '&lt;s&gt;', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', '&lt;/s&gt;', '&lt;s&gt;', '(', '&lt;/s&gt;', '&lt;s&gt;', 'n', 'l', 'p', '&lt;/s&gt;', '&lt;s&gt;', ')', '&lt;/s&gt;'
</code></pre>

<p>why my input turns into characters?
i did this work in another way but with no luck :</p>

<pre><code>paddedLine = pad_both_ends(word_tokenize(s),n=2);
#train, vocab = padded_everygram_pipeline(2, tokens)
#train = everygrams(paddedLine,max_len = 2);

train = ngrams(paddedLine,2);
vocab = Vocabulary(paddedLine,unk_cutoff = 1);
print(list(train))

lm = MLE(2);
lm.fit(train,vocab)
</code></pre>

<p>when i run this code my train is absolute nothing,empty! it shows me ""[]"" !!
wired thing is when i comment at this line from above code:</p>

<pre><code>vocab = Vocabulary(paddedLine,unk_cutoff = 1);
</code></pre>

<p>now my train data is ok and something like this that is correct :</p>

<pre><code>[('&lt;s&gt;', 'natural-language'), ('natural-language', 'processing'), ('processing', '('), ('(', 'nlp'), ('nlp', ')'), (')', 'is'), ('is', 'an'), ('an', 'area'), ('area', 'of'), ('of', 'computer'), ('computer', 'science'), ('science', 'and'), ('and', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'concerned'), ('concerned', 'with'), ('with', 'the'), ('the', 'interactions'), ('interactions', 'between'), ('between', 'computers'), ('computers', 'and'), ('and', 'human'), ('human', '('), ('(', 'natural'), ('natural', ')'), (')', 'languages'), ('languages', '.'), ('.', '&lt;/s&gt;')]
</code></pre>

<p>whats wrong with it? 
by the way, I have to say that I'm not an expert in python or NLTK and it's my first experience.
The next question is how can I use kneser-ney smoothing or add-one smoothing on the training language model? 
and am I doing language model training the right way?
my training data is simple :</p>

<pre><code>""Natural-language processing (NLP) is an area of computer science "" \
    ""and artificial intelligence concerned with the interactions "" \
    ""between computers and human (natural) languages.""
</code></pre>

<p>thanks.</p>
",Training and Model Evaluation,nltk language modeling confusion want train language model using nltk python got several problem first know word turn character write something like printed vocab something like clearly correct want work character part output input turn character work another way luck run code train absolute nothing empty show wired thing comment line code train data ok something like correct whats wrong way say expert python nltk first experience next question use kneser ney smoothing add one smoothing training language model language model training right way training data simple thanks
SBERT&#39;s Interpretability: can we better understand the final results?,"<p>I'm familiar with SBERT and its pre-trained models and they are amazing! But at the same time, I want to understand how the results are calculated, and I can't find anything more specific in their <a href=""https://www.sbert.net/docs/pretrained_models.html"" rel=""nofollow noreferrer"">website</a>.
For example, I have a document and I want to find other documents that are similar to it. I used 2 documents containing 200-250 words each (I changed the model.max_seq_length to 350 so the model can handle bigger texts), and in the end we can see that the cosine-similarity is 0.79. Is that all we can see? Is there a way to extract the main phrases/keywords that made the model return this high value of similarity?</p>
<p>Thanks in advance!</p>
",Training and Model Evaluation,sbert interpretability better understand final result familiar sbert pre trained model amazing time want understand result calculated find anything specific website example document want find document similar used document containing word changed model max seq length model handle bigger text end see cosine similarity see way extract main phrase keywords made model return high value similarity thanks advance
How to make and train a Model which read data after extracting pdf,"<p>Here i share my code</p>
<blockquote>
<p>main.py</p>
</blockquote>
<pre><code>from fitz import fitz
import spacy


location = &quot;D:\python\Resume-Sample.pdf&quot;
text = ''

with fitz.open(str(location)) as doc:
    for page in doc:
        text+=page.get_text(&quot;block&quot;)

    NER = spacy.load(&quot;en_core_web_sm&quot;)

text1 = NER(text)
for word in text1.ents:
    print(word.text, word.label_)
</code></pre>
<blockquote>
<p>Result</p>
</blockquote>
<p>:Abdul Moeez :E-mail- amoeez14@gmail.com : Phone +1111111111 : Address Karachi, Sindh, Pakistan</p>
<p>How i make and train a model so it recognizes Name Email Phone Address</p>
",Training and Model Evaluation,make train model read data extracting pdf share code main py result abdul moeez e mail amoeez gmail com phone address karachi sindh pakistan make train model recognizes name email phone address
Knowledgebase faq fallback issue google dialogflow,"<p>I have set a default fallback intent which is triggered for all the questions that are designed as intents but for the knowledgebase questions fallback, intent is not getting triggered.</p>
<p>It just shows no answer present.</p>
<p><a href=""https://i.sstatic.net/S9m5p.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/S9m5p.png"" alt=""FAQ Knowledgebase"" /></a></p>
<p><a href=""https://i.sstatic.net/1wxjt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1wxjt.png"" alt=""Fallback Intent"" /></a></p>
<p>I want to know if I have set classification threshold value to 0.8 how are some of the faq intents triggered with 0.5 accuracy</p>
",Training and Model Evaluation,knowledgebase faq fallback issue google dialogflow set default fallback intent triggered question designed intent knowledgebase question fallback intent getting triggered show answer present want know set classification threshold value faq intent triggered accuracy
Spacy 3.1 NLP is not working after deploying on Azure app service,"<p>Working on a project where I need to train a model using Spacy 3.1, All is working as expected on the local system, but when I deploy it on the Azure app service training model is not working [seems to be like it got stuck]. If anyone has faced a similar kind of issue please guide me?</p>
",Training and Model Evaluation,spacy nlp working deploying azure app service working project need train model using spacy working expected local system deploy azure app service training model working seems like got stuck anyone ha faced similar kind issue please guide
How can I count entities by their label for precision and recall,"<p>I have some data which are like this:</p>
<pre><code>           True         Predicted

A          M            M
Pizza      B-Food       B-Food
with       I-Food       I-Food
Peppers    I-Food       I-Food   [correct prediction]
And        M            En
Cuba       B-Drink      B-Drink  
Libre      I-Drink      B-Drink  [wrong prediction]
</code></pre>
<p>I want to count those entities in order to find True Positive, False Positive and False Negative for precision and recall calculation. I'm very confused on how can I compare those tags since an entity must be composed of a sequence of tags. In this example an entity is Pizza with Peppers which is tagged as B-Food, I-Food, I-Food so as u can see this match with the prediction and It can be considered a True Positive for the -Food tag.</p>
<p>I read that precision is TP / (TP + FP) and recall is TP / (TP + FN) but I can't understand how to store those values for the tags in order to computer precision and recall. I can't use any libraries, I have to compare, count and compute.</p>
<pre><code>def precision_recall(true_tags, predicted_tags):

    # storing all the true and predicted tags for comparing
    t_tags = [pair[1] for pair in true_tags]
    p_tags = [pair[1] for pair in predicted_tags]

    #compare here
</code></pre>
<p>In the end I want a thing like this:</p>
<pre><code>        Precision    Recall
Food      0.810      0.123
Loc       0.340      0.723
Drink     0.114      0.654 
</code></pre>
",Training and Model Evaluation,count entity label precision recall data like want count entity order find true positive false positive false negative precision recall calculation confused compare tag since entity must composed sequence tag example entity pizza pepper tagged b food food food u see match prediction considered true positive food tag read precision tp tp fp recall tp tp fn understand store value tag order computer precision recall use library compare count compute end want thing like
"I try to use BertForSequenceClassification for binary sentiment analysis task, but during the evaluating process, all the logits all the same","<p>It seems all the things all right during the training process, and every 10 step i try to evaluate the model by using dev_data_set, the model give every the sample of the batch the same logits.</p>
<p>This is my code:</p>
<pre><code>&lt;!-- language: lang-python --&gt;

       def train(self):
        params = [{'params' : self.model.parameters()}]
        optimizer = torch.optim.Adam(params, lr=self.args.lr, weight_decay=self.args.weight_decay)
        my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=self.args.decay_rate)
        best_eval_loss = 1e9
        for epoch in range(int(self.args.epochs)):
            total_train_loss = 0
            step = 0
            train_batch_num = 0
            for batch_index, batch in tqdm(enumerate(self.train_loader)):
                self.model.train()
                step += 1
                train_batch_num += 1
                # optimizer.zero_grad()
                sources = list(batch[0])
                label = batch[1]
            
                inputs = self.tokenizer(sources,return_tensors='pt', padding=True).to(self.device)
                labels = torch.tensor(label).to(self.device)
                labels = labels.squeeze(0)

                logits = self.model(**inputs, labels=labels).logits

                loss = self.model(**inputs, labels=labels).loss
                
                # logits = outputs.logits
                # loss = self.loss(logits , labels)

                # print('train loss per step:', loss.item())
                total_train_loss += loss.item()

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
            
                if step % 10 == 0:
                    total_dev_loss = 0
                    self.model.eval()
                    dev_num = 0
                    with torch.no_grad():
                        for batch_index, batch in tqdm(enumerate(self.dev_loader)):
                            sources = list(batch[0])
                            label = batch[1]
                            dev_num += 1
                            

                            inputs = self.tokenizer(sources,return_tensors='pt', padding=True).to(self.device)
                            labels = torch.tensor(label).to(self.device)
                            labels = labels.squeeze(0)

                            logits = self.model(**inputs, labels=labels).logits

                            loss = self.model(**inputs, labels=labels).loss

</code></pre>
<p>During the training process:
the logits are:
<strong>training logits:  tensor([[-0.3385, -1.0671],
[-0.1526, -1.0956]], device='cuda:0', grad_fn=)</strong> . It seems all right.</p>
<p>But after pass the line :  <strong>if step % 10 == 0:</strong>,
no matter what the data is, for example:</p>
<p><a href=""https://i.sstatic.net/eAVId.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>or
<a href=""https://i.sstatic.net/DDdpz.png"" rel=""nofollow noreferrer"">enter image description here</a>
the eval logits become the same:</p>
<ul>
<li><p>eval logits:  tensor([[-0.4534, -0.9629],
[-0.4533, -0.9628]], device='cuda:0')</p>
</li>
<li><p>eval logits:  tensor([[-0.4534, -0.9629],
[-0.4534, -0.9630]], device='cuda:0')</p>
</li>
</ul>
",Training and Model Evaluation,try use bertforsequenceclassification binary sentiment analysis task evaluating process logits seems thing right training process every step try evaluate model using dev data set model give every sample batch logits code training process logits training logits tensor device cuda grad fn seems right pas line step matter data example enter image description enter image description eval logits become eval logits tensor device cuda eval logits tensor device cuda
sklearn.pipeline.Pipeline: Fitting CountVectorizer in different corpus than training text,"<p>I am going through the <strong>Sample pipeline for text feature extraction and evaluation</strong> <a href=""https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html"" rel=""nofollow noreferrer"">example</a> from the <code>scikit-learn</code> documentation. In there, they show the following pipeline</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.linear_model import SGDClassifier

pipeline = Pipeline(
    [
        (&quot;vect&quot;, CountVectorizer()),
        (&quot;tfidf&quot;, TfidfTransformer()),
        (&quot;clf&quot;, SGDClassifier()),
    ]
)
</code></pre>
<p>which they later proceed to use with <code>GridSearchCV</code>. In the example they fit the <code>CountVectorizer</code> on the training dataset and then extract the features. What I am looking to do is to fit the <code>CountVectorizer</code> on a bigger corpus and then apply it to the training data to obtain the feature vectors. Is there a straightforward way of doing so while maintaining the <code>sklearn.pipeline.Pipeline</code> API i.e., without subclassing <code>sklearn.pipeline.Pipeline</code> and significantly changing its methods?</p>
<p>I want to maintain the <code>sklearn.pipeline.Pipeline</code> API as I am looking to make use of <code>GridSearchCV</code> and having it structured in this manner will be quite convenient and clean.</p>
",Training and Model Evaluation,sklearn pipeline pipeline fitting countvectorizer different corpus training text going sample pipeline text feature extraction evaluation example documentation show following pipeline later proceed use example fit training dataset extract feature looking fit bigger corpus apply training data obtain feature vector straightforward way maintaining api e without subclassing significantly changing method want maintain api looking make use structured manner quite convenient clean
Word2Vec + LSTM Good Training and Validation but Poor on Test,"<p>currently I'am training my Word2Vec + LSTM for Twitter sentiment analysis. I use the pre-trained GoogleNewsVectorNegative300 word embedding. The reason I used the pre-trained GoogleNewsVectorNegative300 because the performance much worse when I trained my own Word2Vec using own dataset. The problem is why my training process had validation acc and loss stuck at 0.88 and 0.34 respectively. Then, my confussion matrix also seems wrong. Here several processes that I have done before fitting the model</p>
<p>Text Pre processing:</p>
<ol>
<li>Lower casing</li>
<li>Remove hashtag, mentions, URLs, numbers, change words to numbers, non-ASCII characters, retweets &quot;RT&quot;</li>
<li>Expand contractions</li>
<li>Replace negations with antonyms</li>
<li>Remove puncutations</li>
<li>Remove stopwords</li>
<li>Lemmatization</li>
</ol>
<p>I split my dataset into 90:10 for train:test as follows:</p>
<pre><code>def split_data(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, 
                                                        y,
                                                        train_size=0.9, 
                                                        test_size=0.1, 
                                                        stratify=y,
                                                        random_state=0)
    return X_train, X_test, y_train, y_test
</code></pre>
<p>The split data resulting in training has 2060 samples with 708 positive sentiment class, 837 negative sentiment class, and 515 sentiment neutral class</p>
<p>Then, I implemented the text augmentation that is EDA (Easy Data Augmentation) on all the training data as follows:</p>
<pre><code>class TextAugmentation:
    def __init__(self):
        self.augmenter = EDA()

    def replace_synonym(self, text):
        augmented_text_portion = int(len(text)*0.1) 
        synonym_replaced = self.augmenter.synonym_replacement(text, n=augmented_text_portion)
        return synonym_replaced

    def random_insert(self, text):
        augmented_text_portion = int(len(text)*0.1) 
        random_inserted = self.augmenter.random_insertion(text, n=augmented_text_portion)
        return random_inserted

    def random_swap(self, text):
        augmented_text_portion = int(len(text)*0.1)
        random_swaped = self.augmenter.random_swap(text, n=augmented_text_portion)
        return random_swaped

    def random_delete(self, text):
        random_deleted = self.augmenter.random_deletion(text, p=0.5)
        return random_deleted

text_augmentation = TextAugmentation()
</code></pre>
<p>The data augmentation resulting in training has 10300 samples with 3540 positive sentiment class, 4185 negative sentiment class, and 2575 sentiment neutral class</p>
<p>Then, I tokenized the sequence as follows:</p>
<pre><code># Tokenize the sequence
pfizer_tokenizer = Tokenizer(oov_token='OOV')
pfizer_tokenizer.fit_on_texts(df_pfizer_train['text'].values)

X_pfizer_train_tokenized = pfizer_tokenizer.texts_to_sequences(df_pfizer_train['text'].values)
X_pfizer_test_tokenized = pfizer_tokenizer.texts_to_sequences(df_pfizer_test['text'].values)

# Pad the sequence
X_pfizer_train_padded = pad_sequences(X_pfizer_train_tokenized, maxlen=100)
X_pfizer_test_padded = pad_sequences(X_pfizer_test_tokenized, maxlen=100)

pfizer_max_length = 100
pfizer_num_words = len(pfizer_tokenizer.word_index) + 1

# Encode label
y_pfizer_train_encoded = df_pfizer_train['sentiment'].factorize()[0]
y_pfizer_test_encoded = df_pfizer_test['sentiment'].factorize()[0]

y_pfizer_train_category = to_categorical(y_pfizer_train_encoded)
y_pfizer_test_category = to_categorical(y_pfizer_test_encoded)
</code></pre>
<p>Resulting in 8869 unique words and 100 maximum sequence length</p>
<p>Finally, I fit the into my model using pre trained GoogleNewsVectorNegative300 word embedding but only use the weight and LSTM, and I split my training data again with 10% for validation as follows:</p>
<pre><code># Build single LSTM model
def build_lstm_model(embedding_matrix, max_sequence_length):
    # Input layer
    input_layer = Input(shape=(max_sequence_length,), dtype='int32')
    
    # Word embedding layer
    embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],
                                output_dim=embedding_matrix.shape[1],
                                weights=[embedding_matrix],
                                input_length=max_sequence_length,
                                trainable=True)(input_layer)
    
    # LSTM model layer
    lstm_layer = LSTM(units=128,
                      dropout=0.5,
                      return_sequences=True)(embedding_layer)
    batch_normalization = BatchNormalization()(lstm_layer)
    
    lstm_layer = LSTM(units=128,
                      dropout=0.5,
                      return_sequences=False)(batch_normalization)
    batch_normalization = BatchNormalization()(lstm_layer)

    # Dense model layer
    dense_layer = Dense(units=128, activation='relu')(batch_normalization)
    dropout_layer = Dropout(rate=0.5)(dense_layer)
    batch_normalization = BatchNormalization()(dropout_layer)
    
    output_layer = Dense(units=3, activation='softmax')(batch_normalization)

    lstm_model = Model(inputs=input_layer, outputs=output_layer)

    return lstm_model

# Building single LSTM model
sinovac_lstm_model = build_lstm_model(SINOVAC_EMBEDDING_MATRIX, SINOVAC_MAX_SEQUENCE)
sinovac_lstm_model.summary()
sinovac_lstm_model.compile(loss='categorical_crossentropy',
                               optimizer=Adam(learning_rate=0.001),
                               metrics=['accuracy'])
sinovac_lstm_history = sinovac_lstm_model.fit(x=X_sinovac_train,
                                                  y=y_sinovac_train,
                                                  batch_size=64,
                                                  epochs=20,
                                                  validation_split=0.1,
                                                  verbose=1)
</code></pre>
<p><a href=""https://i.sstatic.net/csV70.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/csV70.png"" alt=""enter image description here"" /></a></p>
<p>The training result:
<a href=""https://i.sstatic.net/G0nto.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/G0nto.png"" alt=""enter image description here"" /></a></p>
<p>The evaluation result:
<a href=""https://i.sstatic.net/Rh1qL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Rh1qL.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/Xw2yz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Xw2yz.png"" alt=""enter image description here"" /></a></p>
<p>I really need some suggestions or insights to have a good accuracy on my test</p>
",Training and Model Evaluation,word vec lstm good training validation poor test currently training word vec lstm twitter sentiment analysis use pre trained googlenewsvectornegative word embedding reason used pre trained googlenewsvectornegative performance much worse trained word vec using dataset problem training process validation acc loss stuck respectively confussion matrix also seems wrong several process done fitting model text pre processing lower casing remove hashtag mention url number change word number non ascii character retweets rt expand contraction replace negation antonym remove puncutations remove stopwords lemmatization split dataset train test follows split data resulting training ha sample positive sentiment class negative sentiment class sentiment neutral class implemented text augmentation eda easy data augmentation training data follows data augmentation resulting training ha sample positive sentiment class negative sentiment class sentiment neutral class tokenized sequence follows resulting unique word maximum sequence length finally fit model using pre trained googlenewsvectornegative word embedding use weight lstm split training data validation follows training result evaluation result really need suggestion insight good accuracy test
Word2Vec + CNN Overfitting,"<p>Currently I'am training my Word2Vec + CNN for Twitter sentiment analysis about COVID-19 vaccine domain. I used the pre-trained GoogleNewsVectorNegative300 word embedding. The problem is why I heavily overfit on training proses. The reason I used the pre-trained GoogleNewsVectorNegative300 because the performance much worse when I trained my own Word2Vec using own dataset. Here several processes that I have done before fitting the model:</p>
<p>Text Pre processing:</p>
<ol>
<li>Lower casing</li>
<li>Remove hashtag, mentions, URLs, numbers, change words to numbers, non-ASCII characters, retweets &quot;RT&quot;</li>
<li>Expand contractions</li>
<li>Replace negations with antonyms</li>
<li>Remove puncutations</li>
<li>Remove stopwords</li>
<li>Lemmatization</li>
</ol>
<p>I split my dataset into 90:10 for train:test as follows:</p>
<pre><code>def split_data(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, 
                                                        y,
                                                        train_size=0.9, 
                                                        test_size=0.1, 
                                                        stratify=y,
                                                        random_state=0)
    return X_train, X_test, y_train, y_test
</code></pre>
<p>The split data resulting in training has 2060 samples with 708 positive sentiment class, 837 negative sentiment class, and 515 sentiment neutral class
Training:
<a href=""https://i.sstatic.net/xeLgv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xeLgv.png"" alt=""enter image description here"" /></a>
Testing:
<a href=""https://i.sstatic.net/E0fpC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/E0fpC.png"" alt=""enter image description here"" /></a></p>
<p>Then, I implemented the text augmentation that is EDA (Easy Data Augmentation) on all the training data as follows:</p>
<pre><code>class TextAugmentation:
    def __init__(self):
        self.augmenter = EDA()

    def replace_synonym(self, text):
        augmented_text_portion = int(len(text)*0.1) 
        synonym_replaced = self.augmenter.synonym_replacement(text, n=augmented_text_portion)
        return synonym_replaced

    def random_insert(self, text):
        augmented_text_portion = int(len(text)*0.1) 
        random_inserted = self.augmenter.random_insertion(text, n=augmented_text_portion)
        return random_inserted

    def random_swap(self, text):
        augmented_text_portion = int(len(text)*0.1)
        random_swaped = self.augmenter.random_swap(text, n=augmented_text_portion)
        return random_swaped

    def random_delete(self, text):
        random_deleted = self.augmenter.random_deletion(text, p=0.5)
        return random_deleted

text_augmentation = TextAugmentation()
</code></pre>
<p>The data augmentation resulting in training has 10300 samples with 3540 positive sentiment class, 4185 negative sentiment class, and 2575 sentiment neutral class</p>
<p>Then, I tokenized the sequence as follows:</p>
<pre><code># Tokenize the sequence
pfizer_tokenizer = Tokenizer(oov_token='OOV')
pfizer_tokenizer.fit_on_texts(df_pfizer_train['text'].values)

X_pfizer_train_tokenized = pfizer_tokenizer.texts_to_sequences(df_pfizer_train['text'].values)
X_pfizer_test_tokenized = pfizer_tokenizer.texts_to_sequences(df_pfizer_test['text'].values)

# Pad the sequence
X_pfizer_train_padded = pad_sequences(X_pfizer_train_tokenized, maxlen=100)
X_pfizer_test_padded = pad_sequences(X_pfizer_test_tokenized, maxlen=100)

pfizer_max_length = 100
pfizer_num_words = len(pfizer_tokenizer.word_index) + 1

# Encode label
y_pfizer_train_encoded = df_pfizer_train['sentiment'].factorize()[0]
y_pfizer_test_encoded = df_pfizer_test['sentiment'].factorize()[0]

y_pfizer_train_category = to_categorical(y_pfizer_train_encoded)
y_pfizer_test_category = to_categorical(y_pfizer_test_encoded)
</code></pre>
<p>Resulting in 8869 unique words and 100 maximum sequence length</p>
<p>Finally, I fit the into my model using pre trained GoogleNewsVectorNegative300 word embedding and CNN, and I split my training data again with 10% for validation as follows:</p>
<pre><code># Build single CNN model
def build_cnn_model(embedding_matrix, max_sequence_length):
    # Input layer
    input_layer = Input(shape=(max_sequence_length,))

    # Word embedding layer
    embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],
                                output_dim=embedding_matrix.shape[1],
                                weights=[embedding_matrix],
                                input_length=max_sequence_length,
                                trainable=True)(input_layer)

    # CNN model layer
    cnn_layer = Conv1D(filters=256,
                        kernel_size=2,
                        strides=1,
                        padding='valid',
                        activation='relu')(embedding_layer)
    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)
    cnn_layer = Dropout(rate=0.5)(cnn_layer)
    batch_norm_layer = BatchNormalization()(cnn_layer)

    
    cnn_layer = Conv1D(filters=256,
                        kernel_size=2,
                        strides=1,
                        padding='valid',
                        activation='relu')(batch_norm_layer)
    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)
    cnn_layer = Dropout(rate=0.5)(cnn_layer)
    batch_norm_layer = BatchNormalization()(cnn_layer)

    
    cnn_layer = Conv1D(filters=256,
                        kernel_size=2,
                        strides=1,
                        padding='valid',
                        activation='relu')(batch_norm_layer)
    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)
    cnn_layer = Dropout(rate=0.5)(cnn_layer)
    batch_norm_layer = BatchNormalization()(cnn_layer)


    flatten = Flatten()(batch_norm_layer)
    
    # Dense model layer
    dense_layer = Dense(units=10, activation='relu')(flatten)
    batch_norm_layer = BatchNormalization()(dense_layer)
    output_layer = Dense(units=3, activation='softmax')(batch_norm_layer)
  
    cnn_model = Model(inputs=input_layer, outputs=output_layer)
  
    return cnn_model

    return lstm_model

sinovac_cnn_history = sinovac_cnn_model.fit(x=X_sinovac_train,
                                                  y=y_sinovac_train,
                                                  batch_size=128,
                                                  epochs=100,
                                                  validation_split=0.1,
                                                  verbose=1)
</code></pre>
<p><a href=""https://i.sstatic.net/S2oef.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/S2oef.png"" alt=""enter image description here"" /></a></p>
<p>The training result:
<a href=""https://i.sstatic.net/7soxo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7soxo.png"" alt=""enter image description here"" /></a></p>
<p>I really need some suggestions or insights because I have been doing this without any performance progress to my model</p>
",Training and Model Evaluation,word vec cnn overfitting currently training word vec cnn twitter sentiment analysis covid vaccine domain used pre trained googlenewsvectornegative word embedding problem heavily overfit training prose reason used pre trained googlenewsvectornegative performance much worse trained word vec using dataset several process done fitting model text pre processing lower casing remove hashtag mention url number change word number non ascii character retweets rt expand contraction replace negation antonym remove puncutations remove stopwords lemmatization split dataset train test follows split data resulting training ha sample positive sentiment class negative sentiment class sentiment neutral class training testing implemented text augmentation eda easy data augmentation training data follows data augmentation resulting training ha sample positive sentiment class negative sentiment class sentiment neutral class tokenized sequence follows resulting unique word maximum sequence length finally fit model using pre trained googlenewsvectornegative word embedding cnn split training data validation follows training result really need suggestion insight without performance progress model
FastText 0.9.2 - why is recall &#39;nan&#39;?,"<p>I trained a supervised model in FastText using the Python interface and I'm getting weird results for precision and recall.</p>
<p>First, I trained a model:</p>
<pre class=""lang-py prettyprint-override""><code>model = fasttext.train_supervised(&quot;train.txt&quot;, wordNgrams=3, epoch=100, pretrainedVectors=pretrained_model)
</code></pre>
<p>Then I get results for the test data:</p>
<pre class=""lang-py prettyprint-override""><code>def print_results(N, p, r):
    print(&quot;N\t&quot; + str(N))
    print(&quot;P@{}\t{:.3f}&quot;.format(1, p))
    print(&quot;R@{}\t{:.3f}&quot;.format(1, r))

print_results(*model.test('test.txt'))
</code></pre>
<p>But the results are always odd, because they show precision and recall @1 as identical, even for different datasets, e.g. one output is:</p>
<pre><code>N   46425
P@1 0.917
R@1 0.917
</code></pre>
<p>Then when I look for the precision and recall for each label, I always get recall as 'nan':</p>
<pre class=""lang-py prettyprint-override""><code>print(model.test_label('test.txt'))
</code></pre>
<p>And the output is:</p>
<pre><code>{'__label__1': {'precision': 0.9202150724134941, 'recall': nan, 'f1score': 1.8404301448269882}, '__label__5': {'precision': 0.9134956983264135, 'recall': nan, 'f1score': 1.826991396652827}}
</code></pre>
<p>Does anyone know why this might be happening?</p>
<p>P.S.: To try a reproducible example of this behavior, please refer to <a href=""https://github.com/facebookresearch/fastText/issues/1072"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/fastText/issues/1072</a> and run it with FastText 0.9.2</p>
",Training and Model Evaluation,fasttext recall nan trained supervised model fasttext using python interface getting weird result precision recall first trained model get result test data result always odd show precision recall identical even different datasets e g one output look precision recall label always get recall nan output doe anyone know might happening p try reproducible example behavior please refer run fasttext
Question Answering with pre-trained model T5,"<p>I want to use the pre-trained T5 model <a href=""https://huggingface.co/docs/transformers/model_doc/t5"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/model_doc/t5</a> on the task of Question Answering on the <a href=""https://huggingface.co/datasets/boolq"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/boolq</a> knowing that my inputs will be the passage and the question and the output is the boolean true or false that is the answer for the question.</p>
<p>I have seen some people tuning the model to this specific task. But, I want to know if there is a way to do it with pre-trained model to get some outputs and then compare them with the model after tuning.</p>
<p>Thanks!</p>
",Training and Model Evaluation,question answering pre trained model want use pre trained model task question answering knowing input passage question output boolean true false answer question seen people tuning model specific task want know way pre trained model get output compare model tuning thanks
How to train Naive Bayes Classifier for n-gram (movie_reviews),"<p>Below is the code of training <code>Naive Bayes Classifier</code> on <code>movie_reviews</code> dataset for <code>unigram</code> model. I want to train and analyze its performance by considering <code>bigram</code>, <code>trigram</code> model. How can we do it.</p>

<pre><code>import nltk.classify.util
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import movie_reviews
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

def create_word_features(words):
    useful_words = [word for word in words if word not in stopwords.words(""english"")] 
    my_dict = dict([(word, True) for word in useful_words])
    return my_dict

pos_data = []
for fileid in movie_reviews.fileids('pos'):
    words = movie_reviews.words(fileid)
    pos_data.append((create_word_features(words), ""positive""))    

neg_data = []
for fileid in movie_reviews.fileids('neg'):
    words = movie_reviews.words(fileid)
    neg_data.append((create_word_features(words), ""negative"")) 

train_set = pos_data[:800] + neg_data[:800]
test_set =  pos_data[800:] + neg_data[800:]

classifier = NaiveBayesClassifier.train(train_set)

accuracy = nltk.classify.util.accuracy(classifier, test_set)
</code></pre>
",Training and Model Evaluation,train naive bayes classifier n gram movie review code training dataset model want train analyze performance considering model
How does the finetune on transformer (t5) work?,"<p>I am using pytorch lightning to finetune t5 transformer on a specific task. However, I was not able to understand how the finetuning works. I always see this code :</p>
<p><code>tokenizer = AutoTokenizer.from_pretrained(hparams.model_name_or_path)                                     model = AutoModelForSeq2SeqLM.from_pretrained(hparams.model_name_or_path)</code></p>
<p>I don't get how the finetuning is done, are they freezing the whole model and training the head only, (if so how can I change the head) or are they using the pre-trained model as a weight initializing? I have been looking for an answer for couple days already. Any links or help are appreciated.</p>
",Training and Model Evaluation,doe finetune transformer work using pytorch lightning finetune transformer specific task however wa able understand finetuning work always see code get finetuning done freezing whole model training head change head using pre trained model weight initializing looking answer couple day already link help appreciated
"ValueError: Unable to create tensor, you should probably activate padding with &#39;padding=True&#39;","<p>I am trying to evaluate <code>facebook/hubert-base-ls9601</code> Huggingface pre-trained model after fine-tuning on a private dataset.</p>
<p>I am using <code>facebook/hubert-base-ls9601</code> pre-trained model, and Wav2vec2 feature extractor, and pooling mode set to <code>mean</code>.</p>
<p>Here's the evaluation code:</p>
<pre><code>test_dataset = load_dataset(&quot;csv&quot;, data_files={&quot;test&quot;: &quot;/content/drive/MyDrive/freelancing/test.csv&quot;}, delimiter=&quot;\t&quot;)[&quot;test&quot;]

def speech_file_to_array_fn(batch):
    speech_array, sampling_rate = torchaudio.load(batch[&quot;path&quot;])
    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)
    speech = resampler(speech_array).squeeze().numpy()
    batch[&quot;speech&quot;] = speech_array
    return batch


def predict(batch):
    features = feature_extractor(batch[&quot;speech&quot;], sampling_rate=feature_extractor.sampling_rate, return_tensors=&quot;pt&quot;, padding=True)

    input_values = features.input_values.to(device)

    with torch.no_grad():
        logits = model(input_values).logits 

    pred_ids = torch.argmax(logits, dim=-1).detach().cpu().numpy()
    batch[&quot;predicted&quot;] = pred_ids
    return batch

test_dataset = test_dataset.map(speech_file_to_array_fn)
result = test_dataset.map(predict, batched=True, batch_size=2)
</code></pre>
<p>On the last line of code, I encounter the following error block:</p>
<pre><code>---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

/usr/local/lib/python3.7/dist-packages/transformers/feature_extraction_utils.py in convert_to_tensors(self, tensor_type)
    168                 if not is_tensor(value):
--&gt; 169                     tensor = as_tensor(value)
    170 

ValueError: could not broadcast input array from shape (2,220683) into shape (2,)


During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)

12 frames

&lt;ipython-input-73-7bd88adad349&gt; in &lt;module&gt;()
----&gt; 1 result = test_dataset.map(predict, batched=True, batch_size=2)

/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)
   1970                 new_fingerprint=new_fingerprint,
   1971                 disable_tqdm=disable_tqdm,
-&gt; 1972                 desc=desc,
   1973             )
   1974         else:

/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)
    517             self: &quot;Dataset&quot; = kwargs.pop(&quot;self&quot;)
    518         # apply actual function
--&gt; 519         out: Union[&quot;Dataset&quot;, &quot;DatasetDict&quot;] = func(self, *args, **kwargs)
    520         datasets: List[&quot;Dataset&quot;] = list(out.values()) if isinstance(out, dict) else [out]
    521         for dataset in datasets:

/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)
    484         }
    485         # apply actual function
--&gt; 486         out: Union[&quot;Dataset&quot;, &quot;DatasetDict&quot;] = func(self, *args, **kwargs)
    487         datasets: List[&quot;Dataset&quot;] = list(out.values()) if isinstance(out, dict) else [out]
    488         # re-apply format to the output

/usr/local/lib/python3.7/dist-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)
    456             # Call actual function
    457 
--&gt; 458             out = func(self, *args, **kwargs)
    459 
    460             # Update fingerprint of in-place transforms + update in-place history of transforms

/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in _map_single(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)
   2340                                 indices,
   2341                                 check_same_num_examples=len(input_dataset.list_indexes()) &gt; 0,
-&gt; 2342                                 offset=offset,
   2343                             )
   2344                         except NumExamplesMismatchError:

/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in apply_function_on_filtered_inputs(inputs, indices, check_same_num_examples, offset)
   2217             if with_rank:
   2218                 additional_args += (rank,)
-&gt; 2219             processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
   2220             if update_data is None:
   2221                 # Check if the function returns updated examples

/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in decorated(item, *args, **kwargs)
   1912                 )
   1913                 # Use the LazyDict internally, while mapping the function
-&gt; 1914                 result = f(decorated_item, *args, **kwargs)
   1915                 # Return a standard dict
   1916                 return result.data if isinstance(result, LazyDict) else result

&lt;ipython-input-71-6f845da29c00&gt; in predict(batch)
     11 
     12 def predict(batch):
---&gt; 13     features = feature_extractor(batch[&quot;speech&quot;], sampling_rate=feature_extractor.sampling_rate, return_tensors=&quot;pt&quot;, padding=True)
     14 
     15     input_values = features.input_values.to(device)

/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py in __call__(self, raw_speech, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors, sampling_rate, **kwargs)
    200             truncation=truncation,
    201             pad_to_multiple_of=pad_to_multiple_of,
--&gt; 202             return_attention_mask=return_attention_mask,
    203         )
    204 

/usr/local/lib/python3.7/dist-packages/transformers/feature_extraction_sequence_utils.py in pad(self, processed_features, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors)
    230                 batch_outputs[key].append(value)
    231 
--&gt; 232         return BatchFeature(batch_outputs, tensor_type=return_tensors)
    233 
    234     def _pad(

/usr/local/lib/python3.7/dist-packages/transformers/feature_extraction_utils.py in __init__(self, data, tensor_type)
     78     def __init__(self, data: Optional[Dict[str, Any]] = None, tensor_type: Union[None, str, TensorType] = None):
     79         super().__init__(data)
---&gt; 80         self.convert_to_tensors(tensor_type=tensor_type)
     81 
     82     def __getitem__(self, item: str) -&gt; Union[Any]:

/usr/local/lib/python3.7/dist-packages/transformers/feature_extraction_utils.py in convert_to_tensors(self, tensor_type)
    174                     raise ValueError(&quot;Unable to create tensor returning overflowing values of different lengths. &quot;)
    175                 raise ValueError(
--&gt; 176                     &quot;Unable to create tensor, you should probably activate padding &quot;
    177                     &quot;with 'padding=True' to have batched tensors with the same length.&quot;
    178                 )

ValueError: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.
</code></pre>
<p>I am working on Google Colab. Those are the environment variables:</p>
<pre><code>%env LC_ALL=C.UTF-8
%env LANG=C.UTF-8
%env TRANSFORMERS_CACHE=/content/cache
%env HF_DATASETS_CACHE=/content/cache
%env CUDA_LAUNCH_BLOCKING=1
</code></pre>
<p>The padding is already activated in the <code>predict</code> function.</p>
<p>Can you please help me fix it?</p>
",Training and Model Evaluation,valueerror unable create tensor probably activate padding padding true trying evaluate huggingface pre trained model fine tuning private dataset using pre trained model wav vec feature extractor pooling mode set evaluation code last line code encounter following error block working google colab environment variable padding already activated function please help fix
focal loss NLP/text data pytorch - improving results,"<p>I have a NLP/text data classification problem where there is a very skewed distribution - <code>class 0 - 98%, class 1 - 2%</code>
For my training and validation data I am doing oversampling and my class distribution is <code>class 0 - 55%, class 1 - 45%</code>.
The test data has skewed distribution</p>
<p>i built a model using <code>nn.BCEWithLogitsLoss(pos_weight=tensor(1.2579, device='cuda:0'))</code> . <code>pos_weight</code> was calculated using <code>55/45</code> (class distribution in training data.)</p>
<p>and on my class 1 of test data I got <code>f1</code> performance of <code>0.07</code>,
<code>true negatives, false positives, false negative, true positive = (28809, 13258, 537, 495)</code></p>
<p>I changed to focal loss using below code and my performance didnt improve a lot. <code>f1</code> on class 1 of test data is still same and
<code>true negatives, false positives, false negative, true positive = (32527, 9540, 640, 392)</code></p>
<p><code>kornia.losses.binary_focal_loss_with_logits(probssss, labelsss,alpha=0.25,gamma=2.0,reduction='mean')</code></p>
<ol>
<li>are my alpha and gamma parameters wrong? Are there any specific values that I should try? I could try to tune them but it might take a lot of time and resources. therefore I am looking for recommendations</li>
<li>for my <code>nn.BCEWithLogitsLoss(pos_weight=tensor(1.2579, device='cuda:0'))</code> should I use any other value for <code>pos_weight</code>? Please remember that my goal is to get maximum <code>f1</code> performance for test data class 1</li>
</ol>
<p><strong>#update</strong></p>
<p>I am building a CNN using glove embedding - i take my text and find their glove embedding - i am removing all punctuation and apart from that no other major data cleaning. I am interested in tuning parameters of the focal loss - alpha and gamma</p>
<p>My model is as below</p>
<pre><code>class CNN(nn.Module):
    
    def __init__(self,
                 pretrained_embedding,
                 embed_dim,
                 filter_sizes,
                 num_filters,
                 fc1_neurons,
                 fc2_neurons,
                 dropout):

        super(CNN, self).__init__()
        
        # Embedding layer
        self.vocab_size, self.embed_dim = pretrained_embedding.shape
        self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,
                                                      freeze=True)

        # Conv Network
        self.conv1d_list = nn.ModuleList([
            nn.Conv1d(in_channels=self.embed_dim,
                      out_channels=num_filters[i],
                      kernel_size=filter_sizes[i])
            for i in range(len(filter_sizes))
        ])
        
        #Batchnorm
        self.batch_norm1 = nn.BatchNorm1d(num_filters[0] * len(filter_sizes))
        
        # Dropout Layer
        self.dropout = nn.Dropout(p=dropout)
        
        # RELU activation function
        self.relu =  nn.ReLU()
        
        # Fully-connected layers
#         self.fc1 = nn.Linear(np.sum(num_filters), fc1_neurons)
        
        self.batch_norm2 = nn.BatchNorm1d(num_filters)
        
        self.fc2 = nn.Linear(np.sum(num_filters), fc2_neurons)
        
        self.batch_norm3 = nn.BatchNorm1d(fc2_neurons)
        
        self.fc3 = nn.Linear(fc2_neurons, 1)
</code></pre>
",Training and Model Evaluation,focal loss nlp text data pytorch improving result nlp text data classification problem skewed distribution training validation data oversampling class distribution test data ha skewed distribution built model using wa calculated using class distribution training data class test data got performance changed focal loss using code performance didnt improve lot class test data still alpha gamma parameter wrong specific value try could try tune might take lot time resource therefore looking recommendation use value please remember goal get maximum performance test data class update building cnn using glove embedding take text find glove embedding removing punctuation apart major data cleaning interested tuning parameter focal loss alpha gamma model
How to verify if two text datasets are from different distribution?,"<p>I have two text datasets. Each dataset consists of multiple sequences and each sequence can have more than one sentence.</p>
<p>How do I measure if both datasets are from same distribution?</p>
<p>The purpose is to verify transfer learning from one distribution to another only if the difference between the distributions is statistically significant.</p>
<p>I am panning to use chi-square test but not sure if it will help for text data considering the high degrees of freedom.</p>
<p>update:
Example:
Supppose I want to train a sentiment classification model. I train a model on IMDb dataset and evaluate on IMDb and Yelp datasets. I found that my model trained on IMDb still does well on Yelp. But the question is how different these datasets are?</p>
<p>Train Dataset : <a href=""https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format?select=Train.csv"" rel=""noreferrer"">https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format?select=Train.csv</a></p>
<p>Eval 1: <a href=""https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format?select=Valid.csv"" rel=""noreferrer"">https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format?select=Valid.csv</a></p>
<p>Eval 2: <a href=""https://www.kaggle.com/omkarsabnis/sentiment-analysis-on-the-yelp-reviews-dataset"" rel=""noreferrer"">https://www.kaggle.com/omkarsabnis/sentiment-analysis-on-the-yelp-reviews-dataset</a></p>
<p>Now,</p>
<ol>
<li>How different are train and eval 1?</li>
<li>How different are train and eval 2?</li>
<li>Is the dissimilarity between train and eval 2 by chance ? What is the statistical significance and p value?</li>
</ol>
",Training and Model Evaluation,verify two text datasets different distribution two text datasets dataset consists multiple sequence sequence one sentence measure datasets distribution purpose verify transfer learning one distribution another difference distribution statistically significant panning use chi square test sure help text data considering high degree freedom update example supppose want train sentiment classification model train model imdb dataset evaluate imdb yelp datasets found model trained imdb still doe well yelp question different datasets train dataset eval eval different train eval different train eval dissimilarity train eval chance statistical significance p value
Accuracy reduce in Concatenate LSTM when epochs size is increased,"<p>First I'm new to these concepts. I used two lstm layers and concatenation. but when I train the model with 2 epochs it works fine(I believe because it gives around 90% accuracy). but when I increase the epochs size(epochs=10) val_accuracy which shows while the training remains same(0.5601) and val_loss(6.7802) is also higher and remain in same.
but when I train using 2 epochs, val_accuracy and val_loss is fine(val_accuracy=0.9924, val_loss=0.0392(not remain in same they have changed))</p>
<p>this is the code for concatenation LSTM</p>
<p>dataset was preprocessed using one-hot encoding. and embedded layer length is 50.</p>
<pre><code>inputs = Input(shape=(50))
lstm1 = Embedding(voc_size,embedding_vector_features,input_length=sent_length)(inputs)
lstm1 = LSTM(50)(lstm1)
lstm1 = Dense(1,activation='sigmoid')(lstm1)

lstm2 = Embedding(voc_size,embedding_vector_features,input_length=sent_length)(inputs)
lstm2 = LSTM(50)(lstm2)
lstm2 = Dense(1,activation='sigmoid')(lstm2)

concatenated = Concatenate(name='concatenate_1')([lstm1,lstm2])
output1 = Dense(1, name='dense_1')(concatenated)
model = Model(inputs=inputs, outputs=output1)
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=20,batch_size=32)
</code></pre>
<p>any solution for resolve this issue. thanks</p>
",Training and Model Evaluation,accuracy reduce concatenate lstm epoch size increased first new concept used two lstm layer concatenation train model epoch work fine believe give around accuracy increase epoch size epoch val accuracy show training remains val loss also higher remain train using epoch val accuracy val loss fine val accuracy val loss remain changed code concatenation lstm dataset wa preprocessed using one hot encoding embedded layer length solution resolve issue thanks
understanding gpu usage huggingface classification - Total optimization steps,"<p>I am training huggingface longformer for a classification problem and got below output.</p>
<ol>
<li><p>I am confused about <code>Total optimization steps</code>. As I have 7000 training data points and 5 epochs and <code>Total train batch size (w. parallel, distributed &amp; accumulation) = 64</code>, shouldn't I get
<code>7000*5/64</code> steps? that comes to <code>546.875</code>? why is it showing <code> Total optimization steps = 545</code></p>
</li>
<li><p>Why in the below output, there are 16 steps of <code>Input ids are automatically padded from 1500 to 1536 to be a multiple of config.attention_window: 512</code> then <code> [ 23/545 14:24 &lt; 5:58:16, 0.02 it/s, Epoch 0.20/5]</code>? what are these steps?</p>
</li>
</ol>
<p>==========================================================</p>
<pre><code>***** Running training *****
  Num examples = 7000
  Num Epochs = 5
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed &amp; accumulation) = 64
  Gradient Accumulation steps = 16
  Total optimization steps = 545
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
 [ 23/545 14:24 &lt; 5:58:16, 0.02 it/s, Epoch 0.20/5]
Epoch   Training Loss   Validation Loss
</code></pre>
<hr />
<p><strong>#update</strong></p>
<p>adding <code>Trainer</code> and <code>TrainingArguments</code></p>
<pre><code>#class weights
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get(&quot;labels&quot;)
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get(&quot;logits&quot;)
        # compute custom loss (suppose one has 3 labels with different weights)
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 0.5243])).to(device)
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1)).to(device)
        return (loss, outputs) if return_outputs else loss

 trainer = CustomTrainer(
        model=model,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=train_df_tuning_dataset_tokenized,
        eval_dataset=val_dataset_tokenized
    )



# define the training arguments
training_args = TrainingArguments(
    
    
num_train_epochs = 5,# changed this from 5
per_device_train_batch_size = 4,#4,#8,
gradient_accumulation_steps = 16,
per_device_eval_batch_size= 16,#16
evaluation_strategy = &quot;epoch&quot;,

save_strategy = &quot;epoch&quot;,
learning_rate=2e-5,
load_best_model_at_end=True,
greater_is_better=False,

disable_tqdm = False, 

weight_decay=0.01,
optim=&quot;adamw_torch&quot;,#removing on 18 march from huggingface example notebook
run_name = 'longformer-classification-16March2022'
)
</code></pre>
",Training and Model Evaluation,understanding gpu usage huggingface classification total optimization step training huggingface longformer classification problem got output confused training data point epoch get step come showing output step step update adding
Error while using categorical_crossentropy,"<p>I am learning deep learning with tensorflow. I made a simple NLP code predicting the next word on a given sentence</p>
<pre><code>model = tf.keras.Sequential()
model.add(Embedding(num,64,input_length = max_len-1))   # we subtract 1 coz we cropped the laste word from X in out data
model.add(Bidirectional(LSTM(32)))
model.add(Dense(num,activation = 'softmax'))


model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])

history = model.fit(X,Y,epochs = 500)
</code></pre>
<p>however using categorical_crossentropy gives me the following error</p>
<pre><code>ValueError: You are passing a target array of shape (453, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:
```
from keras.utils import to_categorical
y_binary = to_categorical(y_int)
```

Alternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.
</code></pre>
<p>Can someone explain me what does this mean and why i cant use categorical crossentropy loss function?
Thank you so much!
Any help would be appreciated!</p>
",Training and Model Evaluation,error using categorical crossentropy learning deep learning tensorflow made simple nlp code predicting next word given sentence however using categorical crossentropy give following error someone explain doe mean cant use categorical crossentropy loss function thank much help would appreciated
Adding New Vocabulary Tokens to the Models and saving it for downstream model,"<p>Is the mean initialisation of new tokens correct? Also how should I save new tokenizer( after adding new tokens to it) to use it in downstream model?</p>
<p>I train a MLM model by adding new tokens and taking mean. How should I use the fine tuned MLM model for new classification task?</p>
<pre><code>tokenizer_org = tr.BertTokenizer.from_pretrained(&quot;/home/pc/bert_base_multilingual_uncased&quot;)
tokenizer.add_tokens(joined_keywords)
model = tr.BertForMaskedLM.from_pretrained(&quot;/home/pc/bert_base_multilingual_uncased&quot;, return_dict=True)

# prepare input
text = [&quot;Replace me by any text you'd like&quot;]
encoded_input = tokenizer(text, truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;)
print(encoded_input)


# add embedding params for new vocab words
model.resize_token_embeddings(len(tokenizer))
weights = model.bert.embeddings.word_embeddings.weight
    
# initialize new embedding weights as mean of original tokens
with torch.no_grad():
    emb = []
    for i in range(len(joined_keywords)):
        word = joined_keywords[i]
        # first &amp; last tokens are just string start/end; don't keep
        tok_ids = tokenizer_org(word)[&quot;input_ids&quot;][1:-1]
        tok_weights = weights[tok_ids]

        # average over tokens in original tokenization
        weight_mean = torch.mean(tok_weights, axis=0)
        emb.append(weight_mean)
    weights[-len(joined_keywords):,:] = torch.vstack(emb).requires_grad_()

model.to(device)

</code></pre>
<p><code>trainer.save_model(&quot;/home/pc/Bert_multilingual_exp_TCM/model_mlm_exp1&quot;)</code></p>
<p><strong>It saves model, config, training_args. How to save the new tokenizer as well??</strong></p>
",Training and Model Evaluation,adding new vocabulary token model saving downstream model mean initialisation new token correct also save new tokenizer adding new token use downstream model train mlm model adding new token taking mean use fine tuned mlm model new classification task save model config training args save new tokenizer well
ValueError: Classification metrics can&#39;t handle a mix of multilabel-indicator and multiclass targets,"<p>I am trying to predict a model using Independent variable (Arabic Sentence) and Dependent variables (Multiclass but using One Hot encoding technique. I used Tokenizer Technique for Train and test set</p>
<p>The Model:</p>
<pre><code>model = Sequential()

model.add(Embedding(num_words,32,input_length=max_length))
model.add(LSTM(64,dropout=0.1))
model.add(Dense(4,activation='sigmoid'))

model.compile(loss='binary_crossentropy',optimizer=optimizer, metrics=['accuracy'])

# some code here

model.fit(train_padded,y_train,epochs=1, validation_data=(test_padded,y_test))
</code></pre>
<p>The problem is when I use score = <code>f1_score(y_test, ynew, average='weighted')</code> as evaluation. It shows the following error:</p>
<pre><code>ValueError: Classification metrics can't handle a mix of multilabel-indicator and multiclass targets
</code></pre>
<p><em>ynew</em> and <em>y_test</em> values are the following:</p>
<pre><code>ynew= array([2, 1, 3, ..., 3, 0, 1]`, dtype=int64)

y_test = array([[0, 0, 1, 0],
       [0, 1, 0, 0],
       [0, 0, 0, 1],
       ...,
       [0, 0, 0, 1],
       [1, 0, 0, 0],
       [0, 1, 0, 0]], dtype=uint8)
</code></pre>
",Training and Model Evaluation,valueerror classification metric handle mix multilabel indicator multiclass target trying predict model using independent variable arabic sentence dependent variable multiclass using one hot encoding technique used tokenizer technique train test set model problem use score evaluation show following error ynew test value following
Seq2seq LSTM fails to produce sensible summaries,"<p>I am training an encoder-decoder LSTM in keras for text summarization and the CNN dataset with the following architecture</p>

<p><a href=""https://i.sstatic.net/32db9.jpg"" rel=""nofollow noreferrer"">Picture of bidirectional encoder-decoder LSTM</a>
<br></p>

<ol>
<li><p>I am pretraining the word embedding (of size 256) using skip-gram and </p></li>
<li><p>I then pad the input sequences with zeros so all articles are of equal length</p></li>
<li><p>I put a vector of 1's in each summary to act as the ""start"" token</p></li>
<li><p>Use MSE, RMSProp, tanh activation in the decoder output later</p></li>
<li><p>Training: 20 epochs, batch_size=100, clip_norm=1,dropout=0.3, hidden_units=256, LR=0.001, training examples=10000, validation_split=0.2</p></li>
<li>The network trains and training and validation MSE go down to 0.005, however during inference, the decoder keeps producing a repetition of a few words that make no sense and are nowhere near the real summary.</li>
</ol>

<p>My question is, is there anything fundamentally wrong in my training approach, the padding, loss function, data size, training time so that the network fails to generalize?</p>
",Training and Model Evaluation,seq seq lstm fails produce sensible summary training encoder decoder lstm kera text summarization cnn dataset following architecture picture bidirectional encoder decoder lstm pretraining word embedding size using skip gram pad input sequence zero article equal length put vector summary act start token use mse rmsprop tanh activation decoder output later training epoch batch size clip norm dropout hidden unit lr training example validation split network train training validation mse go however inference decoder keep producing repetition word make sense nowhere near real summary question anything fundamentally wrong training approach padding loss function data size training time network fails generalize
Increase Precision of Text Parsed via Facebook Duckling?,"<p>I'm using <a href=""https://github.com/facebook/duckling"" rel=""nofollow noreferrer"">Facebook Duckling</a> to parse some text but the results include some odd dimensions:</p>
<pre><code>String text = &quot;Tomorrow, February 28&quot;;
String result = Duckling.parseText(text);

Result:
 [
  {
    &quot;body&quot;: &quot;Tomorrow, February 28&quot;,
    &quot;start&quot;: 0,
    &quot;value&quot;: {
      &quot;values&quot;: [
        {
          &quot;value&quot;: &quot;2022-02-28T00:00:00.000-08:00&quot;,
          &quot;grain&quot;: &quot;day&quot;,
          &quot;type&quot;: &quot;value&quot;
        }
      ],
      &quot;value&quot;: &quot;2022-02-28T00:00:00.000-08:00&quot;,
      &quot;grain&quot;: &quot;day&quot;,
      &quot;type&quot;: &quot;value&quot;
    },
    &quot;end&quot;: 21,
    &quot;dim&quot;: &quot;time&quot;,
    &quot;latent&quot;: false
  },
  {
    &quot;body&quot;: &quot;28'&quot;,
    &quot;start&quot;: 19,
    &quot;value&quot;: {
      &quot;value&quot;: 28,
      &quot;type&quot;: &quot;value&quot;,
      &quot;minute&quot;: 28,
      &quot;unit&quot;: &quot;minute&quot;,
      &quot;normalized&quot;: {
        &quot;value&quot;: 1680,
        &quot;unit&quot;: &quot;second&quot;
      }
    },
    &quot;end&quot;: 22,
    &quot;dim&quot;: &quot;duration&quot;,
    &quot;latent&quot;: false
  },
  {
    &quot;body&quot;: &quot;28'&quot;,
    &quot;start&quot;: 19,
    &quot;value&quot;: {
      &quot;value&quot;: 28,
      &quot;type&quot;: &quot;value&quot;,
      &quot;unit&quot;: &quot;foot&quot;
    },
    &quot;end&quot;: 22,
    &quot;dim&quot;: &quot;distance&quot;,
    &quot;latent&quot;: false
  }
]
</code></pre>
<p>This result is odd since from the context of the query the text &quot;28&quot; is clearly referring to the day of month but <code>Duckling</code> also returns data as if it were referring to the <code>Distance</code> dimension.</p>
<p>Is there a way to make <code>Duckling</code> context aware and have it only return results matching the full query? Passing &quot;dimensions&quot; as argument is not ideal since I don't know the dimensions in advance.</p>
<p>Thanks</p>
",Training and Model Evaluation,increase precision text parsed via facebook duckling using facebook duckling parse text result include odd dimension result odd since context query text clearly referring day month also return data referring dimension way make context aware return result matching full query passing dimension argument ideal since know dimension advance thanks
Higher Testing Accuracy and Lower Trainning Accuracy,"<p>I am rather new to the process of NLP, and I am running into a situation where my training accuracy is around 70% but my test accuracy is 80%. I have roughly 6000 entries from 2020 to be used as training data and 300 entires from first quarter of 2021 to be used as test data (due to unavailability of Q2,Q3,Q4 data). Each entire would have at least 2-3 paragraphs within them.</p>
<p>I have setup cross validation using RepeatedStratifiedKFold with 10 split and 3 repeat, and using grideserachCV with C=.1 and kernel = linear. Setup stop words (I did customized it somewhat such as include top 100 common names, month, as well as some of more common words that doesn't mean much in my setting),  lowercased everything, and used Snowball stemmer.
The resulting confusion matrix for the test set is as appeared</p>
<pre><code>[[165  34]
[ 27  96]]
</code></pre>
<p>with F1 score of 81%
However upon examing my trainning set
it had means and std of 0.720 (+/-0.036)</p>
<p>I am trying to make out why there is a 9% difference between the trainning and test sets with test set getting a higher result as well as not  sure what else I could do to further improve the accuracy.</p>
<p>My goal is to predict the unavailable data in Q2,Q3,Q4 and ultimately comparing those 3 when it is available</p>
",Training and Model Evaluation,higher testing accuracy lower trainning accuracy rather new process nlp running situation training accuracy around test accuracy roughly entry used training data entire first quarter used test data due unavailability q q q data entire would least paragraph within setup cross validation using repeatedstratifiedkfold split repeat using grideserachcv c kernel linear setup stop word customized somewhat include top common name month well common word mean much setting lowercased everything used snowball stemmer resulting confusion matrix test set appeared f score however upon examing trainning set mean std trying make difference trainning test set test set getting higher result well sure else could improve accuracy goal predict unavailable data q q q ultimately comparing available
View train error metrics for Hugging Face Sagemaker model,"<p>I have trained a model using Hugging Face's integration with Amazon Sagemaker <a href=""https://huggingface.co/docs/sagemaker/train"" rel=""nofollow noreferrer"">and their Hello World example</a>.</p>
<p>I can easily calculate and view the metrics generated on the evaluation test set: accuracy, f-score, precision, recall etc. by calling <code>training_job_analytics</code> on the trained model: <code>huggingface_estimator.training_job_analytics.dataframe()</code></p>
<p>How can I also see the same metrics on training sets (or even training error for each epoch)?</p>
<p>Training code is basically the same as the link with extra parts of the docs added:</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.huggingface import HuggingFace

# optionally parse logs for key metrics
# from the docs: https://huggingface.co/docs/sagemaker/train#sagemaker-metrics
metric_definitions = [
    {'Name': 'loss', 'Regex': &quot;'loss': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'learning_rate', 'Regex': &quot;'learning_rate': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'eval_loss', 'Regex': &quot;'eval_loss': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'eval_accuracy', 'Regex': &quot;'eval_accuracy': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'eval_f1', 'Regex': &quot;'eval_f1': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'eval_precision', 'Regex': &quot;'eval_precision': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'eval_recall', 'Regex': &quot;'eval_recall': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'eval_runtime', 'Regex': &quot;'eval_runtime': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'eval_samples_per_second', 'Regex': &quot;'eval_samples_per_second': ([0-9]+(.|e\-)[0-9]+),?&quot;},
    {'Name': 'epoch', 'Regex': &quot;'epoch': ([0-9]+(.|e\-)[0-9]+),?&quot;}
]

# hyperparameters, which are passed into the training job
hyperparameters={
    'epochs': 5,
    'train_batch_size': batch_size,
    'model_name': model_checkpoint,
    'task': task,
}

# init the model (but not yet trained)
huggingface_estimator = HuggingFace(
    entry_point='train.py',
    source_dir='./scripts',
    instance_type='ml.p3.2xlarge',
    instance_count=1,
    role=role,
    transformers_version='4.6',
    pytorch_version='1.7',
    py_version='py36',
    hyperparameters = hyperparameters,
    metric_definitions=metric_definitions
)
# starting the train job with our uploaded datasets as input
huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})

# does not return metrics on training - only on eval!
huggingface_estimator.training_job_analytics.dataframe()
</code></pre>
",Training and Model Evaluation,view train error metric hugging face sagemaker model trained model using hugging face integration amazon sagemaker hello world example easily calculate view metric generated evaluation test set accuracy f score precision recall etc calling trained model also see metric training set even training error epoch training code basically link extra part doc added
Keras LSTM: how to handle sequence tagging for sentence boundary detection (NLP)?,"<p>I want to build a sentence boundary detection model for texts using Keras LSTMs. The goal is that I can give the model a text in the form of a list of word-tokens and then I get a list of labels representing each token in the input.
I have the following training data: 6 texts of different lengths, each of these texts are annotated at word level with one of the following labels: [&quot;B-SEN&quot;, &quot;E-SEN&quot;, &quot;O&quot;].
Example data:</p>
<pre><code>X = [
[&quot;This&quot;, &quot;is&quot;, &quot;text&quot;, &quot;one&quot;, &quot;.&quot;, ...],
[&quot;This&quot;, &quot;is&quot;, &quot;text&quot;, &quot;two&quot;, &quot;.&quot;, ...],
...
[&quot;This&quot;, &quot;is&quot;, &quot;text&quot;, &quot;six&quot;, &quot;.&quot;, ...]]

y = [
[&quot;B-SEN&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;E-SEN&quot;, ...],
[&quot;B-SEN&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;E-SEN&quot;, ...],
...
[&quot;B-SEN&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;E-SEN&quot;, ...]]
</code></pre>
<p>My training data X I have now converted that each token/word is converted to a feature vector of 8 features. y I have also encoded starting at 1 for B-SEN. Since the texts have different lengths I have padded the training data based on the longest of my samples. The longest sample/text of these 6 texts is 80451 tokens long. The shorter texts were padded with 0-vectors in X representing the padding and the y arrays with 0 entries.</p>
<p>Since I am completely new to this topic, I wanted to start with a simple model consisting of one LSTM layer and then extend it later on.
Now I have the question of how to model this problem/approach best?
The most trivial approach in my opinion would be to say I choose my <code>input_shape = (6, 80451, 8)</code></p>
<pre><code>model = Sequential()
model.add(LSTM(128, input_shape=(6, 80451, 8), return_sequences=True))
</code></pre>
<p>That would mean that I would look at a whole sample at one timestep, right? Does this make sense for this problem? Most sentences have a length of &lt; 100 tokens. For example, if I wanted my model to look at only 100 tokens per timestep per sample, how could I do that, would I have to restructure my training data?</p>
",Training and Model Evaluation,kera lstm handle sequence tagging sentence boundary detection nlp want build sentence boundary detection model text using kera lstms goal give model text form list word token get list label representing token input following training data text different length text annotated word level one following label b sen e sen example data training data x converted token word converted feature vector feature also encoded starting b sen since text different length padded training data based longest sample longest sample text text token long shorter text padded vector x representing padding array entry since completely new topic wanted start simple model consisting one lstm layer extend later question model problem approach best trivial approach opinion would say choose would mean would look whole sample one timestep right doe make sense problem sentence length token example wanted model look token per timestep per sample could would restructure training data
Load batch to GPU problem in pytorch using BERT model,"<p>I have created a function for evaluation a function. It takes as an input the model and validation data loader and return the validation accuracy, validation loss and f1_weighted score.</p>
<pre><code>def evaluate(model, val_dataloader):
    &quot;&quot;&quot;
    After the completion of each training epoch, measure the model's performance
    on our validation set.
    &quot;&quot;&quot;
    # Put the model into the evaluation mode. The dropout layers are disabled during
    # the test time.
    model.eval()

    # Tracking variables
    val_accuracy = []
    val_loss = []
    f1_weighted = []

    # For each batch in our validation set...
    for batch in val_dataloader:
        # Load batch to GPU
        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

        # Compute logits
        with torch.no_grad():
            logits = model(b_input_ids, b_attn_mask)

        # Compute loss
        loss = loss_fn(logits, b_labels)
        val_loss.append(loss.item())

        # Get the predictions
        preds = torch.argmax(logits, dim=1).flatten()

        # Calculate the accuracy rate
        accuracy = (preds == b_labels).cpu().numpy().mean() * 100
        val_accuracy.append(accuracy)

        # Calculate the f1 weighted score
        f1_metric = F1Score('weighted') 
        f1_weighted = f1_metric(preds, b_labels)

    # Compute the average accuracy and loss over the validation set.
    val_loss = np.mean(val_loss)
    val_accuracy = np.mean(val_accuracy)
    f1_weighted = np.mean(f1_weighted)

    return val_loss, val_accuracy, f1_weighted 
</code></pre>
<p>The core for f1 score can be found here
<a href=""https://stackoverflow.com/questions/62265351/measuring-f1-score-for-multiclass-classification-natively-in-pytorch"">Measuring F1 score for multiclass classification natively in PyTorch</a></p>
<p>Before the evaluation function there is a function which trains a bert model and has the following inputs</p>
<pre><code>train(model, train_dataloader, val_dataloader, epochs, evaluation).
</code></pre>
<p>Thus if the evaluation = True, then the validation accuracy seems in the end of each epoch.</p>
<p>As for the dataloaders are created with the following way:</p>
<pre><code># Convert other data types to torch.Tensor
train_labels = torch.tensor(authors_train)

# Create the DataLoader for our training set
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)
</code></pre>
<p>With a similar way you cal create the dataloader for validation and testing set.</p>
<p><strong>Update:</strong>
I changed the line</p>
<pre><code>f1_weighted = f1_metric(preds, b_labels)
</code></pre>
<p>with this one</p>
<pre><code>f1_weighted.append(f1_metric(preds, b_labels))
</code></pre>
<p>and now I have the following error</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-49-0e0f6d227c4f&gt; in &lt;module&gt;()
      1 set_seed(42)    # Set seed for reproducibility
      2 bert_classifier, optimizer, scheduler = initialize_model(epochs=4)
----&gt; 3 train(bert_classifier, train_dataloader, val_dataloader, epochs=4, evaluation=True)
      4 
      5 #1. 77.28

3 frames
&lt;__array_function__ internals&gt; in mean(*args, **kwargs)

/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py in _mean(a, axis, dtype, out, keepdims)
    168             ret = arr.dtype.type(ret / rcount)
    169         else:
--&gt; 170             ret = ret.dtype.type(ret / rcount)
    171     else:
    172         ret = ret / rcount

AttributeError: 'torch.dtype' object has no attribute 'type'
</code></pre>
",Training and Model Evaluation,load batch gpu problem pytorch using bert model created function evaluation function take input model validation data loader return validation accuracy validation loss f weighted score core f score found href f score multiclass classification natively pytorch evaluation function function train bert model ha following input thus evaluation true validation accuracy seems end epoch dataloaders created following way similar way cal create dataloader validation testing set update changed line one following error
Bert Multi-lingual fine-tuning for multilabel classification,"<p>I’m trying to make French email sentences multilabel classification with certain categories such as commitmemt, proposition, meeting, request, subjectif, etc. .</p>
<p>The first problem I faced is that I don’t have labeled sentences rather I have french emails as dataset). Based on this I found the BC3 dataset (English emails) which has sentences annotated with some of labels listed above. So I came up with this approah; <strong>First finetune a bert multilingual on this BC3 dataset on multilabel classification task and then make a zero-shot transfert learning with the finetuned model (or simply use it in inference) on sentences of my French emails</strong>. What do you think about this approach?</p>
<p>So I started by prepropcessing the BC3 dataset and obtain 848 sentences, each of them with their occurences annotations according to each categorty. On the image below, the last 5 columns represent the number of time each annotator labeled a sentence for a specific label.
<a href=""https://i.sstatic.net/YaOrf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YaOrf.png"" alt=""enter image description here"" /></a></p>
<p><strong>Are those 848 samples enough to fine tune a Bert multilingual model?</strong></p>
<p>I try to fine tune by representing category as on the image below.</p>
<p><a href=""https://i.sstatic.net/hbhto.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hbhto.png"" alt=""enter image description here"" /></a></p>
<p>With one epoch, BATCH_SIZE = 4, the loss function did’t converge, rather it oscillates between 0.79 and 0.34.</p>
<p>What kind of advices would you give the solve this kind of problem?</p>
<p>Thanks.</p>
",Training and Model Evaluation,bert multi lingual fine tuning multilabel classification trying make french email sentence multilabel classification certain category commitmemt proposition meeting request subjectif etc first problem faced labeled sentence rather french email dataset based found bc dataset english email ha sentence annotated label listed came approah first finetune bert multilingual bc dataset multilabel classification task make zero shot transfert learning finetuned model simply use inference sentence french email think approach started prepropcessing bc dataset obtain sentence occurences annotation according categorty image last column represent number time annotator labeled sentence specific label sample enough fine tune bert multilingual model try fine tune representing category image one epoch batch size loss function converge rather oscillates kind advice would give solve kind problem thanks
"When preprocessing Words in NLP, it changes the assumed numeric values after starting a new kernel","<p>I'm using Anaconda Spyder as the editor( python 3.8),
my code is about to identify the phishing URLs based on NLP. Data was preprocessed using 'One hot representation' technique and trained using LSTM. after the training model gets around 90% accuracy. the problem is when I start a new kernel /after restarting the computer model does not give the accuracy as it gave in training.( problem doesn't occur when I use same kernel which used to train). I found that it is because every time I start a new kernel and preprocess data using 'One hot representation' it assumes different values.</p>
<p>ex- if google.com assumed as google = 1 and com = 2  when I use a new kernel it can be assumed as google = 5 and com = 6. so the model was trained using different values but prediction cannot do best because of value changing.</p>
<p>so is there any technique or technology which I can solve this problem. IF I can assume same value for every time I guess problem can be solved.</p>
<p>code used for preprocessing</p>
<pre><code>url = 'https://www.google.com'
messages = urlparse(url).netloc  #taking only the domain name from URL

corpus=[]
voc_size = 10000

review = re.sub('[^a-zA-Z]',' ',messages)  #remove none a-z characters 
review = review.lower()
review = review.split()
review=' '.join(review)
corpus.append(review)


#assuming numerical values for words

from tensorflow.keras.preprocessing.text import one_hot
onehot_repr=[one_hot(words,voc_size)for words in corpus]
print(onehot_repr)
</code></pre>
<p>If there is any technique of library that I can surpass this problem It would be helpful.</p>
",Training and Model Evaluation,preprocessing word nlp change assumed numeric value starting new kernel using anaconda spyder editor python code identify phishing url based nlp data wa preprocessed using one hot representation technique trained using lstm training model get around accuracy problem start new kernel restarting computer model doe give accuracy gave training problem occur use kernel used train found every time start new kernel preprocess data using one hot representation assumes different value ex google com assumed google com use new kernel assumed google com model wa trained using different value prediction best value changing technique technology solve problem assume value every time guess problem solved code used preprocessing technique library surpass problem would helpful
How to store the Phrase trigrams gensim model after training,"<p>I would like to know can I store the gensim Phrase model after training it on the sentences</p>
<pre><code>documents = [&quot;the mayor of new york was there&quot;, &quot;human computer interaction and 
machine learning has now become a trending research area&quot;,&quot;human computer interaction 
is interesting&quot;,&quot;human computer interaction is a pretty interesting subject&quot;, &quot;human 
computer interaction is a great and new subject&quot;, &quot;machine learning can be useful 
sometimes&quot;,&quot;new york mayor was present&quot;, &quot;I love machine learning because it is a new 
subject area&quot;, &quot;human computer interaction helps people to get user friendly 
applications&quot;]

sentences = [doc.split(&quot; &quot;) for doc in documents]

bigram_transformer = Phrases(sentences)
bigram_sentences = bigram_transformer[sentences]
print(&quot;Bigrams - done&quot;)
# Here we use a phrase model that detects the collocation of 3 words (trigrams).
trigram_transformer = Phrases(bigram_sentences)
trigram_sentences = trigram_transformer[bigram_sentences]
print(&quot;Trigrams - done&quot;)
</code></pre>
<p>How to store trigram_transformer physically  to reuse it again using pickle maybe?</p>
<p>Thank you in advance for your help.</p>
",Training and Model Evaluation,store phrase trigram gensim model training would like know store gensim phrase model training sentence store trigram transformer physically reuse using pickle maybe thank advance help
Keras 1D segmentation model always classifies even number of items,"<p>I'm trying to train a 1D CNN to identify specific parts of a text string.</p>
<p>The inputs are arrays of shape <code>(128,1)</code> containing 128 characters, and the aim is for the network to classify each of the characters into a particular class. For purposes of illustration, an input array could look like this:</p>
<pre><code>array(['3', '!', 'd', 'o', 'g', '.', '?', '8', '7', 'a', 'p', 'p', 'l',
       'e', 'f', 'd', '$', '5'], dtype='&lt;U1')
</code></pre>
<p>And the corresponding label looks like this:</p>
<pre><code>array([0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0])
</code></pre>
<p>The idea being that the network will classify the characters <code>&quot;d&quot;, &quot;o&quot;, &quot;g&quot;</code> into class <code>1</code> (say, animals) and <code>&quot;a&quot;, &quot;p&quot;, &quot;p&quot;, &quot;l&quot;, &quot;e&quot;</code> into class <code>2</code> (fruits) and the rest into class <code>0</code>.</p>
<p>The plan is to build a network with an architecture similar to U-Net, but for now I'm experimenting with a very simple downsample/upsample network which looks like this:</p>
<pre><code>def get_model(seq_size,n_classes):
    
    inputs = tf.keras.Input(shape=seq_size)
    
#     Downsample phase

    x = tf.keras.layers.Conv1D(32,11,padding=&quot;same&quot;)(inputs)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation(&quot;relu&quot;)(x)
    
    x = tf.keras.layers.MaxPooling1D(2,padding=&quot;same&quot;)(x)    
    
    x = tf.keras.layers.Conv1D(64,5,padding=&quot;same&quot;)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation(&quot;relu&quot;)(x)
    
    x = tf.keras.layers.MaxPooling1D(2,padding=&quot;same&quot;)(x)  
    
#     Upsample phase    
    
    x = tf.keras.layers.Conv1DTranspose(128,5,padding=&quot;same&quot;)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation(&quot;relu&quot;)(x)
    
    x = tf.keras.layers.UpSampling1D(2)(x)  
    
    x = tf.keras.layers.Conv1DTranspose(256,7,padding=&quot;same&quot;)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation(&quot;relu&quot;)(x)
    
    x = tf.keras.layers.UpSampling1D(2)(x)     
    
    outputs = tf.keras.layers.Conv1D(n_classes,1,activation=&quot;softmax&quot;,padding=&quot;same&quot;)(x)
    
    model = tf.keras.Model(inputs,outputs)
    return model 
</code></pre>
<p>With an input shape of <code>(128,1)</code> and <code>n_classes = 5</code>.</p>
<p>The model works quite well for a baseline, but it has an interesting quirk which I'm trying to get my head around: when it makes predictions over characters, it always classifies an even number of characters (or &quot;pixels&quot; if thinking about this as analogous to an image segmentation task). So in the above example it would identify <code>!dog</code> or <code>dog.</code> as belonging to class 1, and <code>7apple</code> or <code>applef</code> as belonging to class 2.</p>
<p>This is only a problem if the word contains an odd number of characters, which makes me think that it's probably something to do with max-pooling and upsampling operations. I've tried to find an answer by understanding how these operations work in Keras, but this hasn't been fruitful. So if anybody could shed some light onto why the predictions are always an even number of characters, and how I might rectify that, I would be very grateful!</p>
<p><strong>EDIT</strong> from advice in the comments:</p>
<p>To clarify, the arrays are encoded simply using the <code>ord</code> function, and then min/max normalized to the range 0:1.</p>
<p>I'm using sparse categorical cross-entropy for the loss function, and the training setup is as follows:</p>
<pre><code>loss = tf.keras.losses.SparseCategoricalCrossentropy()
opt = tf.keras.optimizers.Adam()

model.compile(optimizer=opt,loss=loss,metrics=[&quot;accuracy&quot;])

callbacks = [tf.keras.callbacks.ModelCheckpoint(&quot;trial.h5&quot;,save_best_only=True)]

epochs = 10
model.fit(train_gen, epochs=epochs, validation_data=test_gen, callbacks=callbacks)
</code></pre>
<p>Where <code>train_gen</code> and <code>test_gen</code> are data generators built as a <code>tf.keras.utils.Sequence</code> subclass.</p>
",Training and Model Evaluation,kera segmentation model always classifies even number item trying train cnn identify specific part text string input array shape containing character aim network classify character particular class purpose illustration input array could look like corresponding label look like idea network classify character class say animal class fruit rest class plan build network architecture similar u net experimenting simple downsample upsample network look like input shape model work quite well baseline ha interesting quirk trying get head around make prediction character always classifies even number character pixel thinking analogous image segmentation task example would identify belonging class belonging class problem word contains odd number character make think probably something max pooling upsampling operation tried find answer understanding operation work kera fruitful anybody could shed light onto prediction always even number character might rectify would grateful edit advice comment clarify array encoded simply using function min max normalized range using sparse categorical cross entropy loss function training setup follows data generator built subclass
"SpaCy 3.2.1, How to add new entitie without losing the default ones","<p>I am trying to add new entities to the already existing model en_core_web_lg. If I don't train the model recognize all the entities. In the moment that I train it, it seems to forget all the previous entities. Can someone help me please? Here's the code:</p>
<pre><code>def main(new_model_name=new_model_name, output_dir=output_dir, n_iter=n_iter):
print(&quot;\n&quot;)
# Load pre-existing spacy model
nlp=spacy.load(output_dir)

# Getting the pipeline component
if &quot;ner&quot; not in nlp.pipe_names:
    ner = nlp.create_pipe(&quot;ner&quot;)
    nlp.add_pipe(ner, last=True)
else:
    ner = nlp.get_pipe(&quot;ner&quot;)

# Adding labels to the `ner`
for _, annotations in TRAIN_DATA:
    for ent in annotations.get(&quot;entities&quot;):
        ner.add_label(ent[2])

# Disable pipeline components you dont need to change
disabled_pipes = []
for pipe_name in nlp.pipe_names:
    if pipe_name != 'ner':
        nlp.disable_pipes(pipe_name)
        disabled_pipes.append(pipe_name)

# TRAINING THE MODEL
optimizer = nlp.create_optimizer()
for iteration in range(n_iter):
        # Shuffling examples  before every iteration
        random.shuffle(TRAIN_DATA)
        losses = {}

        # Batch up the examples using spaCy's minibatch
        for batch in spacy.util.minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001)):
            for text, annotations in batch:
                # create Example
                doc = nlp.make_doc(text)
                example = Example.from_dict(doc, annotations)
                # Update the model
                nlp.update([example], sgd=optimizer,losses=losses, drop=0.8)

        print(f&quot;{iteration+1}/{n_iter} - Losses {losses}&quot;)

for pipe_name in disabled_pipes:
    nlp.enable_pipe(pipe_name)

print(&quot;\n&quot;)
if output_dir is not None:
    # Save the  model to directory
    nlp.meta[&quot;name&quot;] = new_model_name  
    nlp.to_disk(output_dir)
    print(colored(f&quot;Saved model to {output_dir}&quot;, 'white'))

    # Load the saved model and predict      
    print(colored(f&quot;Loading from {output_dir}&quot;, 'white'))
    nlp2 = spacy.load(output_dir)
    for text, _ in TEST_DATA:
        doc = nlp2(text)
        print(&quot;Entities&quot;, [(ent.text, ent.label_) for ent in doc.ents])
        print(&quot;Tokens&quot;, [(t.text, t.ent_type_, t.ent_iob) for t in doc])
</code></pre>
",Training and Model Evaluation,spacy add new entitie without losing default one trying add new entity already existing model en core web lg train model recognize entity moment train seems forget previous entity someone help please code
How to specify the loss function when finetuning a model using the Huggingface TFTrainer Class?,"<p>I have followed the basic example as given below, from: <a href=""https://huggingface.co/transformers/training.html"" rel=""noreferrer"">https://huggingface.co/transformers/training.html</a></p>
<pre><code>from transformers import TFBertForSequenceClassification, TFTrainer, TFTrainingArguments

model = TFBertForSequenceClassification.from_pretrained(&quot;bert-large-uncased&quot;)

training_args = TFTrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total # of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
)

trainer = TFTrainer(
    model=model,                         # the instantiated 🤗 Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=tfds_train_dataset,    # tensorflow_datasets training dataset
    eval_dataset=tfds_test_dataset       # tensorflow_datasets evaluation dataset
)
trainer.train()
</code></pre>
<p>But there seems to be no way to specify the loss function for the classifier. For-ex if I finetune on a binary classification problem, I would use</p>
<pre><code>tf.keras.losses.BinaryCrossentropy(from_logits=True)
</code></pre>
<p>else I would use</p>
<pre><code>tf.keras.losses.CategoricalCrossentropy(from_logits=True)
</code></pre>
<p>My set up is as follows:</p>
<pre><code>transformers==4.3.2
tensorflow==2.3.1
python==3.6.12
</code></pre>
",Training and Model Evaluation,specify loss function finetuning model using huggingface tftrainer class followed basic example given seems way specify loss function classifier ex finetune binary classification problem would use else would use set follows
CBOW v.s. skip-gram: why invert context and target words?,"<p>In <a href=""https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html#vector-representations-of-words"" rel=""noreferrer"">this</a> page, it is said that: </p>

<blockquote>
  <p>[...] skip-gram inverts contexts and targets, and tries to predict each context word from its target word [...]</p>
</blockquote>

<p>However, looking at the training dataset it produces, the content of the X and Y pair seems to be interexchangeable, as those two pairs of (X, Y): </p>

<blockquote>
  <p><code>(quick, brown), (brown, quick)</code></p>
</blockquote>

<p>So, why distinguish that much between context and targets if it is the same thing in the end? </p>

<p>Also, doing <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb"" rel=""noreferrer"">Udacity's Deep Learning course exercise on word2vec</a>, I wonder why they seem to do the difference between those two approaches that much in this problem: </p>

<blockquote>
  <p>An alternative to skip-gram is another Word2Vec model called CBOW (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. Implement and evaluate a CBOW model trained on the text8 dataset.</p>
</blockquote>

<p>Would not this yields the same results?</p>
",Training and Model Evaluation,cbow v skip gram invert context target word page said skip gram inverts context target try predict context word target word however looking training dataset produce content x pair seems interexchangeable two pair x distinguish much context target thing end also udacity deep learning course exercise word vec wonder seem difference two approach much problem alternative skip gram another word vec model called cbow continuous bag word cbow model instead predicting context word word vector predict word sum word vector context implement evaluate cbow model trained text dataset would yield result
What are some of the ways to convert NLP to SQL?,"<p>Recently, have started working on the idea of conversational chatbot and have been thinking of different ways to convert Natural Language query to SQL. 
These are some of the libraries I have shortlisted to evaluate before writing from scratch. Any other ideas or suggestions?</p>

<ul>
<li><a href=""https://github.com/FerreroJeremy/ln2sql"" rel=""noreferrer"">https://github.com/FerreroJeremy/ln2sql</a></li>
<li><a href=""https://github.com/dadashkarimi/seq2sql"" rel=""noreferrer"">https://github.com/dadashkarimi/seq2sql</a></li>
<li><a href=""https://github.com/xiaojunxu/SQLNet"" rel=""noreferrer"">https://github.com/xiaojunxu/SQLNet</a></li>
<li><a href=""http://www.ling.helsinki.fi/kit/2009s/clt231/NLTK/book/ch10-AnalyzingTheMeaningOfSentences.html#querying-a-database"" rel=""noreferrer"">http://www.ling.helsinki.fi/kit/2009s/clt231/NLTK/book/ch10-AnalyzingTheMeaningOfSentences.html#querying-a-database</a></li>
</ul>
",Training and Model Evaluation,way convert nlp sql recently started working idea conversational chatbot thinking different way convert natural language query sql library shortlisted evaluate writing scratch idea suggestion
Can the increase in training loss lead to better accuracy?,"<p>I'm working on a competition on Kaggle. First, I trained a Longformer base with the competition dataset and achieved a quite good result on the leaderboard. Due to the CUDA memory limit and time limit, I could only train 2 epochs with a batch size of 1. The loss started at about 2.5 and gradually decreased to 0.6 at the end of my training.</p>
<p>I then continued training 2 more epochs using that saved weights. This time I used a little bit larger learning rate (the one on the Longformer paper) and added the validation data to the training data (meaning I no longer split the dataset 90/10). I did this to try to achieve a better result.</p>
<p>However, this time the loss started at about 0.4 and constantly increased to 1.6 at about half of the first epoch. I stopped because I didn't want to waste computational resources.</p>
<p>Should I have waited more? Could it eventually lead to a better test result? I think the model could have been slightly overfitting at first.</p>
",Training and Model Evaluation,increase training loss lead better accuracy working competition kaggle first trained longformer base competition dataset achieved quite good result leaderboard due cuda memory limit time limit could train epoch batch size loss started gradually decreased end training training epoch using saved weight time used little bit larger learning rate one longformer paper added validation data training data meaning longer split dataset try achieve better result however time loss started constantly increased half first epoch stopped want waste computational resource waited could eventually lead better test result think model could slightly overfitting first
Validation loss of CNN lower than training loss but still stuck around 80%,"<p>I am doing research in NLP and deep learning with mental health textual data. While training my CNN model my validation loss is lower than training loss but almost around the training loss. The validation and training loss does not go too low instead are stuck on almost 75-80% loss but accuracy achieved is also 76%. What shall i do? What is the exact interpretation of this?</p>
",Training and Model Evaluation,validation loss cnn lower training loss still stuck around research nlp deep learning mental health textual data training cnn model validation loss lower training loss almost around training loss validation training loss doe go low instead stuck almost loss accuracy achieved also shall exact interpretation
How to get generated tokens in T5 training_step for using user-defined metrics?,"<p>I am fine-tuning T5 for question answering generation and want to add additional measures (e.g., BLEU, ROUGE) for the generated answers, in addition to the loss function.</p>
<p>For that, I believe it would be necessary to obtain the generated tokens (answers) at each training_step. However, after reading the source code, I still have no clue how to add that.</p>
<p>Below I leave an excerpt of my code. I can extract the <code>output.loss</code> and <code>output.logits</code>, but I didn't find a way to get the generated tokens to use additional evaluation metrics.</p>
<p>Thanks in advance.</p>
<pre><code>class MyQAModel(pl.LightningModule):
  def __init__(self):
    super().__init__()
    self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)

  def forward(self, input_ids, attention_mask, labels=None):
    output = self.model(
        input_ids, 
        attention_mask=attention_mask,
        labels=labels)

    return output.loss, output.logits

  def training_step(self, batch, batch_idx):
    input_ids = batch['input_ids']
    attention_mask=batch['attention_mask']
    labels = batch['labels']
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log(&quot;train_loss&quot;, loss, prog_bar=True, logger=True)
    return {&quot;loss&quot;: loss, &quot;predictions&quot;:outputs, &quot;labels&quot;: labels}
    
    ...
    (code continues...)
    ....
</code></pre>
",Training and Model Evaluation,get generated token training step using user defined metric fine tuning question answering generation want add additional measure e g bleu rouge generated answer addition loss function believe would necessary obtain generated token answer training step however reading source code still clue add leave excerpt code extract find way get generated token use additional evaluation metric thanks advance
Sparse Cross Entropy Loss for calculating Loss in NLP problems. PyTorch,"<p>My Input tensor Looks like :</p>
<pre><code>torch.Size([8, 23])

// where,
// 8 -&gt; batch size
// 23 -&gt; words in each of them
</code></pre>
<p>My output tensor Looks like :</p>
<pre><code>torch.Size([8, 23, 103])

// where,
// 8 -&gt; batch size
// 23 -&gt; words predictions
// 103 -&gt; vocab size.
</code></pre>
<p>I want to calculate sparse cross Entropy Loss for this task, but I can’t since PyTorch only calculates the loss single element. How can I code it to work? Thanks for your help.</p>
",Training and Model Evaluation,sparse cross entropy loss calculating loss nlp problem pytorch input tensor look like output tensor look like want calculate sparse cross entropy loss task since pytorch calculates loss single element code work thanks help
How to improve language model ex: BERT on unseen text in training?,"<p>so I am using pre-trained language model for binary classification. I fine-tune the model by training on data my downstream task. The results are good almost 98% F-measure.</p>
<p>However, when I remove a specific similar sentence from the training data and add it to my test data, the classifier fails to predict the class of that sentence. For example, sentiment analysis task</p>
<blockquote>
<p><strong>&quot;I love the movie more specifically the acting was great&quot;</strong></p>
</blockquote>
<p>I removed from training all sentences containing the words <strong>&quot; more specifically&quot;</strong> and surprisingly in the test set they were all misclassified, so the precision decreased by a huge amount.</p>
<p>Any ideas on how can I further fine-tune/improve my model to work better on unseen text in training to avoid the problem I described above? (of course without feeding the model on sentences containing the words <strong>&quot;more specifically&quot;</strong></p>
<p>Note: I observed the same performance regardless of the language model in use (BERT, RoBERTa etc).</p>
",Training and Model Evaluation,improve language model ex bert unseen text training using pre trained language model binary classification fine tune model training data downstream task result good almost f measure however remove specific similar sentence training data add test data classifier fails predict class sentence example sentiment analysis task love movie specifically acting wa great removed training sentence containing word specifically surprisingly test set misclassified precision decreased huge amount idea fine tune improve model work better unseen text training avoid problem described course without feeding model sentence containing word specifically note observed performance regardless language model use bert roberta etc
55 Hour Long Single Epoch Colab GPU Training Time for a Small RoBERTa-like Transformer,"<p>I am following this guide: <a href=""https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb</a></p>
<p>To train a transformer for Pali.  A dead language but has applicability for Buddhist texts.  I've got the Pali Cannon as training material (110+MB of text).  However, the training of a transformer on a single epoch with Colab takes 55 hours.  The transformer has 55m parameters.  This doesn't seem reasonable to me.  One of the problems that could be occurring is this is one piece of text.  I don't know if the model is interpreting each carriage return (\n) as new text, or if it's interpreting the whole of the Pali Canon as a single document.  Both of which I can see causing the rediculous training time.  From the tutorial, I gathered that &lt;s&gt; and &lt;\s&gt; mean the beginning of a document and end of document tokens in the corpora. However, I still don't know what keywords in the document set off the &lt;s&gt; markers. At first, I thought it might be \n but it looks like this doesn't set if off when using the OSCAR dataset the tutorial uses. I haven't integrated anything to denote end of document flags into my text of the Pali Canon. My question is right now the Pali Cannon text file is not formatted correctly for the transformer to train in optimal time so how should it be formatted so that it is correct?</p>
<p>Here is the link to the Pali Cannon text: <a href=""https://drive.google.com/file/d/1d1J1ib8LYnvapqY9FAWiMTHEA76XxC7b/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1d1J1ib8LYnvapqY9FAWiMTHEA76XxC7b/view?usp=sharing</a></p>
<p>Here is the link to the folder containing the tokenizer files: <a href=""https://drive.google.com/drive/folders/1-2la-kfV_az0NdhCya5fPyleqTc4RJox?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/1-2la-kfV_az0NdhCya5fPyleqTc4RJox?usp=sharing</a></p>
<p>Here is my code:</p>
<pre><code>from transformers import RobertaTokenizerFast
from transformers import RobertaConfig
from transformers import RobertaForMaskedLM
from transformers import LineByLineTextDataset
from transformers import DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments

import os


tokenizer = RobertaTokenizerFast(&quot;/content/drive/MyDrive/PaliBert/vocab.json&quot;,&quot;/content/drive/MyDrive/PaliBert/merges.txt&quot;, max_len=512)



config = RobertaConfig(
    vocab_size=52_000,
    max_position_embeddings=512,
    num_attention_heads=8,
    num_hidden_layers=2,
    type_vocab_size=1,
)


model = RobertaForMaskedLM(config=config)

dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path=&quot;/content/drive/MyDrive/Pali Cannon Analysis/Pali Cannon Text.txt&quot;,
    block_size=128,
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)



training_args = TrainingArguments(
    output_dir=&quot;./EsperBERTo&quot;,
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_gpu_train_batch_size=64,
    save_steps=10_000,
    save_total_limit=2,
    prediction_loss_only=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

trainer.train()

</code></pre>
",Training and Model Evaluation,hour long single epoch colab gpu training time small roberta like transformer following guide train transformer pali dead language ha applicability buddhist text got pali cannon training material mb text however training transformer single epoch colab take hour transformer ha parameter seem reasonable one problem could occurring one piece text know model interpreting carriage return n new text interpreting whole pali canon single document see causing rediculous training time tutorial gathered mean beginning document end document token corpus however still know keywords document set marker first thought might n look like set using oscar dataset tutorial us integrated anything denote end document flag text pali canon question right pali cannon text file formatted correctly transformer train optimal time formatted correct link pali cannon text link folder containing tokenizer file code
Filtering values in a row according to the value of another column,"<p>I would like to create a list of specific negative words for three different labels in my training data. The word should only appears for a specific label, not the three. I have a dataframe with 3 columns: id, sentences, labels</p>
<p>I have also 2 different lexicon files for positive (all in lemmatized form)</p>
<p>I would like to create list of word for each labels</p>
<p>so far I succeeded in creating columns which display negative words for each sentence. but I clueless how to do after, to select and extract word which are unique for each label. That means words which appears only in sentence labelled as one of the three classes.</p>
<p>To summarize: I would like to create a list of words of my lexicon of words negative which appears on sentence labelled as A, B or C. Those words should be unique for each class as show on the expected output list</p>
<p>Datafile below</p>
<p><a href=""https://i.sstatic.net/1ZpCK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1ZpCK.png"" alt=""enter image description here"" /></a></p>
<p>Part of script my script  (I just put the example with negative words)</p>
<pre><code>lexiconneg = lexiconneg_feel['Word'].values # a list of negative word
print(lexiconneg)

def extract_word_neg(text, word_list):
    text_list = tokenize_lemmatize_spacy(text) # call tokenize and lemmatisation function using spacy
    
    intersection = [w for w in text_list if w in word_list]
    
    return intersection
    

datafile['list_mots_négatifs'] = datafile['phrases'].apply(extract_word_neg, args= (lexiconneg, ))

datafile.to_excel('négatif_mots.xlsx')
</code></pre>
<p>Datafile after applying script below</p>
<p><a href=""https://i.sstatic.net/X1o78.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/X1o78.png"" alt=""enter image description here"" /></a></p>
<p>So for my example, I will have the following list as the expected result:</p>
<pre><code>A=  problème, polluer, corrosif, pouvoir, sujet
B=  contrer, extrémité, bouillir
C= vider pression
</code></pre>
",Training and Model Evaluation,filtering value row according value another column would like create list specific negative word three different label training data word appears specific label three dataframe column id sentence label also different lexicon file positive lemmatized form would like create list word label far succeeded creating column display negative word sentence clueless select extract word unique label mean word appears sentence labelled one three class summarize would like create list word lexicon word negative appears sentence labelled b c word unique class show expected output list datafile part script script put example negative word datafile applying script example following list expected result
Keras GPU memory overflow using with keras.utils.sequence and generator,"<p><strong>Dataset.py</strong></p>
<pre><code>import os
import random
from skimage import io
import cv2
from skimage.transform import resize
import numpy as np
import tensorflow as tf

import keras
import Augmentor

def iter_sequence_infinite(seq):
    &quot;&quot;&quot;Iterate indefinitely over a Sequence.
    # Arguments
        seq: Sequence object
    # Returns
        Generator yielding batches.
    &quot;&quot;&quot;
    while True:
        for item in seq:
            yield item

# data generator class
class DataGenerator(keras.utils.Sequence):
    def __init__(self, ids, imgs_dir, masks_dir, batch_size=10, img_size=128, n_classes=1, n_channels=3, shuffle=True):
        self.id_names = ids
        self.indexes = np.arange(len(self.id_names))
        self.imgs_dir = imgs_dir
        self.masks_dir = masks_dir
        self.batch_size = batch_size
        self.img_size = img_size
        self.n_classes = n_classes
        self.n_channels = n_channels
        self.shuffle = shuffle
        self.on_epoch_end()

    # for printing the statistics of the function
    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.id_names))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation__(self, id_name):
        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)
        # Initialization
        img_path = os.path.join(self.imgs_dir, id_name)  # polyp segmentation/images/id_name.jpg
        mask_path = os.path.join(self.masks_dir, id_name) # polyp segmenatation/masks/id_name.jpg

        img = io.imread(img_path)
        mask = cv2.imread(mask_path)

        p = Augmentor.DataPipeline([[img, mask]])
        p.resize(probability=1.0, width=self.img_size, height=self.img_size)
        p.rotate_without_crop(probability=0.3, max_left_rotation=10, max_right_rotation=10)
        #p.random_distortion(probability=0.3, grid_height=10, grid_width=10, magnitude=1)
        p.shear(probability=0.3, max_shear_left=1, max_shear_right=1)
        #p.skew_tilt(probability=0.3, magnitude=0.1)
        p.flip_random(probability=0.3)

        sample_p = p.sample(1)
        sample_p = np.array(sample_p).squeeze()

        p_img = sample_p[0]
        p_mask = sample_p[1]
        augmented_mask = (p_mask // 255) * 255  # denoising

        q = Augmentor.DataPipeline([[p_img]])
        q.random_contrast(probability=0.3, min_factor=0.2, max_factor=1.0)  # low to High
        q.random_brightness(probability=0.3, min_factor=0.2, max_factor=1.0)  # dark to bright

        sample_q = q.sample(1)
        sample_q = np.array(sample_q).squeeze()

        image = sample_q
        mask = augmented_mask[::, ::, 0]

        &quot;&quot;&quot;
        # reading the image from dataset
        ## Reading Image
        image = io.imread(img_path)  # reading image to image vaiable
        image = resize(image, (self.img_size, self.img_size), anti_aliasing=True)  # resizing input image to 128 * 128

        mask = io.imread(mask_path, as_gray=True)  # mask image of same size with all zeros
        mask = resize(mask, (self.img_size, self.img_size), anti_aliasing=True)  # resizing mask to fit the 128 * 128 image
        mask = np.expand_dims(mask, axis=-1)
        &quot;&quot;&quot;

        # image normalization
        image = image / 255.0
        mask = mask / 255.0

        return image, mask

    def __len__(self):
        &quot;Denotes the number of batches per epoch&quot;
        return int(np.floor(len(self.id_names) / self.batch_size))

    def __getitem__(self, index):  # index : batch no.
        # Generate indexes of the batch
        # Generate indexes of the batch
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        batch_ids = [self.id_names[k] for k in indexes]

        imgs = list()
        masks = list()

        for id_name in batch_ids:
            img, mask = self.__data_generation__(id_name)
            imgs.append(img)
            masks.append(np.expand_dims(mask,-1))

        imgs = np.array(imgs)
        masks = np.array(masks)

        return imgs, masks  # return batch
</code></pre>
<p><strong>train.py</strong></p>
<pre><code>import argparse
import logging
import os
import sys
from tqdm import tqdm # progress bar
import numpy as np
import matplotlib.pyplot as plt

from keras import optimizers
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import segmentation_models as sm
from segmentation_models.utils import set_trainable
from dataset import DataGenerator, iter_sequence_infinite



def train_model(model, train_gen, valid_gen, epochs, save_cp=True):
    total_batch_count = 0
    train_img_num = len(train_gen.id_names)
    train_batch_num = len(train_gen)
    train_gen_out = iter_sequence_infinite(train_gen)

    valid_batch_num = len(valid_gen)
    valid_img_num = len(valid_gen.id_names)
    valid_gen_out = iter_sequence_infinite(valid_gen)

    for epoch in range(epochs): # interation as many epochs
        set_trainable(model)

        epoch_loss = 0 # loss in this epoch
        epoch_iou = 0
        count = 0

        with tqdm(total=train_img_num, desc=f'Epoch {epoch + 1}/{epochs}',  position=0, leave=True, unit='img') as pbar:  # make progress bar
            for _ in range(train_batch_num):
                batch = next(train_gen_out)
                imgs = batch[0]
                true_masks = batch[1]
                loss, iou = model.train_on_batch(imgs, true_masks)  # value of loss of this batch
                epoch_loss += loss
                epoch_iou += iou

                pbar.set_postfix(**{'Batch loss': loss, 'Batch IoU': iou})  # floating the loss at the post in the pbar

                pbar.update(imgs.shape[0])  # update progress
                count += 1
                total_batch_count += 1

        train_gen.on_epoch_end()
        print( &quot;Epoch : loss: {}, IoU : {}&quot;.format(epoch_loss/count, epoch_iou/count))

        # Do validation
        validation_model(model, valid_gen_out, valid_batch_num, valid_img_num)
        valid_gen.on_epoch_end()

        if save_cp:
            try:
                if not os.path.isdir(checkpoint_dir):
                    os.mkdir(checkpoint_dir)
                    logging.info('Created checkpoint directory')
                else:
                    pass
            except OSError:
                pass
            model.save_weights(os.path.join(checkpoint_dir , f'CP_epoch{epoch + 1}.h5'))
            logging.info(f'Checkpoint {epoch + 1} saved !')

def validation_model(model, valid_gen_out, valid_batch_num, valid_img_num):
    epoch_loss = 0  # loss in this epoch
    epoch_iou = 0
    count = 0

    with tqdm(total=valid_img_num, desc='Validation round',  position=0, leave=True, unit='img') as pbar:  # make progress bar
        for _ in range(valid_batch_num):
            batch = next(valid_gen_out)
            imgs = batch[0]
            true_masks = batch[1]
            loss, iou = model.test_on_batch(imgs, true_masks)  # value of loss of this batch
            epoch_loss += loss
            epoch_iou += iou

            pbar.set_postfix(**{'Batch, loss': loss, 'Batch IoU': iou})  # floating the loss at the post in the pbar

            pbar.update(imgs.shape[0])  # update progress
            count += 1

    print(&quot;Validation loss: {}, IoU: {}&quot;.format(epoch_loss / count, epoch_iou / count))
    pred_mask = model.predict(np.expand_dims(imgs[0],0))
    plt.subplot(131)
    plt.imshow(imgs[0])
    plt.subplot(132)
    plt.imshow(true_masks[0].squeeze(), cmap=&quot;gray&quot;)
    plt.subplot(133)
    plt.imshow(pred_mask.squeeze(), cmap=&quot;gray&quot;)
    plt.show()
    print()


def get_args():
    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks',
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('-e', '--epochs', metavar='E', type=int, default=50,
                        help='Number of epochs', dest='epochs')
    parser.add_argument('-b', '--batch_size', metavar='B', type=int, nargs='?', default=2,
                        help='Batch size', dest='batch_size')
    parser.add_argument('-l', '--learning-rate', metavar='LR', type=float, nargs='?', default=1e-5,
                        help='Learning rate', dest='lr')
    parser.add_argument('-bb', '--backbone', default='resnet50', metavar='FILE',
                        help=&quot;backcone name&quot;)
    parser.add_argument('-w', '--weight', dest='load', type=str, default=False,
                        help='Load model from a .h5 file')
    parser.add_argument('-s', '--resizing', dest='resizing', type=int, default=384,
                        help='Downscaling factor of the images')
    parser.add_argument('-v', '--validation', dest='val', type=float, default=20.0,
                        help='Percent of the data that is used as validation (0-100)')

    return parser.parse_args()


if __name__ == '__main__':
    img_dir = './data/train/imgs/'  # ./data/train/imgs/CVC_Original/'
    mask_dir = './data/train/masks/'  # ./data/train/masks/CVC_Ground Truth/'
    checkpoint_dir = './checkpoints'
    args = get_args()

    # train path
    train_ids = os.listdir(img_dir)
    # Validation Data Size
    n_val = int(len(train_ids) * args.val/100)  # size of validation set


    valid_ids = train_ids[:n_val]  # list of image ids used for validation of result 0 to 9
    train_ids = train_ids[n_val:]  # list of image ids used for training dataset
    # print(valid_ids, &quot;\n\n&quot;)
    print(&quot;training_size: &quot;, len(train_ids), &quot;validation_size: &quot;, len(valid_ids))

    train_gen = DataGenerator(train_ids, img_dir, mask_dir, img_size=args.resizing, batch_size=args.batch_size)
    valid_gen = DataGenerator(valid_ids, img_dir, mask_dir, img_size=args.resizing, batch_size=args.batch_size)

    print(&quot;total training batches: &quot;, len(train_gen))
    print(&quot;total validaton batches: &quot;, len(valid_gen))
    train_steps = len(train_ids) // args.batch_size
    valid_steps = len(valid_ids) // args.batch_size

    # define model
    model = sm.Unet(args.backbone, encoder_weights='imagenet')

    optimizer = optimizers.Adam(lr=args.lr, decay=1e-4)
    model.compile(
        optimizer=optimizer,
        #        &quot;Adam&quot;,
        loss=sm.losses.bce_dice_loss,  # sm.losses.bce_jaccard_loss, # sm.losses.binary_crossentropy,
        metrics=[sm.metrics.iou_score],
    )
    #model.summary()

    callbacks = [
        EarlyStopping(patience=6, verbose=1),
        ReduceLROnPlateau(factor=0.1, patience=3, min_lr=1e-7, verbose=1),
        ModelCheckpoint('./weights.Epoch{epoch:02d}-Loss{loss:.3f}-VIou{val_iou_score:.3f}.h5', verbose=1,
                        monitor='val_accuracy', save_best_only=True, save_weights_only=True)
                ]


    train_model(model=model, train_gen=train_gen, valid_gen=valid_gen, epochs=args.epochs)
</code></pre>
<p>When I try to run this code, some epochs are well progressed but, in 20epochs, it occurs gpu memory overflow error like below</p>
<pre><code>(0) Resource exhausted: OOM when allocating tensor with shape[2,64,96,96] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
     [[{{node decoder_stage2b_bn/FusedBatchNorm}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
</code></pre>
<p>so, I think that it is because of data generation.</p>
<p>This code generate batch in this order.</p>
<ol>
<li><p>in train.py, initialize <strong>Datageneratr class which is sequence model</strong> that is implemented in <strong>Dataset.py</strong></p>
<p><strong>train_gen = DataGenerator(train_ids, img_dir, mask_dir, img_size=args.resizing, batch_size=args.batch_size)</strong></p>
<p><strong>valid_gen = DataGenerator(valid_ids, img_dir, mask_dir, img_size=args.resizing, batch_size=args.batch_size)</strong></p>
</li>
<li><p>At the first in the <strong>function 'train_model'</strong> convert Datagenerator(sequence model) to generator with using function 'iter_sequence_infinite'</p>
<p><strong>train_gen_out = iter_sequence_infinite(train_gen)</strong></p>
<p><strong>valid_gen_out = iter_sequence_infinite(valid_gen)</strong></p>
</li>
<li><p>using magic-function, 'next', get batch</p>
<p><strong>batch = next(train_gen_out)</strong></p>
</li>
</ol>
<p>I think that there will be no memory problem but it's occurred.
What is the problem and how to solve it?
Thanks.</p>
",Training and Model Evaluation,kera gpu memory overflow using kera utils sequence generator dataset py train py try run code epoch well progressed epoch occurs gpu memory overflow error like think data generation code generate batch order train py initialize datageneratr class sequence model implemented dataset py train gen datagenerator train id img dir mask dir img size args resizing batch size args batch size valid gen datagenerator valid id img dir mask dir img size args resizing batch size args batch size first function train model convert datagenerator sequence model generator using function iter sequence infinite train gen iter sequence infinite train gen valid gen iter sequence infinite valid gen using magic function next get batch batch next train gen think memory problem occurred problem solve thanks
Is there a way to extend the Vocabulary size of pre-trained Embeddings (re-train word Embeddings on custom dataset on top of existing)?,"<p><strong>TL;DR: Is there a way in <code>Gensim</code> to use the existing Embedding and extend it's vocab + learning by training it again on your custom data set?</strong></p>
<p>One can simply train their own Embedding in <code>Gensim</code> as:</p>
<pre><code>from gensim.models import FastText # or any other model
corpus = your_custom_text_corpus

model = FastText(vector_size=4, window=3, min_count=1)  # instantiate
model.build_vocab(corpus)
model.train(corpus, total_examples=len(common_texts), epochs=10)
</code></pre>
<p>Secondly, you can load the Embeddings in <code>Keras / Tensorflow / PyTorch</code> and set the <code>trainable = True</code> as:
<code>layers.Embedding(vocab_size, embed_dim, weights=[embedding_matrix], trainable = True)</code> which might help you to update your Embedding context faster and maybe more usable.</p>
<p>Thirdly, you can <a href=""https://github.com/mfaruqui/retrofitting"" rel=""nofollow noreferrer"">use Retrofitting to extend the existing knowledge</a> but you can't extend the vocab in that one.</p>
<p>I have a labelled training data of <code>SCIENCE / PCMB / STEM)</code> for over 100000 instances for classification task. <strong>BUT I have over 1.5 Million unlabelled data</strong> which can help me train my corpus from scratch to make new <code>Glove / Word2Vec</code> from scratch.</p>
<p>What I want to know to is:</p>
<p><strong>Is there a way to use the existing trained Embedding and then using that as a base, add some vocab to it</strong>? So that when I train my data of 100000 instances, it'll be relatively easy and it'll be based on top of the Old Learning too, but a bit shifted my dataset?</p>
<p><a href=""https://datascience.stackexchange.com/questions/33792/how-to-retrain-glove-vectors-on-top-of-my-own-data"">A <em>Kind Of idea</em> is  given here</a>. Can someone confirm if it can be used?</p>
",Training and Model Evaluation,way extend vocabulary size pre trained embeddings train word embeddings custom dataset top existing tl dr way use existing embedding extend vocab learning training custom data set one simply train embedding secondly load embeddings set might help update embedding context faster maybe usable thirdly use retrofitting extend existing knowledge extend vocab one labelled training data instance classification task million unlabelled data help train corpus scratch make new scratch want know way use existing trained embedding using base add vocab train data instance relatively easy based top old learning bit shifted dataset href em kind idea given someone confirm used
How to set vocabulary size in python tokenizers library?,"<p>I would like to fit my own tokenizer and use it further for the pre-trained model, however, when fitting a new tokenizer there seems to be no way to choose the vocabulary size. So when I call <code>tokenizer.get_vocab()</code> it always returns a dictionary with 30000 elements. How do I change that? Here is what I do:</p>
<pre><code>from tokenizers import Tokenizer
from tokenizers.models import BPE
tokenizer = Tokenizer(BPE(unk_token=&quot;[UNK]&quot;))

from tokenizers.trainers import BpeTrainer
trainer = BpeTrainer(special_tokens=[&quot;[UNK]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;, &quot;[MASK]&quot;])

from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer = Whitespace()

tokenizer.train(['transcripts.raw'], trainer) # Here there are no additional arguments for some reason
</code></pre>
",Training and Model Evaluation,set vocabulary size python tokenizers library would like fit tokenizer use pre trained model however fitting new tokenizer seems way choose vocabulary size call always return dictionary element change
ValueError: tf.function-decorated function tried to create variables on non-first call while using Custom Loss Function,"<p>I am trying to create a triplet loss function to calculate the similarity between two sentences as follows:</p>
<pre class=""lang-py prettyprint-override""><code>def TripletLoss(y_true,y_pred, margin=0.25,batch_size = 64):
    v1, v2 = y_pred[:,:128],y_pred[:,-128:]
    scores = K.dot(v1, K.transpose(v2))
    positive = tf.linalg.diag_part(scores)
    negative_without_positive = scores - 2 * K.eye(batch_size)

    closest_negative = tf.reduce_max(negative_without_positive, axis=1)

    negative_zero_on_duplicate = scores * (1.0 - K.eye(batch_size))
    
    mean_negative = K.sum(negative_zero_on_duplicate, axis=1) / (batch_size-1)
    
    triplet_loss1 = K.maximum(0.0, margin - positive + closest_negative)
    
    triplet_loss2 = K.maximum(0.0, margin - positive + mean_negative)
    
    triplet_loss = K.mean(triplet_loss1 + triplet_loss2)

    return triplet_loss
</code></pre>
<p>My Model is as follows:</p>
<pre class=""lang-py prettyprint-override""><code>input1 = keras.Input(shape=(train_data1.shape[1],))
input2 = keras.Input(shape=(train_data1.shape[1],))

encoding1 = base_model(input1)
encoding2 = base_model(input2)

merged = layers.Concatenate()([encoding1, encoding2])

model = models.Model(inputs = [input1, input2], outputs = merged)
</code></pre>
<p>where the base model is:</p>
<pre class=""lang-py prettyprint-override""><code>def calculate_mean(x, axis=1):
    return K.mean(x, axis=axis)

def normalize(x):
        return x / K.sqrt(K.sum(x * x, axis=-1, keepdims=True))

base_model = models.Sequential()
base_model.add(layers.Embedding(input_dim=len(vocab)+2, output_dim=128))
base_model.add(layers.LSTM(128, return_sequences=True))
base_model.add(layers.Lambda(calculate_mean, name='mean'))
base_model.add(layers.Lambda(normalize, name='normalize'))
</code></pre>
<p>Now when I use that loss function to compile the model with</p>
<pre class=""lang-py prettyprint-override""><code>model.compile(
    optimizer = Adam(0.001),
    loss = TripletLoss
)
</code></pre>
<p>It does not give any error. But when I train it using the fit method it gives me errors as:</p>
<pre class=""lang-py prettyprint-override""><code>ValueError: tf.function-decorated function tried to create variables on non-first call.
</code></pre>
<p>if I use other losses it works perfectly. I don't know what is wrong with the loss function here.</p>
",Training and Model Evaluation,valueerror tf function decorated function tried create variable non first call using custom loss function trying create triplet loss function calculate similarity two sentence follows model follows base model use loss function compile model doe give error train using fit method give error use loss work perfectly know wrong loss function
difference between model-best and model-last in spacy,"<p>after model training, spacy has generated <code>model\model-best</code> and <code>model\model-last</code> folders. What's the difference between the two models and which one should be used for predictions?</p>
",Training and Model Evaluation,difference model best model last spacy model training spacy ha generated folder difference two model one used prediction
BERT Domain Adaptation,"<p>I am using <code>transformers.BertForMaskedLM</code> to further pre-train the BERT model on my custom dataset. I first serialize all the text to a <code>.txt</code> file by separating the words by a whitespace. Then, I am using <code>transformers.TextDataset</code> to load the serialized data with a BERT tokenizer given as <code>tokenizer</code> argument. Then, I am using <code>BertForMaskedLM.from_pretrained()</code> to load the pre-trained model (which is what <code>transformers</code> library presents). Then, I am using <code>transformers.Trainer</code> to further pre-train the model on my custom dataset, i.e., domain adaptation, for 3 epochs. I save the model with <code>trainer.save_model()</code>. Then, I want to load the further pre-trained model to get the embeddings of the words in my custom dataset. To load the model, I am using <code>AutoModel.from_pretrained()</code> but this pops up a warning.</p>
<pre><code>Some weights of the model checkpoint at {path to my further pre-trained model} were not used when initializing BertModel
</code></pre>
<p>So, I know why this pops up. Because I further pre-trained using <code>transformers.BertForMaskedLM</code> but when I load with <code>transformers.AutoModel</code>, it loads it as <code>transformers.BertModel</code>. What I do not understand is if this is a problem or not. I just want to get the embeddings, e.g., embedding vector with a size of 768.</p>
",Training and Model Evaluation,bert domain adaptation using pre train bert model custom dataset first serialize text file separating word whitespace using load serialized data bert tokenizer given argument using load pre trained model library present using pre train model custom dataset e domain adaptation epoch save model want load pre trained model get embeddings word custom dataset load model using pop warning know pop pre trained using load load understand problem want get embeddings e g embedding vector size
LSTM Seq2Seq model accuracy stuck at ~80%,"<p>I am training an LSTM Seq2Seq model to solve an address parsing problem. The problem entails taking an initial address string (such as &quot;184 Park Ave, NYC, NY&quot;) and breaking it into its individual components: {number: 184, street: Park Ave, city: NYC, region: NY}.</p>
<p>I am using an LSTM model to take in character embedded sequences (length=80) of the initial address string and output a sequence of individual address components (length=153). My vocabulary size is 41. Each character, number, or special character is mapped to a numerical value between 0 and 41 including spaces.</p>
<p>Below I have attached the code to retrieve the training and testing datasets:</p>
<pre><code>test1 = list()
train2 = list()
test2 = list()

X_train, X_test, y_train, y_test = np.load(&quot;X_train.npy&quot;), np.load(&quot;X_test.npy&quot;), np.load(&quot;y_train.npy&quot;), np.load(&quot;y_test.npy&quot;),  

with tensorflow.device('/device:GPU:0'):
  for seq1, seq2, seq3, seq4 in zip(X_train, y_train, X_test, y_test): 
    train1.append(to_categorical(seq1, num_classes=40+1))
    test1.append(to_categorical(seq3, num_classes=40+1))
    train2.append(to_categorical(seq2, num_classes=40+1))
    test2.append(to_categorical(seq4, num_classes=40+1))

train1, test1, train2, test2 = np.array(train1), np.array(test1), np.array(train2), np.array(test2)

print(train1.shape, test1.shape, train2.shape, test2.shape)
</code></pre>
<p>-&gt; (100000, 80, 41) (100000, 80, 41) (100000, 153, 41) (100000, 153, 41)</p>
<p>When training the model on these sequences, I reach a training accuracy of <strong>~80%</strong> after about 10,000 examples and the model suddenly stop improving. Below is the code for my model:</p>
<pre><code>model = Sequential()
model.add(LSTM(100, input_shape=(n_in_seq_length, 41)))
model.add(RepeatVector(n_out_seq_length))
model.add(LSTM(100))
model.add(RepeatVector(n_out_seq_length))
model.add(LSTM(100, return_sequences=True))
model.add(TimeDistributed(Dense(41, activation='softmax')))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

print(model.summary())

with tensorflow.device('/device:GPU:0'):
  model.fit(train1, train2, epochs=1, batch_size=n_batch)
</code></pre>
<p>I have tried GRUs and Vanilla RNNs along with various model architectures and number of hidden layers but nothing seems to improve performance. My goal is to train the model to around <strong>95%</strong> accuracy. How do I improve my model's training accuracy?</p>
<p>Any feedback would be much appreciated. Thank you!</p>
<p><strong>------------------Edit--------------------</strong></p>
<p>The dataset files are located at: <a href=""https://drive.google.com/drive/folders/1onv-NUY_8Lx0v--YKXAFh5IrQlNSCLSN?usp=sharing"" rel=""nofollow noreferrer"">here</a></p>
",Training and Model Evaluation,lstm seq seq model accuracy stuck training lstm seq seq model solve address parsing problem problem entail taking initial address string park ave nyc ny breaking individual component number street park ave city nyc region ny using lstm model take character embedded sequence length initial address string output sequence individual address component length vocabulary size character number special character mapped numerical value including space attached code retrieve training testing datasets training model sequence reach training accuracy example model suddenly stop improving code model tried grus vanilla rnns along various model architecture number hidden layer nothing seems improve performance goal train model around accuracy improve model training accuracy feedback would much appreciated thank edit dataset file located
Applying RAND index with cluster numbers and cluster labels,"<p>I have a set of reviews and I've clustered them with k-means and got the clusters each review belongs to (Ex: 1,2,3...). I also have the real labels of which clusters these belongs to Ex: location, food etc.) and I need to compare them with Rand index.</p>
<p>As I have cluster numbers and cluster labels how I can I apply Rand index to compare?</p>
<p>Is there any intermediate step that I should follow?</p>
<p>Edit:
I've seen the post <a href=""https://stackoverflow.com/questions/49586742/rand-index-function-clustering-performance-evaluation"">Rand Index function (clustering performance evaluation)</a> but it does not answer my question.</p>
<p>In that question, you have</p>
<pre><code>labels_true = [1, 1, 0, 0, 0, 0]
labels_pred = [0, 0, 0, 1, 0, 1]
</code></pre>
<p>but what I have is something like below,</p>
<pre><code>labels_true = ['food', 'view', 'room', 'food', 'staff', 'staff']
labels_pred = [0, 0, 0, 1, 0, 1]
</code></pre>
<p>Any help is highly appreciated.</p>
",Training and Model Evaluation,applying rand index cluster number cluster label set review clustered k mean got cluster review belongs ex also real label cluster belongs ex location food etc need compare rand index cluster number cluster label apply rand index compare intermediate step follow edit seen post href index function clustering performance evaluation doe answer question question something like help highly appreciated
how I can find almost the same texts?,"<p>I have a column of job postings. I want to remove almost the same texts. They are very similar to each other but they have a small differences so drop_duplications function doesn't work. I tried the following code but I like to find a better way since this one is not very accurate.
In this method I am taking 150 letter of the text and search in al of other text and find similar ones and at the end I keep one and delete all other ones.</p>
<pre><code>bad=[]
ind=[]
ids_sub=[]
datatext=data.copy()
for i in range(datatext.shape[0]):
    datatext['originalText'].values[i]=str(datatext['originalText'].values[i]).replace('(', '').replace(')', '').replace('+', ' ').replace('?', ' ').replace('*', ' ').replace('|',' ').replace('-',' ').replace(':',' ').replace('@',' ')
    datatext['originalText'].values[i]=str(datatext['originalText'].values[i]).replace(&quot;\'&quot;, ' ')
    doc=datatext['originalText'].values[i][350:550]
    if len(doc)==200 and doc!=' ':
                u=datatext[datatext['originalText'].str.contains('|'.join([str(doc)]),na=False)]
                datatext=datatext[~datatext.index.isin(u[1:].index)]
                print(datatext.shape, u.shape,datatext['id'][i+1:i+2])
</code></pre>
<p>do u have a better method? How I can get almost similar text? any NLP method?</p>
<p>For example two following texts are almost the same:</p>
<p><code>&quot;Full job description\nCompany Description\n\n\nARE YOU FORGED FOR AIM RECYCLING?\n\n\nRecycles globally. Join us in our mission to recycle more than 3,000,000 tonnes of metals worldwide each year. At AIM Recycling, we recycle metals to their maximum capacity. For over 80 years, we've been working together to make a positive difference.\n\n\nBe part of our team to contribute to the growth of our company and to support our recycling activities in North America. It's simple: we do it right. We strive for excellence.\n\n\nJob Description\n\n\nUnder the supervision of Manager Asset Data Analytics, the incumbent will be responsible for supporting the deployment of data management and analytical strategies related to the asset fleet of our AIM sites (90 sites) in order to meet asset management excellence objectives.\n\n\nPosition Summary\n\n\nAnalyze data related to AIM's asset fleet to facilitate decision making related to the purchase, allocation, sale, and disposal of mobile and fixed asset in all our Recycling, Kenny and Feeder yard locations.\n\n\nMake recommendations to maximize the value of the equipment fleet and increase its operational availability.\n\n\nParticipate in the drafting of procedures or process mapping related to the mobile asset purchase, asset transfer, and asset disposal processes.\n\n\n\nAsset Maintenance Strategy:\n\nConduct useful life analysis assessment activities for the asset fleet considering asset life cycle data and failure mode patterns.\n\n\nAnalyze financial (acquisition cost, rebuild cost, NBV, depreciation) and maintenance data (cost, estimated life) related to the assets to proceed with data modeling that will include financial indicators such as PV, NPV scenarios.\n\n\nPerform a company wide asset reconciliation for high-level value assets so that the IT maintenance system (Maximo) and financial accounting system (365) reflect the field data (the physical asset inventory)\n\n\nConduct and report on internal and external maintenance cost analyses. Participate in the capital asset replacement plan.\n\n\n\nAsset monitoring_ Business Intelligence and Telematics\n\nLead and facilitate the implementation of specific programs related to the asset management for mobile fleet of heavy vehicles such as the development of daily inspection sheets via electronic tablet.\n\n\nCarry out various business intelligence and telematics projects related to assets in collaboration with the IT department. Promote the tools and ensure users support in the use of these reports and tools.\n\n\nAnalyze data and programs related to our assets in collaboration with the maintenance and finance departments.\n\n\nCollaborate in the validation and update of all site’s global assets/high-level assets register for the Recycling, Kenny and Feeder yard sites. This includes working with the Site Managers to verify/audit the assert inventory at our sites.\n\n\n\nAsset Life Cycle and Performance\n\nDevelop and maintain a set of key performance indicators and targets via business intelligence reports measuring the effectiveness of acquisition programs, asset utilization and maintenance costs for each asset category/site.\n\n\nSupport sites as needed in their asset disposal activities. Conduct asset disposition/sale activities at the appropriate time in the asset lifecycle, with the assistance of a web/auction tool.\n\n\nQualifications\n\n\n\nUniversity degree in engineering or computer science / or equivalent experience\nMaster's degree (an asset)\nSix sigma certification, process mapping (an asset)\nMinimum of 10 years of experience in data analysis/business intelligence\nKnowledge of data manipulation/programming in Advanced Excel, Macro, Pivot table, VLOOKUP\nPrevious experience in financial analysis (desirable)\nSkill in updating SharePoint content (asset) &amp; Cognos (strong asset);\nFamiliarity with Maximo, Visio, Outlook, Excel advanced and 365.\nKnowledge of heavy equipment vehicles, fleet management (an asset)\nHave a valid driver's license.\n\nThe position requires occasional travel\n\n\nAdditional Information\n\n\n\nWorking hours:\n 40 hours/week from Monday to Friday\n\n\nPermanent position Full time Stimulating, dynamic and pleasant work environment\n\n\n\nWhat we offer:\n\nGroup insurance; Group RRSP; Free coffee; Free parking; Subsidized dinner; Gym on site; Bonus plan; Social events (BBQ, Taffy on snow, raffles, etc.).\n\n\nThe American Iron &amp; Metal Company and its subsidiaries offer equal employment opportunities to all. The masculine is only used to lighten the text. Only those selected for an interview will be contacted.&quot;</code></p>
<p>'\n\n\n\n\n\n\n\n\n\nCompany Description\nARE YOU FORGED FOR AIM RECYCLING \nRecycles globally.\nJoin us in our mission to recycle more than 3,000,000 tons of metals worldwide each year. At AIM Recyclage, we recycle metals to the maximum of their capacity. For more than 80 years, we have been working together to make a positive difference. Be part of our team to help grow our company and support our recycling activities in North America.\nIt s simple  we do it well. We strive for excellence.\nJob Description\nUnder the supervision of Manager Asset Data Analytics, the incumbent will be responsible for supporting the deployment of data management and analytical strategies related to the asset fleet of our AIM sites 90 sites in order to meet asset management excellence objectives.\nAnalyze data related to AIM s asset fleet to facilitate decision making related to the purchase, allocation, sale, and disposal of mobile and fixed asset in all our Recycling, Kenny and Feeder yard locations;\nMake recommendations to maximize the value of the equipment fleet and increase its operational availability;\nParticipate in the drafting of procedures or process mapping related to the mobile asset purchase, asset transfer, and asset disposal processes.\nAsset Maintenance Strategy\nConduct useful life analysis assessment activities for the asset fleet considering asset life cycle data and failure mode patterns;\nAnalyze financial acquisition cost, rebuild cost, NBV, depreciation and maintenance data cost, estimated life related to the assets to proceed with data modeling that will include financial indicators such as PV, NPV scenarios;\nPerform a company wide asset reconciliation for high level value assets so that the IT maintenance system Maximo and financial accounting system 365 reflect the field data the physical asset inventory;\nConduct and report on internal and external maintenance cost analyses. Participate in the capital asset replacement plan\nAsset monitoring_ Business Intelligence and Telematics \nLead and facilitate the implementation of specific programs related to the asset management for mobile fleet of heavy vehicles such as the development of daily inspection sheets via electronic tablet;\nCarry out various business intelligence and telematics projects related to assets in collaboration with the IT department. Promote the tools and ensure users support in the use of these reports and tools;\nAnalyze data and programs related to our assets in collaboration with the maintenance and finance departments;\nCollaborate in the validation and update of all site’s global assets/high level assets register for the Recycling, Kenny and Feeder yard sites. This includes working with the Site Managers to verify/audit the assert inventory at our sites.\nAsset Life Cycle And Performance\nDevelop and maintain a set of key performance indicators and targets via business intelligence reports measuring the effectiveness of acquisition programs, asset utilization and maintenance costs for each asset category/site;\nSupport sites as needed in their asset disposal activities. Conduct asset disposition/sale activities at the appropriate time in the asset lifecycle, with the assistance of a web/auction tool;\nCollaborate in value asset evaluation activity related to our new site acquisitions when required and issue asset market value for sale. Coordinate the sale process between the various sites, finance, site managers and buyers;\nParticipate in management of new requests/emails for the Asset Management team as required;\nParticipate in the planning and organization related to asset fleet for all areas of AIM and various one time projects.\nQualifications\nUniversity degree in engineering or computer science / or equivalent experience;\nMaster s degree an asset;\nSix sigma certification, process mapping an asset;\nMinimum of 10 years of experience in data analysis/business intelligence;\nKnowledge of data manipulation/programming in Advanced Excel, Macro, Pivot table, VLOOKUP;\nPrevious experience in financial analysis desirable;\nSkill in updating SharePoint content asset &amp; Cognos strong asset;\nFamiliarity with Maximo, Visio, Outlook, Excel advanced and 365;\nKnowledge of heavy equipment vehicles, fleet management an asset;\nHave a valid driver s license.\nThe position requires occasional travel\nAdditional Information\nWorking hours40 hours/week From Monday to Friday;\nType of employment  permanent, full time.\nWhat We Offer\nGroup insurance after 3 months;\nGroup RRSP after 6 months;\nFree coffee and parking;\nSubsidized dinner;\nGym on site;\nSocial events BBQ, Snow Shoot, Draws, etc..\nThe American Iron &amp; Metals Company and its subsidiaries offer equal employment opportunities to all. The masculine is only used to lighten the text. Only those selected for interview will be contacted.\n      \n\n\n\n        Show more\n\n        \n\n\n\n\n\n        Show less\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n            Seniority level\n          \n\n\n\n            Associate\n          \n\n\n\n\n\n\n\n          Employment type\n        \n\n\n\n          Full time\n        \n\n\n\n\n\n\n\n            Job function\n          \n\n\n\n            Information Technology\n          \n\n\n\n\n\n\n\n            Industries\n          \n\n\n\n          Logistics and Supply Chain, Financial Services, and Accounting\n          \n\n\n\n\n\n'</p>
",Training and Model Evaluation,find almost text column job posting want remove almost text similar small difference drop duplication function work tried following code like find better way since one accurate method taking letter text search al text find similar one end keep one delete one u better method get almost similar text nlp method example two following text almost n n n n n n n n n ncompany description nare forged aim recycling nrecycles globally njoin u mission recycle ton metal worldwide year aim recyclage recycle metal maximum capacity year working together make positive difference part team help grow company support recycling activity north america nit simple well strive excellence njob description nunder supervision manager asset data analytics incumbent responsible supporting deployment data management analytical strategy related asset fleet aim site site order meet asset management excellence objective nanalyze data related aim asset fleet facilitate decision making related purchase allocation sale disposal mobile fixed asset recycling kenny feeder yard location nmake recommendation maximize value equipment fleet increase operational availability nparticipate drafting procedure process mapping related mobile asset purchase asset transfer asset disposal process nasset maintenance strategy nconduct useful life analysis assessment activity asset fleet considering asset life cycle data failure mode pattern nanalyze financial cost rebuild cost nbv depreciation maintenance data cost estimated life related asset proceed data modeling include financial indicator pv npv scenario nperform company wide asset reconciliation high level value asset maintenance system maximo financial accounting system reflect field data physical asset inventory nconduct report internal external maintenance cost analysis capital asset replacement plan nasset monitoring business intelligence telematics nlead facilitate implementation specific program related asset management mobile fleet heavy vehicle development daily inspection sheet via electronic tablet ncarry various business intelligence telematics project related asset department promote tool ensure user support use report tool nanalyze data program related asset maintenance finance department ncollaborate validation update site global asset high level asset register recycling kenny feeder yard site includes working site manager verify audit assert inventory site nasset life cycle performance ndevelop maintain set key performance indicator target via business intelligence report measuring effectiveness program asset utilization maintenance cost asset category site nsupport site needed asset disposal activity conduct asset disposition sale activity appropriate time asset lifecycle assistance web auction tool ncollaborate value asset evaluation activity related new site required issue asset market value sale coordinate sale process various site finance site manager buyer nparticipate management new request email asset management team required nparticipate planning organization related asset fleet area aim various one time project nqualifications nuniversity degree engineering computer science equivalent experience nmaster degree asset nsix sigma certification process mapping asset nminimum year experience data analysis business intelligence nknowledge data manipulation programming advanced excel macro pivot table vlookup nprevious experience financial analysis desirable nskill updating sharepoint content asset cognos strong asset nfamiliarity maximo visio outlook excel advanced nknowledge heavy equipment vehicle fleet management asset nhave valid driver license position requires occasional travel nadditional information nworking hour hour week monday friday ntype employment permanent full time nwhat offer ngroup insurance month ngroup rrsp month nfree coffee parking nsubsidized dinner ngym site nsocial event bbq snow shoot draw etc american iron metal company subsidiary offer equal employment opportunity masculine used lighten text selected interview contacted n n n n n show n n n n n n n n show le n n n n n n n n n n n n n n n n seniority level n n n n n associate n n n n n n n n n employment type n n n n n full time n n n n n n n n n job function n n n n n information technology n n n n n n n n n industry n n n n n logistics supply chain financial service accounting n n n n n n n
Instances required for word2vec to learn new word,"<p>I am new to NLP. I am learning about word2vec and trying to understand when it is useful vs not. In this case I'm trying to figure out if word2vec be useful for looking at new words/usages as they develop?
I would like to know ballpark how many instances/samples (in context) are required for a pre-trained word2vec model to learn a new word.
I have seen information on the total number of words overall required to train a model but can't find this.
Thanks for any advice.</p>
",Training and Model Evaluation,instance required word vec learn new word new nlp learning word vec trying understand useful v case trying figure word vec useful looking new word usage develop would like know ballpark many instance sample context required pre trained word vec model learn new word seen information total number word overall required train model find thanks advice
Training Tesseract for Urdu language,"<p>I want to train Tesseract for Urdu language. I have installed Tesseract 4.00 and have a representative dataset for Urdu available. I have dataset with images like the one attached:</p>
<p><a href=""https://i.sstatic.net/0fNXH.png"" rel=""nofollow noreferrer"">Training Image</a></p>
<p>I have the label for each image e.g the label for the above image is:</p>
<p>بعد نجی ٹی وی سے گفتگو کرتے ہوئے وزیر خارجہ شاہ محمود قریشی نے بتایا کہ ملاقات</p>
<p>Images are in these fonts: Pak Nastaleeq, Alvi Nastaleeq, Jameel Noori Nastaleeq, Nafees Nastaleeq.</p>
<p>What is the process of training tesseract? How do I tag (make boxes around Urdu word) each Urdu word in the images?</p>
",Training and Model Evaluation,training tesseract urdu language want train tesseract urdu language installed tesseract representative dataset urdu available dataset image like one attached training image label image e g label image image font pak nastaleeq alvi nastaleeq jameel noori nastaleeq nafees nastaleeq process training tesseract tag make box around urdu word urdu word image
Entropy Calculation as a Linguist Feature,"<p>I was going through a paper on Sarcasm Detection which proposed a Hybrid Attention Based LSTM. The Model included some linguistic features to be extracted first, which included Entropy of a word in a sentence. Unfortunately, the paper does not explain it very well. The explanation given is:</p>
<blockquote>
<p>The entropy of a word is defined as the degree of randomness of a word
concerning the sentence. It is calculated using the following
equation:<br/>S(A/B) = (p(ai / bj) ∗ log(p(ai))) / p(bj)<br/> where
S(A/B) represents the probability of the word ‘A’ with respect to the
sentence ‘B’, and p(ai / bj) means the probability of ai given bi.</p>
</blockquote>
<p>If someone understands the terms, kindly explain these.</p>
<p>Title of Paper: <strong>Hybrid attention-based Long Short-Term Memory network for
sarcasm identification</strong><br/>
<a href=""https://www.sciencedirect.com/science/article/abs/pii/S1568494621002714#b41"" rel=""nofollow noreferrer"">Link to Paper</a></p>
",Training and Model Evaluation,entropy calculation linguist feature wa going paper sarcasm detection proposed hybrid attention based lstm model included linguistic feature extracted first included entropy word sentence unfortunately paper doe explain well explanation given entropy word defined degree randomness word concerning sentence calculated using following equation b p ai bj log p ai p bj b represents probability word respect sentence b p ai bj mean probability ai given bi someone understands term kindly explain title paper hybrid attention based long short term memory network sarcasm identification link paper
Retrain a BERT Model,"<p>I have trained a BERT model using pytorch for about a million text data for a classification task. After testing this model with new data I get False Positives and False Negatives. Now I want retrain the existing model only with FN and FP. I do not want to append the FN and FP to the exisiting dataset and then train the entire model again. How do I retrain this bert model only with these FN and Fp over the previosuly trained model.</p>
",Training and Model Evaluation,retrain bert model trained bert model using pytorch million text data classification task testing model new data get false positive false negative want retrain existing model fn fp want append fn fp exisiting dataset train entire model retrain bert model fn fp previosuly trained model
Has anyone done training on custom data using AllenNLP for coreference resolution?,"<p>I'm trying to train AllenNLP on custom data instead of using the pre-trained model for coreference resolution. The instructions are <a href=""https://demo.allennlp.org/coreference-resolution"" rel=""nofollow noreferrer"">here</a> but they are very vague and I am not sure how to progress, in particular I don't know how to modify the JSONNET file to indicate the path to my train, test and dev ConLL-2012 training files. Has anyone ever accomplished this before? Thank you very much.</p>
",Training and Model Evaluation,ha anyone done training custom data using allennlp coreference resolution trying train allennlp custom data instead using pre trained model coreference resolution instruction vague sure progress particular know modify jsonnet file indicate path train test dev conll training file ha anyone ever accomplished thank much
"spaCy, preparing training data: doc.char_span returning &#39;None&#39;","<p>I'm following the instructions in spaCy's documentation to prepare my own training data (<a href=""https://spacy.io/usage/training#training-data"" rel=""nofollow noreferrer"">here</a>).</p>
<p>My problem begins at this line:</p>
<pre><code>span = doc.char_span(start, end, label=label)
</code></pre>
<p>For entities which I'm labelling as an organization ('ORG'), it seems to work fine i.e. it returns a span object. However, for entities which I'm labelling as money ('MONEY'), it returns a None object.</p>
<p>Here's two examples from my training set:</p>
<pre><code>('Payments from the Guardian, Kings Place, 90 York Way, London N1 9GU, for articles:', [(18, 26, 'ORG')]) // Returns a span object for 'Guardian'

('24 July 2020, received Â£100. Hours: 1 hr. (Registered 02 February 2021)', [(24, 28, 'MONEY')]) // Returns None for '£100'
</code></pre>
<p>Note: the Â appears in the console, but it's not in the original json text file. Leaving it in in case it's somehow part of the issue</p>
<p>Does anyone please have any suggestions where I'm going wrong?</p>
<p>[I'm very new to spacy (started learning last week), so please ELI5!]</p>
<p><strong>UPDATE: As it seems the Â could be the problem, below is how I'm loading the data. How do I get rid of the Â's? (which aren't visible in the original file)</strong></p>
<pre><code>with open('training_data.json') as train_data:
    train_data_json = json.load(train_data)
</code></pre>
",Training and Model Evaluation,spacy preparing training data doc char span returning none following instruction spacy documentation prepare training data problem begin line entity labelling organization org seems work fine e return span object however entity labelling money money return none object two example training set note appears console original json text file leaving case somehow part issue doe anyone please suggestion going wrong new spacy started learning last week please eli update seems could problem loading data get rid visible original file
What are co-occurence matrixes and how are they used in NLP?,"<p>The <a href=""https://pypi.python.org/pypi/google-ngram-downloader/"" rel=""noreferrer"">pypi docs for a google ngram downloader</a> say that &quot;sometimes you need an aggregate data over the dataset. For example to build a co-occurrence matrix.&quot;</p>
<p>The wikipedia for co-occurence matrix has to do with image processing and googling the term seems to bring up some sort of SEO trick.</p>
<p>So what are co-occurrence matrixes (in computational linguistics/NLP)? How are they used in NLP?</p>
",Training and Model Evaluation,co occurence matrix used nlp pypi doc google ngram downloader say sometimes need aggregate data dataset example build co occurrence matrix wikipedia co occurence matrix ha image processing googling term seems bring sort seo trick co occurrence matrix computational linguistics nlp used nlp
Find index of word2vec result in dataframe,"<p>I am building a book recommendation system using word2vec model. Where the outcome is the top 10 recommended books.</p>
<p>Training data</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Book_id</th>
<th>Book_Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>10201</td>
<td>A Scanner Darkly</td>
<td>some description</td>
</tr>
<tr>
<td>10210</td>
<td>Absalom, Absalom!</td>
<td>some description</td>
</tr>
</tbody>
</table>
</div>
<pre><code>from genism.models import Word2Vec
df=pd.read_csv(&quot;books.csv&quot;)
sentences=[i.split(&quot; &quot;) for i in df['Description']]
model=Word2Vec(sentences,min_count=1,workers=4,epochs=5,window=20)
model.wv.most_similar('scanner',topn=2)
</code></pre>
<p>Output -</p>
<pre><code>[
   ('scanner',0.87455466655556666),
   ('absalom',0.71455466655556666),
]
</code></pre>
<p>The code predicted the correct keywords. Now I need to extract the course_id for the given keywords from dataframe. so the <strong>output</strong> will be</p>
<pre><code>[
     (10201,'A Scanner Darkly','scanner',0.87455466655556666),
     ('10210','Absalom, Absalom!','absalom',0.71455466655556666),   
]
</code></pre>
<p>I tried using <code>model.wv.index_to_key</code> but it's incorrect. I can find index in <code>Doc2Vec</code> by using <strong>TaggedDocument</strong> but how can perform the same in <code>Word2Vec</code> to get the index of keyword in dataframe.</p>
",Training and Model Evaluation,find index word vec result dataframe building book recommendation system using word vec model outcome top recommended book training data book id book name description scanner darkly description absalom absalom description output code predicted correct keywords need extract course id given keywords dataframe output tried using incorrect find index using taggeddocument perform get index keyword dataframe
Keras ROC different from Scikit ROC?,"<p>From the code below, it looks like evaluating the roc with keras and with scikit actually makes a difference. Does anybody know an explanation?</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from keras.layers import Dense, Input, Dropout
from keras import Sequential
import keras
from keras.constraints import maxnorm
from sklearn.metrics import roc_auc_score

# training data: X_train, y_train
# validation data: X_valid, y_valid

# Define the custom callback we will be using to evaluate roc with scikit
class MyCustomCallback(tf.keras.callbacks.Callback):

    def on_epoch_end(self,epoch, logs=None):
        y_pred = model.predict(X_valid)
        print(""roc evaluated with scikit = "",roc_auc_score(y_valid, y_pred))
        return

# Define the model.

def model(): 

    METRICS = [ 
          tf.keras.metrics.BinaryAccuracy(name='accuracy'),
          tf.keras.metrics.AUC(name='auc'),
    ]

    optimizer=""adam""
    dropout=0.1
    init='uniform'
    nbr_features= vocab_size-1 #2500
    dense_nparams=256

    model = Sequential()
    model.add(Dense(dense_nparams, activation='relu', input_shape=(nbr_features,), kernel_initializer=init,  kernel_constraint=maxnorm(3)))
    model.add(Dropout(dropout))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer=optimizer,metrics = METRICS)
    return model

# instantiate the model
model = model()

# fit the model
history = model.fit(x=X_train, y=y_train, batch_size = 8, epochs = 8, verbose=1,validation_data = (X_valid,y_valid), callbacks=[MyCustomCallback()], shuffle=True, validation_freq=1, max_queue_size=10, workers=4, use_multiprocessing=True)
</code></pre>

<p>Output:</p>

<pre><code>Train on 4000 samples, validate on 1000 samples
Epoch 1/8
4000/4000 [==============================] - 15s 4ms/step - loss: 0.7950 - accuracy: 0.7149 - auc: 0.7213 - val_loss: 0.7551 - val_accuracy: 0.7608 - val_auc: 0.7770
roc evaluated with scikit =  0.78766515781747
Epoch 2/8
4000/4000 [==============================] - 15s 4ms/step - loss: 0.0771 - accuracy: 0.8235 - auc: 0.8571 - val_loss: 1.0803 - val_accuracy: 0.8574 - val_auc: 0.8954
roc evaluated with scikit =  0.7795984218252997
Epoch 3/8
4000/4000 [==============================] - 14s 4ms/step - loss: 0.0085 - accuracy: 0.8762 - auc: 0.9162 - val_loss: 1.2084 - val_accuracy: 0.8894 - val_auc: 0.9284
roc evaluated with scikit =  0.7705172905961992
Epoch 4/8
4000/4000 [==============================] - 14s 4ms/step - loss: 0.0025 - accuracy: 0.8982 - auc: 0.9361 - val_loss: 1.1700 - val_accuracy: 0.9054 - val_auc: 0.9424
roc evaluated with scikit =  0.7808804338960933
Epoch 5/8
4000/4000 [==============================] - 14s 4ms/step - loss: 0.0020 - accuracy: 0.9107 - auc: 0.9469 - val_loss: 1.1887 - val_accuracy: 0.9150 - val_auc: 0.9501
roc evaluated with scikit =  0.7811174659489438
Epoch 6/8
4000/4000 [==============================] - 14s 4ms/step - loss: 0.0018 - accuracy: 0.9184 - auc: 0.9529 - val_loss: 1.2036 - val_accuracy: 0.9213 - val_auc: 0.9548
roc evaluated with scikit =  0.7822898825544409
Epoch 7/8
4000/4000 [==============================] - 14s 4ms/step - loss: 0.0017 - accuracy: 0.9238 - auc: 0.9566 - val_loss: 1.2231 - val_accuracy: 0.9258 - val_auc: 0.9579
roc evaluated with scikit =  0.7817036742516923
Epoch 8/8
4000/4000 [==============================] - 14s 4ms/step - loss: 0.0016 - accuracy: 0.9278 - auc: 0.9592 - val_loss: 1.2426 - val_accuracy: 0.9293 - val_auc: 0.9600
roc evaluated with scikit =  0.7817419052279585
</code></pre>

<p>As you may see, from epoch 2 onwards keras' and scikit's validation ROCs begin diverging. The same happens if I fit the model and then use keras' <code>model.evaluate(X_valid, y_valid)</code>. Any help is greatly appreciated. </p>

<p>EDIT: testing the model on a separate test set, I get roc =0.76 so scikit seems to give the correct answer ( btw X_train has 4000 entries, X_valid has 1000 and test has 15000, quite an unconventional splitting but it is forced by external factors).<br>
Also, suggestions on how to improve performance are equally appreciated.</p>

<p>EDIT2: To answer the reply by @arpitrathi, i modified the callbak but unfortunately without success:</p>

<pre><code>class MyCustomCallback(tf.keras.callbacks.Callback):

    def on_epoch_end(self,epoch, logs=None):
        y_pred = model.predict_proba(X_valid)
        print(""roc evaluated with scikit = "",roc_auc_score(y_valid, y_pred))
        return

model = model()

history = model.fit(x=X_trainl, y=y_train, batch_size = 8, epochs = 3, verbose=1,validation_data = (X_valid,y_valid), callbacks=[MyCustomCallback()], shuffle=True, validation_freq=1, max_queue_size=10, workers=4, use_multiprocessing=True)


Train on 4000 samples, validate on 1000 samples
Epoch 1/3
4000/4000 [==============================] - 20s 5ms/step - loss: 0.8266 - accuracy: 0.7261 - auc: 0.7409 - val_loss: 0.7547 - val_accuracy: 0.7627 - val_auc: 0.7881
roc evaluated with scikit =  0.7921764130168828
Epoch 2/3
4000/4000 [==============================] - 15s 4ms/step - loss: 0.0482 - accuracy: 0.8270 - auc: 0.8657 - val_loss: 1.0831 - val_accuracy: 0.8620 - val_auc: 0.9054
roc evaluated with scikit =  0.78525915504445
Epoch 3/3
4000/4000 [==============================] - 15s 4ms/step - loss: 0.0092 - accuracy: 0.8794 - auc: 0.9224 - val_loss: 1.2226 - val_accuracy: 0.8928 - val_auc: 0.9340
roc evaluated with scikit =  0.7705555215724655
</code></pre>

<p>Also, if I plot training and validation accuracy, i see that they both rapidly converge to 1. Is it strange?</p>
",Training and Model Evaluation,kera roc different scikit roc code look like evaluating roc kera scikit actually make difference doe anybody know explanation output may see epoch onwards kera scikit validation roc begin diverging happens fit model use kera help greatly appreciated edit testing model separate test set get roc scikit seems give correct answer btw x train ha entry x valid ha test ha quite unconventional splitting forced external factor also suggestion improve performance equally appreciated edit answer reply arpitrathi modified callbak unfortunately without success also plot training validation accuracy see rapidly converge strange
Is there any ideal parameter values for fasttext train_supervised function?,"<p>I working on NLP problem and try to make text classification with word embedding method. I am training my model with fasttext's train_supervised but is there any ideal or best parameter values for this function that you can advise me also I am using Kfold with some values how can I find best K-fold number in this problem ?</p>
<p>My solution is I am using fasttext's autotune function to find best param values for model to train but is there any possible suggestion to give me ? Following image shows my best params in the model. Finally , I am using fasttext's pretrained word vector model for my training.
<a href=""https://i.sstatic.net/cD4tM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cD4tM.png"" alt=""enter image description here"" /></a></p>
",Training and Model Evaluation,ideal parameter value fasttext train supervised function working nlp problem try make text classification word embedding method training model fasttext train supervised ideal best parameter value function advise also using kfold value find best k fold number problem solution using fasttext autotune function find best param value model train possible suggestion give following image show best params model finally using fasttext pretrained word vector model training
Why does the difference in network architecture make a huge difference in name classification,"<p>I tried to build a RNN by myself following this tutorial <a href=""https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial</a>. I built my own version with this following network architecture, which is different from the tutorial.<a href=""https://i.sstatic.net/yibHW.png"" rel=""nofollow noreferrer"">a stands for input layer, h hidden, o output</a>. Here's my code:</p>
<pre><code>class RNN(nn.Module):
def __init__(self,input_size,hidden_size,output_size,initial_hidden):
    super(RNN, self).__init__()
    self.linear1 = nn.Linear(input_size,hidden_size)
    self.linear2 = nn.Linear(hidden_size,hidden_size,bias=False)
    self.linear3 = nn.Linear(hidden_size,output_size)
    self.prev_hidden = initial_hidden

def forward(self,X):
    input = torch.add(self.linear1(X).view(1,-1),self.linear2(self.prev_hidden.to(device))
    hidden = nn.ReLU()(input)
    self.prev_hidden = hidden.detach()
    output = self.linear3(hidden)
    return output
</code></pre>
<p>This model stops at loss = 12000 over all samples and doesn't really drop anymore. However, after switching to the model described in the tutorial, which the hidden and input layers share the same weight, the loss drops to 4000 with the same hyper parameter. Here's the code:</p>
<pre><code>class RNN(nn.Module):
def __init__(self, input_size, hidden_size, output_size):
    super(RNN, self).__init__()

    self.hidden_size = hidden_size

    self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
    self.i2o = nn.Linear(input_size + hidden_size, output_size)
    self.softmax = nn.LogSoftmax(dim=1)

def forward(self, input, hidden):
    combined = torch.cat((input, hidden), 1)
    hidden = self.i2h(combined)
    output = self.i2o(combined)
    output = self.softmax(output)
    return output, hidden

def initHidden(self):
    return torch.zeros(1, self.hidden_size)
</code></pre>
<p>Why does the model architecture in the tutorial outperforms my version so much?</p>
",Training and Model Evaluation,doe difference network architecture make huge difference name classification tried build rnn following tutorial built version following network architecture different tutorial stand input layer h hidden output code model stop loss sample really drop anymore however switching model described tutorial hidden input layer share weight loss drop hyper parameter code doe model architecture tutorial outperforms version much
BERT finetuning : Is it right to train BERT Classification model at once?,"<p>I have a question about training BERT classification(or pretrained model).</p>
<p>BERT classifier model usually constructed 2 models. BERT model and classifier.</p>
<p>Many BERT fine tuning example code is training BERT model and classifier layer at once.
But I think, classifier is training first and BERT weight should not updated. After classifier trained, training all model layers.</p>
<p>Example</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import BertForSequenceClassification
model = BertForSequenceClassification()
...

# training1
for name, param in model.named_parameters():
    if 'classifier' in name:
        param.requires_grad = True # only classifier update
    else:
        param.requires_grad = False # tied other layer

...
# And after training1, we can using BERT model that is trained only classfier.
model = BertForSequenceClassification()
model.load_state_dict(torch.load({model only trained classifier})
for name, param in model.named_parameters():
    param.requires_grad = True # training all 

# training BERT Classification model
</code></pre>
<p>Why BERT Classification model training at once?
Thank you.</p>
",Training and Model Evaluation,bert finetuning right train bert classification model question training bert classification pretrained model bert classifier model usually constructed model bert model classifier many bert fine tuning example code training bert model classifier layer think classifier training first bert weight updated classifier trained training model layer example bert classification model training thank
Questions when training language models from scratch with Huggingface,"<p>I'm following the guide here (<a href=""https://github.com/huggingface/blog/blob/master/how-to-train.md"" rel=""nofollow noreferrer"">https://github.com/huggingface/blog/blob/master/how-to-train.md</a>, <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">https://huggingface.co/blog/how-to-train</a>) to train a RoBERTa-like model from scratch. (With my own tokenizer and dataset)</p>
<p>However, when I run <strong>run_mlm.py</strong> (<a href=""https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py</a>) to train my model with masking task, the following messages appear:</p>
<pre><code>All model checkpoint weights were used when initializing RobertaForMaskedLM.

All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.

If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.
</code></pre>
<p>I'm wondering does it mean that I'm training from scratch with <strong>&quot;the pretrained weight&quot;</strong> of RoBERTa? And if it's training from the pretrained weights, is there a way to use randomly initiated weights rather than the pretrained ones?</p>
<p>==== 2021/10/26 Updated ===</p>
<p>I  am training the model with Masked Language Modeling task by following commands:</p>
<pre><code>python transformer_run_mlm.py \
--model_name_or_path roberta-base  \
--config_name ./my_dir/ \
--tokenizer_name ./my_dir/ \
--no_use_fast_tokenizer \
--train_file ./my_own_training_file.txt \
--validation_split_percentage 10 \
--line_by_line \
--output_dir /my_output_dir/ \
--do_train \
--do_eval \
--per_device_train_batch_size 64 \
--per_device_eval_batch_size 16 \
--learning_rate 1e-4 \
--max_seq_length 1024 \
--seed 42 \
--num_train_epochs 100 
</code></pre>
<p>The  <strong>./my_dir/</strong> consists of three files:</p>
<p><strong>config.json</strong> produced by the following codes:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import RobertaModel

model = RobertaModel.from_pretrained('roberta-base')
model.config.save_pretrained(MODEL_CONFIG_PATH)
</code></pre>
<p>And here's the content:</p>
<pre><code>{
  &quot;_name_or_path&quot;: &quot;roberta-base&quot;,
  &quot;architectures&quot;: [
    &quot;RobertaForMaskedLM&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;bos_token_id&quot;: 0,
  &quot;classifier_dropout&quot;: null,
  &quot;eos_token_id&quot;: 2,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.1,
  &quot;hidden_size&quot;: 768,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-05,
  &quot;max_position_embeddings&quot;: 514,
  &quot;model_type&quot;: &quot;roberta&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;pad_token_id&quot;: 1,
  &quot;position_embedding_type&quot;: &quot;absolute&quot;,
  &quot;transformers_version&quot;: &quot;4.12.0.dev0&quot;,
  &quot;type_vocab_size&quot;: 1,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 50265
}

</code></pre>
<p><strong>vocab.json, merges.tx</strong>t produced by the following codes:</p>
<pre class=""lang-py prettyprint-override""><code>from tokenizers.implementations import ByteLevelBPETokenizer

tokenizer = ByteLevelBPETokenizer()

tokenizer.train(files=OUTPUT_DIR + &quot;seed.txt&quot;, vocab_size=52_000, min_frequency=2, special_tokens=[
    &quot;&lt;s&gt;&quot;,
    &quot;&lt;pad&gt;&quot;,
    &quot;&lt;/s&gt;&quot;,
    &quot;&lt;unk&gt;&quot;,
    &quot;&lt;mask&gt;&quot;,
])

# Save files to disk
tokenizer.save_model(MODEL_CONFIG_PATH)
</code></pre>
<p>And here's the content of <strong>vocab.json</strong> (A proportion of)</p>
<pre><code>{&quot;&lt;s&gt;&quot;:0,&quot;&lt;pad&gt;&quot;:1,&quot;&lt;/s&gt;&quot;:2,&quot;&lt;unk&gt;&quot;:3,&quot;&lt;mask&gt;&quot;:4,&quot;!&quot;:5,&quot;\&quot;&quot;:6,&quot;#&quot;:7,&quot;$&quot;:8,&quot;%&quot;:9,&quot;&amp;&quot;:10,&quot;'&quot;:11,&quot;(&quot;:12,&quot;)&quot;:13,&quot;*&quot;:14,&quot;+&quot;:15,&quot;,&quot;:16,&quot;-&quot;:17,&quot;.&quot;:18,&quot;/&quot;:19,&quot;0&quot;:20,&quot;1&quot;:21,&quot;2&quot;:22,&quot;3&quot;:23,&quot;4&quot;:24,&quot;5&quot;:25,&quot;6&quot;:26,&quot;7&quot;:27,&quot;8&quot;:28,&quot;9&quot;:29,&quot;:&quot;:30,&quot;;&quot;:31,&quot;&lt;&quot;:32,&quot;=&quot;:33,&quot;&gt;&quot;:34,&quot;?&quot;:35,&quot;@&quot;:36,&quot;A&quot;:37,&quot;B&quot;:38,&quot;C&quot;:39,&quot;D&quot;:40,&quot;E&quot;:41,&quot;F&quot;:42,&quot;G&quot;:43,&quot;H&quot;:44,&quot;I&quot;:45,&quot;J&quot;:46,&quot;K&quot;:47,&quot;L&quot;:48,&quot;M&quot;:49,&quot;N&quot;:50,&quot;O&quot;:51,&quot;P&quot;:52,&quot;Q&quot;:53,&quot;R&quot;:54,&quot;S&quot;:55,&quot;T&quot;:56,&quot;U&quot;:57,&quot;V&quot;:58,&quot;W&quot;:59,&quot;X&quot;:60,&quot;Y&quot;:61,&quot;Z&quot;:62,&quot;[&quot;:63,&quot;\\&quot;:64,&quot;]&quot;:65,&quot;^&quot;:66,&quot;_&quot;:67,&quot;`&quot;:68,&quot;a&quot;:69,&quot;b&quot;:70,&quot;c&quot;:71,&quot;d&quot;:72,&quot;e&quot;:73,&quot;f&quot;:74,&quot;g&quot;:75,&quot;h&quot;:76,&quot;i&quot;:77,&quot;j&quot;:78,&quot;k&quot;:79,&quot;l&quot;:80,&quot;m&quot;:81,&quot;n&quot;:82,&quot;o&quot;:83,&quot;p&quot;:84,&quot;q&quot;:85,&quot;r&quot;:86,&quot;s&quot;:87,&quot;t&quot;:88,&quot;u&quot;:89,&quot;v&quot;:90,&quot;w&quot;:91,&quot;x&quot;:92,&quot;y&quot;:93,&quot;z&quot;:94,&quot;{&quot;:95,&quot;|&quot;:96,&quot;}&quot;:97,&quot;~&quot;:98,&quot;¡&quot;:99,&quot;¢&quot;:100,&quot;£&quot;:101,&quot;¤&quot;:102,&quot;¥&quot;:103,&quot;¦&quot;:104,&quot;§&quot;:105,&quot;¨&quot;:106,&quot;©&quot;:107,&quot;ª&quot;:108,&quot;«&quot;:109,&quot;¬&quot;:110,&quot;®&quot;:111,&quot;¯&quot;:112,&quot;°&quot;:113,&quot;±&quot;:114,&quot;²&quot;:115,&quot;³&quot;:116,&quot;´&quot;:117,&quot;µ&quot;:118,&quot;¶&quot;:119,&quot;·&quot;:120,&quot;¸&quot;:121,&quot;¹&quot;:122,&quot;º&quot;:123,&quot;»&quot;:124,&quot;¼&quot;:125,&quot;½&quot;:126,&quot;¾&quot;:12
</code></pre>
<p>And here's the content of <strong>merges.txt</strong> (A proportion of)</p>
<pre><code>#version: 0.2 - Trained by `huggingface/tokenizers`
e n
T o
k en
Ġ To
ĠTo ken
E R
V ER
VER B
a t
P R
PR O
P N
PRO PN
Ġ n
U N
N O
NO UN
E n
i t
t it
En tit
Entit y
b j
c o
Ġ a

</code></pre>
",Training and Model Evaluation,question training language model scratch huggingface following guide train roberta like model scratch tokenizer dataset however run run mlm py train model masking task following message appear wondering doe mean training scratch pretrained weight roberta training pretrained weight way use randomly initiated weight rather pretrained one updated training model masked language modeling task following command dir consists three file config json produced following code content vocab json merges txt produced following code content vocab json proportion content merges txt proportion
"NLP , Specific Text Extraction","<p>I have to train a model which can identify the country from a random text. I have the country list.</p>
<p>Now I am struggling to find a solution that can train the model on the country list and when I provide a random text to that model as an input, it identifies the country name as an output.</p>
<p>eg:-</p>
<p>&quot;I live in India&quot; will give &quot;India&quot;
&quot;London is the capital of United Kingdom&quot; will give &quot;United Kingdom&quot;</p>
<p>Thanks in advance.</p>
",Training and Model Evaluation,nlp specific text extraction train model identify country random text country list struggling find solution train model country list provide random text model input identifies country name output eg live india give india london capital united kingdom give united kingdom thanks advance
"Binary text classification: why can LSTM fail to fit, while Dense net fits normally?","<p>I have a dataset of texts with two columns that contain textual data and binary target variable. The classes are balanced. Using Keras, first I constructed a two-input Dense net:</p>
<pre><code># Constructing a MLP with two inputs (one for each sentence in pair) and shared output part
# First head
input1 = layers.Input(shape=(sent_vector_len,))
emb1 = layers.Embedding(input_dim=vocab_sizes[0], 
                           output_dim=int(sent_vector_len / 4), 
                           input_length=sent_vector_len)(input1)
flat1 = layers.Flatten()(emb1)
dense11 = layers.Dense(128)(flat1)
dropout11 = layers.Dropout(0.5)(dense11)
dense12 = layers.Dense(128)(dropout11)
dropout12 = layers.Dropout(0.5)(dense12)
dense13 = layers.Dense(128)(dropout12)

# Second head
input2 = layers.Input(shape=(sent_vector_len,))
emb2 = layers.Embedding(input_dim=vocab_sizes[1], 
                           output_dim=int(sent_vector_len / 4), 
                           input_length=sent_vector_len)(input2)
flat2 = layers.Flatten()(emb2)
dense21 = layers.Dense(128)(flat2)
dropout21 = layers.Dropout(0.5)(dense21)
dense22 = layers.Dense(128)(dropout21)
dropout22 = layers.Dropout(0.5)(dense22)
dense23 = layers.Dense(128)(dropout22)

# Shared output layers
merged = layers.Concatenate(axis=1)([dense13, dense23])
output = layers.Dense(1, activation='sigmoid')(merged)
model_mlp = models.Model(inputs=[input1, input2], outputs=output)
</code></pre>
<pre><code>model_mlp.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=[Precision(), Recall(), f1, AUC()])

model_mlp.fit([train_inputs[0], train_inputs[1]], Y_train,
                epochs=100, validation_data=([val_inputs[0], val_inputs[1]], Y_val),
                callbacks=[callbacks.EarlyStopping(monitor='val_f1',
                                           patience=5,
                                           verbose=1,
                                           restore_best_weights=True,
                                           mode='max')])
</code></pre>
<p>It results with around 0.7 of both F1 and AUC on test set, which I find to be OK as the layers are simple and I didn't use any pretrained word embeddings.</p>
<p>As the next step, I tried to create an LSTM net:</p>
<pre><code># Constructing a LSTM with similar structure
# First head
input1 = layers.Input(shape=(sent_vector_len,))
emb1 = layers.Embedding(input_dim=vocab_sizes[0], 
                           output_dim=int(sent_vector_len / 4), 
                           input_length=sent_vector_len)(input1)
lstm1 = layers.LSTM(128, activation='relu')(emb1)

# Second head
input2 = layers.Input(shape=(sent_vector_len,))
emb2 = layers.Embedding(input_dim=vocab_sizes[1], 
                           output_dim=int(sent_vector_len / 4), 
                           input_length=sent_vector_len)(input2)
lstm2 = layers.LSTM(128, activation='relu')(emb2)

# Shared output layers
merged = layers.Concatenate(axis=1)([lstm1, lstm2])
output = layers.Dense(1, activation='sigmoid')(merged)
model_lstm = models.Model(inputs=[input1, input2], outputs=output)
</code></pre>
<pre><code>model_lstm.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=[Precision(), Recall(), f1, AUC()])

model_lstm.fit([train_inputs[0], train_inputs[1]], Y_train,
                epochs=100, validation_data=([val_inputs[0], val_inputs[1]], Y_val),
                callbacks=[callbacks.EarlyStopping(monitor='val_f1',
                                           patience=5,
                                           verbose=1,
                                           restore_best_weights=True,
                                           mode='max')])
</code></pre>
<p>Output:</p>
<pre><code>Epoch 1/100

2021-10-20 16:48:59.225294: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)

3633/3633 [==============================] - 1605s 441ms/step - loss: 0.6932 - precision: 0.5009 - recall: 0.5091 - f1: 0.3371 - auc: 0.5006 - val_loss: 0.6932 - val_precision: 0.5005 - val_recall: 1.0000 - val_f1: 0.6622 - val_auc: 0.5000
Epoch 2/100
3633/3633 [==============================] - 1613s 444ms/step - loss: 0.6932 - precision: 0.4999 - recall: 0.5326 - f1: 0.3529 - auc: 0.4995 - val_loss: 0.6932 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1: 0.0000e+00 - val_auc: 0.5000
Epoch 3/100
3633/3633 [==============================] - 1613s 444ms/step - loss: 0.6932 - precision: 0.5014 - recall: 0.5835 - f1: 0.3863 - auc: 0.4994 - val_loss: 0.6932 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1: 0.0000e+00 - val_auc: 0.5000
Epoch 4/100
  91/3633 [..............................] - ETA: 25:44 - loss: 0.6933 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00 - auc: 0.4894
etc...
</code></pre>
<p>The loss freezes at 0.6932 instantly, and AUC doesn't grow over 0.5. Adding more layers and Dropouts didn't help. I find this strange as recurrent nets should outperform Dense ones in NLP.</p>
<p>Both nets were run in Kaggle GPU environment.</p>
<p>Thanks for all possible recommendations!</p>
<h2>Update 21.10.2021</h2>
<p><em>NB:</em> Both arrays of my training data (after using Tokenizer(num_words=20000) and padding the sequences) have shape of (116240, 124).</p>
<p>I created a 1D convolutional network which improved test F1/AUC to 0.74/0.77 respectively. The structure is as follows:</p>
<pre><code># Constructing a CNN with similar structure
# First head
input1 = layers.Input(shape=(sent_vector_len,))
emb1 = layers.Embedding(input_dim=vocab_sizes[0], 
                           output_dim=int(sent_vector_len / 4), 
                           input_length=sent_vector_len)(input1)
conv1 = layers.Conv1D(256, 5, activation='relu')(emb1)
pool1 = layers.MaxPooling1D(pool_size=2)(conv1)
drop1 = layers.Dropout(0.5)(pool1)
flat1 = layers.Flatten()(drop1)
dense1 = layers.Dense(128, activation='relu')(flat1)

# Second head
input2 = layers.Input(shape=(sent_vector_len,))
emb2 = layers.Embedding(input_dim=vocab_sizes[1], 
                           output_dim=int(sent_vector_len / 4), 
                           input_length=sent_vector_len)(input2)
conv2 = layers.Conv1D(256, 5, activation='relu')(emb2)
pool2 = layers.MaxPooling1D(pool_size=2)(conv2)
drop2 = layers.Dropout(0.5)(pool2)
flat2 = layers.Flatten()(drop2)
dense2 = layers.Dense(128, activation='relu')(flat2)

# Shared output layers
merged = layers.Concatenate(axis=1)([dense1, dense2])
dense_out = layers.Dense(128, activation='relu')(merged)
output = layers.Dense(1, activation='sigmoid')(dense_out)
model_conv = models.Model(inputs=[input1, input2], outputs=output)
</code></pre>
<pre><code>model_conv.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=[Precision(), Recall(), f1, AUC()])

model_conv.fit([train_inputs[0], train_inputs[1]], Y_train,
                epochs=100, validation_data=([val_inputs[0], val_inputs[1]], Y_val),
                callbacks=[callbacks.EarlyStopping(monitor='val_f1',
                                           patience=5,
                                           verbose=1,
                                           restore_best_weights=True,
                                           mode='max')])
</code></pre>
<p>Also, I built another LSTM model according to @MarcFelix 's tips and tried to tweak the learning rate:</p>
<pre><code># First head
input1 = layers.Input(shape=(sent_vector_len,))
emb1 = layers.Embedding(input_dim=vocab_sizes[0], 
                           output_dim=int(sent_vector_len / 4), 
                           input_length=sent_vector_len)(input1)
lstm1 = layers.LSTM(256, activation='sigmoid')(emb1)

# Second head
input2 = layers.Input(shape=(sent_vector_len,))
emb2 = layers.Embedding(input_dim=vocab_sizes[0], 
                           output_dim=int(sent_vector_len / 4), 
                           input_length=sent_vector_len)(input2)
lstm2 = layers.LSTM(256, activation='sigmoid')(emb2)

# Shared output layers
merged = layers.Concatenate(axis=1)([lstm1, lstm2])
dense_out = layers.Dense(128, activation='relu')(merged)
output = layers.Dense(1, activation='sigmoid')(dense_out)
model_lstm = models.Model(inputs=[input1, input2], outputs=output)
</code></pre>
<pre><code>model_lstm.compile(optimizer=optimizers.Adam(lr=1e-4),
              loss='binary_crossentropy',
              metrics=[Precision(), Recall(), f1, AUC()])

model_lstm.summary()
</code></pre>
<pre><code>Model: &quot;model_2&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 124)]        0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            [(None, 124)]        0                                            
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 124, 31)      3323696     input_5[0][0]                    
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 124, 31)      3323696     input_6[0][0]                    
__________________________________________________________________________________________________
lstm_4 (LSTM)                   (None, 256)          294912      embedding_4[0][0]                
__________________________________________________________________________________________________
lstm_5 (LSTM)                   (None, 256)          294912      embedding_5[0][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 512)          0           lstm_4[0][0]                     
                                                                 lstm_5[0][0]                     
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 128)          65664       concatenate_2[0][0]              
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 1)            129         dense_4[0][0]                    
==================================================================================================
Total params: 7,303,009
Trainable params: 7,303,009
Non-trainable params: 0
</code></pre>
<pre><code>model_lstm.fit([train_inputs[0], train_inputs[1]], Y_train,
                epochs=100, validation_data=([val_inputs[0], val_inputs[1]], Y_val),
                callbacks=[callbacks.EarlyStopping(monitor='val_f1',
                                           patience=5,
                                           verbose=1,
                                           restore_best_weights=True,
                                           mode='max')])
</code></pre>
<p>The result is quite similar to the first one, LSTM still fails to fit:</p>
<pre><code>Epoch 1/100
3633/3633 [==============================] - 1624s 446ms/step - loss: 0.6957 - precision: 0.5000 - recall: 0.5164 - f1: 0.3421 - auc: 0.4992 - val_loss: 0.6935 - val_precision: 0.5005 - val_recall: 1.0000 - val_f1: 0.6622 - val_auc: 0.5000
Epoch 2/100
3633/3633 [==============================] - 1620s 446ms/step - loss: 0.6949 - precision: 0.4982 - recall: 0.4964 - f1: 0.3294 - auc: 0.4990 - val_loss: 0.6931 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1: 0.0000e+00 - val_auc: 0.5000
Epoch 3/100
3633/3633 [==============================] - 1641s 452ms/step - loss: 0.6943 - precision: 0.4995 - recall: 0.4903 - f1: 0.3250 - auc: 0.4990 - val_loss: 0.6945 - val_precision: 0.5005 - val_recall: 1.0000 - val_f1: 0.6622 - val_auc: 0.5000
Epoch 4/100
3633/3633 [==============================] - 1646s 453ms/step - loss: 0.6943 - precision: 0.5009 - recall: 0.4871 - f1: 0.3226 - auc: 0.5003 - val_loss: 0.6938 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1: 0.0000e+00 - val_auc: 0.5000
Epoch 5/100
...
</code></pre>
",Training and Model Evaluation,binary text classification lstm fail fit dense net fit normally dataset text two column contain textual data binary target variable class balanced using kera first constructed two input dense net result around f auc test set find ok layer simple use pretrained word embeddings next step tried create lstm net output loss freeze instantly auc grow adding layer dropout help find strange recurrent net outperform dense one nlp net run kaggle gpu environment thanks possible recommendation update nb array training data using tokenizer num word padding sequence shape created convolutional network improved test f auc respectively structure follows also built another lstm model according marcfelix tip tried tweak learning rate result quite similar first one lstm still fails fit
Train Hugging face AutoModel defined using AutoConfig,"<p>I have defined the configration for a model in <code>transformers</code>. Later, I have used this configration to initialise the classifier as follows</p>
<pre><code>from transformers import AutoConfig, AutoModel

config = AutoConfig.from_pretrained('bert-base-uncased')
classifier = AutoModel.from_config(config)
</code></pre>
<p>I have check the list of functions available for this class which are</p>
<pre><code>&gt;&gt;&gt; dir(classifier)

&gt;&gt;&gt;
['add_memory_hooks',
 'add_module',
 'adjust_logits_during_generation',
 'apply',
 'base_model',
 'base_model_prefix',
 'beam_sample',
 'beam_search',
 'bfloat16',
 'buffers',
 'children',
 'config',
 'config_class',
 'cpu',
 'cuda',
 'device',
 'double',
 'dtype',
 'dummy_inputs',
 'dump_patches',
 'embeddings',
 'encoder',
 'estimate_tokens',
 'eval',
 'extra_repr',
 'float',
 'floating_point_ops',
 'forward',
 'from_pretrained',
 'generate',
 'get_buffer',
 'get_extended_attention_mask',
 'get_head_mask',
 'get_input_embeddings',
 'get_output_embeddings',
 'get_parameter',
 'get_position_embeddings',
 'get_submodule',
 'gradient_checkpointing_disable',
 'gradient_checkpointing_enable',
 'greedy_search',
 'group_beam_search',
 'half',
 'init_weights',
 'invert_attention_mask',
 'is_parallelizable',
 'load_state_dict',
 'load_tf_weights',
 'modules',
 'name_or_path',
 'named_buffers',
 'named_children',
 'named_modules',
 'named_parameters',
 'num_parameters',
 'parameters',
 'pooler',
 'prepare_inputs_for_generation',
 'prune_heads',
 'push_to_hub',
 'register_backward_hook',
 'register_buffer',
 'register_forward_hook',
 'register_forward_pre_hook',
 'register_full_backward_hook',
 'register_parameter',
 'requires_grad_',
 'reset_memory_hooks_state',
 'resize_position_embeddings',
 'resize_token_embeddings',
 'retrieve_modules_from_names',
 'sample',
 'save_pretrained',
 'set_input_embeddings',
 'share_memory',
 'state_dict',
 'supports_gradient_checkpointing',
 'tie_weights',
 'to',
 'to_empty',
 'train',
 'training',
 'type',
 'xpu',
 'zero_grad']
</code></pre>
<p>Out of this only <code>train</code> method seemed relevant. However, upon checking the doc string for the function, I got</p>
<pre><code>&gt;&gt;&gt; print(classifier.train.__doc__)
&gt;&gt;&gt; Sets the module in training mode.

        This has any effect only on certain modules. See documentations of
        particular modules for details of their behaviors in training/evaluation
        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,
        etc.

        Args:
            mode (bool): whether to set training mode (``True``) or evaluation
                         mode (``False``). Default: ``True``.

        Returns:
            Module: self
</code></pre>
<p>How do I train this classifier on custom dataset (preferably in the <code>transformers</code> or in <code>tensorflow</code>)?</p>
",Training and Model Evaluation,train hugging face automodel defined using autoconfig defined configration model later used configration initialise classifier follows check list function available class method seemed relevant however upon checking doc string function got train classifier custom dataset preferably
INT8 quantization for FP32 matrix multiplication,"<p>I tried to apply INT8bit quantization before FloatingPoint32bit Matrix Multiplication, then requantize accumulated INT32bit output to INT8bit. After all, I guess there's a couple of mix-ups somewhere in the process. I feel stuck in spotting those trouble spots.</p>
<p>data flow [Affine Quantization]:</p>
<p>input(fp32) -&gt; quant(int8) ____\ matmul(int32) -&gt; requant(int8) -&gt;deq(fp32)<br />
input(fp32) -&gt; quant(int8) ----/</p>
<pre><code>My Pseudo Code
INPUT(FP32) :
 Embedded Words in Tensor (shape : [1, 4, 1024, 256]) A and B (B is the same as A)
</code></pre>
<p>input A(=B) : <a href=""https://i.sstatic.net/uQrbW.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<pre><code>EXPECTING OUTPUT(FP32) : 
 Embedded Words in Tensor (shape : [1, 4, 1024, 1024]) AB(after matrix multiplication to itself)

do while(true):
    # convert A and B of FP32 into INT8
    A_zero_offset = torch.empty(A.shape)
    A_zero_offset = torch.zeros_like(A_zero_offset)    # offset to be zero **[Question1]**
    scale = 255 / (torch.max(A) - torch.min(B))    # 2^8 - 1 = 255
    A_quantized = np.round((A - A_zero_offset) * scale)

    # likewise
    B_quantized = A_quantized

    AB = A_quantized.matmul(B_quantized.transpose(-1, -2))
    # now accumulated datatype is INT32

    AB_offset = torch.empty(AB.shape)
    AB_offset = AB_offset.new_full(AB.shape, torch.min(AB)) # offset to be AB's min element **[Question 1]**
    scale_AB = 255 / (torch.max(AB) - torch.min(AB))    **[Question 2]** 
    AB_requantized = np.round((AB - AB_offset) * scale_AB)

    # dequantize AB(INT8 at the status quo) into FP32
    **[Question 3]**
</code></pre>
<p>[Question 1] : does it make sense to set A's offset to be zero and AB's to be min(AB)?</p>
<p>[Question 2] : What operation should I follow with the scale calculation, &quot;max(AB) - min(AB)&quot; or any otherwise method?</p>
<p>[Question 3] : After all,  what operation do I have to follow especially with the scale and offset calculation when to dequantize the result into FP32?</p>
",Training and Model Evaluation,int quantization fp matrix multiplication tried apply int bit quantization floatingpoint bit matrix multiplication requantize accumulated int bit output int bit guess couple mix ups somewhere process feel stuck spotting trouble spot data flow affine quantization input fp quant int matmul int requant int deq fp input fp quant int input b enter image description question doe make sense set offset zero ab min ab question operation follow scale calculation max ab min ab otherwise method question operation follow especially scale offset calculation dequantize result fp
When doing text classification do the train and test text need to have the same shape after being tokenized,"<p>I am revisiting a project I did with the reuters dataset and while my model has some slight overfitting the training accuracy being 99 and validation being around 96. When I evaluate the model on the test data my accuracy is around 27%. So I was wondering if this is because the training and test data have a different shape.</p>
<pre><code>print(one_hot_train_results.shape)
print(one_hot_test_results.shape)
</code></pre>
<p>returned</p>
<p>(5485, 10000)
(2189, 10000)</p>
",Training and Model Evaluation,text classification train test text need shape tokenized revisiting project reuters dataset model ha slight overfitting training accuracy validation around evaluate model test data accuracy around wa wondering training test data different shape returned
Sentence VAE Loss Layer Implementation On Keras Giving Issues,"<p>So I've been implementing the sentence VAE on TF-Keras (latest versions). The custom function below calculates the VAE loss from sparse categorical outputs.</p>
<pre><code>def vae_loss(encoder_inputs, decoder_outputs):
     sen_loss = K.sparse_categorical_crossentropy(encoder_inputs, decoder_outputs, from_logits=True)
     sen_loss = K.sum(sen_loss, axis=-1)
     kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma))
     loss = K.mean(sen_loss + kl_loss)
     return loss

optimizer = keras.optimizers.Adam(learning_rate=0.01)
model.compile(optimizer=optimizer, loss=vae_loss)
model.fit([seq_in,seq_out],
          seq_lab,
          batch_size=batch_size, 
          epochs=epochs,
          validation_split=0.1)

#Seq_in shape = (no_of_samples, maxlen)
#Seq_out shape = (no_of_samples, maxlen)
#Seq_lab shape = (no_of_samples, maxlen, 1)
</code></pre>
<p>while attempting to train, it gave the error:</p>
<pre><code>TypeError: Cannot convert a symbolic Keras input/output to a numpy array. 
This error may indicate that you're trying to pass a symbolic value to a NumPy call, 
which is not supported. 
Or, you may be trying to pass Keras symbolic inputs/outputs to a TF API that does not 
register dispatching, 
preventing Keras from automatically converting the API call to a lambda layer in the 
Functional Model.
</code></pre>
<p>I then disabled eager tensors to allow the use of computational graphs. However, it gave the error:</p>
<pre><code>FailedPreconditionError: 2 root error(s) found.
(0) Failed precondition: Could not find variable 
training_6/Adam/embedding_16/embeddings/v. 
This could mean that the variable has been deleted. In TF1, it can also mean the 
variable is uninitialized. Debug info: container=localhost, status=Not found: 
Resource localhost/training_6/Adam/embedding_16/embeddings/v/N10tensorflow3VarE does 
not exist.
[[{{node training_6/Adam/Adam/update_embedding_16/embeddings/ReadVariableOp_3}}]]
[[_arg_keras_learning_phase_0_3/_722]]

(1) Failed precondition: Could not find variable 
training_6/Adam/embedding_16/embeddings/v. 
This could mean that the variable has been deleted. In TF1, it can also mean the 
variable is uninitialized. 
Debug info: container=localhost, status=Not found: Resource 
localhost/training_6/Adam/embedding_16/embeddings/v/N10tensorflow3VarE does not 
exist.
[[{{node training_6/Adam/Adam/update_embedding_16/embeddings/ReadVariableOp_3}}]]
0 successful operations.
0 derived errors ignored.
</code></pre>
<p>The code to the model is this:</p>
<pre><code>#################################### ENCODER LAYER ################################
encoder_inputs = Input(shape=(maxlen, ))

# Encoder Embedding
enc_emb = Embedding(vocab_size, embedding_dim,
                trainable=True)(encoder_inputs)

# Encoder LSTM
encoder_lstm3 = LSTM(latent_dim)
encoder_outputs = encoder_lstm3(enc_emb)


#################################### VAE Z LAYER ################################
z_mean = Dense(units=latent_dim)(encoder_outputs)
z_log_sigma = Dense(units=latent_dim)(encoder_outputs)

def sampling(args):
    z_mean, z_log_sigma = args
    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1.0)
    return z_mean + z_log_sigma * epsilon

z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])

expandz_h = Dense(latent_dim)
z_exp_h = expandz_h(z)

expandz_c = Dense(latent_dim)
z_exp_c = expandz_c(z)


#################################### DECODER LAYER ################################
# Set up the decoder, using z layer outputs as the initial state
decoder_inputs = Input(shape=(maxlen, ))

# Embedding layer
dec_emb = Embedding(vocab_size, embedding_dim, trainable=True)(decoder_inputs)

# Decoder LSTM
decoder_lstm = LSTM(latent_dim, return_sequences=True,
                return_state=True, dropout=0.4,
                recurrent_dropout=0.0)
(decoder_outputs, decoder_fwd_state, decoder_back_state) = \
decoder_lstm(dec_emb, initial_state=[z_exp_h, z_exp_c])

# Dense layer
decoder_dense = TimeDistributed(Dense(vocab_size, activation='softmax'))
decoder_outputs = decoder_dense(decoder_outputs)

# Define the model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.summary()
</code></pre>
<p>I believe the problem lies in the implementation of the loss_function in tf-keras but I could be wrong and would really appreciate guidance.</p>
",Training and Model Evaluation,sentence vae loss layer implementation kera giving issue implementing sentence vae tf kera latest version custom function calculates vae loss sparse categorical output attempting train gave error disabled eager tensor allow use computational graph however gave error code model believe problem lie implementation loss function tf kera could wrong would really appreciate guidance
Identify Phrases/Words in a Sentence,"<p>I'm looking for an API or library that can identify certain phrases/words in a sentence or short phrase?</p>
<p>The application of this is to find items which are not permitted.  For instance, we will have a definitive list of phrases/words that are not permitted e.g., knife, battery, oil, paint, nail polish, glass etc.</p>
<p>We will then have a list of short phrases that need to be checked against this list.  Ideally the API should handle pluralisation, misspellings and number substitutions e.g., 0 as o</p>
<p>UPDATE 15 Oct 2021:</p>
<p>Over the past few months, I've been experimenting with Fuse (JavaScript) and FuzzySharp.  However, I’m still struggling to find a solution that can accurately identify words/phrases.</p>
<p>Using FuzzySharp, some examples of false positives are:</p>
<ul>
<li><p>“used hot water bottle” – this matched “water” with a score of 90</p>
</li>
<li><p>“waterproof trousers” – this matched “water” with a score of 90</p>
</li>
<li><p>“oil” – this matched “toiletries” with a score of 90</p>
</li>
<li><p>“toilet” – this matched “toiletries” with a score of 90</p>
</li>
</ul>
<p>I understand why these have a high score, however I’m unsure of what technology I should be using improve accuracy.  At the moment we only have 298 phrases that we want to search against</p>
",Training and Model Evaluation,identify phrase word sentence looking api library identify certain phrase word sentence short phrase application find item permitted instance definitive list phrase word permitted e g knife battery oil paint nail polish glass etc list short phrase need checked list ideally api handle pluralisation misspelling number substitution e g update oct past month experimenting fuse javascript fuzzysharp however still struggling find solution accurately identify word phrase using fuzzysharp example false positive used hot water bottle matched water score waterproof trouser matched water score oil matched toiletry score toilet matched toiletry score understand high score however unsure technology using improve accuracy moment phrase want search
BERT models: how robust are they to typos?,"<p>let me introduce the context briefly: I'm fine tuning a generic BERT model for the context of food and beverage. The final goal is a classification task.</p>
<p>To train this model, I'm using a corpus of text gathered from blog posts, articles, magazines etc... that cover the topic.</p>
<p>I am however facing a predicament that I don't know how to handle: specifically, there are sometimes words that either contain a typo, or maybe different accents, but that are semantically the same.</p>
<p>Let me give you an example to briefly illustrate what I mean:</p>
<p>The wine <code>Gewürztraminer</code> is correctly written with the <code>ü</code>, however sometimes you also find it written with just a normal <code>u</code>, or some other times even just <code>Gewurtz</code>. There are several situations like this one.</p>
<p>Now, a human being would obviously know that we're talking exactly about the same thing, but I have absolutely no idea about how BERT would handle these situations. Would it understand that they're the same thing? Would it consider them instead to be completely different words?</p>
<p>I am currently in the process of cleaning my training data, fixing the typos and trying to even out all these inconsistencies, but at this point I'm not even sure if I should do that at all, considering that the text that will need to be classified can potentially contain typos and situations like the one described above.</p>
<p>What would you guys suggest?</p>
",Training and Model Evaluation,bert model robust typo let introduce context briefly fine tuning generic bert model context food beverage final goal classification task train model using corpus text gathered blog post article magazine etc cover topic however facing predicament know handle specifically sometimes word either contain typo maybe different accent semantically let give example briefly illustrate mean wine correctly written however sometimes also find written normal time even several situation like one human would obviously know talking exactly thing absolutely idea bert would handle situation would understand thing would consider instead completely different word currently process cleaning training data fixing typo trying even inconsistency point even sure considering text need classified potentially contain typo situation like one described would guy suggest
Gensim Word2Vec Training Data,"<p>I am currently trying to train my own word2vec model with my own training data and I am utterly confused about the training data preprocessing.</p>
<p>I ran a short script over my text which lemmatizes and also lower-cases the words in the text such that in the end my training data from a sentence (in German) like:</p>
<p><code>&quot;Er hat heute zwei Birnen gegessen.&quot;</code></p>
<p>the following comes out:</p>
<p><code>[er, haben, heute, zwei, birne, essen] </code></p>
<p>translated in English:</p>
<p><code>He ate two pears today. </code></p>
<p>results in:</p>
<p><code>[he, eat, two, pear, today] </code></p>
<p>Now the problem is: I haven't seen anyone do this to their training data. The words are kept in uppercase and also not lemmatized and I absolutely don't get how this works. Especially for German there are so many inflections of verbs. Should I just leave them that way? I don't understand how it works not doing the lemmatization since gensim doesn't even know which language it is trained on right?</p>
<p>So in short: Should I do lemmatization and/or lowercasing or just leave every word as it is?</p>
<p>Thanks a lot!</p>
",Training and Model Evaluation,gensim word vec training data currently trying train word vec model training data utterly confused training data preprocessing ran short script text lemmatizes also lower case word text end training data sentence german like following come translated english result problem seen anyone training data word kept uppercase also lemmatized absolutely get work especially german many inflection verb leave way understand work lemmatization since gensim even know language trained right short lemmatization lowercasing leave every word thanks lot
updating element in a list where list is the value of a dictionary,"<p>I am trying to make a viterbi table with possibility of a specific word in a part of speech occurring inserted into a table where the rows represent all possible part of speeches and the rows are sequence of words in a sentence. I want to update the  cell with corresponding the likelihood (TP*EP) that a word of a specific part of speech can occur at in a sentence. But I'm stuck on inserting these likelihoods into the dictionary list.</p>
<p>I'm using the following for insertion, but it does not work. Is there an alternative to changing a specific element in a list where the lists are values of a dictionary?</p>
<pre><code>likelihood = EP * TP
q[possible_pos][j] = likelihood
</code></pre>
<p>When I use this, I still get a table of zeroes when I am expecting some values in the table to be updated to floats(likelihood)</p>
<pre><code>for i in range(2):  
    for j in range(len(word_list[i])):
        for pos in pos_list:
            q[pos] = [0 for x in range(len(word_list[i]))]
        try:
            word = word_list[i][j].split(&quot;\t&quot;)[0]
            pos = word_list[i][j].split(&quot;\t&quot;)[1]
            for possible_pos in word_dict[word].keys():
                try:
                    print(possible_pos)
                    if j == 0:
                        TP = Transition['Begin_Sent'][possible_pos]
                        EP = pos_dict[possible_pos][word]
                    elif word_list[i][j + 1] == &quot;.\t.&quot;:
                        TP = Transition['End_Sent'][possible_pos]
                        EP = 1
                    else:
                        prev_pos = word_list[i][j - 1].split(&quot;\t&quot;)[1]
                        TP = Transition[prev_pos][possible_pos]
                        EP = pos_dict[possible_pos][word]
                    likelihood = EP * TP
                    q[possible_pos][j] = likelihood
                except KeyError:
                    pass
        except IndexError:
            pass
    df = pd.DataFrame.from_dict(q, orient='index')
    print(df)
</code></pre>
<pre><code>       0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19  20  21  22  23  24
DT     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
NN     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
POS    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
MD     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
VB     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
VBN    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
IN     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
JJ     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
NNS    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
CC     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
RBS    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
NNP    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
VBZ    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
TO     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
CD     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
VBG    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
RB     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
VBD    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
PRP    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
PRP$   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
VBP    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
WRB    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
EX     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
RBR    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
WP     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
JJR    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
WDT    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
RP     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
PDT    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
UH     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
NNPS   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
JJS    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
SYM    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
FW     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
WP$    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
LS     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</code></pre>
",Training and Model Evaluation,updating element list list value dictionary trying make viterbi table possibility specific word part speech occurring inserted table row represent possible part speech row sequence word sentence want update cell corresponding likelihood tp ep word specific part speech occur sentence stuck inserting likelihood dictionary list using following insertion doe work alternative changing specific element list list value dictionary use still get table zero expecting value table updated float likelihood
Tensorflow Transformer: Hyperparameters from the base paper give gibberish results,"<p>I used the <a href=""https://www.tensorflow.org/beta/tutorials/text/transformer"" rel=""nofollow noreferrer"">Tensorflow 2.0 Transformer Code</a> and trained the model on a new data set. I used the values given in the docs for training: They had mentioned that these values are used to keep the model training faster for demo purposes.  I got decent results for 70k sentences (Span-Eng), 25 epochs.</p>

<pre><code>num_layers = 4
d_model = 128
dff = 512
num_heads = 8

input_vocab_size = tokenizer_pt.vocab_size + 2
target_vocab_size = tokenizer_en.vocab_size + 2
dropout_rate = 0.1
</code></pre>

<p>As per the suggestion from the same docs (values from <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention is all you need</a> paper), I used the following values for 50 epochs and it gives gibberish results. Any idea what am I missing here?</p>

<pre><code>num_layers=6, d_model = 512, dff = 2048. 

#Results
Input: Hola! cómo estás
Predicted translation: Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! 
Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! 
</code></pre>
",Training and Model Evaluation,tensorflow transformer hyperparameters base paper give gibberish result used tensorflow transformer code trained model new data set used value given doc training mentioned value used keep model training faster demo purpose got decent result k sentence span eng epoch per suggestion doc value attention need paper used following value epoch give gibberish result idea missing
How to use SciBERT in the best manner?,"<p>I'm trying to use BERT models to do text classification. As the text is about scientific texts, I intend to use the <strong>SicBERT</strong> pre-trained model: <a href=""https://github.com/allenai/scibert"" rel=""nofollow noreferrer"">https://github.com/allenai/scibert</a></p>
<p>I have faced several limitations which I want to know if there is any solutions for them:</p>
<ol>
<li><p>When I want to do tokenization and batching, it only allows me to use <code>max_length</code> of &lt;=512. Is there any way to use more tokens. Doen't this limitation of 512 mean that I am actually not using all the text information during training? Any solution to use all the text?</p>
</li>
<li><p>I have tried to use this pretrained library with other models such as DeBERTa or RoBERTa. But it doesn't let me. I has only worked with BERT. Is there anyway I can do that?</p>
</li>
<li><p>I know this is a general question, but any suggestion that I can improve my fine tuning (from data to hyper parameter, etc)? Currently, I'm getting ~75% accuracy. Thanks</p>
</li>
</ol>
<p><strong>Codes:</strong></p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')

encoded_data_train = tokenizer.batch_encode_plus(
    df_train.text.values, 
    add_special_tokens=True, 
    return_attention_mask=True, 
    padding=True,
    max_length=256
)

input_ids_train = encoded_data_train['input_ids']
attention_masks_train = encoded_data_train['attention_mask']
labels_train = torch.tensor(df_train.label.values)

dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)

dataloader_train = DataLoader(dataset_train, 
                              sampler=RandomSampler(dataset_train), 
                              batch_size=batch_size)

model = BertForSequenceClassification.from_pretrained('allenai/scibert_scivocab_uncased',
                                                      num_labels=len(labels),
                                                      output_attentions=False,
                                                      output_hidden_states=False)

epochs = 1

optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)

scheduler = get_linear_schedule_with_warmup(optimizer,
num_training_steps=len(dataloader_train)*epochs)
</code></pre>
",Training and Model Evaluation,use scibert best manner trying use bert model text classification text scientific text intend use sicbert pre trained model faced several limitation want know solution want tokenization batching allows use way use token doen limitation mean actually using text information training solution use text tried use pretrained library model deberta roberta let ha worked bert anyway know general question suggestion improve fine tuning data hyper parameter etc currently getting accuracy thanks code
UnimplementedError: Cast string to float is not supported,"<p>I am trying to run code below.Everything is going well until I have tried to fit training data and label.</p>

<p>I keep taking below error. I could not find why. Could you please help me?</p>

<blockquote>
  <p>UnimplementedError:  Cast string to float is not supported     [[node
  metrics/accuracy/Cast (defined at :1)
  ]] [Op:__inference_distributed_function_53201]</p>
  
  <p>Function call stack: distributed_function</p>
</blockquote>

<pre><code>import numpy as np
import pandas as pd    
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, GRU, Embedding, CuDNNGRU, Activation
from tensorflow.python.keras.optimizers import Adam
from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf

datas=pd.read_csv('data.csv', sep='delimiter', engine='python')
targets=pd.read_csv('label.csv', sep='delimiter', engine='python')

data=datas['XDESCRIPTION'].values.tolist()
target=targets['YMode'].values.tolist()

cutoff=int(len(data)*0.80)
x_train,x_test=data[:cutoff],data[cutoff:]
y_train,y_test=target[:cutoff],target[cutoff:]


tokenizer=Tokenizer()
tokenizer.fit_on_texts(data)
tokenizer.fit_on_texts(target)

x_train_tokens=tokenizer.texts_to_sequences(x_train)
num_tokens=[len(tokens) for tokens in x_train_tokens +x_test_tokens]
num_tokens=np.array(num_tokens)
np.mean(num_tokens)

max_tokens=np.mean(num_tokens)+2*np.std(num_tokens)
max_tokens=int(max_tokens)
max_tokens

np.sum(num_tokens&lt;max_tokens)/len(num_tokens)

x_train_pad=pad_sequences(x_train_tokens, maxlen=max_tokens)
x_test_pad=pad_sequences(x_test_tokens, maxlen=max_tokens)

idx=tokenizer.word_index
inverse_map=dict(zip(idx.values(),idx.keys()))

def tokens_to_string(tokens):
    words=[inverse_map[token] for token in tokens if token!=0]
    text="" "".join(words)
    return text

model=Sequential()
embedding_size=41
model.add(Embedding(input_dim=num_words,output_dim=embedding_size,input_length=max_tokens))
model.add(GRU(units=16,return_sequences=True))
model.add(GRU(units=8,return_sequences=True))
model.add(GRU(units=4))
model.add(Dense(1,activation=""sigmoid""))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.fit(x=np.array(x_train_pad), y=np.array(y_train),epochs=2,batch_size=256)
</code></pre>
",Training and Model Evaluation,unimplementederror cast string float supported trying run code everything going well tried fit training data label keep taking error could find could please help unimplementederror cast string float supported node metric accuracy cast defined op inference distributed function function call stack distributed function
How to adjust the learning rate after N number of epochs?,"<p>I am using Hugginface's Trainer.
How to adjust the learning rate after N number of epochs?
For example, I have an initial learning rate set to <code>lr=2e-6</code>, and I would like to change the learning rate to <code>lr=1e-6</code> after the first epoch and stay on it the rest of the training.</p>
<p>I tried this so far:</p>
<pre><code>optimizer = AdamW(model.parameters(),
              lr = 2e-5,
              eps = 1e-8
            )

epochs = 5
batch_number = len(small_train_dataset) / 8
total_steps = batch_number * epochs


scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = 0,
                                            num_training_steps = total_steps,
                                            last_epoch=-1
                                            )
</code></pre>
<p>I know that there is <a href=""https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR</a> but here it drops learning rate every epoch but that is not what i want to do. I want it to drop after 1 epoch and then stay on it rest of the training process.</p>
",Training and Model Evaluation,adjust learning rate n number epoch using hugginface trainer adjust learning rate n number epoch example initial learning rate set would like change learning rate first epoch stay rest training tried far know drop learning rate every epoch want want drop epoch stay rest training process
How can I use Ensemble learning of two models with different features as an input?,"<p>I have a fake news detection problem and it predicts the binary labels &quot;1&quot;&amp;&quot;0&quot; by vectorizing the 'tweet' column, I use three different models for detection but I want to use the ensemble method to increase the accuracy but they use different vectorezer.</p>
<blockquote>
<p>I have 3 KNN models the first and the second one vectorizes the 'tweet' column using TF-IDF.</p>
</blockquote>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
    vector = TfidfVectorizer(max_features =5000, ngram_range=(1,3))
    X_train = vector.fit_transform(X_train['tweet']).toarray()
    X_test = vector.fit_transform(X_test['tweet']).toarray()
</code></pre>
<blockquote>
<p>for the third model I used fastText for sentence vectorization</p>
</blockquote>
<pre><code>%%time
sent_vec = []
for index, row in X_train.iterrows():
    sent_vec.append(avg_feature_vector(row['tweet']))
%%time
sent_vec1 = []
for index, row in X_test.iterrows():
    sent_vec1.append(avg_feature_vector(row['tweet']))
</code></pre>
<blockquote>
<p>after scaling and... my third model fits the input like this</p>
</blockquote>
<pre><code>scaler.fit(sent_vec)
scaled_X_train= scaler.transform(sent_vec)
scaled_X_test= scaler.transform(sent_vec1)
.
.
.
knn_model1.fit(scaled_X_train, y_train)
</code></pre>
<blockquote>
<p>now I want to combine the three models like this and I want the ensemble method to give me the majority just like<code>VotingClassifier</code>, but I have no idea how can I deal with the different inputs (<strong>TF-IDF &amp; fastText</strong>) is there another way to do that?</p>
</blockquote>
",Training and Model Evaluation,use ensemble learning two model different feature input fake news detection problem predicts binary label vectorizing tweet column use three different model detection want use ensemble method increase accuracy use different vectorezer knn model first second one vectorizes tweet column using tf idf third model used fasttext sentence vectorization scaling third model fit input like want combine three model like want ensemble method give majority like idea deal different input tf idf fasttext another way
Keras Text Preprocessing - Saving Tokenizer object to file for scoring,"<p>I've trained a sentiment classifier model using Keras library by following the below steps(broadly).</p>

<ol>
<li>Convert Text corpus into sequences using Tokenizer object/class</li>
<li>Build a model using the model.fit() method </li>
<li>Evaluate this model</li>
</ol>

<p>Now for scoring using this model, I was able to save the model to a file and load from a file. However I've not found a way to save the Tokenizer object to file. Without this I'll have to process the corpus every time I need to score even a single sentence. Is there a way around this?</p>
",Training and Model Evaluation,kera text preprocessing saving tokenizer object file scoring trained sentiment classifier model using kera library following step broadly convert text corpus sequence using tokenizer object class build model using model fit method evaluate model scoring using model wa able save model file load file however found way save tokenizer object file without process corpus every time need score even single sentence way around
Why my LSTM for Multi-Label Text Classification underperforms?,"<p>I'm using Windows 10 machine.
Libraries: Keras with Tensorflow 2.0
Embeddings:Glove(100 dimensions)</p>
<p>I am trying to implement an LSTM architecture for multi-label text classification.</p>
<p>My problem is that no matter how much fine-tuning I do, the results are really bad.</p>
<p>I am not experienced in DL practical implementations that's why I ask for your advice.</p>
<p>Below I will state basic information about my dataset and my model so far.</p>
<p><strong>I can't embed images since I am a new member so they appear as links.</strong></p>
<p><a href=""https://i.sstatic.net/042Xw.png"" rel=""nofollow noreferrer"">Dataset form+Embedings form+train-test-split form</a></p>
<p><a href=""https://i.sstatic.net/kVnTV.png"" rel=""nofollow noreferrer"">Dataset's labels distribution</a></p>
<p><a href=""https://i.sstatic.net/114rf.png"" rel=""nofollow noreferrer"">My Implementation of LSTM</a></p>
<p><a href=""https://i.sstatic.net/dgbwA.png"" rel=""nofollow noreferrer"">Model's Summary</a></p>
<p><a href=""https://i.sstatic.net/eaS2e.png"" rel=""nofollow noreferrer"">Model's Accuracy plot</a></p>
<p><a href=""https://i.sstatic.net/lf0Ql.png"" rel=""nofollow noreferrer"">Model's Loss plot</a></p>
<p>As you can see my dataset is really small (~6.000 examples) and maybe that's one reason why I cannot achieve better results. Still, I chose it because it's unbiased.</p>
<ol>
<li><p>I'd like to know if there is any fundamental mistake in my code regarding the dimensions, shape, activation functions, and loss functions for multi-label text classification?</p>
</li>
<li><p>What would you recommend to achieve better results on my model? Also any general advice regarding optimizing, methods,# of nodes, layers, dropouts, etc is very welcome.</p>
</li>
</ol>
<p>Model's best val accuracy that I achieved so far is ~0.54 and even if I tried to raise it, it seems stuck there.</p>
",Training and Model Evaluation,lstm multi label text classification underperforms using window machine library kera tensorflow embeddings glove dimension trying implement lstm architecture multi label text classification problem matter much fine tuning result really bad experienced dl practical implementation ask advice state basic information dataset model far embed image since new member appear link dataset form embedings form train test split form dataset label distribution implementation lstm model summary model accuracy plot model loss plot see dataset really small example maybe one reason achieve better result still chose unbiased like know fundamental mistake code regarding dimension shape activation function loss function multi label text classification would recommend achieve better result model also general advice regarding optimizing method node layer dropout etc welcome model best val accuracy achieved far even tried raise seems stuck
How to Prepare Training data for NLP Bag of words model?,"<p>I have a Machine Learning Problem: have a set of words: ex, Diameter, Item Number, Phone Number, etc.
When user gives an input Dia, the model should predict the nearest word, Diameter
If user givens an input Part Number, the model should predict: Item Number
How should I prepare training data for this: In this case, are the feature and label the same? Any help? (Bag of words? Hashing)</p>
",Training and Model Evaluation,prepare training data nlp bag word model machine learning problem set word ex diameter item number phone number etc user give input dia model predict nearest word diameter user given input part number model predict item number prepare training data case feature label help bag word hashing
Gensim fasttext cannot get latest training loss,"
<h4>Problem description</h4>
<p>It seems that the <code>get_latest_training_loss</code> function in <code>fasttext</code> returns only 0. Both gensim <strong>4.1.0</strong> and <strong>4.0.0</strong> do not work.</p>
<pre><code>from gensim.models.callbacks import CallbackAny2Vec
from pprint import pprint as print
from gensim.models.fasttext import FastText
from gensim.test.utils import datapath

class callback(CallbackAny2Vec):
    '''Callback to print loss after each epoch.'''

    def __init__(self):
        self.epoch = 0

    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        print('Loss after epoch {}: {}'.format(self.epoch, loss))
        self.epoch += 1

# Set file names for train and test data
corpus_file = datapath('lee_background.cor')

model = FastText(vector_size=100, callbacks=[callback()])

# build the vocabulary
model.build_vocab(corpus_file=corpus_file)

# train the model
model.train(
    corpus_file=corpus_file, epochs=model.epochs,
    total_examples=model.corpus_count, total_words=model.corpus_total_words,
    callbacks=model.callbacks, compute_loss=True,
)

print(model)
</code></pre>
<pre><code>'Loss after epoch 0: 0.0'
'Loss after epoch 1: 0.0'
'Loss after epoch 2: 0.0'
'Loss after epoch 3: 0.0'
'Loss after epoch 4: 0.0'
</code></pre>
<p><strong>If currently FastText does not support <code>get_latest_training_loss</code>, the documentation here needs to be removed:</strong></p>
<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText.get_latest_training_loss"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText.get_latest_training_loss</a></p>
<h4>Versions</h4>
<p>I have tried this in three different environments and neither of them works.</p>
<p><strong>First environment:</strong></p>
<pre><code>[GCC 9.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import platform; print(platform.platform())
Linux-3.10.0-1160.36.2.el7.x86_64-x86_64-with-glibc2.17
&gt;&gt;&gt; import sys; print(&quot;Python&quot;, sys.version)
Python 3.9.6 | packaged by conda-forge | (default, Jul 11 2021, 03:39:48)
[GCC 9.3.0]
&gt;&gt;&gt; import struct; print(&quot;Bits&quot;, 8 * struct.calcsize(&quot;P&quot;))
Bits 64
&gt;&gt;&gt; import numpy; print(&quot;NumPy&quot;, numpy.__version__)
NumPy 1.21.2
&gt;&gt;&gt; import scipy; print(&quot;SciPy&quot;, scipy.__version__)
SciPy 1.7.1
&gt;&gt;&gt; import gensim; print(&quot;gensim&quot;, gensim.__version__)
gensim 4.1.0
&gt;&gt;&gt; from gensim.models import word2vec;print(&quot;FAST_VERSION&quot;, word2vec.FAST_VERSION)
FAST_VERSION 0
</code></pre>
<p><strong>Second environment:</strong></p>
<pre><code>Python 3.9.5 (default, May 18 2021, 12:31:01)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import platform; print(platform.platform())
macOS-10.16-x86_64-i386-64bit
&gt;&gt;&gt; import sys; print(&quot;Python&quot;, sys.version)
Python 3.9.5 (default, May 18 2021, 12:31:01)
[Clang 10.0.0 ]
&gt;&gt;&gt; import struct; print(&quot;Bits&quot;, 8 * struct.calcsize(&quot;P&quot;))
Bits 64
&gt;&gt;&gt; import numpy; print(&quot;NumPy&quot;, numpy.__version__)
NumPy 1.20.3
&gt;&gt;&gt; import scipy; print(&quot;SciPy&quot;, scipy.__version__)
SciPy 1.7.1
&gt;&gt;&gt; import gensim; print(&quot;gensim&quot;, gensim.__version__)
gensim 4.1.0
&gt;&gt;&gt; from gensim.models import word2vec;print(&quot;FAST_VERSION&quot;, word2vec.FAST_VERSION)
FAST_VERSION 0
</code></pre>
<p><strong>Third environment:</strong></p>
<pre><code>Python 3.9.5 (default, May 18 2021, 12:31:01)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import platform; print(platform.platform())
macOS-10.16-x86_64-i386-64bit
&gt;&gt;&gt; import sys; print(&quot;Python&quot;, sys.version)
Python 3.9.5 (default, May 18 2021, 12:31:01)
[Clang 10.0.0 ]
&gt;&gt;&gt; import struct; print(&quot;Bits&quot;, 8 * struct.calcsize(&quot;P&quot;))
Bits 64
&gt;&gt;&gt; import numpy; print(&quot;NumPy&quot;, numpy.__version__)
NumPy 1.20.3
&gt;&gt;&gt; import scipy; print(&quot;SciPy&quot;, scipy.__version__)
SciPy 1.7.1
&gt;&gt;&gt; import gensim; print(&quot;gensim&quot;, gensim.__version__)
/Users/jinhuawang/miniconda3/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package &lt;https://pypi.org/project/python-Levenshtein/&gt; is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.
  warnings.warn(msg)
gensim 4.0.0
&gt;&gt;&gt; from gensim.models import word2vec;print(&quot;FAST_VERSION&quot;, word2vec.FAST_VERSION)
FAST_VERSION 0
</code></pre>
",Training and Model Evaluation,gensim fasttext get latest training loss problem description seems function return gensim work currently fasttext doe support documentation need removed version tried three different environment neither work first environment second environment third environment
Algorithm to extract legal citations in a document,"<p>I want to train a machine learning model to learn and extract legal citation patterns in a text document. What is the best algorithm can I use?  My training data sample set of legal citations that looks like,</p>
<pre><code>    sample set: 
    Brill v. Guardian Life Ins. Co. of America, 142 N.J. 520, 529 (1995)
    Della v. Guard Lifal Ins. Co. of SA, 142 N.J. 420, 549 (2011)
    Heljon Mgmt. Corp. v. DiLeo, 55 N.J. Super. 306, 312-13 (No Citations. This was 
    extracted from NJ Sup..)
    Ocean Cape Hotel Corp. v. Masefield Corp., 63 N.J. Super. 369, 383 (App. Div. 1960)
</code></pre>
<p>The citations training sample were extracted from documents using regex code,</p>
<pre><code>r'(?:[A-Z]\w*\.? )+v\. .*?\d{4}\)'
</code></pre>
<p>I have tried spaCy entity matching but that is not working (I bet my code is not perfect). My code</p>
<pre><code>import re
import en_core_web_sm
nlp = en_core_web_sm.load()

nlp = spacy.load('en_core_web_sm')

from spacy.matcher import Matcher
m_tool = Matcher(nlp.vocab)

doc = open(file='text1.txt', mode='r', encoding='utf-8').read()
#print(text)

doc = nlp(doc)
#print([(ent.text, ent.label_) for ent in doc.ents])


p1 = [{'IS_TITLE': 'NN'}, {'LOWER': 'v'}, {'IS_PUNCT': True}, {'IS_TITLE': 'NN'}]
p2 = [{'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'LOWER': 'v'}, {'IS_PUNCT': True}, 
{'IS_TITLE': 'NN'}]
p3 = [{'IS_TITLE': 'NN'}, {'LOWER': 'v'}, {'IS_PUNCT': True}, {'IS_TITLE': 'NN'}, 
{'IS_TITLE': 'NN'},]
p4 = [{'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'LOWER': 'v'}, {'IS_PUNCT': True}, 
{'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}]
p5 = [{'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'LOWER': 'v'}, 
{'IS_PUNCT': True}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}]
p6 = [{'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'LOWER': 'v'}, {'IS_PUNCT': True}, 
{'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}]
p7 = [{'IS_TITLE': 'NN'}, {'LOWER': 'v'}, {'IS_PUNCT': True}, {'IS_TITLE': 'NN'}, 
{'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}]
p8 = [{'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'LOWER': 'v'}, 
{'IS_PUNCT': True}, {'IS_TITLE': 'NN'}]
p9 = [{'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'LOWER': 'v'}, 
{'IS_PUNCT': True}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}]
p10 = [{'label': 'PERSON'}]
P11 = [{'label': 'ORG'}, {'label': 'PERSON'}]
p12 = [{'label': 'PERSON'}, {'label': 'ORG'}]
p13 = [{'label': 'ORG'}, {'label': 'ORG'}, {'label': 'ORG'}, {'label': 'ORG'}]

m_tool.add('QBF', None, p1, p2, p3, p4, p5, p6, p6, p7, p8, p9, p10, p11, p12, p13)

phrase_matches = m_tool(doc)
print(phrase_matches)
</code></pre>
",Training and Model Evaluation,algorithm extract legal citation document want train machine learning model learn extract legal citation pattern text document best algorithm use training data sample set legal citation look like citation training sample extracted document using regex code tried spacy entity matching working bet code perfect code
Word2Vec: Any way to train model fastly?,"<p>I use <code>Gensim</code> <code>Word2Vec</code> to train word sets in my database.</p>

<p>I have about 400,000 phrase(Each phrase is short. Total 700MB) in my <code>PostgreSQL</code> database.</p>

<p>This is how I train these data using <code>Django ORM</code>: </p>

<pre><code>post_vector_list = []
for post in Post.objects.all():
    post_vector = my_tokenizer(post.category.name)
    post_vector.extend(my_tokenizer(post.title))
    post_vector.extend(my_tokenizer(post.contents))
    post_vector_list.append(post_vector)
word2vec_model = gensim.models.Word2Vec(post_vector_list, window=10, min_count=2, size=300) 
</code></pre>

<p>But this job getting a lot of time and feels like not efficient.</p>

<p>Especially, creating <code>post_vector_list</code> part took a lot of time and space..</p>

<p>I want to improve speed of training but have no idea how to do.</p>

<p>Want to get your advices. Thanks.</p>
",Training and Model Evaluation,word vec way train model fastly use train word set database phrase phrase short total mb database train data using job getting lot time feel like efficient especially creating part took lot time space want improve speed training idea want get advice thanks
GRU model not learning,"<p>I’m trying to fit a GRU model on text data, to predict one of 26 labels. The problem is that the model is not really learning (accuracy is around 4%, which is just as random chance). Since I know that the problem is “learnable”, I suspect that there’s a bug in my code, but I can't figure out what it is.</p>
<p>My data consists of (tokenized and word-encoded) 100K sentences per label (each sentence is mapped to one of 26 labels). My task is to predict the label of a new unseen sentence.
I tried several approaches, such as using a batch size &gt; 1 together with padding, but the approach I'm sticking with right now is joining every 20 sentences to a single batch, so my samples become a little bit larger, and fit the model with 1 batch a time.</p>
<p>Model:</p>
<pre><code>class GRU(nn.Module):
    def __init__(self, input_size, num_classes, batch_size):
        super(GRU, self).__init__()
        self.hidden_state = None
        self._batch_first = True
        self.batch_size = batch_size
        self.hidden_size = 256
        self.num_layers = 1
        embedding_dim = 256
        self.embedding = nn.Embedding(input_size, embedding_dim=embedding_dim)
        nn.init.uniform_(self.embedding.weight, -1.0, 1.0)
        self.gru = nn.GRU(embedding_dim, self.hidden_size, self.num_layers, batch_first=self._batch_first)
        self.fc = nn.Linear(self.hidden_size, num_classes)
    
    def init_hidden(self):
        self.hidden_state = torch.randn(self.num_layers, self.batch_size, self.hidden_size).to(device)

    def forward(self, x):
        embeds = self.embedding(x)
        out, self.hidden_state = self.gru(embeds, self.hidden_state)
        out = out[:, -1, :]
        out = self.fc(out)
        return out

   
# Loss and optimizer
criterion = nn.CrossEntropyLoss()
learning_rate = 0.001
optimizer = lambda mdl: torch.optim.Adam(mdl.parameters(), lr=learning_rate)

model = RNN(len(vocab), len(encoded_lbls), BATCH_SIZE).to(device)

# RNN(
#   (embedding): Embedding(19353, 256)
#   (rnn): GRU(256, 256, batch_first=True)
#   (fc): Linear(in_features=256, out_features=26, bias=True)
# )
</code></pre>
<p>I tried different learning rates, and different losses such as NLLLoss with a LogSoftmax, but that made no difference.</p>
<p>Since I think that word ngrams are a good feature for this problem, I split each batch to word trigrams, and fed them to the model ngram by ngram, while resetting the hidden state before every batch:</p>
<pre><code>model.train(mode=True)
for epoch in range(epochs):
    for label,encoded_txt in train_loader:
        encoded_txt, label = encoded_txt.to(device), label.to(device)
        model.init_hidden()
        output, loss, _ = evaluate(model, optim, encoded_txt, label, train=True)

    # validation eval...
</code></pre>
<p>Here's the <code>evaluate()</code> function:</p>
<pre><code>def evaluate(model, optim, txt, label, train=False):
    for ngram in txt.split(NGRAM_LEN):  # NGRAM_LEN = 3
        output = model(ngram)
    loss = criterion(output, label)    

    if train:
        optim.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)
        for p in model.parameters():
            p.data.add_(p.grad, alpha=-learning_rate)
        optim.step()
        
    accuracy = np.mean(np.array([item.item() for item in torch.argmax(output, dim=1)]) == label.cpu().numpy())

    return output, loss.item(), accuracy
</code></pre>
<p>This is what I'm getting after 10 epochs:</p>
<pre><code>Epoch 0: Training Loss: 3.3762, Validation Loss: 3.4029, Validation Accuracy: 3.87%
Epoch 1: Training Loss: 3.3084, Validation Loss: 3.5362, Validation Accuracy: 3.89%
Epoch 2: Training Loss: 3.1202, Validation Loss: 3.8107, Validation Accuracy: 4.32%
Epoch 3: Training Loss: 2.9897, Validation Loss: 4.0599, Validation Accuracy: 4.57%
Epoch 4: Training Loss: 2.9118, Validation Loss: 4.3766, Validation Accuracy: 3.93%
Epoch 5: Training Loss: 2.9161, Validation Loss: 4.4962, Validation Accuracy: 4.23%
Epoch 6: Training Loss: 2.9117, Validation Loss: 4.7663, Validation Accuracy: 4.47%
Epoch 7: Training Loss: 2.9203, Validation Loss: 4.9078, Validation Accuracy: 4.55%
Epoch 8: Training Loss: 2.9253, Validation Loss: 5.1911, Validation Accuracy: 4.49%
Epoch 9: Training Loss: 2.9592, Validation Loss: 5.4946, Validation Accuracy: 4.23%
</code></pre>
<p>I'm hoping for at least 60% accuracy on the validation set, but as you can see it's just as random chance.
The training loss is not really decreasing, and the validation loss is increasing.
I can't say it's overfitting since the training loss is pretty high so it's not really learning.</p>
<p>Can anyone spot a bug in the code or suggest ways to debug this?</p>
",Training and Model Evaluation,gru model learning trying fit gru model text data predict one label problem model really learning accuracy around random chance since know problem learnable suspect bug code figure data consists tokenized word encoded k sentence per label sentence mapped one label task predict label new unseen sentence tried several approach using batch size together padding approach sticking right joining every sentence single batch sample become little bit larger fit model batch time model tried different learning rate different loss nllloss logsoftmax made difference since think word ngrams good feature problem split batch word trigram fed model ngram ngram resetting hidden state every batch function getting epoch hoping least accuracy validation set see random chance training loss really decreasing validation loss increasing say overfitting since training loss pretty high really learning anyone spot bug code suggest way debug
Regex in Python to detect ellipsis,"<p>I have a large text corpus that I want to process a little bit and then train a Word2Vec model based on it. There are cases that words are deleted due to ellipsis, like:</p>
<blockquote>
<p>But seeing them playing to <em>seven-</em> and <em>eight-year-olds</em> is beautiful</p>
</blockquote>
<p>or</p>
<blockquote>
<p>The country was in the uproar of <em>pre-</em> and then <em>post-independence</em> civil war but the mood here is most often joyous</p>
</blockquote>
<p>Now I want to undo these deletes (<em>inspired</em> and <em>second</em> respectively). This is what I wrote:</p>
<pre><code>re.sub(r'- (and|to|or)( [^ -]+?){1,2}-(.+?)( |$|\n)', '-\\3 \\1\\2-\\3\\4', text)
</code></pre>
<p>But it doesn't work, since if there is more than one word between <code>and/or/to</code> and the second word with <code>-</code>, only the first will be shown.
My desired outputs are:</p>
<blockquote>
<p>But seeing them playing to <em>seven-year-olds</em> and <em>eight-year-olds</em> is beautiful</p>
</blockquote>
<p>and</p>
<blockquote>
<p>The country was in the uproar of <em>pre-independence</em> and then <em>post-independence</em> civil war but the mood here is most often joyous</p>
</blockquote>
",Training and Model Evaluation,regex python detect ellipsis large text corpus want process little bit train word vec model based case word deleted due ellipsis like seeing playing seven eight year old beautiful country wa uproar pre post independence civil war mood often joyous want undo deletes inspired second respectively wrote work since one word second word first shown desired output seeing playing seven year old eight year old beautiful country wa uproar pre independence post independence civil war mood often joyous
Sklearn train_test_split split a dataset to compare predicted labels with ground truth labels,"<p>I am trying to perform a multi-class text classification using SVM with a small dataset by adapting from <a href=""https://medium.com/@ruixuanl/multi-class-text-classification-with-extremely-small-data-set-deep-learning-b38dfb386f8e"" rel=""nofollow noreferrer"">this guide</a>. The input csv contains a 'text' column and a 'label' column (which have been manually assigned for this specific task).</p>
<p>One label needs to be assigned for each text entry. By using the LinearSVC model and TfidfVectorizer I obtained an accuracy score of 75% which seems more than expected for a very small dataset of only 400 samples. In order to further raise the accuracy I wanted to have a look at the entries that were not correctly classified but here I have an issue. Since I used train_test_split like this:</p>
<pre class=""lang-py prettyprint-override""><code>Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, y, test_size=0.1, random_state = 1004)
</code></pre>
<p>I don't know which text entries have been used by the train_test_split function (as far as I understand the function chooses randomly the 10% entries for the test_size). So I don't know against which subset of the corpus original entries labels should I compare the list of predicted labels for the test dataset. In other words is there a method to enforce a subset to be assigned for the test_size i.e the last 40 entries from the 400 total entries in the dataset?</p>
<p>This would help to manually compare the predicted labels vs the ground truth labels.</p>
<p>Below is the code:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score

import pandas as pd
import numpy as np
import os


class Config:

    # Data and output directory config
    data_path = r'./take3/Data'
    code_train = r'q27.csv'



if __name__ == &quot;__main__&quot;:


    print('--------Code classification--------\n')

    Corpus = pd.read_csv(os.path.join(Config.data_path, Config.code_train), sep = ',', encoding='cp1252', usecols=['text', 'label'])

    train_text = ['' if type(t) == float else t for t in Corpus['text'].values]


    # todo fine tunining
    tfidf = TfidfVectorizer(
        sublinear_tf=True,
        min_df=3, norm='l2',
        encoding='latin-1',
        ngram_range=(1, 2),
        stop_words='english')

    X = tfidf.fit_transform(train_text)             # Learn vocabulary and idf, return document-term matrix.

    # print('Array mapping from feature integer indices to feature name',tfidf.get_feature_names())
    print('X.shape:', X.shape)

    y = np.array(list(Corpus['label']))
    print('The corpus original labels:',y)
    print('y.shape:', y.shape)

    Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, y, test_size=0.1, random_state = 1004)


    model = LinearSVC(random_state=1004)
    model.fit(Train_X, Train_Y)

    SVM_predict_test = model.predict(Test_X)
    accuracy = accuracy_score(Test_Y, SVM_predict_test, normalize=True, sample_weight=None)*100
    print('Predicted labels for the test dataset', SVM_predict_test)
    print(&quot;SVM accuracy score: {:.4f}&quot;.format(accuracy))
</code></pre>
<p>And this is the received output:</p>
<pre><code>
                         
--------Code classification--------

X.shape: (400, 136)
The corpus original labels: [15 20  9 14 98 12  3  4  4 22 99  3 98 20 99  1 10 20  8 15 98 12 18  7
 20 99  8  8 13  2  8  6 22  4 98  5 98 12 18  8 98 18 24  4  3 19 12  5
 20  6  8 15  5 14 19 22 16 10 24 16 98  8  8 16  2 20  4  8 20  6 22 98
  3 98 15 12  2 13  5  8  8  1 10 16 20 12  7 20 98 22 99 10 12  8  8 16
 16  4  4 99 20  8 16  2 12 15 16 10  5 22  8  7  7  4  5 12 16 14  1 10
 22 20  4  4  5 99 16  3  5 22 99  5  3  4  4  3  6 99  8 20  2 10 98  6
  6  8 99  3  8 99  2  5 15  6  6  7  8 14  9  4 20  3 99  5 98 15  5  5
 20 10  4 99 99 16 22  8 10 22 98 12  3  5  9 99 14  8  9 18 20 14 15 20
 20  1  6 23 22 20  6  1 18  8 12 10 15 10  6 10  3  4  8 24 14 22  5  3
 22 24 98 98 98  4 15 19  5  8  1 17 16  6 22 19  4  8  2 15 12 99 16  8
  9  1  8 22 14  5 20  2 10 10 22 12 98  3 19  5 98 14 19 22 18 16 98 16
  6  4 24 98 24 98 15  1  3 99  5 10 22  4 16 98 22  1  8  4 20  8  8  5
 20  4  3 20 22  4 20 12  7 21  5  4 16  8 22 20 99  5  6 99  8  3  4 99
  6  8 12  3 10  4  8  5 14 20  6 99  4  4  6  4 98 21  1 23 20 98 19  6
  4 22 98 98 20 10  8 10 19 16 14 98 14 12 10  4 22 14  3 98 10 20 98 10
  9  7  3  8  3  6  6 98  8 99  1 20 18  8  2  6 99 99 99 14 14 16 20 99
  1 98 23  6 12  4  1  3 99 99  3 22  5  7 16 99]
y.shape: (400,)
Predicted labels for the test dataset [ 1  8  5  4 15 10 14 12  6  8  8 16 98 20  7 99 99 12 99 24  4 98 99  3
 20  3  6 14 18 98 99 22  4 99  4 10 14  4  3 98]
SVM accuracy score: 75.0000
</code></pre>
",Training and Model Evaluation,sklearn train test split split dataset compare predicted label ground truth label trying perform multi class text classification using svm small dataset adapting guide input csv contains text column label column manually assigned specific task one label need assigned text entry using linearsvc model tfidfvectorizer obtained accuracy score seems expected small dataset sample order raise accuracy wanted look entry correctly classified issue since used train test split like know text entry used train test split function far understand function chooses randomly entry test size know subset corpus original entry label compare list predicted label test dataset word method enforce subset assigned test size e last entry total entry dataset would help manually compare predicted label v ground truth label code received output
pytorch cnn model stop at loss.backward() without any prompt?,"<p>My aim is to make a five-category text classification</p>

<p>I am running bert fine tuning with <code>cnnbase</code> model but my project stops at <code>loss.backward()</code> without any prompt in <code>cmd</code>.</p>

<p>My program runs successfully in <code>rnn base</code> such as <code>lstm</code> and <code>rcnn</code>.</p>

<p>But when I am running some <code>cnnbase</code> model a strange bug  appears.</p>

<p>My cnn model code:</p>

<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
# from ..Models.Conv import Conv1d
from transformers.modeling_bert import BertPreTrainedModel, BertModel
n_filters = 200
filter_sizes = [2,3,4]
class BertCNN(BertPreTrainedModel):
    def __init__(self, config):
        super(BertPreTrainedModel, self).__init__(config)
        self.num_filters = n_filters
        self.filter_sizes = filter_sizes
        self.bert = BertModel(config)
        for param in self.bert.parameters():
            param.requires_grad = True
        self.convs = nn.ModuleList(
            [nn.Conv2d(1, self.num_filters, (k, config.hidden_size))
                for k in self.filter_sizes])
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.fc_cnn = nn.Linear(self.num_filters *
                                len(self.filter_sizes), config.num_labels)

    def conv_and_pool(self, x, conv):
        x = F.relu(conv(x)).squeeze(3)
        x = F.max_pool1d(x, x.size(2)).squeeze(2)
        return x

    def forward(self, input_ids,
                attention_mask=None, token_type_ids=None, head_mask=None):
        outputs = self.bert(input_ids,
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids,
                            head_mask=head_mask)
        encoder_out, text_cls = outputs
        out = encoder_out.unsqueeze(1)
        out = torch.cat([self.conv_and_pool(out, conv)
                         for conv in self.convs], 1)
        out = self.dropout(out)
        out = self.fc_cnn(out)
        return out
</code></pre>

<p>My train code:</p>

<pre><code>        for step, batch in enumerate(data):
            self.model.train()
            batch = tuple(t.to(self.device) for t in batch)
            input_ids, input_mask, segment_ids, label_ids = batch
            print(""input_ids, input_mask, segment_ids, label_ids SIZE: \n"")   
            print(input_ids.size(), input_mask.size(),segment_ids.size(), label_ids.size()) 
            # torch.Size([2, 80]) torch.Size([2, 80]) torch.Size([2, 80]) torch.Size([2])
            logits = self.model(input_ids, segment_ids, input_mask)
            print(""logits and label ids size: "",logits.size(), label_ids.size())
            # torch.Size([2, 5]) torch.Size([2])
            loss = self.criterion(output=logits, target=label_ids)
            if len(self.n_gpu) &gt;= 2:
                loss = loss.mean()
            if self.gradient_accumulation_steps &gt; 1:
                loss = loss / self.gradient_accumulation_steps
            if self.fp16:
                with amp.scale_loss(loss, self.optimizer) as scaled_loss:
                    scaled_loss.backward()
                clip_grad_norm_(amp.master_params(self.optimizer), self.grad_clip)
            else:
                loss.backward() # I debug find that the program stop at this line without any error prompt
</code></pre>

<p><a href=""https://i.sstatic.net/sdD1R.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/sdD1R.png"" alt=""enter image description here""></a></p>

<p>change the batchsize to 1 
the bug still occured</p>

<p>the step1 logits ：</p>

<p>logits tensor([[ 0.8831, -0.0368, -0.2206, -2.3484, -1.3595]], device='cuda:1',
grad_fn=)</p>

<p>the step1 loss：</p>

<p>tensor(1.5489, device='cuda:1', grad_fn=NllLossBackward>)</p>

<p>but why can't loss.backward()?</p>
",Training and Model Evaluation,pytorch cnn model stop loss backward without prompt aim make five category text classification running bert fine tuning model project stop without prompt program run successfully running model strange bug appears cnn model code train code change batchsize bug still occured step logits logits tensor device cuda grad fn step loss tensor device cuda grad fn nlllossbackward loss backward
How can I pass a 50d vector to fit Naive-Bayes model,"<p>I'm performing sentiment analysis and am new to word embedding. I've converted my phrases to 50d vectors using GloVe word embedding and want to pass these as training data but keep getting</p>
<pre><code>TypeError                                 Traceback (most recent call last) TypeError: only size-1 arrays can be converted to Python scalars

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last) &lt;ipython-input-85-33202bf88193&gt; in &lt;module&gt;
      1 bnbclf = BernoulliNB()
----&gt; 2 bnbclf.fit(X = X_train, y = y_train, sample_weight = [cw[i] for i in y_train])
</code></pre>
<p>My vectors are of the form:</p>
<pre><code>X_train[0]

array([ 13.81449103,   8.60164096,  -9.76383901,  -2.36460596,
        12.91351991,  12.32445506,  -9.92880224,  -7.15983607,
        -0.78202473,   3.11839001,  -2.55677491,   2.92986998,
        -6.47585791,  -6.82789405,  11.60826143,   1.44951305,
         6.06078495,   4.16923777, -14.08995277, -10.54117191,
         0.81230098,   4.05350507,   5.27667667,  -1.81203901,
         6.59123958, -52.43976641, -19.74524475,   5.28675767,
         6.99492514,  -7.60190205, 105.55067018,   1.80315194,
        -5.42133144, -11.38166397,   2.58026635,   0.24258623,
        -1.0791869 ,   2.17247699,  -0.54076034,  -4.01877102,
        -4.01617018,   5.05499033,  -3.4997439 ,   6.451917  ,
        -5.32425116,   2.22513228,  -3.29181632,   2.90443946,
        -1.06864398,  -3.32023967])
</code></pre>
<p>The code I'm running is:</p>
<pre><code>bnbclf = BernoulliNB()
bnbclf.fit(X = X_train, y = y_train, sample_weight = [cw[i] for i in y_train])
</code></pre>
<p>Here are the class weights and y_train for the same:-</p>
<pre><code>cw = {0:9. , 1:3., 2:1., 3:3.,4:8.}
</code></pre>
<pre><code>y_train[1:5]
72290     2
151058    2
80065     2
148397    1
Name: Sentiment, dtype: int64
</code></pre>
",Training and Model Evaluation,pas vector fit naive bayes model performing sentiment analysis new word embedding converted phrase vector using glove word embedding want pas training data keep getting vector form code running class weight train
How to train machine learning model with FastText output,"<p>Is there any method of Fasttext by which I can get the following format (&lt;1x10000 sparse matrix of type '&lt;class 'numpy.float64'&gt;'with 67 stored elements in Compressed Sparse Row format&gt;) from the below output of Fasttext or any method by which I can train my ML model. Since when I used TF-IDF then I get the sparse matrix and I trained the ML model but now I want to train the model with FastText.</p>
<pre><code>fasttext_out=model_ted.wv.most_similar(&quot;The Lemon Drop Kid , a New York City swindler, is illegally touting horses at a Florida racetrack. After several successful hustles, the Kid comes across a beautiful, but gullible, woman intending to bet a lot of money. The Kid convinces her to switch her bet, employing a prefabricated con. Unfortunately for the Kid, the woman belongs to notorious gangster Moose Moran , as does the money. The Kid's choice finishes dead last and a furious Moran demands the Kid provide him with $10,000  by Christmas Eve, or the Kid won't make it to New Year's. The Kid decides to return to New York to try to come up with the money. He first tries his on-again, off-again girlfriend Brainy Baxter . However, when talk of long-term commitment arises, the Kid quickly makes an escape.&quot;)

model_ted.wv.most_similar(&quot;school&quot;)
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>[('Psycho-biddy', 0.9323669672012329),
 ('Slasher', 0.8850599527359009),
 ('Demonic child', 0.8805997967720032),
 ('Giallo', 0.8504119515419006),
 ('Road-Horror', 0.821454644203186),
 ('Anthology', 0.8191317915916443),
 ('Czechoslovak New Wave', 0.8187490105628967),
 ('Supernatural', 0.813347339630127),
 ('Psychological thriller', 0.8018383979797363),
 ('Kitchen sink realism', 0.8017964959144592)]
</code></pre>
<p>My main intention is to change the output into vectors and train the Machine Learning model. Please confirm.</p>
",Training and Model Evaluation,train machine learning model fasttext output method fasttext get following format x sparse matrix type class numpy float stored element compressed sparse row format output fasttext method train ml model since used tf idf get sparse matrix trained ml model want train model fasttext output main intention change output vector train machine learning model please confirm
Gensim FastText compute Training Loss,"<p>I am training a <code>fastText</code> model using <a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""noreferrer""><code>gensim.models.fasttext</code></a>. However, I can't seem to find a method to compute the loss of the iteration for logging purposes. If I look at <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer""><code>gensim.models.word2vec</code></a>, it has the <code>get_latest_training_loss</code> method which allows you to print the training loss. Are there any alternatives or it is simply impossible?</p>
",Training and Model Evaluation,gensim fasttext compute training loss training model using however seem find method compute loss iteration logging purpose look ha method allows print training loss alternative simply impossible
Internal error: Tried to take gradients (or similar) of a variable without handle data in Tensorflow,"<p>I am finetuning BERT for a binary sentiment analysis class using Tensorflow. I want to use a custom training loop/loss function. However, when I train the model I get the following error: <code>ValueError: Internal error: Tried to take gradients (or similar) of a variable without handle data: Tensor(&quot;transformer_encoder/StatefulPartitionedCall:1019&quot;, shape=(), dtype=resource)</code>.</p>
<p>To debug, I tried simplifying my training loop to just compute standard binary cross entropy, which should be equivalent to if I called model.fit() with binary cross entropy as the loss function (which works completely fine). However, I get the same error as above when running this simplified training loop and I am not sure what's causing it. Note: I am using tensorflow 2.3.0.</p>
<p>Here is the model:</p>
<pre><code>def create_model():
  max_seq_length = 512
  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                        name=&quot;input_word_ids&quot;)
  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                     name=&quot;input_mask&quot;)
  input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                      name=&quot;input_type_ids&quot;)
  
  bert_layer = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2&quot;, trainable=True)
  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])
  drop = tf.keras.layers.Dropout(0.3)(pooled_output)
  output = tf.keras.layers.Dense(1, activation='sigmoid', name=&quot;output&quot;)(drop)

  model = tf.keras.Model(
      inputs={
          'input_word_ids': input_word_ids,
          'input_mask': input_mask,
          'input_type_ids': input_type_ids
      },
      outputs= output 
  )

  return model
</code></pre>
<p>Here is the training loop function. The issue seems to come up when running <code>ypred = model(train_x)</code> inside tf.GradientTape():</p>
<pre><code>def train_step(train_batch):
  train_x, train_y = train_batch
  with tf.GradientTape() as tape:
    ypred = model(train_x)
    loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(train_y, ypred))
  grads = tape.gradient(loss, model.trainable_weights)
  optimizer.apply_gradients(zip(grads, model.trainable_weights))
  return loss
</code></pre>
<p>Again, this seems to only happen with tf.GradientTape(), since model.fit() does not result in any issues.</p>
<pre><code>model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),
          loss=tf.keras.losses.BinaryCrossentropy(),
          metrics=[tf.keras.metrics.BinaryAccuracy()])

model.fit(train_data,
          validation_data=valid_data,
          epochs=epochs,
          verbose=1)
</code></pre>
",Training and Model Evaluation,internal error tried take gradient similar variable without handle data tensorflow finetuning bert binary sentiment analysis class using tensorflow want use custom training loop loss function however train model get following error debug tried simplifying training loop compute standard binary cross entropy equivalent called model fit binary cross entropy loss function work completely fine however get error running simplified training loop sure causing note using tensorflow model training loop function issue seems come running inside tf gradienttape seems happen tf gradienttape since model fit doe result issue
train data in nlp for extracting skills using LSTM but train and validation accuracy are not as expected,"<p><a href=""https://i.sstatic.net/tTcfE.png"" rel=""nofollow noreferrer"">I have manually created this data as mentioned in the document where 1 is skill and 0 is not_skill</a></p>
<p>this is my LSTM network</p>
<pre><code>model=Sequential()
model.add(Embedding(vocab_size,100,input_length=max_len))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(256))
model.add(Dense(128,activation='softmax'))
model.add(Dense(64,activation='softmax'))
model.add(Dense(32,activation='softmax'))
model.add(Dense(2,activation='softmax'))
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model.summary())
</code></pre>
<p><a href=""https://i.sstatic.net/KGDXK.png"" rel=""nofollow noreferrer"">I dont understand why is there None below the model. Is it affecting models performance or not</a></p>
<pre><code>y=pd.get_dummies(d['Skill_NotSkill'])
xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=0)

model.fit(xtrain,ytrain,epochs=20,batch_size=64,verbose=1,validation_data=(xtest,ytest))
</code></pre>
<p>the training accuracy is fluctuating and validation accuracy is constant.</p>
<p><a href=""https://i.sstatic.net/LgTvT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LgTvT.png"" alt=""enter image description here"" /></a></p>
<p>Can anyone please help in training the model and point out if there are any issues in my code.
Also for extracting skills am I following right steps.</p>
",Training and Model Evaluation,train data nlp extracting skill using lstm train validation accuracy expected manually created data mentioned document skill skill lstm network dont understand none model affecting model performance training accuracy fluctuating validation accuracy constant anyone please help training model point issue code also extracting skill following right step
Doc2Vec find the similar sentence,"<p>I am trying find similar sentence using doc2vec. What I am not able to find is actual sentence that is matching from the trained sentences.</p>
<p>Below is the code from <a href=""https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5"" rel=""nofollow noreferrer"">this article</a>:</p>
<pre><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
data = [&quot;I love machine learning. Its awesome.&quot;,
        &quot;I love coding in python&quot;,
        &quot;I love building chatbots&quot;,
        &quot;they chat amagingly well&quot;]

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]
max_epochs = 100
vec_size = 20
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha, 
                min_alpha=0.00025,
                min_count=1,
                dm =1)
  
model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

model.save(&quot;d2v.model&quot;)
print(&quot;Model Saved&quot;)

model= Doc2Vec.load(&quot;d2v.model&quot;)
#to find the vector of a document which is not in training data
test_data = word_tokenize(&quot;I love building chatbots&quot;.lower())
v1 = model.infer_vector(test_data)
print(&quot;V1_infer&quot;, v1)

# to find most similar doc using tags
similar_doc = model.docvecs.most_similar('1')
print(similar_doc)


# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data
print(model.docvecs['1'])
</code></pre>
<p>But the above code only gives me vectors or numbers. But how can I get the actual sentence matched from training data. For Eg - In this case I am expecting the result as &quot;I love building chatbots&quot;.</p>
",Training and Model Evaluation,doc vec find similar sentence trying find similar sentence using doc vec able find actual sentence matching trained sentence code article code give vector number get actual sentence matched training data eg case expecting result love building chatbots
Pytorch save/load model with lower dev set accuracy?,"<p>Here is the question: I am loading my pytorch model(best result on dev set while training) from the checkpoint file during model evaluation, remembering to do <strong>model.eval()</strong> and <strong>with torch.no_grad()</strong>, I still get a lower accuracy result(with 1-2% drop) on dev set compared with which I get while training.</p>
<p>I have tried:</p>
<ul>
<li>printing the state dict before pytorch save the best result model during training, compared with what I get while loading, which is the same.</li>
<li>check my code, which use lots of dropout and layernorm layers, and get no error.</li>
<li>load model on the same GPU but nothing helpful.</li>
</ul>
<p>My working environment:</p>
<ul>
<li>Python 3.6.10, Pytorch 1.7.1(with cuda 11.1)</li>
<li>GPU: NVIDIA 2080Ti</li>
<li>use the same seed(numpy and pytorch) during training and evaluation</li>
<li>use <strong>model.eval()</strong> and <strong>with torch.no_grad()</strong> on dev set during both model training and evaluating.</li>
<li>the same dev set and the same metric calculation method.</li>
</ul>
<p>Here is my pseudocode during training(the original one is too heavy):</p>
<pre class=""lang-python prettyprint-override""><code># load my data.
train_dataset = FinetuningDataset(vocab=vocab, domains=domains, data_files=data_files, max_len=data_config[&quot;max_len&quot;], giga_embedding_vocab=giga_embedding.word2id)

val_dataset = FinetuningDataset(vocab, domains=domains, data_files=dev_data_path, max_len=data_config['max_len'], giga_embedding_vocab=giga_embedding.word2id)

sp_collator = SortPadCollator(sort_key=lambda x:x[0], ignore_indics=[0])   
train_iter = DataLoader(dataset=train_dataset,  
                        batch_size=data_config[&quot;batch_size&quot;], 
                        shuffle=data_config[&quot;shuffle&quot;],
                        collate_fn=sp_collator)
val_iter = DataLoader(dataset=val_dataset,  
                    batch_size=data_config[&quot;batch_size&quot;], 
                    shuffle=data_config[&quot;shuffle&quot;], 
                    collate_fn=sp_collator)
adatrans = AdaTrans(vocab=vocab, config=model_config, domain_size=len(domains))
adatrans.load_state_dict(torch.load('ckpt_adatrans/litebert_1e-3_50cls_cuda2.pt'))
model = MixLM(adatrans=adatrans, vocab=vocab, config=model_config, giga_embedding=giga_embedding)

# this is my loss function during training.
loss_fn_dct = {&quot;mask_loss&quot;: neg_log_likelihood_loss, &quot;emb_mse_loss&quot;:nn.MSELoss(reduction='none'), &quot;domain_cls_loss&quot;:nn.NLLLoss(reduction='none')}
metrics_fn_dct = {&quot;mask_metrics&quot;:accuracy}

# build a trainer.
trainer = ftTrainer(loss_fn_dct=loss_fn_dct, metrics_fn_dct=metrics_fn_dct, config=trainer_config)
# gets best result on dev set and save it to checkpoint.pt
best_res, best_state_dict = trainer.train(model=model, train_iter=train_iter, val_iter=val_iter, optimizer=trainer_config['optimizer'], device=trainer_config['device'])
print(&quot;best result:: &quot;, best_res)
trainer.save(best_state_dict, trainer_config['model_path'])
</code></pre>
<p>and in <strong>trainer.py</strong>, I save the best state dict result and return:</p>
<pre class=""lang-python prettyprint-override""><code>model.eval()
for dev_batch in val_iter:
    with torch.no_grad():
      # this self.val() runs model forward function and return prediction result.
      dev_res = self.val(dev_batch, model, device)
      dev_loss += dev_res['loss'].item()
# this function gets result metric.(which drops during evaluation.)
dev_metric = model.domain_biaffine._attachment_scores.get_metric(reset=True)
if dev_metric['UAS'] &gt; best_UAS:
    best_UAS = dev_metric['UAS']
    best_res, best_state_dict = dev_metric, model.state_dict()

print(&quot;dev_loss: &quot;, dev_loss / cnt_iter)
print(&quot;dev metric: &quot;, dev_metric)
</code></pre>
<p>In <strong>evaluation.py</strong>, I just load the checkpoint.pt and make prediction:</p>
<pre class=""lang-python prettyprint-override""><code>test_dataset = FinetuningDataset(vocab=vocab, domains=domains, data_files=data_files, max_len=data_config[&quot;max_len&quot;], giga_embedding_vocab=giga_embedding.word2id)

sp_collator = SortPadCollator(sort_key=lambda x:x[0], ignore_indics=[0])   

test_iter = DataLoader(dataset=test_dataset,  
                        batch_size=data_config[&quot;batch_size&quot;], 
                        shuffle=False,
                        collate_fn=sp_collator)

adatrans = AdaTrans(vocab=vocab, config=model_config, domain_size=len(domains))
model = MixLM(adatrans=adatrans, vocab=vocab, config=model_config, giga_embedding=giga_embedding)

# load pytorch checkpoint.pt
model.load_state_dict(torch.load(data_config['model_path'], torch.device('cuda:1')), strict=True)

trainer = ftTrainer(config=trainer_config, vocab=vocab, id2word=giga_embedding.id2word)
# this line makes prediction, which do model.forward and print metric(which is the same as the trainer.py snippet.)
trainer.inference(model=model, test_iter=test_iter, device=trainer_config['device'])
</code></pre>
<p>I have been searching for a long time on Google, but got nothing helpful. This totally bothering me. Could anyone help me with it? Thanks in advance!</p>
",Training and Model Evaluation,pytorch save load model lower dev set accuracy question loading pytorch model best result dev set training checkpoint file model evaluation remembering model eval torch grad still get lower accuracy result drop dev set compared get training tried printing state dict pytorch save best result model training compared get loading check code use lot dropout layernorm layer get error load model gpu nothing helpful working environment python pytorch cuda gpu nvidia ti use seed numpy pytorch training evaluation use model eval torch grad dev set model training evaluating dev set metric calculation method pseudocode training original one heavy trainer py save best state dict result return evaluation py load checkpoint pt make prediction searching long time google got nothing helpful totally bothering could anyone help thanks advance
"Input to reshape is a tensor with 61440 values, but the requested shape has 1024","<p>I am creating a classifier with GRU and attention mechanism. The training dataset has 255 training feature vectors and has binary (0/1) label data.</p>
<p>train = (1000, 255)</p>
<p>label = (1000, ) -&gt;{0/1}</p>
<p><strong>About Model</strong></p>
<p>The GRU takes the training data as input (work as an encoder), the attention mechanism would be performed by taking all the hidden units of encoder per time step, then the context vector (attention + output hidden state of the encoder) pass through a softmax layer for prediction.</p>
<pre><code>max_len = 255

BATCH_SIZE = 60
embedding_dim = 256
units = 1024
vocab_train = len(train_token.word_index)+1


class Attention(tf.keras.layers.Layer):
    def __init__(self, units):
        super(Attention, self).__init__()

        self.s = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.L2(0.001))
        self.t = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.L2(0.001))
        self.a = tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.L2(0.001))

    def call(self, query, value):
        #query: (None, 255, 1024)
        #value: (None, 1024)
        
        query = query[0, :]
        value = tf.expand_dims(value, 1)[0, :]
        #prev: (255, 1024)
        #value: (1, 1024)
        
        score = self.a(tf.keras.activations.relu(self.s(query) + self.t(value)))
        
        attention_weights = tf.keras.activations.softmax(score, axis=1) 

        attention = attention_weights * value
        
        attention = tf.reduce_sum(attention, axis=0) 

        return attention, attention_weights

attention = Attention(units)


# encoder 
encoder = tf.keras.Input(shape=(max_len, ))
enc_embd = tf.keras.layers.Embedding(vocab_train, embedding_dim)(encoder)
encoder_gru = tf.keras.layers.GRU(units, return_sequences=True, return_state=True, kernel_regularizer=tf.keras.regularizers.L2(0.001))
output_e, hidden_e = encoder_gru(enc_embd)

# attention
attention_value, attention_weights = attention(output_e, hidden_e)

# reshaping to concatenate the attention and hidden_e layer
attention_value = tf.reshape(attention_value, (1, attention_value.shape[0]))
hidden_e = tf.reshape(hidden_e, (1, hidden_e.shape[1]))

context_vector = tf.concat([attention_value, hidden_e], axis=1)

attention_layer = tf.keras.layers.Dense(1*context_vector.shape[1], activation='relu')
attention_ = attention_layer(context_vector)

# final output layer
final_output = tf.keras.layers.Dense(1, activation='softmax', kernel_regularizer=tf.keras.regularizers.L2(0.001))
output_f = final_output(attention_)



model = tf.keras.Model(encoder, output_f)

print(model.summary())

model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])

model.fit(train, label, batch_size=60, epochs=10, verbose=1)
</code></pre>
<p>ERROR</p>
<pre><code>InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Input to reshape is a tensor with 61440 values, but the requested shape has 1024
     [[node model_11/tf.reshape_51/Reshape (defined at &lt;ipython-input-336-3c6515bf3e82&gt;:1) ]]
     [[gradient_tape/model_11/embedding_42/embedding_lookup/Reshape/_38]]
  (1) Invalid argument:  Input to reshape is a tensor with 61440 values, but the requested shape has 1024
     [[node model_11/tf.reshape_51/Reshape (defined at &lt;ipython-input-336-3c6515bf3e82&gt;:1) ]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_51709]

Function call stack:
train_function -&gt; train_function

</code></pre>
",Training and Model Evaluation,input reshape tensor value requested shape ha creating classifier gru attention mechanism training dataset ha training feature vector ha binary label data train label model gru take training data input work encoder attention mechanism would performed taking hidden unit encoder per time step context vector attention output hidden state encoder pas softmax layer prediction error
Incremental/ Continue Model Training for Gensin Doc2Vec model,"<p>I have a gensim doc2vec model trained on around 1000 documents. Now I have to incrementally update this existing model by adding 100 newly tagged documents. I am not able to incrementally retrain this model. Can anyone help me with the same.</p>
",Training and Model Evaluation,incremental continue model training gensin doc vec model gensim doc vec model trained around document incrementally update existing model adding newly tagged document able incrementally retrain model anyone help
Why is a simple embedding+linear layer outperforming this LSTM classifier?,"<p>I'm running into a roadblock in my learning about NLP. I'm working on a <a href=""https://www.kaggle.com/c/nlp-getting-started"" rel=""nofollow noreferrer"">beginner's Kaggle competition</a> classifying tweets as &quot;disaster&quot; or &quot;not disaster&quot;. I started out by repurposing a simple network from a <a href=""https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"" rel=""nofollow noreferrer"">PyTorch tutorial</a> comprised of <code>nn.EmbeddingBag</code> and <code>nn.Linear</code> layers and saw decent results during both training and inference:</p>
<pre><code>self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)
self.fc = nn.Linear(embed_dim, num_class)
</code></pre>
<p><a href=""https://i.sstatic.net/G0sRa.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/G0sRa.png"" alt=""enter image description here"" /></a></p>
<p>The loss function is <code>BCEWithLogits</code>, by the way.</p>
<p>I decided to up my game and throw an LSTM into the mix. I took a deep dive into padded/packed sequences and think I understand them pretty well. After perusing around and thinking about it, I came to the conclusion that I should be grabbing the <em>final non-padded</em> hidden state of each sequence's output from the LSTM. That's what I tried below:</p>
<h2 id=""my-attempt-at-upping-my-game-e68x"">My attempt at upping my game:</h2>
<pre class=""lang-py prettyprint-override""><code>
class TextClassificationModel(nn.Module):

    def __init__(self, vocab_size, embed_dim, hidden_size, num_class):
        super(TextClassificationModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True)
        self.fc1 = nn.Linear(hidden_size, num_class)

    def forward(self, padded_seq, lengths):
        
        # embedding layer
        embedded_padded = self.embedding(padded_seq)
        packed_output = pack_padded_sequence(embedded_padded, lengths, batch_first=True)

        # lstm layer
        output, _ = self.lstm(packed_output)
        padded_output, lengths = pad_packed_sequence(output, batch_first=True)

        # get hidden state of final non-padded sequence element:
        h_n = []
        for seq, length in zip(padded_output, lengths):
            h_n.append(seq[length - 1, :])
        
        lstm_out = torch.stack(h_n)
        
        # linear layers
        out = self.fc1(lstm_out)
        return out
</code></pre>
<p>This morning, I ported my notebook over to an IDE and ran the debugger and confirmed that <code>h_n</code> is indeed the final hidden state of each sequence, not including padding.</p>
<p>So everything runs/trains without error but my loss never decreases when I use batch size &gt; 1.</p>
<p>With <code>batch_size = 8</code>:
<a href=""https://i.sstatic.net/FCrES.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FCrES.png"" alt=""enter image description here"" /></a></p>
<p>With <code>batch_size = 1</code>:
<a href=""https://i.sstatic.net/PvLY0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PvLY0.png"" alt=""enter image description here"" /></a></p>
<h3 id=""my-question-n58c"">My Question</h3>
<p>I would have expected this LSTM setup to perform much better on this simple task. So I'm wondering &quot;Where have I gone wrong?&quot;</p>
<h3 id=""additional-information-training-code-xgb7"">Additional Information: Training Code</h3>
<pre><code>def train_one_epoch(model, opt, criterion, lr, trainloader):
    model.to(device)
    model.train()
    
    running_tl = 0
    
    for (label, data, lengths) in trainloader:
        
        opt.zero_grad()
        label = label.reshape(label.size()[0], 1)
        
        output = model(data, lengths)
        loss = criterion(output, label)

        running_tl += loss.item()
        loss.backward()
        opt.step()
        
    return running_tl
        
def validate_one_epoch(model, opt, criterion, lr, validloader):
    
    running_vl = 0
    
    model.eval()
    with torch.no_grad():
        for (label, data, lengths) in validloader:
            label = label.reshape(label.shape[0], 1)
            output = model(data, lengths)
            loss = criterion(output, label)
            running_vl += loss.item()
            
    return running_vl
    

def train_model(model, opt, criterion, epochs, trainload, testload=None, lr=1e-3):
    
    avg_tl_per_epoch = []
    avg_vl_per_epoch = []
    
    for e in trange(epochs):
        running_tl = train_one_epoch(model, opt, criterion, lr, trainload)
        avg_tl_per_epoch.append(running_tl / len(trainload))
        if testload:
            running_vl = validate_one_epoch(model, opt, criterion, lr, validloader)
            avg_vl_per_epoch.append(running_vl / len(testload))
    
    return avg_tl_per_epoch, avg_vl_per_epoch
            
</code></pre>
",Training and Model Evaluation,simple embedding linear layer outperforming lstm classifier running roadblock learning nlp working beginner kaggle competition classifying tweet disaster disaster started repurposing simple network pytorch tutorial comprised layer saw decent result training inference loss function way decided game throw lstm mix took deep dive padded packed sequence think understand pretty well perusing around thinking came conclusion grabbing final non padded hidden state sequence output lstm tried attempt upping game morning ported notebook ide ran debugger confirmed indeed final hidden state sequence including padding everything run train without error loss never decrease use batch size question would expected lstm setup perform much better simple task wondering gone wrong additional information training code
Develop Question Answer System using BERT,"<p>i'm currently developing a Question-Answer system (<strong>in Indonesian language</strong>) using BERT for my thesis.
The dataset and the questions given are in Indonesian.</p>
<p>The problem is, i'm still not clear on how the step-to-step process to develop the Question-Answer system in BERT.</p>
<p>From what I concluded after reading a number of research journals and papers, the process might be like this:</p>
<ol>
<li>Prepare main dataset</li>
<li>Load Pre-Train Data</li>
<li>Train the main dataset with the pre-train data (so that it produce &quot;fine-tuned&quot; model)</li>
<li>Cluster the fine-tuned model</li>
<li>Testing (giving questions to the system)</li>
<li>Evaluation</li>
</ol>
<p>What i want to ask are :</p>
<ul>
<li>Are those steps correct? Or maybe there any missing step(s)?</li>
<li>Also, if the default pre-train data that BERT provide is in English while my main dataset is in Indonesian, how can i create my own indonesian pre-train data?</li>
<li>Does it really need to perform data/model clustering in BERT?</li>
</ul>
<p>I appreciate any helpful answer(s).
Thank you very much in advance.</p>
",Training and Model Evaluation,develop question answer system using bert currently developing question answer system indonesian language using bert thesis dataset question given indonesian problem still clear step step process develop question answer system bert concluded reading number research journal paper process might like prepare main dataset load pre train data train main dataset pre train data produce fine tuned model cluster fine tuned model testing giving question system evaluation want ask step correct maybe missing step also default pre train data bert provide english main dataset indonesian create indonesian pre train data doe really need perform data model clustering bert appreciate helpful answer thank much advance
Gensim 3.8.3 Word2Vec is not updating the weights/parameters on a toy dataset,"<p>I am trying to train word2vec model on a simple toy dateset of 4 sentences.
The Word2vec version that I need is:</p>
<ul>
<li>Skip-gram model</li>
<li>no negative sampling</li>
<li>no hierarchical soft-max</li>
<li>no removal or down-scaling of frequent words</li>
<li>vector size of words is 2</li>
<li>Window size 4 i.e all the words in a sentence are considered context words of each other.</li>
<li>epochs can be varied from 1 to 500</li>
</ul>
<p><strong>Problem that I am facing is:</strong> No matter how I change the above parameters, the word vectors are not being updated/learned. The word vectors for epochs=1 and epochs=500 are being same.</p>
<pre><code>from gensim.models import Word2Vec
import numpy as np
import matplotlib.pyplot as plt
import nltk

# toy dataset with 4 sentences
sents = ['what is the time',
         'what is the day',
         'what time is the meeting',
         'cancel the meeting']

sents = [nltk.word_tokenize(string) for string in sents]

# model initialization and training
model = Word2Vec(alpha=0.5, min_alpha =0.25, min_count = 0, size=2, window=4,
                 workers=1, sg = 1, hs = 0, negative = 0, sample=0, seed = 42)

model.build_vocab(sents)
model.train(sents, total_examples=4, epochs=500)

# getting word vectors into array
vocab = model.wv.vocab.keys()
vocab_vectors = model.wv[vocab]
print(vocab)
print(vocab_vectors)

#plotting word vectors
plt.scatter(vocab_vectors[:,0], vocab_vectors[:,1], c =&quot;blue&quot;)
for i, word in enumerate(vocab):
    plt.annotate(word, (vocab_vectors[i,0], vocab_vectors[i,1]))
</code></pre>
<p>The out put of <code>print(vocab)</code> is as below</p>
<p><code>['what', 'is', 'time', 'cancel', 'the', 'meeting', 'day']</code></p>
<p>The output of <code>print(vocab_vectors)</code> is as below</p>
<pre><code>[[ 0.08136337 -0.05059118]
 [ 0.06549312 -0.22880174]
 [-0.08925873 -0.124718  ]
 [ 0.05645624 -0.03120007]
 [ 0.15067646 -0.14344342]
 [-0.12645201  0.06202405]
 [-0.22905378 -0.01489289]]
</code></pre>
<p>The plotted 2D vectors <a href=""https://i.sstatic.net/1BknG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1BknG.png"" alt=""Here!"" /></a></p>
<p><strong>Why do I think the vectors are not being learned?</strong> I am changing the epochs value to 1, 10, 50, 500... and running the whole code to check the output for each run. For epochs = #any_value &lt;1,10,50,500&gt;, the output (vocab, vocab_vectors, and the plot) is being same for all the runs.</p>
",Training and Model Evaluation,gensim word vec updating weight parameter toy dataset trying train word vec model simple toy dateset sentence word vec version need skip gram model negative sampling hierarchical soft max removal scaling frequent word vector size word window size e word sentence considered context word epoch varied problem facing matter change parameter word vector updated learned word vector epoch epoch put output plotted vector think vector learned changing epoch value running whole code check output run epoch value output vocab vocab vector plot run
How Can I save And reuse One hot encoding in keras?,"<p>I'm working on a project that related to NLP. then i use One hot encode for text representation in google colab Then i fit it into LSTM.
This is my code:</p>
<pre class=""lang-py prettyprint-override""><code>from tensorflow.keras.preprocessing.text import one_hot
voc_size=13000
onehot_repr=[one_hot(words,voc_size)for words in X1] 
</code></pre>
<p>the model seem good but when i want to save it for making prediction with new text i save it using pickle:</p>
<pre class=""lang-py prettyprint-override""><code> import pickle
 with open(&quot;one_hot&quot;, &quot;wb&quot;) as f: 
     pickle.dump(one_hot, f)
</code></pre>
<p>but when i restart the colab and load the saved one_hot again the number that represent a word is difference.</p>
<ul>
<li>Is there any possible way that i can save Onehot and get the same result in colab?</li>
</ul>
<p>Because I can not save one hot encode for using another time that why i save one hot representation  as list and access it by index later:</p>
<pre class=""lang-py prettyprint-override""><code>## load save model
from tensorflow.keras.models import load_model
my_model=load_model(&quot;model9419.h5&quot;)

##load oneHot representation
with open('/content/drive/MyDrive/last_model/on_hot.json', 'rb') as f:
    oneHot=json.load(f)
</code></pre>
<p>In order to predict A word i used simple array access element to find one hot representation of a words.</p>
<ul>
<li><p>Is This a correct way to make a prediction ? Is there any better way than that?</p>
</li>
<li><p>And If I can save OneHot function how can i use in flask server?</p>
</li>
<li><p>Also can anyone recommend word representation that is easy, can save to use in flask and better?</p>
</li>
</ul>
",Training and Model Evaluation,save reuse one hot encoding kera working project related nlp use one hot encode text representation google colab fit lstm code model seem good want save making prediction new text save using pickle restart colab load saved one hot number represent word difference possible way save onehot get result colab save one hot encode using another time save one hot representation list access index later order predict word used simple array access element find one hot representation word correct way make prediction better way save onehot function use flask server also anyone recommend word representation easy save use flask better
"Flair training german NER model: Dev, f1 score almost 0.0, why the model doesn&#39;t learn?","<p>I trained my NER model first with Spacy with the micro F1 score 64.7% (8 classes). Next step I wanted to train Flair, hoping to get better results.
Of course the spacy format data would be convert to proper Flair corpus with some custom codes.</p>
<p>Info about the input data:
Corpus: &quot;Corpus: 4037 train + 840 dev + 448 test sentences&quot;</p>
<p>in training set:
'Kultur' (1512), 'Erreger' (1376), 'Mittel' (1083), 'Auftreten' (583),
'Zeit' (285), 'Witterung' (238), 'BBCH_Stadium' (214), 'Ort' (161)</p>
<p>in test set:
'Erreger' (390), 'Mittel' (311), 'Kultur' (221), 'BBCH_Stadium' (148),
'Auftreten' (54), 'Witterung' (54), 'Ort' (53), 'Zeit' (40)</p>
<p>Corpus look like this:</p>
<pre><code>Der O
Schwerpunkt O
der O
Unkrautbekämpfung O
in O
Kartoffeln S-Kultur
liegt O
im O
Vorauflauf S-BBCH_Stadium
. O

Sind O
die O
mechanischen O
Maßnahmen O
abgeschlossen O
, O
kann O
die O
erste O
Herbizidbehandlung O
auf O
gut O
abgesetzten O
Dämmen O
, O
je O
nach O
Produkt O
bis O
kurz B-BBCH_Stadium
vor I-BBCH_Stadium
dem I-BBCH_Stadium
Durchstoßen E-BBCH_Stadium
der O
Kartoffeln S-Kultur
( O
kvD O
) O
, O
erfolgen O
. O
</code></pre>
<p>The training code:</p>
<pre><code>from flair.datasets import ColumnCorpus
from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings,  FlairEmbeddings
from flair.models import SequenceTagger
from flair.trainers import ModelTrainer
from typing import List
import time
start_time = time.time()

columns = {0: 'text', 1: 'ner'}

 data_path = &quot;path/to/data&quot;

 # initializing the corpus
 corpus: Corpus = ColumnCorpus(data_path, columns,
                          train_file = 'bb_train.txt',
                        #   dev_file = 'bb_test_sm_sm.txt',
                          test_file = 'bb_test_sm_sm.txt',
                          )

 # tag to predict
 tag_type = 'ner'

 tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)

 word_vectors = gensim.models.KeyedVectors.load_word2vec_format('german.model', binary=True)
 word_vectors.save('german.model.gensim')

 german_embedding = WordEmbeddings('german.model.gensim')

# init forward embedding for German
 flair_embedding_forward = FlairEmbeddings('de-forward')
 flair_embedding_backward = FlairEmbeddings('de-backward')

embedding_types: List[TokenEmbeddings] = [
     german_embedding,
     flair_embedding_forward,
      flair_embedding_backward,
 ]

embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)

tagger: SequenceTagger = SequenceTagger(hidden_size=256,
                                    embeddings=embeddings,
                                    tag_dictionary=tag_dictionary,
                                    tag_type=tag_type,
                                    use_crf=True)


trainer : ModelTrainer = ModelTrainer(tagger, corpus)

trainer.train('resources/taggers/ner_bb',
              learning_rate=0.01,
              mini_batch_size=64,
               max_epochs=5,
               )

print(f&quot;It took {time.time() - start_time}&quot;)
</code></pre>
<p>the loss log is:</p>
<pre><code>EPOCH   TIMESTAMP   BAD_EPOCHS  LEARNING_RATE   TRAIN_LOSS  DEV_LOSS    DEV_PRECISION   DEV_RECALL  DEV_F1
1   10:42:23    0   0.0100  42.28642028570175   28.403223037719727  0.0197  0.0748  0.0312
2   10:43:48    0   0.0100  17.928552985191345  14.348283767700195  0.3089  0.0312  0.0567
3   10:45:10    0   0.0100  10.604630261659622  13.98863697052002   0.3089  0.0312  0.0567
4   10:46:36    1   0.0100  10.26459190249443   13.614569664001465  0.3579  0.0279  0.0518
5   10:47:55    2   0.0100  9.987788125872612   13.339178085327148  0.3333  0.0164  0.0313
</code></pre>
<p>Why the score is so low ? the model doesn't learn anything.
I tried until 10 epochs and same results.</p>
<p>Do I need to tune some parameters ? Is something wrong with my corpus ?</p>
<p>Thank you if you have experience with it.</p>
",Training and Model Evaluation,flair training german ner model dev f score almost model learn trained ner model first spacy micro f score class next step wanted train flair hoping get better result course spacy format data would convert proper flair corpus custom code info input data corpus corpus train dev test sentence training set kultur erreger mittel auftreten zeit witterung bbch stadium ort test set erreger mittel kultur bbch stadium auftreten witterung ort zeit corpus look like training code loss log score low model learn anything tried epoch result need tune parameter something wrong corpus thank experience
Examine data after building Machine Learning model in python,"<p>I built a sentiment analysis model in Arabic; in Python; after building the model, how can I test it with external data and how to build the code for that?</p>
<p>When I fitting the model, I extracted the features via tf-idf, and the problem I faced is dealing with it when I want to test external data after I trained the model on the training and test data.</p>
<p><strong>summary :
After I trained the model and reached an accuracy of 88%, I want to build a code that tests the model with external data..</strong></p>
<pre><code># train = 3461 record  
# test = 61 record \
# combi = train + test - to apply tf-idf

train = pd.read_excel('Final_train.xlsx')

test = pd.read_excel('Testing.xlsx' , usecols=['Tweet'])


# merge train &amp; test to apply all function on it 
def combine(tr,te):
   global combi 
   combi = tr.append(te , ignore_index=True)


# this is script to removing all stop words based on NLTK ( Natural Language ToolKit )

def remove_stop(combi1):
    combi1['Tweet'] = combi1['Tweet'].apply(lambda x: &quot; &quot;.join(x for x in x.split() if x not in stop))
    return combi1


#
# this is script to returns Arabic root for the given token Provided by University of Nevada, Las Vegas, USA.

def steeming(combi2):
    st = ISRIStemmer()
    combi2['Tweet'] = combi2['Tweet'].apply(lambda x: &quot; &quot;.join([st.stem(word) for word in x.split()])) 
    return combi2


# This is a List contain many word not Related to our domain we need to remove it from our Dataset
# to make ML Model Work properly &amp; Accurate __ I built it Manually in order to to develop accuracy of model 

def remove_unneded_word(combi3):
    unword =  pd.read_excel('Final_train &amp; MCSA/Un_neededword.xlsx')
    unword = unword.squeeze()
    unword = list(unword)
    combi3['Tweet'] = combi3['Tweet'].apply(lambda x: &quot; &quot;.join(x for x in x.split() if x not in unword))
    return combi3

# Calling the Each function Alone 
combine(train,test)
combi = remove_stop(combi)
combi = steeming(combi)
combi = remove_unneded_word(combi)



#                  ______________________________________________
#                 | Term Frequency–Inverse Document Frequency    |
#
#   To Represent Each word as matrix of numbers 

tfidf_vectorizer = TfidfVectorizer(max_df=0.8,min_df=5, max_features=1600)
# TF-IDF feature matrix
tfidf = tfidf_vectorizer.fit_transform(combi['Tweet'])




from sklearn import svm #Import scikit-learn  to apply support victor machine Algorithm 
from sklearn.model_selection import train_test_split
from sklearn import metrics


train_bow = tfidf[:3641,:]
test_bow = tfidf[3641:,:]

# splitting data into training and validation set
Tr_D_bow, Te_D_bow, Tr_L_bow , Te_L_bow = train_test_split(train_bow, train['Class'], random_state=45, test_size=0.2)
# Create SVM classifer object
SVM = svm.SVC()
# Train SVM Classifer
SVM = SVM.fit(Tr_D_bow,Tr_L_bow)

#Predict the response for test dataset
SVM_pre = SVM.predict(Te_D_bow)



print('The Accuracy of SVMC is --&gt;',metrics.accuracy_score(Te_L_bow, SVM_pre))
</code></pre>
",Training and Model Evaluation,examine data building machine learning model python built sentiment analysis model arabic python building model test external data build code fitting model extracted feature via tf idf problem faced dealing want test external data trained model training test data summary trained model reached accuracy want build code test model external data
NLP - How to add more features?,"<p>I want to use a sklearn classifier to train a model to classify data entries (yes,no) using a text feature (content), a numerical feature (population) and a categorical feature (location). </p>

<p><a href=""https://i.sstatic.net/EA1g4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EA1g4.png"" alt=""enter image description here""></a></p>

<p>The model below is using only the text data to classify each entry. The text is converted with TF-IDF into a sparse matrix before being imported into the classifier.</p>

<p>Is there a way to add/use also the other features? These features are not in sparse matrix format so not sure how to combine them with the text sparse matrix. </p>

<pre class=""lang-py prettyprint-override""><code>
    #import libraries
    import string, re, nltk
    import pandas as pd
    from pandas import Series, DataFrame
    from nltk.corpus import stopwords
    from nltk.stem.porter import PorterStemmer
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.feature_extraction.text import TfidfTransformer
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report
    from sklearn.pipeline import Pipeline

    # read data and remove empty lines
    dataset = pd.read_csv('sample_data.txt',
                           sep='\t',
                           names=['content','location','population','target'])
                           .dropna(how='all')
                           .dropna(subset=['target'])

    df = dataset[1:]

    #reset dataframe index
    df.reset_index(inplace = True)

    #add an extra column which is the length of text
    df['length'] = df['content'].apply(len)

    #create a dataframe that contains only two columns the text and the target class
    df_cont = df.copy()
    df_cont = df_cont.drop(
        ['location','population','length'],axis = 1)

    # function that takes in a string of text, removes all punctuation, stopwords and returns a list of cleaned text

    def text_process(mess):
        # lower case for string
        mess = mess.lower()

        # check characters and removes URLs
       nourl = re.sub(r'http\S+', ' ', mess)

        # check characters and removes punctuation
        nopunc = [char for char in nourl if char not in string.punctuation]

        # join the characters again to form the string and removes numbers
        nopunc = ''.join([i for i in nopunc if not i.isdigit()])

        # remove stopwords
        return [ps.stem(word) for word in nopunc.split() if word not in set(stopwords.words('english'))]

    #split the data in train and test set and train/test the model

    cont_train, cont_test, target_train, target_test = train_test_split(df_cont['content'],df_cont['target'],test_size = 0.2,shuffle = True, random_state = 1)


    pipeline = Pipeline([('bag_of_words',CountVectorizer(analyzer=text_process)),
                         ('tfidf',TfidfTransformer()),
                         ('classifier',MultinomialNB())])

    pipeline.fit(cont_train,target_train)
    predictions = pipeline.predict(cont_test)

    print(classification_report(predictions,target_test))

</code></pre>

<p>The model is expected to return the following: accuracy, precision, recall ,f1-score</p>
",Training and Model Evaluation,nlp add feature want use sklearn classifier train model classify data entry yes using text feature content numerical feature population categorical feature location model using text data classify entry text converted tf idf sparse matrix imported classifier way add use also feature feature sparse matrix format sure combine text sparse matrix model expected return following accuracy precision recall f score
Iterable dataset exhausts after a single epoch,"<p>I wanted to train an RNN on the task of sentiment analysis, for this task I was using the IMDB dataset provided by torchtext which contains 50000 movie reviews and it is a python iterator. I used a <code>split=('train', 'test')</code>.</p>
<p>I first built a vocab using <code>torchtext.vocab.Vocab</code> and tokenized each of the sentence and then performed numericalisation.</p>
<p>To pad the sequence to the same length I used <code>torch.nn.utils.rnn.pad_sequence</code> and also used a <code>collate_fn</code> together with <code>batch_sampler</code>. Then I loaded the data using torch.utils.data.<code>DataLoader</code>.</p>
<p>The implementation of the RNN Network is fine but the dataloader is exhausted after a single epoch as you can see in the image attached below.</p>
<p>Am I following the right approach to load this iterable-dataset? and why is the dataloader exhausted after a single epoch and how do I overcome this issue.</p>
<p>Pleases refer to the shared colab notebook if you want to see my implementation.</p>
<p>PS. I was following the official <a href=""https://github.com/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb"" rel=""nofollow noreferrer"">changelog</a> of torchtext from github</p>
<p>You can find my implementation <a href=""https://colab.research.google.com/drive/1Ep0s2D2d7fNhhUrYgPTtd5A74FPKsytg?usp=sharing"" rel=""nofollow noreferrer"">here</a></p>
<p><a href=""https://i.sstatic.net/0t6Fm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0t6Fm.png"" alt=""Dataloader exhausted after a single epoch"" /></a></p>
",Training and Model Evaluation,iterable dataset exhaust single epoch wanted train rnn task sentiment analysis task wa using imdb dataset provided torchtext contains movie review python iterator used first built vocab using tokenized sentence performed numericalisation pad sequence length used also used together loaded data using torch utils data implementation rnn network fine dataloader exhausted single epoch see image attached following right approach load iterable dataset dataloader exhausted single epoch overcome issue plea refer shared colab notebook want see implementation p wa following official changelog torchtext github find implementation
"What method should be used to tag specific texts, when the dataset is too small for training a model?","<p>My problem is the following:</p>
<p>The task is to use a Machine Learning Algorithm to perform tagging on texts. This would be simple enough if those texts were about common topics where large datasets for training are readily available.</p>
<p>But these texts describe company KPI's and Products. The tagging should improve search inside the upcoming Data Catalog.</p>
<p>What complicates the matter: the amount of already human-tagged texts is very small (around 50)</p>
<p>So I was searching for a method to train a model on a dataset that has nothing to do with our dataset since it is highly specialized.
Is there a way to adapt pre-trained text classifiers to our data?</p>
<p>I'm fairly new to NLP/Machine Learning since I recently started studying this field. I mainly program in Python. If a solution in Python is available, it would be great!</p>
<p>Any help would be much appreciated!</p>
",Training and Model Evaluation,method used tag specific text dataset small training model problem following task use machine learning algorithm perform tagging text would simple enough text common topic large datasets training readily available text describe company kpi product tagging improve search inside upcoming data catalog complicates matter amount already human tagged text small around wa searching method train model dataset ha nothing dataset since highly specialized way adapt pre trained text classifier data fairly new nlp machine learning since recently started studying field mainly program python solution python available would great help would much appreciated
How to extract activations from dense layer,"<p>I am trying to implement the preprocessing code for this <a href=""https://arxiv.org/pdf/1908.11540.pdf"" rel=""nofollow noreferrer"">paper</a> (code in this <a href=""https://github.com/SenticNet/conv-emotion"" rel=""nofollow noreferrer"">repo</a>). The preprocessing code is described in the paper here:</p>

<p>""A convolutional neural network (Kim, 2014) is
used to extract textual features from the transcript
of the utterances. We use a single convolutional
layer followed by max-pooling and a fully connected layer to obtain the feature representations
for the utterances. The input to this network is
the 300 dimensional pretrained 840B GloVe vectors (Pennington et al., 2014). We use filters of
size 3, 4 and 5 with 50 feature maps in each. The
convoluted features are then max-pooled with a
window size of 2 followed by the ReLU activation (Nair and Hinton, 2010). <strong>These are then concatenated and fed to a 100 dimensional fully connected layer, whose activations form the representation of the utterance.</strong> This network is trained at
utterance level with the emotion labels.""</p>

<p>The authors of the paper state that CNN feature extraction code can be found in this <a href=""https://github.com/dennybritz/cnn-text-classification-tf"" rel=""nofollow noreferrer"">repo</a>. However, this code is for a complete model that does sequence classification. It does everything in the quote above except the bolded part (and it goes further to complete do classification). I want the edit the code to build that concatenates and feeds into the 100d layer and then extracts the activations. The data to train on is found in the repo (its the IMDB dataset).</p>

<p>The output should be a (100, ) tensor for each sequence. </p>

<p>Here's the code for the CNN model:</p>

<pre><code>import tensorflow as tf
import numpy as np


class TextCNN(object):
    """"""
    A CNN for text classification.
    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.
    """"""
    def __init__(
      self, sequence_length, num_classes, vocab_size,
      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):

        # Placeholders for input, output and dropout
        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x"")
        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")
        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")

        # Keeping track of l2 regularization loss (optional)
        l2_loss = tf.constant(0.0)

        # Embedding layer
        with tf.device('/cpu:0'), tf.name_scope(""embedding""):
            self.W = tf.Variable(
                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),
                name=""W"")
            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)
            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)

        # Create a convolution + maxpool layer for each filter size
        pooled_outputs = []
        for i, filter_size in enumerate(filter_sizes):
            with tf.name_scope(""conv-maxpool-%s"" % filter_size):
                # Convolution Layer
                filter_shape = [filter_size, embedding_size, 1, num_filters]
                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")
                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=""b"")
                conv = tf.nn.conv2d(
                    self.embedded_chars_expanded,
                    W,
                    strides=[1, 1, 1, 1],
                    padding=""VALID"",
                    name=""conv"")
                # Apply nonlinearity
                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")
                # Maxpooling over the outputs
                pooled = tf.nn.max_pool(
                    h,
                    ksize=[1, sequence_length - filter_size + 1, 1, 1],
                    strides=[1, 1, 1, 1],
                    padding='VALID',
                    name=""pool"")
                pooled_outputs.append(pooled)

        # Combine all the pooled features
        num_filters_total = num_filters * len(filter_sizes)
        self.h_pool = tf.concat(pooled_outputs, 3)
        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])

        # Add dropout
        with tf.name_scope(""dropout""):
            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)

        # Final (unnormalized) scores and predictions
        with tf.name_scope(""output""):
            W = tf.get_variable(
                ""W"",
                shape=[num_filters_total, num_classes],
                initializer=tf.contrib.layers.xavier_initializer())
            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=""b"")
            l2_loss += tf.nn.l2_loss(W)
            l2_loss += tf.nn.l2_loss(b)
            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=""scores"")
            self.predictions = tf.argmax(self.scores, 1, name=""predictions"")

        # Calculate mean cross-entropy loss
        with tf.name_scope(""loss""):
            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)
            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss

        # Accuracy
        with tf.name_scope(""accuracy""):
            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))
            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")
</code></pre>

<p>I want to do the concatenation into the 100d layer to get the activations, I think around line 59 (right before the <code># Add Dropout</code> section near the bottom, and then comment out the rest below it). How do I do this?</p>
",Training and Model Evaluation,extract activation dense layer trying implement preprocessing code paper code repo preprocessing code described paper convolutional neural network kim used extract textual feature transcript utterance use single convolutional layer followed max pooling fully connected layer obtain feature representation utterance input network dimensional pretrained b glove vector pennington et al use filter size feature map convoluted feature max pooled window size followed relu activation nair hinton concatenated fed dimensional fully connected layer whose activation form representation utterance network trained utterance level emotion label author paper state cnn feature extraction code found repo however code complete model doe sequence classification doe everything quote except bolded part go complete classification want edit code build concatenates feed layer extract activation data train found repo imdb dataset output tensor sequence code cnn model want concatenation layer get activation think around line right section near bottom comment rest
Spacy 3.0.1 Accuracy prediction,"<p>How to test accuracy of a spacy pretrained model in version 3.0.1. I want to see my output how accurate my tested model is predicted.This the code below for spacy version 2 but it doesn't work in spacy version 3.can somone tell me the code on spacy version 3.</p>
<pre><code> from spacy.gold import GoldParse
 from spacy.scorer import Scorer

def evaluate(nlp, examples, ent='PERSON'):
scorer = Scorer()
for input_, annot in examples:
    text_entities = []
    for entity in annot.get('entities'):
        if ent in entity:
            text_entities.append(entity)
    doc_gold_text = nlp.make_doc(input_)
    gold = GoldParse(doc_gold_text, entities=text_entities)
    pred_value = nlp(input_)
    scorer.score(pred_value, gold)
return scorer.scores

examples = [
(&quot;Trump says he's answered Mueller's Russia inquiry questions \u2013 live&quot;,{&quot;entities&quot;:[[0,5,&quot;PERSON&quot;],[25,32,&quot;PERSON&quot;],[35,41,&quot;GPE&quot;]]}),
(&quot;Alexander Zverev reaches ATP Finals semis then reminds Lendl who is boss&quot;,{&quot;entities&quot;:[[0,16,&quot;PERSON&quot;],[55,60,&quot;PERSON&quot;]]}),
(&quot;Britain's worst landlord to take nine years to pay off string of fines&quot;,{&quot;entities&quot;:[[0,7,&quot;GPE&quot;]]}),
(&quot;Tom Watson: people's vote more likely given weakness of May's position&quot;,{&quot;entities&quot;:[[0,10,&quot;PERSON&quot;],[56,59,&quot;PERSON&quot;]]}),
]

nlp = spacy.load('en_core_web_sm')
results = evaluate(nlp, examples)
print(results)
</code></pre>
",Training and Model Evaluation,spacy accuracy prediction test accuracy spacy pretrained model version want see output accurate tested model predicted code spacy version work spacy version somone tell code spacy version
Train Spacy TextCategorizer on text that belongs to no label,"<p>I started to experiment with Spacy's <a href=""https://spacy.io/api/textcategorizer"" rel=""nofollow noreferrer"">TextCategorizer</a> and was able to train a model with a few hundred examples and exclusive labels for each example. My idea was to apply this model to text chunks (sentence by sentence, or paragraph by paragraph) and get a label for each chunk. But a lot of chunks should actually be without label, as they do not belong to any category. I had two ideas:</p>
<ul>
<li>Add an additional label <code>other</code> and train examples that don't belong to any other category with this label.</li>
<li>Set the scores of all label to <code>0.0</code> for the examples that don't belong to any other category.</li>
</ul>
<p>Or is there any other approach? Is this something the TextCategorizer can do or are there other models that I can try that might work better?</p>
",Training and Model Evaluation,train spacy textcategorizer text belongs label started experiment spacy textcategorizer wa able train model hundred example exclusive label example idea wa apply model text chunk sentence sentence paragraph paragraph get label chunk lot chunk actually without label belong category two idea add additional label train example belong category label set score label example belong category approach something textcategorizer model try might work better
Identifying isolated elements in homogeneous groups,"<p>I am working on a task of text segmentation in Python.
The texts I am working on should be segmented into 4 sections (based on what they're talking about), let's call them A, B, C and D, usually in this order. These texts are divided in relatively short text segments. The sections are unique (only one per text) and homogeneous (never split, which kinda repeats previous point).</p>
<p>I have got a neural network that identifies the section a segment belongs to with 90% precision, which I'm happy about.
However, when it comes to the last 10%, they are often an isolated segments erroneously tagged, surrounded by other elements righteously tagged.</p>
<p>I can visualise this trough a list of tuples looking like this:</p>
<p><code>[(segment1, A), (segment2, A), (segment3, B), (segment4, A), (segment5, A), (segment6, C), (segment6, C)]</code></p>
<p>In this case, segment3 should be tagged as A, not B, because the sections in the document are always homogeneous. How can I identify a homogeneous group and therefore correct isolated items?</p>
<p>My current method consists in saying &quot;if the element before and the element after are tagged the same, but not the element in the middle, correct the element in the middle&quot; but I'm convinced there's a better way to do this (maybe using a different way of formatting my data?).</p>
<p>However, what am I to do in the case where there are 2 isolated itams next to one another?</p>
<p>Thanks in advance.</p>
",Training and Model Evaluation,identifying isolated element homogeneous group working task text segmentation python text working segmented section based talking let call b c usually order text divided relatively short text segment section unique one per text homogeneous never split kinda repeat previous point got neural network identifies section segment belongs precision happy however come last often isolated segment tagged surrounded element righteously tagged visualise trough list tuples looking like case segment tagged b section document always homogeneous identify homogeneous group therefore correct isolated item current method consists saying element element tagged element middle correct element middle convinced better way maybe using different way formatting data however case isolated itams next one another thanks advance
Naive Gaussian predict probability only returns 0 or 1,"<p>I trained the GaussianNB model from scikit sklearn. When I call the method  <code>classifier.predict_proba</code> it only returns 1 or 0 on new data. It is expected to return a percentage of confidence that the prediction is correct or not. I doubt it can have 100% confidence on new data it has never seen before. I have tested it on multiple different inputs. I use CountVectorizer and TfidfTransformer for the text encoding.</p>
<p>The encoding:</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

count_vect = CountVectorizer()
tfidf_transformer = TfidfTransformer()

X_train_counts = count_vect.fit_transform(X_train_word)
X_train = tfidf_transformer.fit_transform(X_train_counts).toarray()
print(X_train)

X_test_counts = count_vect.transform(X_test_word)
X_test = tfidf_transformer.transform(X_test_counts).toarray()
print(X_test)
</code></pre>
<p>The model: (I am getting an accuracy of 91%)</p>
<pre><code>from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

# Predict Class
y_pred = classifier.predict(X_test)

# Accuracy 
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print(accuracy)
</code></pre>
<p>And finally, when I use the predict_proba method:</p>
<pre><code>y_pred = classifier.predict_proba(X_test)
print(y_pred)
</code></pre>
<p>I am getting an output like:</p>
<pre><code>[[0. 1.]
 [1. 0.]
 [0. 1.]
 ...
 [1. 0.]
 [1. 0.]
 [1. 0.]]
</code></pre>
<p>It doesn't make much sense to have 100% accuracy on new data. Other than on <code>y_test</code> I have tested it on other inputs and it still returns the same. Any help would be appreciated!</p>
<p>Edit for the comments:
The response of <code>.predict_log_proba()</code> is even more strange:</p>
<pre><code>[[ 0.00000000e+00 -6.95947375e+09]
 [-4.83948755e+09  0.00000000e+00]
 [ 0.00000000e+00 -1.26497690e+10]
 ...
 [ 0.00000000e+00 -6.97191054e+09]
 [ 0.00000000e+00 -2.25589894e+09]
 [ 0.00000000e+00 -2.93089863e+09]]
</code></pre>
",Training and Model Evaluation,naive gaussian predict probability return trained gaussiannb model scikit sklearn call method return new data expected return percentage confidence prediction correct doubt confidence new data ha never seen tested multiple different input use countvectorizer tfidftransformer text encoding encoding model getting accuracy finally use predict proba method getting output like make much sense accuracy new data tested input still return help would appreciated edit comment response even strange
Need help in creating an appropriate model to predict semantic similarity between two sentences,"<p>I am new to ML field and trying my hands on creating a model which will predict semantic similarity between two sentences.
I am using following approach:</p>

<p>1.Using word2vec model in gensim package vectorise each word present in the sentences in question</p>

<p>2.Calculate the average vector for all words in every sentence/document </p>

<pre><code>import numpy as np
from scipy import spatial

index2word_set = set(model.wv.index2word)

def avg_feature_vector(sentence, model, num_features, index2word_set):
    words = sentence.split()
    feature_vec = np.zeros((num_features, ), dtype='float32')
    n_words = 0
    for word in words:
        if word in index2word_set:
            n_words += 1
            feature_vec = np.add(feature_vec, model[word])
    if (n_words &gt; 0):
        feature_vec = np.divide(feature_vec, n_words)
    return feature_vec
</code></pre>

<p>3.Next calculate cosine similarity between these two average vectors</p>

<pre><code>s1_afv = avg_feature_vector('this is a sentence', model=model, 
num_features=300, index2word_set=index2word_set)
s2_afv = avg_feature_vector('this is also sentence', model=model, 
num_features=300, index2word_set=index2word_set)
sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)
print(sim)
</code></pre>

<p>Reference stackoverflow question:
<a href=""https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt"">How to calculate the sentence similarity using word2vec model of gensim with python</a></p>

<p>Help needed for the following challenge:</p>

<p>As I want to create a model which would predict semantic similarity between two sentences, I am not quite sure about:</p>

<p>1.Which model would be best suited for this problem</p>

<p>2.Next more importantly how to train that model? </p>

<p>Should I create a matrix where each row will contain two sentences: 
sen1 and sen2 and I would vectorise them and calculate cosine similarity(as per the above mentioned approach)</p>

<p>Then for training data:</p>

<p>X_Train: avg vectors for sen1 and sen2 and their cosine similarity value</p>

<p>y_Train(prediction) : a set of binary values(1 or similar if cosine similarity > 0.7 and 0 otherwise)</p>

<p>I am quite confused whether my approach is correct and how to put a proper approach in the form of a working codebase.</p>

<p>Internet and materials available online are my only teachers to learn ML; thus requesting your guidance in help clearing my gap in understanding and help in coming up with a good working model for my problem.</p>
",Training and Model Evaluation,need help creating appropriate model predict semantic similarity two sentence new ml field trying hand creating model predict semantic similarity two sentence using following approach using word vec model gensim package vectorise word present sentence question calculate average vector word every sentence document next calculate cosine similarity two average vector reference stackoverflow question href calculate sentence similarity using word vec model gensim python help needed following challenge want create model would predict semantic similarity two sentence quite sure model would best suited problem next importantly train model create matrix row contain two sentence sen sen would vectorise calculate cosine similarity per mentioned approach training data x train avg vector sen sen cosine similarity value train prediction set binary value similar cosine similarity otherwise quite confused whether approach correct put proper approach form working codebase internet material available online teacher learn ml thus requesting guidance help clearing gap understanding help coming good working model problem
Unlabeled instances in DOCCANO and SpaCY. Do they offer any value?,"<p>I am using doccano for sequence labelling and spacy for further modeling. Some of the sentences I label do not contain any of the labels I am interested in, so they remain &quot;unlabeled&quot; ie. no tags.</p>
<pre><code>{&quot;id&quot;: 79, &quot;data&quot;: &quot;This powerful charm would protect him until he became of age, or no longer called his aunt's house home.&quot;, &quot;label&quot;: []}
{&quot;id&quot;: 82, &quot;data&quot;: &quot;He began attending Hogwarts School of Witchcraft and Wizardry in 1991.&quot;, &quot;label&quot;: []}
{&quot;id&quot;: 85, &quot;data&quot;: &quot;He later became the youngest Quidditch Seeker in over a century and eventually the captain of the Gryffindor House Quidditch Team in his sixth year, winning two Quidditch Cups.&quot;, &quot;label&quot;: []}
</code></pre>
<p>I want to train SpaCy to recognise character names in all their variations.</p>
<p>Now the questions:</p>
<ul>
<li>is there any value in including unlabeled instances for the purpose of training SpaCy model?</li>
<li>if there is then should I declare this data as &quot;imbalanced dataset&quot; and act accordingly? (boost? smote? over-sampling? etc.)</li>
<li>what are the best practice in cases like this?</li>
</ul>
",Training and Model Evaluation,unlabeled instance doccano spacy offer value using doccano sequence labelling spacy modeling sentence label contain label interested remain unlabeled ie tag want train spacy recognise character name variation question value including unlabeled instance purpose training spacy model declare data imbalanced dataset act accordingly boost smote sampling etc best practice case like
How to evaluate for each custom entity in OpenNLP?,"<p>I am working on a application where users can tag text and train it to a model at the same time. </p>

<p>For this I use OpenNLP (Named Entity Recognition) for Java and so far this works good.</p>

<p>Since the user can add custom tags, I use custom training data. The training works and I get a model.</p>

<p>Now I wan't to test this model by using the Evaluation API. I split my data in test and training data. The test data I use as stream for the evaluation. This works fine too and I get a global precision, recall and f-measure for the entire model.</p>

<p>What I wan't to achieve is this results for every entity separately.</p>

<p>For example:</p>

<pre><code>Although she has always lived in &lt;START:COUNTRY&gt; France &lt;END&gt; , she speaks fluent &lt;START:LANGUAGE&gt; English &lt;END&gt; because her mother was &lt;START:NATIONALITY&gt; American &lt;END&gt; and her father was &lt;START:NATIONALITY&gt; Nigerian &lt;END&gt; .
</code></pre>

<p>Precision for COUNTRY, precision for LANGUAGE, precision for NATIONALITY</p>

<p>Is this possible with OpenNLP? </p>

<p>Please help, thanks!</p>
",Training and Model Evaluation,evaluate custom entity opennlp working application user tag text train model time use opennlp named entity recognition java far work good since user add custom tag use custom training data training work get model wan test model using evaluation api split data test training data test data use stream evaluation work fine get global precision recall f measure entire model wan achieve result every entity separately example precision country precision language precision nationality possible opennlp please help thanks
Pre-trained FastText hyperparameters,"<p>I'm using the pre-trained model:</p>
<pre><code>import fasttext.util 
fasttext.util.download_model('en', if_exists='ignore') # English 
ft = fasttext.load_model('cc.en.300.bin')
</code></pre>
<p>Where can I find an exhaustive list of the values of the hyperparameters used to train the model?
<a href=""https://fasttext.cc/docs/en/options.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/options.html</a> list the default values, that differ from the used one: for example, the dimension of the word vectors is 300 and not 100 (citing <a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a> that doesn't list them all).</p>
",Training and Model Evaluation,pre trained fasttext hyperparameters using pre trained model find exhaustive list value hyperparameters used train model list default value differ used one example dimension word vector citing list
Detecting if a certain text is JavaScript,"<p>I have a text file. Its content may be plain text or JavaScript source code. I need an efficient and high accuracy approach to detect if the text file contains plain text or JavaScript code.</p>
<p><strong>My approach</strong>:
I tried to extract syntax-specific keywords, punctuations, operators, regex based loop detection for this. I added all the values and divided it by the length of the file and got a specific ratio. Let's say the ratio is greater than 0.2 then the file is JavaScript else plain text. The problem I ran in was with regex based loop detection. It was taking too much time for file size &gt; 1 Mega Bytes, hence I abandoned the approach.</p>
<p>Any resources or approaches would be highly appreciated. Thank you.</p>
",Training and Model Evaluation,detecting certain text javascript text file content may plain text javascript source code need efficient high accuracy approach detect text file contains plain text javascript code approach tried extract syntax specific keywords punctuation operator regex based loop detection added value divided length file got specific ratio let say ratio greater file javascript else plain text problem ran wa regex based loop detection wa taking much time file size mega byte hence abandoned approach resource approach would highly appreciated thank
fit word2vec into training set which has data frame structure,"<p>I am begginer in NLP and I have some questions about a classification task. I have a data set in data frame structure which contains two columns, the first on is the texts (so strings) and the second one in the label of each test. So let's say the first column x_train and the seonc one y_train. In order to apply an MLP I could use this code</p>
<pre><code>Tfidf_vect = TfidfVectorizer(max_features = 5000)

Tfidf_vect.fit(input_text)

Train_X_Tfidf = Tfidf_vect.transform(x_train)
Test_X_Tfidf = Tfidf_vect.transform(x_test)
</code></pre>
<p>I want to try the Word2Vec model, but I don't know how to transform my training data into number by using Word2vec. So then I could apply again the MLP model. I would be grateful if you could help me.</p>
",Training and Model Evaluation,fit word vec training set ha data frame structure begginer nlp question classification task data set data frame structure contains two column first text string second one label test let say first column x train seonc one train order apply mlp could use code want try word vec model know transform training data number using word vec could apply mlp model would grateful could help
missing gold corpus in a qa system,"<p>I know that in order to evaluate a QA(question answering) system you need to compare the output(answer) against a gold standard. My question is, in absence of a gold standard, how can I evaluate a QA system?</p>
<p>Thanks in advance!</p>
",Training and Model Evaluation,missing gold corpus qa system know order evaluate qa question answering system need compare output answer gold standard question absence gold standard evaluate qa system thanks advance
Very poor results Word2Vec,"<p>I think there is something I am missing because I am trying to do a Word2Vec, but the results are so bad I think something is wrong.
Here is my code:</p>
<pre><code># 1.65 million documents of 15-20 words each
documents = [['example','of','input'],
             ['second','sentence','of','input'],
             ['this','is','a','list','of','lists']]

model = gensim.models.Word2Vec(documents, size=100, workers=-1, window=3)
model.train(documents, total_examples=model.corpus_count, epochs=30)
</code></pre>
<p>First, the training runs very fast (I expected long time for training with 30 epochs?)
And then when I look for most similar words the results seems completely random, nothing related.</p>
<p>Is there something I am missing?
Thanks</p>
",Training and Model Evaluation,poor result word vec think something missing trying word vec result bad think something wrong code first training run fast expected long time training epoch look similar word result seems completely random nothing related something missing thanks
Loss function for comparing two vectors for categorization,"<p>I am performing a NLP task where I analyze a document and classify it into one of six categories. However, I do this operation at three different time periods. So the final output is an array of three integers (sparse), where each integer is the category 0-5. So a label looks like this: <code>[1, 4, 5]</code>.</p>
<p>I am using BERT and am trying to decide what type of head I should attach to it, as well as what type of loss function I should use. Would it make sense to use BERT's output of size <code>1024</code> and run it through a <code>Dense</code> layer with 18 neurons, then reshape into something of size <code>(3,6)</code>?</p>
<p>Finally, I assume I would use Sparse Categorical Cross-Entropy as my loss function?</p>
",Training and Model Evaluation,loss function comparing two vector categorization performing nlp task analyze document classify one six category however operation three different time period final output array three integer sparse integer category label look like using bert trying decide type head attach well type loss function use would make sense use bert output size run layer neuron reshape something size finally assume would use sparse categorical cross entropy loss function
How to store data from Google Ngram API?,"<p>I need to store the data presented in the graphs on the Google Ngram website. For example, I want to store the occurences of ""it's"" as a percentage from 1800-2008, as presented in the following link: <a href=""https://books.google.com/ngrams/graph?content=it%27s&amp;year_start=1800&amp;year_end=2008&amp;corpus=0&amp;smoothing=3&amp;share=&amp;direct_url=t1%3B%2Cit%27s%3B%2Cc0"" rel=""nofollow"">https://books.google.com/ngrams/graph?content=it%27s&amp;year_start=1800&amp;year_end=2008&amp;corpus=0&amp;smoothing=3&amp;share=&amp;direct_url=t1%3B%2Cit%27s%3B%2Cc0</a>.</p>

<p>The data I want is the data you're able to scroll over on the graph. How can I extract this for about 140 different terms (e.g. ""it's"", ""they're"", ""she's"", etc.)?</p>
",Training and Model Evaluation,store data google ngram api need store data presented graph google ngram website example want store occurences percentage presented following link data want data able scroll graph extract different term e g etc
Minimum Number of Words for Each Sentence for Training Gensim Word2vec Model,"<p>Suppose I have a corpus of short sentences of which the number of words ranges from 1 to around 500 and the average number of words is around 9. If I train a Gensim Word2vec model using window=5(which is the default), should I use all of the sentences? or I should remove sentences with low word count? If so, is there a rule of thumb for the minimum number of words?</p>
",Training and Model Evaluation,minimum number word sentence training gensim word vec model suppose corpus short sentence number word range around average number word around train gensim word vec model using window default use sentence remove sentence low word count rule thumb minimum number word
Training a custom text classification model using spaCy,"<p>I have below csv file called <code>dataset.csv</code>:</p>

<pre><code>is_offensive,text
0,Hi there! My name is oliver
1,Shut up man!
0,What is wrong?
1,Go away idiot!
</code></pre>

<p>Which contains +5000 rows of similar annotated data.</p>

<p>The first column <code>is_offensive</code> contains the label that I want to predict, and the second column <code>text</code> is the actual text used for training.</p>

<p>After looking at the spaCy documentation, I can see that in order to train your own custom text classification model, the training data needs to look like this:</p>

<pre class=""lang-py prettyprint-override""><code>TRAINING_DATA = [
    [""Hi there! My name is oliver"", {""OFFENSIVE"": True}],
    [""Shup up man!"", {""OFFENSIVE"": True}],
    [""What is wrong?"", {""OFFENSIVE"": False}],
    [""Go away idiot!"", {""OFFENSIVE"": True}]
]
</code></pre>

<p>I have created the below method, to parse this CSV file and return it in the format spaCy expects:</p>

<pre class=""lang-py prettyprint-override""><code>def convert():
    TRAINING_DATA = defaultdict(list)
    # Open CSV file.
    with open('train/profanity/data/profanity_cleaned_data.csv', mode='r') as csv_file:
        csv_reader = csv.DictReader(csv_file)
        line_count = 1
        for row in csv_reader:
            if line_count &gt; 0 and line_count &lt; 1000: #Read the first 1000 lines.
                TRAINING_DATA['csv'].append([str(row['text']), {
                    'OFFENSIVE': bool(int(row['is_offensive']))}])
                line_count += 1

    return TRAINING_DATA['csv']
</code></pre>

<p>Now, to train the data, I simply do:</p>

<pre class=""lang-py prettyprint-override""><code>def train():
    output_dir = 'train/profanity/model/'
    TRAINING_DATA = convert()

    nlp = spacy.blank(""en"")
    category = nlp.create_pipe(""textcat"")
    category.add_label(""OFFENSIVE"")
    nlp.add_pipe(category)

    # Start the training
    nlp.begin_training()

    # Loop for 10 iterations
    for itn in range(10):
        # Shuffle the training data
        random.shuffle(TRAINING_DATA)

        # Batch the examples and iterate over them
        for batch in tqdm(spacy.util.minibatch(TRAINING_DATA, size=1)):
            texts = [nlp(text) for text, entities in batch]
            annotations = [{""cats"": entities} for text, entities in batch]
            nlp.update(texts, annotations)

    nlp.to_disk(output_dir)
    print(""Saved model to"", output_dir)
</code></pre>

<p>And then finally, to test the model on a new text:</p>

<pre class=""lang-py prettyprint-override""><code>def testModel():
    test_text = ""You are a very kind person.""
    model_dir = 'train/profanity/model/'
    nlp = spacy.load(model_dir)
    doc = nlp(test_text)
    print(test_text, doc.cats)
</code></pre>

<p>The above method <code>testModel()</code> returns:</p>

<p><code>You are a very kind person. {'OFFENSIVE': 0.9999545812606812}</code></p>

<p>Actually, no matter what text I use as an input, I get back the <code>OFFENSIVE</code> label with the confidence very close to 100. However, as you can see in the input text: <code>You are a very kind person</code> should <strong>not</strong> be classified as <code>OFFENSIVE</code></p>

<p>What am I doing wrong? </p>
",Training and Model Evaluation,training custom text classification model using spacy csv file called contains row similar annotated data first column contains label want predict second column actual text used training looking spacy documentation see order train custom text classification model training data need look like created method parse csv file return format spacy expects train data simply finally test model new text method return actually matter text use input get back label confidence close however see input text classified wrong
Can&#39;t do lazy loading with allennlp,"<p>Currently I'm trying to implement lazy loading with allennlp, but can't.
My code is as the followings.</p>
<pre class=""lang-py prettyprint-override""><code>def biencoder_training():
    params = BiEncoderExperiemntParams()
    config = params.opts
    reader = SmallJaWikiReader(config=config)

    # Loading Datasets
    train, dev, test = reader.read('train'), reader.read('dev'), reader.read('test')
    vocab = build_vocab(train)
    vocab.extend_from_instances(dev)

    # TODO: avoid memory consumption and lazy loading
    train, dev, test = list(reader.read('train')), list(reader.read('dev')), list(reader.read('test'))

    train_loader, dev_loader, test_loader = build_data_loaders(config, train, dev, test)
    train_loader.index_with(vocab)
    dev_loader.index_with(vocab)

    embedder = emb_returner()
    mention_encoder, entity_encoder = Pooler_for_mention(word_embedder=embedder), \
                                      Pooler_for_cano_and_def(word_embedder=embedder)

    model = Biencoder(mention_encoder, entity_encoder, vocab)

    trainer = build_trainer(lr=config.lr,
                            num_epochs=config.num_epochs,
                            model=model,
                            train_loader=train_loader,
                            dev_loader=dev_loader)
    trainer.train()

    return model
</code></pre>
<p>When I commented-out <code> train, dev, test = list(reader.read('train')), list(reader.read('dev')), list(reader.read('test'))</code>, iterator doesn't work and training is conducted with 0 sample.</p>
<pre><code>Building the vocabulary
100it [00:00, 442.15it/s]01, 133.57it/s]
building vocab: 100it [00:01, 95.84it/s]
100it [00:00, 413.40it/s]
100it [00:00, 138.38it/s]
You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
0it [00:00, ?it/s]
0it [00:00, ?it/s]
</code></pre>
<p>I'd like to know if there is any solution for avoid this.
Thanks.</p>
<hr />
<p><strong>Supplement, added at fifth, May.</strong></p>
<p>Currently I am trying to avoid putting all of each sample data on top of memory before training the model.</p>
<p>So I have implemented the _read method as a generator. My understanding is that by calling this method and wrapping it with SimpleDataLoader, I can actually pass the data to the model.</p>
<p>In the DatasetReader, the code for the _read method looks like this. It is my understanding that this is intended to be a generator that avoids memory consumption.</p>
<pre class=""lang-py prettyprint-override""><code>    @overrides
    def _read(self, train_dev_test_flag: str) -&gt; Iterator[Instance]:
        '''
        :param train_dev_test_flag: 'train', 'dev', 'test'
        :return: list of instances
        '''
        if train_dev_test_flag == 'train':
            dataset = self._train_loader()
            random.shuffle(dataset)
        elif train_dev_test_flag == 'dev':
            dataset = self._dev_loader()
        elif train_dev_test_flag == 'test':
            dataset = self._test_loader()
        else:
            raise NotImplementedError(
                &quot;{} is not a valid flag. Choose from train, dev and test&quot;.format(train_dev_test_flag))

        if self.config.debug:
            dataset = dataset[:self.config.debug_data_num]

        for data in tqdm(enumerate(dataset)):
            data = self._one_line_parser(data=data, train_dev_test_flag=train_dev_test_flag)
            yield self.text_to_instance(data)
</code></pre>
<p>Also, <code>build_data_loaders</code> actually looks like this.</p>
<pre class=""lang-py prettyprint-override""><code>def build_data_loaders(config,
    train_data: List[Instance],
    dev_data: List[Instance],
    test_data: List[Instance]) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:

    train_loader = SimpleDataLoader(train_data, config.batch_size_for_train, shuffle=False)
    dev_loader = SimpleDataLoader(dev_data, config.batch_size_for_eval, shuffle=False)
    test_loader = SimpleDataLoader(test_data, config.batch_size_for_eval, shuffle=False)

    return train_loader, dev_loader, test_loader
</code></pre>
<p>But, by somewhat reason I don't know, this code doesn't work.</p>
<pre><code>def biencoder_training():
    params = BiEncoderExperiemntParams()
    config = params.opts
    reader = SmallJaWikiReader(config=config)

    # Loading Datasets
    train, dev, test = reader.read('train'), reader.read('dev'), reader.read('test')
    vocab = build_vocab(train)
    vocab.extend_from_instances(dev)

    train_loader, dev_loader, test_loader = build_data_loaders(config, train, dev, test)
    train_loader.index_with(vocab)
    dev_loader.index_with(vocab)

    embedder = emb_returner()
    mention_encoder, entity_encoder = Pooler_for_mention(word_embedder=embedder), \
                                      Pooler_for_cano_and_def(word_embedder=embedder)

    model = Biencoder(mention_encoder, entity_encoder, vocab)

    trainer = build_trainer(lr=config.lr,
                            num_epochs=config.num_epochs,
                            model=model,
                            train_loader=train_loader,
                            dev_loader=dev_loader)
    trainer.train()

    return model
</code></pre>
<p>In this code, the SimpleDataLoader will wrap the generator type as it is. I would like to do the lazy loading that allennlp did in the 0.9 version.</p>
<p>But this code iterates training over 0 instances, so currently I have added</p>
<p><code>train, dev, test = list(reader.read('train')), list(reader.read('dev')), list(reader.read('test'))</code></p>
<p>before</p>
<p><code>train_loader, dev_loader, test_loader = build_data_loaders(config, train, dev, test)</code>.</p>
<p>And it works. But this means that I can't train or evaluate the model until I have all the instances in memory. Rather, I want each batch to be called into memory only when it is time to train.</p>
",Training and Model Evaluation,lazy loading allennlp currently trying implement lazy loading allennlp code following commented iterator work training conducted sample like know solution avoid thanks supplement added fifth may currently trying avoid putting sample data top memory training model implemented read method generator understanding calling method wrapping simpledataloader actually pas data model datasetreader code read method look like understanding intended generator avoids memory consumption also actually look like somewhat reason know code work code simpledataloader wrap generator type would like lazy loading allennlp version code iterates training instance currently added work mean train evaluate model instance memory rather want batch called memory time train
Error in Keras NLP model training: matrix type cannot be converted to python,"<p>I am trying to create an NLP model (and eventually a graph of semantic similarity) of some text data.</p>
<p>Here's what my data looks like:</p>
<pre><code>topic_model # a tibble of 2 &lt;chr&gt; columns
</code></pre>
<p><a href=""https://i.sstatic.net/PnCJy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PnCJy.png"" alt=""enter image description here"" /></a></p>
<p>And here's where I try to train the model and run into an error:</p>
<pre><code>model %&gt;%
  fit_generator(
    skipgrams_generator(topic_model, tokenizer, skip_window, negative_samples), 
    steps_per_epoch = 100000, epochs = 5
    )
</code></pre>
<p>This code results in the following error:</p>
<pre><code>&quot;Error occurred in generator: Matrix type cannot be converted to python (only integer, numeric, complex, logical, and character matrixes can be converted

Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: generator raised StopIteration&quot;
</code></pre>
<p>What's the problem here? Do I need to convert the tibble to a matrix of characters?</p>
<p>See my full notebook here: <a href=""https://www.kaggle.com/vintners/modal-backgrounds-nlp"" rel=""nofollow noreferrer"">https://www.kaggle.com/vintners/modal-backgrounds-nlp</a></p>
",Training and Model Evaluation,error kera nlp model training matrix type converted python trying create nlp model eventually graph semantic similarity text data data look like try train model run error code result following error problem need convert tibble matrix character see full notebook
"Invalid argument: indices[74,7] = 3298 is not in [0, 1827) on the second Epoch when call model.fit in TensorFlow","<p>I am trying to create NLP using TensorFlow 2</p>
<p>This is my code</p>
<pre><code>def unicode_to_ascii(s):
    return ''.join(c for c in unicodedata.normalize('NFD', s)
                   if unicodedata.category(c) != 'Mn')


def preprocess_eng(w):
    w = unicode_to_ascii(w.lower().strip())

    w = re.sub(r&quot;([?.!,])&quot;, r&quot; \1 &quot;, w)
    w = re.sub(r'[&quot; &quot;]+', &quot; &quot;, w)

    w = re.sub(r&quot;[^a-zA-Z?.!,]+&quot;, &quot; &quot;, w)
    w = w.rstrip().strip()

    return w

def preprocess_chinese(w):
    w = unicode_to_ascii(w.lower().strip())
    w = re.sub(r'[&quot; &quot;]+', &quot;&quot;, w)
    w = w.rstrip().strip()
    w = &quot; &quot;.join(list(w))  # add the space between words
    w = '&lt;start&gt; ' + w + ' &lt;end&gt;'
    return w

en_words, zh_words = create_dataset(path_to_data, max_examples=max_examples)
en_train, en_test, zh_train, zh_test = train_test_split(en_words, zh_words, test_size=0.1)

def max_length(data):
  max_length = max([len(x.split(' ')) for x in data])
  return max_length

# Training Data
max_length_eng_train = max_length(en_train)
max_length_zh_train = max_length(zh_train)

# Test Data
max_length_eng_test = max_length(en_test)
max_length_zh_test = max_length(zh_test)

en_tokenizer = Tokenizer()
en_tokenizer.fit_on_texts(en_train)

en_word_to_index = en_tokenizer.word_index
vocab_size_source = len(en_word_to_index) + 1

en_train = en_tokenizer.texts_to_sequences(en_train)
en_train = pad_sequences(en_train, maxlen=max_length_eng_train, padding='post')

en_test = en_tokenizer.texts_to_sequences(en_test)
en_test = pad_sequences(en_test, maxlen=max_length_eng_train, padding='post')


zh_tokenizer = Tokenizer()
zh_tokenizer.fit_on_texts(zh_train)

zh_word_to_index = zh_tokenizer.word_index
vocab_size_target = len(zh_word_to_index) + 1

zh_train = zh_tokenizer.texts_to_sequences(zh_train)
zh_train = pad_sequences(zh_train, maxlen=max_length_zh_train, padding='post')

zh_test = en_tokenizer.texts_to_sequences(zh_test)
zh_test = pad_sequences(zh_test, maxlen=max_length_zh_train, padding='post')

en_train = np.array(en_train)
zh_train = np.array(zh_train)
en_test = np.array(en_test)
zh_test = np.array(zh_test)
</code></pre>
<p>This is why i got from</p>
<pre><code>vocab_size_source, vocab_size_target
(4195, 1827)
</code></pre>
<p>Then i tried to create layers and model</p>
<pre><code>from attention import AttentionLayer
from keras import backend as K

K.clear_session() 
latent_dim = 256

# Encoder 
encoder_inputs = Input(shape=(max_length_eng_train,)) 
enc_emb = Embedding(vocab_size_source, latent_dim,trainable=True)(encoder_inputs)

#LSTM 1 
encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) 
encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)

#LSTM 2 
encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) 
encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)

#LSTM 3 
encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) 
encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)

# Set up the decoder. 
decoder_inputs = Input(shape=(None,)) 
dec_emb_layer = Embedding(vocab_size_target, latent_dim,trainable=True) 
dec_emb = dec_emb_layer(decoder_inputs)

#LSTM using encoder_states as initial state
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) 
decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])

#Attention Layer
attn_layer = AttentionLayer(name='attention_layer') 
attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])

# Concat attention output and decoder LSTM output 
decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])

#Dense layer
decoder_dense = TimeDistributed(Dense(vocab_size_target, activation='softmax')) 
decoder_outputs = decoder_dense(decoder_concat_input)

# Define the model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs) 

model.compile(optimizer='rmsprop',
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy'])
</code></pre>
<p>Then i train my model with these function</p>
<pre><code>early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1)

EPOCH_NUMBER = 50
BATCH_SIZE = 128

history = model.fit([en_train, zh_train[:,:-1]], 
zh_train.reshape(zh_train.shape[0], zh_train.shape[1],1)[:,1:], 
                epochs=EPOCH_NUMBER, 
                callbacks=[early_stopping],
                batch_size=BATCH_SIZE,
                validation_data = ([en_test, zh_test[:,:-1]],           
                                    zh_test.reshape(zh_test.shape[0], zh_test.shape[1], 1)[:,1:]))
</code></pre>
<p>But after complete the first Epoch i got this error</p>
<pre><code>Invalid argument:  indices[74,7] = 3298 is not in [0, 1827)
</code></pre>
<p><a href=""https://i.sstatic.net/WaWmV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WaWmV.png"" alt=""enter image description here"" /></a></p>
<p>How can i fix this, which part that i got wrong?</p>
<p>Thanks!</p>
",Training and Model Evaluation,invalid argument index second epoch call model fit tensorflow trying create nlp using tensorflow code got tried create layer model train model function complete first epoch got error fix part got wrong thanks
How to force a certain tag in spaCy?,"<p>I'm using spaCy <code>'3.0.0rc2'</code> with a custom model. Unfortunately my training data is low in hyphens (-), therefore the hyphen often gets tagged as <code>NOUN</code>.</p>
<p>Is there some way to force a certain <code>tag</code> <em><strong>or</strong></em> <code>pos</code>, to make sure that <em><strong>all</strong></em> the  <code>-</code> tokens get tagged with <code>PUNCT</code>?</p>
<p>Basically I am looking for a solution like proposed in the answer to this question here:
<a href=""https://stackoverflow.com/questions/51766157/how-to-force-a-pos-tag-in-spacy-before-after-tagger/51776803"">How to force a pos tag in spacy before/after tagger?</a></p>
<p>Unfortunately this does not seem to work <em>anymore</em> (at least for spaCy 3) and raises an error:</p>
<pre><code>ValueError: [E1005] Unable to set attribute 'POS' in tokenizer exception for '{G}'. Tokenizer exceptions are only allowed to specify ORTH and NORM.
</code></pre>
<p><em>(Same when trying to assign the <code>TAG</code> attribute)</em></p>
<p>I know that it would be possible to create a custom component with a <code>Matcher</code> that looks just for the hyphen and assigns the right tag. However this seems to be overkill when considering that I currently just want to handle one token.</p>
<p>Is there some way to force tags in spaCy 3, <em>without re-tagging during processing using a custom component</em>?</p>
<p>Ideally I would want to modify the <code>TAG</code> attribute and let the <code>POS</code> attribute get assigned automatically by spaCy based on that <code>TAG</code> attribute.
As in the <a href=""https://spacy.io/api/annotation#pos-en"" rel=""nofollow noreferrer"">spacy-annotations</a> <code>TAG=HYPH</code> should be mapped to <code>POS=PUNCT</code>.</p>
",Training and Model Evaluation,force certain tag spacy using spacy custom model unfortunately training data low hyphen therefore hyphen often get tagged way force certain make sure token get tagged basically looking solution like proposed answer question spacy annotation mapped
Fine Tuning Bert on Medical Dataset,"<p>I would like to use a language model such as Bert to get a feature vector for a certain text describing a medical condition.</p>
<p>As there are many words in the text unknown to most pre-trained models and tokenizers, I wonder which steps are required to achieve this task?</p>
<p>Using a pre-trained model seems beneficial to me since the dataset describing the medical conditions is quite small.</p>
",Training and Model Evaluation,fine tuning bert medical dataset would like use language model bert get feature vector certain text describing medical condition many word text unknown pre trained model tokenizers wonder step required achieve task using pre trained model seems beneficial since dataset describing medical condition quite small
How to classify same entity based on the context of sentence?,"<p>I am working on a conversational AI that used to understand customer requests and make necessary responses. I have used BERT to train my dataset for NLU tasks intent classification and entity detection / slot filling. Model is having accuracy of 96 percent, but after i noticed that i've also a use case that model should classify similar entity and use it for different purposes.</p>
<p>For example when text like <strong>&quot;Change the name from Kevin to John&quot;, model should able to identify or classify Kevin as the old name and John as the new name entity</strong>.</p>
<p>I tried to label different BIO tags for new names and old names on similar text dataset and trained, but model some time classify old name as new name and vice versa.</p>
<p>With regex also, I'm not seeing any rules that classify old name and new name.</p>
<p>So the requirement is model should able to understand the context of the text and classify which one is new name and old name.</p>
<p>Is there any method in NLP or anything to solve this use case?</p>
",Training and Model Evaluation,classify entity based context sentence working conversational ai used understand customer request make necessary response used bert train dataset nlu task intent classification entity detection slot filling model accuracy percent noticed also use case model classify similar entity use different purpose example text like change name kevin john model able identify classify kevin old name john new name entity tried label different bio tag new name old name similar text dataset trained model time classify old name new name vice versa regex also seeing rule classify old name new name requirement model able understand context text classify one new name old name method nlp anything solve use case
How to represent ELMo embeddings as a 1D array?,"<p>I am using the language model ELMo - <a href=""https://allennlp.org/elmo"" rel=""nofollow noreferrer"">https://allennlp.org/elmo</a> to represent my text data as a numerical vector. This vector will be used as training data for a simple sentiment analysis task.</p>

<p>In this case the data is not in english, so I downloaded a custom ELMo model from - <a href=""https://github.com/HIT-SCIR/ELMoForManyLangs"" rel=""nofollow noreferrer"">https://github.com/HIT-SCIR/ELMoForManyLangs</a> (i assume this behavs similar as the offical allennlp repo)</p>

<p>To convert a text document to an ELMo embedding the function <code>sents2elmo</code> is used. The argument is a list of tokenized sentences if I understood the documentation correct. </p>

<p>So one sample in my training data could be embedded as following:</p>

<pre><code>from elmoformanylangs import Embedder
embedder = Embedder('custom_language') 
embeddings = embedder.sents2elmo([['hello', 'world', 'how', 'are', 'you', '?'], 
                                  ['am', 'great', 'thanks', '!']])
</code></pre>

<p>This will return a list of two numpy arrays, one for each sentence, and each token in the sentence will be represented as one vector of size 1024. And since the default parameter of <code>sents2elmo(output_layer)</code> is -1, this vector represents the average of the 3 internal layers in the language model.</p>

<p>How can the embeddings be represented as a 1D array? Shall I just average all the word vectors for one sentence. And then average all the sentence vectors? </p>

<pre><code>sentence_1 = np.mean(embeddings[0], axis=0)
sentence_2 = np.mean(embeddings[1], axis=0)
document = np.mean([sentence_1, sentence_2], axis=0)
</code></pre>

<p>Does this approach destroy any information? If so, are there other ways of doing this?</p>

<p>Thanks!</p>
",Training and Model Evaluation,represent elmo embeddings array using language model elmo represent text data numerical vector vector used training data simple sentiment analysis task case data english downloaded custom elmo model assume behavs similar offical allennlp repo convert text document elmo embedding function used argument list tokenized sentence understood documentation correct one sample training data could embedded following return list two numpy array one sentence token sentence represented one vector size since default parameter vector represents average internal layer language model embeddings represented array shall average word vector one sentence average sentence vector doe approach destroy information way thanks
Training speed on GPU become slower overtime,"<p>My model training speed <a href=""https://i.sstatic.net/jfZzu.jpg"" rel=""nofollow noreferrer"">becomes slower over time</a>. Every epoch take longer time to train.</p>
<p>Here is the <a href=""https://drive.google.com/file/d/0B10N16RArpisQ1hPVEdHRXF2UGM/view?usp=sharing"" rel=""nofollow noreferrer"">full source code</a> with my preprocess sentiment <a href=""http://nlp.stanford.edu/data/glove.840B.300d.zip"" rel=""nofollow noreferrer"">tree bank data</a> (put <code>glove.840B.300d.txt</code> into <code>data/glove</code>).</p>
<p>Install some python packages:</p>
<pre><code>pip install meowlogtool
pip install tqdm
</code></pre>
<p>Command to run:</p>
<pre><code>python sentiment.py --emblr 0 --rel_dim 0 --tag_dim 0 --optim adagrad --name basic --lr 0.05 --wd 1e-4 --at_hid_dim 0
</code></pre>
<p>Model source code for you to read</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable as Var
import utils
import Constants
from model import SentimentModule
from embedding_model import EmbeddingModel

class SimpleGRU(nn.Module):
    &quot;&quot;&quot;
    w[i] : (300, 1)
    h[i] : (150, 1)
    p[i] : (20, 1)
    r[i] : (20, 1)
    k[i] : (150, 1)
    x[i] : (20 + 150 + 300 + 20 = 490, 1) (490, 1)
    Uz, Ur, Uh : (150, 150) =&gt; 67500 =&gt; (450, 450)
    Wz, Wr, Wh : (150, 20 + 150 + 300 + 20) (150, 490)
    &quot;&quot;&quot;
    def __init__(self, cuda, in_dim, hid_dim):
        super(SimpleGRU, self).__init__()
        self.cudaFlag = cuda

        self.Uz = nn.Linear(hid_dim, hid_dim)
        self.Ur = nn.Linear(hid_dim, hid_dim)
        self.Uh = nn.Linear(hid_dim, hid_dim)

        self.Wz = nn.Linear(in_dim, hid_dim)
        self.Wr = nn.Linear(in_dim, hid_dim)
        self.Wh = nn.Linear(in_dim, hid_dim)

        if self.cudaFlag:
            self.Uz = self.Uz.cuda()
            self.Ur = self.Uz.cuda()
            self.Uh = self.Uz.cuda()

            self.Wz = self.Wz.cuda()
            self.Wr = self.Wr.cuda()
            self.Wh = self.Wh.cuda()

    def forward(self, x, h_prev):
        &quot;&quot;&quot;
    Simple-GRU(compress_x[v], h[t-1]) :
    z[t]         := s(Wz *compress_x[t]+ Uz * h[t-1] + bz)
    r[t]         := s(Wr * compress_x[t] + Ur * h[t-1] + br)
    h_temp[t]     := g(Wh * compress_x[t] + Uh * h[t-1] + bh)
    h[t]         := r[t] .* h[t-1] + (1 - z[t]) .* h_temp[t]
    return h[t]
        :param x: compress_x[t]
        :param h_prev: h[t-1]
        :return:
        &quot;&quot;&quot;
        z = F.sigmoid(self.Wz(x) + self.Uz(h_prev))
        r = F.sigmoid(self.Wr(x) + self.Ur(h_prev))
        h_temp = F.tanh(self.Wh(x) + self.Uh(h_prev))
        h = r*h_prev + (1-z)*h_temp
        return h



class TreeSimpleGRU(nn.Module):
    def __init__(self, cuda, word_dim, tag_dim, rel_dim, mem_dim, at_hid_dim, criterion, leaf_h = None):
        super(TreeSimpleGRU, self).__init__()
        self.cudaFlag = cuda
        # self.gru_cell = nn.GRUCell(word_dim + tag_dim, mem_dim)
        self.gru_cell = SimpleGRU(self.cudaFlag, word_dim+tag_dim, mem_dim)
        self.gru_at = GRU_AT(self.cudaFlag, word_dim + tag_dim + rel_dim + mem_dim, at_hid_dim ,mem_dim)
        self.mem_dim = mem_dim
        self.in_dim = word_dim
        self.tag_dim = tag_dim
        self.rel_dim = rel_dim
        self.leaf_h = leaf_h # init h for leaf node
        if self.leaf_h == None:
            self.leaf_h = Var(torch.rand(1, self.mem_dim))
            torch.save(self.leaf_h, 'leaf_h.pth')

        if self.cudaFlag:
            self.leaf_h = self.leaf_h.cuda()

        self.criterion = criterion
        self.output_module = None


    def getParameters(self):
        &quot;&quot;&quot;
        Get flatParameters
        note that getParameters and parameters is not equal in this case
        getParameters do not get parameters of output module
        :return: 1d tensor
        &quot;&quot;&quot;
        params = []
        for m in [self.gru_cell, self.gru_at]:
            # we do not get param of output module
            l = list(m.parameters())
            params.extend(l)

        one_dim = [p.view(p.numel()) for p in params]
        params = F.torch.cat(one_dim)
        return params


    def set_output_module(self, output_module):
        self.output_module = output_module

    def forward(self, tree, w_emb, tag_emb, rel_emb, training = False):
        loss = Var(torch.zeros(1))  # init zero loss
        if self.cudaFlag:
            loss = loss.cuda()

        for idx in xrange(tree.num_children):
            _, child_loss = self.forward(tree.children[idx], w_emb, tag_emb, rel_emb, training)
            loss = loss + child_loss

        if tree.num_children &gt; 0:
            child_rels, child_k  = self.get_child_state(tree, rel_emb)
            if self.tag_dim &gt; 0:
                tree.state = self.node_forward(w_emb[tree.idx - 1], tag_emb[tree.idx -1], child_rels, child_k)
            else:
                tree.state = self.node_forward(w_emb[tree.idx - 1], None, child_rels, child_k)
        elif tree.num_children == 0:
            if self.tag_dim &gt; 0:
                tree.state = self.leaf_forward(w_emb[tree.idx - 1], tag_emb[tree.idx -1])
            else:
                tree.state = self.leaf_forward(w_emb[tree.idx - 1], None)

        if self.output_module != None:
            output = self.output_module.forward(tree.state, training)
            tree.output = output
            if training and tree.gold_label != None:
                target = Var(utils.map_label_to_target_sentiment(tree.gold_label))
                if self.cudaFlag:
                    target = target.cuda()
                loss = loss + self.criterion(output, target)
        return tree.state, loss


    def leaf_forward(self, word_emb, tag_emb):
        &quot;&quot;&quot;
        Forward function for leaf node
        :param word_emb:  word embedding of current node u
        :param tag_emb: tag embedding of current node u
        :return: k of current node u
        &quot;&quot;&quot;
        h = self.leaf_h
        if self.cudaFlag:
            h = h.cuda()
        if self.tag_dim &gt; 0:
            x = F.torch.cat([word_emb, tag_emb], 1)
        else:
            x = word_emb
        k = self.gru_cell(x, h)
        return k


    def node_forward(self, word_emb, tag_emb, child_rels, child_k):
        &quot;&quot;&quot;
        Foward function for inner node
        :param word_emb: word embedding of current node u
        :param tag_emb: tag embedding of current node u
        :param child_rels (tensor): rels embedding of child node v
        :param child_k (tensor): k of child node v
        :return:
        &quot;&quot;&quot;
        n_child = child_k.size(0)
        h = Var(torch.zeros(1, self.mem_dim))
        if self.cudaFlag:
            h = h.cuda()

        for i in range(0, n_child):
            k = child_k[i]
            x_list = [word_emb, k]
            if self.rel_dim &gt;0:
                rel = child_rels[i]
                x_list.append(rel)
            if self.tag_dim &gt; 0:
                x_list.append(tag_emb)
            x = F.torch.cat(x_list, 1)
            h = self.gru_at(x, h)
        k = h
        return k

    def get_child_state(self, tree, rels_emb):
        &quot;&quot;&quot;
        Get child rels, get child k
        :param tree: tree we need to get child
        :param rels_emb (tensor):
        :return:
        &quot;&quot;&quot;
        if tree.num_children == 0:
            assert False #  never get here
        else:
            child_k = Var(torch.Tensor(tree.num_children, 1, self.mem_dim))
            if self.rel_dim&gt;0:
                child_rels = Var(torch.Tensor(tree.num_children, 1, self.rel_dim))
            else:
                child_rels = None
            if self.cudaFlag:
                child_k = child_k.cuda()
                if self.rel_dim &gt; 0:
                    child_rels = child_rels.cuda()
            for idx in xrange(tree.num_children):
                child_k[idx] = tree.children[idx].state
                if self.rel_dim &gt; 0:
                    child_rels[idx] = rels_emb[tree.children[idx].idx - 1]
        return child_rels, child_k

class AT(nn.Module):
    &quot;&quot;&quot;
    AT(compress_x[v]) := sigmoid(Wa * tanh(Wb * compress_x[v] + bb) + ba)
    &quot;&quot;&quot;
    def __init__(self, cuda, in_dim, hid_dim):
        super(AT, self).__init__()
        self.cudaFlag = cuda
        self.in_dim = in_dim
        self.hid_dim = hid_dim

        self.Wa = nn.Linear(hid_dim, 1)
        self.Wb = nn.Linear(in_dim, hid_dim)

        if self.cudaFlag:
            self.Wa = self.Wa.cuda()
            self.Wb = self.Wb.cuda()

    def forward(self, x):
        out = F.sigmoid(self.Wa(F.tanh(self.Wb(x))))
        return out


class GRU_AT(nn.Module):
    def __init__(self, cuda, in_dim, at_hid_dim ,mem_dim):
        super(GRU_AT, self).__init__()
        self.cudaFlag = cuda
        self.in_dim = in_dim
        self.mem_dim = mem_dim
        self.at_hid_dim = at_hid_dim
        if at_hid_dim &gt; 0:
            self.at = AT(cuda, in_dim, at_hid_dim)
        self.gru_cell = SimpleGRU(self.cudaFlag, in_dim, mem_dim)

        if self.cudaFlag:
            if at_hid_dim &gt; 0:
                self.at = self.at.cuda()
            self.gru_cell = self.gru_cell.cuda()

    def forward(self, x, h_prev):
        &quot;&quot;&quot;

        :param x:
        :param h_prev:
        :return: a * m + (1 - a) * h[t-1]
        &quot;&quot;&quot;
        m = self.gru_cell(x, h_prev)
        if self.at_hid_dim &gt; 0:
            a = self.at.forward(x)
            h = torch.mm(a, m) + torch.mm((1-a), h_prev)
        else:
            h = m
        return h

class TreeGRUSentiment(nn.Module):
    def __init__(self, cuda, in_dim, tag_dim, rel_dim, mem_dim, at_hid_dim, num_classes, criterion):
        super(TreeGRUSentiment, self).__init__()
        self.cudaFlag = cuda
        self.tree_module = TreeSimpleGRU(cuda, in_dim, tag_dim, rel_dim, mem_dim, at_hid_dim, criterion)
        self.output_module = SentimentModule(cuda, mem_dim, num_classes, dropout=True)
        self.tree_module.set_output_module(self.output_module)

    def get_tree_parameters(self):
        return self.tree_module.getParameters()

    def forward(self, tree, sent_emb, tag_emb, rel_emb, training = False):
        # sent_emb = F.torch.unsqueeze(self.word_embedding.forward(sent_inputs), 1)
        # tag_emb = F.torch.unsqueeze(self.tag_emb.forward(tag_inputs), 1)
        # rel_emb = F.torch.unsqueeze(self.rel_emb.forward(rel_inputs), 1)
        # sent_emb, tag_emb, rel_emb = self.embedding_model(sent_inputs, tag_inputs, rel_inputs)

        tree_state, loss = self.tree_module(tree, sent_emb, tag_emb, rel_emb, training)
        output = tree.output
        return output, loss
</code></pre>
",Training and Model Evaluation,training speed gpu become slower overtime model training speed becomes slower time every epoch take longer time train full source code preprocess sentiment tree bank data put install python package command run model source code read
"how to train model in which labels is [5,30]?","<p>How to train on a dataset which has each label of shape <strong>[5,30]</strong>. For example :</p>
<pre><code>[
       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 54, 55, 21, 56, 57,  3,
        22, 19, 58,  6, 59,  4, 60,  1, 61, 62, 23, 63, 23, 64],
       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  1, 65,  7, 66,  2, 67, 68,  3, 69, 70],
       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0, 11, 12,  5, 13, 14,  9, 10,  5, 15, 16, 17,  2,  8],
       [ 0,  0,  0,  0,  0,  2, 71,  1, 72, 73, 74,  7, 75, 76, 77,  3,
        20, 78, 18, 79,  1, 21, 80, 81,  3, 82, 83, 84,  6, 85],
       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,
        86, 87,  3, 88, 89,  1, 90, 91, 22, 92, 93,  4,  6, 94]
]
</code></pre>
<p>One way was to reshape the labels to [150], but that will make the <strong>tokenized sentences</strong> lose their meanings. Please suggest me how to arrange the <strong>keras layers</strong> and which layers to be able to make the model? I want to able to generate sentences later.</p>
<p>My code for model right now is this.</p>
<pre><code>model = tf.keras.Sequential([ feature_layer, 
layers.Dense(128, activation='relu'), 
layers.Dense(128, activation='relu'), 
layers.Dropout(.1), 
layers.Dense(5), 
layers.Dense(30, activation='softmax'), ]) 
opt = Adam(learning_rate=0.01) 
model.compile(optimizer=opt, loss='mean_absolute_percentage_error', metrics=['accuracy']) 
</code></pre>
<p>The actual data.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>state</th>
<th>district</th>
<th>month</th>
<th>rainfall</th>
<th>max_temp</th>
<th>min_temp</th>
<th>max_rh</th>
<th>min_rh</th>
<th>wind_speed</th>
<th>advice</th>
</tr>
</thead>
<tbody>
<tr>
<td>Orissa</td>
<td>Kendrapada</td>
<td>february</td>
<td>0.0</td>
<td>34.6</td>
<td>19.4</td>
<td>88.2</td>
<td>29.6</td>
<td>12.0</td>
<td>chances of foot rot disease in paddy crop; apply  urea  at 3 weeks after transplanting at active tillering stage for paddy;......</td>
</tr>
<tr>
<td>Jharkhand</td>
<td>Saraikela Kharsawan</td>
<td>february</td>
<td>0</td>
<td>35.2</td>
<td>16.6</td>
<td>29.4</td>
<td>11.2</td>
<td>3.6</td>
<td>provide straw mulch and go for intercultural operations to avoid moisture losses from soil; chance of leaf blight disease in potato crop; .......</td>
</tr>
</tbody>
</table>
</div>
<p>I need to be able to generate the advices.</p>
",Training and Model Evaluation,train model label train dataset ha label shape example one way wa reshape label make tokenized sentence lose meaning please suggest arrange kera layer layer able make model want able generate sentence later code model right actual data state district month rainfall max temp min temp max rh min rh wind speed advice orissa kendrapada february chance foot rot disease paddy crop apply urea week transplanting active tillering stage paddy jharkhand saraikela kharsawan february provide straw mulch go intercultural operation avoid moisture loss soil chance leaf blight disease potato crop need able generate advice
How to Find Top N Similar Words in a Dictionary of Words / Things?,"<p>I have a list of <code>str</code> that I want to map against. The words could be &quot;metal&quot; or &quot;st. patrick&quot;. The goal is to map a new string against this list and find Top N Similar items. For example, if I pass through &quot;St. Patrick&quot;, I want to capture &quot;st patrick&quot; or &quot;saint patrick&quot;.</p>
<p>I know there's gensim and fastText, and I have an intuition that I should go for cosine similarity (or I'm all ears if there's other suggestions). I work primarily with time series, and gensim model training doesn't seem to like a list of words.</p>
<p>What should I aim for next?</p>
",Training and Model Evaluation,find top n similar word dictionary word thing list want map word could metal st patrick goal map new string list find top n similar item example pas st patrick want capture st patrick saint patrick know gensim fasttext intuition go cosine similarity ear suggestion work primarily time series gensim model training seem like list word aim next
AttributeError: &#39;str&#39; object has no attribute &#39;to&#39; while training QuestionAnsweringModel from simpletransformers,"<p>I'm trying to train QuestionAnsweringModel bert-base-multilingual-uncased from simpletransformers and faced next problem:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-10-40e9356ccee6&gt; in &lt;module&gt;()
----&gt; 1 model.train(traindata, output_dir='/content/drive/MyDrive')

1 frames
/usr/local/lib/python3.7/dist-packages/simpletransformers/question_answering/question_answering_model.py in train(self, train_dataset, output_dir, show_running_loss, eval_data, verbose, **kwargs)
    578                     steps_trained_in_current_epoch -= 1
    579                     continue
--&gt; 580                 batch = tuple(t.to(device) for t in batch)
    581 
    582                 inputs = self._get_inputs_dict(batch)

/usr/local/lib/python3.7/dist-packages/simpletransformers/question_answering/question_answering_model.py in &lt;genexpr&gt;(.0)
    578                     steps_trained_in_current_epoch -= 1
    579                     continue
--&gt; 580                 batch = tuple(t.to(device) for t in batch)
    581 
    582                 inputs = self._get_inputs_dict(batch)

AttributeError: 'str' object has no attribute 'to'
</code></pre>
<p>My data prepairing:</p>
<pre><code>!wget https://onti2020.ai-academy.ru/task/rucos_test.jsonl
!wget https://onti2020.ai-academy.ru/task/rucos_val.jsonl
!wget https://onti2020.ai-academy.ru/task/rucos_train.jsonl.zip
!unzip rucos_train.jsonl.zip
</code></pre>
<pre><code>!pip install nltk
import nltk
nltk.download('all')
from nltk.tokenize import word_tokenize
</code></pre>
<pre><code>def get_train_data(jsonfile):
    res=[]
    with open(jsonfile, 'r') as data:
        trainlist=list(data)
        for item in tqdm(trainlist):
            item=json.loads(item)
            dictt={}
            dictt['context']=word_tokenize(item['passage']['text'])
            qas=[]
            qlist=item['qas']
            for q in qlist:
                qdict={}
                qdict['id']=str(q['idx']).rjust(6, '0')
                answers=[]
                qdict['is_impossible']=True
                qdict['question']=q['query']
                alist=q['answers']
                for a in alist:
                    adict={}
                    adict['text']=a['text']
                    adict['answer_start']=a['start']
                    answers.append(adict)
                qdict['answers']=answers
                qas.append(qdict)
            dictt['qas']=qas
            res.append(dictt)
    return res
</code></pre>
<pre><code>traindata, evaldata=get_train_data('rucos_train.jsonl'), get_train_data('rucos_val.jsonl')
</code></pre>
<p>Model building:</p>
<pre><code>!pip install simpletransformers
!pip install torch==1.5.0
</code></pre>
<pre><code>from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs
model = QuestionAnsweringModel(
    &quot;bert&quot;,
    &quot;bert-base-multilingual-uncased&quot;,
    args=QuestionAnsweringArgs(n_best_size=2)
)
</code></pre>
<p>Model training:</p>
<pre><code>model.train(traindata, output_dir='/content/drive/MyDrive')
</code></pre>
<p>This code was executing in Colab Pro and based on documentation <a href=""https://simpletransformers.ai/docs/qa-model/"" rel=""nofollow noreferrer"">https://simpletransformers.ai/docs/qa-model/</a>.</p>
<p>Please help me to solve this problem.</p>
",Training and Model Evaluation,attributeerror str object ha attribute training questionansweringmodel simpletransformers trying train questionansweringmodel bert base multilingual uncased simpletransformers faced next problem data prepairing model building model training code wa executing colab pro based documentation please help solve problem
High loss despite high accuracy?,"<p>I'm running Keras on a binary classification problem using binary cross entropy as my loss function. I can get my accuracy on my train data up to 90% but it still reports a high loss 7 or more. How can this be the case? should loss always be aiming to be close to zero?</p>
",Training and Model Evaluation,high loss despite high accuracy running kera binary classification problem using binary cross entropy loss function get accuracy train data still report high loss case loss always aiming close zero
"For Gensim 4.0, how to use a pre-trained model and further train it with my own corpus?","<p>For Gensim 3.8, I can use the following script to initialize a model with pre-train weights and then train it on my own corpus. How to do this in Gensim 4.0?</p>
<pre><code>mod = Word2Vec(size=300, min_count=5, workers=1, sg=1, seed=1)
mod.build_vocab(my_corpus)
mod.min_count = 0
pret_mod = KeyedVectors.load_word2vec_format(&quot;GoogleNews-vectors-negative300.bin&quot;, binary=True)
mod.build_vocab([list(pret_mod.vocab.keys())], update=True)
mod.intersect_word2vec_format(&quot;GoogleNews-vectors-negative300.bin&quot;, binary=True, lockf=1.0)
mod.train(my_corpus, total_examples=mod.corpus_count, epochs=mod.iter)
</code></pre>
",Training and Model Evaluation,gensim use pre trained model train corpus gensim use following script initialize model pre train weight train corpus gensim
Confusion regarding the accuracy of my model,"<p>I'm facing this issue in my college project which is a clickbait news classifier, i.e., it classifies between clickbait headlines and non-clickbait headlines. I'm using a dataset which has 16000 headlines of each type. Now, the main issue I'm facing is on training my network on 70% data, with 30% as my test set size. My validation set is 30% of the training set. But after fitting and evaluating the model on my test set, and I get this</p>
<p>After fitting:</p>
<blockquote>
<p>Epoch 10/10 131/131 [==============================] - 1s 6ms/step - loss: 0.2098 - accuracy: 0.9457 - val_loss: 0.3263 - val_accuracy: 0.9417</p>
</blockquote>
<p>After evaluating on test set:</p>
<blockquote>
<p>300/300 [==============================] - 1s 2ms/step - loss: 0.3030 - accuracy: 0.9432</p>
</blockquote>
<p>Confusion Matrix:</p>
<blockquote>
<p>array([[4638, 162], [ 383, 4417]])</p>
</blockquote>
<p>Now I'm very new to neural networks and i'm not sure if these accuracies are supposed to be this similar to each other. Is this something that I should be concerned about or am I missing something? I appreciate all the help I can get... Thanks!</p>
",Training and Model Evaluation,confusion regarding accuracy model facing issue college project clickbait news classifier e classifies clickbait headline non clickbait headline using dataset ha headline type main issue facing training network data test set size validation set training set fitting evaluating model test set get fitting epoch step loss accuracy val loss val accuracy evaluating test set step loss accuracy confusion matrix array new neural network sure accuracy supposed similar something concerned missing something appreciate help get thanks
val_accuracy does not increase,"<p>Currently I'm trying to train a Keras Sequential Network with pooled output from BERT. The fine tuned BertForSequence Classification yields good results, but using the pooled_output in a Neural Network does not work as intented. As Input data I got 10.000 Values, each consisting of the 768 floats that my BERT-Model provides. I'm trying to do a simple binary classification, so I also got the labels with 1 and 0's.</p>
<p><a href=""https://i.sstatic.net/tzdqz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tzdqz.png"" alt=""enter image description here"" /></a></p>
<p>As you can see my data has a good number of examples for both classes. After shuffling them, I do a normal train test split and create/fit my model with:</p>
<pre><code>model = Sequential()
model.add(Dense(1536, input_shape=(768,), activation='relu'))
model.add(Dense(1536, activation='relu'))
model.add(Dense(1536, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

opt = Adam(learning_rate=0.0001)
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])

#Normally with early stopping so quite a few epochs
history = model.fit(train_features, train_labels, epochs=800, batch_size=68, verbose=1, 
validation_split=0.2, callbacks=[])
</code></pre>
<p>During training the loss decreases and my accuracy increases as expected. BUT the val_loss increases and the val_accuracy stays the same! Sure I'm overfitting, but I would expect that the val_accuracy increases, at least for a few epochs and then decreaes when I'm overfitting.</p>
<p><a href=""https://i.sstatic.net/tP9Ch.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tP9Ch.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/8ASuZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8ASuZ.png"" alt=""enter image description here"" /></a></p>
<p>Has anyone an Idea what I'm doing wrong? Perhaps 10.000 values aren't enough to generalize?</p>
",Training and Model Evaluation,val accuracy doe increase currently trying train kera sequential network pooled output bert fine tuned bertforsequence classification yield good result using pooled output neural network doe work intented input data got value consisting float bert model provides trying simple binary classification also got label see data ha good number example class shuffling normal train test split create fit model training loss decrease accuracy increase expected val loss increase val accuracy stay sure overfitting would expect val accuracy increase least epoch decreaes overfitting ha anyone idea wrong perhaps value enough generalize
PyTorch ValueError: Target size (torch.Size([64])) must be the same as input size (torch.Size([15])),"<p>I'm currently using <a href=""https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb"" rel=""noreferrer"">this repo</a> to perform NLP and learn more about CNN's using my own dataset, and I keep running into an error regarding a shape mismatch:</p>
<pre><code>ValueError: Target size (torch.Size([64])) must be the same as input size (torch.Size([15]))

     10 }
     11 for epoch in tqdm(range(params['epochs'])):
---&gt; 12     train_loss, train_acc = train(model, train_iterator, optimizer, criterion)
     13     valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)
     14     epoch_mins, epoch_secs = epoch_time(start_time, end_time)

     57         print(&quot;PredictionShapeAfter:&quot;)
     58         print(predictions.shape)
---&gt; 59         loss = criterion(predictions, batch.l)
     60 
     61         acc = binary_accuracy(predictions, batch.l)
</code></pre>
<p>Doing some digging, I found that my CNN's prediction is a different size compared to the training data truth it's being compared to:</p>
<pre><code>  Input Shape:
    torch.Size([15, 64])
    Truth Shape:
    torch.Size([64])
    embedded unsqueezed: torch.Size([15, 1, 64, 100])
    cat shape: torch.Size([15, 300])
    Prediction Shape Before Squeeze:
    torch.Size([15, 1])
    PredictionShapeAfter:
    torch.Size([15])
</code></pre>
<p>The model is making the prediction shape (the last value in this list) as the first dimension of the inputs. Is this a common problem and is there a way to rectify this issue?</p>
<p>My Model:</p>
<pre><code>class CNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, 
                 dropout, pad_idx):
        super().__init__()
                
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)
        
        self.convs = nn.ModuleList([
                                    nn.Conv2d(in_channels = 1, 
                                              out_channels = n_filters, 
                                              kernel_size = (fs, embedding_dim)) 
                                    for fs in filter_sizes
                                    ])
        
        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, text): 
        embedded = self.embedding(text)
        embedded = embedded.unsqueeze(1)
        print(f&quot;embedded unsqueezed: {embedded.shape}&quot;)
        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]          
        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]
        cat = self.dropout(torch.cat(pooled, dim = 1))   
        print(f&quot;cat shape: {cat.shape}&quot;)       
        return self.fc(cat)
</code></pre>
<p>My Training function:</p>
<pre><code>def train(model, iterator, optimizer, criterion):
    
    epoch_loss = 0
    epoch_acc = 0
    
    model.train()
    
    for batch in iterator:
        
        optimizer.zero_grad()

        print(&quot;InputShape:&quot;)
        print(batch.t.shape)
        print(&quot;Truth Shape:&quot;)
        print(batch.l.shape)

        predictions = model(batch.t)
        print(&quot;Prediction Shape Before Squeeze:&quot;)
        print(predictions.shape)

        predictions = predictions.squeeze(1)
        print(&quot;PredictionShapeAfter:&quot;)
        print(predictions.shape)
        loss = criterion(predictions, batch.l)
        
        acc = binary_accuracy(predictions, batch.l)
        
        loss.backward()
        
        optimizer.step()
        
        epoch_loss += loss.item()
        epoch_acc += acc.item()
        
    return epoch_loss / len(iterator), epoch_acc / len(iterator)
</code></pre>
<p>My full code can be found at <a href=""https://colab.research.google.com/drive/1aY7XlgUClEe5Ldeu8DjSsxuj9sXTtRw4?usp=sharing"" rel=""noreferrer"">this link.</a></p>
",Training and Model Evaluation,pytorch valueerror target size torch size must input size torch size currently using repo perform nlp learn cnn using dataset keep running error regarding shape mismatch digging found cnn prediction different size compared training data truth compared model making prediction shape last value list first dimension input common problem way rectify issue model training function full code found link
Handling extremely long timestep sequence in LSTM (NLP Multi-label classification),"<p>This is my first time asking a question on stackoverflow so sorry if I am not asking in a correct format. Let's say I am working with some extremely long timestep sequence datas (10000000), with 2701 samples and only one feature, my input array is <code>[2701,10000000,1]</code>,  and my dataset looks like</p>
<pre><code> [ 2.81143e-01  4.98219e-01 -8.08500e-03 ...  1.00000e+02  1.00000e+02
   1.00000e+02]
 [ 1.95077e-01  2.20920e-02 -1.68663e-01 ...  1.00000e+02  1.00000e+02
   1.00000e+02]
 ...
 [ 1.06033e-01  8.96650e-02 -3.20860e-01 ...  1.00000e+02  1.00000e+02
   1.00000e+02]
 [ 6.85510e-02 -3.83653e-01 -2.19265e-01 ...  1.00000e+02  1.00000e+02
   1.00000e+02]
 [ 2.51404e-01  8.02280e-02  2.84610e-01 ...  1.00000e+02  1.00000e+02
   1.00000e+02]]
</code></pre>
<p>However, from what I had read, usually LSTM network perform better in a range of (200~400) time steps, even ignoring the performance, I cannot successfully train with a single sample <code>[1,10000000,1]</code>. I believe the network is functional since I tried to limit the length of each sample to (1500), which is now <code>[2701,1500,1]</code> and it finally stop stucking at the first epoch. Here below is my code if needed:</p>
<pre><code>from keras.utils import Sequence
import numpy as np
from numpy.lib.format import open_memmap
import gc
import platform
import pandas as pd
import numpy
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers import Masking
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras.backend.tensorflow_backend import set_session

config = tf.ConfigProto()
config.gpu_options.allocator_type = 'BFC' #A &quot;Best-fit with coalescing&quot; algorithm, simplified from a version of dlmalloc.
config.gpu_options.per_process_gpu_memory_fraction = 0.9
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config)) 

stock_price=pd.read_csv(&quot;C:/Users/user/Desktop/Final-Year-Project-master/stock_classification_7Days_test.csv&quot;,sep=',', dtype={&quot;ID&quot;:&quot;string&quot;,&quot;Class&quot;:int})
print(stock_price)


print (platform.architecture()) 

y_data=[]
x_data=[]

y_data=pd.get_dummies(stock_price['Class'])


def embedded_reader(file_path):
    with open(file_path) as embedded_raw:
        for line in embedded_raw:
            for word in line.split(','):
                try:
                    val=float(word)
                    yield val
                except:
                    pass
    
    embedded_raw.close()
    gc.collect()   
        

for y in range(len(stock_price)):
    if int(stock_price.at[y,'Class']) is not None:
        i = stock_price.at[y,'ID']
        print(&quot;Company code current: &quot;,i)
        embedded_current=[]

        try:
            gen=embedded_reader(&quot;C:/Users/user/Desktop/Final-Year-Project-master/json_test/{}.jsonl&quot;.format(i))

            
            while True:
                val=next(gen)
                embedded_current.append(val)

        except:
            pass

                    
        fp=np.memmap('embedded_array.mymemmap', dtype=np.uint8,mode='w+',shape=(1,)) 
        fp=np.delete(fp,0)
        fp=np.concatenate((fp,embedded_current),axis=0)
        fp=np.pad(fp, (0,(10000000-len(embedded_current))), 'constant', constant_values=(100, 100))
        print(fp)
        x_data.append(fp)
        print(np.shape(x_data))
        del fp

        print(&quot;embedded_data current: &quot;,len(embedded_current))

        print(&quot;this is number {}&quot;.format(y))
        print(&quot;-----------------------------&quot;)
        gc.collect()
        

    gc.collect()        


                  
print(len(x_data))
print(np.shape(x_data))
print(&quot;-&quot;*20)
print(np.shape(y_data))
print(np.size(y_data))

X_train, X_test, y_train, y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=0)
print(np.shape(X_train))
print(np.shape(X_test))
X_train=np.array(X_train)
X_test=np.array(X_test)
print(np.shape(X_train))
print(np.shape(X_test))
print(X_train)
X_train = np.reshape(X_train, (X_train.shape[0],  X_train.shape[1], 1))
X_test = np.reshape(X_test, (X_test.shape[0],  X_train.shape[1], 1))
print(np.shape(X_train))
print(np.shape(X_test))
y_train=np.array(y_train)
y_test=np.array(y_test)

print(len(X_test[0]))
print(np.shape(y_train))


model=Sequential()

model.add(Masking(mask_value=100, input_shape=(10000000,1)))
model.add(LSTM(units=1, return_sequences = True, input_shape=(10000000,1)))
model.add(LSTM(units=1,return_sequences=False))
model.add(Dense(5,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])


model.summary()
model.fit(X_train,y_train,epochs=50,batch_size=4,verbose=1)
print(model.predict(X_test))
print(&quot;class label:&quot;, reverse_label(model.predict_classes(X_test)))
scores = model.evaluate(X_test, y_test)
print(&quot;\n%s: %.2f%%&quot; % (model.metrics_names[1], scores[1]*100))



model.save('my_model')
</code></pre>
<p>From some tutorials they had mentioned to reshape the array, so I tried to reshape my array into something like <code>[2701*25000, 10000000/25000, 1]</code> but then I got stuck with the problem that x_data sample and y_data sample is not the same. Also I saw ones mentioned <code>model.fit_generator</code> but seems like it is solving the problem with huge sample size, which in my case, the model is not even working with a single sample (I am new to neural network so not sure if I am understanding it correctly). Totally got no clue and would really appreciate for any help, thank you.</p>
<p>Edit: just to state my question clear: &quot;any advice on treating such long input using LSTM?&quot;</p>
",Training and Model Evaluation,handling extremely long timestep sequence lstm nlp multi label classification first time asking question stackoverflow sorry asking correct format let say working extremely long timestep sequence data sample one feature input array dataset look like however read usually lstm network perform better range time step even ignoring performance successfully train single sample believe network functional since tried limit length sample finally stop stucking first epoch code needed tutorial mentioned reshape array tried reshape array something like got stuck problem x data sample data sample also saw one mentioned seems like solving problem huge sample size case model even working single sample new neural network sure understanding correctly totally got clue would really appreciate help thank edit state question clear advice treating long input using lstm
NLTK FreqDist: get the total frequency count with the padding in a set,"<p>As a newbie in NLP, I've built a bigram frequency list by using FreqDist built-in function. Although there are frequency counts of a set of alphabets, I want to get the count with a starting padding to calculate the probability.</p>
<p>Since there's no unigram count for <code>&lt;s&gt;</code>. Is there any built-in function that I can use to count the frequency that the first letter of a set with <code>&lt;s&gt;</code>?</p>
<p>For example, I have a list of alphabet-pair with counts:<code>[(('&lt;s&gt;','n'),100),(('&lt;s&gt;','a'),100)]</code>. The total count of <code>'&lt;s&gt;'</code> would be 2.</p>
<p>My goal is to calculate the probability that equals to <code>count(('&lt;s&gt;','n'))/count('&lt;s&gt;')</code>, which equals to 1/2.</p>
<pre><code>from nltk.util import ngrams, pad_sequence
from collections import Counter

bigram_total_word = FreqDist()

for word in training_set:
    word_with_padding = pad_sequence(word, 2, pad_left = True, left_pad_symbol='&lt;s&gt;')
    bigram = ngrams(word_with_padding,n=2)
    bigram_total_word = bigram_total_word + FreqDist(list(bigram))

print(bigram_total_word.most_common(30))
[(('i', 'n'), 6692), (('e', 'r'), 5676), (('e', 's'), 4528), (('e', 'd'), 4415), (('&lt;s&gt;', 's'), 4371), (('t', 'i'), 4357), (('o', 'n'), 4346), (('r', 'e'), 4181), (('t', 'e'), 3941), (('n', 'g'), 3847), (('&lt;s&gt;', 'c'), 3729), (('e', 'n'), 3659), (('a', 't'), 3586), (('a', 'n'), 3523), (('s', 't'), 3213), (('l', 'e'), 3207), (('&lt;s&gt;', 'p'), 2995), (('a', 'l'), 2949), (('a', 'r'), 2909), (('r', 'a'), 2870), (('r', 'i'), 2781), (('n', 't'), 2751), (('o', 'r'), 2589), (('l', 'i'), 2576), (('&lt;s&gt;', 'a'), 2521), (('i', 's'), 2521), (('&lt;s&gt;', 'b'), 2387), (('d', 'e'), 2369), (('&lt;s&gt;', 'd'), 2337), (('c', 'o'), 2290)]
</code></pre>
",Training and Model Evaluation,nltk freqdist get total frequency count padding set newbie nlp built bigram frequency list using freqdist built function although frequency count set alphabet want get count starting padding calculate probability since unigram count built function use count frequency first letter set example list alphabet pair count total count would goal calculate probability equal equal
Training time of gensim word2vec,"<p>I'm training word2vec from scratch on 34 GB pre-processed MS_MARCO corpus(of 22 GB). (Preprocessed corpus is sentnecepiece tokenized and so its size is more) I'm training my word2vec model using following code : </p>

<pre><code>from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import Word2Vec

class Corpus():
    """"""Iterate over sentences from the corpus.""""""
    def __init__(self):
        self.files = [
            ""sp_cor1.txt"",
            ""sp_cor2.txt"",
            ""sp_cor3.txt"",
            ""sp_cor4.txt"",
            ""sp_cor5.txt"",
            ""sp_cor6.txt"",
            ""sp_cor7.txt"",
            ""sp_cor8.txt""
        ]

    def __iter__(self):
        for fname in self.files:
            for line in open(fname):
                words = line.split()
                yield words

sentences = Corpus()

model = Word2Vec(sentences, size=300, window=5, min_count=1, workers=8, sg=1, hs=1, negative=10)
model.save(""word2vec.model"")

</code></pre>

<p>My model is running now for about more than 30 hours now. This is doubtful since on my i5 laptop with 8 cores, I'm using all the 8 cores at 100% for every moment of time. Plus, my program seems to have read more than 100 GB of data from the disk now. I don't know if there is anything wrong here, but the main reason after my doubt on the training is because of this 100 GB of read from the disk. The whole corpus is of 34 GB, then why my code has read 100 GB of data from the disk? Does anyone know how much time should it take to train word2vec on 34 GB of text, with 8 cores of i5 CPU running all in parallel? Thank you. For more information, I'm also attaching the photo of my process from system monitor. </p>

<p><a href=""https://i.sstatic.net/QrJRM.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/QrJRM.png"" alt=""enter image description here""></a></p>

<p>I want to know why my model has read 112 GB from memory, even when my corpus is of 34 GB in total? Will my training ever get finished? Also I'm bit worried about health of my laptop, since it is running constantly at its peak capacity since last 30 hours. It is really hot now. 
Should I add any additional parameter in <code>Word2Vec</code> for quicker training without much performance loss?</p>
",Training and Model Evaluation,training time gensim word vec training word vec scratch gb pre processed marco corpus gb preprocessed corpus sentnecepiece tokenized size training word vec model using following code model running hour doubtful since laptop core using core every moment time plus program seems read gb data disk know anything wrong main reason doubt training gb read disk whole corpus gb code ha read gb data disk doe anyone know much time take train word vec gb text core cpu running parallel thank information also attaching photo process system monitor want know model ha read gb memory even corpus gb total training ever get finished also bit worried health laptop since running constantly peak capacity since last hour really hot add additional parameter quicker training without much performance loss
Python&#39;s SpaCy EntityRuler does not return me any results,"<p>I want to make SpaCy model that will recognise organisation names. Each organisation name have between 1 and 4 words, that can be titled or capitalised.
I have added more than 3500 names of the organisations like this:</p>
<pre><code>patterns = []
for organisation in organisations_list:
    patterns.append({&quot;label&quot;: &quot;ORG&quot;, &quot;pattern&quot;: organisation.strip()})
</code></pre>
<p>So now i have a list of patterns that look like this:</p>
<pre><code>for p in patterns:
   print(p)
</code></pre>
<p>result:</p>
<pre><code>{'label': 'ORG', 'pattern': 'BLS AG'}
{'label': 'ORG', 'pattern': 'Chemins de fer du Jura'}
{'label': 'ORG', 'pattern': 'Comlux'}
{'label': 'ORG', 'pattern': 'CRH Gétaz Group'}
{'label': 'ORG', 'pattern': 'DKSH Management AG'}
{'label': 'ORG', 'pattern': 'Ferdinand Steck Maschinenfabrik'}
{'label': 'ORG', 'pattern': 'Galenica'}
{'label': 'ORG', 'pattern': 'Givaudan'}
{'label': 'ORG', 'pattern': 'Heliswiss'}
{'label': 'ORG', 'pattern': 'Jet Aviation'}
{'label': 'ORG', 'pattern': 'Kolmar'}
...
...
</code></pre>
<p>So patterns object look like this:</p>
<pre><code>patterns = [{'label': 'ORG', 'pattern': 'BLS AG'}
{'label': 'ORG', 'pattern': 'Chemins de fer du Jura'}
{'label': 'ORG', 'pattern': 'Comlux'}
{'label': 'ORG', 'pattern': 'CRH Gétaz Group'}
{'label': 'ORG', 'pattern': 'DKSH Management AG'}
{'label': 'ORG', 'pattern': 'Ferdinand Steck Maschinenfabrik'}
{'label': 'ORG', 'pattern': 'Galenica'}
{'label': 'ORG', 'pattern': 'Givaudan'}
{'label': 'ORG', 'pattern': 'Heliswiss'}
{'label': 'ORG', 'pattern': 'Jet Aviation'}
{'label': 'ORG', 'pattern': 'Kolmar'}....]
</code></pre>
<p>Then I created a blank model:</p>
<pre><code>nlp = spacy.blank(&quot;en&quot;)
nlp.add_pipe('entity_ruler')
ruler.add_patterns(patterns)
</code></pre>
<p>And then, I have tested it like this:</p>
<pre><code>for full_text in list_of_texts:
    doc = nlp(full_text)
    print(doc.ents.text, doc.ents.label_)
</code></pre>
<p>And it does not recognises anything (even if Im testing it in a sentence that has exact name of the organisations). I have also tried to add <code>tagger</code> and <code>parser</code> to my blank model with <code>entity_ruler</code> but its always the same.</p>
<p>These are some of the examples of text that I have used for testing (each company name in testing texts are also in the patterns with the same capitalisations and spelling):</p>
<pre><code>t1 = &quot;I work in company called DKSH Management AG its very good company&quot;
t2 = &quot;I have stayed in Holiday Inn Express and I really liked it&quot;
t3 = &quot;Have you head for company named AKKA Technologies SE&quot;
t4 = &quot;what do you think about ERYTECH Pharma&quot;
t5 = &quot;did you get an email from ESI Group&quot;
t6 = &quot;Esso S.A.F. sent me an email last week&quot;
</code></pre>
<p>What am I doing wrong?
I have noticed that It works if I do it like this:</p>
<pre><code>ruler = EntityRuler(nlp)
ruler.add_patterns(patterns)
nlp = spacy.load(&quot;en_core_web_trf&quot;)
nlp.add_pipe('entity_ruler', before = 'tagger')
#if i do print(nlp.pipeline) i can see entity_ruler added before tager.
</code></pre>
<p>But then I do not know if it works because of my <code>entity_ruler</code> or because of the pre trained model. I have tested it on 20 example texts and it gives me the same results with entity_ruler and without it, so I cant figure it out if it works better or not.</p>
<p>What am I doing wrong?</p>
",Training and Model Evaluation,python spacy entityruler doe return result want make spacy model recognise organisation name organisation name word titled capitalised added name organisation like list pattern look like result pattern object look like created blank model tested like doe recognises anything even im testing sentence ha exact name organisation also tried add blank model always example text used testing company name testing text also pattern capitalisation spelling wrong noticed work like know work pre trained model tested example text give result entity ruler without cant figure work better wrong
Extracting word embeddings for a xlnet calssification model in simple transformers?,"<p>I am trying to implement a xlnet transformer model using the Simple Transformers library. I am following this particular tutorial - <a href=""https://simpletransformers.ai/docs/multi-class-classification/"" rel=""nofollow noreferrer"">https://simpletransformers.ai/docs/multi-class-classification/</a></p>
<p>According to this, I can train the model on the train_df and then produce results like accuracy, f1 score, etc. but is there a way to extract the word embedding produced by this model when trained on the training data? I would be interested in analyzing the plotting those embeddings for academic purposes but I am unable to figure out a way to do so in the Simple Transformers library.</p>
",Training and Model Evaluation,extracting word embeddings xlnet calssification model simple transformer trying implement xlnet transformer model using simple transformer library following particular tutorial according train model train df produce result like accuracy f score etc way extract word embedding produced model trained training data would interested analyzing plotting embeddings academic purpose unable figure way simple transformer library
r return ngram from a vector which contains specific string,"<p>I want to generate <code>ngram</code> keywords from a vector, given a specific string.
For example, let's say I would need <code>bigram</code>, for each element of the vector I want to extract relevant bigrams and concatenate all the bigrams which has the keyword <code>ncd</code>.</p>
<pre><code>have &lt;- c('add the ncd mse to the website', 'setup new ncd staffs on t&amp;ta for wireless. all new ncd should go to horizon', 'map out current ncd post locations on 1st, 2nd floors')

want_bigram &lt;- c('the ncd | ncd mse', 'new ncd | ncd staffs | new ncd |ncd should', 'current ncd | ncd post')

</code></pre>
<p>Thank you</p>
",Training and Model Evaluation,r return ngram vector contains specific string want generate keywords vector given specific string example let say would need element vector want extract relevant bigram concatenate bigram ha keyword thank
Error raised in model.fit() if validation_data ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(),"<p>I'm trying to run a simple autoencoder model. I'm reading training data from a csv which consists of word embeddings. I have this code, but the error in the title is raised in <code>model.fit()</code>  function and connected with my <code>validation data</code>. I tried many things however the error remained. I'm new in NLP and maybe my logic is totally wrong I don't know. So, I'd be appreciated if anybody can help. Here is my code:</p>
<pre><code>def train_predict(df):
X_train, X_validation = train_test_split(df, test_size=0.3, random_state=42, shuffle=True)
X = X_train.iloc[:, :-1].to_numpy()           #shape is (1880,220) in here
X = tf.expand_dims(X, axis=-1)                #shape is (1880,220,1)
X_val = X_validation.iloc[:,:-1].to_numpy()   #shape is (300,220)
X_val= tf.expand_dims(X_val, axis=-1)         #shape is (300,220,1)

inputs, decoder_output, visualization = autoEncoder(X)

model = Model(inputs=inputs, outputs=decoder_output)
encoder_model = Model(inputs=inputs, outputs=visualization)

batch_size = 128
train_steps = len(X) // batch_size
val_steps = len(X_val) // batch_size
model.summary()
model.compile(optimizer='adam', metrics=['accuracy'], loss='mean_squared_error')
model.fit(X, steps_per_epoch=train_steps, validation_data=X_val, validation_steps=val_steps,epochs=100) 
result = model.evaluate(X_val, steps=10)
</code></pre>
<p>Also the detail of my autoEncoder function code is as follows:</p>
<pre><code>def autoEncoder(X_train):
inputs = tf.keras.layers.Input(shape=(X_train.shape[1],1))
# parameters
conv_1 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)
max_pool_1 = MaxPool1D(pool_size=2)(conv_1)

conv_2 = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(max_pool_1)
max_pool_2 = MaxPool1D(pool_size=2)(conv_2)

# BOTTLE NECK

bottle_neck = Conv1D(filters=256, kernel_size=3, activation='relu', padding='same')(max_pool_2)
visualization = Conv1D(filters=1, kernel_size=3, activation='sigmoid', padding='same')(bottle_neck)

# DECODER
conv_3 = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(bottle_neck)
upsample_1 = UpSampling1D(size=2)(conv_3)

conv_4 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(upsample_1)
upsample_2 = UpSampling1D(size=2)(conv_4)

decoder_output = Conv1D(filters=1, kernel_size=3, activation='sigmoid', padding='same')(upsample_2)

return inputs, decoder_output, visualization
</code></pre>
",Training and Model Evaluation,error raised model fit validation data valueerror truth value array one element ambiguous use trying run simple autoencoder model reading training data csv consists word embeddings code error title raised function connected tried many thing however error remained new nlp maybe logic totally wrong know appreciated anybody help code also detail autoencoder function code follows
gensim LDA training,"<p>I am working with gensim LDA model for a project. I cant seem to find a proper number of topics. My question is, just to be sure, every time I train the model it re-starts, right?
For example, I try it out with 47 topics, terrible results; so then I go back to the cell and change 47 to 80 topics and run it again. It completely starts a new training and erases what it has learned with the 47 topics, right?</p>
<p>I am having terrible results with LDA, similarity comes to 100% or 0% and I am having trouble parameter tuning. LSI has given me excellent results.
Thanks!</p>
",Training and Model Evaluation,gensim lda training working gensim lda model project cant seem find proper number topic question sure every time train model start right example try topic terrible result go back cell change topic run completely start new training era ha learned topic right terrible result lda similarity come trouble parameter tuning lsi ha given excellent result thanks
How to setup TF 2.4 Training data with generator or other means,"<p>I have a model setup with one input and two outputs. I am trying to use any of</p>
<ol>
<li><a href=""https://www.tensorflow.org/guide/data#consuming_python_generators"" rel=""nofollow noreferrer"">tf.data.Dataset.from_generator</a></li>
<li><a href=""https://www.tensorflow.org/guide/keras/train_and_evaluate#other_input_formats_supported"" rel=""nofollow noreferrer"">fit with regular python
generator</a></li>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset"" rel=""nofollow noreferrer"">tf.data.TFRecordDataset</a></li>
</ol>
<p>So far all my attempts have run into errors, which I can only assume is based on the shape/types involved in my output from the generators I've tried setting up. What format should the output of such a generator be?
I am also super open to suggestions for doing this differently
You can <a href=""https://jdc-public-bucket.s3.amazonaws.com/text_extraction.ipynb"" rel=""nofollow noreferrer"">download my whole notebook here</a> if you'd like to look through</p>
<h2>The Input</h2>
<p>The input to the model is of shape</p>
<pre><code>(None,)
</code></pre>
<p>And is of type</p>
<pre><code>tf.string
</code></pre>
<p>I am able to get model output with</p>
<pre><code>model(tf.constant(['Hello TensorFlow!']))
</code></pre>
<h2>The outputs</h2>
<p>There are two output heads for the model, the first is of shape</p>
<pre><code>(None, 128, 5)
</code></pre>
<p>The second is of shape</p>
<pre><code>(None, 128, 3)
</code></pre>
<p>They both are of type</p>
<pre><code>tf.float32
</code></pre>
<p>The loss for my model is sparse categorical crossentropy. (I want a softmax across 5 or 3 classes depending on the head, for each of the 128 outputs, with the None being there for the batch size). I believed for this the proper output format would be a tuple of batch_size instances of the following format</p>
<pre><code>(input_string, (output_for_head1, output_for_head2))
</code></pre>
<p>where input_string is a string, output_for_head1 and output_for_head2 are both numpy arrays of shape (128) and type int.</p>
<h2>Some random things I've tried for fitting on generator directly</h2>
<h3>Yield single item rather than whole batch (using batch size 10 for all testing)</h3>
<p>Gets index out of bounds error- pretty sure this needs to be batched</p>
<h3>Yield whole batch</h3>
<p>Get error</p>
<pre><code>    Data is expected to be in format `x`, `(x,)`, `(x, y)`, or `(x, y, sample_weight)`, found: ((&lt;tf.Tensor: shape=(), dtype=string, numpy=b'Ya Yeet'&gt;, (&lt;tf.Tensor: shape=(128,), dtype=int64, numpy=... ( a very long set of (128,) tensors which is too large to post here)


     [[{{node PyFunc}}]]
     [[IteratorGetNext]] [Op:__inference_train_function_95064]

Function call stack:
train_function
</code></pre>
<p>​</p>
",Training and Model Evaluation,setup tf training data generator mean model setup one input two output trying use tf data dataset generator fit regular python generator tf data tfrecorddataset far attempt run error assume based shape type involved output generator tried setting format output generator also super open suggestion differently download whole notebook like look input input model shape type able get model output output two output head model first shape second shape type loss model sparse categorical crossentropy want softmax across class depending head output none batch size believed proper output format would tuple batch size instance following format input string string output head output head numpy array shape type int random thing tried fitting generator directly yield single item rather whole batch using batch size testing get index bound error pretty sure need batched yield whole batch get error
How do I use BertForMaskedLM or BertModel to calculate perplexity of a sentence?,"<p>I want to use BertForMaskedLM or BertModel to calculate perplexity of a sentence, so I write code like this:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import torch
import torch.nn as nn
from transformers import BertTokenizer, BertForMaskedLM
# Load pre-trained model (weights)
with torch.no_grad():
    model = BertForMaskedLM.from_pretrained('hfl/chinese-bert-wwm-ext')
    model.eval()
    # Load pre-trained model tokenizer (vocabulary)
    tokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm-ext')
    sentence = &quot;我不会忘记和你一起奋斗的时光。&quot;
    tokenize_input = tokenizer.tokenize(sentence)
    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])
    sen_len = len(tokenize_input)
    sentence_loss = 0.

    for i, word in enumerate(tokenize_input):
        # add mask to i-th character of the sentence
        tokenize_input[i] = '[MASK]'
        mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])

        output = model(mask_input)

        prediction_scores = output[0]
        softmax = nn.Softmax(dim=0)
        ps = softmax(prediction_scores[0, i]).log()
        word_loss = ps[tensor_input[0, i]]
        sentence_loss += word_loss.item()

        tokenize_input[i] = word
    ppl = np.exp(-sentence_loss/sen_len)
    print(ppl)
</code></pre>
<p>I think this code is right, but I also notice BertForMaskedLM's paramaters <code>masked_lm_labels</code>, so could I use this paramaters to calculate PPL of a sentence easiler?
I know the input_ids argument is the masked input, the masked_lm_labels argument is the desired output. But I couldn't understand the actual meaning of its output loss, its code like this:</p>
<pre><code>if masked_lm_labels is not None:
    loss_fct = CrossEntropyLoss()  # -100 index = padding token
    masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), 
    masked_lm_labels.view(-1))
    outputs = (masked_lm_loss,) + outputs
</code></pre>
",Training and Model Evaluation,use bertformaskedlm bertmodel calculate perplexity sentence want use bertformaskedlm bertmodel calculate perplexity sentence write code like think code right also notice bertformaskedlm paramaters could use paramaters calculate ppl sentence easiler know input id argument masked input masked lm label argument desired output understand actual meaning output loss code like
How to train Spacy3 project with FP16 mixed precision,"<p><strong>The goal is to run <code>python -m spacy train</code> with FP16 mixed precision</strong> to enable the use of large transformers (<code>roberta-large</code>, <code>albert-large</code>, etc.) in limited VRAM (RTX 2080ti 11 GB).</p>
<p>The new Spacy3 <a href=""https://spacy.io/usage/projects#project-yml"" rel=""nofollow noreferrer"">project.yml approach</a> to training directly uses <a href=""https://huggingface.co/models?filter=pytorch"" rel=""nofollow noreferrer"">Huggingface-transformers models</a> loaded via <a href=""https://github.com/explosion/spacy-transformers"" rel=""nofollow noreferrer"">Spacy-transformers v1.0</a>. Huggingface models can be run with mixed precision just by adding the <code>--fp16</code> flag (<a href=""https://huggingface.co/transformers/examples.html#distributed-training-and-mixed-precision"" rel=""nofollow noreferrer"">as described here</a>).</p>
<p>The spacy config was generated using <code>python -m spacy init config --lang en --pipeline ner --optimize efficiency --gpu -F default.cfg</code>, and checked to be complete by <code>python -m spacy init fill-config default.cfg config.cfg --diff</code>. Yet no FP16 / mixed-precision is to be found.</p>
<h3>To reproduce</h3>
<p>Use the <a href=""https://github.com/explosion/projects/tree/v3/pipelines/ner_wikiner"" rel=""nofollow noreferrer"">spaCy Project: Named Entity Recognition (WikiNER)</a> with changed <code>init-config</code> in <code>project.yml</code> to use a GPU and a transformer (<code>roberta-base</code> by default):</p>
<pre class=""lang-yaml prettyprint-override""><code>commands:
  -
    name: init-config
    help: &quot;Generate a transformer English NER config&quot;
    script:
      - &quot;python -m spacy init config --lang en --pipeline ner --gpu -F --optimize efficiency -C configs/${vars.config}.cfg&quot;
</code></pre>
<h3>What was tested</h3>
<ul>
<li>Added <code>--fp16</code> to <code>python -m spacy project run</code></li>
<li>Added <code>--fp16</code> to <code>python -m spacy train</code></li>
<li>Added <code>fp16 = true</code> to <code>default.cfg</code> in various sections (<code>[components.transformer], [components.transformer.model], [training], [initialize]</code>)</li>
</ul>
<p>The logic was <code>transformers</code> are run in FP16 as such:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import TrainingArguments
TrainingArguments(..., fp16=True, ...)
</code></pre>
<h3>SW stack specifics</h3>
<pre class=""lang-sh prettyprint-override""><code> - spacy              3.0.3
 - spacy-transformers 1.0.1
 - transformers       4.2.2
 - torch              1.6.0+cu101
</code></pre>
",Training and Model Evaluation,train spacy project fp mixed precision goal run fp mixed precision enable use large transformer etc limited vram rtx ti gb new spacy project yml approach training directly us huggingface transformer model loaded via spacy transformer v huggingface model run mixed precision adding flag described spacy config wa generated using checked complete yet fp mixed precision found reproduce use spacy project named entity recognition wikiner changed use gpu transformer default wa tested added added added various section logic wa run fp sw stack specific
How to train a Masked Language Model with a big text corpus(200GB) using PyTorch？,"<p>Recently I am training a masked language model with a big text corpus(200GB) using <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">transformers</a>. The training data is too big to fit into computer equiped with 512GB memory and V100(32GB)*8. Is it possible to find a elegant way to train model with big data?</p>
<p>Now I split entire training data into 20 pieces and create 20 <code>DataLoader</code>s, each dataloader loads corresponding data into memory and uses <code>DistributedDataParallel</code> to train the model. The code is below.</p>
<pre class=""lang-py prettyprint-override""><code>def get_dataloader(rank, world_size, tokenizer, part_of_data, args):
    train_data_df = get_DataFrame(args, part_of_data)
    train_dataset = CustomDataset(train_data_df, tokenizer, args.max_len)

    data_collator = transformers.DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=True, mlm_probability=0.15
    )

    train_sampler = torch.utils.data.distributed.DistributedSampler(
        train_dataset,
        num_replicas=world_size,
        rank=rank,
    )

    train_loader = torch.utils.data.DataLoader(
        dataset=train_dataset,
        batch_size=args.batch_size,
        shuffle=False,            
        num_workers=2,
        pin_memory=True,
        collate_fn=data_collator,
        sampler=train_sampler)

    return train_loader
</code></pre>
<p>By reading PyTorch Doc I find a dataloader called <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset"" rel=""nofollow noreferrer"">IterableDataset</a>, maybe IterableDataset is more suitable for my task, but there is a NOTE in the Doc said <strong>neither sampler nor batch_sampler is compatible with iterable-style datasets, since such datasets have no notion of a key or an index.</strong></p>
<p>So I wonder if it is possible to use <code>IterableDataset</code>, <code>DistributedSampler</code> and <code>DistributedDataParallel</code> to train model, or other elegant method to train model instead of splitting data. Thanks a lot.</p>
",Training and Model Evaluation,train masked language model big text corpus gb using pytorch recently training masked language model big text corpus gb using transformer training data big fit computer equiped gb memory v gb possible find elegant way train model big data split entire training data piece create dataloader load corresponding data memory us train model code reading pytorch doc find dataloader called iterabledataset maybe iterabledataset suitable task note doc said neither sampler batch sampler compatible iterable style datasets since datasets notion key index wonder possible use train model elegant method train model instead splitting data thanks lot
How can I sort words into categories in Python?,"<p>I work on a project where I use Google vision to detect objects in images. The API returns a list of labels. So I have multiple words and I would like to put each word into a category. For example :</p>
<p>Google cloud vision returns :</p>
<pre><code>['Head', 'Lamp', 'Eye', 'Green', 'Arm', 'Piano', 'Mobile phone', 'Blue', 'Toy']
</code></pre>
<p>And I would like to have something like :</p>
<pre><code>{'Object' : ['Lamp', 'Piano', 'Mobile phone', 'Toy'],
'Color' : ['Green', 'Blue'],
'Body parts': ['Head', 'Eye', 'Arm']
}
</code></pre>
<p>I know that word2vec have something called similarity but it means that I have to train a model. Is there any pretained model I can use ? Or maybe another solution to do this ?</p>
",Training and Model Evaluation,sort word category python work project use google vision detect object image api return list label multiple word would like put word category example google cloud vision return would like something like know word vec something called similarity mean train model pretained model use maybe another solution
"ValueError: logits and labels must have the same shape ((None, 1) vs ())","<p>I am getting a ValueError: logits and labels must have the same shape ((None, 1) vs ()) when doing a model evaluate. I get the model to train but when I evaluate is when I have the problem. I used a tf.expand_dims for logits but wondering if this needs to be applied to the labels as well?</p>
<p>here is my code below.</p>
<pre><code>
import tensorflow as tf
import tensorflow_datasets as tfds
</code></pre>
<pre><code>dataset, info = tfds.load('imdb_reviews', with_info=True,
                          as_supervised=True)
train_dataset, test_dataset = dataset['train'], dataset['test']
</code></pre>
<pre><code>BUFFER_SIZE = 10000
BATCH_SIZE = 64
</code></pre>
<pre><code>train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(1)
</code></pre>
<pre><code>VOCAB_SIZE, EMBED_SIZE, NUM_OOV_BUCKETS = 10000, 128, 1000
</code></pre>
<pre><code>encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(
    max_tokens=VOCAB_SIZE)
encoder.adapt(train_dataset.map(lambda text, label: text))
</code></pre>
<pre><code>class AttentionLayer(tf.keras.layers.Layer):

    def __init__(self, **kwargs):

        super(AttentionLayer, self).__init__(**kwargs)

        self.query_layer = tf.keras.layers.Conv1D(
            filters=100,
            kernel_size=4,
            padding='same'
        )

        self.value_layer = tf.keras.layers.Conv1D(
            filters=100,
            kernel_size=4,
            padding='same'
        )

        self.attention_layer = tf.keras.layers.Attention()
    
    def call(self, inputs):

        query = self.query_layer(inputs)
        value = self.value_layer(inputs)

        attention = self.attention_layer([query, value])

        return tf.keras.layers.concatenate([query, attention])

attention_layer = AttentionLayer()

</code></pre>
<pre><code>model1 = tf.keras.models.Sequential([
    tf.keras.Input(shape=(),batch_size=1, dtype=tf.string, name='InputLayer'),
    encoder,
    tf.keras.layers.Embedding(VOCAB_SIZE + NUM_OOV_BUCKETS, EMBED_SIZE, mask_zero=True, name='Embedding_Layer'),
    attention_layer,
    tf.keras.layers.Conv1D(filters=32, kernel_size=4, padding = 'same', activation = 'relu', name='Conv1DLayer'),
    tf.keras.layers.MaxPooling1D(pool_size=2, name='MaxPoolLayer'),
    tf.keras.layers.LSTM(64, dropout = 0.2, name='DropoutLayer'),
    tf.keras.layers.Dense(250, activation = 'relu', name='DenseLayer'),
    tf.keras.layers.Dense(1, activation='sigmoid', name='Output_Layer')
])
</code></pre>
<pre><code>model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;])
</code></pre>
<pre><code>def preprocess_y(x, y):
    return x, tf.expand_dims(y, -1)
</code></pre>
<pre><code>history1 = model1.fit(
    train_dataset.map(preprocess_y),
    batch_size=BATCH_SIZE,
    epochs=1)
</code></pre>
<pre><code>model1.evaluate(test_dataset)
</code></pre>
<p>ValueError: logits and labels must have the same shape ((None, 1) vs ())</p>
",Training and Model Evaluation,valueerror logits label must shape none v getting valueerror logits label must shape none v model evaluate get model train evaluate problem used tf expand dims logits wondering need applied label well code valueerror logits label must shape none v
Loss function for Image captioning with visual attention,"<p>I am trying to understand the TensorFlow implementation of Image captioning with visual attention. I understand what SparseCategoricalCrossentropy is but what is <code>loss_function</code> doing? Can someone explain? <a href=""https://www.tensorflow.org/tutorials/text/image_captioning#checkpoint"" rel=""nofollow noreferrer"">Tensorflow Implementation</a></p>
<pre><code>loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')

def loss_function(real, pred):
  mask = tf.math.logical_not(tf.math.equal(real, 0))
  loss_ = loss_object(real, pred)

  mask = tf.cast(mask, dtype=loss_.dtype)
  loss_ *= mask

  return tf.reduce_mean(loss_)
</code></pre>
",Training and Model Evaluation,loss function image captioning visual attention trying understand tensorflow implementation image captioning visual attention understand sparsecategoricalcrossentropy someone explain tensorflow implementation
"Word Embeddings on Mobiles (Android, iOS)","<p>I would like to use Word Embeddings (aka GloVe) in my Mobile (Android, iOS) app. Nevertheless, the pre-trained weights file is large (starting from 100MB and up to 800MB). what are the generally accepted approaches for storing and using large pre-trained models on Mobiles?</p>
",Training and Model Evaluation,word embeddings mobile android io would like use word embeddings aka glove mobile android io app nevertheless pre trained weight file large starting mb mb generally accepted approach storing using large pre trained model mobile
Abstractive text summary in R,"<p>This is a short question.</p>
<p>Is there a way to perform abstractive text summarization in R?</p>
<p>There are ways to perform extractive summary (ie extract few relevant sentences), as shown here <a href=""https://stackoverflow.com/questions/36064946/text-summarization-in-r-language"">Text summarization in R language</a></p>
<p>However, I was unable to find a way to do abstractive summary &quot;purely&quot; in R. I understand that abstractive summary is way more complex and typically requires model training. The two methods that might work so far are using python through <code>reticulate</code> package or use on of google's APIs. was wondering if anyone is aware of R package that does that without external dependencies.</p>
<p>thank you in advance</p>
",Training and Model Evaluation,abstractive text summary r short question way perform abstractive text summarization r way perform extractive summary ie extract relevant sentence shown href summarization r language however wa unable find way abstractive summary purely r understand abstractive summary way complex typically requires model training two method might work far using python package use google apis wa wondering anyone aware r package doe without external dependency thank advance
Converting pandas dataframe to CoNLL,"<p>I have a processed dataframe which is used as a input to train a NLP model:</p>
<pre><code> sentence_id    words   labels
0   0            a      B-ORG
1   0            b      I-ORG
2   0            c      I-ORG
5   1            d      B-ORG
6   1            e      I-ORG
7   2            f      B-PER
8   2            g      I-PER
</code></pre>
<p>I need to convert this into ConLL text format as below:</p>
<pre><code>a B-ORG
b I-ORG
c I-ORG

d B-ORG
e I-ORG

f B-PER
g I-PER
</code></pre>
<p>The CoNLL format is a text file with one word per line with sentences separated by an empty line. The first word in a line should be the word and the last word should be the label.</p>
<p>Anyone have any idea how to do that?</p>
",Training and Model Evaluation,converting panda dataframe conll processed dataframe used input train nlp model need convert conll text format conll format text file one word per line sentence separated empty line first word line word last word label anyone idea
Restore missing vowels in an array of words,"<p>I have an array of item descriptions, many of which have had some vowels removed from some words. What is the best approach for replacing them, using javascript?</p>
<p>Many answers on here about how to remove vowels, but I can't find anything on how to restore them?</p>
<p>Complete accuracy is not a requirement, a best-guess would be good enough?</p>
",Training and Model Evaluation,restore missing vowel array word array item description many vowel removed word best approach replacing using javascript many answer remove vowel find anything restore complete accuracy requirement best guess would good enough
Use a pre trained model with text2vec?,"<p>I would like to use a pre trained model with text2vec. My understanding was that the benefit here is that these models have been trained on a huge volume of data already, e.g. <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">Google News Model</a>.</p>

<p>Reading the text2vec <a href=""https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html"" rel=""nofollow noreferrer"">documentation</a> it looks like the getting started code reads in text data then trains a model with it:</p>

<pre><code>library(text2vec)
text8_file = ""~/text8""
if (!file.exists(text8_file)) {
  download.file(""http://mattmahoney.net/dc/text8.zip"", ""~/text8.zip"")
  unzip (""~/text8.zip"", files = ""text8"", exdir = ""~/"")
}
wiki = readLines(text8_file, n = 1, warn = FALSE)
</code></pre>

<p>The documentation then proceeds to show one how to create tokens and a vocab:</p>

<pre><code># Create iterator over tokens
tokens &lt;- space_tokenizer(wiki)
# Create vocabulary. Terms will be unigrams (simple words).
it = itoken(tokens, progressbar = FALSE)
vocab &lt;- create_vocabulary(it)
vocab &lt;- prune_vocabulary(vocab, term_count_min = 5L)
# Use our filtered vocabulary
vectorizer &lt;- vocab_vectorizer(vocab)
# use window of 5 for context words
tcm &lt;- create_tcm(it, vectorizer, skip_grams_window = 5L)
</code></pre>

<p>Then, this looks like the step to fit the model:</p>

<pre><code>glove = GlobalVectors$new(word_vectors_size = 50, vocabulary = vocab, x_max = 10)
glove$fit(tcm, n_iter = 20)
</code></pre>

<p>My question is, is the well know Google pre trained word2vec model usable here without the need to rely on my own vocab or my own local device to train the model? If yes, how could I read it in and use it in r?</p>

<p>I think I'm misunderstanding or missing something here? Can I use text2vec for this task?</p>
",Training and Model Evaluation,use pre trained model text vec would like use pre trained model text vec understanding wa benefit model trained huge volume data already e g google news model reading text vec documentation look like getting started code read text data train model documentation proceeds show one create token vocab look like step fit model question well know google pre trained word vec model usable without need rely vocab local device train model yes could read use r think misunderstanding missing something use text vec task
BERT- pythorch- regression task - predicting same score for each instance,"<p>I have implemented bert model for a regression task. trained the model and predict, however, model predicts 0 for each instance.</p>
<p>In a nutshell, task was predicting the positivity/negativity of a scientific article between -1 and 1.</p>
<p>Obviously the problem turns out from train (model does not learn) because validation loss does not decrease as expected. I will share the code of my model and look forward to hearing any suggestions. Please, let me know if you need to see other parts of the code.</p>
<p>Thanks!</p>
<pre><code>class CustomSciBERTModel(nn.Module):
    def __init__(self):
      super(CustomSciBERTModel, self).__init__()
      self.scibert = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')
      self.num_labels = 1
      self.linear_layer = nn.Linear(768, 1)

    def forward(self, ids, mask,labels):
      output = self.scibert(
            input_ids=ids, 
            attention_mask=mask)
      


      logits = self.linear_layer(output[1]) 
     

      loss = None
      # while training return loss and while validation/ testing resturn logits
      if labels is not None:
        loss_fct = nn.MSELoss()
        #loss_fct = nn.CrossEntropyLoss() *************
        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
        return loss
      else:
        return logits
</code></pre>
",Training and Model Evaluation,bert pythorch regression task predicting score instance implemented bert model regression task trained model predict however model predicts instance nutshell task wa predicting positivity negativity scientific article obviously problem turn train model doe learn validation loss doe decrease expected share code model look forward hearing suggestion please let know need see part code thanks
FastText: upper case or lower case,"<p>Using the pre-trained model:</p>
<pre><code>import fasttext.util
fasttext.util.download_model('en', if_exists='ignore')  # English
ft = fasttext.load_model('cc.en.300.bin')
</code></pre>
<p>both queries <code>ft['HOME']</code> and <code>ft['home']</code> works, but return different vectors.</p>
<p>What is the optimal query to make?</p>
<p>If I'm working with an uppercased corpus, should I transform it into a lowercased?</p>
",Training and Model Evaluation,fasttext upper case lower case using pre trained model query work return different vector optimal query make working uppercased corpus transform lowercased
How does CountVectorizer deal with new words in test data?,"<p>I understand how CountVectorizer works in general. It takes word tokens and creates a sparse count matrix of documents (rows) and token counts (columns), that we can use for ML modeling.</p>
<p>However, how does it deal with new words that can presumably show up in test data, that weren't in the training data? Does it just ignore them?</p>
<p>Also, from a modeling standpoint, should the assumption be that if certain words are so rare that they didn't show up in the training data at all, and that they aren't relevant for any modeling you might perform?</p>
",Training and Model Evaluation,doe countvectorizer deal new word test data understand countvectorizer work general take word token creates sparse count matrix document row token count column use ml modeling however doe deal new word presumably show test data training data doe ignore also modeling standpoint assumption certain word rare show training data relevant modeling might perform
"AllenNLP DatasetReader: only loads a single instance, instead of iterating over all instances in the training dataset","<p>I am using <strong>AllenNLP</strong> to train a hierarchical <strong>attention network model</strong>. My training dataset consists of a list of <strong>JSON objects</strong> (eg, each object in the list is a <strong>JSON object</strong> with keys := [&quot;text&quot;, &quot;label&quot;]. The value associated with the text key is a list of lists, eg:</p>
<pre><code>[{&quot;text&quot;:[[&quot;i&quot;, &quot;feel&quot;, &quot;sad&quot;], [&quot;not&quot;, &quot;sure&quot;, &quot;i&quot;, &quot;guess&quot;, &quot;the&quot;, &quot;weather&quot;]], &quot;label&quot;:0} ... {&quot;text&quot;:[[str]], &quot;label&quot;:int}] 
</code></pre>
<p>My DatasetReader class looks like:</p>
<pre><code>@DatasetReader.register(&quot;my_reader&quot;)
class TranscriptDataReader(DatasetReader):
    def __init__(self,
                 token_indexers: Optional[Dict[str, TokenIndexer]] = None,
                 lazy: bool = True) -&gt; None:
        super().__init__(lazy)
        self._token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}

    def _read(self, file_path: str) -&gt; Iterator[Instance]:
        with open(file_path, 'r') as f:
            data = json.loads(f.read())
            for _,data_json in enumerate(data):
                sent_list = []
                for segment in data_json[&quot;text&quot;]:
                    sent_list.append(self.get_text_field(segment))
                yield self.create_instance(sent_list, str(data_json[&quot;label&quot;]))

    def get_text_field(self, segment):
        return TextField([Token(token.lower()) for token in segment],self._token_indexers)


    def create_instance(self, sent_list, label):
        label_field = LabelField(label, skip_indexing=False)
        fields = {'tokens': ListField(sent_list), 'label': label_field}
        return Instance(fields)

</code></pre>
<p>and in my config file, I have:</p>
<pre><code>{
  dataset_reader: {
    type: 'my_reader',
  },

  train_data_path: 'data/train.json',
  validation_data_path: 'data/dev.json',

 data_loader: {
    batch_sampler: {
      type: 'bucket',
      batch_size: 10
    }
 },

</code></pre>
<p>I have tried (alternatively) setting the <code>lazy</code> param for the dataset reader to <code>True</code> and <code>False</code>.</p>
<ul>
<li>When set to <code>True</code>, the model is able to train, however, I observe that only one train and one dev instance actually get loaded, when my dataset contains ~100.</li>
<li>When set to <code>False</code>, I've modified the <code>yield</code> line in <code>_read</code> to be <code>return</code>; however, this causes a type error in the base vocabulary class. I've also tried keeping the <code>yield</code> as is when set to <code>False</code>; in this case, no instances get loaded at all, and since the set of instances is empty, the vocabulary does not get instantiated, and the embedding class throws an error.</li>
</ul>
<p>Would appreciate pointers, and/or tips for debugging.</p>
",Training and Model Evaluation,allennlp datasetreader load single instance instead iterating instance training dataset using allennlp train hierarchical attention network model training dataset consists list json object eg object list json object key text label value associated text key list list eg datasetreader class look like config file tried alternatively setting param dataset reader set model able train however observe one train one dev instance actually get loaded dataset contains set modified line however cause type error base vocabulary class also tried keeping set case instance get loaded since set instance empty vocabulary doe get instantiated embedding class throw error would appreciate pointer tip debugging
How can I calculate the accuracy of words?,"<p>I am using a textbob for calculate the accuracy  of the words.</p>
<pre><code>text = 'thank you calling on product support talking to caafic k y please helping with your a digit employ number   ing'
orig_words = text.split()
fixed_words = TextBlob(text).correct().split()
words=(len([(x,y) for x,y in zip(orig_words, fixed_words) if x != y])) # =&gt; 3
total_word_len=len(orig_words)
correct_words=total_word_len-words
accuraccy=correct_words/total_word_len
</code></pre>
<p>but the problem is it is considering the  'caafic' ,'k', 'y' ,'ing' as the correct word. Ideally these should not be considered as correct words.</p>
",Training and Model Evaluation,calculate accuracy word using textbob calculate accuracy word problem considering caafic k ing correct word ideally considered correct word
Generate natural language based on similar phrases,"<p>I am working on a project and was hoping the community could share some thoughts on how best to approach it.</p>
<p>Situation:</p>
<ul>
<li>In a current business process, people are manually producing similar but slightly distinct descriptions (distinct in terms of syntax, grammar)</li>
<li>These descriptions are derived from a set of structured data points. More of such data points are being generated continuously</li>
<li>The objective is to automatically produce descriptions from these data points</li>
</ul>
<p>The data points are organised as a dictionary in Python, for example:</p>
<p><code>thisdict = { &quot;company&quot;: &quot;Amazon&quot;, &quot;team&quot;: &quot;Global Procurement&quot;, &quot;employees&quot;: 15.0, &quot;contractors&quot;: 2.0 }</code></p>
<p>Below three examples of typical descriptions that are derived from this dictionary:</p>
<ul>
<li>Amazon Global Procurement is supported by 15.0 employees and a team of 2.0 contractors.</li>
<li>15 employees are dedicated to Amazon Global Procurement with 2.0 contractors supporting.</li>
<li>Amazon Global Procurement is supported by 15 heads and an additional 2.0 contractors.</li>
</ul>
<p>I have a rich data set of these descriptions and the accompanying dictionary with which I hope to train a model that produces descriptions based on new additions to the dictionary.</p>
<p>I thought that the sentence generation task could be approached via Markov Chains. However, I need to preserve the associations in the dictionary (i.e. the number 15.0 needs to be linked to the term 'employees' or related synonyms) and I am not sure how to incorporate these dictionary pairs into the model.</p>
<p>Any suggestions would be appreciated.</p>
<p>Thanks</p>
",Training and Model Evaluation,generate natural language based similar phrase working project wa hoping community could share thought best approach situation current business process people manually producing similar slightly distinct description distinct term syntax grammar description derived set structured data point data point generated continuously objective automatically produce description data point data point organised dictionary python example three example typical description derived dictionary amazon global procurement supported employee team contractor employee dedicated amazon global procurement contractor supporting amazon global procurement supported head additional contractor rich data set description dictionary hope train model produce description based new addition dictionary thought sentence generation task could approached via markov chain however need preserve association dictionary e number need linked term employee related synonym sure incorporate dictionary pair model suggestion would appreciated thanks
Fill in the blank in sentences with bidirectional LSTM in Keras,"<p>I'm currently studying RNN, in particular LSTM and I was trying to figure out how to implement a bidirectional LSTM to fill in the missing word in a sentence.
I have a doubt about the strucuture of the train set to be passed to the fit method of the model.
If my list of sentences is composed by elements like this: &quot;HI GUYS, &lt;MISSING&gt; ARE YOU?&quot; and my target label is &quot;HOW&quot;, how could the BI-LSTM understand that it has to predict the missing value and not the next element of the sentence? I saw <a href=""https://stackoverflow.com/questions/43035827/whats-the-difference-between-a-bidirectional-lstm-and-an-lstm"">here</a> that the advantage of a bidirectional LSTM is the ability to look in both past and future tokens to get information about the context and better predict the target, but I still don't get how to implement this in practice. So my questions are:</p>
<ol>
<li>what is the structure of my train set?</li>
<li>Does the BI-LSTM know what token to predict or do I have to specify it? And how?</li>
</ol>
",Training and Model Evaluation,fill blank sentence bidirectional lstm kera currently studying rnn particular lstm wa trying figure implement bidirectional lstm fill missing word sentence doubt strucuture train set passed fit method model list sentence composed element like hi guy missing target label could bi lstm understand ha predict missing value next element sentence saw href advantage bidirectional lstm ability look past future token get information context better predict target still get implement practice question p structure train set doe bi lstm know token predict specify
Error in shape (dimention) and type of Keras model input,"<p>I am desperate to set the Input shape of this simple Keras model :(
Both X and Y are numpy.narray but I don't know what's the wrong with it! I tried different X shape but the error is there! The info of the datasets (dimentions, number of samples, etc.) is available in the code.
The .pkl file for X_train is got from hidden state of a pre-trained model.</p>
<pre><code>import pandas as pd
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils
from keras import Input, Model
from keras.layers import Dense
import numpy as np

############################## X_Train ############################

X_Train_3embed1 = pd.read_pickle(&quot;XX_Train_3embeding.pkl&quot;)


X_Train_3embed = np.array(X_Train_3embed1)

print(&quot;X-Train&quot;)
print(X_Train_3embed.shape)   # (230, 1, 128)
print(type(X_Train_3embed))  # &lt;class 'numpy.ndarray'&gt;
print(X_Train_3embed[0].shape) # (1, 128)
print(type(X_Train_3embed[0])) # &lt;class 'numpy.ndarray'&gt;


############################## Y_Train ############################

Y_Train_labels_list = pd.read_pickle(&quot;lis_Y_all_Train.pkl&quot;)

print(type(Y_Train_labels_list))  #&lt;class 'numpy.ndarray'&gt;
print(type(Y_Train_labels_list[0])) #&lt;class 'str'&gt;

encoder = LabelEncoder()
encoder.fit(Y_Train_labels_list)
encoded_Y = encoder.transform(Y_Train_labels_list)
Y_my_Train = np_utils.to_categorical(encoded_Y)


print(&quot;Y-Train&quot;)
print(Y_my_Train.shape) #(230, 83)
print(type(Y_my_Train)) # &lt;class 'numpy.ndarray'&gt;
print(Y_my_Train[0].shape) # (83,)
print(type(Y_my_Train[0])) # &lt;class 'numpy.ndarray'&gt;

##################################  Model  ##################################

first_input = Input(shape=(1, 128))

first_dense = Dense(128)(first_input)

output_layer = Dense(83, activation='softmax')(first_dense)

model = Model(inputs=first_input, outputs=output_layer)

model.summary()


model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])


history = model.fit((X_Train_3embed, Y_my_Train), epochs=2, batch_size=32)
</code></pre>
<p>Here is the result:</p>
<pre><code>Model: &quot;model_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 128)            0         
_________________________________________________________________
dense_1 (Dense)              (None, 1, 128)            16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1, 83)             10707     
=================================================================
Total params: 27,219
Trainable params: 27,219
Non-trainable params: 0
_________________________________________________________________
Traceback (most recent call last):
  File &quot;/home/vahideh/PycharmProjects/3KArgen-master/MyTransferClassifier2.py&quot;, line 63, in &lt;module&gt;
    history = model.fit((X_Train_3embed, Y_my_Train), epochs=2, batch_size=32)
  File &quot;/home/vahideh/PycharmProjects/MyVirtualEnvs/MyKargo/lib/python3.6/site-packages/keras/engine/training.py&quot;, line 1154, in fit
    batch_size=batch_size)
  File &quot;/home/vahideh/PycharmProjects/MyVirtualEnvs/MyKargo/lib/python3.6/site-packages/keras/engine/training.py&quot;, line 579, in _standardize_user_data
    exception_prefix='input')
  File &quot;/home/vahideh/PycharmProjects/MyVirtualEnvs/MyKargo/lib/python3.6/site-packages/keras/engine/training_utils.py&quot;, line 99, in standardize_input_data
    data = [standardize_single_array(x) for x in data]
  File &quot;/home/vahideh/PycharmProjects/MyVirtualEnvs/MyKargo/lib/python3.6/site-packages/keras/engine/training_utils.py&quot;, line 99, in &lt;listcomp&gt;
    data = [standardize_single_array(x) for x in data]
  File &quot;/home/vahideh/PycharmProjects/MyVirtualEnvs/MyKargo/lib/python3.6/site-packages/keras/engine/training_utils.py&quot;, line 34, in standardize_single_array
    elif x.ndim == 1:
AttributeError: 'tuple' object has no attribute 'ndim'
</code></pre>
<p>How can I feed these dataset to the model? or change the input shape of the model?</p>
",Training and Model Evaluation,error shape dimention type kera model input desperate set input shape simple kera model x numpy narray know wrong tried different x shape error info datasets dimentions number sample etc available code pkl file x train got hidden state pre trained model result feed dataset model change input shape model
The way to combine two BERT model trained from scratch on different corpus to be one model?,"<p>I want to train BERT from scratch on a different language, with almost 150GB data (corpus), and I need to end this in maximum one month.
In some articles, I read that the time it took to train the language models like BERT on like data I have was more than a month over 8 TPUs.</p>
<p>I have two different accounts (we assume it is account-1 and account-2) on Google Cloud TPUs service, which offered me free TPUs for a month. I need to benefit from those different accounts and ending the training process during the free month.</p>
<p>So, I plan to do:</p>
<p>1- Divide the corpus into roughly equal parts, becomes corpus-1, corpus-2.</p>
<p>2- Training two model from scratch (BERT-1, BERT-2), each of them with a section of the corpus(corpus-1, corpus-2), and each model will train on a different account.</p>
<p>3- Saving each model after training.</p>
<p>The question is:</p>
<p>1- Is there any way to combine these two model to be one model?
(i.e, BERT-1 + BERT-2 = BERT-3, where BERT-3 becomes as if it has been trained on all the corpus=150GB), Is this possible?</p>
<p>Lastly, the combined model (BERT-3), I will fine-tune it on different NLP tasks.</p>
<p>I appreciate any contribution. Thank you.</p>
",Training and Model Evaluation,way combine two bert model trained scratch different corpus one model want train bert scratch different language almost gb data corpus need end maximum one month article read time took train language model like bert like data wa month tpus two different account assume account account google cloud tpus service offered free tpus month need benefit different account ending training process free month plan divide corpus roughly equal part becomes corpus corpus training two model scratch bert bert section corpus corpus corpus model train different account saving model training question way combine two model one model e bert bert bert bert becomes ha trained corpus gb possible lastly combined model bert fine tune different nlp task appreciate contribution thank
How to convert output to 1/0,"<p>I am Relatively new to building deep learrning models and I seem to be completely confused and stuck with errors related to shape and size.</p>
<p>Here’s the LSTM model and relevant code:</p>
<pre><code>    class LSTMTagger(nn.Module):

        def __init__(self):
            super(LSTMTagger, self).__init__()
            self.embedding = 
    nn.Embedding(wv.vectors.shape[0],512)#embedding_matrix.shape[1])
            self.lstm1 = nn.LSTM(input_size = 512, hidden_size = 64, dropout = 
    0.1,batch_first=True,bidirectional = True)
            self.dropout = nn.Dropout(p = 0.25)
            self.linear1 = nn.Linear(in_features = 128, out_features = 64)
            self.dropout = nn.Dropout(p = 0.25)
            self.linear2 = nn.Linear(in_features = 64, out_features = 1)
            self.sigmoid = nn.Sigmoid()


        def forward(self, X):
            X_embed = self.embedding(X)
            outr1, _ = self.lstm1(X_embed)
            xr = self.dropout(outr1) 
            xr= self.linear1(xr)
            xr = self.dropout(xr)
            xr= self.linear2(xr)
            outr4 = self.sigmoid(xr)
            outr4 = outr4.view(1,-1)

            return outr4

model = LSTMTagger()
torch.multiprocessing.set_sharing_strategy('file_system')
if torch.cuda.device_count() &gt; 1:
  print(&quot;Using &quot;, torch.cuda.device_count(), &quot; GPUs&quot;)
  # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs

# model =model.load_state_dict(torch.load('best_model_state.bin'))
model = nn.DataParallel(model, device_ids=[0]) #py r
torch.cuda.empty_cache()
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

model = model.to(device)

def train_epoch(
      model,
      data_loader,
      loss_fn,
      optimizer,
      device,
      scheduler,
      n_examples
    ):
      model = model.train()
      losses = []
      correct_predictions = 0
      for d in data_loader:
        print(f&quot;Input ids: {np.shape(d['input_ids'])}\n len: {len(d['input_ids'][0])}&quot;)
        input_ids = d[&quot;input_ids&quot;].to(device)
        targets = d[&quot;targets&quot;].to(device)
        outputs = model(input_ids)
        _, preds = torch.max(outputs, dim=1)
        print(f&quot;outputs is {np.shape(outputs)}&quot;)
        print(f&quot;targets is {targets}&quot;)
        # continue
        loss = criterion(outputs.squeeze(), targets)
        # loss.backward()
        # nn.utils.clip_grad_norm_(model.parameters(), clip)
        # optimizer.step()
        # loss = loss_fn(outputs, targets)
        correct_predictions += torch.sum(preds == targets)
        losses.append(loss.item())
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
      return correct_predictions.double() / n_examples, np.mean(losses)
EPOCHS = 6
optimizer = optim.Adam(model.parameters(), lr=2e-5)
total_steps = len(data_train) * EPOCHS
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)
loss_fn = nn.CrossEntropyLoss().to(device)
history = defaultdict(list)
best_accuracy = 0
criterion = nn.BCELoss()


print('starting training')
# exit()

for epoch in range(EPOCHS):
      print(f'Epoch {epoch + 1}/{EPOCHS}')
      print('-' * 10)
      train_acc, train_loss = train_epoch(
        model,
        data_train,
        loss_fn,
        optimizer,
        device,
        scheduler,
        len(df_train)
      )
</code></pre>
<p>In this instance the sample input is a tensor of size: torch.Size([1, 512])
, that looks like this:</p>
<pre><code>tensor([[44561, 972, 7891, 94, 2191, 131, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0]], device=‘cuda:0’)
</code></pre>
<p>and out put label (targets from train_epoch function) in case is just a simple 1 or 0 label in tensor form such as tensor([1], device=‘cuda:0’).</p>
<p>I have been facing issues consistently with this approach. Initially the output was 1x512x1. So, I added</p>
<pre><code>outr4 = outr4.view(1,-1)
</code></pre>
<p>after the sigmoid layer. Then, the output shape was reduced to 1x512 and I used squeeze function but, still, I face errors such as this one:</p>
<pre><code>ValueError: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([512])) is deprecated. Please ensure they have the same size.
</code></pre>
<p>I have spent a lot of time trying to figure out what was going on but to no avail. Isn’t the output supposed to be either ?1 or 0, instead of being a 1x512 shaped tensor?</p>
<p>I am relatively new to building models, so please excuse my lack of knowledge.</p>
",Training and Model Evaluation,convert output relatively new building deep learrning model seem completely confused stuck error related shape size lstm model relevant code instance sample input tensor size torch size look like put label target train epoch function case simple label tensor form tensor device cuda facing issue consistently approach initially output wa x x added sigmoid layer output shape wa reduced x used squeeze function still face error one spent lot time trying figure wa going avail output supposed either instead x shaped tensor relatively new building model please excuse lack knowledge
NLP - Sentence Segmentation,"<p>I am a newbie trying my hands on sentence segmentation in NLP.
I am aware tokenizers are available for the same in NLTK. But I wanted to build my own sentence segmenter using Machine Learning algorithm like Decision Tree. But I am not able to gather training data for it. How should be the data. How should it be labelled, since I wanted to try first using supervised learning. Any sample data already available? Any help will be useful. I searched in net for nearly a week and now posting the same for help. Thanks in advance.</p>
",Training and Model Evaluation,nlp sentence segmentation newbie trying hand sentence segmentation nlp aware tokenizers available nltk wanted build sentence segmenter using machine learning algorithm like decision tree able gather training data data labelled since wanted try first using supervised learning sample data already available help useful searched net nearly week posting help thanks advance
Is it OK to combine domain specific word2vec embeddings and off the shelf ELMo embeddings for a downstream unsupervised task?,"<p>I am wondering if I am using word embeddings correctly.</p>
<p>I have combined contextualised word vectors with static word vectors because:</p>
<ul>
<li>my domain corpus is too small to effectively train the model from scratch</li>
<li>my domain is too specialised to use general embeddings.</li>
</ul>
<p>I used the off the shelf ELMo small model and trained word2vec model on a small domain specific corpus (around 500 academic papers). I then did a simple concatenation of the vectors from the two different embeddings.</p>
<p>I loosely followed the approach in this paper:
<a href=""https://www.aclweb.org/anthology/P19-2041.pdf"" rel=""nofollow noreferrer"">https://www.aclweb.org/anthology/P19-2041.pdf</a>
But the approach in the paper trains the embeddings for a specific task. In my domain there is no labeled training data. Hence me just training the embeddings on the corpus alone.</p>
<p>I am new to NLP, so apologies if I am asking a stupid question.</p>
",Training and Model Evaluation,ok combine domain specific word vec embeddings shelf elmo embeddings downstream unsupervised task wondering using word embeddings correctly combined contextualised word vector static word vector domain corpus small effectively train model scratch domain specialised use general embeddings used shelf elmo small model trained word vec model small domain specific corpus around academic paper simple concatenation vector two different embeddings loosely followed approach paper approach paper train embeddings specific task domain labeled training data hence training embeddings corpus alone new nlp apology asking stupid question
Why vector normalization can improve the accuracy of clustering and classification?,"<p>It is described in Mahout in Action that normalization can slightly improve the accuracy.
Can anyone explain the reason, thanks!</p>
",Training and Model Evaluation,vector normalization improve accuracy clustering classification described mahout action normalization slightly improve accuracy anyone explain reason thanks
How to train a textual entailment model with my own training set?,"<p>I'd like to train the <a href=""https://demo.allennlp.org/textual-entailment"" rel=""nofollow noreferrer"">decomposable attention + ELMo; SNLI</a> model on the demo with my own dataset. I'm new to nlp. After going through the <a href=""https://guide.allennlp.org/"" rel=""nofollow noreferrer"">guide</a>, I still have no idea of how to start off with my own training set consisting of plain text premise, hypothesis, and label. The data format is displayed below.</p>
<p>Based on the training command on demo, I found its training set is <code>https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_train.jsonl</code>. How can I generate such a training set with my own data?</p>
<p>FYI.
my dataset is like:</p>
<pre><code>{ &quot;premise&quot;:&quot;sentences&quot;, &quot;hypothesis&quot;:&quot;sentences&quot;, &quot;label&quot;:&quot;x&quot;}
{ &quot;premise&quot;:&quot;sentences&quot;, &quot;hypothesis&quot;:&quot;sentences&quot;, &quot;label&quot;:&quot;y&quot;}
...
</code></pre>
<p>The entry in <code>snli_1.0_train.jsonl</code> is like:</p>
<pre><code>{&quot;annotator_labels&quot;: [&quot;neutral&quot;], &quot;captionID&quot;: &quot;3416050480.jpg#4&quot;, &quot;gold_label&quot;: &quot;neutral&quot;, &quot;pairID&quot;: &quot;3416050480.jpg#4r1n&quot;, &quot;sentence1&quot;: &quot;A person on a horse jumps over a broken down airplane.&quot;, &quot;sentence1_binary_parse&quot;: &quot;( ( ( A person ) ( on ( a horse ) ) ) ( ( jumps ( over ( a ( broken ( down airplane ) ) ) ) ) . ) )&quot;, &quot;sentence1_parse&quot;: &quot;(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN on) (NP (DT a) (NN horse)))) (VP (VBZ jumps) (PP (IN over) (NP (DT a) (JJ broken) (JJ down) (NN airplane)))) (. .)))&quot;, &quot;sentence2&quot;: &quot;A person is training his horse for a competition.&quot;, &quot;sentence2_binary_parse&quot;: &quot;( ( A person ) ( ( is ( ( training ( his horse ) ) ( for ( a competition ) ) ) ) . ) )&quot;, &quot;sentence2_parse&quot;: &quot;(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) (VP (VBG training) (NP (PRP$ his) (NN horse)) (PP (IN for) (NP (DT a) (NN competition))))) (. .)))&quot;}
</code></pre>
<p>I really appreciate it if anyone can help. Thanks.</p>
",Training and Model Evaluation,train textual entailment model training set like train decomposable attention elmo snli model demo dataset new nlp going guide still idea start training set consisting plain text premise hypothesis label data format displayed based training command demo found training set generate training set data fyi dataset like entry like really appreciate anyone help thanks
"Train loss is decreasing, but accuracy remain the same","<p>this is the train and development cell for multi-label classification task using Roberta (BERT). the first part is training and second part is development (validation). train_dataloader is my train dataset and dev_dataloader is development dataset. my question is: why train loss is decreasing step by step, but accuracy doesn't increase so much? practically, accuracy is increasing until iterate 4, but train loss is decreasing until the last epoch (iterate). is this ok or there should be a problem?</p>
<pre><code>train_loss_set = []
iterate = 4
for _ in trange(iterate, desc=&quot;Iterate&quot;):
  model.train()

  train_loss = 0 
  nu_train_examples, nu_train_steps = 0, 0
  
  for step, batch in enumerate(train_dataloader):
    batch = tuple(t.to(device) for t in batch)
    batch_input_ids, batch_input_mask, batch_labels = batch
    optimizer.zero_grad()
    output = model(batch_input_ids, attention_mask=batch_input_mask)
    logits = output[0]
    loss_function = BCEWithLogitsLoss() 
    loss = loss_function(logits.view(-1,num_labels),batch_labels.type_as(logits).view(-1,num_labels))
    train_loss_set.append(loss.item())    
    loss.backward()
    optimizer.step()
    train_loss += loss.item()
    nu_train_examples += batch_input_ids.size(0)
    nu_train_steps += 1

  print(&quot;Train loss: {}&quot;.format(train_loss/nu_train_steps))

###############################################################################

  model.eval()
  logits_pred,true_labels,pred_labels,tokenized_texts = [],[],[],[]

  # Predict
  for i, batch in enumerate(dev_dataloader):
    batch = tuple(t.to(device) for t in batch)
    batch_input_ids, batch_input_mask, batch_labels = batch
    with torch.no_grad():
      out = model(batch_input_ids, attention_mask=batch_input_mask)
      batch_logit_pred = out[0]
      pred_label = torch.sigmoid(batch_logit_pred)
      batch_logit_pred = batch_logit_pred.detach().cpu().numpy()
      pred_label = pred_label.to('cpu').numpy()
      batch_labels = batch_labels.to('cpu').numpy()

    tokenized_texts.append(batch_input_ids)
    logits_pred.append(batch_logit_pred)
    true_labels.append(batch_labels)
    pred_labels.append(pred_label)

  pred_labels = [item for sublist in pred_labels for item in sublist]
  true_labels = [item for sublist in true_labels for item in sublist]
  threshold = 0.4
  pred_bools = [pl&gt;threshold for pl in pred_labels]
  true_bools = [tl==1 for tl in true_labels]
  
  print(&quot;Accuracy is: &quot;, jaccard_score(true_bools,pred_bools,average='samples'))
torch.save(model.state_dict(), 'bert_model')
</code></pre>
<p>and the outputs:</p>
<pre><code>Iterate:   0%|          | 0/10 [00:00&lt;?, ?it/s]

Train loss: 0.4024542534684801

/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Accuracy is:  0.5806403013182674

Iterate:  10%|█         | 1/10 [03:21&lt;30:14, 201.64s/it]

Train loss: 0.2972540049911379
Accuracy is:  0.6091337099811676

Iterate:  20%|██        | 2/10 [06:49&lt;27:07, 203.49s/it]

Train loss: 0.26178574864264137
Accuracy is:  0.608361581920904

Iterate:  30%|███       | 3/10 [10:17&lt;23:53, 204.78s/it]

Train loss: 0.23612180122962365
Accuracy is:  0.6096717783158462

Iterate:  40%|████      | 4/10 [13:44&lt;20:33, 205.66s/it]

Train loss: 0.21416303515434265
Accuracy is:  0.6046892655367231

Iterate:  50%|█████     | 5/10 [17:12&lt;17:11, 206.27s/it]

Train loss: 0.1929110718982203
Accuracy is:  0.6030885122410546

Iterate:  60%|██████    | 6/10 [20:40&lt;13:46, 206.74s/it]

Train loss: 0.17280191068465894
Accuracy is:  0.6003766478342749

Iterate:  70%|███████   | 7/10 [24:08&lt;10:21, 207.04s/it]

Train loss: 0.1517329115446631
Accuracy is:  0.5864783427495291

Iterate:  80%|████████  | 8/10 [27:35&lt;06:54, 207.23s/it]

Train loss: 0.12957811209705325
Accuracy is:  0.5818832391713747

Iterate:  90%|█████████ | 9/10 [31:03&lt;03:27, 207.39s/it]

Train loss: 0.11256680189521162
Accuracy is:  0.5796045197740114

Iterate: 100%|██████████| 10/10 [34:31&lt;00:00, 207.14s/it]
</code></pre>
",Training and Model Evaluation,train loss decreasing accuracy remain train development cell multi label classification task using roberta bert first part training second part development validation train dataloader train dataset dev dataloader development dataset question train loss decreasing step step accuracy increase much practically accuracy increasing iterate train loss decreasing last epoch iterate ok problem output
Multi class classification using Bi-LSTM and glove,"<p>I am doing multi class classification using BI LSTM and glove embeddings, when I train my model, on the prediction (model.predict) I get not proper results as below, the results are not between 0 and 1, can anyone help me please?</p>
<p>I also used one hot encoded.<a href=""https://i.sstatic.net/vy7CF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vy7CF.png"" alt=""enter image description here"" /></a></p>
<pre><code>
3916/3916 [==============================] - 17s 4ms/step
[[9.9723792e-01 1.6954101e-03 1.0665554e-03]
 [1.6794224e-01 8.6485274e-02 7.4557245e-01]
 [9.4370516e-03 1.0848863e-03 9.8947805e-01]
 ...
 [1.3264662e-02 9.7078091e-01 1.5954463e-02]
 [1.2019513e-02 9.8711687e-01 8.6356810e-04]
 [8.1863362e-01 1.5828104e-01 2.3085352e-02]]

</code></pre>
<pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,Dense, Dropout,Bidirectional
from tensorflow.keras.layers import SpatialDropout1D
from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.text import Tokenizer


embedding_vector_length = 100

model_2 = Sequential()

model_2.add(Embedding(len(tokenizer.word_index) + 1, embedding_vector_length,     
                                         input_length=409,name=&quot;Bi-LSTM&quot;) )

model_2.add(SpatialDropout1D(0.3))
model_2.add(Bidirectional(LSTM(64, return_sequences=False, recurrent_dropout=0.4)))
model_2.add(Dropout(0.5))
model_2.add(Dense(3,activation='softmax'))
model_2.compile(loss='categorical_crossentropy',optimizer='adam', 
                           metrics=['accuracy'])
print(model_2.summary())


model_2.layers[0].set_weights([embedding_matrix])
model_2.layers[0].trainable = False
print(model_2.summary())




from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)
history_2=model_2.fit(x_train, y_train,
 batch_size=400,
 epochs=30,
 validation_data=(x_val, y_val),
 callbacks=[es])
#We save this model so that we can use in own web app



</code></pre>
",Training and Model Evaluation,multi class classification using bi lstm glove multi class classification using bi lstm glove embeddings train model prediction model predict get proper result result anyone help please also used one hot encoded
FastText precision and recall trade-off,"<p>In FastText I want to change the balance between precision and recall. Can it be done?</p>
",Training and Model Evaluation,fasttext precision recall trade fasttext want change balance precision recall done
Using Gensim Fasttext model with LSTM nn in keras,"<p>I have trained fasttext model with Gensim over the corpus of very short sentences (up to 10 words). I know that my test set includes words that are not in my train corpus, i.e some of the words in my corpus are like &quot;Oxytocin&quot; &quot;Lexitocin&quot;, &quot;Ematrophin&quot;,'Betaxitocin&quot;</p>
<p>given a new word in the test set, fasttext knows pretty well to generate a vector with high cosine-similarity to the other similar words in the train set by using the characters level n-gram</p>
<p>How do i incorporate the fasttext model inside a LSTM keras network without losing the fasttext model to just a list of vectors in the vocab? because then I won't handle any OOV even when fasttext do it well.</p>
<p>Any idea?</p>
",Training and Model Evaluation,using gensim fasttext model lstm nn kera trained fasttext model gensim corpus short sentence word know test set includes word train corpus e word corpus like oxytocin lexitocin ematrophin betaxitocin given new word test set fasttext know pretty well generate vector high cosine similarity similar word train set using character level n gram incorporate fasttext model inside lstm kera network without losing fasttext model list vector vocab handle oov even fasttext well idea
Using Gensim Fasttext model with LSTM nn in keras,"<p>I have trained fasttext model with Gensim over the corpus of very short sentences (up to 10 words). I know that my test set includes words that are not in my train corpus, i.e some of the words in my corpus are like &quot;Oxytocin&quot; &quot;Lexitocin&quot;, &quot;Ematrophin&quot;,'Betaxitocin&quot;</p>
<p>given a new word in the test set, fasttext knows pretty well to generate a vector with high cosine-similarity to the other similar words in the train set by using the characters level n-gram</p>
<p>How do i incorporate the fasttext model inside a LSTM keras network without losing the fasttext model to just a list of vectors in the vocab? because then I won't handle any OOV even when fasttext do it well.</p>
<p>Any idea?</p>
",Training and Model Evaluation,using gensim fasttext model lstm nn kera trained fasttext model gensim corpus short sentence word know test set includes word train corpus e word corpus like oxytocin lexitocin ematrophin betaxitocin given new word test set fasttext know pretty well generate vector high cosine similarity similar word train set using character level n gram incorporate fasttext model inside lstm kera network without losing fasttext model list vector vocab handle oov even fasttext well idea
"Keras model gives: loss: 0.00, val_loss: 0.00 while training?","<p>I have been working on an NLP project for sentiment analysis. I have a movie review dataset. I was able to preprocess, vectorized, and pad the data successfully but when I train my model with the dataset my loss and validation loss stays 0.
I am sure that I did something wrong so here are some parts of the code:</p>
<pre><code>VOCAB_SIZE = 20000
EMBEDDING_DIM = 16
MAXLEN = 250
model = tf.keras.Sequential([
tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAXLEN),
tf.keras.layers.GlobalAveragePooling1D(),
tf.keras.layers.Dense(10, activation='relu'),
tf.keras.layers.Dense(1, activation='sigmoid')])
###
model.compile(loss='sparse_categorical_crossentropy',
         optimizer='adam',
         metrics=['accuracy'])
###
deneme12 = model.fit(train_PAD, train_LABEL, epochs=10,
                validation_data=(test_PAD, test_LABEL), verbose=2)
</code></pre>
<p>Here is the result of the training process:</p>
<pre><code>Epoch 1/10 1291/1291 - 10s - loss: 0.0000e+00 - val_loss: 0.0000e+00
Epoch 2/10 1291/1291 - 10s - loss: 0.0000e+00 - val_loss: 0.0000e+00
</code></pre>
",Training and Model Evaluation,kera model give loss val loss training working nlp project sentiment analysis movie review dataset wa able preprocess vectorized pad data successfully train model dataset loss validation loss stay sure something wrong part code result training process
Word vector similarity precision,"<p>I am trying to implement Gensim's <code>most_similar</code> function by hand but calculate the similarity between the query word and just one other word (avoiding the time to calculate it for the query word with <em>all</em> other words). So far I use</p>
<pre><code>cossim = (np.dot(a, b)
                   / np.linalg.norm(a)
                   / np.linalg.norm(b))
</code></pre>
<p>and this is the same as the similarity result between <code>a</code> and <code>b</code>. I find this works almost exactly but that some precision is lost, for example</p>
<pre><code>from gensim.models.word2vec import Word2Vec
import gensim.downloader as api

model_gigaword = api.load(&quot;glove-wiki-gigaword-300&quot;)

a = 'france'
b = 'chirac'

cossim1 = model_gigaword.most_similar(a)
import numpy as np
cossim2 = (np.dot(model_gigaword[a], model_gigaword[b])
                   / np.linalg.norm(model_gigaword[a])
                   / np.linalg.norm(model_gigaword[b]))
print(cossim1)
print(cossim2)
</code></pre>
<p>Output:</p>
<pre><code>[('french', 0.7344760894775391), ('paris', 0.6580672264099121), ('belgium', 0.620672345161438), ('spain', 0.573593258857727), ('italy', 0.5643460154533386), ('germany', 0.5567398071289062), ('prohertrib', 0.5564222931861877), ('britain', 0.5553334355354309), ('chirac', 0.5362644195556641), ('switzerland', 0.5320892333984375)]
0.53626436
</code></pre>
<p>So the most_similar function gives 0.53626441955... (rounds to 0.53626442) and the calculation with numpy gives 0.53626436. Similarly, you can see differences between the values for 'paris' and 'italy' (in similarity compared to 'france'). These differences suggest that the calculation is not being done to full precision (but it is in Gensim). How can I fix it and get the output for a single similarity to higher precision, exactly as it comes from most_similar?</p>
<p>TL/DR - I want to use function('france', 'chirac') and get 0.536264<strong>4195556641</strong>, not 0.536264<strong>36</strong>.</p>
<p>Any idea what's going on?</p>
<hr />
<p>UPDATE: I should clarify, I want to know and replicate how most_similar does the computation, but for only one (a,b) pair. That's my priority, rather than finding out how to improve the precision of my cossim calculation above. I just assumed the two were equivalent.</p>
",Training and Model Evaluation,word vector similarity precision trying implement gensim function hand calculate similarity query word one word avoiding time calculate query word word far use similarity result find work almost exactly precision lost example output similar function give round calculation numpy give similarly see difference value paris italy similarity compared france difference suggest calculation done full precision gensim fix get output single similarity higher precision exactly come similar tl dr want use function france chirac get idea going update clarify want know replicate similar doe computation one b pair priority rather finding improve precision cossim calculation assumed two equivalent
What&#39;s exactly ntc form in tf-idf vector space?,"<p>I have a collection of text documents. I've been asked to show each document in tf-idf vector space and in ntc form and then, train a svm model based on documents' vectors in python. What does ntc exactly mean here?</p>
<p>I Found that it's the same as tf-idf weights with one step of normalization which is called &quot;cosine normalization&quot;. But i can't find information about such thing. I found &quot;cosine similarity&quot; which is in my idea different from &quot;cosine normalization&quot;. Are they the same? And how can i create this vector in python?</p>
",Training and Model Evaluation,exactly ntc form tf idf vector space collection text document asked show document tf idf vector space ntc form train svm model based document vector python doe ntc exactly mean found tf idf weight one step normalization called cosine normalization find information thing found cosine similarity idea different cosine normalization create vector python
Does allennlp textual entailment model work when hypothesis and premise both involve multiple sentences?,"<p>On allennlp textual entailment <a href=""https://demo.allennlp.org/textual-entailment"" rel=""nofollow noreferrer"">demo website</a>, the hypothesis and premise in examples always only consist of one sentence. Does allennlp textual entailment model work when hypothesis and premise both include multiple sentences? Is it theoretically practical? Or could I train the model on my own labeled dataset to make it work on paragraph texts?</p>
<p>For example:</p>
<ul>
<li>Premise: &quot;Whenever Jack is asked whether he prefers mom or dad, he doesn't know how to respond. To be honest, he has no idea why he has to make a choice. &quot;</li>
<li>Hypothesis: &quot;Whom do you love more, mom or dad? Some adults like to use this question to tease kids. For Jack, he doesn't like this question.&quot;</li>
</ul>
<p>I read the paper <a href=""https://www.semanticscholar.org/paper/A-Decomposable-Attention-Model-for-Natural-Language-Parikh-T%C3%A4ckstr%C3%B6m/2cd8e8f510c89c7c18268e8ad51c061e459ad321"" rel=""nofollow noreferrer"">decomposable attention model (Parikh et al, 2017)</a>. This paper doesn't discuss such a scenario. The idea behind the paper is text alignment. So intuitively, I think it should also be reasonable to work on paragraph texts. But I'm not very confident about it.</p>
<p>I sincerely appreciate it if anyone can help with it.</p>
",Training and Model Evaluation,doe allennlp textual entailment model work hypothesis premise involve multiple sentence allennlp textual entailment demo website hypothesis premise example always consist one sentence doe allennlp textual entailment model work hypothesis premise include multiple sentence theoretically practical could train model labeled dataset make work paragraph text example premise whenever jack asked whether prefers mom dad know respond honest ha idea ha make choice hypothesis love mom dad adult like use question tease kid jack like question read paper decomposable attention model parikh et al paper discus scenario idea behind paper text alignment intuitively think also reasonable work paragraph text confident sincerely appreciate anyone help
How to train semantic role labeling with allennlp?,"<p>Where is the training config file? The link on this page is dead <a href=""https://demo.allennlp.org/semantic-role-labeling"" rel=""nofollow noreferrer"">https://demo.allennlp.org/semantic-role-labeling</a></p>
<blockquote>
<p>Training
The SRL model was evaluated on the CoNLL 2012 dataset. Unfortunately we cannot release this data due to licensing restrictions by the LDC. You can put together evaluation data yourself by following the CoNLL 2012 instructions for working with the data. Once you have compiled the dataset, you can use the configuration file at <a href=""https://github.com/allenai/allennlp/blob/master/training_config/semantic_role_labeler.jsonnet"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp/blob/master/training_config/semantic_role_labeler.jsonnet</a> to train.</p>
</blockquote>
",Training and Model Evaluation,train semantic role labeling allennlp training config file link page dead training srl model wa evaluated conll dataset unfortunately release data due licensing restriction ldc put together evaluation data following conll instruction working data compiled dataset use configuration file train
How to improve Allen NLP question answering performance,"<p>I am trying out Allen NLP pre-trained models for Q&amp;A.</p>
<p>The online demo is here : <a href=""https://demo.allennlp.org/reading-comprehension"" rel=""nofollow noreferrer"">https://demo.allennlp.org/reading-comprehension</a></p>
<p>I have created a python script to try out various models.</p>
<ul>
<li><a href=""https://gist.github.com/sujee/a22f6fa11065791314d5bfede434b516#file-qa-allen-nlp-py"" rel=""nofollow noreferrer"">python script</a></li>
<li><a href=""https://gist.github.com/sujee/a22f6fa11065791314d5bfede434b516#file-output"" rel=""nofollow noreferrer"">script output</a></li>
</ul>
<p>Here is the benchmark summary on my laptop</p>
<ul>
<li>Macbook Pro (2017)</li>
<li>2.9 Ghz Intel i7 quad-core</li>
<li>16 G memory</li>
</ul>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Benchmark</th>
<th>transformer-qa</th>
<th>bidaf-model</th>
<th>bidaf-elmo-model</th>
</tr>
</thead>
<tbody>
<tr>
<td>loading time</td>
<td>31.6 seconds</td>
<td>1.6 seconds</td>
<td>13.8 seconds</td>
</tr>
<tr>
<td>questions</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Who stars in The Matrix?</td>
<td>794  ms</td>
<td>62  ms</td>
<td>1,798 ms</td>
</tr>
<tr>
<td>where does polar bear live</td>
<td>2,211 ms</td>
<td>96 ms</td>
<td>7,125 ms</td>
</tr>
<tr>
<td>how much does a polar bear weigh</td>
<td>2,435 ms</td>
<td>98 ms</td>
<td>7,082 ms</td>
</tr>
<tr>
<td>what is lightning</td>
<td>1,361  ms</td>
<td>69 ms</td>
<td>3,173 ms</td>
</tr>
<tr>
<td>How many lightning bolts strike earth</td>
<td>1,019  ms</td>
<td>47 ms</td>
<td>2,885 ms</td>
</tr>
</tbody>
</table>
</div>
<p>Looking at the <a href=""https://gist.github.com/sujee/a22f6fa11065791314d5bfede434b516#file-output"" rel=""nofollow noreferrer"">output</a>  I can see all 3 models are providing good answers.  I like the <code>transformer-qa</code> model but it takes a while (in the order of seconds) to predict.</p>
<p>Is there a way to speed up prediction times?</p>
<p>thanks!</p>
",Training and Model Evaluation,improve allen nlp question answering performance trying allen nlp pre trained model q online demo created python script try various model python script script output benchmark summary laptop macbook pro ghz intel quad core g memory benchmark transformer qa bidaf model bidaf elmo model loading time second second second question star matrix doe polar bear live much doe polar bear weigh lightning many lightning bolt strike earth looking output see model providing good answer like model take order second predict way speed prediction time thanks
Why FastText is not handling finding multi-word phrases?,"<p>FastText pre-trained model works great for finding similar words:</p>
<pre class=""lang-py prettyprint-override""><code>from pyfasttext import FastText
model = FastText('cc.en.300.bin')
model.nearest_neighbors('dog', k=2000)

[('dogs', 0.8463464975357056),
 ('puppy', 0.7873005270957947),
 ('pup', 0.7692237496376038),
 ('canine', 0.7435278296470642),
 ...
</code></pre>
<p>However, it seems to fail for multi-word phrases, e.g.:</p>
<pre class=""lang-py prettyprint-override""><code>model.nearest_neighbors('Gone with the Wind', k=2000)

[('DEky4M0BSpUOTPnSpkuL5I0GTSnRI4jMepcaFAoxIoFnX5kmJQk1aYvr2odGBAAIfkECQoABAAsCQAAABAAEgAACGcAARAYSLCgQQEABBokkFAhAQEQHQ4EMKCiQogRCVKsOOAiRocbLQ7EmJEhR4cfEWoUOTFhRIUNE44kGZOjSIQfG9rsyDCnzp0AaMYMyfNjS6JFZWpEKlDiUqALJ0KNatKmU4NDBwYEACH5BAUKAAQALAkAAAAQABIAAAhpAAEQGEiQIICDBAUgLEgAwICHAgkImBhxoMOHAyJOpGgQY8aBGxV2hJgwZMWLFTcCUIjwoEuLBym69PgxJMuDNAUqVDkz50qZLi',
  0.71047443151474),
</code></pre>
<p>or</p>
<pre class=""lang-py prettyprint-override""><code>model.nearest_neighbors('Star Wars', k=2000)
[('clockHauser', 0.5432934761047363),
 ('CrônicasEsdrasNeemiasEsterJóSalmosProvérbiosEclesiastesCânticosIsaíasJeremiasLamentaçõesEzequielDanielOséiasJoelAmósObadiasJonasMiquéiasNaumHabacuqueSofoniasAgeuZacariasMalaquiasNovo',
  0.5197194218635559),
</code></pre>
<p>Is it a limitation of FastText pre-trained models?</p>
",Training and Model Evaluation,fasttext handling finding multi word phrase fasttext pre trained model work great finding similar word however seems fail multi word phrase e g limitation fasttext pre trained model
Latency issue with Tensorflow cuDNN model execution,"<p>I am having problems with a cuDNN RNN model I am trying to train on a set of natural language explanations and embeddings for the semantic parsing of texts. Here is what my RNN model architecture looks like on a simplified level:</p>

<pre><code>class Cudnn_RNN:

    def __init__(self, num_layers, num_units, mode=""lstm"", keep_prob=1.0, is_train=None, scope=""cudnn_rnn""):
        self.num_layers = num_layers
        self.rnns = []
        self.mode = mode
        if mode == ""gru"":
            rnn = tf.contrib.cudnn_rnn.CudnnGRU
        elif mode == ""lstm"":
            rnn = tf.contrib.cudnn_rnn.CudnnLSTM
        else:
            raise Exception(""Unknown mode for rnn"")
        for layer in range(num_layers):
            rnn_fw = rnn(1, num_units)
            rnn_bw = rnn(1, num_units)
            self.rnns.append((rnn_fw, rnn_bw, ))

    def __call__(self, inputs, seq_len, keep_prob=1.0, is_train=None, concat_layers=True):
        outputs = [tf.transpose(inputs, [1, 0, 2])]
        for layer in range(self.num_layers):
            rnn_fw, rnn_bw = self.rnns[layer]
            output = dropout(outputs[-1], keep_prob=keep_prob, is_train=is_train)
            with tf.variable_scope(""fw_{}"".format(layer)):
                out_fw, state_fw = rnn_fw(output)
            with tf.variable_scope(""bw_{}"".format(layer)):
                inputs_bw = tf.reverse_sequence(output, seq_lengths=seq_len, seq_axis=0, batch_axis=1)
                out_bw, state_bw = rnn_bw(inputs_bw)
                out_bw = tf.reverse_sequence(out_bw, seq_lengths=seq_len, seq_axis=0, batch_axis=1)
            outputs.append(tf.concat([out_fw, out_bw], axis=2))
        if concat_layers is True:
            res = tf.concat(outputs[1:], axis=2)
        else:
            res = outputs[-1]
        res = tf.transpose(res, [1, 0, 2])
        state_fw = tf.squeeze(state_fw[0], [0])
        state_bw = tf.squeeze(state_bw[0], [0])
        state = tf.concat([state_fw, state_bw], axis=1)
        return res, state
</code></pre>

<p>The model is set up such that after data is loaded, it goes through pretraining, training, and then evaluation. For some reason the data is being loaded with no issues, but as soon as the model starts running it gets stuck, not even making it to the pretraining phase. Here is the data loading and execution code (model executes up until just before <code>print('---Pretrain-----')</code>:</p>

<pre><code>def pseudo_labeling(config, data):
    word2idx_dict, fixed_emb, traiable_emb, train_data, dev_data, test_data,pretrain_data,pretrain_data2 = data

    pretrain_test_data = (pretrain_data[0][:config.pretrain_test_size],pretrain_data[1][:config.pretrain_test_size],pretrain_data[2][:config.pretrain_test_size,:])
    pretrain_data = (pretrain_data[0][config.pretrain_test_size:config.pretrain_test_size+config.pretrain_train_size],pretrain_data[1][config.pretrain_test_size:config.pretrain_test_size+config.pretrain_train_size],pretrain_data[2][config.pretrain_test_size:config.pretrain_test_size+config.pretrain_train_size,:])

    lfs = get_lfs(config, word2idx_dict)
    identifier = ""_{}"".format(config.tag)

    with tf.variable_scope(""models"", reuse=tf.AUTO_REUSE):
        regex = Pat_Match(config)
        match = Soft_Match(config,lfs['lfs'],np.array(lfs['rels'],np.float32),lfs['keywords'],lfs['keywords_rels'], lfs['raw_keywords'],mat=((fixed_emb, traiable_emb, )), word2idx_dict=word2idx_dict, pseudo=True)

    sess_config = tf.ConfigProto(allow_soft_placement=True)
    sess_config.gpu_options.allow_growth = True
    if os.path.exists('labeled_data.pkl'):
        with open('labeled_data.pkl', 'rb') as f:
            labeled_data = pickle.load(f)
        with open('unlabeled_data.pkl', 'rb') as f:
            unlabeled_data = pickle.load(f)
        with open('weights.pkl', 'rb') as f:
            lfs[""weights""] = pickle.load(f)
    else:
        with open('exp2pat.json','r') as f:
            exp2pat = json.load(f)
        exp2pat = {int(key):val for key,val in exp2pat.items()}
        lab_d = []
        unlab_d = []

        tacred_labeled = []
        tacred_unlabeled = []
        labeled_data = []
        unlabeled_data = []
        idxx = -1

        idx2rel = {val:key for key,val in constant.LABEL_TO_ID.items()}

        for x in tqdm(train_data):
            idxx+=1
            batch = [x[""phrase""]]
            res, pred = regex.match(batch)
            lfs[""weights""] += res[0]
            new_dict = {}
            if np.amax(res) &gt; 0:

                x[""rel""] = pred.tolist()[0]
                x[""logic_form""] = np.argmax(res, axis=1).tolist()[0]
                new_dict['tokens'] = x['phrase'].token
                new_dict['start'] = min(x['phrase'].subj_posi,x['phrase'].obj_posi)+1
                new_dict['end'] = max(x['phrase'].subj_posi,x['phrase'].obj_posi)-1
                new_dict['rel'] = pred.tolist()[0]
                try:
                    new_dict['pat'] = exp2pat[np.argmax(res, axis=1).tolist()[0]]
                    lab_d.append(new_dict)
                except:
                    new_dict['pat'] = -1
                    unlab_d.append(new_dict)
                tacred_labeled.append((idxx,idx2rel[x['rel']]))
                labeled_data.append(x)
            else:
                tacred_unlabeled.append(idxx)
                new_dict['tokens'] = x['phrase'].token
                new_dict['start'] = min(x['phrase'].subj_posi,x['phrase'].obj_posi)+1
                new_dict['end'] = max(x['phrase'].subj_posi,x['phrase'].obj_posi)-1
                new_dict['rel'] = pred.tolist()[0]
                new_dict['pat']=-1
                x[""rel""] = 0
                unlab_d.append(new_dict)
                unlabeled_data.append(x)

        new_weight = np.array([elem for i, elem in enumerate(list(lfs['weights'])) if i in exp2pat],np.float32)
        new_weight = new_weight/np.sum(new_weight)
        lfs[""weights""] = lfs[""weights""] / np.sum(lfs[""weights""])

        with open('tacred_labeled.json','w') as f:
            json.dump(tacred_labeled,f)

        with open('tacred_unlabeled.json','w') as f:
            json.dump(tacred_unlabeled,f)

        with open('labeled_data.pkl','wb') as f:
            pickle.dump(labeled_data,f)
        with open('unlabeled_data.pkl','wb') as f:
            pickle.dump(unlabeled_data,f)
        with open('weights.pkl', 'wb') as f:
            pickle.dump(lfs[""weights""], f)

        with open('lab_d.pkl','wb') as f:
            pickle.dump(lab_d,f)
        with open('unlab_d.pkl','wb') as f:
            pickle.dump(unlab_d,f)
        with open('weights_d.pkl','wb') as f:
            pickle.dump(new_weight,f)

    random.shuffle(unlabeled_data)

    print('unlabdel data:',str(len(unlabeled_data)),'labeled data:',str(len(labeled_data)))

    dev_history, test_history = [], []
    dev_history2, test_history2 = [], []

    with tf.Session(config=sess_config) as sess:

        lr = float(config.init_lr)
        writer = tf.summary.FileWriter(config.log_dir + identifier)
        sess.run(tf.global_variables_initializer())

        print('---Pretrain-----')
        for epoch in range(config.pretrain_epoch):
            loss_list,pretrain_loss_lis,sim_loss_lis = [],[],[]
            for batch in get_pretrain_batch(config, pretrain_data, word2idx_dict):
                pretrain_loss_prt,sim_loss_prt,loss,_ = sess.run([match.pretrain_loss,match.sim_loss,match.pretrain_loss_v2,match.pre_train_op],feed_dict={match.pretrain_sents: batch['sents'], match.pretrain_pats: batch['pats'],match.pretrain_labels: batch['labels'],match.is_train:True})
                loss_list.append(loss)
                pretrain_loss_lis.append(pretrain_loss_prt)
                sim_loss_lis.append(sim_loss_prt)
            print(""{} epoch:"".format(str(epoch)))
            print(""loss:{} pretrain_loss:{} sim_loss:{}"".format(str(np.mean(loss_list)),str(np.mean(pretrain_loss_lis)),str(np.mean(sim_loss_lis))))
            pred_labels = []
            goldens = []
            prt_id = 0
            for batch in get_pretrain_batch(config,pretrain_data2,word2idx_dict,shuffle=False):
                prt_id+=1
                pp,ppp,pred_label = sess.run([match.prt_loss,match.prt_pred,match.pretrain_pred_labels],feed_dict={match.pretrain_sents: batch['sents'], match.pretrain_pats: batch['pats'],match.is_train:False,match.pretrain_labels: batch['labels']})
                pred_label = list(pred_label)
                golden = list(np.reshape(batch['labels'],[-1]))
                assert len(golden)==len(pred_label)
                pred_labels.extend(pred_label)
                goldens.extend(golden)
            p,r,f = f_score(pred_labels,goldens)
            print('PRF:',(p,r,f))
            if p&gt;0.9 and r&gt;0.9:
                break
            print('\n')
</code></pre>

<p>Here are my system specifications:<br>
Tensorflow version: 1.14.0 (w/ GPU support)<br>
Operating System: Linux 9.12<br>
OS Distribution: Debian<br>
OS Architecture: x86_64<br>
Python version: 3.7.6<br>
NLTK version: 3.4.5<br>
CUDA version: 10.0<br>
cuDNN version: 7.4.2<br>
NVIDIA graphics card: Tesla T4<br>
NVIDIA driver version: 410.104<br>
Compiler version: GCC 6.3.0<br></p>

<p>If anyone would like to share their thoughts on why my model is unable to properly execute to the pretraining phase and beyond, I would greatly appreciate it. Thank you.</p>
",Training and Model Evaluation,latency issue tensorflow cudnn model execution problem cudnn rnn model trying train set natural language explanation embeddings semantic parsing text rnn model architecture look like simplified level model set data loaded go pretraining training evaluation reason data loaded issue soon model start running get stuck even making pretraining phase data loading execution code model executes system specification tensorflow version w gpu support operating system linux distribution debian architecture x python version nltk version cuda version cudnn version nvidia graphic card tesla nvidia driver version compiler version gcc anyone would like share thought model unable properly execute pretraining phase beyond would greatly appreciate thank
BERT Pre-training accuracy not increasing,"<p>I am trying to pretrain BERT on dataset (wiki103) which contains 150k sentences. After 12 epochs nsp (next sentence prediction) task gives accuracy around 0.76 (overfits if I continue with more epochs) and mlm (masked language modeling) task starts from 0.01 acc and goes till 0.2 at max. What is wrong here? Can I stop nsp at one point and continue to do mlm for longer period of time? My train loader length is 2486 (2486 training steps per epochs) which means 40*2486=99440 training steps.</p>
<p>Here is model config and training config</p>
<pre><code>class Train_Config():
&quot;&quot;&quot; Hyperparameters for training &quot;&quot;&quot;
seed: int = 391275 # random seed
batch_size: int = 64
lr: int = 1e-5 # learning rate
n_epochs: int = 40 # the number of epoch
# `warm up` period = warmup(0.1)*total_steps
# linearly increasing learning rate from zero to the specified value(5e-5)
warmup: float = 0.1
is_dibert: bool = False


class Model_Config():
vocab_size: int = 30522  # Size of Vocabulary
hidden_size: int = 768  # Dimension of Hidden Layer in Transformer Encoder
num_hidden_layers: int = 8  # Numher of Hidden Layers
num_attention_heads: int = 8  # Numher of Heads in Multi-Headed Attention Layers
intermediate_size: int = 768 * 4  # Dimension of Intermediate Layers in Positionwise Feedforward Net
# activ_fn: str = &quot;gelu&quot; # Non-linear Activation Function Type in Hidden Layers
max_len: int = 312  # Maximum Length for Positional Embeddings
n_segments: int = 2  # Number of Sentence Segments
attention_probs_dropout_prob: int = 0.1
</code></pre>
",Training and Model Evaluation,bert pre training accuracy increasing trying pretrain bert dataset wiki contains k sentence epoch nsp next sentence prediction task give accuracy around overfits continue epoch mlm masked language modeling task start acc go till max wrong stop nsp one point continue mlm longer period time train loader length training step per epoch mean training step model config training config
Parallelizing while loop,"<p>I need to parallelize a while loop in R, in order to gain better performance. Is this always possible? In my case, I need to compute BTM models and their coherence in order to evaluate different models. THe loop is extremely simple:</p>
<pre><code>  while(i&lt;=to)
  {
    row=row+1
    df$Topic[row]&lt;-i
    BTM_model&lt;-BTM(data=token, i)
    df$MeanCoherence&lt;-mean(coherenceBTM(model = BTM_model, DTM, N=15))
    i=i+by
  }
</code></pre>
<p>I don't wan this to seem like a homework, so just an explanation about how to implement this would be extremely useful!</p>
",Training and Model Evaluation,parallelizing loop need parallelize loop r order gain better performance always possible case need compute btm model coherence order evaluate different model loop extremely simple wan seem like homework explanation implement would extremely useful
Python. Gensim Word2vec. Words similarity,"<p>I've got a problem/question with <strong>Word2Vec</strong></p>

<p>As I understand: let's train a model on a corpus of text (in my way it's a corpus ~2 Gb size)
Let's take one line from this text and calculate a vector of this line (line's vector = sum of words vectors). It will be smth. like this:</p>

<pre><code>for w in words:
    coords += model[w]
</code></pre>

<p>Than let's calculate length of this vector. With standard library as:</p>

<pre><code>import numpy as np
vectorLen = np.linalg.norm(coords)
</code></pre>

<p>Why do we need Word2Vec? Yes, for converting words to vectors <strong>AND</strong> contextual proximity (near words that are found and words that are close in meaning have similar coordinates)!</p>

<p>And what I want (what I am waiting) - if I will take some line of the text and add some word from the dictionary which is not typical for this line, than again calculate length of this vector, I will get quite different value that if I will calculate only vector of this line without adding some uncharacteristic words to this line from dictionary.</p>

<p>But in fact - the values of this vectors (before adding word(s) and after) are quite the similar! Moreover - they are practically the same! Why am I getting this result? 
If I understand right for the line the coordinates of words will quite the same (contextual proximity), but new words will have rather different coordinates and it should affect to result (vector length of line with new words)!</p>

<p>E.x. it's my W2V model settings:</p>

<pre><code>#Word2Vec model

model = gensim.models.Word2Vec(
    sg=0,
    size=300,
    window=3,
    min_count=1,
    hs=0,
    negative=5,
    workers=10,
    alpha=0.025,
    min_alpha=0.025,
    sample=1e-3,
    iter=20
)

#prepare the model vocabulary
model.build_vocab(sentences, update=False)

#train model
model.train(sentences, epochs=model.iter, total_examples=model.corpus_count)
</code></pre>

<p>OR this:</p>

<pre><code>#Word2Vec model

model = gensim.models.Word2Vec(
    sg=1,
    size=100,
    window=10,
    min_count=1,
    hs=0,
    negative=5,
    workers=10,
    alpha=0.025,
    min_alpha=0.025,
    seed=7,
    sample=1e-3,
    hashfxn=hash,
    iter=20
)

#prepare the model vocabulary
model.build_vocab(sentences, update=False)
</code></pre>

<p>What's the problem? And how can I get necessary result? </p>
",Training and Model Evaluation,python gensim word vec word similarity got problem question word vec understand let train model corpus text way corpus gb size let take one line text calculate vector line line vector sum word vector smth like let calculate length vector standard library need word vec yes converting word vector contextual proximity near word found word close meaning similar coordinate want waiting take line text add word dictionary typical line calculate length vector get quite different value calculate vector line without adding uncharacteristic word line dictionary fact value vector adding word quite similar moreover practically getting result understand right line coordinate word quite contextual proximity new word rather different coordinate affect result vector length line new word e x w v model setting problem get necessary result
How to predict Y_train with two different X_train data,"<p>This is my first time working with python on machine learning.</p>
<p>I have a dataset that I split into a training dataset of length of <code>9000</code> and a test dataset of a length of <code>1000</code>.
I would like to predict articles categories (Y) by articles titles (X1) and abstracts (X2)
<code>X_train_pad1, X_test_pad1</code> (Titles), <code>X_train_pad2,X_test_pad2</code> (Abstracts), <code>Y_train_id</code> and <code>Y_test_id</code> (Categories) are all of type <code>numpy.ndarray</code>,
Their shape are respectively <code>(9000, 36),(1000, 36),(9000, 1446),(1000, 1446),(9000,) and (1000,)</code></p>
<p>Below are the codes for the prediction of Y_train_id (i.e. Categories) by X_train_pad1 (i.e. Titles)
and its evaluation</p>
<pre><code>embed_dim= 128*2
dropout1 = 0.5
conv_filters= 128
conv_kernel = 2
maxpool_size = 2
dense_size = 128
batch_size = 128
epochs = 7 
vocab_size=5000
num_cat=96 #(number of categories)

model_cnn = tf.keras.models.Sequential()
# Embedding 
model_cnn.add(tf.keras.layers.Embedding(vocab_size,embed_dim, input_length= max_len))
# Dropout
model_cnn.add(tf.keras.layers.Dropout(dropout1))
# Convolution 1
model_cnn.add(tf.keras.layers.Conv1D(conv_filters, conv_kernel,padding='valid', strides= 1,activation='relu'))
# Maxpooling
model_cnn.add(tf.keras.layers.MaxPooling1D(maxpool_size))

#Flatten
model_cnn.add(tf.keras.layers.Flatten())
# Dense+ Activation
model_cnn.add(tf.keras.layers.Dense(dense_size,  activation='relu'))

# Classifieur (Dense + activation softmax)
model_cnn.add(tf.keras.layers.Dense(num_cat))
model_cnn.add(tf.keras.layers.Activation('softmax'))

# Compiler le modèle 
model_cnn.compile(loss='sparse_categorical_crossentropy', 
                  optimizer='adam', 
                  metrics= ['accuracy'])
# Afficher le summary du modèle
print(model_cnn.summary())

#Model fitting
model_cnn.fit(X_train_pad1, Y_train_id, batch_size = batch_size , epochs = epochs)

#model evaluation
model_cnn.evaluate(X_test_pad1, Y_test_id)
</code></pre>
<p>I would like to fit a model (with 2 two inputs) that allows me to predict categories by titles <code>(X_train_pad1)</code> and abstract <code>(X_train_pad2)</code> at the same time and to evaluate it.</p>
",Training and Model Evaluation,predict train two different x train data first time working python machine learning dataset split training dataset length test dataset length would like predict article category article title x abstract x title abstract category type shape respectively code prediction train id e category x train pad e title evaluation would like fit model two input allows predict category title abstract time evaluate
Best trained model not correctly saving with pytorch,"<p>i want to save the best model when training but it keeps saving the first trained model</p>
<pre><code>#Train the model for 4 epochs
from collections import defaultdict
#Define history and best accuracy
history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):
    
    print(f'Epoch: {epoch + 1} / {EPOCHS}')
    print('-' * 15)

    #Call the train epoch method
    train_acc, train_loss = train_epoch(model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train))

    #Print the loss and accuracy
    print(f'Train loss: {train_loss}, Train accuracy: {train_acc}')


    #Call the val epoch method
    val_acc, val_loss = eval_model(model, val_data_loader, loss_fn, device, len(df_val))

    #Print the validation loss and accuracy
    print(f'Validation loss: {val_loss}, Validation accuracy: {val_acc}')


    #Update the history dictionary
    history['train_acc'].append(train_acc)
    history['train_loss'].append(train_loss)
    history['val_acc'].append(val_acc)
    history['val_loss'].append(val_loss)

    if val_acc &gt; best_accuracy:
        torch.save(model.state_dict(), 'best_model_state.bin')
        best_accuracy = val_acc
</code></pre>
<p>this is the code for loading the best model file and i get the accuracy of the first model</p>
<pre><code> model.load_state_dict(torch.load(&quot;/content/best_model_state.bin&quot;))
test_accuracy = eval_model(model, test_data_loader, loss_fn, device, len(df_test))
print(&quot;Test accuracy: &quot;, test_accuracy[0].item()) 
</code></pre>
",Training and Model Evaluation,best trained model correctly saving pytorch want save best model training keep saving first trained model code loading best model file get accuracy first model
one_hot Vs Tokenizer for Word representation,"<p>I have seen in many blogs , people using one_hot (from tf.keras.preprocessing.text.one_hot ) to convert the string of words into array of numbers which represent indices. This does not ensure unicity. Whereas Tokenizer class ensures unicity (tf.keras.preprocessing.text.Tokenizer ).
Then why is one_hot prefered over tokenizer?</p>
<p>Update: I got to know that hashing is used in One_hot to convert words into numbers but didn't get its  importance as we can use the tokenizer class to do the same thing with more accuracy.</p>
",Training and Model Evaluation,one hot v tokenizer word representation seen many blog people using one hot tf kera preprocessing text one hot convert string word array number represent index doe ensure unicity whereas tokenizer class ensures unicity tf kera preprocessing text tokenizer one hot prefered tokenizer update got know hashing used one hot convert word number get importance use tokenizer class thing accuracy
Pytorch RNN model not learning anything,"<p>Task: Predicting whether provided disaster tweets are real or not. Have already converted my textual data into tensors and then into train_loader. All the required code is mentioned below.</p>
<p><strong>My Model Architecture</strong></p>
<pre><code>class RealOrFakeLSTM(nn.Module):
    
    def __init__(self, input_size, output_size, embedding_dim, hidden_dim, n_layers, bidirec, drop_prob):
        super().__init__()
        self.output_size=output_size
        self.n_layers=n_layers
        self.hidden_dim=hidden_dim
        self.bidirec=True;
        self.embedding=nn.Embedding(vocab_size, embedding_dim)
        self.lstm1=nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True, bidirectional=bidirec)
        #self.lstm2=nn.LSTM(hidden_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)
        self.dropout=nn.Dropout(drop_prob)
        self.fc=nn.Linear(hidden_dim, output_size)
        self.sigmoid=nn.Sigmoid()
        
    def forward(self, x):
        batch=len(x)
        hidden1=self.init_hidden(batch)
        #hidden2=self.init_hidden(batch)
        embedd=self.embedding(x)
        lstm_out1, hidden1=self.lstm1(embedd, hidden1)
        #lstm_out2, hidden2=self.lstm2(lstm_out1, hidden2)
        lstm_out1=lstm_out1.contiguous().view(-1, self.hidden_dim) # make it lstm_out2, if you un comment the other lstm cell.
        out=self.dropout(lstm_out1)
        out=self.fc(out)
        sig_out=self.sigmoid(out)
        sig_out=sig_out.view(batch, -1)
        sig_out=sig_out[:, -1] 
        return sig_out
    
    def init_hidden(self, batch):
        if (train_on_gpu):
          if self.bidirec==True:
            hidden=(torch.zeros(self.n_layers*2, batch, self.hidden_dim).cuda(),torch.zeros(self.n_layers*2, batch, self.hidden_dim).cuda())
          else:
            hidden=(torch.zeros(self.n_layers, batch, self.hidden_dim).cuda(),torch.zeros(self.n_layers, batch, self.hidden_dim).cuda())
        else:
          if self.bidirec==True:
            hidden=(torch.zeros(self.n_layers*2, batch, self.hidden_dim),torch.zeros(self.n_layers*2, batch, self.hidden_dim))
          else:
            hidden=(torch.zeros(self.n_layers, batch, self.hidden_dim),torch.zeros(self.n_layers, batch, self.hidden_dim))
        return hidden
</code></pre>
<p><strong>Hyper parameters and training</strong></p>
<pre><code>learning_rate=0.005
epochs=50
vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding
output_size = 2
embedding_dim = 300
hidden_dim = 256
n_layers = 2
batch_size=23
net=RealOrFakeLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, True, 0.3)
net.to(device)
criterion=nn.BCELoss()
optimizer=torch.optim.Adam(net.parameters(),lr=learning_rate)
net.train()
loss_arr=np.array([])
lossPerEpoch=np.array([])
for i in range(epochs):
  total_loss=0;
  for input,label in train_loader:
    if train_on_gpu:
      input=input.to(device)
      label=label.to(device)
    optimizer.zero_grad()
    input=input.clone().detach().long()
    out=net(input)
    loss=criterion(out.squeeze(),label.float())
    loss_arr=np.append(loss_arr,loss.cpu().detach().numpy())
    loss.backward()
    optimizer.step()
    total_loss+=loss
  total_loss=total_loss/len(train_loader)
  lossPerEpoch=np.append(lossPerEpoch,total_loss.cpu().detach().numpy())
  print(&quot;Epoch &quot;,i,&quot;: &quot;,total_loss)
  torch.save(net.state_dict(), Path+&quot;/RealOrFakeLSTM.pt&quot;)
  torch.save(net, Path+&quot;/RealOrFakeLSTM.pth&quot;)
current_time=str(time.time())
torch.save(net.state_dict(), Path+&quot;/pt/RealOrFakeLSTM&quot;+'_pt_'+current_time+&quot;.pt&quot;)
torch.save(net, Path+&quot;/pth/RealOrFakeLSTM&quot;+'_pth_'+current_time+&quot;.pth&quot;)
</code></pre>
<p>The total loss values are all almost same, All the outcomes probabilities in the test dataset are exactly same. I am quite new to this, so hyper parameter tuning, i am kinda going with bruteforce, but nothing seems to work, I think my problem is not with the architecture but with the training part, as all the predictions are exactly same.</p>
",Training and Model Evaluation,pytorch rnn model learning anything task predicting whether provided disaster tweet real already converted textual data tensor train loader required code mentioned model architecture hyper parameter training total loss value almost outcome probability test dataset exactly quite new hyper parameter tuning kinda going bruteforce nothing seems work think problem architecture training part prediction exactly
What&#39;s the minimum number of features you need to get good results with a Naive Bayes model?,"<p>I keep on reading that Naive Bayes needs fewer features than many other ML algorithms. But what's the minimum number of features you actually need to get good results (90% accuracy) with a Naive Bayes model? I know there is no objective answer to this -- it depends on your exact features and what in particular you are trying to learn -- but I'm looking for a numerical ballpark answer to this.</p>
<p>I'm asking because I have a dataset with around 280 features and want to understand if this is way too few features to use with Naive Bayes. (I tried running Naive Bayes on my dataset and although I got 86% accuracy, I cannot trust this number as my data is imbalanced and I believe this may be responsible for the high accuracy. I am currently trying to fix this problem.)</p>
<p>In case it's relevant: the exact problem I'm working on is generating time tags for Wikipedia articles. Many times the infobox of a Wikipedia article contains a date. However, many times this date appears in the text of the article but is missing from the infobox. I want to use Naive Bayes to identify which date from all the dates we find in the article's text we should place in the infobox. Every time I find a sentence with a date in it I turn it into a feature vector -- listing what number paragraph I found this in, how many times this particular date appears in the article, etc. I've limited myself to a small subset of Wikipedia articles -- just apple articles -- and as a result, I only have 280 or so features. Any idea if this is enough data?</p>
<p>Thanks!</p>
",Training and Model Evaluation,minimum number feature need get good result naive bayes model keep reading naive bayes need fewer feature many ml algorithm minimum number feature actually need get good result accuracy naive bayes model know objective answer depends exact feature particular trying learn looking numerical ballpark answer asking dataset around feature want understand way feature use naive bayes tried running naive bayes dataset although got accuracy trust number data imbalanced believe may responsible high accuracy currently trying fix problem case relevant exact problem working generating time tag wikipedia article many time infobox wikipedia article contains date however many time date appears text article missing infobox want use naive bayes identify date date find article text place infobox every time find sentence date turn feature vector listing number paragraph found many time particular date appears article etc limited small subset wikipedia article apple article result feature idea enough data thanks
calculate two losses in a model and backpropagate twice,"<p>I'm creating a model using BertModel to identify answer span (without using BertForQA).</p>
<p>I have an indepent linear layer for determining start and end token respectively. In <strong>init</strong>():</p>
<pre><code>self.start_linear = nn.Linear(h, output_dim)

self.end_linear = nn.Linear(h, output_dim)
</code></pre>
<p>In forward(), I output a predicted start layer and predicted end layer:</p>
<pre><code> def forward(self, input_ids, attention_mask):

 outputs = self.bert(input_ids, attention_mask) # input = bert tokenizer encoding

 lhs = outputs.last_hidden_state # (batch_size, sequence_length, hidden_size)

 out = lhs[:, -1, :] # (batch_size, hidden_dim)

 st = self.start_linear(out)

 end = self.end_linear(out) 



 predict_start = self.softmax(st)

 predict_end = self.softmax(end)

 return predict_start, predict_end
</code></pre>
<p>Then in train_epoch(), I tried to backpropagate the losses separately:</p>
<pre><code>def train_epoch(model, train_loader, optimizer):

 model.train()

 total = 0

 st_loss, st_correct, st_total_loss = 0, 0, 0

 end_loss, end_correct, end_total_loss = 0, 0, 0

 for batch in train_loader:

   optimizer.zero_grad()

   input_ids = batch['input_ids'].to(device)

   attention_mask = batch['attention_mask'].to(device)

   start_idx = batch['start'].to(device)

   end_idx = batch['end'].to(device)

   start, end = model(input_ids=input_ids, attention_mask=attention_mask)


   st_loss = model.compute_loss(start, start_idx)

   end_loss = model.compute_loss(end, end_idx)

   st_total_loss += st_loss.item()

   end_total_loss += end_loss.item()

 # perform backward propagation to compute the gradients

   st_loss.backward()

   end_loss.backward()

 # update the weights

   optimizer.step() 
</code></pre>
<p>But then I got on the line of <code>end_loss.backward()</code>:</p>
<pre><code>Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
</code></pre>
<p>Am I supposed to do the backward pass separately? Or should I do it in another way? Thank you!</p>
",Training and Model Evaluation,calculate two loss model backpropagate twice creating model using bertmodel identify answer span without using bertforqa indepent linear layer determining start end token respectively init forward output predicted start layer predicted end layer train epoch tried backpropagate loss separately got line supposed backward pas separately another way thank
How to pre-train the unified language model (UniLm),"<p>I'm working on a non-English <code>NLP</code> project and for that, I need to re-train the <code>UniLm</code> on the <code>bert-base-multilingual-cased</code>, but they did not specify how to in their GitHub repository. </p>

<p>Any help is appreciated! </p>

<p>This is the link for the <a href=""https://github.com/microsoft/unilm"" rel=""nofollow noreferrer"">GitHub repository for Microsoft's UniLm</a></p>
",Training and Model Evaluation,pre train unified language model unilm working non english project need train specify github repository help appreciated link github repository microsoft unilm
How can I use fit with generator on seq2seq model,"<p>I am trying to customise <a href=""https://github.com/aravindpai/How-to-build-own-text-summarizer-using-deep-learning/blob/master/How_to_build_own_text_summarizer_using_deep_learning.ipynb"" rel=""nofollow noreferrer"">this</a> code, because the code is not efficient for training on large datasets.</p>
<p>How do I use fit generator with tokenizer, the problem here is that tokenizer is already working by scanning all text data and giving out <code>x_train</code> in one go.</p>
<p>So it all comes down to creating generator with following snippet of code which works with entire dataset.</p>
<pre><code>x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) 
x_tokenizer.fit_on_texts(list(x_tr))

#convert text sequences into integer sequences
x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) 
x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)

#padding zero upto maximum length
x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')
x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')
</code></pre>
",Training and Model Evaluation,use fit generator seq seq model trying customise code code efficient training large datasets use fit generator tokenizer problem tokenizer already working scanning text data giving one go come creating generator following snippet code work entire dataset
Part-of-Speech tagging: what is the difference between known words and unknown words?,"<p>I am trying to understand the result evaluation table (table 1) of this <a href=""http://www.stefan-evert.de/PUB/GiesbrechtEvert2009_Tagging.pdf"" rel=""nofollow noreferrer"">paper</a>.</p>
<p>There are three different accuracies reported <code>overall</code>, <code>unknown words (UW)</code>, <code>known words (KW)</code>, and <code>percentage of unknown words (% unk.)</code>.</p>
<p><a href=""https://i.sstatic.net/fjZxp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fjZxp.png"" alt=""enter image description here"" /></a></p>
<p>Are the <code>known words</code> the data that is used for training? And, are the <code>unknown words</code> the data that is used for testing and validation?</p>
<p>What is the overall accuracy? How is it computed?</p>
<p>What is the percentage of unknown words <code>% unk.</code>? Is it the percentage of the test set?</p>
<p>Thank you.</p>
",Training and Model Evaluation,part speech tagging difference known word unknown word trying understand result evaluation table table paper three different accuracy reported data used training data used testing validation overall accuracy computed percentage unknown word percentage test set thank
Training and validation loss and accuracy in lstm,"<p>I am doing text classification with 3 classes, after dealing with over fit model, the image below is my model accuracy and loss results after adding regulization l2, now it means my models is learning ?<a href=""https://i.sstatic.net/2DuAi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2DuAi.png"" alt=""enter image description here"" /></a></p>
",Training and Model Evaluation,training validation loss accuracy lstm text classification class dealing fit model image model accuracy loss result adding regulization l mean model learning
How to train a keras tokenizer on a large corpus that doesn&#39;t fit in memory?,"<p>I am trying to train a language model that based on a 2-word input tries to predict a 1-word output. This is the model definition (all the layers are imported from <code>keras.layers</code>):</p>
<pre class=""lang-py prettyprint-override""><code>model = Sequential()
model.add(Embedding(vocab_size, 2, input_length=seq_length))
model.add(LSTM(100, return_sequences=True))
model.add(LSTM(100))
model.add(Dense(100, activation='relu'))
model.add(Dense(vocab_size, activation='softmax'))
print(model.summary())
</code></pre>
<p>The problem is that my dataset has 87 million lines of 3-word data (2 for input, 1 for output) and it does not fit into my memory. I heard that <code>keras.preprocessing.text.Tokenizer</code> creates tokens based on their frequency in text. I am training my tokenizer like this:</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = Tokenizer(oov_token=1)
tokenizer.fit_on_texts(lines)
sequences = tokenizer.texts_to_sequences(lines)
</code></pre>
<p>How am I supposed to fit my tokenizer on all texts if they don't fit into memory?</p>
",Training and Model Evaluation,train kera tokenizer large corpus fit memory trying train language model based word input try predict word output model definition layer imported problem dataset ha million line word data input output doe fit memory heard creates token based frequency text training tokenizer like supposed fit tokenizer text fit memory
Loss on masked tensors,"<p>Suppose I have logits like </p>

<pre><code>[[4.3, -0.5, -2.7, 0, 0],
[0.5, 2.3, 0, 0, 0]]
</code></pre>

<p>where clearly the last two in the first example and last three in the second example are masked (that is, they are zero) and should not affect loss and gradient computations. </p>

<p>How do I compute cross-entropy loss between this logits and corresponding labels? For sanity, the labels for this example can be something like </p>

<pre><code>[[1, 0, 0, 0, 0],
[0, 1, 0, 0, 0]]
</code></pre>

<p>(One issue: Softmax, followed by log, on the logits will be applicable for the masked zeroes too and tf's cross-entropy method will consider the loss for those elements too.)</p>

<p>(Also, you can think about the problem like this: I have logits of varying lengths in a batch, i.e. my logits were length 3 and 2 for eg.1 and eg.2 respectively. Same is followed by the labels.)</p>
",Training and Model Evaluation,loss masked tensor suppose logits like clearly last two first example last three second example masked zero affect loss gradient computation compute cross entropy loss logits corresponding label sanity label example something like one issue softmax followed log logits applicable masked zero tf cross entropy method consider loss element also think problem like logits varying length batch e logits length eg eg respectively followed label
Chatbot text preprocessing necessary on each input?,"<p>I'm trying to learn about NLP and Chatbots.
From what i've read, text preprocessing (e.g. lowercaseing and turn numbers to words) is done on the training data in order to train the model. I'd think, that this is also necessary on each input a user makes, in order to find an appropriate response later, but I can't find any sources confirming this.</p>
<p>Is the request fed into the model as is, or is it preprocessed before?</p>
",Training and Model Evaluation,chatbot text preprocessing necessary input trying learn nlp chatbots read text preprocessing e g lowercaseing turn number word done training data order train model think also necessary input user make order find appropriate response later find source confirming request fed model preprocessed
Low accuracy rate after training Doc2Vec model,"<p>I'm trying to train a Doc2Vec model in order to create a multi-label text classifier.<br />
In order to do that, i have chosen a data set that contains approximately 70000 article, and every article contains between 1500 and 2000 words.<br />
These articles are divided into 5 classes.<br />
while setting up my input, i chosen as tag for my document their corresponding label.
I have done it as follow :
<code>tagged_article = data.apply(lambda r: TaggedDocument(words=r['article'].split(), tags=[r.labels]), axis=1)</code><br />
then i have trained my model with the following line codes:</p>
<pre><code>model_dbow = Doc2Vec(dm=1, vector_size=300, negative=5, min_count=10, workers=cores)
model_dbow.build_vocab([x for x in tqdm(tagged_article.values)])

print(&quot;Training the Doc2Vec model for &quot;, no_epochs, &quot;number of epochs&quot; )
for epoch in range(no_epochs):
     model_dbow.train(utils.shuffle([x for x in tqdm(tagged_article.values)]),total_examples=len(tagged_article.values), epochs=1)
     model_dbow.alpha -= 0.002
     model_dbow.min_alpha = model_dbow.alpha   
</code></pre>
<p>After that I have created a logistic regression model in order to predict tags for every article.</p>
<p>To do that I have created the following functions:\</p>
<pre><code>def vec_for_learning(model, tagged_docs):
sents = tagged_docs.values
targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=inference_steps)) for doc in tqdm(sents)])
return targets, regressors

y_train, X_train = vec_for_learning(model_dbow, tagged_article)

logreg = LogisticRegression(solver='lbfgs',max_iter=1000)
logreg.fit(X_train, y_train)
</code></pre>
<p>Unfortunately i am getting a very bad result. In fact I'm getting 22% as accuracy rate and 21 % as an F1 score</p>
<p>Can you please explain me why i am getting these bad results.</p>
",Training and Model Evaluation,low accuracy rate training doc vec model trying train doc vec model order create multi label text classifier order chosen data set contains approximately article every article contains word article divided class setting input chosen tag document corresponding label done follow trained model following line code created logistic regression model order predict tag every article created following function unfortunately getting bad result fact getting accuracy rate f score please explain getting bad result
Understanding the role of the function build_vocab in Doc2Vec,"<p>I have recently started studying Doc2Vec model.
I have understood its mechanism and how it works.
I'm trying to implement it using gensim framework.
I have transormed my training data into TaggedDocument.
But i have one question :
What is the role of this line <code>model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])</code> ?
is it to create random vectors that represent text ?
Thank you for your help</p>
",Training and Model Evaluation,understanding role function build vocab doc vec recently started studying doc vec model understood mechanism work trying implement using gensim framework transormed training data taggeddocument one question role line create random vector represent text thank help
Removing Softmax from last layer yields a lot better results,"<p>I was solving an nlp task, of converting English sentences to German in Keras. But the model was not learning... But as soon as I removed the softmax from the last layer, it started working! Is this a bug in Keras, or it has to do with something else?</p>
<pre><code>
optimizer = Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')

def loss_function(real, pred):
  mask = tf.math.logical_not(tf.math.equal(real, 0))
  loss_ = loss_object(real, pred)

  mask = tf.cast(mask, dtype=loss_.dtype)
  loss_ *= mask

  return tf.reduce_mean(loss_)

EPOCHS = 20
batch_size = 64

batch_per_epoch = int(train_x1.shape[0] / batch_size)

embed_dim = 256
units = 1024
attention_units = 10

encoder_embed = Embedding(english_vocab_size, embed_dim)
decoder_embed = Embedding(german_vocab_size, embed_dim)

encoder = GRU(units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')
decoder = GRU(units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')

dense = Dense(german_vocab_size)

attention1 = Dense(attention_units)
attention2 = Dense(attention_units)
attention3 = Dense(1)

def train_step(english_input, german_target):
    loss = 0
    
    with tf.GradientTape() as tape:
      enc_output, enc_hidden = encoder(encoder_embed(english_input))

      dec_hidden = enc_hidden

      dec_input = tf.expand_dims([german_tokenizer.word_index['startseq']] * batch_size, 1)

      for i in range(1, german_target.shape[1]):
        attention_weights = attention1(enc_output) + attention2(tf.expand_dims(dec_hidden, axis=1))
        attention_weights = tanh(attention_weights)
        attention_weights = attention3(attention_weights)
        attention_weights = Softmax(axis=1)(attention_weights)

        Context_Vector = tf.reduce_sum(enc_output * attention_weights, axis=1)
        Context_Vector = tf.expand_dims(Context_Vector, axis = 1)

        x = decoder_embed(dec_input)

        x = Concatenate(axis=-1)([x, Context_Vector])

        dec_output, dec_hidden = decoder(x)

        output = tf.reshape(dec_output, (-1, dec_output.shape[2]))

        prediction = dense(output)

        loss += loss_function(german_target[:, i], prediction)

        dec_input = tf.expand_dims(german_target[:, i], 1)

    batch_loss = (loss / int(german_target.shape[1]))

    variables = encoder_embed.trainable_variables + decoder_embed.trainable_variables + encoder.trainable_variables + decoder.trainable_variables + dense.trainable_variables + attention1.trainable_variables + attention2.trainable_variables + attention3.trainable_variables

    gradients = tape.gradient(loss, variables)

    optimizer.apply_gradients(zip(gradients, variables))

    return batch_loss

</code></pre>
<h2>Code Summary</h2>
<p>The code just take the English sentence and German Sentence as input (It takes German sentence as input to implement Teacher-Forcing Method), and predicts the translated German sentence.
The loss function is <code>SparseCategoricalCrossentropy</code>, but it subtracts the loss of the <code>0</code>. For example, lets say, we have a sentence, that is : '<em>StartSeq This is Stackoverflow 0 0 0 0 0 EndSeq</em>' (The sentence also has a zero padding to make all the input sentences of the same length). Now, we would calculate loss for every word but not for the 0's. Doing this makes the model better.
<strong>Note - this model implementation implements Bahdanau Attention</strong></p>
<h2>Question</h2>
<p>When I apply softmax on the predicted probabilities by the last layer, the model doesn't learns anything. But it learns properly without softmax in the last layer. Why is this happening?</p>
",Training and Model Evaluation,removing softmax last layer yield lot better result wa solving nlp task converting english sentence german kera model wa learning soon removed softmax last layer started working bug kera ha something else code summary code take english sentence german sentence input take german sentence input implement teacher forcing method predicts translated german sentence loss function subtracts loss example let say sentence startseq stackoverflow endseq sentence also ha zero padding make input sentence length would calculate loss every word make model better note model implementation implement bahdanau attention question apply softmax predicted probability last layer model learns anything learns properly without softmax last layer happening
Open source pre-trained models for taxonomy/general word classification,"<p>Given any two words I'd like to understand if there's some sort of taxonomy/semantic field based relationship. For example given the words &quot;Dog&quot; and &quot;Cat&quot; I'd like to have a model which can return words in which &quot;Dog&quot; and &quot;Cat&quot; match, for example some words that this model would return in this case could be &quot;Animal&quot;, &quot;Mammal&quot;, &quot;Pet&quot; etcetera.</p>
<p>Is there an open source pre-trained model that can do this out of the box requiring no training dataset beforehand?</p>
",Training and Model Evaluation,open source pre trained model taxonomy general word classification given two word like understand sort taxonomy semantic field based relationship example given word dog cat like model return word dog cat match example word model would return case could animal mammal pet etcetera open source pre trained model box requiring training dataset beforehand
Predicting text in python,"<p><a href=""https://i.sstatic.net/45wkg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/45wkg.png"" alt=""enter image description here"" /></a></p>
<p>I have DataFrame like this in the image. I have to predict &quot;selected_text&quot; column on the basis of text and sentiment columns. How can I train the model on these 2 columns to further predict the &quot;selected_text&quot; column?</p>
",Training and Model Evaluation,predicting text python dataframe like image predict selected text column basis text sentiment column train model column predict selected text column
How to work with n-grams for classification tasks?,"<p>I'm going to train a classifier on a sample dataset using <code>n-gram</code>. I searched for related content and wrote the code below. As I'm a beginner in python, I have <strong>two questions</strong>.</p>
<p>1- Why should the dictionary have this 'True' structure (marked with comment)? Is this related to Naive Bayes Classifier input?</p>
<p>2- Which classifier do you recommend to do this task?</p>
<p>Any other suggestion to shorten the code are welcome :).</p>
<pre><code>from nltk.corpus import movie_reviews
from nltk.corpus import stopwords
from nltk import ngrams
from nltk.classify import NaiveBayesClassifier
import nltk.classify.util


stoplist = set(stopwords.words(&quot;english&quot;))


def stopword_removal(words):
    useful_words = [word for word in words if word not in stoplist]
    return useful_words


def create_ngram_features(words, n):
    ngram_vocab = ngrams(words, n)
    my_dict = dict([(ng, True) for ng in ngram_vocab])  # HERE
    return my_dict


for n in [1,2]:
    positive_data = []
    for fileid in movie_reviews.fileids('pos'):
        words = stopword_removal(movie_reviews.words(fileid))
        positive_data.append((create_ngram_features(words, n), &quot;positive&quot;))
    print('\n\n---------- Positive Data Sample----------\n', positive_data[0])

    negative_data = []
    for fileid in movie_reviews.fileids('neg'):
        words = stopword_removal(movie_reviews.words(fileid))
        negative_data.append((create_ngram_features(words, n), &quot;negative&quot;))
    print('\n\n---------- Negative Data Sample ----------\n', negative_data[0])

    train_set = positive_data[:100] + negative_data[:100]
    test_set = positive_data[100:] + negative_data[100:]

    classifier = NaiveBayesClassifier.train(train_set)

    accuracy = nltk.classify.util.accuracy(classifier, test_set)
    print('\n', str(n)+'-gram accuracy:', accuracy)
</code></pre>
",Training and Model Evaluation,work n gram classification task going train classifier sample dataset using searched related content wrote code beginner python two question dictionary true structure marked comment related naive bayes classifier input classifier recommend task suggestion shorten code welcome
Why is my attention model worse than non-attention model,"<p>My task was to convert english sentence to German sentence. I first did this with normal encoder-decoder network, on which I got fairly good results. Then, I tried to solve the same task with the same exact model as before, but with <strong>Bahdanau Attention</strong> in it. And, the model without attention outperformed the one with the attention.</p>
<p>The Model's loss without attention gone from approximately 8.0 to 1.4 in 5 epochs and gone to 1.0 in 10 epochs and the loss was still reducing but at a slower rate.</p>
<p>The Model's loss with attention gone from approximately 8.0 to 2.6 in 5 epochs and was not learning much more.</p>
<p>None of the models were overfitting as the validation loss was also decreasing in both the models.</p>
<p>Each English sentence had 47 words in it (after padding), and each German sentence had 54 words in it (after padding). I had 7000 English and 7000 German sentence in the training set and 3000 in the validation set.</p>
<p>I tried almost everything like: different learning rates, different optimizer, different batch sizes, different activation functions I used in the model, tried applying batch and layer normalization, and different number of LSTM units for the encoder and decoder, but nothing makes much difference, except the normalization and increasing the data, in which the loss goes down till approx 1.5 but then again stops learning!</p>
<p>Why did this happened? Why did the Model with Bahdanau attention failed while the one without any kind of attention was performing well?</p>
<p>Edit 1 - I tried applying LayerNormalization before the attention, after the attention and both before and after the attention. The results were approximately the same in each case. But, this time, the loss went from approx 8.0 to 2.1 in 5 epochs, and was again not learning much. But most of the learning was done in 1 epoch as at the end of 1 epoch it reached a loss of approx 2.6 and then reached 2.1 in the next epoch, and then again not learning much.</p>
<p>Still, the model without any attention outperforms the one with both attention and LayerNormzalization. What could be the reason to this? Are the results that I got even <em>possible</em>? How can a normal encoder-decoder network without any kind of normalization, without any dropout layer perform better than the model with both attention and LayerNormalization?</p>
<p>Edit 2 - I tried increasing the data (I did it 7 times more than the previous one), this time, both the models performance improved a lot. But still, the model without attention performed better than the model with attention. Why is this happening?</p>
<p>Edit 3 - I tried to debug the model by first passing just one sample from the whole training dataset. The loss started at approx 9.0 and was reducing and converging at 0. Then, I tried by passing 2 samples, the loss again started at approx 9.0, but, this time, it was just wandering between 1.5 and 2.0 for the first 400 epochs and then reducing slowly. This is a plot of how the loss reduces when I trained it with just 2 samples:</p>
<p><a href=""https://i.sstatic.net/DEdVp.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/DEdVp.png"" alt=""enter image description here"" /></a></p>
<p>This is a plot of how the loss reduces when I trained it with just 1 sample:</p>
<p><a href=""https://i.sstatic.net/yzNj7.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/yzNj7.png"" alt=""enter image description here"" /></a></p>
",Training and Model Evaluation,attention model worse non attention model task wa convert english sentence german sentence first normal encoder decoder network got fairly good result tried solve task exact model bahdanau attention model without attention outperformed one attention model loss without attention gone approximately epoch gone epoch loss wa still reducing slower rate model loss attention gone approximately epoch wa learning much none model overfitting validation loss wa also decreasing model english sentence word padding german sentence word padding english german sentence training set validation set tried almost everything like different learning rate different optimizer different batch size different activation function used model tried applying batch layer normalization different number lstm unit encoder decoder nothing make much difference except normalization increasing data loss go till approx stop learning happened model bahdanau attention failed one without kind attention wa performing well edit tried applying layernormalization attention attention attention result approximately case time loss went approx epoch wa learning much learning wa done epoch end epoch reached loss approx reached next epoch learning much still model without attention outperforms one attention layernormzalization could reason result got even possible normal encoder decoder network without kind normalization without dropout layer perform better model attention layernormalization edit tried increasing data time previous one time model performance improved lot still model without attention performed better model attention happening edit tried debug model first passing one sample whole training dataset loss started approx wa reducing converging tried passing sample loss started approx time wa wandering first epoch reducing slowly plot loss reduces trained sample plot loss reduces trained sample
Checking model overfit of doc2vec with infer_vector(),"<p>my aim is to create document embeddings from the column df[&quot;text&quot;] as a first step and then as a second step plug them along with other variables into a XGBoost Regressor model in order to make predictions. This works very well for the train_df.<br />
I am currently trying to evaluate my trained Doc2Vec model by inferring vectors with infer_vector() on the unseen test_df and then again make predictions with it.However, the results are super bad. I got a very large error (RMSE).
I assume, this means that Doc2Vec is massively overfitting?
I am actually not sure if this is the correct way to evaluate my doc2vec model (by infer_vector)?
What to do to prevent doc2vec from overfitting?</p>
<p>Please find my code below for infering vectors from a model:</p>
<pre><code>vectors_test=[]
for i in range(0, len(test_df)):
    vecs=model.infer_vector(tokenize(test_df[&quot;text&quot;][i]))
    vectors_test.append(vecs)
vectors_test= pd.DataFrame(vectors_test)
test_df = pd.concat([test_df, vectors_test], axis=1)
</code></pre>
<p>I then make predictions with my XGBoost model:</p>
<pre><code>np.random.seed(0)
test_df= test_df.reindex(np.random.permutation(test_df.index))

y = test_df['target'].values
X = test_df.drop(['target'], axis=1).values

y_pred = mod.predict(X)
pred = pd.DataFrame()
pred[&quot;Prediction&quot;] = y_pred
rmse = np.sqrt(mean_squared_error(y,y_pred))
print(rmse)
</code></pre>
<p>Please see also the training of my doc2vec model:</p>
<pre><code>doc_tag = train_df.apply(lambda train_df: TaggedDocument(words=tokenize(train_df[&quot;text&quot;]), tags= [train_df.Tag]), axis = 1)

# initializing model, building a vocabulary 

model = Doc2Vec(dm=0, vector_size=200, min_count=1, window=10, workers= cores) 

model.build_vocab([x for x in tqdm(doc_tag.values)])

# train model for 5 epochs 

for epoch in range(5): 
    model.train(utils.shuffle([x for x in tqdm(doc_tag.values)]), total_examples=len(doc_tag.values), epochs=1)
</code></pre>
",Training and Model Evaluation,checking model overfit doc vec infer vector aim create document embeddings column df text first step second step plug along variable xgboost regressor model order make prediction work well train df currently trying evaluate trained doc vec model inferring vector infer vector unseen test df make prediction however result super bad got large error rmse assume mean doc vec massively overfitting actually sure correct way evaluate doc vec model infer vector prevent doc vec overfitting please find code infering vector model make prediction xgboost model please see also training doc vec model
Finding custom entities using NLP,"<p>Basically, from a paragraph, I have to find two entities <code>Role</code> and <code>Oragnization</code>.</p>
<ul>
<li>Org should be captured along with their branch location, not their full address if provided in paragraph</li>
<li>Role can be in bracket and right before bracket at same time like
<code>Org as role(The &quot;Role&quot;)</code></li>
<li>Role can have multiple words</li>
</ul>
<p>Example paragraph would be:</p>
<blockquote>
<p>XXXX, dated as of November 10, 2050, among: (i) <code>&lt;ORG_NAME_1 PLC&gt;</code>,
a public company incorporated under the laws and having
its registered office at the 123 Penss Aven, NY, USA (the “<code>&lt;ROLE1&gt;</code>”), (ii) <code>&lt;ORG_NAME_2 PLC&gt;</code>, a public limited company incorporated
under the laws having its registered office at Manhattan Blvd, Seattle, WA, as <code>&lt;ROLE2&gt;</code> (the
“<code>&lt;ROLE2&gt;</code>”), (iii) the , Guarantors named in some random text hereto, (iv)
<code>&lt;ORG_NAME_3&gt;, N.A., Belfast Branch</code>, as <code>&lt;ROLE3&gt;</code> (the “<code>&lt;ROLE3&gt;</code>”), (v) <code>&lt;ORG_NAME_4&gt;</code> , as <code>&lt;ROLE4&gt;</code>, <code>&lt;ROLE5&gt;</code> and <code>&lt;ROLE6&gt;</code>, and (vi) <code>&lt;ORG_NAME_5&gt;</code> <code>Deutschland AG</code>, as <code>&lt;ROLE7&gt;</code>.</p>
</blockquote>
<p>After processing, desired result would be linking role with organization.</p>
<pre><code>&lt;ROLE1&gt; --&gt; &lt;ORG_NAME_1 PLC&gt;
&lt;ROLE2&gt; --&gt; &lt;ORG_NAME_2 PLC&gt;
&lt;ROLE3&gt; --&gt; &lt;ORG_NAME_3&gt;, N.A., Belfast Branch
&lt;ROLE4&gt;, &lt;ROLE5&gt; and &lt;ROLE6&gt; --&gt; &lt;ORG_NAME_4&gt; 
&lt;ROLE7&gt; --&gt; &lt;ORG_NAME_5&gt; Deutschland AG
</code></pre>
<p>Another example would be</p>
<blockquote>
<p>XXXX dated as of November 28, 2027 among <code>&lt;ORG_NAME_1&gt; A/S</code>, a company incorporated
under the laws of Australia (the “<code>&lt;ROLE1&gt;</code>”), the Guarantors (as defined herein), <code>&lt;ORG_NAME_2&gt;, N.A., Montreal</code>
<code>Branch</code>, as <code>&lt;ROLE2&gt;</code>, <code>&lt;ROLE3&gt;</code>, <code>&lt;ROLE4&gt;</code>, <code>&lt;ROLE5&gt;</code>, <code>&lt;ROLE6&gt;</code> and <code>&lt;ROLE7&gt;</code></p>
</blockquote>
<p>After processing, desired result should be:</p>
<pre><code>&lt;ROLE1&gt; --&gt; &lt;ORG_NAME_1&gt; A/S
&lt;ROLE2&gt;, &lt;ROLE3&gt;, &lt;ROLE4&gt;, &lt;ROLE5&gt;, &lt;ROLE6&gt; and &lt;ROLE7&gt; --&gt; &lt;ORG_NAME_2&gt;, N.A., Montreal Branch
</code></pre>
<p>I tried to use PoS, NER but not desired results.</p>
<ul>
<li>Played with stanford NLP for NER, but organizations are not detected properly, tried to train my own data but accuracy is not accepted enough. It does not detect all organizations properly and is tagged as <code>OTHER</code> rather. Did not tweak with actual CRF model.</li>
<li>Played with NLTK python and tried to make some rule around NNP(proper noun) but sometimes roles are detected as verb, sometimes noun and it depends on case also sometimes, so not sure if it is desired approach.</li>
</ul>
<p>There are not much varieties in paragraph patterns, I can post 1 or 2 more examples of different patterns if needed. Roles are fixed around 40 and organization would be dynamic.</p>
<p>Please suggest if I should read out some specific papers or models. Thanks.</p>
",Training and Model Evaluation,finding custom entity using nlp basically paragraph find two entity org captured along branch location full address provided paragraph role bracket right bracket time like role multiple word example paragraph would xxxx dated november among public company incorporated law registered office pen aven ny usa ii public limited company incorporated law registered office manhattan blvd seattle wa iii guarantor named random text hereto iv v vi processing desired result would linking role organization another example would xxxx dated november among company incorporated law australia guarantor defined herein processing desired result tried use po ner desired result played stanford nlp ner organization detected properly tried train data accuracy accepted enough doe detect organization properly tagged rather tweak actual crf model played nltk python tried make rule around nnp proper noun sometimes role detected verb sometimes noun depends case also sometimes sure desired approach much variety paragraph pattern post example different pattern needed role fixed around organization would dynamic please suggest read specific paper model thanks
Botium K-fold cross validation on DialogFlow throws precondition errors,"<p>I'm trying to run k-fold cross validation of my DialogFlow model (as described <a href=""https://medium.com/@floriantreml/tutorial-benchmark-your-chatbot-on-watson-dialogflow-wit-ai-and-more-92885b4fbd48"" rel=""nofollow noreferrer"">here</a>) on Botium. These are the steps I followed:</p>
<ul>
<li>Extracted training content using <code>botium-cli nlpextract</code> from DialogFlow (around 600 utterances spread over 6 intents)</li>
<li>Setup config files for Dialogflow and IBM Watson</li>
<li>Ran a k-fold cross validation
for both platforms (<code>botium-cli nlpanalytics k-fold</code>)
While the validation ran fine on Watson, on Dialogflow a lot of errors came up for some fold runs while other fold runs ran fine . Example error below:</li>
</ul>
<pre><code>com.google.apps.framework.request.StatusException: &lt;eye3 title='FAILED_PRECONDITION'/&gt; 
generic::FAILED_PRECONDITION: Intent with id '******-****-d8298f23f5af' not found 
among intents in environment '' for agent with id '******-**-****-ab10e1f8c943'.

</code></pre>
<p>Can someone let me know what I'm doing wrong in the DialogFlow tests? The precision and recall also came out very low for these runs as compared to Watson where almost all test results were &gt; 0.9</p>
<p>My Dialogflow config looks like this:</p>
<pre><code>{
  &quot;botium&quot;: {
    &quot;Capabilities&quot;: {
      &quot;PROJECTNAME&quot;: &quot;agentblr&quot;,
      &quot;CONTAINERMODE&quot;: &quot;dialogflow&quot;,
      &quot;DIALOGFLOW_PROJECT_ID&quot;: &quot;&lt;project id&gt;&quot;,
      &quot;DIALOGFLOW_CLIENT_EMAIL&quot;: &quot;&lt;service access email&gt;&quot;,
      &quot;DIALOGFLOW_PRIVATE_KEY&quot;: &quot;&lt;service access key&gt;&quot;,
      &quot;DIALOGFLOW_NLP_PROJECT_ID&quot;:  &quot;&lt;empty test project&gt;&quot;,
      &quot;DIALOGFLOW_NLP_CLIENT_EMAIL&quot;: &quot;&lt;empty test project service access email&gt;&quot;,
      &quot;DIALOGFLOW_NLP_PRIVATE_KEY&quot;: &quot;&lt;empty test project service access key&gt;&quot;,
      &quot;DIALOGFLOW_LANGUAGE_CODE&quot;:  &quot;en&quot;
            
    }
  }
}
</code></pre>
",Training and Model Evaluation,botium k fold cross validation dialogflow throw precondition error trying run k fold cross validation dialogflow model described botium step followed extracted training content using dialogflow around utterance spread intent setup config file dialogflow ibm watson ran k fold cross validation platform validation ran fine watson dialogflow lot error came fold run fold run ran fine example error someone let know wrong dialogflow test precision recall also came low run compared watson almost test result dialogflow config look like
PyTorch - sparse tensors do not have strides,"<p>I am building my first sentiment analysis model for a small dataset of 1000 reviews using TF-IDF approach along with LSTM using the below code. I am preparing the train data by preprocessing it and feeding to the Vectorizer as below</p>

<pre><code>def tfidf_features(X_train, X_val, X_test):
tfidf_vectorizer = TfidfVectorizer(analyzer='word', token_pattern = '(\S+)', min_df = 5, max_df = 
0.9, ngram_range=(1,2))
X_train=tfidf_vectorizer.fit_transform(X_train)
X_val=tfidf_vectorizer.transform(X_val)
X_test=tfidf_vectorizer.transform(X_test)

return X_train, X_val, X_test, tfidf_vectorizer.vocabulary_
</code></pre>

<p>I am converting my csr_matrix to a pytorch tensor using the below code</p>

<pre><code>def spy_sparse2torch_sparse(data):
samples=data.shape[0]
features=data.shape[1]
values=data.data
coo_data=data.tocoo()
indices=torch.LongTensor([coo_data.row,coo_data.col])
t=torch.sparse.FloatTensor(indices,torch.from_numpy(values).float(),[samples,features])
return t
</code></pre>

<p>And I am getting the training sentences tensor as this</p>

<pre><code>   tensor(indices=tensor([[  0,   0,   1,  ..., 599, 599, 599],
                   [ 97, 131,  49,  ..., 109,  65,  49]]),
   values=tensor([0.6759, 0.7370, 0.6076,  ..., 0.3288, 0.3927, 0.3288]),
   size=(600, 145), nnz=1607, layout=torch.sparse_coo)
</code></pre>

<p>I am creating a TensorDataSet using the below code wherein I am also converting my label data from bumpy to a torch tensor</p>

<pre><code>train_data = TensorDataset(train_x, torch.from_numpy(train_y))
</code></pre>

<p>I have defined my LSTM network and calling it with the following parameters</p>

<pre><code>n_vocab = len(vocabulary)
n_embed = 100
n_hidden = 256
n_output = 1   # 1 (""positive"") or 0 (""negative"")
n_layers = 2

net = Sentiment_Lstm(n_vocab, n_embed, n_hidden, n_output, n_layers)
</code></pre>

<p>I have also defined the loss and optimizer. Now I am training my model using the below code</p>

<pre><code>print_every = 100
step = 0
n_epochs = 4  # validation loss increases from ~ epoch 3 or 4
clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN

for epoch in range(n_epochs):
h = net.init_hidden(batch_size)

for inputs, labels in train_loader:
    step += 1

    # making requires_grad = False for the latest set of h
    h = tuple([each.data for each in h])   

    net.zero_grad()
    output, h = net(inputs)
    loss = criterion(output.squeeze(), labels.float())
    loss.backward()
    nn.utils.clip_grad_norm(net.parameters(), clip)
    optimizer.step()

    if (step % print_every) == 0:
        net.eval()
        valid_losses = []
        v_h = net.init_hidden(batch_size)

        for v_inputs, v_labels in valid_loader:
            v_inputs, v_labels = inputs.to(device), labels.to(device)

            v_h = tuple([each.data for each in v_h])

            v_output, v_h = net(v_inputs)
            v_loss = criterion(v_output.squeeze(), v_labels.float())
            valid_losses.append(v_loss.item())

        print(""Epoch: {}/{}"".format((epoch+1), n_epochs),
              ""Step: {}"".format(step),
              ""Training Loss: {:.4f}"".format(loss.item()),
              ""Validation Loss: {:.4f}"".format(np.mean(valid_losses)))
        net.train()
</code></pre>

<p>However, I am getting a major error on the line <code>output, h = net(inputs)</code> as <code>RuntimeError: sparse tensors do not have strides</code></p>

<p>The workarounds given on other websites are not understandable. I am expecting an exact code change I need to make in order to fix this issue. </p>
",Training and Model Evaluation,pytorch sparse tensor stride building first sentiment analysis model small dataset review using tf idf approach along lstm using code preparing train data preprocessing feeding vectorizer converting csr matrix pytorch tensor using code getting training sentence tensor creating tensordataset using code wherein also converting label data bumpy torch tensor defined lstm network calling following parameter also defined loss optimizer training model using code however getting major error line workarounds given website understandable expecting exact code change need make order fix issue
How to use the ground truth answers from the previous time step for the decoders,"<p>I found this on an article <a href=""https://medium.com/analytics-vidhya/neural-machine-translation-using-bahdanau-attention-mechanism-d496c9be30c3"" rel=""nofollow noreferrer"">here</a>:</p>
<blockquote>
<p>Decoder takes the hidden state of the last Encoder RNN cell as the initial state of its first RNN cell along with the  token as the initial input to produce an output sequence. We use Teacher Forcing for faster and efficient training of the decoder. Teacher forcing is a method for quickly and efficiently training recurrent neural network models that use the ground truth from a prior time step as input. In this method, the right answer is given as the beginning of training so that the model will train quickly and efficiently.</p>
</blockquote>
<p><a href=""https://i.sstatic.net/qwft2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qwft2.png"" alt=""enter image description here"" /></a></p>
<p>Now, I am unable to use the Teacher Forcing Method for the decoder. Can someone help me out?</p>
<h2>What I have done so far</h2>
<p>Now, I am taking many English sentences and tokenizing them, and then keeping it in a variable <code>X</code>. The shape of <code>X</code> is <code>[7000, 5]</code>, which means that there are 7000 English sentences, each which has 5 words in it. I am doing the same with German Sentences and keeping it in a variable <code>Y</code>, which has shape of <code>[7000, 10, 1]</code>, which means that there are 7000 total German Sentences, each having 10 words in it (I am not hot-encoding it, as I am using sparse categorical cross-entropy as the loss function).
Then, I defined my model which is this:</p>
<pre><code>    Model = Sequential([
          Embedding(english_vocab_size, 256, input_length=english_max_len, mask_zero=True),
          LSTM(256, activation='relu'),
          RepeatVector(german_max_len),
          LSTM(256, activation='relu', return_sequences=True),
          Dense(german_vocab_size, activation='softmax')
    ])
</code></pre>
<p>The <code>english_vocab_size</code> and <code>german_vocab_size</code> is the total number of English words and German words present in the English and German vocabulary. The <code>english_max_len</code> and <code>german_max_len</code> are total number of words each English and German sentence has in it.</p>
<p>Now, from here what should I do to use the Teacher Forcing Technique?</p>
",Training and Model Evaluation,use ground truth answer previous time step decoder found article decoder take hidden state last encoder rnn cell initial state first rnn cell along token initial input produce output sequence use teacher forcing faster efficient training decoder teacher forcing method quickly efficiently training recurrent neural network model use ground truth prior time step input method right answer given beginning training model train quickly efficiently unable use teacher forcing method decoder someone help done far taking many english sentence tokenizing keeping variable shape mean english sentence ha word german sentence keeping variable ha shape mean total german sentence word hot encoding using sparse categorical cross entropy loss function defined model total number english word german word present english german vocabulary total number word english german sentence ha use teacher forcing technique
How to calculate data drift in text data?,"<p>I am training a binary classifier for text data. I'm having an accuracy score of 98.12% and f1 score of 95%. The problem is i trained my model using a 5 year old data-set. I am pretty sure that data drift will occur at some point and my model's performance will be affected. I am already monitoring the confidence score and if there's any sudden drop in a given window size, i will be alerted. Is there any way to identify the sudden data drift and alert me?. Thanks in advance. </p>
",Training and Model Evaluation,calculate data drift text data training binary classifier text data accuracy score f score problem trained model using year old data set pretty sure data drift occur point model performance affected already monitoring confidence score sudden drop given window size alerted way identify sudden data drift alert thanks advance
what is the role of RNNLanguageModel&#39;s forward method?,"<p>i'm reading a tutorial about character based neural networks using AllenNlp framework, the goal is building a model which can complete a sentence. there is a step of instances building after that i want to train my model. i have the code below, i could not understand the role of forward function, anyone can help ? could someone provide an example</p>
<pre><code>class RNNLanguageModel(Model):
def __init__(self,
             embedder: TextFieldEmbedder,
             hidden_size: int,
             max_len: int,
             vocab: Vocabulary) -&gt; None:
    super().__init__(vocab)

    self.embedder = embedder

    # initialize a Seq2Seq encoder, LSTM
    self.rnn = PytorchSeq2SeqWrapper(
        torch.nn.LSTM(EMBEDDING_SIZE, HIDDEN_SIZE, batch_first=True))

    self.hidden2out = torch.nn.Linear(in_features=self.rnn.get_output_dim(), out_features=vocab.get_vocab_size('tokens'))
    self.hidden_size = hidden_size
    self.max_len = max_len

def forward(self, input_tokens, output_tokens):
    '''
    This is the main process of the Model where the actual computation happens. 
    Each Instance is fed to the forward method. 
    It takes dicts of tensors as input, with same keys as the fields in your Instance (input_tokens, output_tokens)
    It outputs the results of predicted tokens and the evaluation metrics as a dictionary. 
    '''

    mask = get_text_field_mask(input_tokens)
    embeddings = self.embedder(input_tokens)
    rnn_hidden = self.rnn(embeddings, mask)
    out_logits = self.hidden2out(rnn_hidden)
    loss = sequence_cross_entropy_with_logits(out_logits, output_tokens['tokens'], mask)

    return {'loss': loss}
</code></pre>
",Training and Model Evaluation,role rnnlanguagemodel forward method reading tutorial character based neural network using allennlp framework goal building model complete sentence step instance building want train model code could understand role forward function anyone help could someone provide example
Select between skip-gram and CBOW model for training word2Vec in gensim,"<p>Is it possible to choose between the <code>Skip-gram</code> and the <code>CBOW</code> model in <em>Gensim</em> when training a <em>Word2Vec</em> model?</p>
",Training and Model Evaluation,select skip gram cbow model training word vec gensim possible choose model gensim training word vec model
My checkpoint albert files does not change when training,"<p>I train Albert model for question answering task. I have 200 thousand question-answer pairs and I use a saved checkpoint file with 2gb. I trained it on my GPU GeForce 2070 RTX with 1000 steps each time to save checkpoint, during training the checkpoint <code>model.ckpt-96000.data-00000-of-00001</code> files just keep the size of <code>135MB</code> and don't increase. Is this a problem?</p>
<p>I can't see why with a much smaller dataset like 1500 question-answer pairs, it also produces 135 MB checkpoint file. It hasn't stopped training yet but is it possible that the model will improve with this training?</p>
",Training and Model Evaluation,checkpoint albert file doe change training train albert model question answering task thousand question answer pair use saved checkpoint file gb trained gpu rtx step time save checkpoint training checkpoint file keep size increase problem see much smaller dataset like question answer pair also produce mb checkpoint file stopped training yet possible model improve training
Intent classification with large number of intent classes,"<p>I am working on a data set of approximately 3000 questions and I want to perform intent classification. <strong>The data set is not labelled yet</strong>, but from the business perspective, there's a requirement of identifying approximately <strong>80 various intent classes</strong>. Let's assume my training data has approximately equal number of each classes and is not majorly skewed towards some of the classes. I am intending to convert the text to word2vec or Glove and then feed into my classifier.</p>

<p>I am familiar with cases in which I have a smaller number of intent classes, such as 8 or 10 and the choice of machine learning classifiers such as SVM, naive bais or deeplearning (CNN or LSTM).</p>

<p>My question is that if you have had experience with such large number of intent classes before, and which of machine learning algorithm do you think will perform reasonably? do you think if i use deep learning frameworks, still large number of labels will cause poor performance given the above training data?</p>

<p>We need to start labelling the data and it is rather laborious to come up with 80 classes of labels and then realise that it is not performing well, so I want to ensure that I am making the right decision on <strong>how many classes of intent maximum</strong> I should consider and what machine learning algorithm do you suggest?</p>

<p>Thanks in advance...</p>
",Training and Model Evaluation,intent classification large number intent class working data set approximately question want perform intent classification data set labelled yet business perspective requirement identifying approximately various intent class let assume training data ha approximately equal number class majorly skewed towards class intending convert text word vec glove feed classifier familiar case smaller number intent class choice machine learning classifier svm naive bai deeplearning cnn lstm question experience large number intent class machine learning algorithm think perform reasonably think use deep learning framework still large number label cause poor performance given training data need start labelling data rather laborious come class label realise performing well want ensure making right decision many class intent maximum consider machine learning algorithm suggest thanks advance
Is there any benchmarks for keyword extraction?,"<p>I will do keyword extraction from documents. But I can't how to evaluate the accuracy(or precision, recall), because I don't know about ground truths of data.
I want to do evaluate the accuracy(or precision, recall) for my model. Is there any benchmarks?</p>
",Training and Model Evaluation,benchmark keyword extraction keyword extraction document evaluate accuracy precision recall know ground truth data want evaluate accuracy precision recall model benchmark
How to change the structure of a sentence (imperative -&gt; interrogative) in python (NLP),"<p>I would like to build a model that can take a sentence in the imperative form and output a new sentence in an interrogative form (however, the meaning would be the same in both sentences - both sentences are commands). I have seen the following question and have done some research into what kinds of models could be used, but I am stumped. Any advice on where to go from here would be very welcome.</p>
<p><a href=""https://stackoverflow.com/questions/22678496/convert-interrogative-sentence-to-imperative-sentence"">Convert interrogative sentence to imperative sentence</a></p>
<p><strong>Example data:</strong></p>
<p>I have several imperative sentences with their interrogative counterparts.</p>
<pre><code>    Imperative: Make sure you know what your own assets are and operate them accordingly.
    Interrogative 1: Do you know what your own assets are and can you operate them accordingly?
    Interrogative 2: Do you know what your own assets are and how to operate them accordingly?

    Imperative: Hold your hands in position.
    Interrogative 1: Can you hold your hands in position?
    Interrogative 2: Could you hold your hands in position?
</code></pre>
<p>I would prefer to do this with a machine learning approach because I have so many sentences.</p>
<p>The end goal is to be able to input an imperative and have a random interrogative with the same meaning output.</p>
<p><strong>What I have done</strong></p>
<p>I have created a rule-based system that can classify imperatives with 87% accuracy using NLTK's POS tagging chunking. I have also been able to extract the grammar from sentences using NLTK's context free grammar functions. I have done some research on neural language models and LSTMs, but these seem to want to take a paragraph or more of text as training. I want to use single sentences as training with clear output possibilities.</p>
<p><strong>Final question</strong></p>
<p>Is there an algorithm I can use in order to train the grammar differences between an imperative and its interrogative counterparts so that I can simply input an imperative and get an interrogative in return? Is there another approach I should look into?</p>
",Training and Model Evaluation,change structure sentence imperative interrogative python nlp would like build model take sentence imperative form output new sentence interrogative form however meaning would sentence sentence command seen following question done research kind model could used stumped advice go would welcome href interrogative sentence imperative sentence example data several imperative sentence interrogative counterpart would prefer machine learning approach many sentence end goal able input imperative random interrogative meaning output done created rule based system classify imperative accuracy using nltk po tagging chunking also able extract grammar sentence using nltk context free grammar function done research neural language model lstms seem want take paragraph text training want use single sentence training clear output possibility final question algorithm use order train grammar difference imperative interrogative counterpart simply input imperative get interrogative return another approach look
How do I deal with preprocessing and with unseen data in a NLP problem?,"<p>Suppose that I have preprocessed some text data, removed stopwords, urls and so on.</p>
<p>How should I structure these cleaned data in order to make them usable for a classifier like a Neural Network? Is there a preferred structure, or a rule of thumb? (Bag of words, tf-idf or anything else?) Also, can you suggest some package which will automatically do all the work in python?</p>
<p>Now I train the model, and things work properly.
The model performs well on test set too.</p>
<p>How do I have to treat unseen data?
When I decide to implement the model in a real life project it will encounter new data: do I have to store the structure (like the tf-idf structure) I used for training and apply it to these new data?
Also, let's suppose that in the training/validation/test data there was not the word &quot;hello&quot;, so it has not a representation. A real life sentence I have to classify contains the word &quot;hello&quot;
How do I cope with this problem?</p>
<p>Thanks for all the clarifications.</p>
",Training and Model Evaluation,deal preprocessing unseen data nlp problem suppose preprocessed text data removed stopwords url structure cleaned data order make usable classifier like neural network preferred structure rule thumb bag word tf idf anything else also suggest package automatically work python train model thing work properly model performs well test set treat unseen data decide implement model real life project encounter new data store structure like tf idf structure used training apply new data also let suppose training validation test data wa word hello ha representation real life sentence classify contains word hello cope problem thanks clarification
Evaluation metrics for multiple correct answers in QA problem system,"<p>I'm building a QA machine and I have my own data for this task. I have a problem that 1 question can have 2 or more answers. For example:</p>
<p><strong>Questions:</strong> &quot;What does A have to do?&quot;</p>
<p><strong>Correct answers</strong>:</p>
<ul>
<li>&quot;A have to clean the floor&quot;</li>
<li>&quot;A have to hang up the laundry&quot;</li>
</ul>
<p>In my QA model, I can get k best answers.  However, in some cases, not only k is unequal the number of correct answers but also some of the k answers are not correct.</p>
<p>Most of public dataset like SQuAD, triviaQA have a pair with one question and one answer. In my case, my question can have multiple answers. So, what kind of evaluation metrics I should use? Can I use F1 score?</p>
",Training and Model Evaluation,evaluation metric multiple correct answer qa problem system building qa machine data task problem question answer example question doe correct answer clean floor hang laundry qa model get k best answer however case k unequal number correct answer also k answer correct public dataset like squad triviaqa pair one question one answer case question multiple answer kind evaluation metric use use f score
Why does Keras.preprocessing.sequence pad_sequences process characters instead of words?,"<p>I'm working on transcribing speech to text and ran into an issue (I think) when using <code>pad_sequences</code> in Keras. I pretrained a model which used <code>pad_sequences</code> on a dataframe and it fit the data into an array with the same number of columns &amp; rows for each value. However when I used <code>pad_sequences</code> on transcribing text, the number of characters in that spoken string is how many rows are returned as a numpy array.</p>
<p>Say I have a string with 4 characters then it will return a <code>4 X 500</code> Numpy array. For a string with 6 characters it will return <code>6 X 500</code> Numpy array and so on.</p>
<p>My code for clarification:</p>
<pre><code>import speech_recognition as sr
import pyaudio
import pandas as pd
from helperFunctions import *

jurors = ['Zack', 'Ben']
storage = []
storage_df = pd.DataFrame()


while len(storage) &lt; len(jurors):
    print('Juror' + ' ' + jurors[len(storage)] + ' ' + 'is speaking:')
    init_rec = sr.Recognizer()
    with sr.Microphone() as source:
        audio_data = init_rec.adjust_for_ambient_noise(source)
        audio_data = init_rec.listen(source) #each juror speaks for 10 seconds
        audio_text = init_rec.recognize_google(audio_data)
        print('End of juror' + ' ' + jurors[len(storage)] + ' ' + 'speech')
        storage.append(audio_text)
        cleaned = clean_text(audio_text)
        tokenized = tokenize_text(cleaned)
        padded_text = padding(cleaned, tokenized) #fix padded text elongating rows
</code></pre>
<p>I use a helper function script:</p>
<pre><code>def clean_text(text, stem=False):
    text_clean = '@\S+|https?:\S|[^A-Za-z0-9]+'
    text = re.sub(text_clean, ' ', str(text).lower()).strip()
    #text = tf.strings.substr(text, 0, 300) #restrict text size to 300 chars
    return text

def tokenize_text(text):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(text)
    return tokenizer

def padding(text, tokenizer):
    text = pad_sequences(tokenizer.texts_to_sequences(text), 
                       maxlen = 500)
    return text
</code></pre>
<p>The text returned will be fed into a pre-trained model and I'm pretty sure that different length rows will cause an issue.</p>
",Training and Model Evaluation,doe kera preprocessing sequence pad sequence process character instead word working transcribing speech text ran issue think using kera pretrained model used dataframe fit data array number column row value however used transcribing text number character spoken string many row returned numpy array say string character return numpy array string character return numpy array code clarification use helper function script text returned fed pre trained model pretty sure different length row cause issue
Giving higher weight to a labeling function in Snorkel,"<p>I am using snorkel to create labels for my training data. I currently have five labeling functions for the task which I have stored in a list. I am using the following code to apply the labeling function:</p>
<pre><code>lfs = [lf_a, lf_b, lf_c, lf_d, lf_e]
applier = PandasLFApplier(lfs)
L_train = applier.apply(df_data_sample)

# Train the label model and compute the training labels
label_model = LabelModel(cardinality=2, verbose=True)
label_model.fit(L_train, n_epochs=500, log_freq=50, seed=123)
</code></pre>
<p>going into the task I want to give a higher weight to <em>lf_e</em> labeling function as I my testing shows it has a higher accuracy than others. Not being able to do that leads to outputs from other lfs dominating output from <em>lf_e</em>. And I also don't want to remove any of the functions as I reduce my coverage if I do that.</p>
<p>Is there a way to do that in Snorkel?</p>
",Training and Model Evaluation,giving higher weight labeling function snorkel using snorkel create label training data currently five labeling function task stored list using following code apply labeling function going task want give higher weight lf e labeling function testing show ha higher accuracy others able lead output lf dominating output lf e also want remove function reduce coverage way snorkel
Loss on LSTM is starting with a low value and decreasing slowly until it stops,"<p>I have a bidirectional LSTM model that take words of a text as input, goes through an Embedding layer, a Bidirectional LSTM layer and finally though a Dense layer with 4 units and a softmax activation. The goal of this model is to predict if a word is an entity and what type of entity it is.</p>
<p>During training the model starts with a low validation and training loss (≈ 0.01) and decreases slowly until it stagnates. The texts in my dataset can vary in word length so I decided to pad the examples that had less than 2048 up to that value. The ones that had more than 2048 words (&lt;1% of the dataset) were split into two or more texts and the last split was padded until it had 2048 words. I decided to use 2048 as the maximum size because I wanted to avoid splitting the examples and using 2048 would only split less than 1% of the data.</p>
<p>I'm using 4 recall functions (one for each class) as metrics and, although they are not giving awful results right at the start (the worst one is currently giving 75% for one of the classes), they do not improve over time. My guess is that this is a vanishing gradient problem since the sequence length is very large, but I'm not sure of this. I'll try again using inputs of size = 1024. I don't think the dataset size is the problem here since the training dataset that I'm using has around 500k examples, and the validation dataset has 50k. If there is anything more I need to add please let me know and I'll do it as soon as possible.</p>
<p>My model summary:</p>
<pre><code>Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 2048)]            0
_________________________________________________________________
embedding (Embedding)        (None, 2048, 300)         15145800
_________________________________________________________________
bidirectional (Bidirectional (None, 2048, 256)         439296
_________________________________________________________________
dropout (Dropout)            (None, 2048, 256)         0
_________________________________________________________________
dense (Dense)                (None, 2048, 4)           1028
=================================================================
Total params: 15,586,124
Trainable params: 440,324
Non-trainable params: 15,145,800
</code></pre>
",Training and Model Evaluation,loss lstm starting low value decreasing slowly stop bidirectional lstm model take word text input go embedding layer bidirectional lstm layer finally though dense layer unit softmax activation goal model predict word entity type entity training model start low validation training loss decrease slowly stagnates text dataset vary word length decided pad example le value one word dataset split two text last split wa padded word decided use maximum size wanted avoid splitting example using would split le data using recall function one class metric although giving awful result right start worst one currently giving one class improve time guess vanishing gradient problem since sequence length large sure try using input size think dataset size problem since training dataset using ha around k example validation dataset ha k anything need add please let know soon possible model summary
Inconsistent vector representation using transformers BertModel and BertTokenizer,"<p>I have a <code>BertTokenizer</code> (<code>tokenizer</code>) and a <code>BertModel</code> (<code>model</code>) from the <code>transformers</code> library.
<strong>I have pre-trained the model from scratch</strong> with a few wikipedia articles, just to test how it works.</p>
<p>Once the model is pre-trained, <strong>I want to extract a layer vector representation for a given sentence</strong>. For that, I calculate the average of the 11 hidden (768-sized) vectors. I do this as follows (<code>line</code> is a single <code>String</code>):</p>
<pre><code>padded_sequence = tokenizer(line, padding=True)
        
indexed_tokens = padded_sequence['input_ids']
attention_mask = padded_sequence[&quot;attention_mask&quot;]

tokens_tensor = torch.tensor([indexed_tokens])
attention_mask_tensor = torch.tensor([attention_mask])

outputs = model(tokens_tensor, attention_mask_tensor)
hidden_states = outputs[0]

line_vectorized = hidden_states[0].data.numpy().mean(axis=0)
</code></pre>
<p>So far so good. <strong>I can do this for every sentence individually. But now I want to do it in batch</strong>, ie. I have a bunch of sentences and instead of iterating each sentence I send the appropiate tensor representations to get all vectors at once. I do this as follows (<code>lines</code> is a <code>list of Strings</code>):</p>
<pre><code>padded_sequences = self.tokenizer_PYTORCH(lines, padding=True)
        
indexed_tokens_list = padded_sequences['input_ids']
attention_mask_list = padded_sequences[&quot;attention_mask&quot;]
        
tokens_tensors_list = [torch.tensor([indexed_tokens]) for indexed_tokens in indexed_tokens_list]
attention_mask_tensors_list = [torch.tensor([attention_mask ]) for attention_mask in attention_mask_list ]
        
tokens_tensors = torch.cat((tokens_tensors_list), 0)
attention_mask_tensors = torch.cat((attention_mask_tensors_list ), 0)

outputs = model(tokens_tensors, attention_mask_tensors)
hidden_states = outputs[0]

lines_vectorized = [hidden_states[i].data.numpy().mean(axis=0) for i in range(0, len(hidden_states))]
</code></pre>
<p>The problem is the following: <strong>I have to use padding so that I can appropiately concatenate the token tensors</strong>. That means that the indexed tokens and the attention masks can be larger than in the previous case where the sentences were evaluated individually. <strong>But when I use padding, I get different results for the sentences which have been padded</strong>.</p>
<p><em>EXAMPLE</em>:
I have two sentences (in French but it doesn't matter):</p>
<p><code>sentence_A</code> = &quot;appareil digestif un article de wikipedia l encyclopedie libre&quot;</p>
<p><code>sentence_B</code> = &quot;sauter a la navigation sauter a la recherche cet article est une ebauche concernant la biologie&quot;</p>
<p>When I evaluate the two sentences <strong>individually</strong>, I obtain:</p>
<p><code>sentence_A</code>:</p>
<pre><code>indexed_tokens =  [10002, 3101, 4910, 557, 73, 3215, 9630, 2343, 4200, 8363, 10000]
attention_mask =  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
line_vectorized =  [-0.9304411   0.53798294 -1.6231083 ...]
</code></pre>
<p><code>sentence_B</code>:</p>
<pre><code>indexed_tokens =  [10002, 2217, 6496, 1387, 9876, 2217, 6496, 1387, 4441, 405, 73, 6451, 3, 2190, 5402, 1387, 2971, 10000]
attention_mask =  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
line_vectorized =  [-0.8077076   0.56028104 -1.5135447  ...]
</code></pre>
<p>But when I evaluate the two sentences <strong>in batch</strong>, I obtain:</p>
<p><code>sentence_A</code>:</p>
<pre><code>indexed_tokens =  [10002, 3101, 4910, 557, 73, 3215, 9630, 2343, 4200, 8363, 10000, 10004, 10004, 10004, 10004, 10004, 10004, 10004]
attention_mask =  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
line_vectorized =  [-1.0473819   0.6090186  -1.727466  ...]
</code></pre>
<p><code>sentence_B</code>:</p>
<pre><code>indexed_tokens =  [10002, 2217, 6496, 1387, 9876, 2217, 6496, 1387, 4441, 405, 73, 6451, 3, 2190, 5402, 1387, 2971, 10000]
attention_mask =  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
line_vectorized =  [-0.8077076   0.56028104 -1.5135447  ...]
</code></pre>
<p>That is, <strong>since <code>sentence_B</code> is larger than <code>sentence_A</code>, <code>sentence_A</code> has been padded and the attention mask has been padded with zeros as well</strong>. The indexed tokens contain now extra tokens (<code>10004</code> which I assume <code>empty</code>).
The vector representation of <code>sentence_B</code> has NOT changed. But <strong>the vector representation of <code>sentence_A</code> HAS CHANGED</strong>.</p>
<p>I would like to know if this is working as intended or not (I assume not).
And I guess I am doing something wrong but I can't figure out what.</p>
<p>Any ideas?</p>
",Training and Model Evaluation,inconsistent vector representation using transformer bertmodel berttokenizer library pre trained model scratch wikipedia article test work model pre trained want extract layer vector representation given sentence calculate average hidden sized vector follows single far good every sentence individually want batch ie bunch sentence instead iterating sentence send appropiate tensor representation get vector follows problem following use padding appropiately concatenate token tensor mean indexed token attention mask larger previous case sentence evaluated individually use padding get different result sentence padded example two sentence french matter appareil digestif un article de wikipedia l encyclopedie libre sauter la navigation sauter la recherche cet article est ebauche concernant la biologie evaluate two sentence individually obtain evaluate two sentence batch obtain since larger ha padded attention mask ha padded zero well indexed token contain extra token assume vector representation ha changed vector representation ha changed would like know working intended assume guess something wrong figure idea
How to update GloVe models?,"<p>In my work, I used my own corpus to train a Word2Vec model using gensim. Then I used several small corpus to &quot;update&quot; that model (producing different sets of vectors). This process well documented in gensim.</p>
<p>I am trying to replicate a similar process with GloVe model. I could find the code to train my own GloVe model <a href=""https://github.com/stanfordnlp/GloVe"" rel=""nofollow noreferrer"">here</a>.
However, I am not sure how to go about updating this model with different new corpus. Does it even make sense to &quot;update&quot; a GloVe model?</p>
<p><a href=""https://stackoverflow.com/a/56383823/8464088"">This</a> answer says no. But strong confirmation will help.</p>
",Training and Model Evaluation,update glove model work used corpus train word vec model using gensim used several small corpus update model producing different set vector process well documented gensim trying replicate similar process glove model could find code train glove model however sure go updating model different new corpus doe even make sense update glove model href answer say strong confirmation help p
Determining the &quot;goodness&quot; of a phrase based on &quot;grammatical&quot; or &quot;contextual&quot; relevancy,"<p>Given a random string of words, I would like to assign a &quot;goodness&quot; score to the phrase, where &quot;goodness&quot; is some indication of <strong>grammatical</strong> and <strong>contextual</strong> relevancy.
For example:</p>
<pre><code>&quot;the green tree was tall&quot; [Good score]
&quot;delicious tires swim open&quot; [Medium score]
&quot;jump an con porch calmly&quot; [Poor score]
</code></pre>
<p>I've been experimenting with the <a href=""https://www.nltk.org"" rel=""nofollow noreferrer"">Natural Language Toolkit</a>. I'd considered using a trained tagger to assign parts-of-speech to each word in a phrase, and then parse a corpus for occurrences of that POS pattern. This may give me an indication of grammatical &quot;goodness&quot;. However, as the tagger itself is trained on the same corpus that I'm using for validation, I can't imagine the results would be reliable. This approach also does not take into consideration the contextual relevancy of the words.
Is anyone aware of existing projects or research into this sort of thing? How would you approach this?</p>
",Training and Model Evaluation,determining goodness phrase based grammatical contextual relevancy given random string word would like assign goodness score phrase goodness indication grammatical contextual relevancy example experimenting natural language toolkit considered using trained tagger assign part speech word phrase parse corpus occurrence po pattern may give indication grammatical goodness however tagger trained corpus using validation imagine result would reliable approach also doe take consideration contextual relevancy word anyone aware existing project research sort thing would approach
Training word2vec model,"<p>How to train the Google's pre-trained model Word2Vec model(googlenews-negative 300) with new words? I have a corpus of new medical terms but their word vectors aren't present in the model given by google. So, how to train new words in word2vec model?</p>
",Training and Model Evaluation,training word vec model train google pre trained model word vec model googlenews negative new word corpus new medical term word vector present model given google train new word word vec model
RuntimeError: Cost Function returns nan values in its 1th output,"<p>I am working on a temporal model to predict future events. Here is the <a href=""https://drive.google.com/file/d/1B5FTnKpohPlywiRKQBXNjfR4fwka8FiE/view?usp=sharing"" rel=""nofollow noreferrer"">link</a> to my colab notebook. I am facing an issue trying to train the model. I am getting NaN values for Train and valid loss. The loss function is a joint loss consisting of the cross entropy loss and the squared loss.  Link to the blog <a href=""https://sparalic.github.io/post/using-electronic-health-records-to-predict-future-diagnosis-codes-with-gated-recurrent-units/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Tried following solution which didn`t worked -
Smaller learning rate - 0.01, 0.001, 0.0001</p>
<pre><code>class cost_function():
    def __init__(self, yhat, y, L_2=0.001, logEps=1e-8):
        # logEps : log epsilon, very small positive value greater that 0.0
        # CE = - [ ln(y*)(y) + ln(1-y*)(1-y) ],

        self.yhat = yhat
        self.y = y
       
        self.logEps = logEps
        self.L_2 = L_2
        
        self.W_out = nn.Parameter(torch.randn(hiddenDimSize, numClass)*0.01)
        
    def cross_entropy(self):
        ce = -(self.y * torch.log(self.yhat + self.logEps) + (1. - self.y) * torch.log(1. - self.yhat + self.logEps))
        print(&quot;Inside CrossEntrophy Loss fn : &quot;, ce)
        return ce

    def prediction_loss(self):
        # return  (torch.sum(torch.sum(self.cross_entropy(), dim=0),dim=1)).float()/  lengths.float()
        
        tmp_tensor = torch.sum(self.cross_entropy(), dim=0)
        print(&quot;Inside PredictionLoss fn : Sum Dim 0&quot;, tmp_tensor)
        print(&quot;Inside PredictionLoss fn : Sum Dim 1&quot;, torch.sum(tmp_tensor,dim=1))
        print(&quot;Inside PredictionLoss fn : Final Result &quot;, (torch.sum(tmp_tensor,dim=1)).float()/  lengths.float())
        return (torch.sum(tmp_tensor,dim=1)).float()/  lengths.float()
        
    def cost(self):
        print(&quot;Inside Cost fn :&quot;, torch.mean(self.prediction_loss()) + self.L_2 * (self.W_out ** 2).sum())
        return torch.mean(self.prediction_loss()) + self.L_2 * (self.W_out ** 2).sum() # regularize
    
</code></pre>
<p>build_EHRNN class -  I have made modification in the forward method params to resolve the 'h' is not defined error.</p>
<pre><code>torch.manual_seed(1)

class build_EHRNN(nn.Module):
    def __init__(self, inputDimSize=4894, hiddenDimSize=[200,200], batchSize=100, embSize=200, numClass=4894, dropout=0.5, logEps=1e-8):
        super(build_EHRNN, self).__init__()
        
        self.inputDimSize = inputDimSize
        self.hiddenDimSize = hiddenDimSize
        self.numClass = numClass
        self.embSize = embSize
        self.batchSize = batchSize
        self.dropout = nn.Dropout(p=0.5)
        self.logEps = logEps
        
        
        # Embedding inputs
        self.W_emb = nn.Parameter(torch.randn(self.inputDimSize, self.embSize).cuda())
        self.b_emb = nn.Parameter(torch.zeros(self.embSize).cuda())
        
        self.W_out = nn.Parameter(torch.randn(self.hiddenDimSize, self.numClass).cuda())
        self.b_out = nn.Parameter(torch.zeros(self.numClass).cuda())
         
        self.params = [self.W_emb, self.W_out, 
                       self.b_emb, self.b_out] 
    
    # def forward(self,x, y, h, lengths, mask):
    def forward(self,x, y, lengths, mask):
        self.emb = torch.tanh(torch.matmul(x, self.W_emb) + self.b_emb)
        input_values = self.emb
        self.outputs = [input_values]
        for i, hiddenSize in enumerate([self.hiddenDimSize, self.hiddenDimSize]):  # iterate over layers
            rnn = EHRNN(self.inputDimSize,hiddenSize,self.embSize,self.batchSize,self.numClass) # calculate hidden states
            hidden_state = []
            h = self.init_hidden().cuda()
            for i,seq in enumerate(input_values): # loop over sequences in each batch
                h = rnn(seq, h)                    
                hidden_state.append(h)    
            hidden_state = self.dropout(torch.stack(hidden_state))    # apply dropout between layers
            input_values = hidden_state
       
        y_linear = torch.matmul(hidden_state, self.W_out)  + self.b_out # fully connected layer
        yhat = F.softmax(y_linear, dim=1)  # yhat
        yhat = yhat*mask[:,:,None]   # apply mask
        
        # Loss calculation
        cross_entropy = -(y * torch.log(yhat + self.logEps) + (1. - y) * torch.log(1. - yhat + self.logEps))
        last_step = -torch.mean(y[-1] * torch.log(yhat[-1] + self.logEps) + (1. - y[-1]) * torch.log(1. - yhat[-1] + self.logEps))
        prediction_loss = torch.sum(torch.sum(cross_entropy, dim=0),dim=1)/ torch.cuda.FloatTensor(lengths)
        cost = torch.mean(prediction_loss) + 0.000001 * (self.W_out ** 2).sum() # regularize
        return (yhat, hidden_state, cost)

    def init_hidden(self):
        return torch.zeros(self.batchSize, self.hiddenDimSize)  # initial state
</code></pre>
<p>model training</p>
<pre><code>artificalData_seqs = np.array(pickle.load(open(os.path.join(GOOGLE_DRV_PATH,BASE_DIR,'data.encodedDxs'),'rb')))
train, test, valid = load_data(artificalData_seqs, artificalData_seqs)

batchSize = 50     # decreased from 100 to 50
n_batches = int(np.ceil(float(len(train[0])) / float(batchSize)))-1
n_batches_valid = int(np.ceil(float(len(valid[0])) / float(batchSize)))-1
model = build_EHRNN(inputDimSize=4894, hiddenDimSize=200, batchSize=50, embSize=200, numClass=4894, dropout=0.5, logEps=1e-8)
model = model.to(device)



 import torch.nn.functional as F
import pdb

optimizer = torch.optim.Adadelta(model.parameters(), lr = 0.001, rho=0.95)
epochs = 10

counter = 0
# with torch.autograd.detect_anomaly():
for e in range(epochs):
    for x, y in train_dl:
        x, y , mask, lengths = padding(x, y, inputDimSize, numClass)
        output, h = model(x, mask)
        
        loss = cost_function(output, y).cost()
        # pdb.set_trace()
        loss.backward()
        print(&quot;loss &quot;,loss)
        nn.utils.clip_grad_norm_(model.parameters(), 5) # Constraining the weight matrix directly == regularization. 
        optimizer.step()
        optimizer.zero_grad()
    
    with torch.no_grad():
            model.eval()
            val_loss = []
            for x_valid, y_valid in valid_dl:
                    x_val, y_val, mask, lengths = padding(x_valid, y_valid, inputDimSize, numClass)
                    outputs_val, hidden_val = model(x_val,  mask)
                    loss = cost_function(outputs_val, y_val).cost()
                    val_loss.append(loss.item())
            model.train()

            print(&quot;Epoch: {}/{}...&quot;.format(e+1, epochs),
                                &quot;Step: {}...&quot;.format(counter),
                                &quot;Training Loss: {:.4f}...&quot;.format(loss.item()),
                                &quot;Val Loss: {:.4f}&quot;.format(torch.mean(torch.tensor(val_loss))))
</code></pre>
<p>Error (Start getting NaNs in the loss)</p>
<pre><code>Inside PredictionLoss fn : Sum Dim 0 tensor([[0.1008, 0.1539, 0.1211,  ..., 0.1533, 0.1218, 0.1418],
        [0.0253, 0.0449, 0.0249,  ..., 0.0439, 0.0134, 0.0332],
        [0.0306, 0.0799, 0.0570,  ..., 0.0790, 0.0484, 0.0678],
        ...,
        [0.0253, 0.0450, 0.0249,  ..., 0.0439, 0.0134, 0.0332],
        [0.0253, 0.0450, 0.0249,  ..., 0.0439, 0.0134, 0.0332],
        [0.0038, 0.0106, 0.0106,  ..., 0.0098, 0.0004, 0.0106]],
       grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : Sum Dim 1 tensor([  372.4754,   133.2620,   219.1195,    37.5425,   141.3354,    37.5070,
          229.2947,     0.0000,   379.1829,   217.3962,    80.1226,    37.5074,
          138.4665,    82.1034,    89.7893,    81.8173,    92.8159,   141.8856,
           95.9898,   216.0511,   133.2535,   385.0391,   369.4958,   244.9362,
           37.5088,    37.5087,   141.6083,     0.0000,    95.3367,    37.5074,
          735.0569,   378.0407,    37.5135,    40.7778,    82.0872,   225.9998,
          216.6189,   379.0732,    81.4742,   144.4226,    93.3905,   214.0228,
           37.5078,   224.0793,    88.3753,    41.2919,   140.4855,    37.5086,
          226.6366,   148.7171,   137.9226, 13887.5811,    81.1428,    84.6804,
          226.6779,    37.5065,   223.8841,   220.5979,    83.2484,    37.5080,
           84.5247,   384.2115,    80.1173,     0.0000,   146.9714,    37.6982,
          134.6618,    84.1838,    37.5421,   730.5516,    37.5085,     0.0000,
          215.1523,   136.5673,    81.2887,    37.5080,    94.4181,   140.6268,
          133.9295,   136.2485,    80.1173,   386.2103,    39.0282,     0.0000,
           37.5055,    42.1506,    80.1662,   228.5819,    39.3403,   138.7672,
         1768.6033,   143.5350,    40.2060,   147.7809,    37.5085,   380.9214,
          750.6883,   141.0447,   136.9028,    37.5049],
       grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : lengths tensor([5., 3., 4., 1., 3., 1., 4., 0., 5., 4., 2., 1., 3., 2., 2., 2., 2., 3.,
        2., 4., 3., 5., 5., 4., 1., 1., 3., 0., 2., 1., 6., 5., 1., 1., 2., 4.,
        4., 5., 2., 3., 2., 4., 1., 4., 2., 1., 3., 1., 4., 3., 3., 9., 2., 2.,
        4., 1., 4., 4., 2., 1., 2., 5., 2., 0., 3., 1., 3., 2., 1., 6., 1., 0.,
        4., 3., 2., 1., 2., 3., 3., 3., 2., 5., 1., 0., 1., 1., 2., 4., 1., 3.,
        7., 3., 1., 3., 1., 5., 6., 3., 3., 1.])
Inside PredictionLoss fn : Final Result  tensor([  74.4951,   44.4207,   54.7799,   37.5425,   47.1118,   37.5070,
          57.3237,       nan,   75.8366,   54.3491,   40.0613,   37.5074,
          46.1555,   41.0517,   44.8946,   40.9086,   46.4080,   47.2952,
          47.9949,   54.0128,   44.4178,   77.0078,   73.8992,   61.2340,
          37.5088,   37.5087,   47.2028,       nan,   47.6683,   37.5074,
         122.5095,   75.6081,   37.5135,   40.7778,   41.0436,   56.5000,
          54.1547,   75.8146,   40.7371,   48.1409,   46.6952,   53.5057,
          37.5078,   56.0198,   44.1876,   41.2919,   46.8285,   37.5086,
          56.6591,   49.5724,   45.9742, 1543.0646,   40.5714,   42.3402,
          56.6695,   37.5065,   55.9710,   55.1495,   41.6242,   37.5080,
          42.2623,   76.8423,   40.0586,       nan,   48.9905,   37.6982,
          44.8873,   42.0919,   37.5421,  121.7586,   37.5085,       nan,
          53.7881,   45.5224,   40.6443,   37.5080,   47.2090,   46.8756,
          44.6432,   45.4162,   40.0587,   77.2421,   39.0282,       nan,
          37.5055,   42.1506,   40.0831,   57.1455,   39.3403,   46.2557,
         252.6576,   47.8450,   40.2060,   49.2603,   37.5085,   76.1843,
         125.1147,   47.0149,   45.6343,   37.5049], grad_fn=&lt;DivBackward0&gt;)
Inside PredictionLoss fn : Sum Dim 0 tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : Sum Dim 1 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : lengths tensor([2., 1., 1., 2., 3., 5., 3., 3., 1., 4., 2., 2., 2., 3., 2., 2., 5., 2.,
        4., 2., 1., 4., 2., 5., 3., 0., 4., 7., 6., 4., 4., 1., 7., 1., 3., 3.,
        5., 3., 5., 5., 4., 2., 2., 4., 1., 5., 6., 2., 5., 5., 2., 1., 3., 1.,
        4., 4., 3., 3., 0., 2., 2., 4., 2., 2., 1., 0., 3., 2., 3., 0., 1., 2.,
        4., 4., 5., 1., 1., 3., 3., 2., 5., 0., 2., 6., 5., 5., 5., 1., 2., 4.,
        0., 2., 6., 1., 2., 0., 1., 1., 2., 3.])
Inside PredictionLoss fn : Final Result  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], grad_fn=&lt;DivBackward0&gt;)
Inside PredictionLoss fn : Sum Dim 0 tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : Sum Dim 1 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : lengths tensor([3., 2., 4., 4., 6., 1., 1., 4., 2., 2., 4., 2., 1., 1., 1., 1., 3., 1.,
        2., 3., 4., 2., 2., 3., 2., 0., 1., 3., 4., 1., 2., 3., 4., 3., 3., 1.,
        2., 3., 3., 2., 1., 4., 6., 3., 4., 2., 3., 0., 1., 1., 3., 7., 2., 3.,
        4., 2., 3., 0., 3., 3., 2., 1., 1., 3., 6., 2., 2., 3., 2., 3., 3., 2.,
        1., 2., 5., 3., 4., 2., 5., 3., 5., 5., 5., 3., 2., 1., 4., 3., 3., 3.,
        1., 4., 3., 2., 5., 3., 6., 4., 3., 2.])
Inside PredictionLoss fn : Final Result  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], grad_fn=&lt;DivBackward0&gt;)
Inside PredictionLoss fn : Sum Dim 0 tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : Sum Dim 1 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : lengths tensor([4., 6., 2., 2., 4., 5., 0., 3., 2., 3., 4., 3., 3., 3., 4., 3., 0., 1.,
        2., 2., 4., 2., 4., 4., 1., 0., 3., 0., 5., 5., 1., 2., 3., 2., 2., 3.,
        1., 2., 1., 2., 3., 1., 3., 1., 3., 0., 4., 2., 1., 6., 2., 2., 0., 1.,
        4., 4., 2., 2., 0., 3., 5., 5., 5., 5., 3., 4., 4., 4., 5., 4., 2., 2.,
        3., 1., 5., 6., 3., 4., 1., 3., 5., 1., 3., 2., 3., 6., 2., 4., 3., 2.,
        3., 2., 4., 1., 2., 2., 1., 2., 3., 4.])
Inside PredictionLoss fn : Final Result  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], grad_fn=&lt;DivBackward0&gt;)
Inside PredictionLoss fn : Sum Dim 0 tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : Sum Dim 1 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : lengths tensor([1., 5., 1., 5., 3., 0., 1., 4., 3., 3., 5., 5., 2., 0., 1., 2., 3., 2.,
        2., 2., 2., 2., 2., 2., 2., 3., 4., 1., 4., 2., 1., 3., 4., 0., 4., 2.,
        0., 4., 1., 1., 2., 3., 3., 5., 1., 3., 2., 3., 1., 1., 3., 6., 3., 0.,
        2., 3., 2., 3., 3., 4., 1., 5., 2., 6., 2., 1., 5., 3., 2., 1., 3., 5.,
        2., 0., 3., 0., 1., 3., 3., 2., 3., 1., 2., 4., 1., 2., 7., 2., 7., 2.,
        2., 2., 1., 1., 1., 2., 4., 2., 2., 3.])
Inside PredictionLoss fn : Final Result  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], grad_fn=&lt;DivBackward0&gt;)
Inside PredictionLoss fn : Sum Dim 0 tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : Sum Dim 1 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : lengths tensor([2., 2., 1., 4., 3., 2., 2., 4., 5., 2., 3., 1., 1., 2., 3., 1., 3., 1.,
        3., 1., 6., 4., 2., 2., 0., 4., 1., 0., 2., 0., 1., 1., 1., 1., 2., 2.,
        3., 1., 3., 2., 0., 1., 3., 2., 4., 1., 2., 1., 6., 2., 1., 2., 1., 2.,
        2., 1., 1., 1., 4., 1., 0., 2., 2., 2., 3., 6., 5., 1., 1., 3., 3., 3.,
        1., 4., 4., 2., 4., 2., 3., 7., 1., 1., 2., 3., 0., 0., 1., 4., 2., 3.,
        1., 2., 3., 2., 7., 0., 2., 3., 4., 4.])
Inside PredictionLoss fn : Final Result  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], grad_fn=&lt;DivBackward0&gt;)
Inside PredictionLoss fn : Sum Dim 0 tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : Sum Dim 1 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : lengths tensor([1., 2., 6., 7., 2., 1., 1., 3., 1., 2., 2., 4., 3., 3., 4., 3., 2., 2.,
        5., 2., 1., 3., 2., 2., 2., 1., 4., 3., 2., 2., 1., 4., 2., 0., 2., 2.,
        1., 3., 4., 4., 5., 1., 5., 6., 1., 3., 1., 3., 0., 1., 6., 1., 2., 0.,
        3., 2., 2., 3., 4., 3., 3., 2., 3., 2., 2., 3., 4., 3., 2., 0., 3., 1.,
        1., 3., 1., 4., 7., 2., 2., 1., 1., 1., 2., 3., 3., 2., 2., 3., 1., 3.,
        3., 1., 1., 3., 1., 2., 7., 3., 1., 6.])
Inside PredictionLoss fn : Final Result  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], grad_fn=&lt;DivBackward0&gt;)
Inside PredictionLoss fn : Sum Dim 0 tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : Sum Dim 1 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : lengths tensor([2., 2., 4., 2., 5., 1., 2., 4., 2., 2., 2., 5., 1., 2., 1., 4., 1., 2.,
        4., 3., 4., 0., 2., 5., 2., 6., 2., 6., 3., 4., 3., 5., 7., 4., 3., 5.,
        1., 3., 2., 3., 2., 2., 2., 3., 3., 3., 2., 3., 1., 4., 3., 3., 2., 1.,
        6., 9., 2., 1., 6., 3., 1., 5., 1., 6., 2., 2., 6., 2., 4., 2., 4., 3.,
        2., 2., 4., 1., 2., 2., 2., 1., 3., 1., 3., 2., 3., 3., 2., 2., 4., 3.,
        3., 3., 5., 4., 3., 2., 4., 2., 1., 8.])
Inside PredictionLoss fn : Final Result  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], grad_fn=&lt;DivBackward0&gt;)
Inside PredictionLoss fn : Sum Dim 0 tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : Sum Dim 1 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], grad_fn=&lt;SumBackward1&gt;)
Inside PredictionLoss fn : lengths tensor([1., 3., 2., 3., 3., 2., 7., 4., 3., 1., 1., 5., 1., 3., 4., 4., 1., 5.,
        1., 3., 3., 1., 4., 3., 0., 4., 4., 2., 4., 1., 2., 4., 3., 2., 3., 2.,
        2., 2., 2., 3., 3., 2., 5., 1., 2., 1., 2., 2., 3., 2., 2., 2., 3., 3.,
        3., 5., 1., 4., 8., 4., 0., 2., 4., 2., 0., 1., 4., 4., 1., 5., 0., 1.,
        3., 1., 2., 3., 3., 4., 3., 4., 2., 4., 2., 3., 4., 1., 1., 2., 4., 1.,
        1., 0., 0., 0., 4., 3., 1., 3., 4., 3.])
Inside PredictionLoss fn : Final Result  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], grad_fn=&lt;DivBackward0&gt;)
Inside PredictionLoss fn : Sum Dim 0 tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=&lt;SumBackward1&gt;)
</code></pre>
",Training and Model Evaluation,runtimeerror cost function return nan value th output working temporal model predict future event link colab notebook facing issue trying train model getting nan value train valid loss loss function joint loss consisting cross entropy loss squared loss link blog tried following solution worked smaller learning rate build ehrnn class made modification forward method params resolve h defined error model training error start getting nan loss
word2vec: CBOW &amp; skip-gram performance wrt training dataset size,"<p>The question is simple. Which of the CBOW &amp; skip-gram works better for a big dataset? (And the answer for small dataset follows.)</p>

<p>I am confused since, by Mikolov himself, <a href=""https://groups.google.com/forum/#!searchin/word2vec-toolkit/c-bow/word2vec-toolkit/NLvYXU99cAM/E5ld8LcDxlAJ"" rel=""noreferrer"">[Link]</a></p>

<blockquote>
  <p>Skip-gram: works well with <strong>small amount of the training data</strong>, represents well even rare words or phrases. <br/> <br/>
  CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words</p>
</blockquote>

<p>but, according to Google TensorFlow, <a href=""https://www.tensorflow.org/versions/r0.10/tutorials/word2vec/index.html"" rel=""noreferrer"">[Link]</a></p>

<blockquote>
  <p>CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets.<br/><br/>However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have <strong>larger datasets</strong>. We will focus on the skip-gram model in the rest of this tutorial.</p>
</blockquote>

<p>Here is a Quora post which supports the first thought <a href=""https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures-in-laymans-terms"" rel=""noreferrer"">[Link]</a>, and then there is the other Quora post which suggests the second thought <a href=""https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures-in-laymans-terms"" rel=""noreferrer"">[Link]</a>--both seem derivable from the aforementioned credible sources.</p>

<p>Or is it like what Mikolov said:</p>

<blockquote>
  <p>Overall, the best practice is to try few experiments and see what works the best for you, as different applications have different requirements.</p>
</blockquote>

<p>But surely there is an empirical or analytical verdict or final saying on this matter?</p>
",Training and Model Evaluation,word vec cbow skip gram performance wrt training dataset size question simple cbow skip gram work better big dataset answer small dataset follows confused since mikolov link skip gram work well small amount training data represents well even rare word phrase cbow several time faster train skip gram slightly better accuracy frequent word according google tensorflow link cbow smoothes lot distributional information treating entire context one observation part turn useful thing smaller datasets however skip gram treat context target pair new observation tends better larger datasets focus skip gram model rest tutorial quora post support first thought link quora post suggests second thought link seem derivable aforementioned credible source like mikolov said overall best practice try experiment see work best different application different requirement surely empirical analytical verdict final saying matter
Construct word2vec (CBOW) training data from beginning of sentence,"<p>When constructing training data for CBOW, <a href=""https://arxiv.org/abs/1301.3781"" rel=""nofollow noreferrer"">Mikolov et al.</a> suggest using the word from the center of a context window. What is the &quot;best&quot; approach to capturing words at the beginning/end of a sentence (I put best in quotes because I'm sure this depends on the task). Implementations I see online do something like the this:</p>
<pre><code>for i in range(2, len(raw_text) - 2):
    context = [raw_text[i - 2], raw_text[i - 1],
               raw_text[i + 1], raw_text[i + 2]]
</code></pre>
<p>I see two issues arising from this approach.</p>
<ul>
<li><strong>Issue 1:</strong> The approach gives imbalanced focus to the middle of the sentence. For example, the first word of the sentence can only appear in 1 context window and will never appear as the target word. Compare this to the 4th word in the sentence which will appear in 4 context windows and will also be a target word. This will be an issue as some words appear frequently at the beginning of sentences (i.e. however, thus, etc.). Wouldn't this approach minimize their use?</li>
<li><strong>Issue 2:</strong> Sentences with 4 or fewer words are completely ignored, and the importance of short sentences is minimized. For example, a sentence with 5 words can only contribute one training sample while a sentence of length 8 will contribute 4 training samples.</li>
</ul>
<p>Can anyone offer insight as to how much these issues affect the results or any alternative approaches for constructing the training data? (I considered letting the first word be the target word and using the next N words as the context, but this creates issues of it's own).</p>
<p>Related question on Stack Exchange:
<a href=""https://datascience.stackexchange.com/questions/81249/construct-word2vec-cbow-training-data-from-beginning-of-sentence"">Construct word2vec (CBOW) training data from beginning of sentence</a></p>
",Training and Model Evaluation,construct word vec cbow training data beginning sentence constructing training data cbow mikolov et al suggest using word center context window best approach capturing word beginning end sentence put best quote sure depends task implementation see online something like see two issue arising approach issue approach give imbalanced focus middle sentence example first word sentence appear context window never appear target word compare th word sentence appear context window also target word issue word appear frequently beginning sentence e however thus etc approach minimize use issue sentence fewer word completely ignored importance short sentence minimized example sentence word contribute one training sample sentence length contribute training sample anyone offer insight much issue affect result alternative approach constructing training data considered letting first word target word using next n word context creates issue related question stack exchange href word vec cbow training data beginning sentence
How to decode obfuscated text for NLP problem?,"<p>I have training data as obfuscated text for an NLP classification problem. The data has been obfuscated, however the patterns in them are preserved. I want to decode it. I don't know how. Also, I'm not sure if it needs to be decoded for training the models. Any ideas? Two sample lines from the text are below:</p>
<pre><code>    tvletwgzkrqvuhtwamuluhpkskpmpmiwtvuhamqvmviwlrvikquhtwamuluhqgvipmmvulkriwpmqvtwleuhamqvmviwlrvikquhtwamuluhqgqvqvtwviezlemvxeuhamqvmviwlrvikquhtwamuluhpkskvieniwlrvikquhqvmvuhqgpmpmiwletwulenokuhxepmuhtwiwululentvuhtwamuluhvimvuhsktwlemvezskenuhtwtvuhulqvkrezuhamypmvamdfuhulenamguuhraskvipmyptwqvuhtwamuluhxepmuhvimvenulgzenypuhenuhsatvuhvipmdfuhqgletwsklepmuhulqvlemvxeuhtwamuluhxepmuhtwiwululentvuhenuhqvmvuhpmpmiwletwulenok

    qvmvuhskleenmvviengzxyuhqvmvamguuhrakrpmsauhulyptwulpmlegzuhskigbhbhkrpmsauhulyptwulpmlegzuhskigbhbhpmfquhraskiwlepmdfuhtwamuluhiwiwenuhlepmxeuhskentwamuhlekrpmsauhxepmuhlemvenamuhenuhullekramuhulviskiwkrpmdfuhiguhratwezuhxepmuhskmvenlexeenuhtwtvuhulvipmskuhqgsapmtvuhsaqjuhraletwskvikriwtvuhletwulxeenuhletwskvikriwtvuhtwskenezuhtwamuhskvienuhezmvamuhvipmgzkruhqvendfuhtwsatwuhqvvienezuhskiwpmuhtwamuluhulkrtv
</code></pre>
",Training and Model Evaluation,decode obfuscated text nlp problem training data obfuscated text nlp classification problem data ha obfuscated however pattern preserved want decode know also sure need decoded training model idea two sample line text
did I correctly build my siamese network?,"<p>I am trying to build my siamese network for finding text-similarity. below is the model.
Want to know whether There are sharing weigths.</p>
<pre><code>sub_model=tf.keras.models.Sequential([Embedding(vocab_size,300,input_length=79), 
Bidirectional(LSTM(79,return_sequences=True)),
Bidirectional(LSTM(79,return_sequences=True)),
tf.keras.layers.GlobalAveragePooling1D(),
tf.keras.layers.Dense(units=158)
])
ins1=Input((79,),name='input1')
ins2=Input((79,),name='input2')
sub1=sub_model(ins1)
sub2=sub_model(ins2)
norm1=tf.keras.layers.Layer(lambda x: tf.math.l2_normalize(x,axis=1),name='out1')(sub1)
norm2=tf.keras.layers.Layer(lambda x: tf.math.l2_normalize(x,axis=1),name='out2')(sub2)
model=Model({'input1':ins1,'input2':ins2},outputs={'out1':norm1,'out2':norm2})
</code></pre>
<p>when training, gradients are not flowing through the network. I found that by writing custom training loop. the loss function which I wrote is working properly.It is giving the loss value. But gradients are getting 'None' value.
here is the <a href=""https://i.sstatic.net/2gu9H.png"" rel=""nofollow noreferrer"">model_plot</a></p>
",Training and Model Evaluation,correctly build siamese network trying build siamese network finding text similarity model want know whether sharing weigths training gradient flowing network found writing custom training loop loss function wrote working properly giving loss value gradient getting none value model plot
OpenNLP doccat trainer always results in &quot;1 outcome patterns&quot;,"<p>I am evaluating OpenNLP for use as a document categorizer. I have a sanitized training corpus with roughly 4k files, in about 150 categories. The documents have many shared, mostly irrelevant words - but many of those words become relevant in n-grams, so I'm using the following parameters:</p>
<pre><code>        TrainingParameters params = new TrainingParameters();
        params.put(TrainingParameters.ITERATIONS_PARAM, 20000);
        params.put(TrainingParameters.CUTOFF_PARAM, 10);
        DoccatFactory dcFactory = new DoccatFactory(new FeatureGenerator[] { new NGramFeatureGenerator(3, 10) });
        params.put(AbstractTrainer.ALGORITHM_PARAM, NaiveBayesTrainer.NAIVE_BAYES_VALUE);
</code></pre>
<p>Some of these categories apply to documents that are almost completely identical (think boiler-plate legal documents, with maybe only names and addresses different between document instances) - and will be mostly identical to documents in the test set. However, no matter how I tweak these params, I can't break out of the &quot;1 outcome patterns&quot; result. When running a test, every document in the test set is tagged with &quot;Category A.&quot;</p>
<p>I did manage to effect a single minor change in output, by moving from previous use of the BagOfWordsFeatureGenerator to the NGramFeatureGenerator, and from maxent to Naive Bayes; before the change, every document in the test set was assigned &quot;Category A&quot;, but after the change, all the documents were now assigned to &quot;Category B.&quot; But other than that, I can't seem to move the dial at all.</p>
<p>I've tried fiddling with iterations, cutoff, ngram sizes, using maxent instead of bayes, etc; but all to no avail.</p>
<p>Example code from tutorials that I've found on the interweb have used much smaller training sets with less iterations, and are able to perform at least some rudimentary differentation.</p>
<p>Usually in such a situation - bewildering lack of expected behavior - the engineer has forgotten to flip some simple switch, or has some fatal lack of fundamental understanding. I am eminently capable of both those failures. Also, I have no Data Science training, although I have read a couple of O'Reilly books on the subject. So the problem could be procedural. Is the training set too small? Is the number of iterations off by an order of magnitude? Would a different algo be a better fit? I'm utterly surprised that no tweaks have even slightly moved the dial away from the &quot;1 outcome&quot; outcome.</p>
<p>Any response appreciated.</p>
",Training and Model Evaluation,opennlp doccat trainer always result outcome pattern evaluating opennlp use document categorizer sanitized training corpus roughly k file category document many shared mostly irrelevant word many word become relevant n gram using following parameter category apply document almost completely identical think boiler plate legal document maybe name address different document instance mostly identical document test set however matter tweak params break outcome pattern result running test every document test set tagged category manage effect single minor change output moving previous use bagofwordsfeaturegenerator ngramfeaturegenerator maxent naive bayes change every document test set wa assigned category change document assigned category b seem move dial tried fiddling iteration cutoff ngram size using maxent instead bayes etc avail example code tutorial found interweb used much smaller training set le iteration able perform least rudimentary differentation usually situation bewildering lack expected behavior engineer ha forgotten flip simple switch ha fatal lack fundamental understanding eminently capable failure also data science training although read couple reilly book subject problem could procedural training set small number iteration order magnitude would different algo better fit utterly surprised tweak even slightly moved dial away outcome outcome response appreciated
"pytorch: the dropout layer after LayerNorm, There are some magical phenomena","<p>When I add a dropout layer after LayerNorm，the validation set loss reduction at 1.5 epoch firstly，then the loss Substantially increase，and the acc becomes 0; when I remove the dropout layer, it works; when I remove the layernorm, it changes , not zero, but results was very poor.</p>
<p>the model code:</p>
<pre><code>class Model(nn.Module):
    def __init__(self, config):
        super(Model, self).__init__()
        if config.embedding_pretrained is not None:
            #self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)
            self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(config.embedding_pretrained.vectors), freeze=False)
        else:
            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)
        self.lstm = nn.LSTM(config.embed, config.hidden_size, config.num_layers,
                            bidirectional=True, batch_first=True, dropout=config.dropout)
        self.maxpool = nn.MaxPool1d(config.pad_size)
        self.fc = nn.Linear(config.hidden_size * 2 + config.embed, config.num_classes)
        self.dropout = nn.Dropout(config.dropout)
        
        self.ln_emb = nn.LayerNorm(config.embed)
        self.ln_lstm = nn.LayerNorm(config.hidden_size * 2)

    def forward(self, x):
        x, _ = x
        embed = self.embedding(x)  # [batch_size, seq_len, embeding]
        embed = self.ln_emb(embed)
        out, _ = self.lstm(embed)
        out = self.ln_lstm(out)

        out = torch.cat((embed, out), 2)
        out = self.dropout(out)
        #out = F.relu(out)
        out = out.permute(0, 2, 1)
        out = self.maxpool(out).squeeze()
        out = self.fc(out)
        return out
</code></pre>
<ul>
<li>the acc tensorboard:</li>
</ul>
<p><img src=""https://i.sstatic.net/NdnYk.png"" alt=""the acc curve"" /></p>
<ul>
<li>the loss tensorboard:</li>
</ul>
<p><img src=""https://i.sstatic.net/ur0xG.png"" alt=""the loss curve"" /></p>
",Training and Model Evaluation,pytorch dropout layer layernorm magical add dropout layer layernorm validation set loss reduction epoch firstly loss substantially increase acc becomes remove dropout layer work remove layernorm change zero result wa poor model code acc tensorboard loss tensorboard
Why my model training on one epoch takes almost 2 hours?,"<p>I'm working on Neural Machine Translation (Fr to En). I developed (with Quadro P5000 GPU 16278MiB) a my first model which is a simple sequence-to-sequence with LSTM.</p>
<pre class=""lang-py prettyprint-override""><code># Model definition

class EncoderLSTM(nn.Module):
    
    def __init__(
        self,
        embedding_size,
        vocab_size,
        hidden_size,
        n_layers,
        dropout,
        recurrent_dropout
    ):
        super(EncoderLSTM, self).__init__()
        self.embedding_size = embedding_size
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        self.dropout = dropout
        self.recurrent_dropout = recurrent_dropout
        self.embedding = nn.Embedding(vocab_size, embedding_size)
        self.lstm = nn.LSTM(embedding_size, hidden_size,
                            num_layers=n_layers,
                            bidirectional=True,
                            dropout=(recurrent_dropout if n_layers &gt; 1 else 0))
        
    def load_pretrained_embeddings(self, embeddings):
        self.embedding.weight = nn.Parameter(embeddings)
        
    def fine_tuning_embeddings(self, fine_tune=True):
        for p in self.embedding.parameters():
            p.requires_grad = fine_tune
    
    def forward(self, input_sequences, sequence_lengths):
        &quot;&quot;&quot;
        :params
            input_sequences: Tensor[seq_len, batch_size]
            sequence_lengths: Tensor[batch_size,]
            
        :return
            outputs: Tensor[seq_len, batch_size, 2 * hidden_size]
            hn: Tensor[n_layers * 2, batch_size, hidden_size]
            cn: Tensor[n_layers * 2, batch_size, hidden_size]
        &quot;&quot;&quot;
        embedded = self.embedding(input_sequences)
        embedded = F.dropout(embedded, p=self.dropout)
        packed = nn.utils.rnn.pack_padded_sequence(embedded, sequence_lengths)
        outputs, (hn, cn) = self.lstm(packed)
        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)
        return outputs, hn, cn


class DecoderLSTM(nn.Module):
    
    def __init__(
        self,
        embedding_size,
        vocab_size,
        hidden_size,
        n_layers,
        dropout,
        recurrent_dropout
    ):
        super(DecoderLSTM, self).__init__()
        self.embedding_size = embedding_size
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        self.dropout = dropout
        self.recurrent_dropout = recurrent_dropout
        self.embedding = nn.Embedding(vocab_size, embedding_size)
        self.lstm = nn.LSTM(embedding_size, hidden_size,
                            num_layers=n_layers,
                            dropout=(recurrent_dropout if n_layers &gt; 1 else 0))
        self.fc = nn.Linear(hidden_size, vocab_size)
        
    def load_pretrained_embeddings(self, embeddings):
        self.embedding.weight = nn.Parameter(embeddings)
        
    def fine_tuning_embeddings(self, fine_tune=True):
        for p in self.embedding.parameters():
            p.requires_grad = fine_tune
        
    def forward(self, input_word_index, h_state, c_state):
        &quot;&quot;&quot;
        :params
            input_word_index: Tensor[batch_size,]
            h_state: Tensor[num_layers, batch_size, hidden_size]
            c_state: Tensor[num_layers, batch_size, hidden_size]
            
        :return
            logit: Tensor[batch_size, vocab_size]
            h_state: Tensor[num_layers, batch_size, hidden_size]
            c_state: Tensor[num_layers, batch_size, hidden_size]
        &quot;&quot;&quot;
        embedded = self.embedding(input_word_index.unsqueeze(0))
        outputs, (h_state, c_state) = self.lstm(embedded, (h_state, c_state))
        logit = self.fc(F.dropout(outputs, p=self.dropout))
        logit = logit.squeeze(0)
        return logit, h_state, c_state


class SeqToSeqLSTM(nn.Module):
    
    def __init__(self, encoder, decoder, device):
        assert encoder.n_layers == decoder.n_layers, \
            'Encoder and Decoder must have the same number of reccurent layers'
        assert encoder.hidden_size == decoder.hidden_size, \
            'Encoder and Decoder must have the same number of reccurrent hidden units'
        
        super(SeqToSeqLSTM, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.init_h0 = nn.Linear(decoder.n_layers * 2, decoder.n_layers) 
        self.init_c0 = nn.Linear(decoder.n_layers * 2, decoder.n_layers)
        self.device = device
        
    def forward(self, src_sequences, src_lengths, dest_sequences, dest_lengths, tf_ratio):
        &quot;&quot;&quot;
        :params
            src_sequences: Tensor[seq_len, batch_size]
            src_lengths: Tensor[batch_size,]
            dest_sequences: Tensor[seq_len, batch_size]
            dest_lengths: Tensor[batch_size,]
            tf_ratio: float
            
        :return
            logits: Tensor[max(decode_lengths), batch_size, vocab_size]
            sorted_dest_sequences: Tensor[seq_len, batch_size]
            sorted_decode_lengths: Tensor[batch_size,]
            sorted_indices: Tensor[batch_size,]
        &quot;&quot;&quot;
        # Encoding
        _, h_state, c_state = self.encoder(
            input_sequences=src_sequences,
            sequence_lengths=src_lengths
        )
        # h_state: [n_layers * 2, batch_size, hidden_size]
        # c_state: [n_layers * 2, batch_size, hidden_size]
        
        # Sort the batch (dest) by decreasing lengths
        sorted_dest_lengths, sorted_indices = torch.sort(dest_lengths, dim=0, descending=True)
        sorted_dest_sequences = dest_sequences[:, sorted_indices]
        h_state = h_state[:, sorted_indices, :]
        c_state = c_state[:, sorted_indices, :]
        
        # Init hidden and memory states
        h_state = self.init_h0(h_state.permute(1, 2, 0)) # [batch_size, hidden_size, n_layers]
        c_state = self.init_c0(c_state.permute(1, 2, 0)) # [batch_size, hidden_size, n_layers]
        h_state = h_state.permute(2, 0, 1) # [n_layers, batch_size, hidden_size]
        c_state = c_state.permute(2, 0, 1) # [n_layers, batch_size, hidden_size]
        
        # We won't decode at the &lt;eos&gt; position, since we've finished generating as soon as we generate &lt;eos&gt;
        # So, decoding lengths are actual lengths - 1
        sorted_decode_lengths = (sorted_dest_lengths - 1).tolist()
        
        # Decoding
        batch_size, last = dest_sequences.size(1), None
        logits = torch.zeros(max(sorted_decode_lengths), batch_size, self.decoder.vocab_size).to(self.device)
        for t in range(max(sorted_decode_lengths)):
            batch_size_t = sum([l &gt; t for l in sorted_decode_lengths])
            if last is not None:
                if random.random() &lt; tf_ratio:
                    in_ = last[:batch_size_t]
                else:
                    in_ = sorted_dest_sequences[t, :batch_size_t]
            else:
                in_ = sorted_dest_sequences[t, :batch_size_t]
            # in_ [batch_size,]
            logit, h_state, c_state = self.decoder(
                in_, 
                h_state[:, :batch_size_t, :].contiguous(),
                c_state[:, :batch_size_t, :].contiguous()
            )
            # logit: [batch_size, vocab_size]
            # h_state: [num_layers, batch_size, hidden_size]
            # c_state: [num_layers, batch_size, hidden_size]
            logits[t, :batch_size_t, :] = logit
            last = torch.argmax(F.softmax(logit, dim=1), dim=1) # [batch_size,]
        
        return logits, sorted_dest_sequences, sorted_decode_lengths, sorted_indices

# Model initialization
MODEL_NAME = 'seq2seq-lstm'
N_LAYERS = 4
HIDDEN_SIZE = 512
EMBEDDING_SIZE = 300
ENC_DROPOUT = 0.3
ENC_RECURRENT_DROPOUT = 0.25
DEC_DROPOUT = 0.15
DEC_RECURRENT_DROPOUT = 0.2
N_EPOCHS = 15
BATCH_SIZE = 64
LEARNING_RATE = 1e-3
GRAD_CLIP = 1.0
TF_RATIO = 1.0

encoder = EncoderLSTM(
    embedding_size=EMBEDDING_SIZE,
    vocab_size=len(FR.vocab),
    hidden_size=HIDDEN_SIZE,
    n_layers=N_LAYERS,
    dropout=ENC_DROPOUT,
    recurrent_dropout=ENC_RECURRENT_DROPOUT
)
encoder.load_pretrained_embeddings(fr_embeddings)
encoder.fine_tuning_embeddings(fine_tune=True)
decoder = DecoderLSTM(
    embedding_size=EMBEDDING_SIZE,
    vocab_size=len(EN.vocab),
    hidden_size=HIDDEN_SIZE,
    n_layers=N_LAYERS,
    dropout=DEC_DROPOUT,
    recurrent_dropout=DEC_RECURRENT_DROPOUT
)
decoder.load_pretrained_embeddings(en_embeddings)
decoder.fine_tuning_embeddings(fine_tune=True)
seq2seq = SeqToSeqLSTM(encoder=encoder, decoder=decoder, device=DEVICE)
seq2seq.apply(torch_utils.xavier_init_weights)
seq2seq.to(DEVICE)
optimizer = optim.RMSprop(params=seq2seq.parameters(), lr=LEARNING_RATE)
criterion = nn.CrossEntropyLoss()
print(f'Number of parameters of the model: {torch_utils.count_parameters(seq2seq):,}')
# Number of parameters of the model: 41,471,097
</code></pre>
<p>With the model above, the training time for one epoch is 04:31 minutes.</p>
<p>When I add attention mechanism (Luong style), the training time for one epoch is 1:55:47 hours. I'm wondering why?</p>
<pre class=""lang-py prettyprint-override""><code># Model definition
class EncoderLSTM(nn.Module):
    
    def __init__(
        self,
        embedding_size,
        vocab_size,
        hidden_size,
        n_layers,
        dropout,
        recurrent_dropout
    ):
        super(EncoderLSTM, self).__init__()
        self.embedding_size = embedding_size
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        self.dropout = dropout
        self.recurrent_dropout = recurrent_dropout
        self.embedding = nn.Embedding(vocab_size, embedding_size)
        self.lstm = nn.LSTM(embedding_size, hidden_size,
                            num_layers=n_layers,
                            bidirectional=True,
                            dropout=(recurrent_dropout if n_layers &gt; 1 else 0))
        
    def load_pretrained_embeddings(self, embeddings):
        self.embedding.weight = nn.Parameter(embeddings)
        
    def fine_tuning_embeddings(self, fine_tune=True):
        for p in self.embedding.parameters():
            p.requires_grad = fine_tune
    
    def forward(self, input_sequences, sequence_lengths):
        &quot;&quot;&quot;
        :params
            input_sequences: Tensor[seq_len, batch_size]
            sequence_lengths: Tensor[batch_size,]
            
        :return
            outputs: Tensor[seq_len, batch_size, 2 * hidden_size]
            hn: Tensor[n_layers * 2, batch_size, hidden_size]
            cn: Tensor[n_layers * 2, batch_size, hidden_size]
        &quot;&quot;&quot;
        embedded = self.embedding(input_sequences)
        embedded = F.dropout(embedded, p=self.dropout)
        packed = nn.utils.rnn.pack_padded_sequence(embedded, sequence_lengths)
        outputs, (hn, cn) = self.lstm(packed)
        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)
        return outputs, hn, cn


class LuongAttention(nn.Module):
    
    def __init__(self, hidden_size, method):
        if method not in ['dot', 'concat']:
            raise NotImplemented(f'The {method} attention is not defined!')
        
        super(LuongAttention, self).__init__()
        self.hidden_size = hidden_size
        self.method = method
        if method == 'dot':
            pass
        elif method == 'concat':
            self.W = nn.Linear(hidden_size, hidden_size)
            self.V = nn.Linear(hidden_size, 1)
        else:
            raise NotImplementedError(f'{method} not implemented!')
            
    def forward(self, h_state, enc_outputs, mask):
        &quot;&quot;&quot;
        :args
        h_state: Tensor[n_layers, batch_size, hidden_size]
        enc_outputs: Tensor[seq_len, batch_size, hidden_size]
        mask: Tensor[seq_len, batch_size]

        :return
            attn_weights: Tensor[seq_len, batch_size, 1]
        &quot;&quot;&quot;
        if h_state.shape[0] &gt; 1:
            h_state = h_state.sum(dim=0) # [batch_size, hidden_size]
            h_state = h_state.unsqueeze(0) # [1, batch_size, hidden_size]

        # Calculating the alignment scores
        if self.method == 'dot':
            scores = torch.sum(h_state * enc_outputs, dim=2)
            scores = scores.unsqueeze(dim=2) / np.sqrt(self.hidden_size) # [seq_len, batch_size, 1]
        elif self.method == 'concat':
            scores = self.V(
                torch.tanh(self.W(
                    enc_outputs + h_state # [seq_len, batch_size, hidden_size]
                ))
            ) # [seq_len, batch_size, 1]
        else:
            raise NotImplementedError(f'{method} not implemented!')

        # Apply mask to ignore &lt;pad&gt; tokens
        mask = mask.unsqueeze(2) # [seq_len, batch_size, 1]
        scores = scores.masked_fill(mask == 0, -1e10)

        # Calculating the attention weights by softmaxing the alignment scores
        attn_weights = F.softmax(scores, dim=1) # [seq_len, batch_size, 1]

        return attn_weights


class DecoderLSTM(nn.Module):
    
    def __init__(
        self,
        embedding_size,
        vocab_size,
        hidden_size,
        n_layers,
        dropout,
        recurrent_dropout,
        attention
    ):
        super(DecoderLSTM, self).__init__()
        self.embedding_size = embedding_size
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        self.dropout = dropout
        self.recurrent_dropout = recurrent_dropout
        self.attention = attention
        self.embedding = nn.Embedding(vocab_size, embedding_size)
        self.lstm = nn.LSTM(embedding_size, hidden_size,
                            num_layers=n_layers,
                            dropout=(recurrent_dropout if n_layers &gt; 1 else 0))
        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)
        self.fc2 = nn.Linear(hidden_size, vocab_size)
        
    def load_pretrained_embeddings(self, embeddings):
        self.embedding.weight = nn.Parameter(embeddings)
        
    def fine_tuning_embeddings(self, fine_tune=True):
        for p in self.embedding.parameters():
            p.requires_grad = fine_tune
        
    def forward(self, input_word_index, h_state, c_state, enc_outputs, mask):
        &quot;&quot;&quot;
        :params
            input_word_index: Tensor[batch_size,]
            h_state: Tensor[num_layers, batch_size, hidden_size]
            c_state: Tensor[num_layers, batch_size, hidden_size]
            enc_outputs: Tensor[seq_len, batch_size, hidden_size] 
            mask: Tensor[seq_len, batch_size]
            
        :return
            logit: Tensor[batch_size, vocab_size]
            h_state: Tensor[num_layers, batch_size, hidden_size]
            c_state: Tensor[num_layers, batch_size, hidden_size]
            attn_weights: Tensor[batch_size, seq_len]
        &quot;&quot;&quot;
        embedded = self.embedding(input_word_index.unsqueeze(0)) # [seq_len=1, batch_size, embedding_size]
        outputs, (h_state, c_state) = self.lstm(embedded, (h_state, c_state))
        # outputs: [seq_len=1, batch_size, hidden_size]
        # h_state: [n_layers, batch_size, hidden_size]
        # c_state: [n_layers, batch_size, hidden_size]
        
        # Compute Attention Weights
        attn_weights = self.attention(h_state=outputs,
                                      enc_outputs=enc_outputs,
                                      mask=mask) # [seq_len, batch_size, 1]
        
        # Compute Context Vector
        context_vector = torch.bmm(
            enc_outputs.permute(1, 2, 0), # [batch_size, hidden_size, seq_len]
            attn_weights.permute(1, 0, 2), # [batch_size, seq_len, 1]
        ).permute(2, 0, 1) # [1, batch_size, hidden_size]
        
        # New input: concatenate context_vector with hidden_states
        new_input = torch.cat((context_vector, outputs), dim=2) # [1, batch_size, hidden_size * 2]
        
        # Get logit
        out = torch.tanh(self.fc1(new_input.squeeze(0)))
        logit = self.fc2(out) # [batch_size, vocab_size]
        
        return logit, h_state, c_state, attn_weights.squeeze(2)

class SeqToSeqLSTM(nn.Module):
    
    def __init__(self, encoder, decoder, pad_index, device):
        assert encoder.n_layers == decoder.n_layers, \
            'Encoder and Decoder must have the same number of reccurent layers'
        assert encoder.hidden_size == decoder.hidden_size, \
            'Encoder and Decoder must have the same number of reccurrent hidden units'
        
        super(SeqToSeqLSTM, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.pad_index = pad_index
        self.init_h0 = nn.Linear(decoder.n_layers * 2, decoder.n_layers) 
        self.init_c0 = nn.Linear(decoder.n_layers * 2, decoder.n_layers)
        self.fc = nn.Linear(2 * encoder.hidden_size, encoder.hidden_size)
        self.device = device
        
    def create_mask(self, src_sequences):
        &quot;&quot;&quot;
        :params
            src_sequences: Tensor[seq_len, batch_size]
            
        :return
            mask: Tensor[seq_len, batch_size]
        &quot;&quot;&quot;
        mask = (src_sequences != self.pad_index)
        return mask
        
    def forward(self, src_sequences, src_lengths, dest_sequences, dest_lengths, tf_ratio):
        &quot;&quot;&quot;
        :params
            src_sequences: Tensor[seq_len, batch_size]
            src_lengths: Tensor[batch_size,]
            dest_sequences: Tensor[seq_len, batch_size]
            dest_lengths: Tensor[batch_size,]
            tf_ratio: float
            
        :return
            logits: Tensor[max(decode_lengths), batch_size, vocab_size]
            sorted_dest_sequences: Tensor[seq_len, batch_size]
            sorted_decode_lengths: Tensor[batch_size,]
            sorted_indices: Tensor[batch_size,]
        &quot;&quot;&quot;
        mask = self.create_mask(src_sequences) # [seq_len, batch_size]
        
        # Encoding
        enc_outputs, h_state, c_state = self.encoder(
            input_sequences=src_sequences,
            sequence_lengths=src_lengths
        )
        # enc_outputs: [seq_len, batch_size, 2 * hidden_size]
        # h_state: [n_layers * 2, batch_size, hidden_size]
        # c_state: [n_layers * 2, batch_size, hidden_size]
        
        enc_outputs = self.fc(enc_outputs)
        # enc_outputs: [seq_len, batch_size, hidden_size]
        
        # Sort the batch (dest) by decreasing lengths
        sorted_dest_lengths, sorted_indices = torch.sort(dest_lengths, dim=0, descending=True)
        sorted_dest_sequences = dest_sequences[:, sorted_indices]
        enc_outputs = enc_outputs[:, sorted_indices, :]
        h_state = h_state[:, sorted_indices, :]
        c_state = c_state[:, sorted_indices, :]
        
        # Init hidden and memory states
        h_state = self.init_h0(h_state.permute(1, 2, 0)) # [batch_size, hidden_size, n_layers]
        c_state = self.init_c0(c_state.permute(1, 2, 0)) # [batch_size, hidden_size, n_layers]
        h_state = h_state.permute(2, 0, 1) # [n_layers, batch_size, hidden_size]
        c_state = c_state.permute(2, 0, 1) # [n_layers, batch_size, hidden_size]
        
        # We won't decode at the &lt;eos&gt; position, since we've finished generating as soon as we generate &lt;eos&gt;
        # So, decoding lengths are actual lengths - 1
        sorted_decode_lengths = (sorted_dest_lengths - 1).tolist()
        
        # Decoding
        batch_size, last = dest_sequences.size(1), None
        logits = torch.zeros(max(sorted_decode_lengths), batch_size, self.decoder.vocab_size).to(self.device)
        for t in range(max(sorted_decode_lengths)):
            batch_size_t = sum([l &gt; t for l in sorted_decode_lengths])
            if last is not None:
                if random.random() &lt; tf_ratio:
                    in_ = last[:batch_size_t]
                else:
                    in_ = sorted_dest_sequences[t, :batch_size_t]
            else:
                in_ = sorted_dest_sequences[t, :batch_size_t]
            # in_ [batch_size,]
            logit, h_state, c_state, _ = self.decoder(
                in_, 
                h_state[:, :batch_size_t, :].contiguous(),
                c_state[:, :batch_size_t, :].contiguous(),
                enc_outputs[:, :batch_size_t, :],
                mask[:, :batch_size_t]
            )
            # logit: [batch_size, vocab_size]
            # h_state: [num_layers, batch_size, hidden_size]
            # c_state: [num_layers, batch_size, hidden_size]
            logits[t, :batch_size_t, :] = logit
            last = torch.argmax(F.softmax(logit, dim=1), dim=1) # [batch_size,]
        
        return logits, sorted_dest_sequences, sorted_decode_lengths, sorted_indices

# Model initialization
encoder = EncoderLSTM(embedding_size=EMBEDDING_SIZE,
                      vocab_size=len(FR.vocab),
                      hidden_size=HIDDEN_SIZE,
                      n_layers=N_LAYERS,
                      dropout=ENC_DROPOUT,
                      recurrent_dropout=ENC_RECURRENT_DROPOUT)
encoder.load_pretrained_embeddings(fr_embeddings)
encoder.fine_tuning_embeddings(fine_tune=True)
attention = LuongAttention(hidden_size=HIDDEN_SIZE, method='dot')
decoder = DecoderLSTM(embedding_size=EMBEDDING_SIZE,
                      vocab_size=len(EN.vocab),
                      hidden_size=HIDDEN_SIZE,
                      n_layers=N_LAYERS,
                      dropout=DEC_DROPOUT,
                      recurrent_dropout=DEC_RECURRENT_DROPOUT,
                      attention=attention)
decoder.load_pretrained_embeddings(en_embeddings)
decoder.fine_tuning_embeddings(fine_tune=True)
seq2seq = SeqToSeqLSTM(encoder=encoder,
                       decoder=decoder,
                       pad_index=EN.vocab.stoi[EN.pad_token],
                       device=DEVICE)
seq2seq.apply(torch_utils.xavier_init_weights)
seq2seq.to(DEVICE)
optimizer = optim.RMSprop(params=seq2seq.parameters(), lr=LEARNING_RATE)
criterion = nn.CrossEntropyLoss()
print(f'Number of parameters of the model: {torch_utils.count_parameters(seq2seq):,}')
# Number of parameters of the model: 42,520,697
</code></pre>
<p>The two models have nearly the same number of parameters.</p>
<p>The data come from the europarl parallel corpora with 89,752 sampled examples where each example have a length between 15 and 25.</p>
<p>Do someone know why the model with attention mechanism takes so long for one epoch?</p>
",Training and Model Evaluation,model training one epoch take almost hour working neural machine translation fr en developed quadro p gpu mib first model simple sequence sequence lstm model training time one epoch minute add attention mechanism luong style training time one epoch hour wondering two model nearly number parameter data come europarl parallel corpus sampled example example length someone know model attention mechanism take long one epoch
Gensim&#39;s word2vec has a loss of 0 from epoch 1?,"<p>I am using the Word2vec module of Gensim library to train a word embedding, the dataset is 400k sentences with 100k unique words (its not english)</p>
<p>I'm using this code to monitor and calculate the loss :</p>
<pre class=""lang-py prettyprint-override""><code>class MonitorCallback(CallbackAny2Vec):
    def __init__(self, test_words):
        self._test_words = test_words

    def on_epoch_end(self, model):
        print(&quot;Model loss:&quot;, model.get_latest_training_loss())  # print loss
        for word in self._test_words:  # show wv logic changes
            print(model.wv.most_similar(word))


monitor = MonitorCallback([&quot;MyWord&quot;])  # monitor with demo words

w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT  , callbacks=[monitor])

w2v_model.build_vocab(tokenized_corpus)

words = w2v_model.wv.vocab.keys()
vocab_size = len(words)
print(&quot;Vocab size&quot;, vocab_size)

print(&quot;[*] Training...&quot;)

# Train Word Embeddings
w2v_model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=W2V_EPOCH)
</code></pre>
<p>The problem is from epoch 1 the loss is 0 and the vector of the monitored words dont change at all!</p>
<pre><code>[*] Training...
Model loss: 0.0
Model loss: 0.0
Model loss: 0.0
Model loss: 0.0
</code></pre>
<p>so what is the problem here? is this normal?  the tokenized corpus is a list of lists that are something like tokenized_corpus[0] = [ &quot;word1&quot; , &quot;word2&quot; , ...]</p>
<p>I googled and seems like some of the old versions of gensim had problem with calculating loss function, but they are from almost a year ago and it seems like it should be fixed right now?</p>
<p>I tried the code provided in the answer of this question as well but still the loss is 0 :</p>
<p><a href=""https://stackoverflow.com/questions/52038651/loss-does-not-decrease-during-training-word2vec-gensim"">Loss does not decrease during training (Word2Vec, Gensim)</a></p>
<p>EDIT1 : after adding compute_loss=True, the loss shows up, but it keeps going higher and higher, and the top similar words and their similarity doesn't change at all :</p>
<pre><code>Model loss: 2187903.5
Model loss: 3245492.0
Model loss: 4103624.5
Model loss: 4798541.0
Model loss: 5413940.0
Model loss: 5993822.5
Model loss: 6532631.0
Model loss: 7048384.5
Model loss: 7547147.0
</code></pre>
",Training and Model Evaluation,gensim word vec ha loss epoch using word vec module gensim library train word embedding dataset k sentence k unique word english using code monitor calculate loss problem epoch loss vector monitored word dont change problem normal tokenized corpus list list something like tokenized corpus word word googled seems like old version gensim problem calculating loss function almost year ago seems like fixed right tried code provided answer question well still loss href doe decrease training word vec gensim edit adding compute loss true loss show keep going higher higher top similar word similarity change
How to build semantic search for a given domain,"<p>There is a problem we are trying to solve where we want to do a semantic search on our set of data,
i.e we have a domain-specific data (example: sentences talking about automobiles)</p>

<p>Our data is just a bunch of sentences and what we want is to give a phrase and get back the sentences which are:</p>

<ol>
<li>Similar to that phrase</li>
<li>Has a part of a sentence that is similar to the phrase</li>
<li>A sentence which is having contextually similar meanings </li>
</ol>

<p><br/></p>

<p>Let me try giving you an example suppose I search for the phrase ""Buying Experience"", I should get the sentences like:</p>

<ul>
<li>I never thought car buying could take less than 30 minutes to sign
and buy.</li>
<li><p>I found a car that i liked and the purchase process was<br>
straightforward and easy</p></li>
<li><p>I absolutely hated going car shopping, but today i’m glad i did</p></li>
</ul>

<p><br/>
I want to lay emphasis on the fact that we are looking for <strong>contextual similarity</strong> and not just a brute force word search.</p>

<p>If the sentence uses different words then also it should be able to find it.</p>

<p>Things that we have already tried:</p>

<ol>
<li><p><a href=""https://www.opensemanticsearch.org/"" rel=""noreferrer"">Open Semantic Search</a> the problem we faced here is generating ontology from the data we have, or
for that sake searching for available ontology from different domains of our interest.</p></li>
<li><p>Elastic Search(BM25 + Vectors(tf-idf)), we tried this where it gave a few sentences but precision was not that great. The accuracy was bad
as well. We tried against a human-curated dataset, it was able to get around 10% of the sentences only.</p></li>
<li><p>We tried different embeddings like the once mentioned in <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""noreferrer"">sentence-transformers</a> and also went through the <a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.p"" rel=""noreferrer"">example</a> and tried evaluating against our human-curated set
and that also had very low accuracy.</p></li>
<li><p>We tried <a href=""https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604"" rel=""noreferrer"">ELMO</a>. This was better but still lower accuracy than we expected and there is a
cognitive load to decide the cosine value below which we shouldn't consider the sentences. This even applies to point 3.</p></li>
</ol>

<p>Any help will be appreciated. Thanks a lot for the help in advance</p>
",Training and Model Evaluation,build semantic search given domain problem trying solve want semantic search set data e domain specific data example sentence talking automobile data bunch sentence want give phrase get back sentence similar phrase ha part sentence similar phrase sentence contextually similar meaning let try giving example suppose search phrase buying experience get sentence like never thought car buying could take le minute sign buy found car liked purchase process wa straightforward easy absolutely hated going car shopping today glad want lay emphasis fact looking contextual similarity brute force word search sentence us different word also able find thing already tried open semantic search problem faced generating ontology data sake searching available ontology different domain interest elastic search bm vector tf idf tried gave sentence precision wa great accuracy wa bad well tried human curated dataset wa able get around sentence tried different embeddings like mentioned sentence transformer also went example tried evaluating human curated set also low accuracy tried elmo wa better still lower accuracy expected cognitive load decide cosine value consider sentence even applies point help appreciated thanks lot help advance
Lookup table not working in training data of Rasa NLU,"<p>I have examples for a particular intent also showing the entity, and I want the model to recognize other words which could be entities for that particular intent, but it fails to recognize it.</p>

<pre><code>## intent: frequency
* what is the frequency of [region](field)?
* what's the frequency of[region](field)?
* frequency of [region](field)?
* [region](field)s frequency?
* [region](field) frequency?
* frequency [region](field)?

## lookup: field
* price
* phone type
* region
</code></pre>

<p>So when I enter the text ""What is the frequency of region?"" I get the output</p>

<pre><code>{'intent': {'name': 'frequency', 'confidence': 0.9517087936401367},
'entities': [{'start': 17, 'end': 23, 'value': 'region', 
'entity': 'field', 'confidence': 0.9427971487440825, 
'extractor': 'CRFEntityExtractor'}], 'text': 'What is the frequency of region?'}
</code></pre>

<p>but when I enter the text ""What is the frequency of price?"" I get the output</p>

<pre><code>{'intent': {'name': 'frequency', 'confidence': 0.9276150465011597},
'entities': [], 'text': 'What is the frequency of price?'}
</code></pre>
",Training and Model Evaluation,lookup table working training data rasa nlu example particular intent also showing entity want model recognize word could entity particular intent fails recognize enter text frequency region get output enter text frequency price get output
Training GPT2 and Reformer from scratch,"<p>I am looking, for example, script/notebook to train GPT2 and Reformer model from scratch in German.
Something similar to :</p>
<blockquote>
<p><a href=""https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb</a></p>
</blockquote>
<p>I am trying to modify the same notebook but GPT2 doesn't seem to accept LinebyLineDataset or padding.</p>
<p>My Error is:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;timed eval&gt; in &lt;module&gt;

~/anaconda3/envs/thesis_p1/lib/python3.6/site-packages/transformers/trainer.py in train(self, model_path)
    490                 self._past = None
    491 
--&gt; 492             for step, inputs in enumerate(epoch_iterator):
    493 
    494                 # Skip past any already trained steps if resuming training

~/anaconda3/envs/thesis_p1/lib/python3.6/site-packages/tqdm/notebook.py in __iter__(self, *args, **kwargs)
    226     def __iter__(self, *args, **kwargs):
    227         try:
--&gt; 228             for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):
    229                 # return super(tqdm...) will not catch exception
    230                 yield obj

~/anaconda3/envs/thesis_p1/lib/python3.6/site-packages/tqdm/std.py in __iter__(self)
   1128 
   1129         try:
-&gt; 1130             for obj in iterable:
   1131                 yield obj
   1132                 # Update and possibly print the progressbar.

~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)
    344     def __next__(self):
    345         index = self._next_index()  # may raise StopIteration
--&gt; 346         data = self.dataset_fetcher.fetch(index)  # may raise StopIteration
    347         if self.pin_memory:
    348             data = _utils.pin_memory.pin_memory(data)

~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)
     45         else:
     46             data = self.dataset[possibly_batched_index]
---&gt; 47         return self.collate_fn(data)

~/anaconda3/envs/thesis_p1/lib/python3.6/site-packages/transformers/data/data_collator.py in __call__(self, examples)
     79 
     80     def __call__(self, examples: List[torch.Tensor]) -&gt; Dict[str, torch.Tensor]:
---&gt; 81         batch = self._tensorize_batch(examples)
     82         if self.mlm:
     83             inputs, labels = self.mask_tokens(batch)

~/anaconda3/envs/thesis_p1/lib/python3.6/site-packages/transformers/data/data_collator.py in _tensorize_batch(self, examples)
     96             if self.tokenizer._pad_token is None:
     97                 raise ValueError(
---&gt; 98                     &quot;You are attempting to pad samples but the tokenizer you are using&quot;
     99                     f&quot; ({self.tokenizer.__class__.__name__}) does not have one.&quot;
    100                 )

ValueError: You are attempting to pad samples but the tokenizer you are using (GPT2Tokenizer) does not have one.
</code></pre>
<p>Here is my current implementation:</p>
<p>Datasets looks like this (Million lines):</p>
<pre><code>1   &quot;09.05.2019, Flyer: Zeit für Perspektiven - Unterstützung im Haushalt durch professionelle Dienstleistungen&quot;
2   %0A%0ADie Burg Werle (ca. 10 km von hier entfernt) war ein schwer einnehmbarer Schlupfwinkel.
3   %0A%0AHier, abseits der verkehrsreichen Straßen, liegt das idyllische Quellental, ein Naturdenkmal der besonderen Art.
4   ½ bis 1 Tasse (75–150 ml) HEITMANN Reine Citronensäure in ½ Liter Wasser geben und in den Wassertank der Maschine füllen.
5   %0% der anfallenden Kosten ergeben sich aus der Straßenbeleuchtung.
6   ¾ Parken während der Ladezeit in Fußgängerzonen, in denen das Be- oder Entladen für bestimmte Zeiten freigegeben ist.
</code></pre>
<p>First I train Sentence Piece Tokenizer:</p>
<pre><code>from pathlib import Path
import sentencepiece as spm
paths = [str(x) for x in Path(&quot;.&quot;).glob(&quot;**/*.txt&quot;)]
arg='--input=deu-de_web-public_2019_1M-sentences.txt --model_prefix=m_test --vocab_size=52000'
spm.SentencePieceTrainer.train(arg)
</code></pre>
<p>Then I load my GPT2 tokenizer as below:</p>
<pre><code>from transformers import GPT2TokenizerFast

tokenizer = GPT2Tokenizer.from_pretrained(&quot;./German&quot;,additional_special_tokens=[&quot;&lt;s&gt;&quot;,&quot;&lt;pad&gt;&quot;,&quot;&lt;/s&gt;&quot;,&quot;&lt;unk&gt;&quot;,&quot;&lt;mask&gt;&quot;], max_len=512)
</code></pre>
<p>Here is my GPT2 config and language Model:</p>
<pre><code>from transformers import GPT2LMHeadModel, GPT2Config

# Initializing a GPT2 configuration
configuration = GPT2Config(vocab_size=52_000)
model = GPT2LMHeadModel(config=configuration)
</code></pre>
<p>The logic for Dataset Preparation:</p>
<pre><code>from transformers import LineByLineTextDataset

dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path=&quot;./deu-de_web-public_2019_1M-sentences.txt&quot;,
    block_size=128,
)
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False,
)
</code></pre>
<p>The training logic:</p>
<pre><code>from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir=&quot;./output&quot;,
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_gpu_train_batch_size=64,
    save_steps=10_000,
    save_total_limit=2,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
    prediction_loss_only=True,
)
trainer.train()
</code></pre>
",Training and Model Evaluation,training gpt reformer scratch looking example script notebook train gpt reformer model scratch german something similar trying modify notebook gpt seem accept linebylinedataset padding error current implementation datasets look like million line first train sentence piece tokenizer load gpt tokenizer gpt config language model logic dataset preparation training logic
How to change parameters of Bert model for a better performance on test set?,"<p>I'm working on an NLP Task from Kaggle competition, the purpose is to predict if a tweet expresses a real disaster or not. I'm using BertForSequenceClassification.</p>
<p>My Training set size is 10000, I split it into:</p>
<ul>
<li>8000 as Training set</li>
<li>2000 as Validation set</li>
<li>Learning rate : 2e-5</li>
<li>Epochs :4</li>
<li>Batch size :32</li>
</ul>
<p>Even if I have good learning curves, the performance on test set is bad (0.47 when submitting on Kaagle). I tried many changes on Learning rate and Epochs, but I still have the same problem.</p>
<p>How to change parameters of Bert model for a better performance on test set?</p>
<pre><code>from transformers import BertForSequenceClassification,AdamW,BertConfig
from transformers import BertTokenizer
print(&quot;Loading BertTokenizer...&quot;)
tokenizer=BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)


model=BertForSequenceClassification.from_pretrained(
    &quot;bert-base-uncased&quot;,
    num_labels=2,
    output_attentions=False,
    output_hidden_states=False,
    
)

model.cuda()

optimizer=AdamW(model.parameters(),
                lr=1.5e-5,
                eps=1e-8,
               )

from transformers import get_linear_schedule_with_warmup

epochs=4

total_steps=len(train_dataloader)*epochs

scheduler=get_linear_schedule_with_warmup(optimizer,
                                          num_warmup_steps=0,
                                          num_training_steps=total_steps)


import random 

seed_val=42

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

##################################################################
#                        TRAINING                                #
##################################################################     

# loss_values=[]

training_stats = []
for epoch_i in range(0,epochs):

  print(&quot;****Epoch {:} /{:} ******&quot;.format(epoch_i+1,epochs))
  print(&quot;Training...&quot;)

  t0=time.time()
  total_loss=0

  model.train()


  for step,batch in enumerate(train_dataloader):
    if step%100==0 and not step==0:
      elapsed=format_time(time.time()-t0)
      print(&quot; Batch {:&gt;5,} of {:&gt;5,}. Elapsed: {:}&quot;.format(step,len(train_dataloader),elapsed))

    b_input_ids=batch[0].to(device)
    b_input_mask=batch[1].to(device)
    b_labels=batch[2].to(device)
   

    model.zero_grad()
    # outputs=model(b_input_ids,
    #               token_type_ids=None,
    #               # attention_masks=b_input_mask,
    #               labels=b_labels
    #               )
    
    loss, logits = model(b_input_ids,
                                  token_type_ids=None, 
                                  attention_mask=b_input_mask,
                                  labels=b_labels
                           )

  
    total_loss +=loss.item()
    loss.backward()

    torch.nn.utils.clip_grad_norm(model.parameters(),1.0)
    optimizer.step()
    scheduler.step()

  avg_train_loss=total_loss/len(train_dataloader)

  
  print(&quot;&quot;)
  print(&quot; Average training loss :{0:.2f}&quot;.format(avg_train_loss))
  print(&quot;Training epoch took {:}&quot;.format(format_time(time.time()-t0)))
  training_time = format_time(time.time() - t0)



##################################################################
#                        VALIDATION                                #
##################################################################     

  print(&quot;&quot;)
  print(&quot;Runing Validation ...&quot;)

  t0=time.time()
  model.eval()

  total_eval_loss,eval_accuracy=0,0
  nb_eval_steps,nb_eval_examples=0,0

  for batch in validation_dataloader:

    batch=tuple(t.to(device) for t in batch)

    b_input_ids,b_input_mask,b_labels=batch

    with torch.no_grad():
    #   outputs=model(b_input_ids,
    #                 token_type_ids=None,
    #                 # attention_masks=b_input_mask
    #                 )
    # logits=outputs[0]
      loss, logits = model(b_input_ids, 
                                token_type_ids=None, 
                                attention_mask=b_input_mask,
                                labels=b_labels)


      total_eval_loss += loss.item()
    #Move to cpu

    logits=logits.detach().cpu().numpy()
    label_ids=b_labels.to('cpu').numpy()


    # # Accuracy of this batch 
    tmp_eval_accuracy=flat_accuracy(logits,label_ids)
    eval_accuracy+=tmp_eval_accuracy
    nb_eval_steps+=1
    
  print(&quot;  Accuracy: {0:.2f}&quot;.format(eval_accuracy/nb_eval_steps)) 
  print(&quot;  Validation took:{:}&quot;.format(format_time(time.time()- t0)))
  avg_val_loss = total_eval_loss / len(validation_dataloader)
  print(&quot; Average validation loss :{0:.2f}&quot;.format(avg_val_loss))

  avg_val_accuracy = eval_accuracy / len(validation_dataloader)
  validation_time = format_time(time.time() - t0)
  


  training_stats.append(
      {
          'epoch': epoch_i + 1,
          'Training Loss': avg_train_loss,
          'Valid. Loss': avg_val_loss,
          'Valid. Accur.': avg_val_accuracy,
          'Training Time': training_time,
          'Validation Time': validation_time
      }
  )

  
print(&quot;&quot;)   
print(&quot;Training completed!&quot;)
</code></pre>
<p>And I put results on  CSV to submit like this</p>
<pre><code>predictions=predictions[:,1]
predictions[predictions&gt;0]=0
predictions[predictions&lt;0]=1
predictions=predictions.astype(np.int64)

sample_submission=pd.read_csv('sample_submission.csv',sep=',',index_col=0)
sample_submission[&quot;target&quot;]=predictions
sample_submission.head()

to_submit=sample_submission.to_csv(&quot;submission.csv&quot;,index=True)
</code></pre>
<p><img src=""https://i.sstatic.net/nyyTh.png"" alt=""Learning curves"" /></p>
",Training and Model Evaluation,change parameter bert model better performance test set working nlp task kaggle competition purpose predict tweet express real disaster using bertforsequenceclassification training set size split training set validation set learning rate e epoch batch size even good learning curve performance test set bad submitting kaagle tried many change learning rate epoch still problem change parameter bert model better performance test set put result csv submit like
Accuracy stuck at one point,"<p>I am training a sentence classification model and during training I am observing that my model is giving the same accuracy for every epoch. It seems it is stuck at local minima. I have changed optimizers to <code>sgd</code>, <code>rmsprop</code>, <code>adam</code> but nothing is working.</p>
<p>My model code is</p>
<pre><code>model = Sequential()
model.add(Embedding(max_words, 768, input_length=max_len, weights=[embeddings]))
model.add(BatchNormalization())
model.add(Activation('tanh'))
model.add(SpatialDropout1D(0.5))
model.add(Conv1D(16, kernel_size=3, activation='relu'))
model.add(Bidirectional(LSTM(4)))
model.add(BatchNormalization())
model.add(Activation('tanh'))
model.add(Dropout(0.8))
model.add(Dense(2, activation='softmax'))
model.summary()
</code></pre>
<p>And compile code is</p>
<pre><code>from keras.optimizers import SGD
opt = SGD(lr=0.01)
model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'], optimizer = opt)
</code></pre>
<p>This is the training phase</p>
<pre><code>Epoch 1/10
1854/1854 [==============================] - 83s 45ms/step - loss: 0.6396 - accuracy: 0.6596 - val_loss: 0.6640 - val_accuracy: 0.6226
Epoch 2/10
1854/1854 [==============================] - 82s 44ms/step - loss: 0.6419 - accuracy: 0.6518 - val_loss: 0.6405 - val_accuracy: 0.6226
Epoch 3/10
1854/1854 [==============================] - 82s 44ms/step - loss: 0.6377 - accuracy: 0.6481 - val_loss: 0.6463 - val_accuracy: 0.6226
Epoch 4/10
1854/1854 [==============================] - 83s 45ms/step - loss: 0.6389 - accuracy: 0.6514 - val_loss: 0.6322 - val_accuracy: 0.6226
Epoch 5/10
1854/1854 [==============================] - 83s 45ms/step - loss: 0.6350 - accuracy: 0.6538 - val_loss: 0.6267 - val_accuracy: 0.6226
</code></pre>
",Training and Model Evaluation,accuracy stuck one point training sentence classification model training observing model giving accuracy every epoch seems stuck local minimum changed optimizers nothing working model code compile code training phase
Keras model not learning after training,"<p>I am training a keras model for a sentence classification task. The problem is although it is giving an accuracy of 94%, it is not learning anything. When I give a new sentence (not present in the dataset), it gives the same probability for it (in the <code>model.prediction</code> step). I can't figure out why is this happening.</p>
<p>Here is my model</p>
<pre><code>model = Sequential()
model.add(Embedding(max_words, 30, input_length=max_len))
model.add(BatchNormalization())
model.add(Activation('tanh'))
model.add(Dropout(0.5))
model.add(Bidirectional(LSTM(32)))
model.add(BatchNormalization())
model.add(Activation('tanh'))
model.add(Dropout(0.5))
model.add(Dense(2, activation='sigmoid'))
model.summary()
</code></pre>
<p>Here <code>max_words = 2000</code> and <code>max_len=300</code></p>
<p>Here is the model summary</p>
<pre><code>Model: &quot;sequential_3&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_3 (Embedding)      (None, 300, 30)           60000     
_________________________________________________________________
batch_normalization_5 (Batch (None, 300, 30)           120       
_________________________________________________________________
activation_5 (Activation)    (None, 300, 30)           0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 300, 30)           0         
_________________________________________________________________
bidirectional_3 (Bidirection (None, 64)                16128     
_________________________________________________________________
batch_normalization_6 (Batch (None, 64)                256       
_________________________________________________________________
activation_6 (Activation)    (None, 64)                0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 130       
=================================================================
Total params: 76,634
Trainable params: 76,446
Non-trainable params: 188
</code></pre>
<p>And here is the code, the size of my dataset is 20k, with 10% in testing.</p>
<pre><code>model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'], optimizer = 'adam')
history = model.fit(sequences_matrix, Y_train, batch_size=256, epochs=50, validation_split=0.1)
</code></pre>
",Training and Model Evaluation,kera model learning training training kera model sentence classification task problem although giving accuracy learning anything give new sentence present dataset give probability step figure happening model model summary code size dataset k testing
Ktrain |my model giving good performance on train/val data during training but no on test data,"<p>Need some guidance.</p>
<p>I am training one model.I amm getting good training and validation accuracy (98%) but when i test the model on my test data then accury drops alot.</p>
<p>it drops to 80%.</p>
<p>any suggestion on how to improve performance on test data?</p>
<p>PS: My data is text (tweets only). and m only fine tuning a pretrained model on my data. and my using Ktrain library with Bert model</p>
<p>thanks.</p>
<p><strong>Update</strong></p>
<p>I tried Strathfield kfold(10) and score improved a bit. Jumped from 80 to 81.25%.)  Also I used weight decay with train test split (80-20) with weight decay of 0.01 but performance dropped to 75%.</p>
<p>Should I try Strathfield Kfold (10), shuffle= true, with weight decay = 0.01? or any other advise pls</p>
",Training and Model Evaluation,ktrain model giving good performance train val data training test data need guidance training one model amm getting good training validation accuracy test model test data accury drop alot drop suggestion improve performance test data p data text tweet fine tuning pretrained model data using ktrain library bert model thanks update tried strathfield kfold score improved bit jumped also used weight decay train test split weight decay performance dropped try strathfield kfold shuffle true weight decay advise pls
RNN Language Model in PyTorch predicting the same three words repeatedly,"<p>I am attempting to create a word-level language model using an RNN in PyTorch. Whenever I am training the loss stays about the same for the whole training set and when I try to sample a new sentence the same three words are predicted in the same order. For example in my most recent attempt the RNN predicted 'the' then 'same' then 'of' and that sequence just kept repeating. I have tried changing the how I've set up the RNN including using LSTM's, GRU's, and different embeddings but so far nothing has worked.</p>
<p>The way that I am training the RNN is by taking a sentence of 50 words, and selecting a progressively larger part of the sentence with the next word being the target. At the end of the sentence, I have an EOS tag. I am using text from <em>Republic</em> by Plato as my training set and embedding it using a pytorch embedding layer. I am then feeding it into an LSTM and then a linear layer to get the right shape. I am not sure if the problem is in the RNN, the data, the training or what so any help would be greatly appreciated.</p>
<p>If anyone has any experience in nlp or in language modeling I would greatly appreciate any help you could offer for this fixing this problem. My end goal is simply to just be able to generate a sentence. Thanks in advance!</p>
<p>Here is my RNN</p>
<pre><code>class LanguageModel(nn.Module):
  &quot;&quot;&quot;
    Class that defines the reccurent neural network.

    Methods
    -------
    forward(input, h, c)
      Forward propogation through the RNN.
    initHidden()
      Initializes the hidden and cell states.
  &quot;&quot;&quot;
  def __init__(self, vocabSize, seqLen = 51, embeddingDim = 30, hiddenSize = 32, numLayers = 1, bid = False):
    &quot;&quot;&quot;
      Initializes the class

      Parameters
      ----------
      seqLen : int, optional
        The length of the input sequence.
      embeddingDim : int, optional
        The dimension that the embedding dimension for the encoder should be.
      vocabSize : int
        The length of the vocab dictionary.
      hiddenSize : int, optional
        The size that the hidden state should be.
      numLayers : int, optional
        The number of LSTM Layers.
      bid : bool, optional
        Whether the RNN should be bidirctional or not.
    &quot;&quot;&quot;

    super(LanguageModel, self).__init__()
    self.hiddenSize = hiddenSize
    self.numLayers = numLayers

    # Set value of numDirections based on whether or not the RNN is bidirectional.
    if bid == True:
      self.numDirections = 2
    else:
      self.numDirections = 1

    self.encoder = nn.Embedding(vocabSize, embeddingDim)
    self.LSTM = nn.LSTM(input_size = embeddingDim, hidden_size = hiddenSize, num_layers = numLayers, bidirectional = bid)
    self.decoder = nn.Linear(seqLen * self.numDirections * hiddenSize, vocabSize)

  def forward(self, input, h, c):
    &quot;&quot;&quot;
      Forward propogates through the RNN

      Parameters
      ----------
      input : torch.Tensor
        Input to RNN. Should be formatter using makeInput() and padSeq().
      h : torch.Tensor
        Hidden state.
      c : torch.Tensor
        Cell state.

      Returns
      -------
      torch.Tensor
        Log probabilities for the predicted word from the RNN.
    &quot;&quot;&quot;

    emb = self.encoder(input)
    emb.unsqueeze_(1) # Add in the batch dimension so the shape is right for the LSTM

    out, (h, c) = self.LSTM(emb, (h, c))
    out = out.view(1, -1) # Reshaping to fit into the loss function.

    out = self.decoder(out)

    logProbs = F.log_softmax(out, dim = 1)

    return logProbs

  def initHidden(self):
    &quot;&quot;&quot;
      Initializes the hidden and cell states.

      Returns
      -------
      torch.Tensor
        Tensor containing the initial hidden state.
      torch.Tensor
        Tensor containing the intial cell state.
    &quot;&quot;&quot;
    h = torch.zeros(self.numLayers * self.numDirections, 1, self.hiddenSize)
    c = torch.zeros(self.numLayers * self.numDirections, 1, self.hiddenSize)
    
    return h, c
</code></pre>
<p>here is how I create my input and targets</p>
<pre><code>def makeInput(sentence):
  &quot;&quot;&quot;
    Prepares a sentence for input to the RNN.

    Parameters
    ----------
    sentence : list
      The sentence to be converted into input. Should be of form: [str] 

    Returns
    -------
    torch.Tensor
      Tensor of the indices for each word in the input sentence.
  &quot;&quot;&quot;

  sen = sentence[0].split() # Split the list into individual words
  sen.insert(0, 'START')

  input = [word2Idx[word] for word in sen] # Iterate over the words in sentence and convert to indices

  return torch.tensor(input)

def makeTarget(sentence):
  &quot;&quot;&quot;
    Prepares a sentence to be a target.

    Parameters
    ----------
    sentence : str
      The sentence to be made into a target. Should be of form: [str]

    Returns
    -------
    torch.Tensor
      Tensor of the indices for the target phrase including the &lt;EOS&gt; tag.
  &quot;&quot;&quot;

  sen = sentence[0].split() # Split the list into individual words
  sen.append('EOS')
  
  target = [word2Idx[word] for word in sen]
  target = torch.tensor(target, dtype = torch.long)

  return target.unsqueeze_(-1) # Removing dimension for loss function 

def padSeq(seq, refSeq):
  &quot;&quot;&quot;
    Pads a sequence to be the same shape as another sequence.

    Parameters
    ----------
    seq : torch.Tensor
      The sequence to pad.
    refSeq : torch.Tensor
      The reference sequence. seq will be padded to be the same shape as refSeq.

    Returns
    -------
    torch.Tensor
      Tensor containing the padded sequence.
  &quot;&quot;&quot;

  padded = pad_sequence([refSeq, seq])
  tmp = torch.t(padded) # Transpose the padded sequence for easier indexing on return

  return tmp[1] # Return only the padded seq not both sequences
</code></pre>
<p>and here is my training loop</p>
<pre><code>def train():
  &quot;&quot;&quot;
    Trains the model.
  &quot;&quot;&quot;

  start = time.time()
  for i, data in enumerate(trainLoader):
    inputTensor = makeInput(data)
    targetTensor = makeTarget(data)

    targetTensor = targetTensor.to(device)

    h, c = model.initHidden()
    h = h.to(device)
    c = c.to(device)
    
    optimizer.zero_grad()
    loss = 0

    for x in range(inputTensor.size(0)): # Iterate over all of the words in the input sentence
      &quot;&quot;&quot; Preparing input for the rnn &quot;&quot;&quot;
      input = inputTensor[: x + 1] # We only want part of the input so the RNN can learn on predicting the next words
      input = padSeq(input, inputTensor)
      input = input.to(device)

      out = model(input, h, c)
      l = criterion(out, targetTensor[x])
      loss += l

    loss.backward()
    optimizer.step()
  
    if i % 250 == 0: # Print updates to the models loss every 10 iters.
      print('[{}] Epoch: {} -&gt; {}'.format(timeSince(start), i, loss / inputTensor.size(0)))
</code></pre>
",Training and Model Evaluation,rnn language model pytorch predicting three word repeatedly attempting create word level language model using rnn pytorch whenever training loss stay whole training set try sample new sentence three word predicted order example recent attempt rnn predicted sequence kept repeating tried changing set rnn including using lstm gru different embeddings far nothing ha worked way training rnn taking sentence word selecting progressively larger part sentence next word target end sentence eos tag using text republic plato training set embedding using pytorch embedding layer feeding lstm linear layer get right shape sure problem rnn data training help would greatly appreciated anyone ha experience nlp language modeling would greatly appreciate help could offer fixing problem end goal simply able generate sentence thanks advance rnn create input target training loop
"Label tokenizer not working, loss and accuracy cannot be calculated","<p>I am using Keras Tensorflow for NLP, I am currently working on the imdb reviews dataset. I would like to make use of the hub.KerasLayer. I want to pass the actual x and y values directly. So the sentences as x and the labels as y in my model.fit statement. My code:</p>
<pre><code>import csv
import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np
import tensorflow_hub as hub
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

imdb, info = tfds.load(&quot;imdb_reviews&quot;, with_info=True, as_supervised=True)

imdb_train=imdb['train']
imdb_test=imdb['test']

training_sentences=[]
training_labels=[]

test_sentences=[]
test_labels=[]

for a,b in imdb_train:
  training_sentences.append(a.numpy().decode(&quot;utf8&quot;))
  training_labels.append(b.numpy())

for a,b in imdb_test:
  test_sentences.append(a.numpy().decode(&quot;utf8&quot;))
  test_labels.append(b.numpy())

model = &quot;https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1&quot;
hub_layer = hub.KerasLayer(model, output_shape=[20], input_shape=[], 
                           dtype=tf.string, trainable=True)

model = tf.keras.Sequential()
model.add(hub_layer)
model.add(tf.keras.layers.Dense(16, activation='relu'))
model.add(tf.keras.layers.Dense(1))

model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),optimizer='adam', metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])
</code></pre>
<p>Trying</p>
<pre><code>history = model.fit(x=training_sentences,
                      y=training_labels,
                      validation_data=(test_sentences, test_labels),
                      epochs=2)
</code></pre>
<p>doesn't work, because training_labels is not in the correct shape/format. My approach now is to make use of applying the tokenizer again, because I then get the result (from texts_to_sequences) in the correct format/shape. For this I have to first transform it into yes/no (or a/b whatever) string.</p>
<pre><code>training_labels_test=[]
for i in training_labels:
   if i==0: training_labels_test.append(&quot;no&quot;)
   if i==1: training_labels_test.append(&quot;yes&quot;)
  
testtokenizer=Tokenizer()
testtokenizer.fit_on_texts(training_labels_test)
test_labels_pad=testtokenizer.texts_to_sequences(training_labels_test)

val_labels_test=[]
for i in test_labels:
   if i==0: val_labels_test.append(&quot;no&quot;)
   if i==1: val_labels_test.append(&quot;yes&quot;)

testtokenizer.fit_on_texts(val_labels_test)
val_labels_pad=testtokenizer.texts_to_sequences(val_labels_test)
</code></pre>
<p>Because I now have 1 and 2 as labels, I need to update my model:</p>
<pre><code>model = tf.keras.Sequential()
model.add(hub_layer)
model.add(tf.keras.layers.Dense(16, activation='relu'))
model.add(tf.keras.layers.Dense(2))

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
</code></pre>
<p>I then try to fit it:</p>
<pre><code>history = model.fit(x=training_sentences,
                      y=test_labels_pad,
                      validation_data=(test_sentences, val_labels_pad),
                      epochs=2)
</code></pre>
<p>Problem is that loss is nan and accuracy is not calculated correctly.</p>
<p><a href=""https://i.sstatic.net/XhNL7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XhNL7.png"" alt=""examp"" /></a></p>
<p>Where is the mistake?</p>
<p>Please not that my question is really about this specific way and why this tokenizer is not working. I am aware that there are other possibilities which would work.</p>
",Training and Model Evaluation,label tokenizer working loss accuracy calculated using kera tensorflow nlp currently working imdb review dataset would like make use hub keraslayer want pas actual x value directly sentence x label model fit statement code trying work training label correct shape format approach make use applying tokenizer get result text sequence correct format shape first transform yes b whatever string label need update model try fit problem loss nan accuracy calculated correctly mistake please question really specific way tokenizer working aware possibility would work
Well Trained Classifier Does not Perform Well in Same Source of Dataset,"<p>I am doing a tweet classification project, and the task now is to determine whether a tweet is, for instance, road traffic-related or not.</p>
<p>I have collected a huge number of tweets(over 1 million). To train a classifier, I first use keywords(such as crash, accident, road, etc.) to get some candidate traffic-related tweets. Then I manually review these tweets and check whether these tweets are traffic-related or not. I also construct another dataset which are not traffic-related. Finally, I get 1000 traffic-related tweets and 2000 not traffic-related tweets.</p>
<p>Then I use the CNN-LSTM model based on pre-trained word embedding to classify the tweets. The overall structure of the model is:</p>
<pre><code>from tensorflow import keras
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.metrics import binary_accuracy, binary_crossentropy
from tensorflow.keras.layers import Dense, Dropout, Conv2D, Reshape, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import LSTM

def conv2d_lstm_weibo(kernel_height, dropout_rate):
    # Get the input information - tweet only
    tweet_input = Input(shape=(60, 300, 1), name='tweet_input')

    # Create the convolutional layer and lstm layer
    conv2d = Conv2D(filters=100, kernel_size=(kernel_height, 300), padding='valid',
                    activation='relu', use_bias=True, name='conv_1')(tweet_input)
    reshaped_conv2d = Reshape((conv2d.shape[1], 100), name='reshape_1')(conv2d)
    conv_drop = Dropout(dropout_rate, name='dropout_1')(reshaped_conv2d)
    lstm = LSTM(100, return_state=False, activation='tanh',
                recurrent_activation='hard_sigmoid', name='lstm_1')(conv_drop)
    lstm_drop = Dropout(dropout_rate, name='dropout_2')(lstm)
    dense_1 = Dense(100, activation='relu', name='dense_1')(lstm_drop)
    output = Dense(2, activation='softmax', name='output_dense')(dense_1)
    # Build the model
    model = Model(inputs=tweet_input, outputs=output)
    return model
</code></pre>
<p>The dimension of the pre-trained word vector is <code>(300,1)</code>. The training data, validation data, and test data account for 60%, 20%, and 20% of the labeled data. I use Adam optimizer with a learning rate of 0.001. Actually, the model performs quite well, reaching a 0.99 F1 score on the test data. The confusion matrix about the performance of the model on the test data is:</p>
<pre><code>[[384   3]
 [  2 223]]
</code></pre>
<p>However, when I use the trained classifier to make the prediction on all the tweets I have collected. <strong>The performance is really bad</strong>. I manually check the tweet which is labeled by <code>traffic_related</code> by the classifier, and I find that most of the tweets are not traffic-related at all.</p>
<p>I am wondering which part I did is wrong. Any suggestions and insights are very appreciated!</p>
",Training and Model Evaluation,well trained classifier doe perform well source dataset tweet classification project task determine whether tweet instance road traffic related collected huge number tweet million train classifier first use keywords crash accident road etc get candidate traffic related tweet manually review tweet check whether tweet traffic related also construct another dataset traffic related finally get traffic related tweet traffic related tweet use cnn lstm model based pre trained word embedding classify tweet overall structure model dimension pre trained word vector training data validation data test data account labeled data use adam optimizer learning rate actually model performs quite well reaching f score test data confusion matrix performance model test data however use trained classifier make prediction tweet collected performance really bad manually check tweet labeled classifier find tweet traffic related wondering part wrong suggestion insight appreciated
Additional training NLP,"<p>I started working with the Camembert deep learning model, an analogue of Roberta for the French language, and I have a question, how can I retrain such a model for a particular task? Specifically, the task is for the model to learn how to evaluate input sentences for correctness</p>
<pre><code>class newModel(nn.Module):
    def __init__(self, numFeatures=768):
        super(newModel, self).__init__()
        self.camembert = CamembertModel.from_pretrained('camembert-base')
        self.GAP = nn.AdaptiveAvgPool2d((1, numFeatures))
        self.predictionLayer = nn.Linear(numFeatures, 2)
        self.softmax = nn.Softmax()

    def forward(self, x):
        camembertFeatures = self.camembert(**tokenized_sentence) # [BS, inputShape] -&gt; [BS, numToken, numFeatures]
        camembertFeatures = camembertFeatures[0]
        GAPvalues = self.GAP(camembertFeatures) # [BS, numToken, numFeatures] -&gt; [BS, 1, numFeatures]
        GAPshape = GAPvalues.shape
        sentenceFeatures = GAPvalues.view(GAPshape[0], GAPshape[2]) # [BS, 1, numFeatures] -&gt; [BS, numFeatures]
        predictions = self.predictionLayer(sentenceFeatures) # [BS, numFeatures] -&gt; [BS, 2]
        predictions = self.softmax(predictions) # [BS, 2] -&gt; [BS, 2]
        return predictions
</code></pre>
<p>I have built a layer, which needs to be trained, how to do it in the correct way? (it is especially important to understand which optimizer and loss function to use)</p>
",Training and Model Evaluation,additional training nlp started working camembert deep learning model analogue roberta french language question retrain model particular task specifically task model learn evaluate input sentence correctness built layer need trained correct way especially important understand optimizer loss function use
Get sentence vector for a K-means clustering task,"<p>I am working on a project which groups jobs posted on various job portals into clusters based on the description of the jobs using K-means.</p>
<p>I found the work vector using Word2Vec, but i guess this will not serve the purpose as I will need a vector of the whole job description.</p>
<p>I know that I can average out the word vector of a sentence to get the sentence vector but worried about the accuracy as this will loose the ordering of the words.</p>
<p>Is there any other way I can get the vectors ?</p>
",Training and Model Evaluation,get sentence vector k mean clustering task working project group job posted various job portal cluster based description job using k mean found work vector using word vec guess serve purpose need vector whole job description know average word vector sentence get sentence vector worried accuracy loose ordering word way get vector
Increasing batch size decreases trainable parameters,"<p>I am using a LSTM+attention layer for sentence classification task. I have observed that in simple LSTM model, my total trainable parameters were 14705 with batch size of 64, but when I am using attention layer with  LSTM, for the same batch size, the trainable parameters decreases to 230. While for batch size of 4, it increases to 3077. Also, since batch size is 64 in attention layer, it is increasing by 1 only in epochs.</p>
<p>How is it possible?</p>
<p>This is the screenshot for attention layer for batch size 64</p>
<p><a href=""https://i.sstatic.net/iQzSb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iQzSb.png"" alt=""enter image description here"" /></a></p>
<p>And this is the screenshot for LSTM layer for batch size 64.</p>
<p><a href=""https://i.sstatic.net/QYyzZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QYyzZ.png"" alt=""enter image description here"" /></a></p>
",Training and Model Evaluation,increasing batch size decrease trainable parameter using lstm attention layer sentence classification task observed simple lstm model total trainable parameter batch size using attention layer lstm batch size trainable parameter decrease batch size increase also since batch size attention layer increasing epoch possible screenshot attention layer batch size screenshot lstm layer batch size
How to fit NLP in CNN model?,"<p>I am doing research on using CNN machine learning model with NLP (multi-label classification)</p>
<p>I read some papers that mentioned getting good results in applying CNN for multi-label classification</p>
<p>I am trying to test this model on Python.</p>
<p>I read many articles about how to work with NLP an Neural Networks.</p>
<p>I have this code that is not working and giving me many errors ( every time I fix the error I get another error )</p>
<p>I ended seeking paid FreeLancers to help me fix the code, I hired 5 guys but non of them was able to fix the code !</p>
<p>you are my last hope.</p>
<p>I hope someone can helpe me fix this code and get it working.</p>
<p>First this is my dataset (100 record sample, just to make sure that code is working, I know it is not enogh for good accuracy. I will tweak and enhance model later)</p>
<p><a href=""http://shrinx.it/data100.zip"" rel=""nofollow noreferrer"">http://shrinx.it/data100.zip</a></p>
<p>at the time being I just want this code to work. yet tips on how to enhance accuracy are really welcomed.</p>
<p>Some of the errors I got</p>
<pre><code>InvalidArgumentError: indices[1] = [0,13] is out of order. Many sparse ops require sorted indices.
    Use `tf.sparse.reorder` to create a correctly ordered copy.
</code></pre>
<p>and</p>
<pre><code>ValueError: Input 0 of layer sequential_8 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 18644]
</code></pre>
<p>here is my code</p>
<pre><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from keras.layers import *
from keras.callbacks import ModelCheckpoint
from keras.optimizers import Adam
from keras.models import *



# Load Dataset

df_text = pd.read_csv(&quot;J:\\__DataSets\\__Samples\\Test\\data100\\text100.csv&quot;)
df_results = pd.read_csv(&quot;J:\\__DataSets\\__Samples\\Test\\data100\\results100.csv&quot;)

df = pd.merge(df_text,df_results, on=&quot;ID&quot;)


#Prepare multi-label
Labels = [] 

for i in df['Code']: 
  Labels.append(i.split(&quot;,&quot;)) 


df['Labels'] = Labels



multilabel_binarizer = MultiLabelBinarizer()
multilabel_binarizer.fit(df['Labels'])

y = multilabel_binarizer.transform(df['Labels'])
X = df['Text'].values

#TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=1000)


xtrain, xval, ytrain, yval = train_test_split(X, y, test_size=0.2, random_state=9)

tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=1000)

# create TF-IDF features
X_train_count = tfidf_vectorizer.fit_transform(xtrain)
X_test_count = tfidf_vectorizer.transform(xval)


#Prepare Model

input_dim = X_train_count.shape[1]  # Number of features
output_dim=len(df['Labels'].explode().unique())


sequence_length = input_dim
vocabulary_size = X_train_count.shape[0]
embedding_dim = output_dim
filter_sizes = [3,4,5]
num_filters = 512
drop = 0.5

epochs = 100
batch_size = 30



#CNN Model

inputs = Input(shape=(sequence_length,), dtype='int32')
embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)
reshape = Reshape((sequence_length,embedding_dim,1))(embedding)

conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)
conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)
conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)

maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)
maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)
maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)

concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])
flatten = Flatten()(concatenated_tensor)
dropout = Dropout(drop)(flatten)
output = Dense(units=2, activation='softmax')(dropout)


# this creates a model that includes
model = Model(inputs=inputs, outputs=output)


#Compile

checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')
adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)


model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])
print(&quot;Traning Model...&quot;)
model.summary()


#Fit
model.fit(X_train_count, ytrain, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(X_test_count, yval))  # starts training



#Accuracy
loss, accuracy = model.evaluate(X_train_count, ytrain, verbose=False)
print(&quot;Training Accuracy: {:.4f}&quot;.format(accuracy))
loss, accuracy = model.evaluate(X_test_count, yval, verbose=False)
print(&quot;Testing Accuracy:  {:.4f}&quot;.format(accuracy))
</code></pre>
<p>a sample of my dataset</p>
<p>text100.csv</p>
<pre><code>ID  Text
1   Allergies to Drugs  Attending:[**First Name3 (LF) 1**] Chief Complaint: headache and neck stiffne
2   Complaint: fever, chills, rigors  Major Surgical or Invasive Procedure: Arterial l
3   Complaint: Febrile, unresponsive--&gt; GBS meningitis and bacteremia  Major Surgi
4   Allergies to Drugs  Attending:[**First Name3 (LF) 45**] Chief Complaint: PEA arrest .   Major Sur
5   Admitted to an outside hospital with chest pain and ruled in for myocardial infarction.  She was tr
6   Known Allergies to Drugs  Attending:[**First Name3 (LF) 78**] Chief Complaint: Progressive lethargy 
7   Complaint: hypernatremia, unresponsiveness  Major Surgical or Invasive Procedure: PEG/tra
8   Chief Complaint: cough, SOB  Major Surgical or Invasive Procedure: RIJ placed Hemod
</code></pre>
<p>Results100.csv</p>
<pre><code>ID  Code
1   A32,D50,G00,I50,I82,K51,M85,R09,R18,T82,Z51
2   418,475,905,921,A41,C50,D70,E86,F32,F41,J18,R11,R50,Z00,Z51,Z93,Z95
3   136,304,320,418,475,921,998,A40,B37,G00,G35,I10,J15,J38,J69,L27,L89,T81,T85
4   D64,D69,E87,I10,I44,N17
5   E11,I10,I21,I25,I47
6   905,C61,C91,E87,G91,I60,M47,M79,R50,S43
7   304,320,355,E11,E86,E87,F06,I10,I50,I63,I69,J15,J69,L89,L97,M81,N17,Z91
</code></pre>
",Training and Model Evaluation,fit nlp cnn model research using cnn machine learning model nlp multi label classification read paper mentioned getting good result applying cnn multi label classification trying test model python read many article work nlp neural network code working giving many error every time fix error get another error ended seeking paid freelancer help fix code hired guy non wa able fix code last hope hope someone helpe fix code get working first dataset record sample make sure code working know enogh good accuracy tweak enhance model later time want code work yet tip enhance accuracy really welcomed error got code sample dataset text csv result csv
What is the highest accuracy of the Stanford&#39;s Large Movie Review Dataset ever achieved?,"<p>I want to know the NLP model that was able to achieve the highest accuracy using the open Stanford's Large Movie Review Dataset, and also its accuracy.<br />
The link to the <a href=""https://ai.stanford.edu/%7Eamaas/data/sentiment/"" rel=""nofollow noreferrer"">dataset</a>.</p>
",Training and Model Evaluation,highest accuracy stanford large movie review dataset ever achieved want know nlp model wa able achieve highest accuracy using open stanford large movie review dataset also accuracy link dataset
"How to improve accuracy of model for categorical, non-binary, foreign language sentiment analysis in TensorFlow?","<h2>TLDR</h2>
<p>My aim is to categorize sentences in a foreign language (Hungarian) to 3 sentiment categories: <em>negative, neutral &amp; positive</em>. I would like to <strong>improve the accuracy of the model used</strong>, which can be found below in the &quot;<strong>Define, Compile, Fit the Model</strong>&quot; section. The rest of the post is here for completeness and reproducibility.</p>
<p><em>I am new to asking questions on Machine Learning topics, suggestions are welcome here as well: <a href=""https://meta.stackoverflow.com/questions/399477/how-to-ask-a-good-question-on-machine-learning"">How to ask a good question on Machine Learning?</a></em></p>
<hr />
<h2>Data preparation</h2>
<p>For this I have 10000 sentences, given to 5 human annotators, categorized as negative, neutral or positive, available from <a href=""http://metashare.nytud.hu/repository/browse/hungarian-opinion-tagged-sentence-bank/608756be64e211e2aa7c68b599c26a068dd5b3551f024f6281131670412d37d3/"" rel=""nofollow noreferrer"">here</a>. The first few lines look like this:</p>
<p><a href=""https://i.sstatic.net/ane0Q.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ane0Q.png"" alt=""enter image description here"" /></a></p>
<p>I categorize the sentence positive (denoted by <code>2</code>) if sum of the scores by annotators is positive, neutral if it is 0 (denoted by <code>1</code>), and negative (denoted by <code>0</code>) if the sum is negative:</p>
<pre><code>import pandas as pd
sentences_df = pd.read_excel('/content/OpinHuBank_20130106.xls')

sentences_df['annotsum'] = sentences_df['Annot1'] +\
                           sentences_df['Annot2'] +\
                           sentences_df['Annot3'] +\
                           sentences_df['Annot4'] +\
                           sentences_df['Annot5']

def categorize(integer):
    if 0 &lt; integer:  return 2
    if 0 == integer: return 1
    else: return 0

sentences_df['sentiment'] = sentences_df['annotsum'].apply(categorize)
</code></pre>
<p>Following <a href=""https://www.tensorflow.org/tutorials/text/text_classification_rnn"" rel=""nofollow noreferrer"">this tutorial</a>, I use <a href=""https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder"" rel=""nofollow noreferrer"">SubwordTextEncoder</a> to proceed. From <a href=""http://mokk.bme.hu/resources/webcorpus/"" rel=""nofollow noreferrer"">here</a>, I download <code>web2.2-freq-sorted.top100k.nofreqs.txt</code>, which contains <code>100000</code> most frequently used word in the target language. (Both the sentiment data and this data was recommended by <a href=""https://github.com/oroszgy/awesome-hungarian-nlp"" rel=""nofollow noreferrer"">this</a>.)</p>
<p>Reading in list of most frequent words:</p>
<pre><code>wordlist = pd.read_csv('/content/web2.2-freq-sorted.top100k.nofreqs.txt',sep='\n',header=None,encoding = 'ISO-8859-1')[0].dropna()
</code></pre>
<hr />
<h2>Encoding data, conversion to tensors</h2>
<p>Initializing encoder using <a href=""https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder#build_from_corpus"" rel=""nofollow noreferrer"">build_from_corpus</a> method:</p>
<pre><code>import tensorflow_datasets as tfds
encoder = tfds.features.text.SubwordTextEncoder.build_from_corpus(
        corpus_generator=(word for word in wordlist), target_vocab_size=2**16)
</code></pre>
<p>Building on this, encoding the sentences:</p>
<pre><code>import numpy as np
import tensorflow as tf
def applyencoding(string):
    return tf.convert_to_tensor(np.asarray(encoder.encode(string)))
sentences_df['encoded_sentences'] = sentences_df['Sentence'].apply(applyencoding)
</code></pre>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor"" rel=""nofollow noreferrer"">Convert to a tensor</a> each sentence's sentiment:</p>
<pre><code>def tensorise(input):
    return tf.convert_to_tensor(input)
sentences_df['sentiment_as_tensor'] = sentences_df['sentiment'].apply(tensorise)
</code></pre>
<p>Defining how much data to be preserved for testing:</p>
<pre><code>test_fraction = 0.2
train_fraction = 1-test_fraction
</code></pre>
<p>From the <code>pandas dataframe</code>, let's create <code>numpy array</code> of encoded sentence train tensors:</p>
<pre><code>nparrayof_encoded_sentence_train_tensors = \
        np.asarray(sentences_df['encoded_sentences'][:int(train_fraction*len(sentences_df['encoded_sentences']))])
</code></pre>
<p>These tensors have different lengths, so lets use <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"" rel=""nofollow noreferrer"">padding</a> to make them have the same:</p>
<pre><code>padded_nparrayof_encoded_sentence_train_tensors = tf.keras.preprocessing.sequence.pad_sequences(
                                            nparrayof_encoded_sentence_train_tensors, padding=&quot;post&quot;)
</code></pre>
<p>Let's <a href=""https://www.tensorflow.org/api_docs/python/tf/stack"" rel=""nofollow noreferrer"">stack</a> these tensors together:</p>
<pre><code>stacked_padded_nparrayof_encoded_sentence_train_tensors = tf.stack(padded_nparrayof_encoded_sentence_train_tensors)
</code></pre>
<p>Stacking the sentiment tensors together as well:</p>
<pre><code>stacked_nparray_sentiment_train_tensors = \
        tf.stack(np.asarray(sentences_df['sentiment_as_tensor'][:int(train_fraction*len(sentences_df['encoded_sentences']))]))
</code></pre>
<hr />
<h2>Define, Compile, Fit the Model (ie the main point)</h2>
<p>Define &amp; compile the model as follows:</p>
<pre><code>### THE QUESTION IS ABOUT THESE ROWS ###
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(encoder.vocab_size, 64),
    tf.keras.layers.Conv1D(128, 5, activation='sigmoid'),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(6, activation='sigmoid'),
    tf.keras.layers.Dense(3, activation='sigmoid')
]) 
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), optimizer='adam', metrics=['accuracy'])
</code></pre>
<p>Fit it:</p>
<pre><code>NUM_EPOCHS = 40
history = model.fit(stacked_padded_nparrayof_encoded_sentence_train_tensors,
                    stacked_nparray_sentiment_train_tensors,
                    epochs=NUM_EPOCHS)
</code></pre>
<p>The first few lines of the output is:</p>
<p><a href=""https://i.sstatic.net/jsvMG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jsvMG.png"" alt=""enter image description here"" /></a></p>
<hr />
<h2>Testing results</h2>
<p>As in <a href=""https://www.tensorflow.org/tutorials/text/text_classification_rnn"" rel=""nofollow noreferrer"">TensorFlow's RNN tutorial</a>, let's plot the results we gained so far:</p>
<pre><code>import matplotlib.pyplot as plt

def plot_graphs(history):
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['loss'])
  plt.xlabel(&quot;Epochs&quot;)
  plt.ylabel('accuracy / loss')
  plt.legend(['accuracy','loss'])
  plt.show()

plot_graphs(history)
</code></pre>
<p>Which gives us:</p>
<p><a href=""https://i.sstatic.net/M8OHS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M8OHS.png"" alt=""enter image description here"" /></a></p>
<p>Prepare the testing data as we prepared the training data:</p>
<pre><code>nparrayof_encoded_sentence_test_tensors = \
        np.asarray(sentences_df['encoded_sentences'][int(train_fraction*len(sentences_df['encoded_sentences'])):])

padded_nparrayof_encoded_sentence_test_tensors = tf.keras.preprocessing.sequence.pad_sequences(
                                                 nparrayof_encoded_sentence_test_tensors, padding=&quot;post&quot;)

stacked_padded_nparrayof_encoded_sentence_test_tensors = tf.stack(padded_nparrayof_encoded_sentence_test_tensors)

stacked_nparray_sentiment_test_tensors = \
        tf.stack(np.asarray(sentences_df['sentiment_as_tensor'][int(train_fraction*len(sentences_df['encoded_sentences'])):]))
</code></pre>
<p>Evaluate the model using only test data:</p>
<pre><code>test_loss, test_acc = model.evaluate(stacked_padded_nparrayof_encoded_sentence_test_tensors,stacked_nparray_sentiment_test_tensors)
print('Test Loss: {}'.format(test_loss))
print('Test Accuracy: {}'.format(test_acc))
</code></pre>
<p>Giving result:
<a href=""https://i.sstatic.net/IFyo4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IFyo4.png"" alt=""enter image description here"" /></a></p>
<p>Full notebook available <a href=""https://colab.research.google.com/drive/1qIbvy7ZPJjEBp2m7d3c0ZS4sQhst3TBa?usp=sharing"" rel=""nofollow noreferrer"">here</a>.</p>
<hr />
<h2>The question</h2>
<p><strong>How can I change the model definition and compilation rows above to have higher accuracy on the test set after no more than 1000 epochs?</strong></p>
",Training and Model Evaluation,improve accuracy model categorical non binary foreign language sentiment analysis tensorflow aim categorize sentence foreign language hungarian sentiment category negative neutral positive would like improve accuracy model used found define compile fit model section rest post completeness reproducibility new asking question machine learning topic suggestion welcome well first line look like categorize sentence positive denoted sum score annotator positive neutral denoted negative denoted sum negative following tutorial use subwordtextencoder proceed download contains frequently used word target language sentiment data data wa recommended reading list frequent word encoding data conversion tensor initializing encoder using build corpus method building encoding sentence convert tensor sentence sentiment defining much data preserved testing let create encoded sentence train tensor tensor different length let use padding make let stack tensor together stacking sentiment tensor together well define compile fit model ie main point define compile model follows fit first line output testing result tensorflow rnn tutorial let plot result gained far give u prepare testing data prepared training data evaluate model using test data giving result full notebook available question change model definition compilation row higher accuracy test set epoch
Adding attention layer to the Encoder-Decoder model architecture gives worse results,"<p>I initially defined a <strong>Encoder-Decoder Model</strong> architecture for <strong>Next Phrase Prediction</strong> and trained it on some data, I was successfully able to predict using the same model. But when I tried to insert an Attention layer in the architecture the model training was successful but I was not able to define encoder and decoder models separately for predictions. Here is the new Model Architecture defined by me:</p>

<pre><code># Model architecture along with Attention Layer
# Create the Encoder layers first.
encoder_inputs = Input(shape=(len_input,))
encoder_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)

# Bidirectional LSTM or Simple LSTM
encoder_lstm = Bidirectional(LSTM(units=units, return_sequences=True, return_state=True)) # Bidirectional(
encoder_out, fstate_h, fstate_c, bstate_h, bstate_c = encoder_lstm(encoder_emb(encoder_inputs))
state_h = Concatenate()([fstate_h,bstate_h])
state_c = Concatenate()([bstate_h,bstate_c])

encoder_states = [state_h, state_c]

# Now create the Decoder layers.
decoder_inputs = Input(shape=(None,))
decoder_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)
decoder_lstm = LSTM(units=units*2, return_sequences=True, return_state=True) # units=units*2
decoder_lstm_out, _, _ = decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)

# Attention layer
attn_layer = AttentionLayer(name='attention_layer')
attn_out, attn_states = attn_layer([encoder_out, decoder_lstm_out])

# Concat attention input and decoder LSTM output
decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_lstm_out, attn_out])

# Two dense layers
decoder_d1 = TimeDistributed(Dense(units, activation=""relu""))
decoder_d2 = TimeDistributed(Dense(vocab_out_size, activation=""softmax""))
decoder_out = decoder_d2(Dropout(rate=.2)(decoder_d1(Dropout(rate=.2)(decoder_concat_input))))
#decoder_out = decoder_d2(Dropout(rate=.2)(decoder_concat_input))

# combining the encoder and the decoder layers together
model = Model(inputs = [encoder_inputs, decoder_inputs], outputs= decoder_out)

model.compile(optimizer=tf.optimizers.Adam(), loss=""sparse_categorical_crossentropy"", metrics=['sparse_categorical_accuracy'])
model.summary()
</code></pre>

<p>Trained this model and defined another encoder and decoder using the same tensors:</p>

<pre><code># Changed infmodel
# Create the encoder model from the tensors we previously declared, while training
encoder_model = Model(encoder_inputs, [encoder_out, state_h, state_c], name = 'Encoder')

# decoder model
# Generate a new set of tensors for our new inference decoder
state_input_h = Input(shape=(units*2,), name=""state_input_h"") # units*2 if Bidirectional LSTM else units*1
state_input_c = Input(shape=(units*2,), name=""state_input_c"") # units*2
inf_decoder_inputs = Input(shape=(len_input, units), name=""inf_decoder_inputs"")
# similar decoder model architecture with state from encoder model
decoder_res, decoder_h, decoder_c = decoder_lstm(decoder_emb(decoder_inputs),
                                                 initial_state=[state_input_h, state_input_c])

# Attention inference
attn_out_res, attn_states_res = attn_layer([inf_decoder_inputs, decoder_res])
# Concat attention input and decoder LSTM output
decoder_out_concat_res = Concatenate(axis=-1, name='concat_layer')([decoder_res, attn_out_res])

inf_decoder_out = decoder_d2(decoder_d1(decoder_out_concat_res))

# finalizing the deocder model
inf_model = Model(inputs=[decoder_inputs] + [inf_decoder_inputs,  state_input_h, state_input_c], 
                  outputs=[inf_decoder_out, decoder_h, decoder_c], name = 'Decoder')
</code></pre>

<p>The results after model training have become worse, I believe I have some problems with model architecture. I have got to this architecture after trying many permutations. Go through the Model architecture once.</p>
",Training and Model Evaluation,adding attention layer encoder decoder model architecture give worse result initially defined encoder decoder model architecture next phrase prediction trained data wa successfully able predict using model tried insert attention layer architecture model training wa successful wa able define encoder decoder model separately prediction new model architecture defined trained model defined another encoder decoder using tensor result model training become worse believe problem model architecture got architecture trying many permutation go model architecture
Spacy tagger loss is zero while training,"<p>I use this snippet of code to train a tagger in <code>spacy 2.3.0</code>.</p>
<pre><code>TRAIN_DATA = posData.train_data_getter()[:80000]
if model is not None:
    nlp = spacy.load(model)  # load existing spaCy model
    print(&quot;Loaded model '%s'&quot; % model)
else:
    nlp = spacy.blank('fa')

if &quot;tagger&quot; not in nlp.pipe_names:
    tagger = nlp.create_pipe(&quot;tagger&quot;)
    for tag, values in TAG_MAP.items():
        tagger.add_label(tag, values)
    nlp.add_pipe(tagger, first=True)

pipe_exceptions = [&quot;tagger&quot;]
other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]
# nlp.tokenizer = Tokenizer(nlp.vocab)
with nlp.disable_pipes(*other_pipes):  # only train parser
    optimizer = nlp.begin_training()
    for i in range(n_iter):
        random.shuffle(TRAIN_DATA)
        losses = {}
        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            texts, annotations = zip(*batch)
            l = []
            for t in texts:
                l.append(normalize(t, remove_punc=True))
            texts = tuple(l)
            nlp.update(texts, annotations, sgd=optimizer, losses=losses)
        print(&quot;Losses&quot;, losses)
</code></pre>
<p>The problem is that the loss value is always zero. What am I doing wrong?</p>
",Training and Model Evaluation,spacy tagger loss zero training use snippet code train tagger problem loss value always zero wrong
Get tag-specific accuracy for pos tagger in spacy tagger evaluation,"<p>I'm using the code below for evaluate the pos tagger that I trained in <code>spacy 2.3.0</code> (built from source- last commit : <code>9860b8399ed2a3d1d680e1c1cd31d85926422709</code>):</p>
<pre><code>  def evaluate(nlp, examples):
    scorer = Scorer()
    nlp.tokenizer = Tokenizer(nlp.vocab)
    for input_, annot in examples:
        doc_gold_text = nlp.make_doc(input_)
        gold = GoldParse(doc_gold_text, tags=annot['tags'])
        pred_value = nlp(input_)
        scorer.score(pred_value, gold)
    return scorer.scores


def main(model='model'):
    test_data = train_data_getter()[80000:]
    nlp = spacy.load(model)
    # nlp = None
    pp = pprint.PrettyPrinter()
    pp.pprint(evaluate(nlp, test_data))
</code></pre>
<p>and the result is:</p>
<pre><code> {'ents_f': 0.0,
 'ents_p': 0.0,
 'ents_per_type': {},
 'ents_r': 0.0,
 'las': 0.0,
 'las_per_type': {'': {'f': 0.0, 'p': 0.0, 'r': 0.0}},
 'tags_acc': 88.9152111081426,
 'textcat_score': 0.0,
 'textcats_per_cat': {},
 'token_acc': 100.0,
 'uas': 0.0}
</code></pre>
<p>I wonder if there is a way to get accuracy per every part-of-speech tag?</p>
",Training and Model Evaluation,get tag specific accuracy po tagger spacy tagger evaluation using code evaluate po tagger trained built source last commit result wonder way get accuracy per every part speech tag
Re-implementing TF 1.0 sampled_softmax_loss funtion for seq2seq model in to TF 2 Keras model,"<p>I have a TF 1.0.1 code of seq2seq model. I am trying to rewrite it using Tensorflow Keras.</p>
<p><strong>TF 1.0.1 code has following decoder architecure:</strong></p>
<pre><code>with tf.variable_scope(&quot;decoder_scope&quot;) as decoder_scope:

    # output projection
    # we need to specify output projection manually, because sampled softmax needs to have access to the the projection matrix 

    output_projection_w_t = tf.get_variable(&quot;output_projection_w&quot;, [vocabulary_size, state_size], dtype=DTYPE)
    output_projection_w = tf.transpose(output_projection_w_t)
    output_projection_b = tf.get_variable(&quot;output_projection_b&quot;, [vocabulary_size], dtype=DTYPE)
    
    decoder_cell = tf.contrib.rnn.LSTMCell(num_units=state_size)
    decoder_cell = DtypeDropoutWrapper(cell=decoder_cell, output_keep_prob=tf_keep_probabiltiy, dtype=DTYPE)
    decoder_cell = contrib_rnn.MultiRNNCell(cells=[decoder_cell] * num_lstm_layers, state_is_tuple=True)   
    
    # define decoder train netowrk
    decoder_outputs_tr, _ , _ = dynamic_rnn_decoder( 
        cell=decoder_cell, 
        decoder_fn= simple_decoder_fn_train(last_encoder_state, name=None),
        inputs=decoder_inputs, 
        sequence_length=decoder_sequence_lengths,
        parallel_iterations=None,
        swap_memory=False,
        time_major=False)
    
    # define decoder inference network
    decoder_scope.reuse_variables()    
</code></pre>
<p><strong>Here is how the sampled_softmax_loss is calculated:</strong></p>
<pre><code>decoder_forward_outputs = tf.reshape(decoder_outputs_tr,[-1, state_size])
decoder_target_labels  = tf.reshape(decoder_labels ,[-1, 1]) #decoder_labels is target sequnce of decoder

sampled_softmax_losses = tf.nn.sampled_softmax_loss(
    weights = output_projection_w_t,
    biases = output_projection_b,
    inputs = decoder_forward_outputs,
    labels = decoder_target_labels , 
    num_sampled = 500,
    num_classes=vocabulary_size,
    num_true = 1,
)    
total_loss_op = tf.reduce_mean(sampled_softmax_losses) 
</code></pre>
<p><strong>And, this is my decoder in Keras:</strong></p>
<pre><code>decoder_inputs = tf.keras.Input(shape=(None,), name='decoder_input')
emb_layer = tf.keras.layers.Embedding(vocabulary_size, state_size)
x_d = emb_layer(decoder_inputs)

d_lstm_layer = tf.keras.layers.LSTM(embed_dim, return_sequences=True)
d_lstm_out = d_lstm_layer(x_d, initial_state=encoder_states)
</code></pre>
<p><strong>This is my sampled_softmax_loss function I use for Keras model:</strong></p>
<pre><code>class SampledSoftmaxLoss(object):

  def __init__(self, model):
    self.model = model
    output_layer = model.layers[-1]
    self.input = output_layer.input
    self.weights = output_layer.weights

  def loss(self, y_true, y_pred, **kwargs):
    loss = tf.nn.sampled_softmax_loss(
        weights=self.weights[0],
        biases=self.weights[1],
        labels=tf.reshape(y_true ,[-1, 1]),
        inputs=tf.reshape(d_lstm_out,[-1, state_size]),
        num_sampled = 500,
        num_classes = vocabulary_size
    )
</code></pre>
<p>But, it does not work.
Can anyone help me to implement sampled_loss_funtion in Keras correctly.</p>
",Training and Model Evaluation,implementing tf sampled softmax loss funtion seq seq model tf kera model tf code seq seq model trying rewrite using tensorflow kera tf code ha following decoder architecure sampled softmax loss calculated decoder kera sampled softmax loss function use kera model doe work anyone help implement sampled loss funtion kera correctly
Contextual understanding of text using NLP,"<p>I am building a voice assistant using Google Speech to Text API. The text that is transcribed through the API is later used for building reports the client can use. 
The challenge I am facing with this is that about 2/10 times, the Speech to Text conversion is not accurate. eg. ""Hello"" might be transcribed as ""ello"" and complexer word combinations like ""Thymic Horns"" (medical word) are transcribed as ""Thymic vows"". While these issues are most definitely caused by incorrect pronounciation, there is still a need to fix these spelling errors using contextual understanding of sentences. 
My question is, what are the best algorithms in NLP that I can use to address these issues? And what is the kind of data I have to use to train the model to attain maximum possible accuracy?</p>
",Training and Model Evaluation,contextual understanding text using nlp building voice assistant using google speech text api text transcribed api later used building report client use challenge facing time speech text conversion accurate eg hello might transcribed ello complexer word combination like thymic horn medical word transcribed thymic vow issue definitely caused incorrect pronounciation still need fix spelling error using contextual understanding sentence question best algorithm nlp use address issue kind data use train model attain maximum possible accuracy
Techniques for calculating adjective frequency,"<p>I need to calculate word frequencies of a given set of adjectives in a large set of customer support reviews. However I don't want to include those that are negated. </p>

<p>For example suppose my list of adjectives was: [helpful, knowledgeable, friendly]. I want to make sure ""friendly"" isn't counted in a sentence such as ""The representative was not very friendly."" </p>

<p>Do I need to do a full NLP parse of the text or is there an easier approach? I don't need super high accuracy. </p>

<p>I'm not at all familiar with NLP. I'm hoping for something that doesn't have such a steep learning curve and isn't so processor intensive.</p>

<p>Thanks</p>
",Training and Model Evaluation,technique calculating adjective frequency need calculate word frequency given set adjective large set customer support review however want include negated example suppose list adjective wa helpful knowledgeable friendly want make sure friendly counted sentence representative wa friendly need full nlp parse text easier approach need super high accuracy familiar nlp hoping something steep learning curve processor intensive thanks
to get key /values of a list matching the dictionary,"<p>A training data(data_tr-&gt;consists of 11300 doc encoded in numeric values given below as array for each documents). The vocabulary is a dictionay consisting of keys and values of 2000 most frequent words. My problem is to convert the array of data_tr(numeric) to words using dictionary (key,values ) pairs for the whole training data(data_tr). I am new to python programming.</p>
<pre><code>data_tr= [array([ 700,  152,  572,  572,  619,  724,  326, 1571,  572,   99,  724,
        326, 1571,  276,    1,  281, 1232,  267,  267,  222,   11,    2,.......dtype=int64)
 array([ 331,  152,  397, ..., 1273,   89,  228], dtype=int64)...............

vocab = {'limited': 1481, 'child': 181, 'four': 586, 'sleep': 1714, 'hate': 1141, 'forget': 607, 'whose': 902, 'violate': 1945, 'bike': 370, 'swap': 1616, 'lord': 889, 'sorry': 480, 'worth': 625, 'risk': 911, 'rise': 1474, 'every': 119,...........}
</code></pre>
<p><strong>I tried to solved the problem as :</strong></p>
<pre><code>    keys=list(vocab.keys())
    values=list(vocab.values())
    for e in data_tr[0]:
         print (keys[values.index(e)])
</code></pre>
<p>Here it works fine for one document(data_tr[0]) but fail to produce words (data_tr)for the whole training data(data_tr).I wanted to get back the words for all the documents(11300 doc)</p>
",Training and Model Evaluation,get key value list matching dictionary training data data tr consists doc encoded numeric value given array document vocabulary dictionay consisting key value frequent word problem convert array data tr numeric word using dictionary key value pair whole training data data tr new python programming tried solved problem work fine one document data tr fail produce word data tr whole training data data tr wanted get back word document doc
Predicting unobservable data using classification modeling?,"<p>I have a dataset of teacher evaluations. The question is &quot;how can the instructor improve?&quot; I want to be able to classify whether the comment made by the student is a suggestion or not. I have labeled a little over 1000 observations, 1 if it is a suggestion and 0 if it is not.</p>
<p>Here is some of my code:</p>
<pre><code>df2= df[df['suggestion_improve_instructor']&gt;=0]

df2['text']=df2['How_instructor_improve'].str.lower()

def process_text(text):
    #1 Remove punctuation from text
    #2 Remove stopwords
    #3 return a list of clean text words
    
    #1 
    nopunc=[char for char in text if char not in string.punctuation]
    nopunc= ''.join(nopunc)
    
    #2
    clean_words = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]
    
    #3
    return clean_words
    
 
# show the tokenization

df2['text'].head(15).apply(process_text)

# Convert a collection of text to a matrix of tokens
messages_bow= CountVectorizer(analyzer=process_text).fit_transform(df2['text'])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test= train_test_split(messages_bow, df2['suggestion_improve_instructor'], test_size=0.20, random_state=0)
                                 
clf = LogisticRegression(random_state=0).fit(X_train, y_train)

y_log_pred= clf.predict(X_test)

</code></pre>
<p>Doing all of this gives me an accuracy of 91%.</p>
<p>I want to use my model to classify the rest of the comments that I did not label myself. However, when I try to input all of the other observations:</p>
<pre><code>bow= CountVectorizer(analyzer=process_text).fit_transform(df['text'])
prediction= clf.predict(bow)

I get the error:
ValueError: X has 5955 features per sample; expecting 2026
</code></pre>
<p>I know it's because the bag of words of the bigger set of comments is much bigger than the bag of words used to train the model. I just don't know how I can fix this.</p>
<p>What can I do to solve this or is there a better way to use my model to predict unlabeled data with the model I have?</p>
<p>Thank you in advance!</p>
",Training and Model Evaluation,predicting unobservable data using classification modeling dataset teacher evaluation question instructor improve want able classify whether comment made student suggestion labeled little observation suggestion code give accuracy want use model classify rest comment label however try input observation know bag word bigger set comment much bigger bag word used train model know fix solve better way use model predict unlabeled data model thank advance
How can I train the word2vec model on my own corpus in R?,"<p>I would like to train the word2vec model on my own corpus using the <code>rword2vec</code> package in R. </p>

<p>The <code>word2vec</code> function that is used to train the model requires a <code>train_file</code>. The package's documentation in R simply notes that this is the training text data, but doesn't specify how it can be created. </p>

<p>The training data used in the example on GitHub can be downloaded here:
<a href=""http://mattmahoney.net/dc/text8.zip"" rel=""nofollow noreferrer"">http://mattmahoney.net/dc/text8.zip</a>. I can't figure out what type of file it is. </p>

<p>I've looked through the README file on the <a href=""https://github.com/mukul13/rword2vec"" rel=""nofollow noreferrer"">rword2vec GitHub page</a> and checked out the official word2vec page on <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">Google Code</a>.</p>

<p>My corpus is a <code>.csv</code> file with about 68,000 documents. File size is roughly 300MB. I realize that training the model on a corpus of this size might take a long time (or be infeasible), but I'm willing to train it on a subset of the corpus. I just don't know how to create the <code>train_file</code> required by the function.</p>
",Training and Model Evaluation,train word vec model corpus r would like train word vec model corpus using package r function used train model requires package documentation r simply note training text data specify created training data used example github downloaded figure type file looked readme file rword vec github page checked official word vec page google code corpus file document file size roughly mb realize training model corpus size might take long time infeasible willing train subset corpus know create required function
Training Data Format with Spacy,"<p>I am trying to build NLP with Spacy, but I am having trouble formatting the training data. I want my app to be able to recognize entities and intents. For example, in &quot;I want to place an order for pizza&quot;. The intent would be &quot;place_order&quot; and the entity would be pizza. How do I format the training data for BOTH entities and intents in Spacy?</p>
",Training and Model Evaluation,training data format spacy trying build nlp spacy trouble formatting training data want app able recognize entity intent example want place order pizza intent would place order entity would pizza format training data entity intent spacy
learning rate AdamW Optimizer,"<p>I train with BERT (from huggingface) sentiment analysis which is a NLP task.</p>
<p>My question refers to the learning rate.</p>
<pre><code>EPOCHS = 5                                                                                                                                                                                
optimizer = AdamW(model.parameters(), lr=1e-3, correct_bias=True)                  
total_steps = len(train_data_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(                                    
  optimizer,
  num_warmup_steps=0,                                                          
  num_training_steps=total_steps
)
loss_fn = nn.CrossEntropyLoss().to(device)
</code></pre>
<p>Can you please explain how to read 1e-3?</p>
<p>Is this the density of steps or is this a value to decay.</p>
<p>If the latter, is it a linear decay?</p>
<p>If I train with a value 3e-5, which is a recommended value of huggingface for NLP tasks, my model overfits very quickly: loss for training decreases to a minimum, loss for validation increases.</p>
<p>Learning rate 3e-5:</p>
<p><img src=""https://i.sstatic.net/yPkqa.png"" alt=""3e-5"" /></p>
<p>If I train with a value of 1e-2, I get a steady improvement in the loss value of validation. but the validation accuracy does not improve after the first epoch. See picture. Why does the validation value not increase, even though the loss falls. Isn't that a contradiction? I thought these two values were an interpretation of each other.</p>
<p>Learning rate 1e-2:</p>
<p><img src=""https://i.sstatic.net/lEqAc.png"" alt=""1e-2"" /></p>
<p>What would you recommend?</p>
",Training and Model Evaluation,learning rate adamw optimizer train bert huggingface sentiment analysis nlp task question refers learning rate please explain read e density step value decay latter linear decay train value e recommended value huggingface nlp task model overfits quickly loss training decrease minimum loss validation increase learning rate e train value e get steady improvement loss value validation validation accuracy doe improve first epoch see picture doe validation value increase even though loss fall contradiction thought two value interpretation learning rate e would recommend
Build vocabulary only from training data or entire data?,"<p>Should I build the vocabulary only from train data or all data, wouldn't that effect test data in both ways? I mean :</p>
<ul>
<li><p>If we only build the vocab from train data, The model wouldn't recognize a lot of the words in the validation and testing data, if the word is not available in the vocabulary.</p>
</li>
<li><p>Would considering a pre-trained word embedding help in this situation (i.e. the model learns the new word not from training data but from the pre-trained word embedding)?</p>
</li>
<li><p>If yes, Would a randomly Initialized word embedding have the same effect?</p>
</li>
<li><p>On the contrary, I've seen many examples where the coders build their vocab from the entire data,  testing and validation data are shared with training data. Wouldn't this be an obvious data leakage problem?</p>
</li>
</ul>
",Training and Model Evaluation,build vocabulary training data entire data build vocabulary train data data effect test data way mean build vocab train data model recognize lot word validation testing data word available vocabulary would considering pre trained word embedding help situation e model learns new word training data pre trained word embedding yes would randomly initialized word embedding effect contrary seen many example coder build vocab entire data testing validation data shared training data obvious data leakage problem
how to predict from manually trained spacy model,"<p>I have the following code to create and train a new spacy model.
i don't know how to predict entities from the new text?</p>
<p>can anybody help?</p>
<pre><code>TRAIN_DATA = [
    (&quot;Uber blew through $1 million a week&quot;, [(0, 4, 'ORG')]),
    (&quot;Android Pay expands to Canada&quot;, [(0, 11, 'PRODUCT'), (23, 30, 'GPE')]),
    (&quot;Spotify steps up Asia expansion&quot;, [(0, 8, &quot;ORG&quot;), (17, 21, &quot;LOC&quot;)]),
    (&quot;Google Maps launches location sharing&quot;, [(0, 11, &quot;PRODUCT&quot;)]),
    (&quot;Google rebrands its business apps&quot;, [(0, 6, &quot;ORG&quot;)]),
    (&quot;look what i found on google!&quot;, [(21, 27, &quot;PRODUCT&quot;)])]

nlp = spacy.blank(&quot;en&quot;)
optimizer = nlp.begin_training()
from spacy.gold import GoldParse  #&lt;--- add this

for i in range(20):
    random.shuffle(TRAIN_DATA)
    for text, annotations in TRAIN_DATA:
        text = nlp.make_doc(text)
        gold = GoldParse(text, entities=annotations)  #&lt;--- add this
        nlp.update([text], [gold], sgd=optimizer)
</code></pre>
",Training and Model Evaluation,predict manually trained spacy model following code create train new spacy model know predict entity new text anybody help
Text Summarization Evaluation - BLEU vs ROUGE,"<p>With the results of two different summary systems (sys1 and sys2) and the same reference summaries, I evaluated them with both BLEU and ROUGE. The problem is: All ROUGE scores of sys1 was higher than sys2 (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4, ROUGE-L, ROUGE-SU4, ...) but the BLEU score of sys1 was less than the BLEU score of sys2 (quite much).</p>
<p>So my question is: Both ROUGE and BLEU are based on n-gram to measure the similar between the summaries of systems and the summaries of human. So why there are differences in results of evaluation like that? And what's the main different of ROUGE vs BLEU to explain this issue?</p>
",Training and Model Evaluation,text summarization evaluation bleu v rouge result two different summary system sys sys reference summary evaluated bleu rouge problem rouge score sys wa higher sys rouge rouge rouge rouge rouge l rouge su bleu score sys wa le bleu score sys quite much question rouge bleu based n gram measure similar summary system summary human difference result evaluation like main different rouge v bleu explain issue
Is there a particular range for good perplexity value in NLP?,"<p>I'm fine-tuning a language model and am calculating training and validation losses along with the training and validation perplexities. It s calculated by taking the exponential of the loss, in my program. I'm aware that lower perplexities represent better language models and is wondering what the range of values are for a good model. Any help is appreciated. Thank you.</p>
",Training and Model Evaluation,particular range good perplexity value nlp fine tuning language model calculating training validation loss along training validation perplexity calculated taking exponential loss program aware lower perplexity represent better language model wondering range value good model help appreciated thank
NL training without input,"<p>I'm a beginner at developing using bixby, and I was trying to create and modify the sample <strong>Dice</strong> capsule that they have on their github. I wanted to add an extra feature of <strong>snake-eyes</strong> to it, which basically is an extra boolean concept which is true for 1-1 roll on a pair of dice.</p>
<p>Another difference is that I am not taking any input as the sample capsule. In the sample capsule, the user gives the input of number of dice to be roller and the number of sides each die has. I have restricted the number of dice to be 2 with 6 sides each, so there is no point of taking input, and it basically boils down to a simple dice rolling application, which spits out the rolls and the sum.</p>
<p>In the simulator, I am able to get the output perfectly fine by using the <em>&quot;intent { goal : .... }&quot;</em> syntax, but i want to train the model to be able to run on prompt like <em>&quot;roll die&quot;</em>, although there is no concept to match the <strong>roll</strong> command to!</p>
<p>Is there any way to do this?
Thanks in advance!</p>
",Training and Model Evaluation,nl training without input beginner developing using bixby wa trying create modify sample dice capsule github wanted add extra feature snake eye basically extra boolean concept true roll pair dice another difference taking input sample capsule sample capsule user give input number dice roller number side die ha restricted number dice side point taking input basically simple dice rolling application spit roll sum simulator able get output perfectly fine using intent goal syntax want train model able run prompt like roll die although concept match roll command way thanks advance
How to calculate word similarity with lists of words,"<p>Supposing I have three lists which contains words of different categories (database attributes):</p>
<pre><code>products = ['price','quantity','vendor','name']
customers = ['age','location','name','gender']
employee = ['office','title','manager'] 
</code></pre>
<p>Now I have a phrase such as 'product name' which I want to classify them into one of the categories above. Is there any tools where I can calculate the similarity of the phrases with words in the three lists? This is under the assumption that I don't have a large training data sets (only small samples as shown in example above)</p>
",Training and Model Evaluation,calculate word similarity list word supposing three list contains word different category database attribute phrase product name want classify one category tool calculate similarity phrase word three list assumption large training data set small sample shown example
How to calculate the likelihood of a sentence using a Word2Vec model?,"<p>I need to train a Word2Vec model, which I have done, and then I need to use it to calculate the likelihood of previously unseen data. I'm stuck on how to do this though. I've seen WMD but that's for calculating similarity between 2 sentences, while I'm trying to calculate the likelihood of text with my model.</p>
",Training and Model Evaluation,calculate likelihood sentence using word vec model need train word vec model done need use calculate likelihood previously unseen data stuck though seen wmd calculating similarity sentence trying calculate likelihood text model
ML or rule based,"<p>I already have 85 accuracy on my sklearn text classifier. What are the advantages and disadvantages of making a rule based system? Can save doing double the work? Maybe you can provide me with sources and evidence for each side, so that I can make the decision baed on my cirucumstances. Again, I want to know when ruls-based approach is favorable versus when a ML based approach is favorable? Thanks!</p>
",Training and Model Evaluation,ml rule based already accuracy sklearn text classifier advantage disadvantage making rule based system save double work maybe provide source evidence side make decision baed cirucumstances want know ruls based approach favorable versus ml based approach favorable thanks
How to choose the OKAPI BM25 parameters : b and k1,"<p>I was actually wondering, how can we validate or evaluate empirically the values of b and k1 in the BM25 formulas? in other terms what is the most 'scientific' way to evaluate it?</p>

<p>Is there any research paper that we can refer to in order to see how this types of evaluations is done?</p>
",Training and Model Evaluation,choose okapi bm parameter b k wa actually wondering validate evaluate empirically value b k bm formula term scientific way evaluate research paper refer order see type evaluation done
NLP techniques for evaluating grammatical correctness?,"<p>I'm curious about applying NLP to predict/evaluate someone's level of education (or adherence to correct grammar, spelling, etc.) by analyzing text written by them.</p>

<p>It would be something like: f(t) = s where t is a text and s is some score which rates the grammatical correctness of that text.</p>

<p>Does that exist? I don't know how to search for it. If it does, I'd like some references to relevant papers or algorithms.</p>
",Training and Model Evaluation,nlp technique evaluating grammatical correctness curious applying nlp predict evaluate someone level education adherence correct grammar spelling etc analyzing text written would something like f text score rate grammatical correctness text doe exist know search doe like reference relevant paper algorithm
NLP Sentiment Analysis net is not learning,"<p>I want to train a neural net for sentiment analysis. I have followed the tutorials on the keras webpage but I had to adapt the code to my usecase in order to be able to use the net afterwards. </p>

<p>For this purpose I decode back the texts from the imdb dataset from keras from numbers to text, and then I stemmize the text because I need to use the text stemmized. After that, since I want to control the way I am doing the word embeddings rather than using text_to_sequences an pad_sequences I am training a doc2vec embeddings and I am using it on the training set, so that I can obtain the embeddings from the text I want to classify. </p>

<p>The problem is that, the net does not learn anything, the accuracy does not improve and I can not reduce the loss function. I have tried many many things, like the architecture of the net, all the hyperparameters and changing the last layer from 2 nets to 1 and from sparse_categorical_entropy to binary_crossentropy. Let's see if anybody can help and show some light to my problem. I plug the code here and thanks in advance.</p>

<pre><code>from keras.datasets import imdb
max_features = 40000
(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=max_features)

import numpy as np
data = np.concatenate((training_data, testing_data), axis=0)
targets = np.concatenate((training_targets, testing_targets), axis=0)


index = imdb.get_word_index()
reverse_index = dict([(value, key) for (key, value) in index.items()])
decoded = "" "".join([reverse_index.get(i - 3, """") for i in data[0]])

import nltk
from nltk .stem import LancasterStemmer

toke_corpus = list()
lan = LancasterStemmer()

from tqdm import tqdm
lista_reviews = list()

for review in tqdm(data):
  lista_reviews.append(np.array([lan.stem(reverse_index.get(i - 3, '')) for i in review][1:]))

train_x, test_x = lista_reviews[10000:], lista_reviews[:10000]
train_y, test_y = targets[10000:], targets[:10000]

 from gensim.models.callbacks import CallbackAny2Vec

 class EpochLogger(CallbackAny2Vec):
     '''Callback to log information about training'''
     def __init__(self):
         self.epoch = 0
     def on_epoch_begin(self, model):
         print(""Epoch #{} start"".format(self.epoch))
     def on_epoch_end(self, model):
         print(""Epoch #{} end"".format(self.epoch))
         self.epoch += 1


from gensim.models.doc2vec import Doc2Vec, TaggedDocument

documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(lista_reviews)]
print(""DOcuments already built"")
epoch_logger = EpochLogger()
model = Doc2Vec(documents, vector_size=512, window=5, min_count=3, workers=8, epochs = 7, callbacks=[epoch_logger])


encoded_x_train, encoded_x_test = list(), list()
from tqdm import tqdm
for i in tqdm(train_x):
    encoded_x_train.append(model.infer_vector(i))
for k in tqdm(test_x):
    encoded_x_test.append(model.infer_vector(k))

import keras

reduce_lr = keras.callbacks.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.50, patience=2, verbose=1, mode='auto', cooldown=0, min_lr=0.00001)

early = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=1, mode='auto')

from keras import models
from keras.models import Sequential
from keras import layers
from keras.layers import Embedding, Bidirectional, Dense, LSTM, Conv1D, MaxPooling1D, Flatten

model1 = Sequential()
model1.add(Embedding(input_dim = max_features, input_length=512, output_dim=128, trainable=False))

model1.add(Conv1D(filters=64,
                 kernel_size=5,
                 padding='valid',
                 activation='linear',
                 strides=1))
model1.add(MaxPooling1D(pool_size=4))
model1.add(Dense(64, activation='linear'))
model1.add(LSTM(32, activation='tanh'))
# model1.add(Dense(32, activation='relu'))
# model1.add(Flatten())
# model1.add(Dense(1, activation='sigmoid'))
model1.add(Dense(2, activation='softmax'))
model1.summary()


from keras import optimizers
# sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)
adam = optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False)


model1.compile(loss='sparse_categorical_crossentropy',
              optimizer=adam,
              metrics=['accuracy'])

history  = model1.fit( np.array(encoded_x_train), np.array(train_y),
 epochs= 20,
 batch_size = 500,
 validation_data = (np.array(encoded_x_test), np.array(test_y)), callbacks = [reduce_lr, early]
)
</code></pre>
",Training and Model Evaluation,nlp sentiment analysis net learning want train neural net sentiment analysis followed tutorial kera webpage adapt code usecase order able use net afterwards purpose decode back text imdb dataset kera number text stemmize text need use text stemmized since want control way word embeddings rather using text sequence pad sequence training doc vec embeddings using training set obtain embeddings text want classify problem net doe learn anything accuracy doe improve reduce loss function tried many many thing like architecture net hyperparameters changing last layer net sparse categorical entropy binary crossentropy let see anybody help show light problem plug code thanks advance
How to improve a German text classification model in spaCy,"<p>I am working on a text classification project and using <code>spacy</code> for this. Right now I have an accuracy equal to almost 70% but that is not enough. I've been trying to improve the model for past two weeks, but no successful results so far. And here I am looking for an advice about what I should do or try. Any help would be highly appreciated! </p>

<p>So, here is what I do so far:</p>

<blockquote>
  <p>1) Preparing the data:</p>
</blockquote>

<p>I have an unbalanced dataset of German news with 21 categories (like <code>POLITICS</code>, <code>ECONOMY</code>, <code>SPORT</code>, <code>CELEBRITIES</code> etc). In order to make categories equal I duplicate small classes. As a result I have 21 files with almost <code>700 000</code> lines of text. I then normalize this data using the following code:</p>

<pre><code>import spacy
from charsplit import Splitter

POS = ['NOUN', 'VERB', 'PROPN', 'ADJ', 'NUM']  # allowed parts of speech

nlp_helper = spacy.load('de_core_news_sm')
splitter = Splitter()

def normalizer(texts):
    arr = []  # list of normalized texts (will be returned from the function as a result of normalization)

    docs = nlp_helper.pipe(texts)  # creating doc objects for multiple lines
    for doc in docs:  # iterating through each doc object
        text = []  # list of words in normalized text
        for token in doc:  # for each word in text
            token = token.lemma_.lower()

            if token not in stop_words and token.pos_ in POS:  # deleting stop words and some parts of speech
                if len(word) &gt; 8 and token.pos_ == 'NOUN':  # only nouns can be splitted
                    _, word1, word2 = splitter.split_compound(word)[0]  # checking only the division with highest prob
                    word1 = word1.lower()
                    word2 = word2.lower()
                    if word1 in german and word2 in german:
                        text.append(word1)
                        text.append(word2)
                    elif word1[:-1] in german and word2 in german:  # word[:-1] - checking for 's' that joins two words
                        text.append(word1[:-1])
                        text.append(word2)
                    else:
                        text.append(word)
                else:
                    text.append(word)
        arr.append(re.sub(r'[.,;:?!""()-=_+*&amp;^@/\']', ' ', ' '.join(text))) # delete punctuation
    return arr
</code></pre>

<p>Some explanations to the above code:</p>

<p><code>POS</code> - a list of allowed parts of speech. If the word I'm working with at the moment is a part of speech that is not in this list -> I delete it.</p>

<p><code>stop_words</code> - just a list of words I delete.</p>

<p><code>splitter.split_compound(word)[0]</code> - returns a tuple with the most likely division of the compound word (I use it to divide long German words into shorter and more widely used). Here is the <a href=""https://github.com/dtuggener/CharSplit"" rel=""nofollow noreferrer"">link</a> to the repository with this functionality.</p>

<p>To sum up: I find the lemma of the word, make it lower case, delete stop words and some parts of speech, divide compound words, delete punctuation. I then join all the words and return an array of normalized lines.</p>

<blockquote>
  <p>2) Training the model</p>
</blockquote>

<p>I train my model using <code>de_core_news_sm</code> (to make it possible in the future to use this model not only for classification but also for normalization). Here is the code for training:</p>

<pre><code>nlp = spacy.load('de_core_news_sm')

textcat = nlp.create_pipe('textcat', config={""exclusive_classes"": False, ""architecture"": 'simple_cnn'})
nlp.add_pipe(textcat, last=True)
for category in categories:
    textcat.add_label(category)

pipe_exceptions = [""textcat""]
other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]

with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.begin_training()

    for i in range(n_iter):
        shuffle(data)
        batches = spacy.util.minibatch(data)

        for batch in batches:
            texts, annotations = zip(*batch)
            nlp.update(texts, annotations, sgd=optimizer, drop=0.25)
</code></pre>

<p>Some explanations to the above code:</p>

<p><code>data</code> - list of lists, where each list includes a line of text and a dictionary with categories (just like in the <a href=""https://spacy.io/usage/training#textcat"" rel=""nofollow noreferrer"">docs</a>)</p>

<p>'categories' - list of categories</p>

<p>'n_iter' - number of iterations for training</p>

<blockquote>
  <p>3) At the end I just save the model with <code>to_disk</code> method.</p>
</blockquote>

<p>With the above code I managed to train a model with 70% accuracy. Here is a list of what I've tried so far to improve this score:</p>

<blockquote>
  <p>1) Using another architecture (<code>ensemble</code>) - didn't give any improvements</p>
  
  <p>2) Training on non normalized data - the result was much worse </p>
  
  <p>3) Using pretrained BERT model - could'n do it (<a href=""https://stackoverflow.com/questions/61943409/spacys-bert-model-doesnt-learn"">here</a> is my unanswered question about it)</p>
  
  <p>4) Training <code>de_core_news_md</code> instead of <code>de_core_news_sm</code> - didn't give any improvements (tried it because according to the <a href=""https://spacy.io/usage/vectors-similarity#basics"" rel=""nofollow noreferrer"">docs</a> there could be an improvement thanks to the vectors (if I understood it correctly). Correct me if I'm wrong)</p>
  
  <p>5) Training on data, normalized in a slightly different way (without lower casing and punctuation deletion) - didn't give any improvements</p>
  
  <p>6) Changing dropout - didn't help</p>
</blockquote>

<p>So right now I am a little stuck about what to do next. I would be very grateful for any hint or advice.</p>

<p>Thanks in advance for your help!</p>
",Training and Model Evaluation,improve german text classification model spacy working text classification project using right accuracy equal almost enough trying improve model past two week successful result far looking advice try help would highly appreciated far preparing data unbalanced dataset german news category like etc order make category equal duplicate small class result file almost line text normalize data using following code explanation code list allowed part speech word working moment part speech list delete list word delete return tuple likely division compound word use divide long german word shorter widely used link repository functionality sum find lemma word make lower case delete stop word part speech divide compound word delete punctuation join word return array normalized line training model train model using make possible future use model classification also normalization code training explanation code list list list includes line text dictionary category like doc category list category n iter number iteration training end save model method code managed train model accuracy list tried far improve score using another architecture give improvement training non normalized data result wa much worse using pretrained bert model could n doc could improvement thanks vector understood correctly correct wrong training data normalized slightly different way without lower casing punctuation deletion give improvement changing dropout help right little stuck next would grateful hint advice thanks advance help
How to do language model training on BERT,"<p>I want to train BERT on a target corpus. I am looking at this <a href=""https://github.com/huggingface/transformers/tree/master/examples/language-modeling#robertabertdistilbert-and-masked-language-modeling"" rel=""nofollow noreferrer"">HuggingFace implementation</a>.
They are using .raw files for the training data. If I have .txt files of my training data, how can I use their implementation?</p>
",Training and Model Evaluation,language model training bert want train bert target corpus looking huggingface implementation using raw file training data txt file training data use implementation
Spacy - ValueError: Can&#39;t read file: models/model-best/accuracy.json,"<p>I wanted to train a Spacy model on the Swedish UD Treebank.</p>

<p>To do this, I followed the instructions on the spacy page:
<a href=""https://spacy.io/usage/training#spacy-train-cli"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#spacy-train-cli</a></p>

<p>The training itself runs, fine but at the end it is trying to open a file that for some reason doesn't exist. At least not at this location.</p>

<pre><code>USER@Ubuntu18:~/spacy_models/sv$ python -m spacy train sv models talbanken-json/sv_talbanken-ud-train.json talbanken-json/sv_talbanken-ud-dev.json 
⚠ Output directory is not empty
This can lead to unintended side effects when saving the model. Please use an
empty directory or a different path instead. If the specified output path
doesn't exist, the directory will be created for you.
Training pipeline: ['tagger', 'parser', 'ner']
Starting with blank model 'sv'
Counting training words (limit=0)

Itn  Tag Loss    Tag %    Dep Loss    UAS     LAS    NER Loss   NER P   NER R   NER F   Token %  CPU WPS
---  ---------  --------  ---------  ------  ------  ---------  ------  ------  ------  -------  -------
  1  23722.240    87.792  74889.173  67.796  56.740      0.000   0.000   0.000   0.000  100.000    15740                                                                                                                             
....                                                                                                                          
 30    780.531    93.508  13930.092  81.295  75.906      0.000   0.000   0.000   0.000  100.000    15569                                                                                                                             
✔ Saved model to output directory
models/model-final

Traceback (most recent call last):
  File ""/home/USER/miniconda3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/USER/miniconda3/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/USER/miniconda3/lib/python3.7/site-packages/spacy/__main__.py"", line 33, in &lt;module&gt;
    plac.call(commands[command], sys.argv[1:])
  File ""/home/USER/miniconda3/lib/python3.7/site-packages/plac_core.py"", line 328, in call
    cmd, result = parser.consume(arglist)
  File ""/home/USER/miniconda3/lib/python3.7/site-packages/plac_core.py"", line 207, in consume
    return cmd, self.func(*(args + varargs + extraopts), **kwargs)
  File ""/home/USER/miniconda3/lib/python3.7/site-packages/spacy/cli/train.py"", line 497, in train
    best_model_path = _collate_best_model(meta, output_path, nlp.pipe_names)
  File ""/home/USER/miniconda3/lib/python3.7/site-packages/spacy/cli/train.py"", line 559, in _collate_best_model
    bests[component] = _find_best(output_path, component)
  File ""/home/USER/miniconda3/lib/python3.7/site-packages/spacy/cli/train.py"", line 578, in _find_best
    accs = srsly.read_json(epoch_model / ""accuracy.json"")
  File ""/home/USER/miniconda3/lib/python3.7/site-packages/srsly/_json_api.py"", line 50, in read_json
    file_path = force_path(location)
  File ""/home/USER/miniconda3/lib/python3.7/site-packages/srsly/util.py"", line 21, in force_path
    raise ValueError(""Can't read file: {}"".format(location))
ValueError: Can't read file: models/model-best/accuracy.json
</code></pre>

<p>The respective folder itself only contains a <code>meta.json</code> file:</p>

<pre><code>$ ls models/model-best/
meta.json  ner  parser  tagger  tokenizer  vocab
</code></pre>

<p>Fortunately the training of the model itself is not affected by this problem. Still it would be nice if I could run it without crashing.</p>

<p><strong>Is there anything I can do to fix this?</strong></p>
",Training and Model Evaluation,spacy valueerror read file model model best accuracy json wanted train spacy model swedish ud treebank followed instruction spacy page training run fine end trying open file reason exist least location respective folder contains file fortunately training model affected problem still would nice could run without crashing anything fix
"Gensim doc2vec, how to get the value of loss function in each step","<pre class=""lang-py prettyprint-override""><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from random import shuffle
import logging

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

tagged_data = []
clas = ['type1', 'type2', 'type3']
for cla in clas:
  with open(f'../data/jieba/{cla}train.txt', 'r', encoding='UTF-8')as f:
    i = 0
    lines = f.readlines()
    for line in lines:
      tagged_data.append(TaggedDocument(words=line.split(' ')[:-1], tags=[cla + str(i)]))
      i += 1

num_doc = len(tagged_data)
shuffle(tagged_data)

model = Doc2Vec(dm=1, vector_size=128, window=5, alpha=0.01, min_alpha=0.0001, max_vocab_size=100000, sample=1e-5, workers=4, epochs=3, hs=1, dm_mean=1)
model.build_vocab(tagged_data)
model.train(documents=tagged_data, epochs=model.epochs, total_examples=num_doc)
model.save(""d2v.model"")
</code></pre>

<p>The above is my code and the output is like</p>

<pre><code>2019-05-11 01:11:48,177 : INFO : EPOCH 1 - PROGRESS: at 3.64% examples, 307751 words/s, in_qsize 7, out_qsize 0
2019-05-11 01:11:49,195 : INFO : EPOCH 1 - PROGRESS: at 7.63% examples, 316010 words/s, in_qsize 7, out_qsize 0
2019-05-11 01:11:50,196 : INFO : EPOCH 1 - PROGRESS: at 11.44% examples, 316465 words/s, in_qsize 8, out_qsize 0
</code></pre>

<p>How to get the value of loss function in each step so I can visualize it?</p>
",Training and Model Evaluation,gensim doc vec get value loss function step code output like get value loss function step visualize
Transforms of NLP dependency trees into binary trees?,"<p>Spacy (and Core NLP, and other parsers) output dependency trees that can contain varying numbers of children. In spacy for example,  each node has a <code>.lefts</code> and <code>.rights</code> relations (multiple left branches and multiple right branches):</p>

<p><a href=""https://i.sstatic.net/3yIW2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3yIW2.png"" alt=""![enter image description here""></a></p>

<p>Pattern pattern matching algorithms of are considerably simpler (and more efficient) when they  worked over predicates trees who's node have a fixed set of arities.</p>

<p>Is there any standard transformation from these multi-trees into from binary trees?</p>

<p>For example, in this example, we have ""publish"" with two <code>.lefts=[just, journal]</code> and, one <code>.right=[piece]</code>.  Can sentences such (generally) be tranformed into a strict binary tree notation (where each node has 0 or 1 left, and 0 or 1 right branch) without much loss of information, or are multi-trees essential to correctly carrying information?</p>
",Training and Model Evaluation,transforms nlp dependency tree binary tree spacy core nlp parser output dependency tree contain varying number child spacy example node ha relation multiple left branch multiple right branch pattern pattern matching algorithm considerably simpler efficient worked predicate tree node fixed set arity standard transformation multi tree binary tree example example publish two one sentence generally tranformed strict binary tree notation node ha left right branch without much loss information multi tree essential correctly carrying information
Training loss not changing at all (PyTorch),"<p>I am trying to solve a text classification problem. My training data has input as a sequence of 80 numbers in which each represent a word and target value is just a number between 1 and 3.
I pass it through this model:</p>

<pre><code>class Model(nn.Module):
    def __init__(self, tokenize_vocab_count):
        super().__init__()
        self.embd = nn.Embedding(tokenize_vocab_count+1, 300)
        self.embd_dropout = nn.Dropout(0.3)
        self.LSTM = nn.LSTM(input_size=300, hidden_size=100, dropout=0.3, batch_first=True)
        self.lin1 = nn.Linear(100, 1024)
        self.lin2 = nn.Linear(1024, 512)
        self.lin_dropout = nn.Dropout(0.8)
        self.lin3 = nn.Linear(512, 3)

    def forward(self, inp):
        inp = self.embd_dropout(self.embd(inp))
        inp, (h_t, h_o) = self.LSTM(inp)
        h_t = F.relu(self.lin_dropout(self.lin1(h_t)))
        h_t = F.relu(self.lin_dropout(self.lin2(h_t)))
        out = F.softmax(self.lin3(h_t))
        return out
</code></pre>

<p>My training loop is as follows:</p>

<pre><code>model = Model(tokenizer_obj.count+1).to('cuda')

optimizer = optim.AdamW(model.parameters(), lr=1e-2)
loss_fn = nn.CrossEntropyLoss()

EPOCH = 10

for epoch in range(0, EPOCH):
     for feature, target in tqdm(author_dataloader):
         train_loss = loss_fn(model(feature.to('cuda')).view(-1,  3), target.to('cuda'))
         optimizer.zero_grad()
         train_loss.backward()
         optimizer.step()
      print(f""epoch: {epoch + 1}\tTrain Loss : {train_loss}"")
</code></pre>

<p>I printed out the feature and target dimension and it is as follows:</p>

<pre><code>torch.Size([64, 80]) torch.Size([64])
</code></pre>

<p>Here 64 is the batch_size.
I am not doing any validation as of now.
When I train I am getting a constant loss value and no change</p>

<pre><code>/home/koushik/Software/miniconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  ""num_layers={}"".format(dropout, num_layers))
  0%|                                                                                                                                                 | 0/306 [00:00&lt;?, ?it/s]/media/koushik/Backup Plus/Code/Machine Deep Learning/NLP/src/Deep Learning/model.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = F.softmax(self.lin3(h_t))
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 306/306 [00:03&lt;00:00, 89.36it/s]
epoch: 1        Train Loss : 1.0986120700836182
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 306/306 [00:03&lt;00:00, 89.97it/s]
epoch: 2        Train Loss : 1.0986120700836182
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 306/306 [00:03&lt;00:00, 89.35it/s]
epoch: 3        Train Loss : 1.0986120700836182
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 306/306 [00:03&lt;00:00, 89.17it/s]
epoch: 4        Train Loss : 1.0986120700836182
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 306/306 [00:03&lt;00:00, 88.72it/s]
epoch: 5        Train Loss : 1.0986120700836182
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 306/306 [00:03&lt;00:00, 87.75it/s]
epoch: 6        Train Loss : 1.0986120700836182
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 306/306 [00:03&lt;00:00, 85.67it/s]
epoch: 7        Train Loss : 1.0986120700836182
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 306/306 [00:03&lt;00:00, 85.40it/s]
epoch: 8        Train Loss : 1.0986120700836182
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 306/306 [00:03&lt;00:00, 84.49it/s]
epoch: 9        Train Loss : 1.0986120700836182
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 306/306 [00:03&lt;00:00, 84.21it/s]
epoch: 10       Train Loss : 1.0986120700836182
</code></pre>

<p>Can anyone please help</p>
",Training and Model Evaluation,training loss changing pytorch trying solve text classification problem training data ha input sequence number represent word target value number pas model training loop follows printed feature target dimension follows batch size validation train getting constant loss value change anyone please help
sklearn TfidfVectorizer custom ngrams without characters from regex pattern,"<p>I would like to perform custom ngram vectorization using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">sklearn TfidfVectorizer</a>. The generated ngrams should not contain any character from a given regex pattern. Unfortunately the custom tokenizer function is completely ignored when <code>analyzer='char'</code> (ngram mode). See the following example:</p>

<pre class=""lang-py prettyprint-override""><code>import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

pattern = re.compile(r'[\.-]'). # split on '.' and on '-'

def tokenize(text):
    return pattern.split(text)

corpus = np.array(['abc.xyz', 'zzz-m.j'])

# word vectorization
tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize, analyzer='word', stop_words='english')
tfidf_vectorizer.fit_transform(corpus)
print(tfidf_vectorizer.vocabulary_)
# Output -&gt; {'abc': 0, 'xyz': 3, 'zzz': 4, 'm': 2, 'j': 1}
# This is ok!

# ngram vectorization
tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize, analyzer='char', ngram_range=(2, 2))
tfidf_vectorizer.fit_transform(corpus)
print(tfidf_vectorizer.vocabulary_)
# Output -&gt; {'ab': 3, 'bc': 4, 'c.': 5, '.x': 2, 'xy': 7, 'yz': 8, 'zz': 10, 'z-': 9, '-m': 0, 'm.': 6, '.j': 1}
# This is not ok! I don't want ngrams to include the '.' and '-' chars used for tokenization
</code></pre>

<p>What is the best way to do it?</p>
",Training and Model Evaluation,sklearn tfidfvectorizer custom ngrams without character regex pattern would like perform custom ngram vectorization using sklearn tfidfvectorizer generated ngrams contain character given regex pattern unfortunately custom tokenizer function completely ignored ngram mode see following example best way
How to print Categorical features in Machine Learning?,"<p>assume I have a train dataset</p>

<p>r1: cheap, expensive -> price</p>

<p>r2: excited          -> entertainment</p>

<p>r3: hot, summer      -> weather</p>

<p>r4: money            -> price</p>

<p>r5: rain             -> weather</p>

<hr>

<p>then I want to display it in this pattern:</p>

<p>price         -> cheap, expensive, money</p>

<p>entertainment -> excited</p>

<p>weather       -> hot, summer, rain</p>

<p>anyone knows? I'am doing a NLP research. thankyou. </p>
",Training and Model Evaluation,print categorical feature machine learning assume train dataset r cheap expensive price r excited entertainment r hot summer weather r money price r rain weather want display pattern price cheap expensive money entertainment excited weather hot summer rain anyone know nlp research thankyou
Text Image Combined Classification Model,"<p>I am working on a problem statement where I have to match (text, image) pair. Given a furniture description and furniture image, I have to say they are same or not. This is a binary classification problem but I have to combine both text and image data. 
One possible solution I am trying as follows
 <a href=""https://i.sstatic.net/E1EIu.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/E1EIu.jpg"" alt=""enter image description here""></a>
In the above diagram, I am combining the feature from the pre-trained text and image model and training the linear layer end to end. </p>

<p>Is there any other way to handle this type of problem. any leads are most welcome. thanks a lot in advance for your help. </p>
",Training and Model Evaluation,text image combined classification model working problem statement match text image pair given furniture description furniture image say binary classification problem combine text image data one possible solution trying follows diagram combining feature pre trained text image model training linear layer end end way handle type problem lead welcome thanks lot advance help
Sequence Tagging with BertForTokenClassification from Transformers,"<p>I'm trying to use BertForTokenClassification from Transformers with the pretrained multilingual BERT model to do a sequence tagging task. During training, everything seems fine and loss is decreasing per epoch, but when I put the model in evaluation mode and feed it inputs, the logits for each token are all the same. </p>

<p>My code looks like this:</p>

<pre><code>class PretrainedBert(nn.Module):

    def __init__(self,
                 device,
                 pretrained_path='bert-base-multilingual-cased',
                 output_dim=5,
                 learning_rate=0.01,
                 eps=1e-8,
                 max_grad_norm=1.0,
                 model_name='bert_multilingual',
                 cased=True
                 ):
        super(PretrainedBert, self).__init__()

        self.device = device
        self.output_dim = output_dim
        self.learning_rate = learning_rate
        self.max_grad_norm = max_grad_norm
        self.model_name = model_name
        self.cased = cased

        # Load pretrained bert
        self.bert = BertForTokenClassification.from_pretrained(pretrained_path, num_labels=output_dim, output_attentions = False, output_hidden_states = False)
        self.tokenizer = BertTokenizer.from_pretrained(pretrained_path, do_lower_case=not cased)

        param_optimizer = list(self.bert.named_parameters())
        no_decay = ['bias', 'gamma', 'beta']
        optimizer_grouped_parameters = [
            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
             'weight_decay_rate': 0.01},
            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
             'weight_decay_rate': 0.0}]
        self.optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=eps)


    def forward(self, input_ids, attention_mask, labels=None):
        output = self.bert(input_ids, attention_mask=attention_mask, labels=labels)        
        return output


    def fit(self, train_loader, dev_loader, epochs=10):
        """"""
        :param test_loader: torch.utils.data.DataLoader object
        :param test_loader: torch.utils.data.DataLoader object
        :param epochs: int
        """"""
        scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=0,num_training_steps=len(train_loader)*epochs)

        for epoch in range(epochs):
            # Iterate over training data
            epoch_time = time.time()
            self.train()
            epoch_loss = 0
            num_batches = 0

            for raw_text, x, y, idx in tqdm(train_loader):                
                self.zero_grad()

                # Get gold labels and extend them to cover the wordpieces created by bert
                y = self._extend_labels(raw_text, y)
                y, _ = pad_packed_sequence(y, batch_first=True)
                batches_len, seq_length = y.size()
                y.to(self.device)

                # Run input through bert and get logits
                bert_tokenized = self.tokenizer.batch_encode_plus([' '.join(
                    text) for text in raw_text], max_length=seq_length, pad_to_max_length=True)

                input_ids, token_type_ids, attention_masks = bert_tokenized[
                    'input_ids'], bert_tokenized['token_type_ids'], bert_tokenized['attention_mask']

                loss, logits = self.forward(torch.LongTensor(
                    input_ids), torch.LongTensor(attention_masks), labels=y)    

                # Help prevent the ""exploding gradients"" problem.
                torch.nn.utils.clip_grad_norm_(parameters=self.bert.parameters(), max_norm=self.max_grad_norm)

                # update parameters and learning rate
                loss.backward()
                self.optimizer.step()
                scheduler.step()

                epoch_loss += loss.item()
                num_batches += 1

            epoch_time = time.time() - epoch_time

            print()
            print(""Epoch {0} loss: {1:.3f}"".format(epoch + 1,
                                                   epoch_loss / num_batches))

            print(""Dev"")
            binary_f1, propor_f1 = self.evaluate(dev_loader)


    def predict(self, test_loader):
        """"""
        :param test_loader: torch.utils.data.DataLoader object with
                            batch_size=1
        """"""
        self.eval()
        predictions, golds, sents = [], [], []

        for raw_text, x, y, idx in tqdm(test_loader):     
            # Run input through bert and get logits
            bert_tokenized = self.tokenizer.encode_plus(' '.join(raw_text[0]))

            input_ids, token_type_ids, attention_masks = bert_tokenized[
                'input_ids'], bert_tokenized['token_type_ids'], bert_tokenized['attention_mask']

            with torch.no_grad():
                logits = self.forward(torch.LongTensor(
                    input_ids).unsqueeze(0), torch.LongTensor(attention_masks).unsqueeze(0)) 

            # remove batch dim and [CLS]+[SEP] tokens from logits
            logits = logits[0].squeeze(0)[1:-1:]

            # mean pool wordpiece rows of logits and argmax to get predictions 
            preds = self._logits_to_preds(logits, y[0], raw_text[0])

            predictions.append(preds)
            golds.append(y[0])
            sents.append(raw_text[0])

        return predictions, golds, sents


    def evaluate(self, test_loader):
        """"""
        Returns the binary and proportional F1 scores of the model on the examples passed via test_loader.
        :param test_loader: torch.utils.data.DataLoader object with
                            batch_size=1
        """"""
        preds, golds, sents = self.predict(test_loader)


        flat_preds = [int(i) for l in preds for i in l]
        flat_golds = [int(i) for l in golds for i in l]

        analysis = get_analysis(sents, preds, golds)
        binary_f1 = binary_analysis(analysis)
        propor_f1 = proportional_analysis(flat_golds, flat_preds)

        return binary_f1, propor_f1


    def _extend_labels(self, text, labels):
        extended_labels = []

        for idx, (text_seq, label_seq) in enumerate(zip(text, labels)):
            extended_seq_labels = []

            for word, label in zip(text_seq, label_seq):
                n_subwords = len(self.tokenizer.tokenize(word))
                extended_seq_labels.extend([label.item()] * n_subwords)

            extended_labels.append(torch.LongTensor(extended_seq_labels))

        extended_labels = pack_sequence(extended_labels, enforce_sorted=False)

        return extended_labels


    def _logits_to_preds(self, logits, y, raw_text):
        preds = torch.zeros(len(y))
        head = 0
        for idx, word in enumerate(raw_text):
            n_subwords = len(self.tokenizer.tokenize(word))

            if n_subwords &gt; 1:
                preds[idx] = torch.argmax(torch.mean(
                    logits[head:head+n_subwords, :], dim=0))
            else:
                preds[idx] = torch.argmax(logits[head, :])

            head = head + n_subwords

        return preds
</code></pre>

<p>Example input:</p>

<pre><code>raw_text = [['Donkey', 'Kong', 'Country', ':']]
y =  [tensor([0, 0, 0, 0])]
</code></pre>

<p>Logits produced by running these lines</p>

<pre><code>self.eval()

bert_tokenized = self.tokenizer.encode_plus(' '.join(raw_text[0]))

input_ids, token_type_ids, attention_masks = bert_tokenized['input_ids'], bert_tokenized['token_type_ids'], bert_tokenized['attention_mask']

with torch.no_grad():
    logits = self.forward(torch.LongTensor(input_ids).unsqueeze(0), torch.LongTensor(attention_masks).unsqueeze(0)) 
</code></pre>

<p>Resulting logits:</p>

<pre><code>logits = tensor([[-3.2811,  1.1715,  1.2381,  0.5201,  1.0921],
        [-3.2813,  1.1715,  1.2382,  0.5201,  1.0922],
        [-3.2815,  1.1716,  1.2383,  0.5202,  1.0923],
        [-3.2814,  1.1716,  1.2383,  0.5202,  1.0922],
        [-3.2811,  1.1715,  1.2381,  0.5201,  1.0921]])
</code></pre>

<p>Any ideas for what I'm doing wrong? Thanks for any help in advance. </p>
",Training and Model Evaluation,sequence tagging bertfortokenclassification transformer trying use bertfortokenclassification transformer pretrained multilingual bert model sequence tagging task training everything seems fine loss decreasing per epoch put model evaluation mode feed input logits token code look like example input logits produced running line resulting logits idea wrong thanks help advance
Keras NLP validation loss increases while training accuracy increases,"<p>I have looked at other posts with similar problems and it seems that my model is overfitting. However, I've tried regularization, dropout, reducing parameters, decreasing the learning rate and changing the loss function, but nothing seems to help.</p>

<p>Here is my model:</p>

<pre><code>model = Sequential([
Embedding(max_words, 64),
Dropout(.5),
Bidirectional(GRU(64, return_sequences = True), merge_mode='concat'),
GlobalMaxPooling1D(),
Dense(64),
Dropout(.5),
Dense(1, activation='sigmoid')
])
model.summary()

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(x_train,y_train, batch_size=32, epochs=25, verbose=1, validation_data=(x_test, y_test),shuffle=True)
</code></pre>

<p>And my training output:</p>

<pre><code>Model: ""sequential_3""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_3 (Embedding)      (None, None, 64)          320000    
_________________________________________________________________
dropout_6 (Dropout)          (None, None, 64)          0         
_________________________________________________________________
bidirectional_3 (Bidirection (None, None, 128)         49920     
_________________________________________________________________
global_max_pooling1d_3 (Glob (None, 128)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 64)                8256      
_________________________________________________________________
dropout_7 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 65        
=================================================================
Total params: 378,241
Trainable params: 378,241
Non-trainable params: 0
_________________________________________________________________
Epoch 1/25
229/229 [==============================] - 7s 32ms/step - loss: 0.6952 - accuracy: 0.4939 - val_loss: 0.6923 - val_accuracy: 0.5240
Epoch 2/25
229/229 [==============================] - 7s 30ms/step - loss: 0.6917 - accuracy: 0.5144 - val_loss: 0.6973 - val_accuracy: 0.4815
Epoch 3/25
229/229 [==============================] - 7s 30ms/step - loss: 0.6709 - accuracy: 0.5881 - val_loss: 0.7164 - val_accuracy: 0.4784
Epoch 4/25
229/229 [==============================] - 7s 30ms/step - loss: 0.6070 - accuracy: 0.6711 - val_loss: 0.7704 - val_accuracy: 0.4977
Epoch 5/25
229/229 [==============================] - 7s 30ms/step - loss: 0.5370 - accuracy: 0.7325 - val_loss: 0.8411 - val_accuracy: 0.4876
Epoch 6/25
229/229 [==============================] - 7s 30ms/step - loss: 0.4770 - accuracy: 0.7714 - val_loss: 0.9479 - val_accuracy: 0.4784
Epoch 7/25
229/229 [==============================] - 7s 30ms/step - loss: 0.4228 - accuracy: 0.8016 - val_loss: 1.0987 - val_accuracy: 0.4884
Epoch 8/25
229/229 [==============================] - 7s 30ms/step - loss: 0.3697 - accuracy: 0.8344 - val_loss: 1.2714 - val_accuracy: 0.4760
Epoch 9/25
229/229 [==============================] - 7s 30ms/step - loss: 0.3150 - accuracy: 0.8582 - val_loss: 1.4184 - val_accuracy: 0.4822
Epoch 10/25
229/229 [==============================] - 7s 31ms/step - loss: 0.2725 - accuracy: 0.8829 - val_loss: 1.6053 - val_accuracy: 0.4946
Epoch 11/25
229/229 [==============================] - 7s 31ms/step - loss: 0.2277 - accuracy: 0.9056 - val_loss: 1.8131 - val_accuracy: 0.4884
Epoch 12/25
229/229 [==============================] - 7s 31ms/step - loss: 0.1929 - accuracy: 0.9253 - val_loss: 1.9327 - val_accuracy: 0.4977
Epoch 13/25
229/229 [==============================] - 7s 30ms/step - loss: 0.1717 - accuracy: 0.9318 - val_loss: 2.2280 - val_accuracy: 0.4900
Epoch 14/25
229/229 [==============================] - 7s 30ms/step - loss: 0.1643 - accuracy: 0.9324 - val_loss: 2.2811 - val_accuracy: 0.4915
Epoch 15/25
229/229 [==============================] - 7s 30ms/step - loss: 0.1419 - accuracy: 0.9439 - val_loss: 2.4530 - val_accuracy: 0.4830
Epoch 16/25
229/229 [==============================] - 7s 30ms/step - loss: 0.1255 - accuracy: 0.9521 - val_loss: 2.6692 - val_accuracy: 0.4992
Epoch 17/25
229/229 [==============================] - 7s 30ms/step - loss: 0.1124 - accuracy: 0.9558 - val_loss: 2.8106 - val_accuracy: 0.4892
Epoch 18/25
229/229 [==============================] - 7s 30ms/step - loss: 0.1130 - accuracy: 0.9556 - val_loss: 2.6792 - val_accuracy: 0.4907
Epoch 19/25
229/229 [==============================] - 7s 30ms/step - loss: 0.1085 - accuracy: 0.9610 - val_loss: 2.8966 - val_accuracy: 0.5093
Epoch 20/25
229/229 [==============================] - 7s 30ms/step - loss: 0.0974 - accuracy: 0.9656 - val_loss: 2.8636 - val_accuracy: 0.5147
Epoch 21/25
229/229 [==============================] - 7s 30ms/step - loss: 0.0921 - accuracy: 0.9663 - val_loss: 2.9874 - val_accuracy: 0.4977
Epoch 22/25
229/229 [==============================] - 7s 30ms/step - loss: 0.0888 - accuracy: 0.9685 - val_loss: 3.0295 - val_accuracy: 0.4969
Epoch 23/25
229/229 [==============================] - 7s 30ms/step - loss: 0.0762 - accuracy: 0.9731 - val_loss: 3.0607 - val_accuracy: 0.4884
Epoch 24/25
229/229 [==============================] - 7s 30ms/step - loss: 0.0842 - accuracy: 0.9692 - val_loss: 3.0552 - val_accuracy: 0.4900
Epoch 25/25
229/229 [==============================] - 7s 30ms/step - loss: 0.0816 - accuracy: 0.9693 - val_loss: 2.9571 - val_accuracy: 0.5015
</code></pre>

<p>My validation loss seems to always increases no matter what. I am trying to predict political affiliation from tweets. The dataset I am using has worked well on other models, so perhaps there is something wrong with my data preprocessing instead?</p>

<pre><code>import pandas as pd
dataset = pd.read_csv('political_tweets.csv')
dataset.head()
dataset = pd.read_csv('political_tweets.csv')[""tweet""].values
y_train = pd.read_csv('political_tweets.csv')[""dem_or_rep""].values

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(dataset, y_train, test_size=0.15, shuffle=True)
print(x_train[0])
print(x_test[0])
max_words = 10000
max_len = 25

tokenizer = Tokenizer(num_words = max_words, filters='!""#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n1234567890', lower=False,oov_token=""&lt;OOV&gt;"")

tokenizer.fit_on_texts(x_train)

x_train = tokenizer.texts_to_sequences(x_train)
x_train = pad_sequences(x_train, max_len, padding='post', truncating='post')

tokenizer.fit_on_texts(x_test)
x_test = tokenizer.texts_to_sequences(x_test)
x_test = pad_sequences(x_test, max_len, padding='post', truncating='post')
</code></pre>

<p>I am really stumped. Any help is appreciated.</p>
",Training and Model Evaluation,kera nlp validation loss increase training accuracy increase looked post similar problem seems model overfitting however tried regularization dropout reducing parameter decreasing learning rate changing loss function nothing seems help model training output validation loss seems always increase matter trying predict political affiliation tweet dataset using ha worked well model perhaps something wrong data preprocessing instead really stumped help appreciated
"algorithm in Python for optimal splitting in train, validation and test","<p>I am trying to optimize the distribution of samples into train, validation, and test sets, for example 80%, 10%, 10%. The concrete problems arises in the constitution of these sets for Universal Dependencies that <a href=""https://universaldependencies.org/release_checklist.html"" rel=""nofollow noreferrer"">says</a>: ""If the treebank contains running text (rather than random shuffled sentences), make sure you split the data on document boundaries."" I've looked at <a href=""https://stackoverflow.com/questions/3674409/how-to-split-partition-a-dataset-into-training-and-test-datasets-for-e-g-cros"">quite</a> <a href=""https://stackoverflow.com/questions/51064254/splitting-data-into-test-and-train-including"">a</a> <a href=""https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test"">few</a> <a href=""https://stackoverflow.com/questions/60547879/how-to-find-the-optimal-values-for-splitting-the-data-into-test-and-train"">pages</a> about sample distribution, but they all seem to be based on randomness.</p>

<p>So here is the actual data of my 80 samples:</p>

<pre class=""lang-py prettyprint-override""><code>split=[10,10,80]
file2len = {'f0': 2472, 'f1': 1480, 'f2': 592, 'f3': 1439, 'f4': 2310, 'f5': 2081, 'f6': 2201, 'f7': 2647, 'f8': 1998, 'f9': 861, 'f10': 2373, 'f11': 1473, 'f12': 475, 'f13': 2227, 'f14': 3117, 'f15': 2461, 'f16': 880, 'f17': 2781, 'f18': 1041, 'f19': 1620, 'f20': 1294, 'f21': 2274, 'f22': 2640, 'f23': 1920, 'f24': 1756, 'f25': 1476, 'f26': 1675, 'f27': 1484, 'f28': 1432, 'f29': 872, 'f30': 951, 'f31': 1175, 'f32': 655, 'f33': 642, 'f34': 1905, 'f35': 1078, 'f36': 950, 'f37': 1684, 'f38': 1140, 'f39': 1045, 'f40': 771, 'f41': 1035, 'f42': 694, 'f43': 1730, 'f44': 1105, 'f45': 932, 'f46': 1437, 'f47': 2678, 'f48': 1883, 'f49': 1807, 'f50': 951, 'f51': 1924, 'f52': 1417, 'f53': 1739, 'f54': 1902, 'f55': 1950, 'f56': 1959, 'f57': 1630, 'f58': 1588, 'f59': 784, 'f60': 1475, 'f61': 1765, 'f62': 3996, 'f63': 1345, 'f64': 1330, 'f65': 579, 'f66': 1989, 'f67': 806, 'f68': 1301, 'f69': 1888, 'f70': 1380, 'f71': 786, 'f72': 1650, 'f73': 2723, 'f74': 1648, 'f75': 1378, 'f76': 1274, 'f77': 1458, 'f78': 529, 'f79': 2939}
totalt=sum(file2len.values())
</code></pre>

<p>Here's the distribution function I'm using:</p>

<pre class=""lang-py prettyprint-override""><code>import random
def makedevtraintest(file2len, totalt, split):
    splitfiles=[]
    actualdistri=[]
    infilelist=list(file2len.keys())
    for spli in split:
        already=0
        thisselection=[]
        goal=spli/100*totalt
        while already&lt;goal and infilelist:
            f = random.choice(infilelist)
            already+=file2len[f]
            infilelist.remove(f)
            thisselection+=[f]
        actualdistri+=[already]
        splitfiles+=[thisselection]
    assert infilelist==[]
    return splitfiles, actualdistri
</code></pre>

<p>And then I call it brute-force 10000 times to get the best distribution:</p>

<pre class=""lang-py prettyprint-override""><code>minidis = 100
for i in range(10000):
    splitfiles, actualdistri = makedevtraintest(file2len, totalt, split)
    dis = abs(actualdistri[-1]/totalt*100-split[-1])        
    if dis&lt;minidis:
        optisplitfiles, optiactualdistri = splitfiles, actualdistri
        minidis=dis
        print('yay',i,dis,[t/totalt for t in actualdistri])
print([t/totalt for t in optiactualdistri])
</code></pre>

<p>which gives quite decent results:
<code>[0.10004576586813117, 0.10002209386737367, 0.7999321402644951]</code>
(although my algorithm will only produce results where the last sample set is not full, &lt;80%).</p>

<p>I was wondering whether there is a non-exponential algorithm to find the optimal distribution, the one that is closest to the 80,10,10. This looks like a common problem in algorithmics but I can't come up with the right search terms to find it.</p>
",Training and Model Evaluation,algorithm python optimal splitting train validation test trying optimize distribution sample train validation test set example concrete problem arises constitution set universal dependency say treebank contains running text rather random shuffled sentence make sure split data document boundary looked actual data sample distribution function using call brute force time get best distribution give quite decent result although algorithm produce result last sample set full wa wondering whether non exponential algorithm find optimal distribution one closest look like common problem algorithmics come right search term find
Loss of a LSTM implemented in Keras with TF2 backend keeps a constant and the parameters are basically not learned while training on a large dataset,"<p>We are implementing this following architecture in Tensorflow 2. RNN means LSTM in this model.</p>

<p><a href=""https://i.sstatic.net/ZmcsX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZmcsX.png"" alt=""Dual Encoder LSTM""></a></p>

<p>This model is used to realize a retrieval-based chatbot found in this <a href=""https://arxiv.org/pdf/1506.08909.pdf"" rel=""nofollow noreferrer"">paper</a> and this <a href=""http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/"" rel=""nofollow noreferrer"">blog</a>. The blog author has a <a href=""https://github.com/dennybritz/chatbot-retrieval"" rel=""nofollow noreferrer"">working model</a> written in tf1.</p>

<p>Our Colab notebook can be accessed <a href=""https://colab.research.google.com/drive/1cOTl4lK9nFNOxi5uoNzucQ1vD9mr-i5C"" rel=""nofollow noreferrer"">here</a>. And the model architecture output from our code is </p>

<p><a href=""https://i.sstatic.net/WfuNv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WfuNv.png"" alt=""this""></a> </p>

<p>The ? symbol means batch_size we can predefine. 160 is the sentence length, which means the repetitions of the LSTM network, since we feed the LSTM one word at a time. 256 is the output shape of the LSTM network. The training dataset looks like this</p>

<p><a href=""https://i.sstatic.net/v9O7c.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/v9O7c.png"" alt=""enter image description here""></a></p>

<p>This model was claimed to be working while training on a Ubuntu Corpus dataset with 1M records. However, when we train our model, the loss remains at 0.6931 after 2~3 iterations and we assume then the parameters, especially M, are not learned during training. We had the model run for 12 hours and the loss did not decrease at all. Here is the output from training</p>

<pre><code>Now training the model...
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.6938(195600,)
1000000/1000000 [==============================] - 2111s 2ms/sample - loss: 0.6938
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.7739(195600,)
1000000/1000000 [==============================] - 2106s 2ms/sample - loss: 0.7739
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.6939(195600,)
1000000/1000000 [==============================] - 2163s 2ms/sample - loss: 0.6939
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.6932(195600,)
1000000/1000000 [==============================] - 2101s 2ms/sample - loss: 0.6932
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.6931(195600,)
1000000/1000000 [==============================] - 2099s 2ms/sample - loss: 0.6931
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.6932(195600,)
1000000/1000000 [==============================] - 2098s 2ms/sample - loss: 0.6932
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.6931(195600,)
1000000/1000000 [==============================] - 2102s 2ms/sample - loss: 0.6931
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.6931(195600,)
1000000/1000000 [==============================] - 2097s 2ms/sample - loss: 0.6931
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.6931(195600,)
1000000/1000000 [==============================] - 2107s 2ms/sample - loss: 0.6931
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.6931(195600,)
1000000/1000000 [==============================] - 2103s 2ms/sample - loss: 0.6931
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.6931(195600,)
1000000/1000000 [==============================] - 2103s 2ms/sample - loss: 0.6931
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.6931(195600,)
1000000/1000000 [==============================] - 2092s 2ms/sample - loss: 0.6931
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.6931(195600,)
1000000/1000000 [==============================] - 2099s 2ms/sample - loss: 0.6931
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.6931(195600,)
1000000/1000000 [==============================] - 2099s 2ms/sample - loss: 0.6931
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.6931(195600,)
1000000/1000000 [==============================] - 2096s 2ms/sample - loss: 0.6931
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.6931(195600,)
1000000/1000000 [==============================] - 2125s 2ms/sample - loss: 0.6931
Train on 1000000 samples
 999936/1000000 [============================&gt;.] - ETA: 0s - loss: 0.6931(195600,)
1000000/1000000 [==============================] - 2132s 2ms/sample - loss: 0.6931
Train on 1000000 samples
  33792/1000000 [&gt;.............................] - ETA: 24:23 - loss: 0.6931
</code></pre>

<p>However, our model will work if we truncate our dataset to 100,000 records, which means the loss will be decreasing steadily.</p>

<p>What could be the reason that this model could train on a small dataset but not on a large dataset?</p>

<p>Thanks.</p>
",Training and Model Evaluation,loss lstm implemented kera tf backend keep constant parameter basically learned training large dataset implementing following architecture tensorflow rnn mean lstm model model used realize retrieval based chatbot found paper blog blog author ha working model written tf colab notebook accessed model architecture output code symbol mean batch size predefine sentence length mean repetition lstm network since feed lstm one word time output shape lstm network training dataset look like model wa claimed working training ubuntu corpus dataset record however train model loss remains iteration assume parameter especially learned training model run hour loss decrease output training however model work truncate dataset record mean loss decreasing could reason model could train small dataset large dataset thanks
"word2vec, using document body or keywords as training corpus","<p>I would like to train a <code>word2vec</code> model using what is an unordered list of keywords and categories for each document. Therefore my vocabulary is quite small around 2.5k tokens.</p>

<p>Would the performance be improved if at the training step, I used actual sentences from the document?</p>

<p>From example:</p>

<pre><code>doc_keywords = ['beach', 'holiday', 'warm']
doc_body = 'Going on a beach holiday it can be very warm'
</code></pre>

<p>If there is a benefit to using the full documents, could someone also explain why this is the case?</p>

<p>Since the model predicts the next word in a document, what would be the benefit to it learning <code>very -&gt; warm</code> as two words which often come together, given that <code>very</code> is not in my vocabulary.</p>
",Training and Model Evaluation,word vec using document body keywords training corpus would like train model using unordered list keywords category document therefore vocabulary quite small around k token would performance improved training step used actual sentence document example benefit using full document could someone also explain case since model predicts next word document would benefit learning two word often come together given vocabulary
Making a News Category Classifier using LSTM,"<p>I am making a news classification model using LSTM. When I train the model my training accuracy keeps on improving but validation accuracy is not increasing beyond 57%. I have nearly 200k news paragraphs and classifying them into 30 different categories.</p>

<p>My model is as follow:</p>

<pre><code>model=Sequential()
model.add(Embedding(30000,64,input_length=X_train.shape[1],mask_zero=True))
model.add(Bidirectional(LSTM(64)))
model.add(Dropout(0.5))
model.add(Dense(64,activation='relu'))
model.add(Dense(30))
model.add(Activation(activation='softmax'))
model.compile(loss=""categorical_crossentropy"",optimizer='adam',metrics=['accuracy'])
</code></pre>
",Training and Model Evaluation,making news category classifier using lstm making news classification model using lstm train model training accuracy keep improving validation accuracy increasing beyond nearly k news paragraph classifying different category model follow
Substring evaluation of word components in R for NLP,"<p>I'm trying to do some string evaluations on given words such that the output is a list of the components of the word in 2 letter combinations.</p>

<p>Eg </p>

<p>'House' becomes 'ho','ou','us','se'</p>

<p>Producing this outcome is relatively easy using 'substr' as below:</p>

<pre><code>y= 'house'

substr(y, start = 1, stop = 2)
substr(y, start = 2, stop = 3)
substr(y, start = 3, stop = 4)
substr(y, start = 4, stop = 5)
</code></pre>

<p>What I would like to be able to do however, is do this almost recursively so that any word of any length will be outputted to its component 2 letter combinations.</p>

<p>So 'Motorcar' become 'mo','ot','to','or','rc','ca','ar'. Etc Etc.</p>

<p>Is there a way this can perhaps be done using loops or a function? Does the lenght of the word need to be a condition of the function?</p>

<p>Any thoughts greatly appreciated.</p>
",Training and Model Evaluation,substring evaluation word component r nlp trying string evaluation given word output list component word letter combination eg house becomes ho ou u se producing outcome relatively easy using substr would like able however almost recursively word length outputted component letter combination motorcar become mo ot rc ca ar etc etc way perhaps done using loop function doe lenght word need condition function thought greatly appreciated
Feature extraction using flairNLP,"<p>I'm trying to use <a href=""https://github.com/flairNLP/flair"" rel=""nofollow noreferrer"">flair</a> for sentiment analysis, but I also need to know how much each word influenced the score of a sentence.</p>

<p>I've followed this <a href=""https://medium.com/@b.terryjack/nlp-pre-trained-sentiment-analysis-1eb52a9d742c"" rel=""nofollow noreferrer"">article</a> to predict the sentiment, but it doesn't show how to extract the features of the given sentence.
I'm assuming there is a way to do this feature extraction because of the way it's presented in that article, but I can't find it. I've tried reading the flair documentation and the code itself but didn't see a way to do so.</p>

<p>What I'm looking for is a functionality of this sort:</p>

<pre><code>import flair
text = flair.data.Sentence(&lt;string-with-sentiment&gt;)
model = flair.models.TextClassifier.load('en-sentiment')
model.predict(text)
print(s.individual_sentiments)
</code></pre>

<p>Result:</p>

<pre><code>[('i', 0.08), ('do', 0.09), ('like', 1.0), ('you', -0.32)]
</code></pre>

<p>I'm not trying to train my own model, but rather use a pre trained one like in the code example above.</p>

<p>Note: I'm not bound to flair, if a different framework with this functionality exists I'll be happy to know about it as well. I'm trying to use flair because it out performed Textblob and nltk's VADER in accuracy when I tested it.</p>
",Training and Model Evaluation,feature extraction using flairnlp trying use flair sentiment analysis also need know much word influenced score sentence followed article predict sentiment show extract feature given sentence assuming way feature extraction way presented article find tried reading flair documentation code see way looking functionality sort result trying train model rather use pre trained one like code example note bound flair different framework functionality exists happy know well trying use flair performed textblob nltk vader accuracy tested
Imbalanced text classification by oversampling: correction class probability,"<p>My dataset has 3 class and 900 examples for training. Class distribution is 220, 185, and 500.</p>

<p>I found that if I oversample the training data then I have to correct/calibrate the predicted probability of the test data because after oversampling the training and testing data distribution are not same. This is nicely described <a href=""https://www.knime.com/blog/correcting-predicted-class-probabilities-in-imbalanced-datasets"" rel=""nofollow noreferrer"">here</a>.</p>

<p>I have three questions:</p>

<ol>
<li><p>Do I have to do this also for predicting validation dataset (used for early stopping)?</p></li>
<li><p>Do I have to correct the probabilities for loss calculation? </p></li>
<li><p>Is this a mandatory step? I am asking this because this might hurt the overall accuracy. Because this will penalize the probabilities of the classes which have less example. </p></li>
</ol>
",Training and Model Evaluation,imbalanced text classification oversampling correction class probability dataset ha class example training class distribution found oversample training data correct calibrate predicted probability test data oversampling training testing data distribution nicely described three question also predicting validation dataset used early stopping correct probability loss calculation mandatory step asking might hurt overall accuracy penalize probability class le example
BERT pre-training loss not decreasing,"<p>I'm pre-training BERT with Bulgarian dataset on a single Cloud TPU v2 8 using the original parameters (learning rate = 5e-5, training batch size = 32, number of training steps = 100000).
The problem is that it finishes training very fast (3 hours) and the loss doesn't go below 3. My training data is 40 GB and I'm using tensorflow 1.15 <a href=""https://i.sstatic.net/klA2W.jpg"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>Do you have any idea what the problem might be?</p>
",Training and Model Evaluation,bert pre training loss decreasing pre training bert bulgarian dataset single cloud tpu v using original parameter learning rate e training batch size number training step problem finish training fast hour loss go training data gb using tensorflow enter image description idea problem might
Word not in vocabulary of GoogleNews-vectors-negative300.bin,"<p>I am trying to see what pre-trained model has included common phrases in news and I thought GoogleNews-vectors-negative300.bin should be a comprehensive one but it turned out that it does not even include deep_learning, machine_learning, social_network, social_responsibility. What pre-trained model could include those words that often occur in news, public reports?</p>

<pre><code>import gensim

# Load Google's pre-trained Word2Vec model.
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)

model.similarity('deep_learning', 'machine_learning')
</code></pre>
",Training and Model Evaluation,word vocabulary googlenews vector negative bin trying see pre trained model ha included common phrase news thought googlenews vector negative bin comprehensive one turned doe even include deep learning machine learning social network social responsibility pre trained model could include word often occur news public report
Is there any way to improve Google NLP API result accuracy?,"<p>I am using Google-Vision API to get text from images and using its result into NLP API.
So far i intend to get Name,Location,address,email,contact number,job title,company name etc.
when i scan a business card.Results so far are not that much accurate as sometimes the results are too vague,also NLP API return multiple entries for the same content text <em>i.e Multiple value in names field,location field sometimes incorrect classifications too</em>.Any suggestions on how i can improve its results?</p>

<p><strong>Reference</strong> </p>

<ol>
<li><a href=""https://cloud.google.com/vision/docs/ocr"" rel=""nofollow noreferrer"">Google vision API</a> </li>
<li><p><a href=""https://cloud.google.com/natural-language/docs/analyzing-entities"" rel=""nofollow noreferrer"">Google language processing API</a> </p>

<p>Say for this business card  <a href=""https://i.sstatic.net/9CryP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9CryP.jpg"" alt=""enter image description here""></a></p>

<p>VISION API results into <a href=""https://i.sstatic.net/3zglA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3zglA.png"" alt=""enter image description here""></a></p>

<p>NLP results into <a href=""https://i.sstatic.net/A9mly.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/A9mly.png"" alt=""enter image description here""></a></p></li>
</ol>
",Training and Model Evaluation,way improve google nlp api result accuracy using google vision api get text image using result nlp api far intend get name location address email contact number job title company name etc scan business card result far much accurate sometimes result vague also nlp api return multiple entry content text e multiple value name field location field sometimes incorrect classification suggestion improve result reference google vision api google language processing api say business card vision api result nlp result
Python - How to intuit word from abbreviated text using NLP?,"<p>I was recently working on a data set that used abbreviations for various words. For example,</p>

<pre><code>wtrbtl = water bottle
bwlingbl = bowling ball
bsktball = basketball
</code></pre>

<p>There did not seem to be any consistency in terms of the convention used, i.e. sometimes they used vowels sometimes not. I am trying to build a mapping object like the one above for abbreviations and their corresponding words without a complete corpus or comprehensive list of terms (i.e. abbreviations could be introduced that are not explicitly known). For simplicity sake say it is restricted to stuff you would find in a gym but it could be anything.</p>

<p>Basically, if you only look at the left hand side of the examples, what kind of model could do the same processing as our brain in terms of relating each abbreviation to the corresponding full text label. </p>

<p>My ideas have stopped at taking the first and last letter and finding those in a dictionary. Then assign a priori probabilities based on context. But since there are a large number of morphemes without a marker that indicates end of word I don't see how its possible to split them. </p>

<p>UPDATED: </p>

<p>I also had the idea to combine a couple string metric algorithms like a Match Rating Algorithm to determine a set of related terms and then calculate the Levenshtein Distance between each word in the set to the target abbreviation. However, I am still in the dark when it comes to abbreviations for words not in a master dictionary. Basically, inferring word construction - may a Naive Bayes model could help but I am concerned that any error in precision caused by using the algorithms above will invalid any model training process. </p>

<p>Any help is appreciated, as I am really stuck on this one.</p>
",Training and Model Evaluation,python intuit word abbreviated text using nlp wa recently working data set used abbreviation various word example seem consistency term convention used e sometimes used vowel sometimes trying build mapping object like one abbreviation corresponding word without complete corpus comprehensive list term e abbreviation could introduced explicitly known simplicity sake say restricted stuff would find gym could anything basically look left hand side example kind model could processing brain term relating abbreviation corresponding full text label idea stopped taking first last letter finding dictionary assign priori probability based context since large number morpheme without marker indicates end word see possible split updated also idea combine couple string metric algorithm like match rating algorithm determine set related term calculate levenshtein distance word set target abbreviation however still dark come abbreviation word master dictionary basically inferring word construction may naive bayes model could help concerned error precision caused using algorithm invalid model training process help appreciated really stuck one
get topic of sentence from pre-trained model,"<p>I have a list of 10 sentences from a text file.
I want to use an existing topics model to get the topics of every sentence.</p>

<p>In all the tutorials I found - they trained the topic model on their corpus. I want to use one that was trained in existing corpus and just apply it on my sentences.</p>

<p>Is this possible?</p>
",Training and Model Evaluation,get topic sentence pre trained model list sentence text file want use existing topic model get topic every sentence tutorial found trained topic model corpus want use one wa trained existing corpus apply sentence possible
How to map detailed text to a unigram or a bigram,"<p>I am trying to figure out solution for requirement where in I am required to map long text to unigrams or bigrams. 
For example
""Ability to motivate and manage team. You should be able to track the progress of the team and intervene to improve the progress"". This long text should be mapped to ""Team management"". Basically I am trying to figure out communication/analytical skills from the long text seen in document like Job descriptions. I am struggling to figure out a solution for this. I do not want to hard code as the long text keep changing. Thanks for any help.</p>
",Training and Model Evaluation,map detailed text unigram bigram trying figure solution requirement required map long text unigrams bigram example ability motivate manage team able track progress team intervene improve progress long text mapped team management basically trying figure communication analytical skill long text seen document like job description struggling figure solution want hard code long text keep changing thanks help
Machine Learning - How to extract features from pipeline,"<p>I am totaly new to the field and currently I am stuck. Here is What I want and what I did:</p>

<p>I have a Dataframe tht is solit in Train and Test dataset. The Training features are twitter messages, the lables are assigned categories. I set up a tokenizer (called <code>clean_text</code>) that keeps only relevant words and strips the messages down to the core information. The model including a grid search, that looks as follows:</p>

<pre><code>def build_model():
   pipeline = Pipeline([
        ('vectorizer', CountVectorizer(tokenizer=clean_text)),
        ('tfidf', TfidfTransformer()),
        ('clf', MultiOutputClassifier(
                RandomForestClassifier()
                ))      
        ])

    # parameters to grid search
    parameters = { 'vectorizer__max_features' : [50],#, 72, 144, 288, 576, 1152],
            'clf__estimator__n_estimators' : [100]}#, 100] }

    # initiating GridSearchCV method
    model = GridSearchCV(pipeline, param_grid=parameters, cv = 5)

    return model
</code></pre>

<p>The fitting works fine, as well as the evaluation.
Not I am not sure, if the model is set up correctly and if the features are the most used tokens in the messsages (in the above case 50) or if there is an error.</p>

<p>Now the question:
Is there a way to print the 50 features and see if they look right?</p>

<p>Best
Felix</p>
",Training and Model Evaluation,machine learning extract feature pipeline totaly new field currently stuck want dataframe tht solit train test dataset training feature twitter message lables assigned category set tokenizer called keep relevant word strip message core information model including grid search look follows fitting work fine well evaluation sure model set correctly feature used token messsages case error question way print feature see look right best felix
what is the meaning of heads in spacy training data?,"<p>I'm trying to train a model on my own data and I'm using Spacy library. </p>

<p>But I'm confused about ""# index of token head"" in a code example.</p>

<p>what exactly heads mean here? </p>

<pre><code># training data: texts, heads and dependency labels
# for no relation, we simply chose an arbitrary dependency label, e.g. '-'
TRAIN_DATA = [
    (
        ""find a cafe with great wifi"",
        {
            ""heads"": [0, 2, 0, 5, 5, 2],  # index of token head
            ""deps"": [""ROOT"", ""-"", ""PLACE"", ""-"", ""QUALITY"", ""ATTRIBUTE""],
        },
    )
</code></pre>
",Training and Model Evaluation,meaning head spacy training data trying train model data using spacy library confused index token head code example exactly head mean
Problem with accuracy and loss in deep learning,"<p>I'm doing a self-study, learning to use deep learning for text classification.
I use the Bi-LSTM model and I tried to change parameters with 10 epochs to compare results.</p>

<p>The dataset contains approximately 35k rows, in total with more than 1,200k tokenized words. The dataset is labeled with 2 classes. After tuning the parameters, I did the stratified 10-fold cross validation, 8 scenarios gave normal results, but these 2 scenarios happened.</p>

<p>-- The fist result:
it seems like the model couldn't improve the accuracy. What might be the cause of problem?
<a href=""https://i.sstatic.net/fz3nv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fz3nv.png"" alt=""enter image description here""></a>
<a href=""https://i.sstatic.net/P2JmV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/P2JmV.png"" alt=""enter image description here""></a></p>

<p>-- the second result:
this might be worse than the first one, in this case, does it has something to do with the network’s weights update?
<a href=""https://i.sstatic.net/JENBt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JENBt.png"" alt=""enter image description here""></a>
<a href=""https://i.sstatic.net/JUwzQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JUwzQ.png"" alt=""enter image description here""></a></p>

<p>Correct me if I'm worng.
I'm quite new with deep learning techniques and not familiar with some technical terms, so excuse me if I used some wrong words.
Thanks in advance.</p>
",Training and Model Evaluation,problem accuracy loss deep learning self study learning use deep learning text classification use bi lstm model tried change parameter epoch compare result dataset contains approximately k row total k tokenized word dataset labeled class tuning parameter stratified fold cross validation scenario gave normal result scenario happened fist result seems like model improve accuracy might cause problem second result might worse first one case doe ha something network weight update correct worng quite new deep learning technique familiar technical term excuse used wrong word thanks advance
"When creating a new feature of similarity in ham vs spam case, should I include the similarity of spam with itself in the average of samp similarity?","<p>I want to improve my model by adding a new feature column to my data, the data of ham and spam texts.
I have already created the square Cosine similarity matrix between all the texts, the diagonal of the matrix are 1s = cos(0).</p>

<p>I extract all the spam text index in the training data, and I created the column of similarity, for each cell in the column, I add the individual similarity between this text and all the spam and average them.</p>

<p><strong>My question: for the text that is ham, it makes sense to do above. But for the text are spam, when calculating the similarity, should I exclude the similarity between itself? Will it causes data leakage?</strong></p>

<p>If we have n text of sample size, I represent the similarity value of ham_1 as 
 average(ham_1~spam_1, ham_1~spam_2, ..., ham_1~spam_n)</p>

<p>My question is:</p>

<p>For spam text spam_5, similarity value = average(spam_5~spam_1, spam_5~spam_2, ..., spam_5~spam_5, ..., spam_5~spam_n)</p>

<p>Or</p>

<p>For spam text spam_5, similarity value = average(spam_5~spam_1, spam_5~spam_2, ..., <s>spam_5~spam_5</s>, ..., spam_5~spam_n)</p>
",Training and Model Evaluation,creating new feature similarity ham v spam case include similarity spam average samp similarity want improve model adding new feature column data data ham spam text already created square cosine similarity matrix text diagonal matrix co extract spam text index training data created column similarity cell column add individual similarity text spam average question text ham make sense text spam calculating similarity exclude similarity cause data leakage n text sample size represent similarity value ham average ham spam ham spam ham spam n question spam text spam similarity value average spam spam spam spam spam spam spam spam n spam text spam similarity value average spam spam spam spam spam spam spam spam n
Automatic Topic Labeling Evaluation metric,"<p>I am trying to do a topic labeling problem on a large dataset of research papers. The idea is that I can give each paper a few relevant labels.</p>

<p>I have 2 questions.</p>

<p>I know you can do topic modeling in a variety of ways like using LDA and NMF, but what can you do to later extract possible labels from those topics?</p>

<p>Also, assuming I have extracted a bunch of labels, how can I mathematically estimate their accuracy? Is there some kind of metric available that can determine say, the variance of the information explained by a label in a document, or something along those lines? How would I evaluate my labels without a large group of humans doing qualitative analysis?</p>
",Training and Model Evaluation,automatic topic labeling evaluation metric trying topic labeling problem large dataset research paper idea give paper relevant label question know topic modeling variety way like using lda nmf later extract possible label topic also assuming extracted bunch label mathematically estimate accuracy kind metric available determine say variance information explained label document something along line would evaluate label without large group human qualitative analysis
"How to print f-score, precison and recall using scikit learn in python?","<p>I want to calculate and print many metrics (recall, f-score, accuracy) using sickit learn in python.
I am doing nlp, basicaly y_pred and y_test are list of words, I vectorise them with <code>pipe_vect.transform</code>, and i use sklearn.metrics to print metrics.</p>

<p>My code : </p>

<pre><code>from sklearn.metrics import precision_recall_fscore_support as score
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.15, random_state=42)
x_train, x_valid, y_train, y_valid = train_test_split(x_train,y_train, test_size=0.1, random_state=42)
x_train = np.array(x_train)
y_train = np.array(y_train)
x_valid = np.array(x_valid)
y_valid = np.array(y_valid)
y_test = np.array(y_test)
x_test = np.array(x_test) 
x_train = np.concatenate((x_train, x_valid))
 y_train = np.concatenate((y_train, y_valid))
    model = RandomForestClassifier(n_estimators=int(clf_params['n_estimators']),
                                   max_features=clf_params['max_features'])
model.fit(pipe_vect.transform(x_train), y_train)
x_test_vect = pipe_vect.transform(x_test)
y_pred = model.predict_proba(x_test_vect)
 #y_pred.shape # (417,1)
y_pred = y_pred.flatten()
y_pred.shape # (417,)
print('y_pred',  y_pred)
print('y_pred dimension: ', y_pred.shape) #y_pred dimension:  (417,)
print('y_test dimension: ', y_test.shape) #y_test dimension:  (417,)
precision, recall, fscore, support = score(y_test, y_pred)
print('precision: {}'.format(precision))
print('recall: {}'.format(recall))
print('fscore: {}'.format(fscore))
print('support: {}'.format(support))
</code></pre>

<p>print : </p>

<pre><code>y_pred dimension:  (417,)
y_test dimension:  (417,)
precision: [0. 0.]
recall: [0. 0.]
fscore: [0. 0.]
support: [  0. 417.]
</code></pre>

<p>I don't understand why my print are 0.</p>
",Training and Model Evaluation,print f score precison recall using scikit learn python want calculate print many metric recall f score accuracy using sickit learn python nlp basicaly pred test list word vectorise use sklearn metric print metric code print understand print
Keras model - How to get accuracy - NLP Task,"<p>I have made a Keras model for an NLP Multi-class classification problem. The data consists of titles and tags. I have trained the model on the titles to predict tags. I had converted the tags into one-hot using sklearn.preprocessing LabelEncoder, OneHotEncoder. </p>

<p><strong>OneHot Encoding</strong></p>

<pre><code>def onehot(df):
    values = array(df)
    # integer encode
    label_encoder = LabelEncoder()
    integer_encoded = label_encoder.fit_transform(values)
    # binary encode
    onehot_encoder = OneHotEncoder(sparse=False)
    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)
    return label_encoder, onehot_encoded
</code></pre>

<p>I have used categorical_crossentropy and adam for my model. Here is the code for the model.</p>

<p><strong>Keras CNN Model</strong></p>

<pre><code>def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):
embedding_layer = Embedding(num_words, embedding_dim, weights=[embeddings],
                           input_length = max_sequence_length,
                           trainable = False)
sequence_input = Input(shape = (max_sequence_length,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)
convs = []
filter_sizes = [2,3,4,5,6]
for filter_size in filter_sizes:
    l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)
    l_pool = GlobalMaxPooling1D()(l_conv)
    convs.append(l_pool)

l_merge = concatenate(convs, axis=1)  
x = Dropout(0.1)(l_merge)  
x = Dense(128, activation='relu')(x)
x = Dropout(0.2)(x)

preds = Dense(labels_index, activation='sigmoid')(x)
model = Model(sequence_input, preds)    
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])    
model.summary()    
return model
</code></pre>

<p><strong>Prediction</strong></p>

<pre><code>predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)  
</code></pre>

<p>I get an array like this from predictions</p>

<pre><code>print(predictions[5, :])
Output:
array([8.8067267e-08, 5.1040554e-15, 1.9745098e-16, ..., 8.0959568e-17,
       2.1070798e-17, 1.1202571e-18], dtype=float32)
</code></pre>

<p>What I understand is that these are probabilities or confidence score that the following sentence belongs to this tag.</p>

<p><strong>How do I convert the predicted arrays into tags so I can compare it with the test dataset tags to accuracy?</strong></p>
",Training and Model Evaluation,kera model get accuracy nlp task made kera model nlp multi class classification problem data consists title tag trained model title predict tag converted tag one hot using sklearn preprocessing labelencoder onehotencoder onehot encoding used categorical crossentropy adam model code model kera cnn model prediction get array like prediction understand probability confidence score following sentence belongs tag convert predicted array tag compare test dataset tag accuracy
"RNN and CNN-RNN won&#39;t train correctly, always predict one class","<p>I am currently developing a model to detect emotion from text using deep learning algorithms. I have a relatively small labelled dataset(~7500) with 7 different emotions as classes. I developed a CNN and achieved an accuracy of ~63% but when I tried to apply a RNN, using LSTM, and a CNN-RNN, also using LSTM, they just don't seem to train properly at all and always end up predicting the same class. I believe my models to be fundamentally sound but with some mistakes with the parameters. I have the dataset split up into 85% for training, with a further 20% of that for validation, and the remaining 15% for testing. My embedding matrix is developed using the word representations from the Google News word2vec and the word index is developed using keras Tokenizer.</p>

<p>Dataset breakdown:</p>

<p>Emotion          </p>

<p>anger          1086</p>

<p>disgust        1074</p>

<p>fear           1086</p>

<p>guilt          1062</p>

<p>joy            1089</p>

<p>sadness        1080</p>

<p>shame          1058</p>

<p>CNN implementation    </p>

<pre><code>def make_model(kernel_sizes, num_filters, dropout, hidden_units):

    submodels = []
    for kernel_size in kernel_sizes:
        submodel = Sequential()

        submodel.add(Embedding(input_dim = input_dim,
                            output_dim   = output_dim,
                            weights      = [embedding_matrix],
                            input_length = max_len,
                            trainable    = True))

        submodel.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='same',activation='relu',strides=1))
        submodel.add(GlobalMaxPooling1D())
        submodels.append(submodel)

    submodel_outputs = [model.output for model in submodels]    
    submodel_inputs = [model.input for model in submodels]

    merged = Concatenate(axis=1)(submodel_outputs)
    x = Dropout(dropout)(merged)

    if(hidden_units &gt; 0):
        x = Dense(hidden_units, activation='relu')(x)
        x = Dropout(dropout)(x)

    x = Dense(7,activation='softmax', kernel_initializer=""uniform"")(x)
    out = Activation('sigmoid')(x)

    model = Model(submodel_inputs, out)
    model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])

    return model
</code></pre>

<pre><code>def fit_model(model, kernel_sizes, num_epochs, batch_size, x_train, y_train):

    x_train = [x_train]*len(kernel_sizes)

    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_split=0.2)

    return history
</code></pre>

<pre><code>kernel_sizes  = [2,6]
num_filters   = 100
dropout       = 0.6
num_hidden    = 270
callbacks     = callbacks_list
num_epochs    = 15
batch_size = 64
model = make_model(kernel_sizes, num_filters, dropout, num_hidden)
print(model.summary())
history = fit_model(model, kernel_sizes, num_epochs, batch_size, x_train, y_train)
</code></pre>

<p>Model: ""model_1""</p>

<hr>

<h1>Layer (type)                    Output Shape         Param #     Connected to</h1>

<p>embedding_1_input (InputLayer)  (None, 179)          0                                            </p>

<hr>

<p>embedding_2_input (InputLayer)  (None, 179)          0                                            </p>

<hr>

<p>embedding_1 (Embedding)         (None, 179, 300)     2729400     embedding_1_input[0][0]          </p>

<hr>

<p>embedding_2 (Embedding)         (None, 179, 300)     2729400     embedding_2_input[0][0]          </p>

<hr>

<p>conv1d_1 (Conv1D)               (None, 179, 100)     60100       embedding_1[0][0]                </p>

<hr>

<p>conv1d_2 (Conv1D)               (None, 179, 100)     180100      embedding_2[0][0]                </p>

<hr>

<p>global_max_pooling1d_1 (GlobalM (None, 100)          0           conv1d_1[0][0]                   </p>

<hr>

<p>global_max_pooling1d_2 (GlobalM (None, 100)          0           conv1d_2[0][0]                   </p>

<hr>

<p>concatenate_1 (Concatenate)     (None, 200)          0           global_max_pooling1d_1[0][0]<br>
                                                                 global_max_pooling1d_2[0][0]     </p>

<hr>

<p>dropout_1 (Dropout)             (None, 200)          0           concatenate_1[0][0]              </p>

<hr>

<p>dense_1 (Dense)                 (None, 270)          54270       dropout_1[0][0]                  </p>

<hr>

<p>dropout_2 (Dropout)             (None, 270)          0           dense_1[0][0]                    </p>

<hr>

<p>dense_2 (Dense)                 (None, 7)            1897        dropout_2[0][0]                  </p>

<hr>

<h1>activation_1 (Activation)       (None, 7)            0           dense_2[0][0]</h1>

<p>Total params: 5,755,167
Trainable params: 5,755,167
Non-trainable params: 0</p>

<hr>

<p><a href=""https://i.sstatic.net/v1GeS.png"" rel=""nofollow noreferrer"">Training and Validation results for CNN</a></p>

<p><a href=""https://i.sstatic.net/e88si.png"" rel=""nofollow noreferrer"">CNN confusion matrix</a></p>

<hr>

<p>RNN Implementation</p>

<pre><code>def make_model(lstm_units, dropout, hidden_units):

    model = Sequential()   

    model.add(Embedding(input_dim = input_dim,
                        output_dim   = output_dim,
                        weights      = [embedding_matrix],
                        input_length = max_len,
                        trainable    = False))

    model.add(LSTM(lstm_units))

    model.add(Dropout(dropout))

    if(hidden_units &gt; 0):
        model.add(Dense(hidden_units, activation='elu'))
        model.add(Dropout(dropout))

    model.add(Dense(7,activation='softmax', kernel_initializer=""uniform""))
    model.add(Activation('sigmoid'))

    model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])

    return model
</code></pre>

<pre><code>lstm_units = 120
dropout = 0.5
hidden_units = 550
callbacks = [tensorboard, early]
num_epochs = 20
batch_size = 60

model = make_model(lstm_units, dropout, hidden_units)
print(model.summary())
history = fit_model(model, num_epochs, batch_size, x_train, y_train)
</code></pre>

<p>Model: ""sequential_6""</p>

<hr>

<h1>Layer (type)                 Output Shape              Param #</h1>

<p>embedding_6 (Embedding)      (None, 179, 300)          2729400   </p>

<hr>

<p>lstm_8 (LSTM)                (None, 120)               202080    </p>

<hr>

<p>dropout_5 (Dropout)          (None, 120)               0         </p>

<hr>

<p>dense_6 (Dense)              (None, 550)               66550     </p>

<hr>

<p>dropout_6 (Dropout)          (None, 550)               0         </p>

<hr>

<p>dense_7 (Dense)              (None, 7)                 3857      </p>

<hr>

<h1>activation_3 (Activation)    (None, 7)                 0</h1>

<p>Total params: 3,001,887
Trainable params: 272,487
Non-trainable params: 2,729,400</p>

<hr>

<p><a href=""https://i.sstatic.net/d7XIF.png"" rel=""nofollow noreferrer"">RNN training and validation scores</a></p>

<p><a href=""https://i.sstatic.net/l2CQe.png"" rel=""nofollow noreferrer"">RNN confusion matrix</a></p>

<hr>

<p>CNN-RNN implementation</p>

<pre><code>def make_model(kernel_sizes, num_filters, dropout, hidden_units, lstm_units):

    submodels = []
    for kernel_size in kernel_sizes:
        submodel = Sequential()

        submodel.add(Embedding(input_dim = input_dim,
                            output_dim   = output_dim,
                            weights      = [embedding_matrix],
                            input_length = max_len,
                            trainable    = True))

        submodel.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='same',activation='relu',strides=1))
        submodel.add(MaxPooling1D(pool_size=2, strides = 2))
        submodel.add(Dropout(dropout))
        submodel.add(LSTM(lstm_units)) 
        submodels.append(submodel)

    submodel_outputs = [model.output for model in submodels]    
    submodel_inputs = [model.input for model in submodels]

    merged = Concatenate(axis=1)(submodel_outputs)
    x = Dropout(dropout)(merged)

    if(hidden_units &gt; 0):
        x = Dense(hidden_units, activation='relu')(x)
        x = Dropout(dropout)(x)

    x = Dense(7,activation='softmax', kernel_initializer=""uniform"")(x)
    out = Activation('sigmoid')(x)

    model = Model(submodel_inputs, out)
    model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])

    return model
</code></pre>

<pre><code>kernel_sizes  = [2,3,6]
num_filters   = 100
dropout       = 0.6
num_hidden    = 270
lstm_units = 80
callbacks     = [tensorboard, early]
num_epochs    = 20
batch_size = 64

model = make_model(kernel_sizes, num_filters, dropout, num_hidden, lstm_units)
print(model.summary())
history = fit_model(model, kernel_sizes, num_epochs, batch_size, x_train, y_train)
</code></pre>

<p>Model: ""model_2""</p>

<hr>

<h1>Layer (type)                    Output Shape         Param #     Connected to</h1>

<p>embedding_8_input (InputLayer)  (None, 179)          0                                            </p>

<hr>

<p>embedding_9_input (InputLayer)  (None, 179)          0                                            </p>

<hr>

<p>embedding_10_input (InputLayer) (None, 179)          0                                            </p>

<hr>

<p>embedding_8 (Embedding)         (None, 179, 300)     2729400     embedding_8_input[0][0]          </p>

<hr>

<p>embedding_9 (Embedding)         (None, 179, 300)     2729400     embedding_9_input[0][0]          </p>

<hr>

<p>embedding_10 (Embedding)        (None, 179, 300)     2729400     embedding_10_input[0][0]         </p>

<hr>

<p>conv1d_8 (Conv1D)               (None, 179, 100)     60100       embedding_8[0][0]                </p>

<hr>

<p>conv1d_9 (Conv1D)               (None, 179, 100)     90100       embedding_9[0][0]                </p>

<hr>

<p>conv1d_10 (Conv1D)              (None, 179, 100)     180100      embedding_10[0][0]               </p>

<hr>

<p>max_pooling1d_7 (MaxPooling1D)  (None, 89, 100)      0           conv1d_8[0][0]                   </p>

<hr>

<p>max_pooling1d_8 (MaxPooling1D)  (None, 89, 100)      0           conv1d_9[0][0]                   </p>

<hr>

<p>max_pooling1d_9 (MaxPooling1D)  (None, 89, 100)      0           conv1d_10[0][0]                  </p>

<hr>

<p>dropout_9 (Dropout)             (None, 89, 100)      0           max_pooling1d_7[0][0]            </p>

<hr>

<p>dropout_10 (Dropout)            (None, 89, 100)      0           max_pooling1d_8[0][0]            </p>

<hr>

<p>dropout_11 (Dropout)            (None, 89, 100)      0           max_pooling1d_9[0][0]            </p>

<hr>

<p>lstm_2 (LSTM)                   (None, 80)           57920       dropout_9[0][0]                  </p>

<hr>

<p>lstm_3 (LSTM)                   (None, 80)           57920       dropout_10[0][0]                 </p>

<hr>

<p>lstm_4 (LSTM)                   (None, 80)           57920       dropout_11[0][0]                 </p>

<hr>

<p>concatenate_3 (Concatenate)     (None, 240)          0           lstm_2[0][0]<br>
                                                                 lstm_3[0][0]<br>
                                                                 lstm_4[0][0]                     </p>

<hr>

<p>dropout_12 (Dropout)            (None, 240)          0           concatenate_3[0][0]              </p>

<hr>

<p>dense_3 (Dense)                 (None, 270)          65070       dropout_12[0][0]                 </p>

<hr>

<p>dropout_13 (Dropout)            (None, 270)          0           dense_3[0][0]                    </p>

<hr>

<p>dense_4 (Dense)                 (None, 7)            1897        dropout_13[0][0]                 </p>

<hr>

<h1>activation_2 (Activation)       (None, 7)            0           dense_4[0][0]</h1>

<p>Total params: 8,759,227
Trainable params: 8,759,227
Non-trainable params: 0</p>

<hr>

<p><a href=""https://i.sstatic.net/fFTYk.png"" rel=""nofollow noreferrer"">CNN-RNN training and validation scores</a>
<a href=""https://i.sstatic.net/nUet2.png"" rel=""nofollow noreferrer"">CNN-RNN confusion matrix</a></p>

<p>I understand there is no magic formula to neural networks and no one size fits all approach, I am just looking for some guidance in the areas which I may have made mistakes in when implementing the CNN-RNN and RNN.</p>

<p>Apologies in advance for any formatting errors as this is my first question asked. If there is any other info required please let me know.</p>

<p>Thanks very much.</p>
",Training and Model Evaluation,rnn cnn rnn train correctly always predict one class currently developing model detect emotion text using deep learning algorithm relatively small labelled dataset different emotion class developed cnn achieved accuracy tried apply rnn using lstm cnn rnn also using lstm seem train properly always end predicting class believe model fundamentally sound mistake parameter dataset split training validation remaining testing embedding matrix developed using word representation google news word vec word index developed using kera tokenizer dataset emotion anger disgust fear guilt joy sadness shame cnn implementation model model layer type output shape param connected embedding input inputlayer none embedding input inputlayer none embedding embedding none embedding input embedding embedding none embedding input conv conv none embedding conv conv none embedding global max pooling globalm none conv global max pooling globalm none conv concatenate concatenate none global max pooling global max pooling dropout dropout none concatenate dense dense none dropout dropout dropout none dense dense dense none dropout activation activation none dense total params trainable params non trainable params training validation result cnn cnn confusion matrix rnn implementation model sequential layer type output shape param embedding embedding none lstm lstm none dropout dropout none dense dense none dropout dropout none dense dense none activation activation none total params trainable params non trainable params rnn training validation score rnn confusion matrix cnn rnn implementation model model layer type output shape param connected embedding input inputlayer none embedding input inputlayer none embedding input inputlayer none embedding embedding none embedding input embedding embedding none embedding input embedding embedding none embedding input conv conv none embedding conv conv none embedding conv conv none embedding max pooling maxpooling none conv max pooling maxpooling none conv max pooling maxpooling none conv dropout dropout none max pooling dropout dropout none max pooling dropout dropout none max pooling lstm lstm none dropout lstm lstm none dropout lstm lstm none dropout concatenate concatenate none lstm lstm lstm dropout dropout none concatenate dense dense none dropout dropout dropout none dense dense dense none dropout activation activation none dense total params trainable params non trainable params cnn rnn training validation score cnn rnn confusion matrix understand magic formula neural network one size fit approach looking guidance area may made mistake implementing cnn rnn rnn apology advance formatting error first question asked info required please let know thanks much
Annotate the own dataset to convert into forge&#39;s format,"<p>I am using <a href=""https://github.com/mailgun/talon"" rel=""nofollow noreferrer"">talon's email_extractor</a> to extract signature from the mail. But it doesn't seems to work properly with my data. I wanted to train the talon's classifier to my dataset but I need to change the dataset to <a href=""https://github.com/mailgun/forge"" rel=""nofollow noreferrer"">forge's format</a>. How do I change the my data to the following forge's format:</p>

<pre><code>#sig#--
#sig#Mike Smith
#sig#555-243-0623
</code></pre>

<p>I have dataset from <a href=""https://developer.zendesk.com/"" rel=""nofollow noreferrer"">zendesk's api</a></p>

<p>EDIT-1:</p>

<p>My dataset is in this format :</p>

<pre><code>Hi there!\n\nOur new project description is as follows:\n\nABC
is a .. company committed to producing works both new 
and\nrediscovered . We b 
performances\nare exactly alike; with every ads, new 
thoughtsfeelings......
**\n\n \n\nBest, \n\XYZ\n\n     \n\XYZ, xxx 
xxx, xxx, xxx, xxx\n\nUnsubscribe (https://media.XYZ.org/hs/manage- 
preferences/unsubscribe-all? 
d=Vnk5T44_PyVnW1b1YkV4mCt9cW3Z_sBZ3zhsYkW3Fbt723zd- 
J2W3P3Q7w3ZsjYmW3_fWSh6pjXx2VmWcP5724cW0N6P6s3r25DrQW7ZSHL  
sFPu8fNOrW7ByvX2ISZHyRP5Kjm4vKXy60COlsUqmvd1t9W- 
) Manage preferences (https://media.xyz.org/hs/manage/unsubscribe? 
d=Vnk5T44_PyVnW1b1YkV4mCt9cW3Z- 
J2W3P3Q7w3ZsjYmW3_fWSh6pjXx2VmWcP5724cW0N6P6s3r25DrQW7ZSHL- 
=hs_email&amp;utm_medium=email&amp;utm_content=756&amp;_hsenc=JegN- 
sFPu8fNOrW7ByvX2ISZHyRP5KjmW
)\n\n\n\n--\n\nTheABC Company\nxx-xxx-xxx | 
ABC@gmail.com | ABC.org 
</code></pre>

<p>I want :</p>

<pre><code>#sig#Best
#sig#....
#sig#The ABC Company
#sig#ABC@gmail.com | ABC.org
</code></pre>
",Training and Model Evaluation,annotate dataset convert forge format using talon email extractor extract signature mail seems work properly data wanted train talon classifier dataset need change dataset forge format change data following forge format dataset zendesk api edit dataset format want
How to understand byte pair encoding?,"<p>I read a lot of tutorial about BPE but I am still confuse how it works.</p>

<p>for example.
In a tutorial online, they said the folowing : </p>

<p>Algorithm</p>

<p>Prepare a large enough training data (i.e. corpus)</p>

<p>Define a desired subword vocabulary size</p>

<p>Split word to sequence of characters and appending suffix “” to end of</p>

<p>word with word frequency. So the basic unit is character in this stage. For example, the frequency of “low” is 5, then we rephrase it to “l o w ”: 5
    Generating a new subword according to the high frequency occurrence.
    Repeating step 4 until reaching subword vocabulary size which is defined in step 2 or the next highest frequency pair is 1.</p>

<p>Taking “low: 5”, “lower: 2”, “newest: 6” and “widest: 3” as an example, the highest frequency subword pair is e and s. It is because we get 6 count from newest and 3 count from widest. Then new subword (es) is formed and it will become a candidate in next iteration.</p>

<p>In the second iteration, the next high frequency subword pair is es (generated from previous iteration )and t. It is because we get 6count 
from newest and 3 count from widest.</p>

<p>I do not understand why low is 5   and lower is  2:</p>

<p>does this meand  l , o, w , lo, ow +  = 6 and then lower  equal two but why is not e, r, er which gives three ?</p>
",Training and Model Evaluation,understand byte pair encoding read lot tutorial bpe still confuse work example tutorial online said folowing algorithm prepare large enough training data e corpus define desired subword vocabulary size split word sequence character appending suffix end word word frequency basic unit character stage example frequency low rephrase l w generating new subword according high frequency occurrence repeating step reaching subword vocabulary size defined step next highest frequency pair taking low lower newest widest example highest frequency subword pair e get count newest count widest new subword e formed become candidate next iteration second iteration next high frequency subword pair e generated previous iteration get count newest count widest understand low lower doe meand l w lo ow lower equal two e r er give three
Neural Network to count elements in text,"<h2>Problem</h2>

<p>I'm trying to create a neural network in keras(though i'm open to other suggestions) that would take the text (part of doctor's examination of a patient) and count the number of systems that were examined</p>

<h2>Data Example</h2>

<p><strong>General</strong>: Alert and oriented, wel nourished.</p>

<p><strong>Eye</strong>: EOMI, normal conjunctiva.</p>

<p><strong>HENT</strong>: Normocephalic, normal hearing, moist oral mucosa, no scleral icterus,</p>

<p><strong>Neck</strong>: Supple, non-tender, no lymphadenopathy.</p>

<p>Breast Exam: Not clinically necessary</p>

<p><strong>Musculoskeletal</strong>: Norma range of motion and strength X3 bilateral upper extremity, left lower extremity, decreased range of motion </p>

<p><strong>Skin</strong>: Skin is warm, dry and pink, no rashes or lesions, no ecchymosis.</p>

<p><strong>Neurologic</strong>: Awake, alert, and oriented X3, no focal deficits.</p>

<p><strong>Psychiatric</strong>: Cooperative, appropriate mood and affect.</p>

<p>The correct result for this example is 8 (by the count of systems/organs i've marked in bold) , excluding Breast Exam which was mentioned but not done</p>

<h2>Training data and results</h2>

<p>I have 50k labeled data points (the text + system counts)
The data was cleaned of items that do not contribute to counting systems and was lemmatized so i'd say the quality of inputs is rather good.</p>

<p>So far the best results i got is around 40% accuracy , which is not good enough (ideally, i'd like to get to 90+% accuracy).</p>

<h2>Question</h2>

<p>My question is twofold:</p>

<ol>
<li><p>If overall i'm moving in right direction, what prevents the model from giving better results?</p></li>
<li><p>If there is a better way to approach such task, what should i look at?</p></li>
</ol>

<p>Thank you</p>

<h2>What i have tried</h2>

<p>I have tried a few approaches to build the model:</p>

<ol>
<li>LSTM model with regression neuron on top(code below) - it tends to favor one specific output given enough epochs to learn - i've studied the existing questions on that and tried the suggestions - did not help (like changing the batch size, using class_weights, lowering the LR etc - did not help)</li>
<li>Replaced LSTM with Flatten + Dense layer(s) . The resulting accuracy was never higher than 40% , the result was +-1 of the correct result</li>
<li>Tried to see it as a classification problem , so LSTM followed by softmax activation. Same results as in #1 - highly favors one class</li>
<li>Removed embeddings and just passed tokenized and padded texts to dense layers - did not help</li>
<li>Played with hyperparameters for all the configurations above: embedding dims, hidden unit counts, batch size, padding length etc - mostly did not help</li>
<li>I considered just having a dictionary of systems and finding their occurrences in a text. But it would not be that easy as the system names could be misspelled, abbreviated, replaced with a synonym (like 'Respiratory' = 'lungs' = 'chest' etc) and we need to count out those systems that are mentioned but not reviewed and the wording for such cases can vary</li>
</ol>

<p>Observations</p>

<ol>
<li>In an LSTM attempts , i monitored the validation loss between batches during the training  and it seems like it is not gradually decreasing but rather jumps  - like if the model is predicting a '4' for every sample and at some point decides to predict '9', which overall gives better validation score </li>
<li>Spatial dropout after the Embedding gave like +10% accuracy</li>
<li>Lowering the padding size helped with accuracy (maybe we get some sort of vanishing gradient problem when there is too many zeros in padded sequences)</li>
</ol>

<h2>Code</h2>

<pre class=""lang-py prettyprint-override""><code>X_train, X_test_initial, y_train, y_test = train_test_split(training_data['text'], training_data['num_exam'], test_size=0.02,
                                                        random_state=42)

    tokenizer = Tokenizer(num_words=5000,lower=True)
    tokenizer.fit_on_texts(training_data['text'])

    vocab_size = len(tokenizer.word_index) + 1

    maxlen = 40
    X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), padding='post', maxlen=maxlen)
    X_test = pad_sequences(tokenizer.texts_to_sequences(X_test_initial), padding='post', maxlen=maxlen)

    embedding_dim = 50
    hidden_size = 50
    model = Sequential()
    model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))
    model.add(SpatialDropout1D(0.5))
    model.add(LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2))
    model.add(Dense(1, activation='relu'))
    model.compile(optimizer='adam',
                  loss='mean_squared_error',
                  metrics=['mse'])
    model.summary()

    history = model.fit(X_train, y_train, shuffle=False,
                        epochs=20, batch_size=128, verbose=1,
                        validation_split=0.02)
</code></pre>
",Training and Model Evaluation,neural network count element text problem trying create neural network kera though open suggestion would take text part doctor examination patient count number system examined data example general alert oriented wel nourished eye eomi normal conjunctiva hent normocephalic normal hearing moist mucosa scleral icterus neck supple non tender lymphadenopathy breast exam clinically necessary musculoskeletal norma range motion strength x bilateral upper extremity left lower extremity decreased range motion skin skin warm dry pink rash lesion ecchymosis neurologic awake alert oriented x focal deficit psychiatric cooperative appropriate mood affect correct result example count system organ marked bold excluding breast exam wa mentioned done training data result k labeled data point text system count data wa cleaned item contribute counting system wa lemmatized say quality input rather good far best result got around accuracy good enough ideally like get accuracy question question overall moving right direction prevents model giving better result better way approach task look thank tried tried approach build model lstm model regression neuron top code tends favor one specific output given enough epoch learn studied existing question tried suggestion help like changing batch size using class weight lowering lr etc help replaced lstm flatten dense layer resulting accuracy wa never higher result wa correct result tried see classification problem lstm followed softmax activation result highly favor one class removed embeddings passed tokenized padded text dense layer help played hyperparameters configuration embedding dims hidden unit count batch size padding length etc mostly help considered dictionary system finding occurrence text would easy system name could misspelled abbreviated replaced synonym like respiratory lung chest etc need count system mentioned reviewed wording case vary observation lstm attempt monitored validation loss batch training seems like gradually decreasing rather jump like model predicting every sample point decides predict overall give better validation score spatial dropout embedding gave like accuracy lowering padding size helped accuracy maybe get sort vanishing gradient problem many zero padded sequence code
Conversational Data for building a chat bot,"<p>I am building a chat bot with rasa-nlu. I went through the tutorial and I have built a simple bot. However, I need lots of training data for building a chat bot that is able to book a taxi. So I need data to build a specific bot. </p>

<p>Is there a repository, or corpus, for booking a taxi? 
Or is there a way to generate this kind of dataset?</p>
",Training and Model Evaluation,conversational data building chat bot building chat bot rasa nlu went tutorial built simple bot however need lot training data building chat bot able book taxi need data build specific bot repository corpus booking taxi way generate kind dataset
Training times for Spacy Entity Linking model,"<p>I'm trying to train a Spacy Entity Linking model using Wikidata and Wikipedia, using the scripts in <a href=""https://github.com/explosion/spaCy/tree/master/bin/wiki_entity_linking"" rel=""noreferrer"">https://github.com/explosion/spaCy/tree/master/bin/wiki_entity_linking</a>. I've generated the KB and moved to training the model, but that is not done yet after more than a week. How long should that take normally? (I'm not using a GPU)</p>

<p>Alternatively, is there a pretrained Wikidata entity linking model I can use?</p>

<p>Thanks</p>
",Training and Model Evaluation,training time spacy entity linking model trying train spacy entity linking model using wikidata wikipedia using script generated kb moved training model done yet week long take normally using gpu alternatively pretrained wikidata entity linking model use thanks
LDA model generates different topics everytime i train on the same corpus,"<p>I am using python <code>gensim</code> to train an Latent Dirichlet Allocation (LDA) model from a small corpus of 231 sentences. However, each time i repeat the process, it generates different topics. </p>

<p><strong>Why does the same LDA parameters and corpus generate different topics everytime?</strong></p>

<p><strong>And how do i stabilize the topic generation?</strong></p>

<p>I'm using this corpus (<a href=""http://pastebin.com/WptkKVF0"">http://pastebin.com/WptkKVF0</a>) and this list of stopwords (<a href=""http://pastebin.com/LL7dqLcj"">http://pastebin.com/LL7dqLcj</a>) and here's my code:</p>

<pre><code>from gensim import corpora, models, similarities
from gensim.models import hdpmodel, ldamodel
from itertools import izip
from collections import defaultdict
import codecs, os, glob, math

stopwords = [i.strip() for i in codecs.open('stopmild','r','utf8').readlines() if i[0] != ""#"" and i != """"]

def generateTopics(corpus, dictionary):
    # Build LDA model using the above corpus
    lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=50)
    corpus_lda = lda[corpus]

    # Group topics with similar words together.
    tops = set(lda.show_topics(50))
    top_clusters = []
    for l in tops:
        top = []
        for t in l.split("" + ""):
            top.append((t.split(""*"")[0], t.split(""*"")[1]))
        top_clusters.append(top)

    # Generate word only topics
    top_wordonly = []
    for i in top_clusters:
        top_wordonly.append("":"".join([j[1] for j in i]))

    return lda, corpus_lda, top_clusters, top_wordonly

####################################################################### 

# Read textfile, build dictionary and bag-of-words corpus
documents = []
for line in codecs.open(""./europarl-mini2/map/coach.en-es.all"",""r"",""utf8""):
    lemma = line.split(""\t"")[3]
    documents.append(lemma)
texts = [[word for word in document.lower().split() if word not in stopwords]
             for document in documents]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda, corpus_lda, topic_clusters, topic_wordonly = generateTopics(corpus, dictionary)

for i in topic_wordonly:
    print i
</code></pre>
",Training and Model Evaluation,lda model generates different topic everytime train corpus using python train latent dirichlet allocation lda model small corpus sentence however time repeat process generates different topic doe lda parameter corpus generate different topic everytime stabilize topic generation using corpus
Why do we neglect &#39;not&#39; in NLP?,"<p>While cleaning the texts, we remove the words like 'the', 'this' and even 'not'.</p>

<p>As I have analyzed, the y_pred vector has a 1 for the second review because it neglected 'not'. If it had considered 'not' also, maybe the accuracy would have been increased.</p>

<p>Is there a way/advanced method to include such important role-playing words?</p>
",Training and Model Evaluation,neglect nlp cleaning text remove word like even analyzed pred vector ha second review neglected considered also maybe accuracy would increased way advanced method include important role playing word
Text summarization,"<p>I built a model for text summarization, I created a small document (text file) with its summary, then I trained the model on that, I created again the same type of document for test, the training and test documents are pretty similar but with different data.</p>

<p>For example, the training document contains: </p>

<pre class=""lang-none prettyprint-override""><code>name : train

family name : train
</code></pre>

<p>The test document:</p>

<pre class=""lang-none prettyprint-override""><code>name : test

family name : test
</code></pre>

<p>I was hoping that after training the model, it will remember the structure of the important sentences, after testing I've got an accuracy of 100%.</p>

<p>The problem is when I train the model on another document, the previous test gives lower accuracy, it's like it forgets the previous training.
here is my model:</p>

<pre><code>model = Sequential()
model.add(Embedding(200,64, input_length=max_sent_length))
model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) 
for i in range(0,len(xtrains)):
    model.fit(xtrains[i],ytrains[i], epochs=200, batch_size=64, shuffle=False)
</code></pre>

<p>I've searched about that and the answers I got is that refitting the model doesn't reset weights, so I was wondering why whenever I train the model on new documents I get lower accuracy for the previous tests, whereas at the beginning I received an accuracy of 100%.</p>

<p>How can I solve this problem?</p>
",Training and Model Evaluation,text summarization built model text summarization created small document text file summary trained model created type document test training test document pretty similar different data example training document contains test document wa hoping training model remember structure important sentence testing got accuracy problem train model another document previous test give lower accuracy like forgets previous training model searched answer got refitting model reset weight wa wondering whenever train model new document get lower accuracy previous test whereas beginning received accuracy solve problem
Unigram tagging in NLTK,"<p>Using <code>NLTK</code> Unigram Tagger, I am training sentences in <code>Brown Corpus</code></p>

<p>I try different <code>categories</code> and I get about the same value. The value is around <code>0.9328</code>... for each <code>categories</code> such as <code>fiction</code>, <code>romance</code> or <code>humor</code></p>

<pre><code>from nltk.corpus import brown


# Fiction    
brown_tagged_sents = brown.tagged_sents(categories='fiction')
brown_sents = brown.sents(categories='fiction')
unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)
unigram_tagger.evaluate(brown_tagged_sents)
&gt;&gt;&gt; 0.9415956079897209

# Romance
brown_tagged_sents = brown.tagged_sents(categories='romance')
brown_sents = brown.sents(categories='romance')
unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)
unigram_tagger.evaluate(brown_tagged_sents)
&gt;&gt;&gt; 0.9348490474422324
</code></pre>

<p>Why is it that the case? is it because they are from the same <code>corpus</code>? or are their <code>part-of-speech</code> tagging is the same?</p>
",Training and Model Evaluation,unigram tagging nltk using unigram tagger training sentence try different get value value around case tagging
How can I do performance evaluation for aspect-based opinion mining?,"<p>I have calculated some value for every aspect and identified its polarity using sentiwordnet.</p>

<p>For example, the movie is great. here movie is an aspect and I identified its value using some metric for example movie=1.5677 and polarity is positive. hereafter how can I identify the precision and recall?</p>
",Training and Model Evaluation,performance evaluation aspect based opinion mining calculated value every aspect identified polarity using sentiwordnet example movie great movie aspect identified value using metric example movie polarity positive hereafter identify precision recall
Training accuracy is less than validation accuracy,"<p>I have created a CNN model for classifying text data. Please help me interpret my result and tell me why is my training accuracy less than validation accuracy?</p>

<p>I have a total of 2619 Data, all of them are text data. There are two different classes. Here is a sample of my dataset. </p>

<p><a href=""https://i.sstatic.net/q3ukZ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/q3ukZ.jpg"" alt=""Dataset Sample""></a></p>

<p><strong>The validation set has 34 data. Rest of 2619 data are training data.</strong> </p>

<p>I have done RepeatedKfold cross-validation. Here is my code.</p>

<pre><code>from sklearn.model_selection import RepeatedKFold 
kf = RepeatedKFold(n_splits=75, n_repeats=1, random_state= 42) 

for train_index, test_index in kf.split(X,Y):
      #print(""Train:"", train_index, ""Validation:"",test_index)
      x_train, x_test = X.iloc[train_index], X.iloc[test_index] 
      y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]
</code></pre>

<p>I have used CNN. Here is my model.</p>

<pre><code>model = Sequential()
model.add(Embedding(2900,2 , input_length=1))
model.add(Conv1D(filters=2, kernel_size=3, kernel_regularizer=l2(0.0005 ), bias_regularizer=l2(0.0005 ), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(1, kernel_regularizer=l2(0.0005 ), bias_regularizer=l2(0.0005 ), activation='sigmoid'))
model.add(Dropout(0.25))
adam = optimizers.Adam(lr = 0.0005, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad = False)
model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])
print(model.summary())
history = model.fit(x_train, y_train, epochs=300,validation_data=(x_test, y_test), batch_size=128, shuffle=False)
# Final evaluation of the model
scores = model.evaluate(x_test, y_test, verbose=0)
print(""Accuracy: %.2f%%"" % (scores[1]*100))
</code></pre>

<p>And here is the result.</p>

<pre><code>Epoch 295/300
2585/2585 [==============================] - 0s 20us/step - loss: 1.6920 - acc: 0.7528 - val_loss: 0.5839 - val_acc: 0.8235
Epoch 296/300
2585/2585 [==============================] - 0s 20us/step - loss: 1.6532 - acc: 0.7617 - val_loss: 0.5836 - val_acc: 0.8235
Epoch 297/300
2585/2585 [==============================] - 0s 27us/step - loss: 1.5328 - acc: 0.7551 - val_loss: 0.5954 - val_acc: 0.8235
Epoch 298/300
2585/2585 [==============================] - 0s 20us/step - loss: 1.6289 - acc: 0.7524 - val_loss: 0.5897 - val_acc: 0.8235
Epoch 299/300
2585/2585 [==============================] - 0s 21us/step - loss: 1.7000 - acc: 0.7582 - val_loss: 0.5854 - val_acc: 0.8235
Epoch 300/300
2585/2585 [==============================] - 0s 25us/step - loss: 1.5475 - acc: 0.7451 - val_loss: 0.5934 - val_acc: 0.8235
Accuracy: 82.35%
</code></pre>

<p>Please help me with my problem. Thank you. </p>
",Training and Model Evaluation,training accuracy le validation accuracy created cnn model classifying text data please help interpret result tell training accuracy le validation accuracy total data text data two different class sample dataset validation set ha data rest data training data done repeatedkfold cross validation code used cnn model result please help problem thank
How to update vocabulary of pre-trained bert model while doing my own training task?,"<p>I am now working on a task of predicting masked word using BERT model. Unlike others, the answer needs to be chosen from specific options.</p>

<p>For instance:</p>

<pre><code>sentence: ""In my daily [MASKED], ...""
options: A.word1 B.word2 C.word3 D.word4
the predict word will be chosen from four given words
</code></pre>

<p>I use hugging face's BertForMaskedLM to do this task. This model will give me a probability matrix which representing every word's probability of appearing in the [MASK] and I just need to compare the probability of word in options to select the answser. </p>

<pre><code># Predict all tokens
with torch.no_grad():
    predictions = model(tokens_tensor, segments_tensors)
#predicted_index = torch.argmax(predictions[0, masked_index]).item()
#predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]
A = predictions[0, masked_pos][tokenizer.convert_tokens_to_ids([option1])]
B = predictions[0, masked_pos][tokenizer.convert_tokens_to_ids([option2])]
C = predictions[0, masked_pos][tokenizer.convert_tokens_to_ids([option3])]
D = predictions[0, masked_pos][tokenizer.convert_tokens_to_ids([option4])]
#And then select from ABCD
</code></pre>

<p>But the problem is:
    If the options are not in the “bert-vocabulary.txt”, the above method is not going to work since the output matrix does not give their probability. The same problem will also appear if the option is not a single word. </p>

<p>Should I update the vocabulary and how to do that? Or how to train the model 
 to add new words on the basis of pre-training?</p>
",Training and Model Evaluation,update vocabulary pre trained bert model training task working task predicting masked word using bert model unlike others answer need chosen specific option instance use hugging face bertformaskedlm task model give probability matrix representing every word probability appearing mask need compare probability word option select answser problem option bert vocabulary txt method going work since output matrix doe give probability problem also appear option single word update vocabulary train model add new word basis pre training
Should punctuation be removed from Rasa NLU training data?,"<p>In the NLU training data, should punctuation (commas, apostrophes, question marks, uppercase letters, etc.) on utterances for the intents be left as is, removed, or does it matter at all?</p>
",Training and Model Evaluation,punctuation removed rasa nlu training data nlu training data punctuation comma apostrophe question mark uppercase letter etc utterance intent left removed doe matter
document classification using keras - hierarchical sequence of digits,"<p>I'm currently approaching a classification problem with the following situation:</p>

<p>The labels are always 5 digits long, e.g.:</p>

<pre><code>99923 this is sample document one
56743 this is sample document two
...
</code></pre>

<p>where the first single digit stands for a certain category, every following digit for a subcategory and so on. </p>

<p>Currently I'm using Keras with the following settings:</p>

<pre><code>model = Sequential()
model.add(Dense(512, input_shape=(vocab_size,)))
model.add(Activation('relu'))
model.add(Dropout(0.3))
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.3))
model.add(Dense(num_labels))
model.add(Activation('softmax'))
model.summary()
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
</code></pre>

<p>Because my training data is limited (around 80k samples overall), I decided to use only the first digit to estimate the main-category and I got pretty good results with ~90% accuracy without any preprocessing which yet has to be done.</p>

<pre><code>5 - this is sample of maincategory 5
9 - this is sample of maincategory 9
...
</code></pre>

<p>Now I wanted to approach a level further and use two digits to predict the main-category and the first subcategory. That brought me to the problem that there is often not a single sample for e.g. the combination ""12"".</p>

<pre><code>51 - this is sample of maincategory 51
95 - this is sample of maincategory 95
...
</code></pre>

<p>I told Keras to only use the labels with at least 1 sample (knowing that this is crap) and got around 40 labels with an overall accuracy of 85% which seems to be pretty good given the fact that I've lost many samples.</p>

<p><strong>My question is</strong>:</p>

<p>Can this kind of prediction be done more easily/efficiently? If I let my ""two-digits"" model predict an unseen sample out of a category which hasn't been trained, I would run into the problem of fitting a sample into a wrong category...</p>

<p>Can I solve this prediction problem using Keras?</p>
",Training and Model Evaluation,document classification using kera hierarchical sequence digit currently approaching classification problem following situation label always digit long e g first single digit stand certain category every following digit subcategory currently using kera following setting training data limited around k sample overall decided use first digit estimate main category got pretty good result accuracy without preprocessing yet ha done wanted approach level use two digit predict main category first subcategory brought problem often single sample e g combination told kera use label least sample knowing crap got around label overall accuracy seems pretty good given fact lost many sample question kind prediction done easily efficiently let two digit model predict unseen sample category trained would run problem fitting sample wrong category solve prediction problem using kera
"Virtual Assistant -&gt; LUIS, QnA, Dispatcher best practice","<p>I have some question about some ""best practice"" for certain issues that we are facing using LUIS, QnA Maker, in particular for the Dispatcher: </p>

<p>1) Is there any best practice in case we have more that 15k utterances in the Dispatcher? That's looks like a limitation of the LUIS apps but the scalability of the model in the long run will be questionable.</p>

<p>2) Bing Spell Check for LUIS changes names and surnames for example, how to avoid this? I guess that Bing Spell Check is necessary when we are talking about ChatBots, since the typo are always behind the door, but using it for names is dangerous.</p>

<p>3) Cross validation is not supported out of the box, you would have split your data to folds with custom code (not difficult), use the command line to train and publish your model on your k-1/k folds, then send the k-fold utterances to the API one-by-one. Batch upload is only supported through the UI <a href=""https://cognitive.uservoice.com/forums/551524-language-understanding-luis/suggestions/20082157-add-api-to-batch-test-model"" rel=""nofollow noreferrer"">https://cognitive.uservoice.com/forums/551524-language-understanding-luis/suggestions/20082157-add-api-to-batch-test-model</a> and is limited to a test set of 1,000 utterances. If we use the one-by-one approach, we pay $1,50 per 1k transactions <a href=""https://azure.microsoft.com/de-de/pricing/details/cognitive-services/language-understanding-intelligent-services/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/de-de/pricing/details/cognitive-services/language-understanding-intelligent-services/</a> and this means to get cross-validation metrics for the 5 folds for example, we could be paying about 20$ for a single experiment with our current data, more if we add more data.</p>

<p>4) Model is a black box, which doesn't give us the ability to use custom features if needed.</p>
",Training and Model Evaluation,virtual assistant luis qna dispatcher best practice question best practice certain issue facing using luis qna maker particular dispatcher best practice case k utterance dispatcher look like limitation luis apps scalability model long run bing spell check luis change name surname example avoid guess bing spell check necessary talking chatbots since typo always behind door using name dangerous cross validation supported box would split data fold custom code difficult use command line train publish model k k fold send k fold utterance api one one batch upload supported ui limited test set utterance use one one approach pay per k transaction mean get cross validation metric fold example could paying single experiment current data add data model black box give u ability use custom feature needed
Doc2vec LSTM accuracy does not improve,"<pre><code>print(X.shape,y.shape)
(2591, 300) (2591,)
</code></pre>

<p>I have this shape X,y <br/> this is Doc2vec matrix <br/></p>

<p>I tried LogisticRegression,MLPClassifier this gives around 80% accuracy <br/></p>

<pre><code>I choose to build 3 hidden layers
EMBEDDING_DIM = 100
model = Sequential()
model.add(Embedding(unknown, EMBEDDING_DIM, input_length=300))
model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2 ))
model.add(Dense(len(dic), activation='softmax'))
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
</code></pre>

<p>but when I tried the LSTM the accuracy does not improve <br/></p>

<blockquote>
  <p>Epoch 1/100 2591/2591 [==============================] - 5s 2ms/step -
  loss: 1.5062 - accuracy: 0.3547 Epoch 2/100 2591/2591
  [==============================] - 4s 2ms/step - loss: 1.5079 -<br/>
  accuracy: 0.3551 Epoch 3/100 2591/2591
  [==============================] - 4s 2ms/step - loss: 1.5057 -<br/>
  accuracy: 0.3547 Epoch 4/100 2591/2591
  [==============================] - 4s 2ms/step - loss: 1.5089 -<br/>
  accuracy: 0.3547 Epoch 5/100 2591/2591
  [==============================] - 5s 2ms/step - loss: 1.5062 -
  accuracy: 0.3547 Epoch 6/100</p>
</blockquote>

<p>in this case what is the problem?</p>
",Training and Model Evaluation,doc vec lstm accuracy doe improve shape x doc vec matrix tried logisticregression mlpclassifier give around accuracy tried lstm accuracy doe improve epoch step loss accuracy epoch step loss accuracy epoch step loss accuracy epoch step loss accuracy epoch step loss accuracy epoch case problem
Pytorch dataloader for sentences,"<p>I have collected a small dataset for binary text classification and my goal is to train a model with the method proposed by <a href=""https://arxiv.org/pdf/1408.5882.pdf"" rel=""nofollow noreferrer"">Convolutional Neural Networks for Sentence Classification</a></p>

<p>I started my implementation by using the <code>torch.util.data.Dataset</code>. Essentially every sample in my dataset <code>my_data</code> looks like this (as example):</p>

<pre><code>{""words"":[0,1,2,3,4],""label"":1},
{""words"":[4,9,20,30,4,2,3,4,1],""label"":0}
</code></pre>

<p>Next I took a look at <a href=""https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"" rel=""nofollow noreferrer"">Writing custom dataloaders with pytorch</a>:
using:</p>

<pre><code>dataloader = DataLoader(my_data, batch_size=2,
                    shuffle=False, num_workers=4)
</code></pre>

<p>I would suspect that enumerating over a batch would yield something the following:</p>

<pre><code>{""words"":[[0,1,2,3,4],[4,9,20,30,4,2,3,4,1]],""labels"":[1,0]}
</code></pre>

<p>However it is more like this:</p>

<pre><code>{""words"":[[0,4],[1,9],[2,20],[3,30],[4,4]],""label"":[1,0]}
</code></pre>

<p>I guess it has something to do that they are not equal size.
Do they need to be the same size and if so how can i achieve it? For people knwoing about this paper, what does your training data look like?</p>

<p>edit:</p>

<pre><code>class CustomDataset(Dataset):
def __init__(self, path_to_file, max_size=10, transform=None):

    with open(path_to_file) as f:
        self.data = json.load(f)
    self.transform = transform
    self.vocab = self.build_vocab(self.data)
    self.word2idx, self.idx2word = self.word2index(self.vocab)

def get_vocab(self):
    return self.vocab

def get_word2idx(self):
    return self.word2idx, self.idx2word

def __len__(self):
    return len(self.data)

def __getitem__(self, idx):
    if torch.is_tensor(idx):
        idx = idx.tolist()
    inputs_ = word_tokenize(self.data[idx][0])
    inputs_ = [w for w in inputs_ if w not in stopwords]
    inputs_ = [w for w in inputs_ if w not in punctuation]
    inputs_ = [self.word2idx[w] for w in inputs_]  # convert words to index

    label = {""positive"": 1,""negative"": 0}
    label_ = label[self.data[idx][1]] #convert label to 0|1

    sample = {""words"": inputs_, ""label"": label_}

    if self.transform:
        sample = self.transform(sample)

    return sample

def build_vocab(self, corpus):
    word_count = {}
    for sentence in corpus:
        tokens = word_tokenize(sentence[0])
        for token in tokens:
            if token not in word_count:
                word_count[token] = 1
            else:
                word_count[token] += 1
    return word_count

def word2index(self, word_count):
    word_index = {w: i for i, w in enumerate(word_count)}
    idx_word = {i: w for i, w in enumerate(word_count)}
    return word_index, idx_word
</code></pre>
",Training and Model Evaluation,pytorch dataloader sentence collected small dataset binary text classification goal train model method proposed convolutional neural network sentence classification started implementation using essentially every sample dataset look like example next took look writing custom dataloaders pytorch using would suspect enumerating batch would yield something following however like guess ha something equal size need size achieve people knwoing paper doe training data look like edit
how to use build_vocab in gensim?,"<ol>
<li>Build_vocab extend my old vocabulary? </li>
</ol>

<p>For example, my idea is when I use doc2vec(s) to train a model, it just builds the vocabulary from the datasets.  If I want to extend it, I need to use build_vocab()</p>

<ol start=""2"">
<li>Where should I use it?  Should I put it after ""gensim.doc2vec()""?  </li>
</ol>

<p>For example:</p>

<pre><code>sentences = gensim.models.doc2vec.TaggedLineDocument(f_path)
dm_model = gensim.models.doc2vec.Doc2Vec(sentences, dm=1, size=300, window=8, min_count=5, workers=4)
dm_model.build_vocab()
</code></pre>
",Training and Model Evaluation,use build vocab gensim build vocab extend old vocabulary example idea use doc vec train model build vocabulary datasets want extend need use build vocab use put gensim doc vec example
Documents in training data belongs to a particular topic in LDA,"<p>I am working on a problem where I have the Text data with around 10,000 documents. I have create a app where if user enters some random comment , It should display all the similar comments/documents present in the training data.
Exactly like in Stack overflow, if you ask an question it shows all related questions asked earlier.
So if anyone has any suggestions how to do it please answer.</p>

<p>Second I am trying LDA(Latent Dirichlet Allocation) algorithm, where I can get the topic with which my new document belongs to, but how will I get the similar documents from training data. Also how shall I choose the num_topics in LDA.</p>

<p>If anyone has any suggestions of algorithms other than LDA , please tell me. </p>
",Training and Model Evaluation,document training data belongs particular topic lda working problem text data around document create app user enters random comment display similar comment document present training data exactly like stack overflow ask question show related question asked earlier anyone ha suggestion please answer second trying lda latent dirichlet allocation algorithm get topic new document belongs get similar document training data also shall choose num topic lda anyone ha suggestion algorithm lda please tell
How to integer coding values for text data?,"<p>I've been looking at how to prepare dataset for deep learning models.</p>

<p>If we have a data like this,</p>

<pre><code>data = [['this', 'is'], ['not', 'with']]
</code></pre>

<p>first they get the frequency of words in our corpus. Based on a word frequency integer label was assigned to word.</p>

<p>The word which is more frequent got assigned 1, then 2 and so on..</p>

<p>My question is why do we need to do that? Can't we just randomly assigned integer values for words. Does it increase accuracy if we following that rule.</p>
",Training and Model Evaluation,integer coding value text data looking prepare dataset deep learning model data like first get frequency word corpus based word frequency integer label wa assigned word word frequent got assigned question need randomly assigned integer value word doe increase accuracy following rule
training data format for NLTK punkt,"<p>I would like to run <code>nltk</code> <code>Punkt</code> to split sentences. There is no training model so I train model separately, but I am not sure if the training data format I am using is correct.</p>

<p>My training data is one sentence per line. I wasn't able to find any documentation about this, only this thread (<a href=""https://groups.google.com/forum/#!topic/nltk-users/bxIEnmgeCSM"" rel=""nofollow noreferrer"">https://groups.google.com/forum/#!topic/nltk-users/bxIEnmgeCSM</a>) sheds some light about training data format.</p>

<p>What is the correct training data format for <code>NLTK</code> <code>Punkt</code> sentence tokenizer?</p>
",Training and Model Evaluation,training data format nltk punkt would like run split sentence training model train model separately sure training data format using correct training data one sentence per line able find documentation thread shed light training data format correct training data format sentence tokenizer
Test Maximum Entropy classifier,"<p>Is it possible to classify new data trough the Stanford Maximum Entropy classifier WITHOUT creating an external file including all the features?</p>

<p>In other words i have a test file in the following format:</p>

<p><strong>token1 \t feature1_1 \t ... \t feature1_N \t goldLabel1</strong></p>

<p><strong>...</strong></p>

<p><strong>tokenM \t featureM_1 \t ... \t featureM_N \t goldLabelM</strong></p>

<p>I was wondering if it is possible to use a data structure to include test data
without creating an external file.</p>
",Training and Model Evaluation,test maximum entropy classifier possible classify new data trough stanford maximum entropy classifier without creating external file including feature word test file following format token feature feature n goldlabel tokenm featurem featurem n goldlabelm wa wondering possible use data structure include test data without creating external file
Customized StanfordNER,"<p>I am trying to build a customized StanfordNer model, training data and properties file are ready.<br>
But when I am trying to run the following code :</p>

<pre><code>java -cp ""stanford-ner.jar:lib/*"" -mx4g edu.stanford.nlp.ie.crf.CRFClassifier -prop download.txt
</code></pre>

<p>This error is popping out :</p>

<blockquote>
  <p>Error: Could not find or load main class
  edu.stanford.nlp.ie.crf.CRFClassifier</p>
</blockquote>

<p><em>Steps followed:</em></p>

<ol>
<li>Downloaded and extracted stanford-ner-2018-10-16.zip file.<br></li>
<li>Java 8 installed and $JAVA_HOME has been set.<br></li>
<li>The properties file (download.txt) has been placed in the folder where stanford-ner-2018-10-16.zip is extracted.</li>
</ol>
",Training and Model Evaluation,customized stanfordner trying build customized stanfordner model training data property file ready trying run following code error popping error could find load main class edu stanford nlp ie crf crfclassifier step followed downloaded extracted stanford ner zip file java installed java home ha set property file download txt ha placed folder stanford ner zip extracted
scikit-learn: FeatureUnion to include hand crafted features,"<p>I am performing multi-label classification on text data. 
I wish to use combined features of <code>tfidf</code> and custom linguistic features similar to the example <a href=""https://towardsdatascience.com/the-triune-pipeline-for-three-major-transformers-in-nlp-18c14e20530"" rel=""nofollow noreferrer"">here</a> using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html"" rel=""nofollow noreferrer"">FeatureUnion</a>. </p>

<p>I already have generated the custom linguistic features, which are in the form of a dictionary where keys represent the labels and (list of) values represent the features. </p>

<pre><code>custom_features_dict = {'contact':['contact details', 'e-mail'], 
                       'demographic':['gender', 'age', 'birth'],
                       'location':['location', 'geo']}
</code></pre>

<p>Training data structure is as follows:</p>

<pre><code>text                                            contact  demographic  location
---                                              ---      ---          ---
'provide us with your date of birth and e-mail'  1        1            0
'contact details and location will be stored'    1        0            1
'date of birth should be before 2004'            0        1            0
</code></pre>

<p>How can the above <code>dict</code> be incorporated into <code>FeatureUnion</code>? My understanding is that a user-defined function should be called that returns boolean values corresponding to the presence or absence of string values (from <code>custom_features_dict</code>) in the training data. </p>

<p>This gives the following <code>list</code> of <code>dict</code> for the given training data:</p>

<pre><code>[
    {
       'contact':1,
       'demographic':1,
       'location':0
    },
    {
       'contact':1,
       'demographic':0,
       'location':1
    },
    {
       'contact':0,
       'demographic':1,
       'location':0
    },
] 
</code></pre>

<p>How can the above <code>list</code> be used to implement fit and transform?</p>

<p>The code is given below:</p>

<pre><code>import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import DictVectorizer
#from sklearn.metrics import accuracy_score
from sklearn.multiclass import OneVsRestClassifier
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from io import StringIO

data = StringIO(u'''text,contact,demographic,location
provide us with your date of birth and e-mail,1,1,0
contact details and location will be stored,0,1,1
date of birth should be before 2004,0,1,0''')

df = pd.read_csv(data)

custom_features_dict = {'contact':['contact details', 'e-mail'], 
                        'demographic':['gender', 'age', 'birth'],
                        'location':['location', 'geo']}

my_features = [
    {
       'contact':1,
       'demographic':1,
       'location':0
    },
    {
       'contact':1,
       'demographic':0,
       'location':1
    },
    {
       'contact':0,
       'demographic':1,
       'location':0
    },
]

bow_pipeline = Pipeline(
    steps=[
        (""tfidf"", TfidfVectorizer(stop_words=stop_words)),
    ]
)

manual_pipeline = Pipeline(
    steps=[
        # This needs to be fixed
        (""custom_features"", my_features),
        (""dict_vect"", DictVectorizer()),
    ]
)

combined_features = FeatureUnion(
    transformer_list=[
        (""bow"", bow_pipeline),
        (""manual"", manual_pipeline),
    ]
)

final_pipeline = Pipeline([
            ('combined_features', combined_features),
            ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),
        ]
)

labels = ['contact', 'demographic', 'location']

for label in labels:
    final_pipeline.fit(df['text'], df[label]) 
</code></pre>
",Training and Model Evaluation,scikit learn featureunion include hand crafted feature performing multi label classification text data wish use combined feature custom linguistic feature similar example using featureunion already generated custom linguistic feature form dictionary key represent label list value represent feature training data structure follows incorporated understanding user defined function called return boolean value corresponding presence absence string value training data give following given training data used implement fit transform code given
Is there a way to compare 2 unsupervised models?,"<p>I understand it’s usually difficult to evaluate unsupervised models. However, since I’m new to NLP, I figured I’d ask to see if there’s a way to compare 2 topic models. I have an STM and an LDA models, but if someone asks how do I choose one vs the other, I have no answer. Is there a way where given the data I have, I can show some metric that I can compare between models and state with some degree of confidence the LDA is better or worse than STM?</p>
",Training and Model Evaluation,way compare unsupervised model understand usually difficult evaluate unsupervised model however since new nlp figured ask see way compare topic model stm lda model someone asks choose one v answer way given data show metric compare model state degree confidence lda better worse stm
fairseq toolkit not using GPU to train NMT model,"<p>I am training an English-Vietnamese NMT model using fairseq.</p>

<p>fairseq tells that it is training the model on 1 GPU. However, when I check the GPU, It seems not to be used and the training process is very slow.</p>

<p><a href=""https://i.sstatic.net/p8paQ.jpg"" rel=""nofollow noreferrer"">screenshot: GPU usage</a></p>

<p>Training on 63k sentences corpus: an epoch takes about 1 hours. (model: fconv)</p>

<p>Training on 233k sentences corpus: an epoch takes about 4 hours. (model: transformer)</p>

<p><a href=""https://i.sstatic.net/atXeg.jpg"" rel=""nofollow noreferrer"">screenshot: console log</a></p>

<p>My GPU is NVIDIA GeForce GTX 1050 and the CUDA version is 10.2.</p>

<p><strong>Am I successfully training the model on GPU?</strong></p>

<p>Glad to see your solutions/suggestions.</p>
",Training and Model Evaluation,fairseq toolkit using gpu train nmt model training english vietnamese nmt model using fairseq fairseq tell training model gpu however check gpu seems used training process slow screenshot gpu usage training k sentence corpus epoch take hour model fconv training k sentence corpus epoch take hour model transformer screenshot console log gpu nvidia gtx cuda version successfully training model gpu glad see solution suggestion
Finding meaningful sub-sentences from a sentence,"<p>Is there a way to to find all the sub-sentences of a sentence that still are meaningful and contain at least one subject, verb, and a predicate/object?</p>

<p>For example, if we have a sentence like ""I am going to do a seminar on NLP at SXSW in Austin next month"". We can extract the following meaningful sub-sentences from this sentence: ""I am going to do a seminar"", ""I am going to do a seminar on NLP"", ""I am going to do a seminar on NLP at SXSW"", ""I am going to do a seminar at SXSW"", ""I am going to do a seminar in Austin"", ""I am going to do a seminar on NLP next month"", etc.</p>

<p>Please note that there is no deduced sentences here (e.g. ""There will be a NLP seminar at SXSW next month"". Although this is true, we don't need this as part of this problem.) . All generated sentences are strictly part of the given sentence.</p>

<p>How can we approach solving this problem? I was thinking of creating annotated training data that has a set of legal sub-sentences for each sentence in the training data set. And then write some supervised learning algorithm(s) to generate a model.</p>

<p>I am quite new to NLP and Machine Learning, so it would be great if you guys could suggest some ways to solve this problem.</p>
",Training and Model Evaluation,finding meaningful sub sentence sentence way find sub sentence sentence still meaningful contain least one subject verb predicate object example sentence like going seminar nlp sxsw austin next month extract following meaningful sub sentence sentence going seminar going seminar nlp going seminar nlp sxsw going seminar sxsw going seminar austin going seminar nlp next month etc please note deduced sentence e g nlp seminar sxsw next month although true need part problem generated sentence strictly part given sentence approach solving problem wa thinking creating annotated training data ha set legal sub sentence sentence training data set write supervised learning algorithm generate model quite new nlp machine learning would great guy could suggest way solve problem
Is there any model/classifier that works best for NLP based projects like this?,"<p>I've written a program to analyze a given piece of text from a website and make conclusory classifications as to its validity. The code basically vectorizes the description (taken from the HTML of a given webpage in real-time) and takes in a few inputs from that as features to make its decisions. There are some more features like the domain of the website and some keywords I've explicitly counted.</p>

<p>The highest accuracy I've been able to achieve is with a RandomForestClassifier, (>90%). I'm not sure what I can do to make this accuracy better except incorporating a more sophisticated model. I tried using an MLP but for no set of hyperparameters does it seem to exceed the previous accuracy. I have around 2000 data points available for training.</p>

<p>Is there any classifier that works best for such projects? Does anyone have any suggestions as to how I can bring about improvements? (If anything needs to be elaborated, I'll do so.)</p>

<p>Any suggestions on how I can improve on this project in general? Should I include the text on a webpage as well? How should I do so? I tried going through a few sites, but the next doesn't seem to be contained in any specific element whereas the description is easy to obtain from the HTML. Any help?</p>

<p>What else can I take as features? If anyone could suggest any creative ideas, I'd really appreciate it.</p>
",Training and Model Evaluation,model classifier work best nlp based project like written program analyze given piece text website make conclusory classification validity code basically vectorizes description taken html given webpage real time take input feature make decision feature like domain website keywords explicitly counted highest accuracy able achieve randomforestclassifier sure make accuracy better except incorporating sophisticated model tried using mlp set hyperparameters doe seem exceed previous accuracy around data point available training classifier work best project doe anyone suggestion bring improvement anything need elaborated suggestion improve project general include text webpage well tried going site next seem contained specific element whereas description easy obtain html help else take feature anyone could suggest creative idea really appreciate
Is there any ML classifier that generally works best for NLP projects?,"<p>I've written a program that reads word vectors from a particular website and makes conclusary classifications.</p>

<p>I'm getting the highest accuracy and F Score for a RandomForestClassifier. I'm not sure what I can do to make this accuracy higher except changing the model. I tried to use MLPs but landed with a lower accuracy. Should I use some other neural network?</p>

<p>Does anyone know what models generally work for such NLP based programs? </p>

<p>In a nutshell, what the program does is look through the HTML of a given webpage for certain features, vectorize the words it can find (using predefined vector spaces) and make classifications based on that. I'm getting an accuracy over 90% for the RandomForestClassifier. Any help?</p>
",Training and Model Evaluation,ml classifier generally work best nlp project written program read word vector particular website make conclusary classification getting highest accuracy f score randomforestclassifier sure make accuracy higher except changing model tried use mlps landed lower accuracy use neural network doe anyone know model generally work nlp based program nutshell program doe look html given webpage certain feature vectorize word find using predefined vector space make classification based getting accuracy randomforestclassifier help
NLP ML How to know the weight of words used in text classifier?,"<p>I am building a tweet classifier where I try to train different ML models to classify tweets from 2 different tweeter accounts. So far I have train Logistic Regression model, K Neighbors Classifier and decision tree classifier.</p>

<p>Is there a way to know what words in the tweets those classifiers used to predict the account? like the weight of words in the classification process?? I am open to train new classifiers that can do that as well.</p>

<p>Already did some ngram analysis on the tweets like word frequency.</p>

<p>thanks in advance!   </p>
",Training and Model Evaluation,nlp ml know weight word used text classifier building tweet classifier try train different ml model classify tweet different tweeter account far train logistic regression model k neighbor classifier decision tree classifier way know word tweet classifier used predict account like weight word classification process open train new classifier well already ngram analysis tweet like word frequency thanks advance
"Extract Keras concatenated layer of 3 embedding layers, but it&#39;s an empty list","<p>I am constructing a Keras Classification model with Multiple Inputs (3 actually) to predict one single output. Specifically, my 3 <strong>inputs</strong> are:</p>

<ol>
<li>Actors</li>
<li>Plot Summary</li>
<li>Relevant Movie Features</li>
</ol>

<p><strong>Output:</strong></p>

<ol>
<li>Genre tags</li>
</ol>

<p><strong>Python Code (create the multiple input keras)</strong></p>

<pre><code>def kera_multy_classification_model():

    sentenceLength_actors = 15
    vocab_size_frequent_words_actors = 20001

    sentenceLength_plot = 23
    vocab_size_frequent_words_plot = 17501

    sentenceLength_features = 69
    vocab_size_frequent_words_features = 20001

    model = keras.Sequential(name='Multy-Input Keras Classification model')

    actors = keras.Input(shape=(sentenceLength_actors,), name='actors_input')
    plot = keras.Input(shape=(sentenceLength_plot,), name='plot_input')
    features = keras.Input(shape=(sentenceLength_features,), name='features_input')

    emb1 = layers.Embedding(input_dim = vocab_size_frequent_words_actors + 1,
                            # based on keras documentation input_dim: int &gt; 0. Size of the vocabulary, i.e. maximum integer index + 1.
                            output_dim = Keras_Configurations_model1.EMB_DIMENSIONS,
                            # int &gt;= 0. Dimension of the dense embedding
                            embeddings_initializer = 'uniform', 
                            # Initializer for the embeddings matrix.
                            mask_zero = False,
                            input_length = sentenceLength_actors,
                            name=""actors_embedding_layer"")(actors)
    encoded_layer1 = layers.LSTM(100)(emb1)

    emb2 = layers.Embedding(input_dim = vocab_size_frequent_words_plot + 1,
                            output_dim = Keras_Configurations_model2.EMB_DIMENSIONS,
                            embeddings_initializer = 'uniform',
                            mask_zero = False,
                            input_length = sentenceLength_plot,
                            name=""plot_embedding_layer"")(plot)
    encoded_layer2 = layers.LSTM(100)(emb2)

    emb3 = layers.Embedding(input_dim = vocab_size_frequent_words_features + 1,
                            output_dim = Keras_Configurations_model3.EMB_DIMENSIONS,
                            embeddings_initializer = 'uniform',
                            mask_zero = False,
                            input_length = sentenceLength_features,
                            name=""features_embedding_layer"")(features)
    encoded_layer3 = layers.LSTM(100)(emb3)

    merged = layers.concatenate([encoded_layer1, encoded_layer2, encoded_layer3])

    layer_1 = layers.Dense(Keras_Configurations_model1.BATCH_SIZE, activation='relu')(merged)

    output_layer = layers.Dense(Keras_Configurations_model1.TARGET_LABELS, activation='softmax')(layer_1)

    model = keras.Model(inputs=[actors, plot, features], outputs=output_layer)

    print(model.output_shape)

    print(model.summary())

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['sparse_categorical_accuracy'])
</code></pre>

<p><strong>Model's Structure</strong></p>

<p><a href=""https://i.sstatic.net/9wfri.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9wfri.png"" alt=""enter image description here""></a></p>

<p><strong>My problem:</strong></p>

<p>After successfully fitting and training the model on some training data, I would like to extract the embeddings of this model for later use. My main approach before using a multiple input keras model, was to train 3 different keras models and extract 3 different embedding layers of shape 100. Now that I have the multiple input keras model, <strong>I want to extract the concatenated embedding layer</strong> with output shape (None, 300).</p>

<p>Although, when I try to use this python command:</p>

<pre><code>embeddings = model_4.layers[9].get_weights()
print(embeddings)
</code></pre>

<p>or </p>

<pre><code>embeddings = model_4.layers[9].get_weights()[0]
print(embeddings)
</code></pre>

<p>I get either an empty list (1st code sample) either an <em>IndenError: list index out of range</em> (2nd code sample).</p>

<p>Thank you in advance for any advice or help on this matter. Feel free to ask on the comments any additional information that I may have missed, to make this question more complete.</p>

<p><em>Note: Python code and model's structure have been also presented to this previously answered <a href=""https://stackoverflow.com/questions/59489625/model-fit-keras-classification-multiple-inputs-single-output-gives-error-attr"">question</a></em></p>
",Training and Model Evaluation,extract kera concatenated layer embedding layer empty list constructing kera classification model multiple input actually predict one single output specifically input actor plot summary relevant movie feature output genre tag python code create multiple input kera model structure problem successfully fitting training model training data would like extract embeddings model later use main approach using multiple input kera model wa train different kera model extract different embedding layer shape multiple input kera model want extract concatenated embedding layer output shape none although try use python command get either empty list st code sample either indenerror list index range nd code sample thank advance advice help matter feel free ask comment additional information may missed make question complete note python code model structure also presented previously answered href
I have a dataset on which I want to do Phrase extraction using NLP but I am unable to do so?,"<p>How can I extract a phrase from a sentence using a dataset which has some set of the sentence and corresponding label in the form of </p>

<pre><code>Sentence1:I want to play cricket 
Label1: play cricket

Sentence2: Need to wash my clothes
Label2: wash clothes
</code></pre>

<p>I have tried using chunking with nltk but I am not able to use training data along with the chunks. </p>
",Training and Model Evaluation,dataset want phrase extraction using nlp unable extract phrase sentence using dataset ha set sentence corresponding label form tried using chunking nltk able use training data along chunk
Topic label of each document in LDA model using textmineR,"<p>I'm using textmineR to fit a LDA model to documents similar to <a href=""https://cran.r-project.org/web/packages/textmineR/vignettes/c_topic_modeling.html"" rel=""nofollow noreferrer"">https://cran.r-project.org/web/packages/textmineR/vignettes/c_topic_modeling.html</a>. Is it possible to get the topic label for each document in the data set?</p>

<pre><code>&gt;library(textmineR)
&gt;data(nih_sample)
&gt; # create a document term matrix 
&gt; dtm &lt;- CreateDtm(doc_vec = nih_sample$ABSTRACT_TEXT,doc_names = 
 nih_sample$APPLICATION_ID, ngram_window = c(1, 2), stopword_vec = 
 c(stopwords::stopwords(""en""), stopwords::stopwords(source = ""smart"")),lower 
 = TRUE, remove_punctuation = TRUE,remove_numbers = TRUE, verbose = FALSE, 
 cpus = 2) 
 &gt;dtm &lt;- dtm[,colSums(dtm) &gt; 2]
 &gt;set.seed(123)
 &gt; model &lt;- FitLdaModel(dtm = dtm, k = 20,iterations = 200,burnin = 
 180,alpha = 0.1, beta = 0.05, optimize_alpha = TRUE, calc_likelihood = 
 TRUE,calc_coherence = TRUE,calc_r2 = TRUE,cpus = 2)
</code></pre>

<p>then adding the labels to the model: </p>

<pre><code> &gt; model$labels &lt;- LabelTopics(assignments = model$theta &gt; 0.05, dtm = dtm, 
   M = 1)
</code></pre>

<p>now I want the topic labels for each of 100 document in <code>nih_sample$ABSTRACT_TEXT</code> </p>
",Training and Model Evaluation,topic label document lda model using textminer using textminer fit lda model document similar possible get topic label document data set adding label model want topic label document
Is there any way by which we can change the number of target classes in multi-class classification problem?,"<p>I have a text dataset with 13 classes. There are some classes which are overlapping. However, to prove that these classes are overlapping, I wish to train a machine learning classifier on the data with 13 labels and then test it on fewer labels say 10,11 etc to find the what is the minimum number of non-overlapping target labels would be there in the dataset.</p>

<p>Please tell how can I change the number of taget labels while testing a classifier?</p>

<p>Thankyou in advance!</p>
",Training and Model Evaluation,way change number target class multi class classification problem text dataset class class overlapping however prove class overlapping wish train machine learning classifier data label test fewer label say etc find minimum number non overlapping target label would dataset please tell change number taget label testing classifier thankyou advance
How to train only for specific entity labels in spaCy?,"<p>I want to update an existing spacy model that has the following entity labels:</p>

<pre><code>('CARDINAL',  'DATE',  'EVENT',  'FAC',  'GPE',  'LANGUAGE',  'LAW',  'LOC',  'MONEY', 'NORP',
 'ORDINAL',  'ORG',  'PERCENT',  'PERSON',  'PRODUCT',  'QUANTITY',  'TIME',  'WORK_OF_ART')
</code></pre>

<p>However, I only want to update ('ORG','GPE','PERSON','PRODUCT') while keeping the rest the same. How to do so ?</p>
",Training and Model Evaluation,train specific entity label spacy want update existing spacy model ha following entity label however want update org gpe person product keeping rest
How to initialize a gensim model with the vocabulary from another model?,"<p>I'm training some embeddings on a large corpus.  I gather from <code>gensim</code>'s documentation that it builds the vocabulary before beginning training.  In my case, building the vocabulary takes many hours.  I'd like to save time by re-using the vocabulary from the first model.  How can I do this?  the <code>.build_vocab</code> method can't take the <code>vocabulary</code> object from another model.  </p>

<p>Here's a dummy example:</p>

<pre><code>from gensim.models import FastText, Word2Vec
sentences = [""where are my goats"", ""yay i found my goats""]
m1 = Word2Vec(sentences, size  = 3)
m2 = Word2Vec(size = 4)
m2.build_vocab(m1.vocabulary) # doesn't work
</code></pre>
",Training and Model Evaluation,initialize gensim model vocabulary another model training embeddings large corpus gather documentation build vocabulary beginning training case building vocabulary take many hour like save time using vocabulary first model method take object another model dummy example
Train the model first and Test multiple times,"<p>I have been trying to use python's NLP script with my QT GUI based C++ application. 
Basically in the application I am trying to access the NLP script through command line:</p>

<pre><code>QString path = ""D:/DS Project/Treegramming"";
QString  command(""py"");
QStringList params = QStringList() &lt;&lt; ""nlp.py"";
params &lt;&lt; text;
QProcess *process = new QProcess();
process-&gt;setWorkingDirectory(path);
process-&gt;start(command, params);
process-&gt;waitForFinished();
QString result = process-&gt;readAll();
</code></pre>

<p>The above is working perfectly. but the problem is, it is taking about 40-50 seconds to execute, as it is first training the model and then testing.
But I want to train the model first and test it multiple times as we do in Jupyter Notebook.
for that I made a separate function for testing and trying to access it with command line:</p>

<blockquote>
  <p>PS D:\DS Project\Treegramming> py nlp.py ""test('it was amazing')""</p>
</blockquote>

<p>but again this thing is executing the whole script first and then executing the function. is there anything I can do to solve this?</p>

<p>python script:</p>

<pre><code># -*- coding: utf-8 -*-
""""""
Created on Fri Dec  6 16:18:01 2019

@author: Muhammad Ahmed
""""""

import nltk
import sys
import random
import re,string
from nltk.corpus import twitter_samples
from nltk.corpus import stopwords
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
from nltk.corpus import twitter_samples
from nltk import classify
from nltk import NaiveBayesClassifier
from nltk import FreqDist
from nltk.stem.wordnet import WordNetLemmatizer

positive_tweets = twitter_samples.strings('positive_tweets.json')
negative_tweets = twitter_samples.strings('negative_tweets.json')
text = twitter_samples.strings('tweets.20150430-223406.json')

tweet_tokens = twitter_samples.tokenized('positive_tweets.json')

def lemmatize_sentence(tokens):
    sentence = []
    lematizer = WordNetLemmatizer()
    for word, tag in pos_tag(tokens):
        if tag.startswith('NN'):
            pos = 'n'
        elif tag.startswith('VB'):
            pos = 'v'
        else:
            pos = 'a'
        sentence.append( lematizer.lemmatize( word , pos ) )
    return sentence

def remove_noise(tokens , stop_words = ()):
    sentence = []
    for token, tag in pos_tag( tokens ):
        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+#]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+' , '',token)
        token = re.sub(""(@[A-Za-z0-9_]+)"","""",token)

        if tag.startswith(""NN""):
            pos = 'n'
        elif tag.startswith('VB'):
            pos = 'v'
        else:
            pos = 'a'

        lemmatizer = WordNetLemmatizer()
        token = lemmatizer.lemmatize(token, pos)

        if len(token) &gt; 0 and token not in string.punctuation and token.lower() not in stop_words:
            sentence.append( token.lower() )
    return sentence

def get_all_words(tokens_list):
    for tokens in tokens_list:
        for token in tokens:
            yield token

def get_tweets_for_model(tokens_list):
    for tweets in tokens_list:
        yield dict([token,True] for token in tweets)

stop_words = stopwords.words('english')

positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')
negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')

positive_cleaned_tokens_list = []
negative_cleaned_tokens_list = []

for tokens in positive_tweet_tokens:
    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))

for tokens in negative_tweet_tokens:
    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))

all_pos_words = get_all_words( positive_cleaned_tokens_list )
all_neg_words = get_all_words( negative_cleaned_tokens_list )

freq_dis_pos = FreqDist( all_pos_words )
freq_dis_neg = FreqDist( all_neg_words )

positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)
negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)

pos_dataset = [(tweets,""Positive"") for tweets in positive_tokens_for_model]
neg_dataset = [(tweets,""Negative"") for tweets in negative_tokens_for_model]

dataset = pos_dataset + neg_dataset
random.shuffle(dataset)

train_data = dataset[:7000]
test_data = dataset[7000:]

classifier = NaiveBayesClassifier.train(train_data)

def test( custom_tweet ):
    custom_tokens = remove_noise(word_tokenize(custom_tweet))
    res = classifier.classify(dict([token, True] for token in custom_tokens))
    print(res)
    f = open( ""result.txt"" , ""w"" )
    f.write(res)    
    f.close() 

eval( sys.argv[1] );
</code></pre>
",Training and Model Evaluation,train model first test multiple time trying use python nlp script qt gui based c application basically application trying access nlp script command line working perfectly problem taking second execute first training model testing want train model first test multiple time jupyter notebook made separate function testing trying access command line p project treegramming py nlp py test wa amazing thing executing whole script first executing function anything solve python script
scikit learn: How to check if analyzer parameter from CountVectorize() is working correctly,"<p>This is my analyzer function</p>

<pre><code>def text_processing(text):
    text = denoise_text(text) #noise removal
    text = word_tokenize(text) #tokenization
    text = normalize(text) #normalization
    text = lemmas(text) #lemmatization
    return text
</code></pre>

<p>This is one of my pipeline function</p>

<pre><code>pipeline = Pipeline([
    ('bow', CountVectorizer(analyzer=text_processing)), # strings to token integer counts
    ('tfidf', TfidfTransformer()), # integer counts to weighted TF-IDF scores
    ('classifier', linear_model.SGDClassifier(max_iter=1000, tol=1e-3)) # train on TF-IDF vectors w/ Naive Bayes classifier
])

pipeline.fit(x_train, y_train)
</code></pre>

<p>The result is</p>

<pre><code>---------------------------------------
Stochastic Gradient Descent Result
---------------------------------------
              precision    recall  f1-score   support

    not spam       0.96      0.94      0.95       142
        spam       0.93      0.95      0.94       118

    accuracy                           0.95       260
   macro avg       0.95      0.95      0.95       260
weighted avg       0.95      0.95      0.95       260
</code></pre>

<p>If  i remove the analyzer parameter function from the pipeline </p>

<pre><code>pipeline = Pipeline([
    ('bow', CountVectorizer()), # strings to token integer counts
    ('tfidf', TfidfTransformer()), # integer counts to weighted TF-IDF scores
    ('classifier', linear_model.SGDClassifier(max_iter=1000, tol=1e-3)) # train on TF-IDF vectors w/ Naive Bayes classifier
])
</code></pre>

<p>It gives me almost the same accuracy. I'm wondering if the <code>text_processing</code> function working correctly or not, how to check this?</p>
",Training and Model Evaluation,scikit learn check analyzer parameter countvectorize working correctly analyzer function one pipeline function result remove analyzer parameter function pipeline give almost accuracy wondering function working correctly check
Why my NLP model has tagged wrong word as a new entity?,"<p>I'm new to NLP. Working on this from last 2/3 days. Using <code>spacy</code> for this. I'm trying to ""train an additional entity type"" by using the following piece of code...</p>

<pre><code>""""""Example of training an additional entity type

This script shows how to add a new entity type to an existing pre-trained NER
model. To keep the example short and simple, only four sentences are provided
as examples. In practice, you'll need many more — a few hundred would be a
good start. You will also likely need to mix in examples of other entity
types, which might be obtained by running the entity recognizer over unlabelled
sentences, and adding their annotations to the training set.

The actual training is performed by looping over the examples, and calling
`nlp.entity.update()`. The `update()` method steps through the words of the
input. At each word, it makes a prediction. It then consults the annotations
provided on the GoldParse instance, to see whether it was right. If it was
wrong, it adjusts its weights so that the correct action will score higher
next time.

After training your model, you can save it to a directory. We recommend
wrapping models as Python packages, for ease of deployment.

For more details, see the documentation:
* Training: https://spacy.io/usage/training
* NER: https://spacy.io/usage/linguistic-features#named-entities

Compatible with: spaCy v2.1.0+
Last tested with: v2.1.0
""""""
from __future__ import unicode_literals, print_function

import plac
import random
from pathlib import Path
import spacy
from spacy.util import minibatch, compounding


# new entity label
LABEL = ""CATID:1000012""

# training data
# Note: If you're using an existing model, make sure to mix in examples of
# other entity types that spaCy correctly recognized before. Otherwise, your
# model might learn the new type, but ""forget"" what it previously knew.
# https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting
TRAIN_DATA = [
    (
        ""The mobile phone can be used to communicate over long distances without wires."",
        {""entities"": [(11, 16, LABEL)]},
    ),
    (
        ""A smartphone is a mobile phone that can do more than other phones."",
        {""entities"": [(2, 12, LABEL)]},
    ),
    (
        ""Feature phones run on proprietary firmware with third-party software support through platforms such as Java ME or BREW."",
        {""entities"": [(8, 14, LABEL)]},
    ),
    (
        ""As mobile phones became more popular, they began to cost less money, and more people could afford them."",
        {""entities"": [(10, 16, LABEL)]},
    ),
    (
        ""The majority of smartphones run on Apple iOS or Google Android but others use Windows Phone or BlackBerry OS."",
        {""entities"": [(16, 27, LABEL)]},
    ),
    (
        ""Feature phones are often more durable, less complex, and more affordable."",
        {""entities"": [(8, 14, LABEL)]},
    ),
]


@plac.annotations(
    model=(""Model name. Defaults to blank 'en' model."", ""option"", ""m"", str),
    new_model_name=(""New model name for model meta."", ""option"", ""nm"", str),
    output_dir=(""Optional output directory"", ""option"", ""o"", Path),
    n_iter=(""Number of training iterations"", ""option"", ""n"", int),
)
def main(model=""en_core_web_sm"", new_model_name=""CATID"", output_dir=""Model"", n_iter=30):
    """"""Set up the pipeline and entity recognizer, and train the new entity.""""""
    random.seed(0)
    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank(""en"")  # create blank Language class
        print(""Created blank 'en' model"")
    # Add entity recognizer to model if it's not in the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if ""ner"" not in nlp.pipe_names:
        ner = nlp.create_pipe(""ner"")
        nlp.add_pipe(ner)
    # otherwise, get it, so we can add labels to it
    else:
        ner = nlp.get_pipe(""ner"")

    ner.add_label(LABEL)  # add new entity label to entity recognizer
    # Adding extraneous labels shouldn't mess anything up
    ner.add_label(""VEGETABLE"")
    if model is None:
        optimizer = nlp.begin_training()
    else:
        optimizer = nlp.resume_training()
    move_names = list(ner.move_names)
    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]
    with nlp.disable_pipes(*other_pipes):  # only train NER
        sizes = compounding(1.0, 4.0, 1.001)
        # batch up the examples using spaCy's minibatch
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            batches = minibatch(TRAIN_DATA, size=sizes)
            losses = {}
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(texts, annotations, sgd=optimizer,
                        drop=0.35, losses=losses)
            print(""Losses"", losses)

    # test the trained model
    test_text = ""Mobile phones under 10k.""
    doc = nlp(test_text)
    print(""Entities in '%s'"" % test_text)
    for ent in doc.ents:
        print(ent.label_, ent.text)

    # save model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.meta[""name""] = new_model_name  # rename model
        nlp.to_disk(output_dir)
        print(""Saved model to"", output_dir)

        # test the saved model
        print(""Loading from"", output_dir)
        nlp2 = spacy.load(output_dir)
        # Check the classes have loaded back consistently
        assert nlp2.get_pipe(""ner"").move_names == move_names
        doc2 = nlp2(test_text)
        for ent in doc2.ents:
            print(ent.label_, ent.text)


if __name__ == ""__main__"":
    plac.call(main)
</code></pre>

<p>What I've done at here is, created a new entity by the name ""CATID:1000012"" &amp; trying to teach the existing model(en_core_web_sm) by providing some training data to identify the new entity(<code>CATID:1000012</code>). </p>

<p>But, after training when I use the statement(<code>Mobile phones under 10k.</code>) for testing to identifying the new entity I'm getting word <code>under</code> tagged as my new entity. I Can't able to understand why it's happening. </p>

<p>Here is the output of the training result for reference.</p>

<pre><code>nlp python train_new_entity_type.py 
Loaded model 'en_core_web_sm'
Losses {'ner': 97.52074194946908}
Losses {'ner': 93.69056796826771}
Losses {'ner': 104.9913784135133}
Losses {'ner': 106.799345289357}
Losses {'ner': 95.95352823211579}
Losses {'ner': 95.5921512588784}
Losses {'ner': 104.20201236551293}
Losses {'ner': 91.30133426242173}
Losses {'ner': 83.3817401985325}
Losses {'ner': 108.10902537551738}
Losses {'ner': 90.79526191594738}
Losses {'ner': 92.66721615749748}
Losses {'ner': 89.48430367704572}
Losses {'ner': 79.65045529220826}
Losses {'ner': 81.69409873239893}
Losses {'ner': 78.08388914307191}
Losses {'ner': 75.96670668302312}
Losses {'ner': 85.84131752077208}
Losses {'ner': 83.16802654485699}
Losses {'ner': 74.70389228454836}
Losses {'ner': 82.74640468226158}
Losses {'ner': 86.27583874967632}
Losses {'ner': 91.80043086154723}
Losses {'ner': 71.57743340098828}
Losses {'ner': 89.68161530740633}
Losses {'ner': 68.54411317529383}
Losses {'ner': 79.08097473334223}
Losses {'ner': 80.63091049017571}
Losses {'ner': 87.19688005072365}
Losses {'ner': 87.32719076574251}
Entities in 'Mobile phones under 10k.'
CATID:1000012 phones
CATID:1000012 under
CATID:1000012 .
Saved model to Model
Loading from Model
CATID:1000012 phones
CATID:1000012 under
VEGETABLE .
</code></pre>
",Training and Model Evaluation,nlp model ha tagged wrong word new entity new nlp working last day using trying train additional entity type using following piece code done created new entity name catid trying teach existing model en core web sm providing training data identify new entity training use statement testing identifying new entity getting word tagged new entity able understand happening output training result reference
Word to vector where should I start?,"<p>I'm trying to implement a neural networks model on labeled data that I have. The data contains several columns (categorical and numeric features as well).<br>
Few columns in this data contains a short description, written by users which I also want to analyze but I don't know how to start. 
The data looks something like this:</p>

<pre><code>problem ID   status   description                        labels
1            closed   short description of the problem   CRM
2            open     short description of the problem   ERP 
3            closed   short description of the problem   CRM
</code></pre>

<p>Using status (which I will convert into dummy variables) and description (this is where I need you guys), I want to train the model to predict the labels. </p>

<p>Any idea about how should I start? How can I convert the description columns into a useful data? </p>

<p>Thanks!   </p>
",Training and Model Evaluation,word vector start trying implement neural network model labeled data data contains several column categorical numeric feature well column data contains short description written user also want analyze know start data look something like using status convert dummy variable description need guy want train model predict label idea start convert description column useful data thanks
Training conversations using sequence models,"<p>I have a question regarding training conversations, the context is that the next statement is not necessarily a function of the previous statement but also of any statement in the body of conversation for example:</p>

<p>person1: what is your favorite food and restaurant
person 2: my favorite food is burger and McDonald is my fav restaurant
Person1: why do you like burger
person 2: because i dont care about the health aspect while eating
person1: why do you like mcdonalds when there are so many places where you can buy a burger</p>

<p>now as we can see the last question was derived from an answer received 3 steps before...
In this context how do i train an lstm so that it remembers all the previous contexts..
essentially i am looking for an approach to create my training data and output sentence..</p>
",Training and Model Evaluation,training conversation using sequence model question regarding training conversation context next statement necessarily function previous statement also statement body conversation example person favorite food restaurant person favorite food burger mcdonald fav restaurant person like burger person dont care health aspect eating person like mcdonalds many place buy burger see last question wa derived answer received step context train lstm remembers previous context essentially looking approach create training data output sentence
fasttext pre trained sentences similarity,"<p>I want to use fasttext pre-trained models to compute similarity
a sentence between a set of sentences.
can anyone help me?
what is the best approach?</p>

<p>I computed the similarity between sentences by train a tfidf model. write code like this.
is it possible to change it and use fasttext pre-trained models? for example use vectors to train a tfidf model?</p>

<pre><code>def generate_tfidf_model(sentences):
    print(""generating TfIdf model"")
    texts = [[sentence for sentence in doc.split()] for doc in sentences]
    dictionary = gensim.corpora.Dictionary(texts)    
    feature_cnt = len(dictionary.token2id)
    mycorpus = [dictionary.doc2bow(doc, allow_update=True) for doc in texts]
    tfidf_model = gensim.models.TfidfModel(mycorpus)
    index = gensim.similarities.SparseMatrixSimilarity(tfidf_model[mycorpus]
                                                        , num_features = feature_cnt)
    return tfidf_model, index, dictionary

def query_search(query, tfidf_model, index, dictionary):
    query = normal_stemmer_sentence(query)
    query_vector = dictionary.doc2bow(query.split())
    similarity = index[tfidf_model[query_vector]]
    return similarity
</code></pre>
",Training and Model Evaluation,fasttext pre trained sentence similarity want use fasttext pre trained model compute similarity sentence set sentence anyone help best approach computed similarity sentence train tfidf model write code like possible change use fasttext pre trained model example use vector train tfidf model
Can I match words or sentences to a pre-vectorized corpus of sentences in Python for NL processing?,"<p>I've been searching for an answer to this specific question for a few hours and while I've learned a lot, I still haven't figured it out. </p>

<p>I have a dataset of ~70,000 sentences with subset of about 4,000 sentences that have been appropriately categorized, the rest are uncategorized. Currently I'm using a scikit pipeline with CountVectorizer and TfidfTransformer to vectorize the data, however I'm only vectorizing based off the 4,000 sentences and then testing various models via cross-validation. </p>

<p>I'm wondering if there is a way to use Word2Vec or something similar to vectorize the entire corpus of data and then use these vectors with my subset of 4,000 sentences. My intention is to increase the accuracy of my model predictions by using word vectors that incorporate all of the semantic data in the corpus rather than just data from the 4,000 sentences. </p>

<p>The code I'm currently using is: </p>

<pre><code>    svc = Pipeline([('vect', CountVectorizer(ngram_range=(3, 5))),
               ('tfidf', TfidfTransformer()),
               ('clf', LinearSVC()),
               ])

nb.fit(X_train, y_train)

y_pred = svc.predict(X_test)
</code></pre>

<p>Where X_train and y_train are my features and labels, respectively. I also have a list z_all which includes all remaining uncategorized features. </p>

<p>Just getting pointed in the right direction (or told whether or not this is possible) would be super helpful.</p>

<p>Thank you!</p>
",Training and Model Evaluation,match word sentence pre vectorized corpus sentence python nl processing searching answer specific question hour learned lot still figured dataset sentence subset sentence appropriately categorized rest uncategorized currently using scikit pipeline countvectorizer tfidftransformer vectorize data however vectorizing based sentence testing various model via cross validation wondering way use word vec something similar vectorize entire corpus data use vector subset sentence intention increase accuracy model prediction using word vector incorporate semantic data corpus rather data sentence code currently using x train train feature label respectively also list z includes remaining uncategorized feature getting pointed right direction told whether possible would super helpful thank
Custom Loss Function with Spacy Textcat,"<p>I've been looking around for a while now. I would like to know if it's possible to modify/customize the loss function of the spaCy <a href=""https://spacy.io/api/textcategorizer"" rel=""nofollow noreferrer"">textcategorizer</a>. </p>

<p>I mean, when you want to distill a model (for instance BERT) and want to add a regression component in the loss function to optimize (regarding the probabilities of each class instead only the labels), I don't understand where I should look for. I tried to explore some <a href=""https://github.com/explosion/spaCy/blob/master/spacy/pipeline/pipes.pyx"" rel=""nofollow noreferrer"">spaCy code</a> but there is only a function to get the loss. </p>

<p>If someone know where to look for to visualize the loss function and change it (by writing a subclass for instance) it would be nice !</p>

<p>Thanks</p>

<p>Arnault</p>
",Training and Model Evaluation,custom loss function spacy textcat looking around would like know possible modify customize loss function spacy textcategorizer mean want distill model instance bert want add regression component loss function optimize regarding probability class instead label understand look tried explore spacy code function get loss someone know look visualize loss function change writing subclass instance would nice thanks arnault
Memory failure while trying to train a logistic regression model,"<p>I'm getting this failure:</p>

<p>File ""C:\Users\ophirbh\AppData\Roaming\Python\Python38\site-packages\scipy\special_logsumexp.py"", line 112, in logsumexp
    tmp = np.exp(a - a_max)
MemoryError: Unable to allocate array with shape (950028, 45) and data type float64</p>

<p>for that code:</p>

<pre><code>from sklearn import datasets
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

X, y = datasets.load_svmlight_file('converted_features')
clf = LogisticRegression(random_state = 0, solver = 'lbfgs', multi_class = 
'multinomial',max_iter=2).fit(X,y)
print(cross_val_score(clf, X, y, scoring='recall_macro', cv = 5))
</code></pre>

<p>Is there any way around it?</p>
",Training and Model Evaluation,memory failure trying train logistic regression model getting failure file c user ophirbh appdata roaming python python site package scipy special logsumexp py line logsumexp tmp np exp max memoryerror unable allocate array shape data type float code way around
Is there any specific metric or method to drop tails of TF IDF vocabulary?,"<p>I have a TF IDF vocabulary I already get from gensim or tfidfvectorizer. Is there any specific metric or method to drop tails of TF IDF vocabulary?  I mean tails at Zipf diagram. How to visualize it?
I would like to see how accuracy changes when I drop number of words in vocabulary. For instance, I have vocabulary that has 175000 of words. </p>
",Training and Model Evaluation,specific metric method drop tail tf idf vocabulary tf idf vocabulary already get gensim tfidfvectorizer specific metric method drop tail tf idf vocabulary mean tail zipf diagram visualize would like see accuracy change drop number word vocabulary instance vocabulary ha word
Proper way to extract embedding weights for CBOW model?,"<p>I'm currently trying to implement the CBOW model on managed to get the training and testing, but am facing some confusion as to the ""proper"" way to finally extract the weights from the model to use as our word embeddings.</p>

<h2>Model</h2>

<pre><code>class CBOW(nn.Module):
    def __init__(self, config, vocab):
        self.config = config # Basic config file to hold arguments.
        self.vocab = vocab
        self.vocab_size = len(self.vocab.token2idx)
        self.window_size = self.config.window_size

        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.config.embed_dim)
        self.linear = nn.Linear(in_features=self.config.embed_dim, out_features=self.vocab_size)

    def forward(self, x):
        x = self.embed(x)
        x = torch.mean(x, dim=0) # Average out the embedding values.
        x = self.linear(x)

        return x
</code></pre>

<h2>Main process</h2>

<p>After I run my model through a Solver with the training and testing data, I basically told the <code>train</code> and <code>test</code> functions to also return the model that's used. Then I assigned the embedding weights to a separate variable and used those as the word embeddings.</p>

<p>Training and testing was conducted using cross entropy loss, and each training and testing sample is of the form <code>([context words], target word)</code>.</p>

<pre><code>def run(solver, config, vocabulary):
    for epoch in range(config.num_epochs):
        loss_train, model_train = solver.train()
        loss_test, model_test = solver.test()

        embeddings = model_train.embed.weight
</code></pre>

<p>I'm not sure if this is the correct way of going about extracting and using the embeddings. Is there usually another way to do this? Thanks in advance.</p>
",Training and Model Evaluation,proper way extract embedding weight cbow model currently trying implement cbow model managed get training testing facing confusion proper way finally extract weight model use word embeddings model main process run model solver training testing data basically told function also return model used assigned embedding weight separate variable used word embeddings training testing wa conducted using cross entropy loss training testing sample form sure correct way going extracting using embeddings usually another way thanks advance
Using trained model to predict classes with data of different input_shape,"<p>I have a saved model I trained on a small text (messaging) data corpus, and I'm trying to use that same model to predict either positive or negative sentiment (i.e. binary classification) on another corpus. I based the NLP model on a GOOGLE dev ML guide, which you can review here (if you think it useful - I used option A for all). </p>

<p>I keep getting an input shape error, I know that the error means I have to reshape the input to fit the expected shape. However, the data I want to predict on is not of this size. The error statement is:</p>

<pre><code>ValueError: Error when checking input: expected dropout_8_input to have shape (519,) but got array with shape (184,)
</code></pre>

<p>The reason why the model expects the shape (519,) is because during training the corpus fed into the first dropout layer (in TfidfVectorized form) is <code>print(x_train.shape) #(454, 519)</code>.</p>

<p>I'm new to ML, but it doesn't make sens to me that all the data I try to predict on after optimizing a model should be the same shape as the data that was used to train the model. 
<strong>Has anyone experienced an issue similar to this? Is there something that I'm missing, in how to train the model so that a different sized input can be predicted on? Or, am I misunderstanding on how models are to be used for class prediction?</strong></p>

<p>I am basing myself on the following functions for model training:</p>

<pre><code>from tensorflow.python.keras import models
from tensorflow.python.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.python.keras.layers import Convolution2D, MaxPooling2D

def mlp_model(layers, units, dropout_rate, input_shape, num_classes):
    """"""Creates an instance of a multi-layer perceptron model.

    # Arguments
        layers: int, number of `Dense` layers in the model.
        units: int, output dimension of the layers.
        dropout_rate: float, percentage of input to drop at Dropout layers.
        input_shape: tuple, shape of input to the model.
        num_classes: int, number of output classes.

    # Returns
        An MLP model instance.
    """"""
    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)
    model = models.Sequential()
    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))

#     print(input_shape)

    for _ in range(layers-1):
        model.add(Dense(units=units, activation='relu'))
        model.add(Dropout(rate=dropout_rate))

    model.add(Dense(units=op_units, activation=op_activation))
    return mode





def train_ngram_model(data,
                      learning_rate=1e-3,
                      epochs=1000,
                      batch_size=128,
                      layers=2,
                      units=64,
                      dropout_rate=0.2):
    """"""Trains n-gram model on the given dataset.

    # Arguments
        data: tuples of training and test texts and labels.
        learning_rate: float, learning rate for training model.
        epochs: int, number of epochs.
        batch_size: int, number of samples per batch.
        layers: int, number of `Dense` layers in the model.
        units: int, output dimension of Dense layers in the model.
        dropout_rate: float: percentage of input to drop at Dropout layers.

    # Raises
        ValueError: If validation data has label values which were not seen
            in the training data.

    # Reference
        For tuning hyperparameters, please visit the following page for
        further explanation of each argument:
        https://developers.google.com/machine-learning/guides/text-classification/step-5
    """"""
    # Get the data.
    (train_texts, train_labels), (val_texts, val_labels) = data

    # Verify that validation labels are in the same range as training labels.
    num_classes = get_num_classes(train_labels)
    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]
    if len(unexpected_labels):
        raise ValueError('Unexpected label values found in the validation set:'
                         ' {unexpected_labels}. Please make sure that the '
                         'labels in the validation set are in the same range '
                         'as training labels.'.format(
                             unexpected_labels=unexpected_labels))

    # Vectorize texts.
    x_train, x_val = ngram_vectorize(
        train_texts, train_labels, val_texts)

    # Create model instance.
    model = mlp_model(layers=layers,
                                  units=units,
                                  dropout_rate=dropout_rate,
                                  input_shape=x_train.shape[1:],
                                  num_classes=num_classes) 
                                # num_classes determine which activation fn to use

    # Compile model with learning parameters.
    if num_classes == 2:
        loss = 'binary_crossentropy'
    else:
        loss = 'sparse_categorical_crossentropy'
    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)
    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])

    # Create callback for early stopping on validation loss. If the loss does
    # not decrease in two consecutive tries, stop training.
    callbacks = [tf.keras.callbacks.EarlyStopping(
        monitor='val_loss', patience=2)]

    # Train and validate model.
    history = model.fit(
            x_train,
            train_labels,
            epochs=epochs,
            callbacks=callbacks,
            validation_data=(x_val, val_labels),
            verbose=2,  # Logs once per epoch.
            batch_size=batch_size)

    # Print results.
    history = history.history
    print('Validation accuracy: {acc}, loss: {loss}'.format(
            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))

    # Save model.
    model.save('MCTR2.h5')
    return history['val_acc'][-1], history['val_loss'][-1]
</code></pre>

<p>From this I get the architecture of the model to be:</p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dropout (Dropout)            (None, 519)               0         
_________________________________________________________________
dense (Dense)                (None, 64)                33280     
_________________________________________________________________
dropout_1 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 65        
=================================================================
Total params: 33,345
Trainable params: 33,345
Non-trainable params: 0
_________________________________________________________________
</code></pre>
",Training and Model Evaluation,using trained model predict class data different input shape saved model trained small text messaging data corpus trying use model predict either positive negative sentiment e binary classification another corpus based nlp model google dev ml guide review think useful used option keep getting input shape error know error mean reshape input fit expected shape however data want predict size error statement reason model expects shape training corpus fed first dropout layer tfidfvectorized form new ml make sen data try predict optimizing model shape data wa used train model ha anyone experienced issue similar something missing train model different sized input predicted misunderstanding model used class prediction basing following function model training get architecture model
BERT Model Evaluation Measure in terms of Syntax Correctness and Semantic Coherence,"<p>For example I have an original sentence. The  word <strong>barking</strong> corresponds to the word that is missing.</p>

<pre><code>Original Sentence : The dog is barking.
Incomplete Sentence : The dog is ___________.
</code></pre>

<p>For example, using the BERT model, it predicts the word crying instead of the word barking.
How will I measure the accuracy of the BERT Model in terms of how syntactically correct  and semantically coherent the predicted word is? </p>

<p>(For an instance, there are a lot of incomplete sentences, and the task is to evaluate BERT accuracy based on these incomplete sentences.)Please help.</p>
",Training and Model Evaluation,bert model evaluation measure term syntax correctness semantic coherence example original sentence word barking corresponds word missing example using bert model predicts word cry instead word barking measure accuracy bert model term syntactically correct semantically coherent predicted word instance lot incomplete sentence task evaluate bert accuracy based incomplete sentence please help
How to predict the labels in a test data set using the train data set where label is already defined in train data set?,"<p>Suppose we have a train dataset where the columns are bathrooms, bedrooms, sqft living, view and some other features, and <strong>price</strong> is defined there.
In a Test data set all the above features are there except the price.
How come we can predict the price from the data set ?</p>
",Training and Model Evaluation,predict label test data set using train data set label already defined train data set suppose train dataset column bathroom bedroom sqft living view feature price defined test data set feature except price come predict price data set
CNN - Confusion Matrix wrong display,"<p>I have trained a model for handwritten digits multiclass classification using CNN in Keras. I am trying to evaluate the model with the same training images to get an estimate of the accuracy of the algorithm; however, when I evaluate the CNN confusion matrix, it gives a one column only of the form:</p>

<pre><code>[[4132    0    0    0    0    0    0    0    0    0]
 [4684    0    0    0    0    0    0    0    0    0]
 [4177    0    0    0    0    0    0    0    0    0]
 [4351    0    0    0    0    0    0    0    0    0]
 [4072    0    0    0    0    0    0    0    0    0]
 [3795    0    0    0    0    0    0    0    0    0]
 [4137    0    0    0    0    0    0    0    0    0]
 [4401    0    0    0    0    0    0    0    0    0]
 [4063    0    0    0    0    0    0    0    0    0]
 [4188    0    0    0    0    0    0    0    0    0]]
</code></pre>

<p>I guess the algorithm is giving the correct result since those are the total numbers of each digit in the database; however, the confusion matrix should give something like this:</p>

<pre><code>[[4132    0    0    0    0    0    0    0    0    0]
 [   0 4684    0    0    0    0    0    0    0    0]
 [   0    0 4177    0    0    0    0    0    0    0]
 [   0    0    0 4351    0    0    0    0    0    0]
 [   0    0    0    0 4072    0    0    0    0    0]
 [   0    0    0    0    0 3795    0    0    0    0]
 [   0    0    0    0    0    0 4137    0    0    0]
 [   0    0    0    0    0    0    0 4401    0    0]
 [   0    0    0    0    0    0    0    0 4063    0]
 [   0    0    0    0    0    0    0    0    0 4188]]
</code></pre>

<p><a href=""https://github.com/elopezfune/Number_Digit_CNN"" rel=""nofollow noreferrer"">The code is in this link</a> </p>

<p><a href=""https://www.kaggle.com/c/digit-recognizer/data"" rel=""nofollow noreferrer"">The data can be taken from the ""train.csv"" file in this Kaggle project.</a></p>

<p>I would like to ask you guys what am I doing wrong in the code, such that I obtain this weird result.</p>
",Training and Model Evaluation,cnn confusion matrix wrong display trained model handwritten digit multiclass classification using cnn kera trying evaluate model training image get estimate accuracy algorithm however evaluate cnn confusion matrix give one column form guess algorithm giving correct result since total number digit database however confusion matrix give something like code link data taken train csv file kaggle project would like ask guy wrong code obtain weird result
Fastest way to retrieve word vectors of a sequence and fed into model?,"<p>For training, I have to feed the model sequence of word vector. Each sequence has on average 40 words. So, if I use a dictionary of pre-trained word embedding (like Glove), For each sequence have to hit the embedding dictionary around 40 times and for each batch, it will be around <code>batch_size*40</code> times. The dataset is divided into many batches and the whole dataset has to be iterate (epoch) several times also. So, you can imagine how many times the dictionary will get hit.
This is the approach I have done already and it is taking really a lot of time. </p>

<p>To solve this, I tried to make a dictionary of sequence to vector. This dictionary should contain a <code>sequence</code> as key and a 2d python list (each row is a word vector) as a value of the key. The hope is to, I just have to look for the sequence and get the values. This should decrease the time a lot but the dictionary would be very big (I estimated the size by saved the data (sequence->vectors) in mongodb and exported it and the file is 23gb). A dictionary of size 23gb should not be problem because my I am using shared server where I can allocate as much as 100gb memory. But the program gets <code>killed</code> while loading the dictionary. So this is not working. </p>

<p>Another approach I am thinking about is to copy the word embedding vector into pytorch's nn.Embedding().</p>

<pre><code>input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])
embedding(input)` 
</code></pre>

<p>Here the numbers are indices of the word. Regarding this approach,pytorch embedding uses numpy matrix as lookup table. So, my concern is, isn’t for executing the previous code, there will be 7 hits on the numpy matrix? Or it will be retrieved parallelly? Even it runs parallelly, there should be another dictionary to convert word to indices. That also needs 7 hits on the word2indices dictionary.</p>

<p>So, what do you think, what is the fastest and efficient way to retrieve word vectors of a sequence and fed into model?</p>
",Training and Model Evaluation,fastest way retrieve word vector sequence fed model training feed model sequence word vector sequence ha average word use dictionary pre trained word embedding like glove sequence hit embedding dictionary around time batch around time dataset divided many batch whole dataset ha iterate epoch several time also imagine many time dictionary get hit approach done already taking really lot time solve tried make dictionary sequence vector dictionary contain key python list row word vector value key hope look sequence get value decrease time lot dictionary would big estimated size saved data sequence vector mongodb exported file gb dictionary size gb problem using shared server allocate much gb memory program get loading dictionary working another approach thinking copy word embedding vector pytorch nn embedding number index word regarding approach pytorch embedding us numpy matrix lookup table concern executing previous code hit numpy matrix retrieved parallelly even run parallelly another dictionary convert word index also need hit word index dictionary think fastest efficient way retrieve word vector sequence fed model
Laplace smooth with trigrams,"<p>I have a trigram model with Laplace smoothing ,
my training sentence is     <code>A cat sat on the mat. A fat cat sat on the mat. A rat sat on the mat. The rat sat on the cat. A bat spat on the rat that sat on the cat on the mat.</code></p>

<p>I want to evaluate the test sentence <code>a cat sat on the mat</code> (evaluation means calculating the likelihood).
the answer Is log(likelihood) = -4.297.(the answer is from my prof).
the problem is i cant figure out the calculation.
what I have tried is this : 
<code>log(p(a))+log(cat|a)+log(sat|a cat)+log(on|cat sat)+log(the|sat on)+log(mat|on the)=
-1.0492   + -0.9030  +  -0.8129     +   -0.845      +   -0.452      + -0.5797</code>
can some one help me figure out what I am doing wrong?</p>
",Training and Model Evaluation,laplace smooth trigram trigram model laplace smoothing training sentence want evaluate test sentence evaluation mean calculating likelihood answer log likelihood answer prof problem cant figure calculation tried one help figure wrong
Difference between context-sensitive tensors and word vectors,"<p>I am currently working in python with spacy and there are different pre-trained models like the en_core_web_sm or the en_core_web_md. One of them is using words vectors to find word similarity and the other one is using context-sensitive tensors. 
What is the difference between using context-sensitive tensors and using word vectors? And what is context-senstiive tensors exactly?</p>
",Training and Model Evaluation,difference context sensitive tensor word vector currently working python spacy different pre trained model like en core web sm en core web md one using word vector find word similarity one using context sensitive tensor difference using context sensitive tensor using word vector context senstiive tensor exactly
Deep Learning Model to Predict Clicks from Keywords,"<p><a href=""https://i.sstatic.net/4JTQg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4JTQg.png"" alt=""enter image description here""></a></p>

<p>I have a dataset of keywords and clicks.
I'm trying to build a model where it takes in a phrase of keyword ( not more than 5 words, eg: mechanical engineer ) and outputs a value (like clicks, eg: 56). I'm using the bag of words approach which resulted in about 40% accuracy which is not good enough. Can I get some opinions on what approach you would take to improve the accuracy?</p>

<p>Or perhaps my approach is wrong ?</p>

<p>After cleaning,
Here's my code:</p>

<pre><code>words = []

for row in df['Keyword']:
    row = nltk.word_tokenize(row)
    for i in row:
        words.append(i)
words = sorted(list(set(words)))
training = []
for x in df['Keyword']:
    bag = []
    wrds = nltk.word_tokenize(x)
    for w in words:
        if w in wrds:
            bag.append(1)
        else:
            bag.append(0)
    training.append(bag)

model = keras.Sequential()
inputs = keras.Input(shape=(858,))
x = layers.Embedding(858, 8, input_length=5)(inputs)
x = layers.Flatten()(x)
outputs = layers.Dense(1, activation='relu')(x)
model = keras.Model(inputs=inputs, outputs=outputs, name='my_model')
model.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])
history = model.fit(X_train, Y_train,
                    batch_size=50,
                    epochs=20,
                    validation_split=0.2,
                   verbose = 1)

</code></pre>

<p>Here's a sample output of my X_train and Y_train. </p>

<p>X_train:</p>

<pre><code>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
</code></pre>

<p>Y_train:</p>

<pre><code>257.43
</code></pre>

<p>I have about 330k samples.</p>

<p>Any input in appreciated. Thanks</p>
",Training and Model Evaluation,deep learning model predict click keywords dataset keywords click trying build model take phrase keyword word eg mechanical engineer output value like click eg using bag word approach resulted accuracy good enough get opinion approach would take improve accuracy perhaps approach wrong cleaning code sample output x train train x train train k sample input appreciated thanks
Finding both target and center word2vec matrices,"<p>I've read and heard(In the CS224 of Stanford) that the Word2Vec algorithm actually trains two matrices(that is, two sets of vectors.) These two are the U and the V set, one for words being a target and one for words being the context. The final output is the average of these two.
I have two questions in mind. one is that:  </p>

<ul>
<li><p>Why do we get an average of two vectors? Why it makes sense? Don't we lose some information?  </p></li>
<li><p>The second question is, using pre-trained word2vec models, how can I get access to both matrices? Is there any downloadable word2vec with both sets of vectors? I don't have enough resources to train a new one.</p></li>
</ul>

<p>Thanks</p>
",Training and Model Evaluation,finding target center word vec matrix read heard c stanford word vec algorithm actually train two matrix two set vector two u v set one word target one word context final output average two two question mind one get average two vector make sense lose information second question using pre trained word vec model get access matrix downloadable word vec set vector enough resource train new one thanks
What are the input and the output of the Transformer?,"<p>I have questions about google implementation of the Transformer <a href=""https://www.tensorflow.org/beta/tutorials/text/transformer"" rel=""nofollow noreferrer"">here</a>.</p>

<ul>
<li><p>In <code>train_step(input, tar)</code> function: the <code>inp</code> dimension is a 256*40 tensor and the transformer returns a 256*39*8089 tensor. Is each row in <code>inp</code> a sentence?  I expected Transformer to take a batch of sentences (a batch_size of 2D matrix in which each row is a word) and calculate attention weights and outputs at once and then pass them to decoder (see <a href=""http://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">here</a>. ). However, I cannot see that being implemented in the code.  </p></li>
<li><p>In <code>train_step(input, tar)</code> function: the ""predictions"" is a 256*39*8089 tensor. Is it [batch size, max number of words in a sentence, target vocab size]? How does loss_function calculate loss while this format is different from ```tar_real`` which is [256 * 39]? </p></li>
<li><p>In <code>def evaluate(inp_sentence)</code>: Why in each iteration it sends the Transformer the entire encoder input? What I expect is that the encoder calculates attention weights and output once and then inside the for loop we send the output of the attentions and the predictions so far.</p></li>
</ul>

<p>Thank you</p>
",Training and Model Evaluation,input output transformer question google implementation transformer function dimension tensor transformer return tensor row sentence expected transformer take batch sentence batch size matrix row word calculate attention weight output pas decoder see however see implemented code function prediction tensor batch size max number word sentence target vocab size doe loss function calculate loss format different iteration sends transformer entire encoder input expect encoder calculates attention weight output inside loop send output attention prediction far thank
Should I be using whole available data for training my deep learning model ? What are the pros and cons of using only a subset?,"<p>I have a very complex LSTM based neural network model which I'm training on Quora Duplicate Question pairs. There are approximately 400 000 sentence pairs in the original dataset. It would take a lot of processing power and computation time to train on the entire (or 80%) dataset. Would it be unwise if I choose a random subset of the dataset (say 8000 pairs only) for training and 2000 for testing? Would it have a severe impact on the performance? Is always ""more the data, better the model"" true? </p>
",Training and Model Evaluation,using whole available data training deep learning model pro con using subset complex lstm based neural network model training quora duplicate question pair approximately sentence pair original dataset would take lot processing power computation time train entire dataset would unwise choose random subset dataset say pair training testing would severe impact performance always data better model true
"Quora Question Pairs challenge, predict if two questions ask the same thing using binary cross entropy loss to evaluate the predicition","<p>I have a csv file containing pairs of questions from the Quora Question Pairs Challenge. For each pair there is a corresponding label that specifies whether the questions are the same or not. I want to create a method so that if we have unknown pairs of questions I can answer if they ask the same thing or not. The accuracy of the result should be determined with the use of binary cross entropy loss.</p>

<p>This is a project that I have to do about a course of Information Retrieval. The problem is that all the solutions that I have found so far include Machine Learning (e.g. Neural Networks) and we haven't been taught how to use any Machine Learning models in this course. How can I solve this problem without using any Machine Learning?</p>

<p>I thought about cleaning the data (e.g. stop word reomval and punctuation removal) calculating the tf-idf and then applying cosine similarity between the two pairs. Like this I can find how similar two questions that are already given are, without using the labels. However, how can I use the labels to my advantage and predict the similarity between two unknown pairs of questions with no Machine Learning, is there a simple way that I am missing?</p>
",Training and Model Evaluation,quora question pair challenge predict two question ask thing using binary cross entropy loss evaluate predicition csv file containing pair question quora question pair challenge pair corresponding label specifies whether question want create method unknown pair question answer ask thing accuracy result determined use binary cross entropy loss project course information retrieval problem solution found far include machine learning e g neural network taught use machine learning model course solve problem without using machine learning thought cleaning data e g stop word reomval punctuation removal calculating tf idf applying cosine similarity two pair like find similar two question already given without using label however use label advantage predict similarity two unknown pair question machine learning simple way missing
How to create a custom loss function in Keras that evaluates prediction after each epoch?,"<p>I'm working on a neural network in Keras that translates English sentences into a custom language. For this, I'd like to create a custom loss function that takes the prediction for each sentence and evaluates whether it complies with the grammar rules of the custom language and if not adds value to the standard loss function.</p>

<p><strong>How can I evaluate a tensor after each epoch but not during compilation?</strong></p>

<p>Below is my custom loss function. As during compilation of the model there is no batch yet, y_pred has the shape (None, x, y) and can't be evaluated to get the prediction. My idea to circumvent this was to assign a standard loss function during compilation and when batches arrive calculate the custom loss. Unfortunately the custom loss is never reached.</p>

<pre class=""lang-py prettyprint-override""><code>def custom_loss(tokenizer, punishment_rate):

    def compile_loss(y_true, y_pred):
        shape = K.int_shape(y_pred)
        #standard loss function
        loss = K.sparse_categorical_crossentropy(y_true, y_pred)

        if shape[0] is not None:
            #THIS is never reached and that's the problem
            prediction = logits_to_text(K.eval(y_pred), tokenizer)

            #test if prediction complies to grammar rules
            compileable = compiles(prediction) ^ 1

            compile_error = compileable * punishment_rate
            loss =  K.sparse_categorical_crossentropy(y_true, y_pred, axis=-1) * (1 + compile_error)

        return loss

    return compile_loss
</code></pre>

<p>Is there any workaround for evaluating a tensor only when it was filled with a batch? Or alternatively, change the loss function after compilation of the model via a callback without it having to recompile the model?</p>
",Training and Model Evaluation,create custom loss function kera evaluates prediction epoch working neural network kera translates english sentence custom language like create custom loss function take prediction sentence evaluates whether complies grammar rule custom language add value standard loss function evaluate tensor epoch compilation custom loss function compilation model batch yet pred ha shape none x evaluated get prediction idea circumvent wa assign standard loss function compilation batch arrive calculate custom loss unfortunately custom loss never reached workaround evaluating tensor wa filled batch alternatively change loss function compilation model via callback without recompile model
Prediction is identical for all input data in Multi-Label Classification (NLP),"<p>I'm trying to build a deep learning model to predict the top 5 probable movie genres, using movies' synopses as input. The movie genres I'm including in the data are 19, but regardless of test input, the model always predicts the same 5 movie genres. Below is my code building the model. However, the accuracy during fitting is 90%. Can you point me to the right direction as to what I'm doing wrong? </p>

<pre><code>from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense
from keras.layers import Flatten, LSTM
from keras.layers import GlobalMaxPooling1D
from keras.models import Model
from keras.layers.embeddings import Embedding
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.layers import Input
from keras.layers.merge import Concatenate

import pandas as pd
import numpy as np
import re

data = pd.read_csv('train.csv', encoding = 'utf-8')
#Create column with comma separated genres
data['genres_comma'] = data['genres'].str.split()
mlb = MultiLabelBinarizer()
#Create new dataframe with one hot encoded labels
train = pd.concat([
    data.drop(['genres', 'genres_comma'], 1),
    pd.DataFrame(mlb.fit_transform(data['genres_comma']), columns=mlb.classes_),
], 1)

genre_names = list(mlb.classes_)
genres = train.drop(['movie_id', 'synopsis'], 1)

def preprocess_text(sen):
    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', sen)

    # Single character removal
    sentence = re.sub(r""\s+[a-zA-Z]\s+"", ' ', sentence)

    # Removing multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)

    return sentence

X = []
sentences = list(train['synopsis'])
for sen in sentences:
    X.append(preprocess_text(sen))

y = genres.values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

#Convert text inputs into embedded vectors.
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train)

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

vocab_size = len(tokenizer.word_index) + 1

maxlen = 200

X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

#GloVe word embeddings to convert text inputs to their numeric counterparts
from numpy import asarray
from numpy import zeros

embeddings_dictionary = dict()

glove_file = open('glove.6B.100d.txt', encoding=""utf8"")

for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = asarray(records[1:], dtype='float32')
    embeddings_dictionary[word] = vector_dimensions
glove_file.close()

embedding_matrix = zeros((vocab_size, 100))
for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

#Model Creation
deep_inputs = Input(shape=(maxlen,))
embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(deep_inputs)
LSTM_Layer_1 = LSTM(128)(embedding_layer)
dense_layer_1 = Dense(19, activation='sigmoid')(LSTM_Layer_1)
model = Model(inputs=deep_inputs, outputs=dense_layer_1)

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])

print(model.summary())


history = model.fit(X_train, y_train, batch_size=128, epochs=5, verbose=1, validation_split=0.2)

score = model.evaluate(X_test, y_test, verbose=1)
</code></pre>
",Training and Model Evaluation,prediction identical input data multi label classification nlp trying build deep learning model predict top probable movie genre using movie synopsis input movie genre including data regardless test input model always predicts movie genre code building model however accuracy fitting point right direction wrong
Choosing a margin for contrastive loss in a siamese network,"<p>I'm building a siamese network for a metric-learning task, using a contrastive loss function, and I'm uncertain on how to set the 'margin' hyperparameter for the loss. </p>

<p>My inputs to the loss function are currently 1024-dimension dense embeddings from an RNN layer - Does the dimensionality of that input affect how I pick a margin? Should I use a dense layer to project it to a lower-dimensional space first? Any pointers on how to pick a specific margin value (or any relevant research) would be really appreciated! In case it matters, I'm using PyTorch. </p>
",Training and Model Evaluation,choosing margin contrastive loss siamese network building siamese network metric learning task using contrastive loss function uncertain set margin hyperparameter loss input loss function currently dimension dense embeddings rnn layer doe dimensionality input affect pick margin use dense layer project lower dimensional space first pointer pick specific margin value relevant research would really appreciated case matter using pytorch
How to parse the data after Bert embedding?,"<p>I am doing binary classification for title sentences in news. (To determinate whether the new is political biased)
I am using the Bert embedding from <a href=""https://pypi.org/project/bert-embedding/"" rel=""nofollow noreferrer"">https://pypi.org/project/bert-embedding/</a> to embedding training sentences (one raw one title sentence) in Dataframes then feed vectorised Data into logistic regression, but the output data shape from the Bert embedding doesn't support logistic regression model. How can I parse this to make it fit logistic regression model?</p>

<p>Before I used tifdVectorizer it works perfectly and the output is numpy array  like </p>

<pre><code>[[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
</code></pre>

<p>each row is vectorised data for one sentence and It's an array with size of 1903 
And I have 516 titles in training data.
The output shapes are:</p>

<pre><code>train_x.shape: (516, 1903) test_x.shape (129, 1903)
train_y.shape: (516,) test_y.shape (129,)
</code></pre>

<p>But after I switched into Bert_Embedding 
the output vector for ONE row is numpy array list like </p>

<pre><code>[list([array([ 9.79349554e-01, -7.06475616e-01 ...... ]dtype=float32),
 array([ ........ ],dtype=float32), ......................
 array([ ........ ],dtype=float32)]
</code></pre>

<p>the output shape is like:
    train_x.shape: (516, 1) test_x.shape (129, 1)
    train_y.shape: (516,) test_y.shape (129,)</p>

<pre><code> def transform_to_Bert(articles_file: str, classified_articles_file: str):
    df = get_df_from_articles_file(articles_file, classified_articles_file)
    df_train, df_test, _, _ = train_test_split(df, df.label, stratify=df.label, test_size=0.2)
    bert_embedding = BertEmbedding()
    df_titles_values=df_train.title.values.tolist()
    result_train = bert_embedding(df_titles_values)
    result_test = bert_embedding(df_test.title.values.tolist())
    train_x = pd.DataFrame(result_train, columns=['A', 'Vector'])
    train_x = train_x.drop(columns=['A'])

    test_x = pd.DataFrame(result_test, columns=['A', 'Vector'])
    test_x=test_x.drop(columns=['A'])
    test_x=test_x.values
    train_x=train_x.values
    print(test_x)
    print(train_x)
    train_y = df_train.label.values
    test_y = df_test.label.values
    return {'train_x': train_x, 'test_x': test_x, 'train_y': train_y, 'test_y': test_y, 'input_length': train_x.shape[1], 'vocab_size': train_x.shape[1]}
</code></pre>

<p>Column A is the original title string in the result. So I just drop it.</p>

<hr>

<p>Below is the code where I use tifd vectoriser which works for logistical model.</p>

<pre><code>def transform_to_tfid(articles_file: str, classified_articles_file: str):
    df = get_df_from_articles_file(articles_file, classified_articles_file)
    df_train, df_test, _, _ = train_test_split(df, df.label, stratify=df.label, test_size=0.2)
    vectorizer = TfidfVectorizer(stop_words='english', )
    vectorizer.fit(df_train.title)
    train_x= vectorizer.transform(df_train.title)
    train_x=train_x.toarray()
    print(type(train_x))
    print(train_x)
    test_x= vectorizer.transform(df_test.title)
    test_x=test_x.toarray()
    print(test_x)
    train_y = df_train.label.values
    test_y = df_test.label.values
    return {'train_x': train_x, 'test_x': test_x, 'train_y': train_y, 'test_y': test_y, 'input_length': train_x.shape[1], 'vocab_size': train_x.shape[1]}


model=LogisticRegression(solver='lbfgs')
model.fit(train_x, train_y)
</code></pre>

<p>the error is ValueError: setting an array element with a sequence.
I expected output shape from Bert: <code>train_x.shape: (516, 1) test_x.shape (129, 1)</code> is like that from tifd: <code>train_x.shape: (516, 1903) test_x.shape (129, 1903)</code>so that it fits the logistic model</p>
",Training and Model Evaluation,parse data bert embedding binary classification title sentence news determinate whether new political biased using bert embedding embedding training sentence one raw one title sentence dataframes feed vectorised data logistic regression output data shape bert embedding support logistic regression model parse make fit logistic regression model used tifdvectorizer work perfectly output numpy array like row vectorised data one sentence array size title training data output shape switched bert embedding output vector one row numpy array list like output shape like train x shape test x shape train shape test shape column original title string result drop code use tifd vectoriser work logistical model error valueerror setting array element sequence expected output shape bert like tifd fit logistic model
Ignore test features not present in training data,"<p>I have a problem where I am tasked with creating three classifiers (two ""out of box"", one ""optimized"") for predicting sentiment analysis using sklearn.</p>

<p>The instructions are to:</p>

<ol>
<li>Ingest the training set, train classifiers</li>
<li>Save the classifiers to disk</li>
<li>In a separate program, load the classifiers from disk</li>
<li>Predict using the test set</li>
</ol>

<p>Steps 1-3 are no problem and quite frankly work well, the issue is using <code>model.predict()</code>. I am using sklearn's <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer""><code>TfidfVectorizer</code></a>, which creates a feature vector from text. My issue lies in that the feature vector I create for the training set is <strong><em>different than the training vector that is created for the testing set</em></strong>, since the text that is being provided is different.</p>

<p>Below is an example from the <code>train.tsv</code> file...</p>

<pre><code>4|z8DDztUxuIoHYHddDL9zQ|So let me set the scene first, My church social group took a trip here last saturday. We are not your mothers church. The churhc is Community Church of Hope, We are the valleys largest GLBT church so when we desended upon Organ stop Pizza, in LDS land you know we look a little out of place. We had about 50 people from our church come and boy did we have fun.  There was a baptist church a couple rows down from us who didn't see it coming. Now we aren't a bunch of flamers frolicking around or anything but we do tend to get a little loud and generally have a great time. I did recognized some of the music  so I was able to sing along with those.  This is a great place to take anyone over 50.  I do think they might be washing dirtymob money or something since the business is cash only.........which I think caught a lot of people off guard including me.  The show starts at 530  so dont be late !!!!!!
:-----:|:-----:|:-----:
2|BIeDBg4MrEd1NwWRlFHLQQ|Decent but terribly inconsistent food. I've had some great dishes and some terrible ones, I love chaat and 3 out of 4 times it was great, but once it was just a fried greasy mess (in a bad way, not in the good way it usually is.) Once the matar paneer was great, once it was oversalted and the peas were just plain bad. I don't know how they do it, but it's a coinflip between good food and an oversalted overcooked bowl.  Either way, portions are generous.
4|NJHPiW30SKhItD5E2jqpHw|Looks aren't everything.......  This little divito looks a little scary looking, but like I've said before ""you can't judge a book by it's cover"".   Not necessarily the kind of place you will take your date (unless she's blind and hungry), but man oh man is the food ever good!   We have ordered breakfast, lunch, &amp; dinner, and it is all fantastico. They make home-made corn tortillas and several salsas. The breakfast burritos are out of this world and cost about the same as a McDonald's meal.   We are a family that eats out frequently and we are frankly tired of pretty places with below average food. This place is sure to cure your hankerin for a tasty Mexican meal.
2|nnS89FMpIHz7NPjkvYHmug|Being a creature of habit anytime I want good sushi I go to Tokyo Lobby.  Well, my group wanted to branch out and try something new so we decided on Sakana. Not a fan.  And what's shocking to me is this place was packed!  The restaurant opens at 5:30 on Saturday and we arrived at around 5:45 and were lucky to get the last open table.  I don't get it...  Messy rolls that all tasted the same.  We ordered the tootsie roll and the crunch roll, both tasted similar, except of course for the crunchy captain crunch on top.  Just a mushy mess, that was hard to eat.  Bland tempura.  No bueno.  I did, however, have a very good tuna poke salad, but I would not go back just for that.   If you want good sushi on the west side, or the entire valley for that matter, say no to Sakana and yes to Tokyo Lobby.
2|FYxSugh9PGrX1PR0BHBIw|I recently told a friend that I cant figure out why there is no good Mexican restaurants in Tempe. His response was what about MacAyo's? I responded with ""why are there no good Mexican food restaurants in Tempe?""  Seriously if anyone out there knows of any legit Mexican in Tempe let me know. And don't say restaurant Mexico!
</code></pre>

<p>Here is the <code>train.py</code> file:</p>

<pre><code>import nltk, re, pandas as pd
from nltk.corpus import stopwords
import sklearn, string
import numpy as np
from sklearn.neural_network import MLPClassifier
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from itertools import islice
import time
from joblib import dump, load

def ID_to_Num(arr):
    le = preprocessing.LabelEncoder()
    new_arr = le.fit_transform(arr)
    return new_arr

def Num_to_ID(arr):
    le = preprocessing.LabelEncoder()
    new_arr = le.inverse_transform(arr)
    return new_arr

def check_performance(preds, acts):
    preds = list(preds)
    acts = pd.Series.tolist(acts)
    right = 0
    total = 0
    for i in range(len(preds)):
        if preds[i] == acts[i]:
            right += 1
        total += 1

    return (right / total) * 100

# This function removes numbers from an array
def remove_nums(arr): 
    # Declare a regular expression
    pattern = '[0-9]'  
    # Remove the pattern, which is a number
    arr = [re.sub(pattern, '', i) for i in arr]    
    # Return the array with numbers removed
    return arr

# This function cleans the passed in paragraph and parses it
def get_words(para):   
    # Create a set of stop words
    stop_words = set(stopwords.words('english'))
    # Split it into lower case    
    lower = para.lower().split()
    # Remove punctuation
    no_punctuation = (nopunc.translate(str.maketrans('', '', string.punctuation)) for nopunc in lower)
    # Remove integers
    no_integers = remove_nums(no_punctuation)
    # Remove stop words
    dirty_tokens = (data for data in no_integers if data not in stop_words)
    # Ensure it is not empty
    tokens = [data for data in dirty_tokens if data.strip()]
    # Ensure there is more than 1 character to make up the word
    tokens = [data for data in tokens if len(data) &gt; 1]

    # Return the tokens
    return tokens 

def minmaxscale(data):
    scaler = MinMaxScaler()
    df_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)
    return df_scaled

# This function takes the first n items of a dictionary
def take(n, iterable):
    #https://stackoverflow.com/questions/7971618/python-return-first-n-keyvalue-pairs-from-dict
    #Return first n items of the iterable as a dict
    return dict(islice(iterable, n))

def main():

    tsv_file = ""filepath""
    csv_table=pd.read_csv(tsv_file, sep='\t', header=None)
    csv_table.columns = ['class', 'ID', 'text']

    s = pd.Series(csv_table['text'])
    new = s.str.cat(sep=' ')
    vocab = get_words(new)

    s = pd.Series(csv_table['text'])
    corpus = s.apply(lambda s: ' '.join(get_words(s)))

    csv_table['dirty'] = csv_table['text'].str.split().apply(len)
    csv_table['clean'] = csv_table['text'].apply(lambda s: len(get_words(s)))

    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(corpus)

    df = pd.DataFrame(data=X.todense(), columns=vectorizer.get_feature_names())

    result = pd.concat([csv_table, df], axis=1, sort=False)

    Y = result['class']

    result = result.drop('text', axis=1)
    result = result.drop('ID', axis=1)
    result = result.drop('class', axis=1)

    X = result

    mlp = MLPClassifier()
    rf = RandomForestClassifier()    
    mlp_opt = MLPClassifier(
        activation = 'tanh',
        hidden_layer_sizes = (1000,),
        alpha = 0.009,
        learning_rate = 'adaptive',
        learning_rate_init = 0.01,
        max_iter = 250,
        momentum = 0.9,
        solver = 'lbfgs',
        warm_start = False
    )    

    print(""Training Classifiers"")
    mlp_opt.fit(X, Y)
    mlp.fit(X, Y)
    rf.fit(X, Y)

    dump(mlp_opt, ""C:\\filepath\Models\\mlp_opt.joblib"")
    dump(mlp, ""C:\\filepath\\Models\\mlp.joblib"")
    dump(rf, ""C:\\filepath\\Models\\rf.joblib"")

    print(""Trained Classifiers"")

main()
</code></pre>

<p>And here is the <code>Tester.py</code> file:</p>

<pre><code>from nltk.corpus import stopwords
import sklearn, string, nltk, re, pandas as pd, numpy, time
from sklearn.neural_network import MLPClassifier
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from joblib import dump, load

def ID_to_Num(arr):
    le = preprocessing.LabelEncoder()
    new_arr = le.fit_transform(arr)
    return new_arr

def Num_to_ID(arr):
    le = preprocessing.LabelEncoder()
    new_arr = le.inverse_transform(arr)
    return new_arr

def check_performance(preds, acts):
    preds = list(preds)
    acts = pd.Series.tolist(acts)
    right = 0
    total = 0
    for i in range(len(preds)):
        if preds[i] == acts[i]:
            right += 1
        total += 1

    return (right / total) * 100

# This function removes numbers from an array
def remove_nums(arr): 
    # Declare a regular expression
    pattern = '[0-9]'  
    # Remove the pattern, which is a number
    arr = [re.sub(pattern, '', i) for i in arr]    
    # Return the array with numbers removed
    return arr

# This function cleans the passed in paragraph and parses it
def get_words(para):   
    # Create a set of stop words
    stop_words = set(stopwords.words('english'))
    # Split it into lower case    
    lower = para.lower().split()
    # Remove punctuation
    no_punctuation = (nopunc.translate(str.maketrans('', '', string.punctuation)) for nopunc in lower)
    # Remove integers
    no_integers = remove_nums(no_punctuation)
    # Remove stop words
    dirty_tokens = (data for data in no_integers if data not in stop_words)
    # Ensure it is not empty
    tokens = [data for data in dirty_tokens if data.strip()]
    # Ensure there is more than 1 character to make up the word
    tokens = [data for data in tokens if len(data) &gt; 1]

    # Return the tokens
    return tokens 

def minmaxscale(data):
    scaler = MinMaxScaler()
    df_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)
    return df_scaled

# This function takes the first n items of a dictionary
def take(n, iterable):
    #https://stackoverflow.com/questions/7971618/python-return-first-n-keyvalue-pairs-from-dict
    #Return first n items of the iterable as a dict
    return dict(islice(iterable, n))

def main():

    tsv_file = ""filepath\\dev.tsv""
    csv_table=pd.read_csv(tsv_file, sep='\t', header=None)
    csv_table.columns = ['class', 'ID', 'text']

    s = pd.Series(csv_table['text'])
    new = s.str.cat(sep=' ')
    vocab = get_words(new)

    s = pd.Series(csv_table['text'])
    corpus = s.apply(lambda s: ' '.join(get_words(s)))

    csv_table['dirty'] = csv_table['text'].str.split().apply(len)
    csv_table['clean'] = csv_table['text'].apply(lambda s: len(get_words(s)))

    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(corpus)

    df = pd.DataFrame(data=X.todense(), columns=vectorizer.get_feature_names())

    result = pd.concat([csv_table, df], axis=1, sort=False)

    Y = result['class']

    result = result.drop('text', axis=1)
    result = result.drop('ID', axis=1)
    result = result.drop('class', axis=1)

    X = result

    mlp_opt = load(""C:\\filepath\\Models\\mlp_opt.joblib"")
    mlp = load(""C:\\filepath\\Models\\mlp.joblib"")
    rf = load(""C:\\filepath\\Models\\rf.joblib"")

    print(""Testing Classifiers"")
    mlp_opt_preds = mlp_opt.predict(X)
    mlp_preds = mlp.predict(X)
    rf_preds = rf.predict(X)

    mlp_opt_performance = check_performance(mlp_opt_preds, Y)
    mlp_performance = check_performance(mlp_preds, Y)
    rf_performance = check_performance(rf_preds, Y)

    print(""MLP OPT PERF: {}"".format(mlp_opt_performance))
    print(""MLP PERF: {}"".format(mlp_performance))
    print(""RF PERF: {}"".format(rf_performance))

main()
</code></pre>

<p>What I end up with is an error:</p>

<pre><code>Testing Classifiers
Traceback (most recent call last):
  File ""Reader.py"", line 121, in &lt;module&gt;
    main()
  File ""Reader.py"", line 109, in main
    mlp_opt_preds = mlp_opt.predict(X)
  File ""C:\Users\Jerry\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py"", line 953, in predict
    y_pred = self._predict(X)
  File ""C:\Users\Jerry\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py"", line 676, in _predict
    self._forward_pass(activations)
  File ""C:\Users\Jerry\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py"", line 102, in _forward_pass
    self.coefs_[i])
  File ""C:\Users\Jerry\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\utils\extmath.py"", line 173, in safe_sparse_dot
    return np.dot(a, b)
**ValueError: shapes (2000,13231) and (12299,1000) not aligned: 13231 (dim 1) != 12299 (dim 0)**
</code></pre>

<p>I know the error is related to the differences in the feature vector
size -- since the vectors are being created from the text in the data.
I do not know enough about NLP or Machine Learning to devise a
solution to workaround this problem. How can I create a way to have
the model predict using the feature sets in the test data?</p>

<p>I tried making edits per answers below to save the feature vector:</p>

<p><code>Train.py</code> now looks like:</p>

<pre><code>import nltk, re, pandas as pd
from nltk.corpus import stopwords
import sklearn, string
import numpy as np
from sklearn.neural_network import MLPClassifier
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from itertools import islice
import time
import pickle
from joblib import dump, load

def ID_to_Num(arr):
    le = preprocessing.LabelEncoder()
    new_arr = le.fit_transform(arr)
    return new_arr

def Num_to_ID(arr):
    le = preprocessing.LabelEncoder()
    new_arr = le.inverse_transform(arr)
    return new_arr

def check_performance(preds, acts):
    preds = list(preds)
    acts = pd.Series.tolist(acts)
    right = 0
    total = 0
    for i in range(len(preds)):
        if preds[i] == acts[i]:
            right += 1
        total += 1

    return (right / total) * 100

# This function removes numbers from an array
def remove_nums(arr): 
    # Declare a regular expression
    pattern = '[0-9]'  
    # Remove the pattern, which is a number
    arr = [re.sub(pattern, '', i) for i in arr]    
    # Return the array with numbers removed
    return arr

# This function cleans the passed in paragraph and parses it
def get_words(para):   
    # Create a set of stop words
    stop_words = set(stopwords.words('english'))
    # Split it into lower case    
    lower = para.lower().split()
    # Remove punctuation
    no_punctuation = (nopunc.translate(str.maketrans('', '', string.punctuation)) for nopunc in lower)
    # Remove integers
    no_integers = remove_nums(no_punctuation)
    # Remove stop words
    dirty_tokens = (data for data in no_integers if data not in stop_words)
    # Ensure it is not empty
    tokens = [data for data in dirty_tokens if data.strip()]
    # Ensure there is more than 1 character to make up the word
    tokens = [data for data in tokens if len(data) &gt; 1]

    # Return the tokens
    return tokens 

def minmaxscale(data):
    scaler = MinMaxScaler()
    df_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)
    return df_scaled

# This function takes the first n items of a dictionary
def take(n, iterable):
    #https://stackoverflow.com/questions/7971618/python-return-first-n-keyvalue-pairs-from-dict
    #Return first n items of the iterable as a dict
    return dict(islice(iterable, n))

def main():

    tsv_file = ""filepath\\train.tsv""
    csv_table=pd.read_csv(tsv_file, sep='\t', header=None)
    csv_table.columns = ['class', 'ID', 'text']

    s = pd.Series(csv_table['text'])
    new = s.str.cat(sep=' ')
    vocab = get_words(new)

    s = pd.Series(csv_table['text'])
    corpus = s.apply(lambda s: ' '.join(get_words(s)))

    csv_table['dirty'] = csv_table['text'].str.split().apply(len)
    csv_table['clean'] = csv_table['text'].apply(lambda s: len(get_words(s)))

    vectorizer = TfidfVectorizer()
    test = vectorizer.fit_transform(corpus)

    df = pd.DataFrame(data=test.todense(), columns=vectorizer.get_feature_names())

    result = pd.concat([csv_table, df], axis=1, sort=False)

    Y = result['class']

    result = result.drop('text', axis=1)
    result = result.drop('ID', axis=1)
    result = result.drop('class', axis=1)

    X = result

    mlp = MLPClassifier()
    rf = RandomForestClassifier()    
    mlp_opt = MLPClassifier(
        activation = 'tanh',
        hidden_layer_sizes = (1000,),
        alpha = 0.009,
        learning_rate = 'adaptive',
        learning_rate_init = 0.01,
        max_iter = 250,
        momentum = 0.9,
        solver = 'lbfgs',
        warm_start = False
    )    

    print(""Training Classifiers"")
    mlp_opt.fit(X, Y)
    mlp.fit(X, Y)
    rf.fit(X, Y)

    dump(mlp_opt, ""filepath\\Models\\mlp_opt.joblib"")
    dump(mlp, ""filepath\\Models\\mlp.joblib"")
    dump(rf, ""filepath\\Models\\rf.joblib"")
    pickle.dump(test, open(""filepath\\tfidf_vectorizer.pkl"", 'wb'))

    print(""Trained Classifiers"")

main()
</code></pre>

<p>And <code>Test.py</code> now looks like:</p>

<pre><code>from nltk.corpus import stopwords
import sklearn, string, nltk, re, pandas as pd, numpy, time
from sklearn.neural_network import MLPClassifier
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from joblib import dump, load
import pickle

def ID_to_Num(arr):
    le = preprocessing.LabelEncoder()
    new_arr = le.fit_transform(arr)
    return new_arr

def Num_to_ID(arr):
    le = preprocessing.LabelEncoder()
    new_arr = le.inverse_transform(arr)
    return new_arr

def check_performance(preds, acts):
    preds = list(preds)
    acts = pd.Series.tolist(acts)
    right = 0
    total = 0
    for i in range(len(preds)):
        if preds[i] == acts[i]:
            right += 1
        total += 1

    return (right / total) * 100

# This function removes numbers from an array
def remove_nums(arr): 
    # Declare a regular expression
    pattern = '[0-9]'  
    # Remove the pattern, which is a number
    arr = [re.sub(pattern, '', i) for i in arr]    
    # Return the array with numbers removed
    return arr

# This function cleans the passed in paragraph and parses it
def get_words(para):   
    # Create a set of stop words
    stop_words = set(stopwords.words('english'))
    # Split it into lower case    
    lower = para.lower().split()
    # Remove punctuation
    no_punctuation = (nopunc.translate(str.maketrans('', '', string.punctuation)) for nopunc in lower)
    # Remove integers
    no_integers = remove_nums(no_punctuation)
    # Remove stop words
    dirty_tokens = (data for data in no_integers if data not in stop_words)
    # Ensure it is not empty
    tokens = [data for data in dirty_tokens if data.strip()]
    # Ensure there is more than 1 character to make up the word
    tokens = [data for data in tokens if len(data) &gt; 1]

    # Return the tokens
    return tokens 

def minmaxscale(data):
    scaler = MinMaxScaler()
    df_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)
    return df_scaled

# This function takes the first n items of a dictionary
def take(n, iterable):
    #https://stackoverflow.com/questions/7971618/python-return-first-n-keyvalue-pairs-from-dict
    #Return first n items of the iterable as a dict
    return dict(islice(iterable, n))

def main():

    tfidf_vectorizer = pickle.load(open(""filepath\\tfidf_vectorizer.pkl"", 'rb'))

    tsv_file = ""filepath\\dev.tsv""
    csv_table=pd.read_csv(tsv_file, sep='\t', header=None)
    csv_table.columns = ['class', 'ID', 'text']

    s = pd.Series(csv_table['text'])
    new = s.str.cat(sep=' ')
    vocab = get_words(new)

    s = pd.Series(csv_table['text'])
    corpus = s.apply(lambda s: ' '.join(get_words(s)))

    csv_table['dirty'] = csv_table['text'].str.split().apply(len)
    csv_table['clean'] = csv_table['text'].apply(lambda s: len(get_words(s)))

    print(type(corpus))
    print(corpus.head())

    X = tfidf_vectorizer.transform(corpus)

    print(X)

    df = pd.DataFrame(data=X.todense(), columns=tfidf_vectorizer.get_feature_names())

    result = pd.concat([csv_table, df], axis=1, sort=False)

    Y = result['class']

    result = result.drop('text', axis=1)
    result = result.drop('ID', axis=1)
    result = result.drop('class', axis=1)

    X = result

    mlp_opt = load(""filepath\\Models\\mlp_opt.joblib"")
    mlp = load(""filepath\\Models\\mlp.joblib"")
    rf = load(""filepath\\Models\\rf.joblib"")

    print(""Testing Classifiers"")
    mlp_opt_preds = mlp_opt.predict(X)
    mlp_preds = mlp.predict(X)
    rf_preds = rf.predict(X)

    mlp_opt_performance = check_performance(mlp_opt_preds, Y)
    mlp_performance = check_performance(mlp_preds, Y)
    rf_performance = check_performance(rf_preds, Y)

    print(""MLP OPT PERF: {}"".format(mlp_opt_performance))
    print(""MLP PERF: {}"".format(mlp_performance))
    print(""RF PERF: {}"".format(rf_performance))

main()
</code></pre>

<p>But that yields:</p>

<pre><code>Traceback (most recent call last):
  File ""Filepath\Reader.py"", line 128, in &lt;module&gt;
    main()
  File ""Filepath\Reader.py"", line 95, in main
    X = tfidf_vectorizer.transform(corpus)
  File ""C:\Users\Jerry\AppData\Local\Programs\Python\Python37\lib\site-packages\scipy\sparse\base.py"", line 689, in __getattr__
    raise AttributeError(attr + "" not found"")
AttributeError: transform not found
</code></pre>
",Training and Model Evaluation,ignore test feature present training data problem creating three classifier two box one optimized predicting sentiment analysis using sklearn instruction ingest training set train classifier save classifier disk separate program load classifier disk predict using test set step problem quite frankly work well issue using using sklearn creates feature vector text issue lie feature vector create training set different training vector created testing set since text provided different example file file file end error know error related difference feature vector size since vector created text data know enough nlp machine learning devise solution workaround problem create way model predict using feature set test data tried making edits per answer save feature vector look like look like yield
What should be the format of training data for Stanford NER CRF Classifier?,"<p>I am trying to train my own Address classifier model using Stanford <code>CRF-NER</code> but the performance is very low. I am confused about the format of the training data I have trained with. The training data is typically the list of districts, cities, provinces and their respective labels. But the model is not tagging the respective address tags to its tokens.</p>

<p>The format of the training data is as below:</p>

<ul>
<li>BARAT    PROVINCE</li>
<li>MALUKU    PROVINCE</li>
<li>MALUKU    PROVINCE</li>
<li>KABUPATEN REGENCY</li>
<li>SIMEULUE  REGENCY</li>
<li>KABUPATEN REGENCY</li>
<li>ACEH  REGENCY</li>
</ul>

<p>This is the just a sample of training data in csv format, There are 3 labels <strong>PROVINCE, REGENCY and DISTRICT</strong></p>

<p>Here is the output of tagged tokens:</p>

<p><a href=""https://i.sstatic.net/QQKCA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QQKCA.png"" alt=""OUTPUT of the Stanford NER Tgger""></a></p>

<p>You can all tokens has been tagged as DISTRICT though I have REGENCY, DISTRICT AND PROVINCE as labelled data.</p>

<p><strong>I wanted to know if my format of training data is correct is only works on contextual data at sentence level Since I saw Stanford <code>NER</code> working well on sentence level.</strong></p>
",Training and Model Evaluation,format training data stanford ner crf classifier trying train address classifier model using stanford performance low confused format training data trained training data typically list district city province respective label model tagging respective address tag token format training data barat province maluku province maluku province kabupaten regency simeulue regency kabupaten regency aceh regency sample training data csv format label province regency district output tagged token token ha tagged district though regency district province labelled data wanted know format training data correct work contextual data sentence level since saw stanford working well sentence level
Seq to Seq model training,"<p>I have couple of questions:</p>

<ol>
<li>In a seq to seq model with varying input length, if you don't use the attention mask the RNN may end up computing the hidden state value for padded element? So thus it mean attention mask is mandatory else my output will be wrong?</li>
<li>How to deal with varying length labels then, let's say I have padded for passing it in batch. Now I don't want my padded elements to have an impact on my loss, so how do I ignore that? </li>
</ol>
",Training and Model Evaluation,seq seq model training couple question seq seq model varying input length use attention mask rnn may end computing hidden state value padded element thus mean attention mask mandatory else output wrong deal varying length label let say padded passing batch want padded element impact loss ignore
How to calculate the recall and precision in Entity Linking?,"<p>I am pretty confused about how to calculate the recall and precision in the process of entity linking. When we disambiguate an entity to the correct sense(T) or incorrect sense(F), we only have this two situation. But how can we get those four situations: TP, TN, FP, FN ?</p>
",Training and Model Evaluation,calculate recall precision entity linking pretty confused calculate recall precision process entity linking disambiguate entity correct sense incorrect sense f two situation get four situation tp tn fp fn
Automatic &quot;Distractor Generation&quot; for Multiple Choice Questions using Machine Learning,"<p>I have three columns in my train dataset - Question, Answer and Distractor.</p>

<p>As we know, MCQ has a question and 4 options. Out of that 1 answer is correct and other 3 are distractor.</p>

<p>I have two columns in my test set - Question and Answer. And I need to predict Distractor.</p>

<p>Please suggest how to deal with the problem.</p>

<p><a href=""https://i.sstatic.net/ebgcC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ebgcC.png"" alt=""enter image description here""></a></p>
",Training and Model Evaluation,automatic distractor generation multiple choice question using machine learning three column train dataset question answer distractor know mcq ha question option answer correct distractor two column test set question answer need predict distractor please suggest deal problem
SpaCy: Do I need to perform early stopping while training the model for custom entities?,"<p>I have divided my data into training and testing.</p>

<p><a href=""https://spacy.io/usage/training#ner"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#ner</a></p>

<p>As per the code snippet given by spacy for training the custom entities, it seems like there is no early stopping. So I have a question here??</p>

<p>Should I write a custom code which performs the following set of things after every iteration:
1. Iteration completed.
2. Check the model accuracy on the testing data.
3. If the accuracy is more than the previous model then save it else continue.
4. Perform the next iteration.</p>

<p>Or I the final model after completing all the iteration for example 30 iteration is the best model??</p>

<p>Sample output of my custom code:
<a href=""https://i.sstatic.net/XDYYY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XDYYY.png"" alt=""enter image description here""></a></p>

<p>As per the above output is it right to say the best model is at iteration no 13?</p>
",Training and Model Evaluation,spacy need perform early stopping training model custom entity divided data training testing per code snippet given spacy training custom entity seems like early stopping question write custom code performs following set thing every iteration iteration completed check model accuracy testing data accuracy previous model save else continue perform next iteration final model completing iteration example iteration best model sample output custom code per output right say best model iteration
Can I suggest top 5 classes for a new data using Multi-class Classification?,"<p>I have a dataset of tickets the machine gets for some machine components failure. The ticket are in text form. </p>

<p>For each failure we have around 8-10 diagnosis labels. This tells what the issue might have been and being used to fix the ticket. </p>

<p>Now in the training data I have one ticket and 1 diagnosis label. So it's a Multi-class training data. </p>

<p>After training a multi-class ML model, given a new text issue, can I suggest top 5 diagnosis for that possible ticket based on ranking of probabilities? </p>

<p>My concern is this makes sense for multi-label data where training data also has multiple labels and u can put sigmoid activation in the end to get proper prob of each diagnosis working for that ticket. </p>

<p>But if training data is framed as multi-class (meaning one ticket had only one corresponding label to it), is it still ok for me to suggest multiple diagnosis labels as rank order? </p>

<p>Thanks</p>
",Training and Model Evaluation,suggest top class new data using multi class classification dataset ticket machine get machine component failure ticket text form failure around diagnosis label tell issue might used fix ticket training data one ticket diagnosis label multi class training data training multi class ml model given new text issue suggest top diagnosis possible ticket based ranking probability concern make sense multi label data training data also ha multiple label u put sigmoid activation end get proper prob diagnosis working ticket training data framed multi class meaning one ticket one corresponding label still ok suggest multiple diagnosis label rank order thanks
"How to train a simple, vanilla transformers translation model from scratch with Fairseq","<p>I have been familiarizing myself with the fairseq library recently, and have tried a couple of pretrained models. I thought that a good way to teach myself would be to train a plain vanilla transformers model with the data I have, and then I can modify and maybe add bells and whistles like pre-training from there. The fairseq documentation has an example of this with fconv architecture, and I basically would like to do the same with transformers.</p>

<p>Below is the code I tried:</p>

<p>In data preparation, I cleaned the data with moses script, tokenized words, and then applied BPE using subword-nmt, where I set number of BPE tokens to 15000.</p>

<p>For preprocessing:</p>

<pre><code>fairseq-preprocess --source-lang zh --target-lang en \
    --trainpref data/train --validpref data/valid --testpref data/test \
    --joined-dictionary \
    --destdir data-bin \
    --workers 20
</code></pre>

<p>For training:</p>

<pre><code>CUDA_VISIBLE_DEVICES=0,1,2,3

fairseq-train data-bin \
    --clip-norm 0.1 --dropout 0.2 --max-tokens 2048 \
    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \
    --lr 5e-4 --lr-scheduler inverse_sqrt \
    --criterion label_smoothed_cross_entropy \
    --lazy-load \
    --update-freq 4 \
    --keep-interval-updates 100 --save-interval-updates 3000  --log-interval 50 \
    --arch transformer --save-dir checkpoints/transformer
</code></pre>

<p>I trained this on a data set of ~19M samples, on 4 NVIDIA P100 GPUs, for about 8 hours -- at that point I had completed 1 epoch and a bit more. I tested this against my checkpoints -- for the first checkpoint at update 3000, the prediction was all ""the the the""s -- but that might be ok because it was just the first checkpoint. However, I then tested this against the last checkpoint, and the prediction was the same sentence for all test samples!! -- The prediction was ""committee on the peaceful uses of outer space"" for everything, and the BLEU score was 0. My test set is not at all about outer space. </p>

<p>So after this extremely disappointing result, I realized that I should ask for some pointers on creating a basic transformers model:</p>

<ul>
<li><p>First of all, is my result actually within expectation? The paper on which transformer.py is based, Jointly Learning to Align and Translate, stated that state of the art results are achieved on 64 Volta GPUs for 30k updates (!!!) -- my set up was much smaller, so maybe the result was expected? However, I have achieved better results in less time with less data, so I doubt that. Is it just that the learning rate was not set right so that it got stuck in some weird local minima? Or are there more things wrong with my setup above?</p></li>
<li><p>When would the above model stop? max_epoch and max_update are not required parameters and are set to math.inf when not given. From train.py, it looks like training goes on until learning rate gets to below args.min_lr, however I can't find where min_lr is set, and it is not a parameter in the documentation, so what is min_lr? Is it 0?</p></li>
<li><p>What is the best architecture to use for the ""vanilla"" transformer model that I'm looking for? </p></li>
</ul>

<p>Thank you!</p>
",Training and Model Evaluation,train simple vanilla transformer translation model scratch fairseq familiarizing fairseq library recently tried couple pretrained model thought good way teach would train plain vanilla transformer model data modify maybe add bell whistle like pre training fairseq documentation ha example fconv architecture basically would like transformer code tried data preparation cleaned data moses script tokenized word applied bpe using subword nmt set number bpe token preprocessing training trained data set sample nvidia p gpus hour point completed epoch bit tested checkpoint first checkpoint update prediction wa might ok wa first checkpoint however tested last checkpoint prediction wa sentence test sample prediction wa committee peaceful us outer space everything bleu score wa test set outer space extremely disappointing result realized ask pointer creating basic transformer model first result actually within expectation paper transformer py based jointly learning align translate stated state art result achieved volta gpus k update set wa much smaller maybe result wa expected however achieved better result le time le data doubt learning rate wa set right got stuck weird local minimum thing wrong setup would model stop max epoch max update required parameter set math inf given train py look like training go learning rate get args min lr however find min lr set parameter documentation min lr best architecture use vanilla transformer model looking thank
Evaluate Machine Learning Text Classifier,"<p>I have built a binary text classifier. Trained it to recognize sentences for clients based on 'New' or 'Return'. My issue is that real data may not always have a clear distinction between new or return, even to an actual person reading the sentence. 
My model was trained to 0.99% accuracy with supervised learning using Logistic Regression. </p>

<pre><code>#train model
def train_model(classifier, feature_vector_train, label, feature_vector_valid,valid_y, is_neural_net=False):
    classifier.fit(feature_vector_train, label)
    predictions = classifier.predict(feature_vector_valid)
    if is_neural_net:
        predictions = predictions.argmax(axis=-1)
    return classifier , metrics.accuracy_score(predictions, valid_y)

# Linear Classifier on Count Vectors
    model, accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xtest_count,test_y)
    print (':::  Accuracy on Test Set   :::')
    print ('Linear Classifier, BoW Vectors: ', accuracy)
</code></pre>

<p>And this would give me an accuracy of 0.998.
I now can pass a whole list of sentences to test this model and it would catch if the sentences has a <strong>new</strong> or <strong>return</strong> word yet I need an evaluation metric because some sentences will have no chance of being <strong>new</strong> or <strong>return</strong> as real data is messy as always. </p>

<p>My question is: <strong>What evaluation metrics can I use so that each new sentence that gets passed through the model shows a score?</strong>
Right now I only use the following code</p>

<pre><code>with open('realdata.txt', 'r') as f:
    samples = f.readlines()
vecs = count_vect.transform(sentence)
visit = model.predict(vecs)
num_to_label= {0:'New', 1:'Return'}
for s, p in zip(sentence, visit):
    #printing each sentence with the predicted label
    print(s + num_to_label[p])
</code></pre>

<p>For example I would expect</p>

<pre><code>Sentence                      Visit          (Metric X)
New visit 2nd floor           New             0.95
Return visit Evening          Return          0.98
Afternoon visit North         New             0.43
</code></pre>

<p>Therefore I'd know to <strong>not</strong> trust those will metrics below a certain percentage because the tool isnt reliable. </p>
",Training and Model Evaluation,evaluate machine learning text classifier built binary text classifier trained recognize sentence client based new return issue real data may always clear distinction new return even actual person reading sentence model wa trained accuracy supervised learning using logistic regression would give accuracy pas whole list sentence test model would catch sentence ha new return word yet need evaluation metric sentence chance new return real data messy always question evaluation metric use new sentence get passed model show score right use following code example would expect therefore know trust metric certain percentage tool isnt reliable
how to store custom attributes for token information in JSON and use for training,"<p>Using <a href=""https://spacy.io/api/goldparse#docs_to_json"" rel=""nofollow noreferrer"">gold.docs_to_json</a>, I am unable to store custom token attributes in JSON</p>

<pre><code>{
    ""id"": 0,
    ""paragraphs"": [
        {
            ""raw"": ""Complete the nlp task"",
            ""sentences"": [
                {
                    ""tokens"": [
                        {
                            ""id"": 0,
                            ""orth"": ""Complete"",
                            ""tag"": ""VB"",
                            ""head"": 0,
                            ""dep"": ""ROOT"",
                            ""ner"": ""O""
                        },
                        {
                            ""id"": 1,
                            ""orth"": ""the"",
                            ""tag"": ""DT"",
                            ""head"": 2,
                            ""dep"": ""det"",
                            ""ner"": ""O""
                        },
                        {
                            ""id"": 2,
                            ""orth"": ""nlp"",
                            ""tag"": ""NN"",
                            ""head"": 1,
                            ""dep"": ""compound"",
                            ""ner"": ""O""
                        },
                        {
                            ""id"": 3,
                            ""orth"": ""task"",
                            ""tag"": ""NN"",
                            ""head"": -3,
                            ""dep"": ""dobj"",
                            ""ner"": ""O""
                        }
                    ],
                    ""brackets"": []
                }
            ]
        }
    ]
}
</code></pre>

<p>How can I store custom attributes as well?
Also can I train the custom attributes with <code>spacy train</code> as well.</p>

<hr>

<p><strong>irrevelent details:</strong>
I know that a custom doc_to_json can be made which will store the custom attributes as well but If I am unable to train the model to label custom attributes then it is of no use.</p>
",Training and Model Evaluation,store custom attribute token information json use training using gold doc json unable store custom token attribute json store custom attribute well also train custom attribute well irrevelent detail know custom doc json made store custom attribute well unable train model label custom attribute use
how to compare or evaluate two different length strings characters by characters with redundant or missing characters in python?,"<p>I want to mention the question again first, the comparison could not be from the head of two strings, and the lengths of two strings are not equal, so stop say this question is a duplicate question by something like <code>strcmp()</code> like I'm blind.</p>

<p>I'd like to do some kinds of evaluations for license plate recognition.
For a license plate like <code>ABD-3875</code>, I might get the result like 
<code>A80-3875</code>, <code>1A80-3875</code> or <code>ABB0-38B75</code>.
I want to know something like</p>

<p><code>A80-3875</code>, <code>B</code> and <code>D</code> are wrong.</p>

<p><code>1A80-3875</code>, <code>1</code> shouldn't be exist and <code>D</code> is wrong</p>

<p><code>1ABB0-38B75</code>, 1, B B shouldn't be exist
the input should be two strings  like <code>ABD-3875</code>,<code>1A80-3875</code>
the output should be a boolean array : TFFTTTT
and a  int array to [-1, 1, 0, 0, 1, 1, 1, 1, 1]<br>
-1 means the extra character is predicted 
0 means the location is correct but the character is wrong
1 means the character is correct
I know Levenshtein distance helps on this question, but since the long history of computer science, is there any particulate solution or even an opensource library for this question?</p>
",Training and Model Evaluation,compare evaluate two different length string character character redundant missing character python want mention question first comparison could head two string length two string equal stop say question duplicate question something like like blind like kind evaluation license plate recognition license plate like might get result like want know something like wrong exist wrong b b exist input two string like output boolean array tfftttt int array mean extra character predicted mean location correct character wrong mean character correct know levenshtein distance help question since long history computer science particulate solution even opensource library question
Does LUIS keep previous training after importing a new app?,"<p>I have a Luis app “Webapp1”. I do experimental development and testing.</p>

<p>So, imagine the following scenario. I do a lot of development with that app, so it has been through an extensive number of training cycles, let’s say 1000 (is that referred to as “epochs” in Luis as well?).</p>

<p>I export the app by “Export -> export as JSON”. According to Microsoft ( <a href=""https://learn.microsoft.com/en-us/azure/cognitive-services/luis/luis-concept-version"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cognitive-services/luis/luis-concept-version</a>) <em>“The exported file does not contain machine-learned information because the app is retrained after it is imported.”</em>.</p>

<ol>
<li>If I create a new app with the exported file does that mean, I lose
all the training and start from cycle 0? </li>
<li>If I add a couple of
utterances and I do “import version” does that mean that I start
from cycle 0 or 1001?</li>
</ol>

<p>thank you</p>
",Training and Model Evaluation,doe luis keep previous training importing new app luis app webapp experimental development testing imagine following scenario lot development app ha extensive number training cycle let say referred epoch luis well export app export export json according microsoft exported file doe contain machine learned information app retrained imported create new app exported file doe mean lose training start cycle add couple utterance import version doe mean start cycle thank
"Quanteda package, Naive Bayes: How can I predict on different-featured test data?","<p>I used <code>quanteda::textmodel_NB</code> to create a model that categorizes text into one of two categories. I fit the model on a training data set of data from last summer.</p>

<p>Now, I am trying to use it this summer to categorize new text we get here at work. I tried doing this and got the following error:</p>

<pre><code>Error in predict.textmodel_NB_fitted(model, test_dfm) : 
feature set in newdata different from that in training set
</code></pre>

<p>The code in the function that generates the error <a href=""https://github.com/kbenoit/quanteda/blob/5f7e3d26dac85c20b53950faef3e90c4b77d745a/R/textmodel_NB.R"" rel=""nofollow noreferrer"">can be found here at lines 157 to 165.</a></p>

<p>I assume this occurs because the words in the training data set do not <em>exactly match</em> the words used in the test data set. But why does this error occur? I feel as if—to be useful in real-world examples—the model should be able to handle data sets that contain different features, as this is what will probably always happen in applied use.</p>

<p>So my first question is:</p>

<p><strong>1. Is this error a property of the naive Bayes algorithm? Or was it a choice made by the author of the function to do this?</strong></p>

<p>Which then leads me to my second question:</p>

<p><strong>2. How can I remedy this issue?</strong></p>

<p>To get at this second question, I provide reproducible code (the last line generates the error above):</p>

<pre><code>library(quanteda)
library(magrittr)
library(data.table)

train_text &lt;- c(""Can random effects apply only to categorical variables?"",
                ""ANOVA expectation identity"",
                ""Statistical test for significance in ranking positions"",
                ""Is Fisher Sharp Null Hypothesis testable?"",
                ""List major reasons for different results from survival analysis among different studies"",
                ""How do the tenses and aspects in English correspond temporally to one another?"",
                ""Is there a correct gender-neutral singular pronoun (“his” vs. “her” vs. “their”)?"",
                ""Are collective nouns always plural, or are certain ones singular?"",
                ""What’s the rule for using “who” and “whom” correctly?"",
                ""When is a gerund supposed to be preceded by a possessive adjective/determiner?"")

train_class &lt;- factor(c(rep(0,5), rep(1,5)))

train_dfm &lt;- train_text %&gt;% 
  dfm(tolower=TRUE, stem=TRUE, remove=stopwords(""english""))

model &lt;- textmodel_NB(train_dfm, train_class)

test_text &lt;- c(""Weighted Linear Regression with Proportional Standard Deviations in R"",
               ""What do significance tests for adjusted means tell us?"",
               ""How should I punctuate around quotes?"",
               ""Should I put a comma before the last item in a list?"")

test_dfm &lt;- test_text %&gt;% 
  dfm(tolower=TRUE, stem=TRUE, remove=stopwords(""english""))

predict(model, test_dfm)
</code></pre>

<p>The only thing I have thought to do was to manually make the features the same (I assumed that this would fill in <code>0</code> for features that are not present in the object), but this generated a new error. The code for the example above is:</p>

<pre><code>model_features &lt;- model$data$x@Dimnames$features # gets the features of the training data

test_features &lt;- test_dfm@Dimnames$features # gets the features of the test data

all_features &lt;- c(model_features, test_features) %&gt;% # combining the two sets of features...
  subset(!duplicated(.)) # ...and getting rid of duplicate features

model$data$x@Dimnames$features &lt;- test_dfm@Dimnames$features &lt;- all_features # replacing features of model and test_dfm with all_features

predict(model, dfm) # new error?
</code></pre>

<p>However, this code generates a <em>new</em> error:</p>

<pre><code>Error in if (ncol(object$PcGw) != ncol(newdata)) stop(""feature set in newdata different from that in training set"") : 
  argument is of length zero
</code></pre>

<p><strong>How do I apply this naive Bayes model to a new data set with different features?</strong></p>
",Training and Model Evaluation,quanteda package naive bayes predict different test data used create model categorizes text one two category fit model training data set data last summer trying use summer categorize new text get work tried got following error code function generates error found line assume occurs word training data set exactly match word used test data set doe error occur feel useful real world example model able handle data set contain different feature probably always happen applied use first question error property naive bayes algorithm wa choice made author function lead second question remedy issue get second question provide reproducible code last line generates error thing thought wa manually make feature assumed would fill feature present object generated new error code example however code generates new error apply naive bayes model new data set different feature
Gensim Word2Vec model getting worse by increasing the number of epochs,"<p>I'm building a Word2Vec model for a category-recommendation on a dataset consisting of ~35.000 sentences for a total of ~500.000 words but only ~3.000 distinct ones.
I build the model basically like this :</p>

<pre class=""lang-py prettyprint-override""><code>def train_w2v_model(df, epochs):
    w2v_model = Word2Vec(min_count=5,
                                 window=100,
                                 size=230,
                                 sample=0,
                                 workers=cores-1,
                                 batch_words=100)
    vocab = df['sentences'].apply(list)
    w2v_model.build_vocab(vocab)
    w2v_model.train(vocab, total_examples=w2v_model.corpus_count, total_words=w2v_model.corpus_total_words, epochs=epochs, compute_loss=True)
    return w2v_model.get_latest_training_loss()
</code></pre>

<p>I tried to find the right number of epochs for such a model like this :</p>

<pre class=""lang-py prettyprint-override""><code>print(train_w2v_model(1))
=&gt;&gt; 86898.2109375
print(train_w2v_model(100))
=&gt;&gt; 5025273.0
</code></pre>

<p>I find the results very counterintuitive.
I do not understand how increasing the number of epochs could lead to lower the performance.
It seems not to be a misunderstanding from the function <code>get_latest_training_loss</code> since I observe the results with the function <code>most_similar</code> way better with only 1 epoch :</p>

<p>100 epochs :</p>

<pre class=""lang-py prettyprint-override""><code>w2v_model.wv.most_similar(['machine_learning'])
=&gt;&gt; [('salesforce', 0.3464601933956146),
 ('marketing_relationnel', 0.3125850558280945),
 ('batiment', 0.30903393030166626),
 ('go', 0.29414454102516174),
 ('simulation', 0.2930642068386078),
 ('data_management', 0.28968319296836853),
 ('scraping', 0.28260597586631775),
 ('virtualisation', 0.27560457587242126),
 ('dataviz', 0.26913416385650635),
 ('pandas', 0.2685554623603821)]
</code></pre>

<p>1 epoch :</p>

<pre><code>w2v_model.wv.most_similar(['machine_learning'])
=&gt;&gt; [('data_science', 0.9953729510307312),
 ('data_mining', 0.9930223822593689),
 ('big_data', 0.9894922375679016),
 ('spark', 0.9881765842437744),
 ('nlp', 0.9879133701324463),
 ('hadoop', 0.9834049344062805),
 ('deep_learning', 0.9831978678703308),
 ('r', 0.9827396273612976),
 ('data_visualisation', 0.9805369973182678),
 ('nltk', 0.9800992012023926)]
</code></pre>

<p>Any insight on why it behaves like this ? I would have think that increasing the number of epochs would have for sure a positive effect on the <strong>training</strong> loss.</p>
",Training and Model Evaluation,gensim word vec model getting worse increasing number epoch building word vec model category recommendation dataset consisting sentence total word distinct one build model basically like tried find right number epoch model like find result counterintuitive understand increasing number epoch could lead lower performance seems misunderstanding function since observe result function way better epoch epoch epoch insight behaves like would think increasing number epoch would sure positive effect training loss
Prdicting Y when we have some pattern in input text using ML?,"<p>I have a data set of around 20k in which I have 13k unique dependent variable in this 20k. My data looks like this:</p>

<pre><code>       Pattern        Y

0 dd AN dd AN dd AN    Y1
1 dd AN dd AN dd AN    Y1
2 a omnes              Y2
3 agence reuters ralr agence retr sarl   Y3
</code></pre>

<p>Similarly 20k+ observation. So in production, I have to predict Y during production once this pattern data is coming. </p>

<p>My problem is I don't have lot of observations for each unique dependent variable (like 1-5 observations for some or most of the time it is 1).</p>

<p><strong>My approach</strong></p>

<p>I am using tf-idf and training my model on Naive byes. I am getting good accuracy like around 70%.</p>

<p><strong>My problem</strong></p>

<p>I am afraid because I have to train this on like 1-5 observations, so even in little variation in input might create lot bias in output.</p>

<p>So can anybody tell me the best approach for this problem which can go to production?</p>
",Training and Model Evaluation,prdicting pattern input text using ml data set around k k unique dependent variable k data look like similarly k observation production predict production pattern data coming problem lot observation unique dependent variable like observation time approach using tf idf training model naive bye getting good accuracy like around problem afraid train like observation even little variation input might create lot bias output anybody tell best approach problem go production
How should I build an entity recognition model from this text files,"<p>this might be a little naive question but bear with me.</p>

<p>I have a dataset like this.</p>

<blockquote>
<pre><code>Pretty    O
bad   O
storm O
here  O
last  O
evening   O
. O

From  O
Green O
Newsfeed  O
: O
AHFA  B-group
extends   O
deadline  O
for   O
Sage  O
Award O
to    O
Nov   O
. O
</code></pre>
</blockquote>

<p>where <strong>O</strong> is tag for non entity, similarly <strong>B-group</strong> is tag for a group. similarly some other entities are there.</p>

<p>and I am trying to build an <strong>name entity recognition</strong> model. All the models I have came across has sentences and then they go on building a model. Like they directly get PoS tagging for all the words from API by processing them. </p>

<p>but if I want to train a model here.
Can someone suggest me an approach, or direct me towards a resource. Thanks in advance.</p>
",Training and Model Evaluation,build entity recognition model text file might little naive question bear dataset like tag non entity similarly b group tag group similarly entity trying build name entity recognition model model came across ha sentence go building model like directly get po tagging word api processing want train model someone suggest approach direct towards resource thanks advance
What can I do about a large input text that can&#39;t all be fit into memory,"<p>I have a dataset with approximately 300k samples where each sample is a text document with about 25k words. </p>

<p>I can load it all into memory, but I can't really use a GRU network since I just run into a memory error due to the number of parameters</p>

<p>I tried setting the max vocab length to 35000 and max sequence length to 2500 after which I could run a GRU network. I got a slightly good accuracy but I'm  losing 90% of my data.</p>
",Training and Model Evaluation,large input text fit memory dataset approximately k sample sample text document k word load memory really use gru network since run memory error due number parameter tried setting max vocab length max sequence length could run gru network got slightly good accuracy losing data
How to use doc2vec model in production?,"<p>I wonder how to deploy a doc2vec model in production to create word vectors as input features to a classifier. To be specific, let say, a doc2vec model is trained on a corpus as follows.</p>

<pre><code>dataset['tagged_descriptions'] = datasetf.apply(lambda x: doc2vec.TaggedDocument(
            words=x['text_columns'], tags=[str(x.ID)]), axis=1)

model = doc2vec.Doc2Vec(vector_size=100, min_count=1, epochs=150, workers=cores,
                                window=5, hs=0, negative=5, sample=1e-5, dm_concat=1)

corpus = dataset['tagged_descriptions'].tolist()

model.build_vocab(corpus)

model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)

</code></pre>

<p>and then it is dumped into a pickle file. The word vectors are used to train a classifier such as random forests to predict movies sentiment. </p>

<p>Now suppose that in production, there is a document entailing some totally new vocabularies. That being said, they were not among the ones present during the training of the doc2vec model. I wonder how to tackle such a case. </p>

<p>As a side note, I am aware of <a href=""https://stackoverflow.com/questions/47775557/updating-training-documents-for-gensim-doc2vec-model"">Updating training documents for gensim Doc2Vec model</a> and <a href=""https://stackoverflow.com/questions/39252207/gensim-how-to-retrain-doc2vec-model-using-previous-word2vec-model"">Gensim: how to retrain doc2vec model using previous word2vec model</a>. However, I would appreciate more lights to be shed on this matter. </p>
",Training and Model Evaluation,use doc vec model production wonder deploy doc vec model production create word vector input feature classifier specific let say doc vec model trained corpus follows dumped pickle file word vector used train classifier random forest predict movie sentiment suppose production document entailing totally new vocabulary said among one present training doc vec model wonder tackle case side note aware however would appreciate light shed matter
Spacy: Should I train the model on single sentence or I can pass two sentence combined?,"<p>I have multiple sentences like the one below in my database: </p>

<blockquote>
  <p>KP Snacks Ltd recalls certain date codes of 4 variants of McCoy’s
  multi bag crisps. KP Snacks Ltd has undertaken a precautionary recall
  of the products listed below as a very small number of these bags of
  crisps may contain small pieces of plastic.</p>
</blockquote>

<p>Should I first split the sentences or I can just the whole data (2 sentences) to the model?</p>

<pre><code>TRAIN_DATA_1 = [
    (""KP Snacks Ltd recalls certain date codes of 4 variants of McCoy’s multi bag crisps. KP Snacks Ltd has undertaken a precautionary recall of the products listed below as a very small number of these bags of crisps may contain small pieces of plastic."", {""entities"": []}),
    (""I like London and Berlin."", {""entities"": []}),
]

TRAIN_DATA_2 = [
    (""KP Snacks Ltd recalls certain date codes of 4 variants of McCoy’s multi bag crisps."", {""entities"": []}),
(""KP Snacks Ltd has undertaken a precautionary recall of the products listed below as a very small number of these bags of crisps may contain small pieces of plastic."", {""entities"": []}),
    (""I like London and Berlin."", {""entities"": []}),
]
</code></pre>

<p>In short, TRAIN_DATA_1 vs TRAIN_DATA_2 which is correct and why?</p>
",Training and Model Evaluation,spacy train model single sentence pas two sentence combined multiple sentence like one database kp snack ltd recall certain date code variant mccoy multi bag crisp kp snack ltd ha undertaken precautionary recall product listed small number bag crisp may contain small piece plastic first split sentence whole data sentence model short train data v train data correct
Classify intent of random utterance of chat bot from training data and give different graphical visualization using random forest?,"<p>I am creating a nlp model to detect the intent from the provided utterance from a excel file which I am using for training having 2 columns like shown below:</p>

<pre><code>Utterence                                       Intent
hi can I have an Apple Watch                   service
how much I will be paying monthly              service
you still around                              YOU_THERE
are you still there                           YOU_THERE
you there                                     YOU_THERE
Speak to me if you are there.                 YOU_THERE
you around                                    YOU_THERE
</code></pre>

<p>There are like around 3000 utterances in the training files and many intents.</p>

<p>I trained my model using scikit learn module and my code looks like this.</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np
import re
def preprocessing(userQuery):
    letters_only = re.sub(""[^a-zA-Z\\d]"", "" "", userQuery)
    words = letters_only.lower().split()
    return( "" "".join(words ))
#read utterance data from a xlsx file
train = pd.read_excel('training.xlsx')
query_features = train['Utterence']
#create tfidf
tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))
new_query = [preprocessing(query) for query in query_features]
features = tfidf_vectorizer.fit_transform(new_query).toarray()
#create random forest classification model
model = RandomForestClassifier()
model.fit(features, train['Intent'])
#intent prediction on user query
userQuery = ""I want apple watch""
userQueryList=[]
userQueryList.append(preprocessing(userQuery))
utfidf = tfidf_vectorizer.transform(userQueryList)
print("" prediction: "", model.predict(utfidf))
</code></pre>

<p>The one of problem for me here is for example: when i run for utterance <code>I want apple watch</code> it gives predicted intent as <code>you_there</code> instead of <code>service</code> as shown below(confirmation on training snapshot above):</p>

<pre><code>C:\U\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\ensemble\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  ""10 in version 0.20 to 100 in 0.22."", FutureWarning)
 prediction:  ['YOU_THERE']
</code></pre>

<p>Please help me how should i train my  model and what changes should I make to fix such issues and how i can check accuracy? Also I want to see graphical visualization and ROC curve how it can achieved using random forest. I am not very verse in NLP any help would be appreciated.</p>
",Training and Model Evaluation,classify intent random utterance chat bot training data give different graphical visualization using random forest creating nlp model detect intent provided utterance excel file using training column like shown like around utterance training file many intent trained model using scikit learn module code look like one problem example run utterance give predicted intent instead shown confirmation training snapshot please help train model change make fix issue check accuracy also want see graphical visualization roc curve achieved using random forest verse nlp help would appreciated
How does Keras tokenizer handle unseen data?,"<p>Say I have trained a tokenizer with some vocabulary, now what exactly does keras tokenizer() do when it encounters a word which was not present in vocabulary(training data)  does it simply ignore it?</p>
",Training and Model Evaluation,doe kera tokenizer handle unseen data say trained tokenizer vocabulary exactly doe kera tokenizer encounter word wa present vocabulary training data doe simply ignore
Is it possible to fine tune FastText models,"<p>I'm working on a project for text similarity using FastText, the basic example I have found to train a model is:</p>

<pre><code>from gensim.models import FastText

model = FastText(tokens, size=100, window=3, min_count=1, iter=10, sorted_vocab=1)
</code></pre>

<p>As I understand it, since I'm specifying the vector and ngram size,  the model is been trained from scratch here and if the dataset is small I would spect great resutls.</p>

<p>The other option I have found is to load the original Wikipedia model which is a huge file: </p>

<pre><code>from gensim.models.wrappers import FastText

model = FastText.load_fasttext_format('wiki.simple')
</code></pre>

<p>My question is, can I load the Wikipedia or any other model, and fine tune it with my dataset? </p>
",Training and Model Evaluation,possible fine tune fasttext model working project text similarity using fasttext basic example found train model understand since specifying vector ngram size model trained scratch dataset small would spect great resutls option found load original wikipedia model huge file question load wikipedia model fine tune dataset
LUIS - Similar training utterances for two chatbot intents,"<p><strong>PREMISE:</strong> I am using the <a href=""https://www.luis.ai/home"" rel=""nofollow noreferrer"">ML Luis framework</a> for the development of a chatbot. Which is basically a blackbox framework and I don't know how to tune it in the right way for this problem.</p>

<p>I have two intents/classes for my chatbot. For simplicity say:</p>

<ul>
<li><strong>like</strong></li>
<li><strong>don't like</strong></li>
</ul>

<p>are the two said intents. In my training set I have for the two classes:</p>

<p><strong>like</strong> class:</p>

<blockquote>
  <p>I like it</p>
  
  <p>I like cats</p>
  
  <p>I really like mouses</p>
</blockquote>

<p><strong>don't like</strong> class:</p>

<blockquote>
  <p>I <strong>don't</strong> like it</p>
  
  <p>I <strong>don't</strong> like dolphins</p>
  
  <p>I really <strong>don't</strong> like dogs</p>
</blockquote>

<p>The two classes are really similar for the training set phrases, and when I try to do some predictions on a phrase belonging to one of the two classes, the scores are really close, say, for example:</p>

<pre><code> I like armadillos -&gt; 0.86 like | 0.8 don't like
</code></pre>

<p>Basically the two domains/classes have a big overlapping and differ for only one word (<strong>don't</strong> as shown in the examples). Is there a way to train the model efficiently (using Luis<a href=""https://www.luis.ai/home"" rel=""nofollow noreferrer"">1</a>) increasing the scores difference across similar utterances?</p>
",Training and Model Evaluation,luis similar training utterance two chatbot intent premise using ml luis framework development chatbot basically blackbox framework know tune right way problem two intent class chatbot simplicity say like like two said intent training set two class like class like like cat really like mouse like class like like dolphin really like dog two class really similar training set phrase try prediction phrase belonging one two class score really close say example basically two domain class big overlapping differ one word shown example way train model efficiently using luis increasing score difference across similar utterance
can I use numerical features in crf model,"<p>Is it possible/good to add numerical features in crf models? e.g. position in the sequence. </p>

<p>I'm using <a href=""http://www.chokkan.org/software/crfsuite/manual.html"" rel=""nofollow"">CRFsuite</a>. It seems all the features will be converted to string, e.g. 'pos=0', 'pos=1', which then lose it's meaning as euclidean distance. </p>

<p>Or should I use them to train another model, e.g. svm, then ensemble with crf models? </p>
",Training and Model Evaluation,use numerical feature crf model possible good add numerical feature crf model e g position sequence using crfsuite seems feature converted string e g po po lose meaning euclidean distance use train another model e g svm ensemble crf model
Dissimilar Features between two documents,"<p>I am trying to find the dissimilarity between the two documents. I am using gensim and so far have obtained similarity score. </p>

<p>Is there any way to know the dissimilarity score and dissimilar features between two documents?
And how to evaluate it?</p>
",Training and Model Evaluation,dissimilar feature two document trying find dissimilarity two document using gensim far obtained similarity score way know dissimilarity score dissimilar feature two document evaluate
"Python: using scikit-learn to predict, gives blank predictions","<p>I work in customer support, and I'm using scikit-learn to predict tags for our tickets, given a training set of tickets (approx. 40,000 tickets in the training set).</p>

<p>I'm using the classification model based on <a href=""https://stackoverflow.com/questions/10526579/use-scikit-learn-to-classify-into-multiple-categories?rq=1"">this one</a>. It's predicting just ""()"" as the tags for many of my test set of tickets, even though none of the tickets in the training set are without tags.</p>

<p>My training data for tags is a list of lists, like: </p>

<pre><code>tags_train = [['international_solved'], ['from_build_guidelines my_new_idea eligibility'], ['dropbox other submitted_faq submitted_help'], ['my_new_idea_solved'], ['decline macro_backer_paypal macro_prob_errored_pledge_check_credit_card_us loading_problems'], ['dropbox macro__turnaround_time other plq__turnaround_time submitted_help'], ['dropbox macro_creator__logo_style_guide outreach press submitted_help']]
</code></pre>

<p>While my training data for ticket descriptions is just a list of strings, e.g.:</p>

<pre><code>descs_train = ['description of ticket one', 'description of ticket two', etc]
</code></pre>

<p>Here's the relevant part of my code to build the model:</p>

<pre><code>import numpy as np
import scipy
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC

# We have lists called tags_train, descs_train, tags_test, descs_test with the test and train data

X_train = np.array(descs_train)
y_train = tags_train
X_test = np.array(descs_test)  

classifier = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LinearSVC(class_weight='auto')))])

classifier.fit(X_train, y_train)
predicted = classifier.predict(X_test)
</code></pre>

<p>However, ""predicted"" gives a list that looks like:</p>

<pre><code>predicted = [(), ('account_solved',), (), ('images_videos_solved',), ('my_new_idea_solved',), (), (), (), (), (), ('images_videos_solved', 'account_solved', 'macro_launched__edit_update other tips'), ('from_guidelines my_new_idea', 'from_guidelines my_new_idea macro__eligibility'), ()]
</code></pre>

<p>I don't understand why it's predicting blank () when there are none in the training set. Shouldn't it predict the closest tag? Can anyone recommend any improvements to the model I'm using?</p>

<p>Thank you so much for your help in advance!</p>
",Training and Model Evaluation,python using scikit learn predict give blank prediction work customer support using scikit learn predict tag ticket given training set ticket approx ticket training set using classification model based training data tag list list like training data ticket description list string e g relevant part code build model however predicted give list look like understand predicting blank none training set predict closest tag anyone recommend improvement model using thank much help advance
Is it appropriate to train W2V model on entire corpus?,"<p>I have a corpus of free text medical narratives, for which I am going to use for a classification task, right now for about 4200 records. </p>

<p>To begin, I wish to create word embeddings using w2v, but I have a question about a train-test split for this task. </p>

<p>When I train the w2v model, is it appropriate to use all of the data for the model creation? Or should I only use the train data for creating the model? </p>

<p>Really, my question sort of comes down to: do I take the whole dataset, create the w2v model, transform the narratives with the model, and then split, or should I split, create w2v, and then transform the two sets independently?</p>

<p>Thanks!</p>

<p><strong>EDIT</strong></p>

<p>I found an internal project at my place of work which was built by a vendor; they create the split, and create the the w2v model on ONLY the train data, then transform the two sets independently in different jobs; so it's the latter of the two options that I specified above. This is what I thought would be the case, as I wouldn't want to contaminate the w2v model on any of the test data. </p>
",Training and Model Evaluation,appropriate train w v model entire corpus corpus free text medical narrative going use classification task right record begin wish create word embeddings using w v question train test split task train w v model appropriate use data model creation use train data creating model really question sort come take whole dataset create w v model transform narrative model split split create w v transform two set independently thanks edit found internal project place work wa built vendor create split create w v model train data transform two set independently different job latter two option specified thought would case want contaminate w v model test data
Building a predictive model with text data and other predictors,"<p>I'm trying to build a predictive model (random forest, sgd, etc.) using scikit-learn and it seems like every model only allows you to fit text data such as </p>

<pre class=""lang-py prettyprint-override""><code>classifier.fit(X,Y)
</code></pre>

<p>...where <code>Y</code> is the target and <code>X</code> is a text feature vector (count_vec -> tf_idf). Is there any way to have a model which in addition to the text feature matrix also contains several categorical variables? Can I simply append them as new columns on the right side of <code>X</code>?</p>
",Training and Model Evaluation,building predictive model text data predictor trying build predictive model random forest sgd etc using scikit learn seems like every model allows fit text data target text feature vector count vec tf idf way model addition text feature matrix also contains several categorical variable simply append new column right side
Spacy: How to decide parameters to overfitting?,"<p>For training new custom entities we can train a model using the steps mentioned here: <a href=""https://spacy.io/usage/training#ner"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#ner</a></p>

<p>But I want to know how to decide no of iterations, drop and batch size to overfit or underfit the model?</p>

<pre><code>One example of loss is:
Starting training....
Losses:  {'ner': 3875.2103796127717}
Losses:  {'ner': 3091.347521599567}
Losses:  {'ner': 2811.074334355512}
Losses:  {'ner': 2235.2944185569686}
Losses:  {'ner': 2015.7072019365773}
Losses:  {'ner': 1647.0052678292357}
Losses:  {'ner': 1746.1746172501762}
Losses:  {'ner': 1350.2094295662862}
Losses:  {'ner': 1302.3405612718204}
Losses:  {'ner': 1322.3590930188122}
Losses:  {'ner': 1070.3760899125737}
Losses:  {'ner': 990.9221824283309}
Losses:  {'ner': 961.2431416302175}
Losses:  {'ner': 885.3743390914278}
Losses:  {'ner': 838.3100930655886}
Losses:  {'ner': 733.5780730531789}
Losses:  {'ner': 915.0732067395388}
Losses:  {'ner': 734.7598118888878}
Losses:  {'ner': 645.5447305966479}
Losses:  {'ner': 615.6987186405088}
Losses:  {'ner': 624.112212173154}
Losses:  {'ner': 590.4118676242763}
Losses:  {'ner': 411.8125225993247}
Losses:  {'ner': 482.4468110898493}
Losses:  {'ner': 479.08534166022685}
Training completed...
</code></pre>

<p>In the above output, the loss is decreasing and increasing. So at what point should I stop training?</p>

<p>Basically how to decide all the parameters for training?</p>
",Training and Model Evaluation,spacy decide parameter overfitting training new custom entity train model using step mentioned want know decide iteration drop batch size overfit underfit model output loss decreasing increasing point stop training basically decide parameter training
Why a frequent word gets misclassified?,"<p>I am practising NLP and checking using the below function what are the most frequent words per category and then observe how some sentences would be classified. The results are surprisingly wrong (Do you have to suggest another way of doing this helpful part of finding most frequent words per category?):</p>

<pre><code>#The function
def show_top10(classifier, vectorizer, categories):
...     feature_names = np.asarray(vectorizer.get_feature_names())
...     for i, category in enumerate(categories):
...         top10 = np.argsort(classifier.coef_[i])[-10:]
...         print(""%s: %s"" % (category, "" "".join(feature_names[top10])))

#Using the function on the data
show_top10(clf, vectorizer, newsgroups_train.target_names)

#The results seem to be logical
#the most frequent words by category are these:
rec.autos: think know engine don new good just like cars car
rec.motorcycles: riding helmet don know ride bikes dod like just bike
sci.space: don earth think orbit launch moon just like nasa space

#Now, testing these sentences, we see that they are classified wrong and not based 
#on the above most frequent words

texts = [""wheelie"", 
    ""stars are shining"",
    ""galaxy""]
text_features = vectorizer.transform(texts)
predictions = clf.predict(text_features)
for text, predicted in zip(texts, predictions):
   print('""{}""'.format(text))
   print(""  - Predicted as: '{}'"".format(newsgroup_train.target_names[predicted]))
   print("""")
</code></pre>

<p>and the results are:</p>

<pre><code>""wheelie""
  - Predicted as: 'rec.motorcycles'

""stars are shining""
  - Predicted as: 'sci.space'

""galaxy""
  - Predicted as: 'rec.motorcycles'
</code></pre>

<p>The word <strong>galaxy</strong> is mentioned many times in the space texts. Why it can't classify it correctly?</p>

<p>The code of the classification can be seen below if needed.</p>

<pre><code>from sklearn.datasets import fetch_20newsgroups
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import metrics


cats = ['sci.space','rec.autos','rec.motorcycles']
newsgroups_train = fetch_20newsgroups(subset='train',
                           remove=('headers', 'footers', 'quotes'), categories = cats)
newsgroups_test = fetch_20newsgroups(subset='test',
                           remove=('headers', 'footers', 'quotes'), categories = cats)

vectorizer = TfidfVectorizer(max_features = 1000,max_df = 0.5,
                            min_df = 5, stop_words='english')


vectors = vectorizer.fit_transform(newsgroups_train.data)

vectors_test = vectorizer.transform(newsgroups_test.data)

clf = MultinomialNB(alpha=.01)
clf.fit(vectors, newsgroups_train.target)
vectors_test = vectorizer.transform(newsgroups_test.data)
pred = clf.predict(vectors_test)
</code></pre>

<p>Maybe is due to the fact that the accuracy score is 0.77 which renders some to be misclassified. How do you suggest to make the model to perform better? Actually SVM would be what I would like to use but gives worse results and gives as more frequent words just ""00"" in every category.</p>
",Training and Model Evaluation,frequent word get misclassified practising nlp checking using function frequent word per category observe sentence would classified result surprisingly wrong suggest another way helpful part finding frequent word per category result word galaxy mentioned many time space text classify correctly code classification seen needed maybe due fact accuracy score render misclassified suggest make model perform better actually svm would would like use give worse result give frequent word every category
How do you check that a trained vocab and TfidfVectorizer is applied correctly to another corpus?,"<p>I am trying to train an NLP model on one set, save the vocab and the model, then apply it to a separate validation set. The code is running, but how can I be sure it is working as I expect? </p>

<p>In other words, I have saved a vocab and nmodel from the training set, then I created the TFidfVectorizer with saved vocabulary, and finally I use ""fit_transform"" on the new, validation notes. </p>

<p>Is this applying only the trained vocab and model? Is it not ""learning"" anything new from the validation set?</p>

<p>Training, then load the vocab and model and apply to the validation set:</p>

<pre><code>train_vector = tfidf_vectorizer.fit_transform(training_notes)
pickle.dump(tfidf_vectorizer.vocabulary_, open('./vocab/' + '_vocab.pkl', 'wb'))
X_train = train_vector.toarray()
y_train = np.array(train_data['ref_std'])
model.fit(X_train, y_train)
dump(model, './model/' + '.joblib')
train_prediction = model.predict(X_train)


vocab = pickle.load(open('./vocab/' + '_vocab.pkl', 'rb'))
tfidf_vectorizer = TfidfVectorizer(vocabulary = vocab)    
valid_vector = tfidf_vectorizer.fit_transform(validation_notes)
X_valid = valid_vector.toarray()
y_valid = np.array(validation_data['ref_std'])
model = load('./model/' + '.joblib')
valid_prediction = model.predict(X_valid)```
</code></pre>
",Training and Model Evaluation,check trained vocab tfidfvectorizer applied correctly another corpus trying train nlp model one set save vocab model apply separate validation set code running sure working expect word saved vocab nmodel training set created tfidfvectorizer saved vocabulary finally use fit transform new validation note applying trained vocab model learning anything new validation set training load vocab model apply validation set
NLP multi-class classifier loss can’t go down,"<p>I'm building a classifier for a QA bot, and  have <strong>a dataset for 8k questions, and 149 different Answers.</strong></p>

<p>I got some problems when training my model; the ""loss"" won't go down as I expected so I am asking for your help...</p>

<h3>Here is my method:</h3>

<p>I use word2vec to get a word's vector, then use a GRU-based network to get the vector of sentence The w2v model has been trained with wiki data, and works well on another of my NLP projects. The GRU code is from my senior, I think it works well, too.</p>

<pre><code># Part of the code for getting sentence vector
input_size = 400 
hidden_dim = 400  
num_layers = 1
gru = nn.GRU(input_size, hidden_dim,num_layers,batch_first = True)

h0 = torch.rand(num_layers, 7187, hidden_dim) # (num_layers, batch, hidden_dim) 
# shape of input [dataset_len,max_sentence_len,input_feature]
inputSet = torch.tensor(x_train,dtype = torch.float)
sentenceVecs, hidden = gru(inputSet,h0)
sentenceVecs = sentenceVecs[:,-1, :] 
</code></pre>

<p>and here is my classifier model</p>

<pre><code>from argparse import Namespace
args = Namespace(
    dataset_file = 'dataset/waimai_10k_tw.pkl',
    model_save_path='torchmodel/pytorch_bce.model',
    # Training hyper parameters
    batch_size = 100,
    learning_rate = 0.002,
    min_learning_rate = 0.002,
    num_epochs=200,
)

class JWP(nn.Module):
    def __init__(self, 
                 n_feature, 
                 n_hidden,
                 n_hidden2,
                 n_hidden3,
                 n_output):

        super(JWP, self).__init__()
        self.hidden = nn.Linear(n_feature, n_hidden)
        self.hidden2 = nn.Linear(n_hidden, n_hidden2)
        self.hidden3 = nn.Linear(n_hidden2, n_hidden3)
        self.out = nn.Linear(n_hidden3, n_output)

    def forward(self, x, apply_softmax=False):
        x = F.relu(self.hidden(x).squeeze())
        x = F.relu(self.hidden2(x).squeeze())
        x = F.relu(self.hidden3(x).squeeze())
        # 
        if(apply_softmax):
            x = torch.softmax(self.out(x))
        else:
            x = self.out(x)

        return x
</code></pre>

<p>training code</p>

<pre><code>lr = args.learning_rate
min_lr = args.min_learning_rate
def adjust_learning_rate(optimizer, epoch):
    global lr
    if epoch % 10 == 0 and epoch != 0:
        lr = lr * 0.65
        if(lr &lt; min_lr):
            lr = min_lr
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

if __name__ == ""__main__"":
    EPOCH = args.num_epochs
    net = JWP(400,325,275,225,149)
#     net = JWP(400,250,149)
#     net = JWP(400,149)
    print(net)

    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss_func = torch.nn.CrossEntropyLoss()

    for t in range(EPOCH):
        adjust_learning_rate(optimizer,t)

        """"""
        Train phase
        """"""
        net.train() 
        TrainLoss = 0.0
        # Train batch
        for step,(batchData, batchTarget) in enumerate(trainDataLoader):
            optimizer.zero_grad() 
            out = net(batchData)
            loss = loss_func(out,batchTarget) 
            TrainLoss = TrainLoss + loss
            loss.backward() 
            optimizer.step() 
        TrainLoss = TrainLoss / (step+1) # epoch loss

        """"""
        Result
        """"""
        print(
            ""epoch:"",t+1 ,
            ""train_loss:"",round(TrainLoss.item(),3),
            ""LR:"",lr
        )
</code></pre>

<p>Is it that my model is too simple or do I simply use the wrong method?
The loss is always stuck at around 4.6 and I can't lower it any more...</p>

<pre><code>epoch: 2898 train_loss: 4.643 LR: 0.002
epoch: 2899 train_loss: 4.643 LR: 0.002
epoch: 2900 train_loss: 4.643 LR: 0.002
epoch: 2901 train_loss: 4.643 LR: 0.002
</code></pre>
",Training and Model Evaluation,nlp multi class classifier loss go building classifier qa bot dataset k question different answer got problem training model loss go expected asking help method use word vec get word vector use gru based network get vector sentence w v model ha trained wiki data work well another nlp project gru code senior think work well classifier model training code model simple simply use wrong method loss always stuck around lower
Why RNN LSTM model with 92% accuracy fails on unseen data?,"<p>I have built a RNN LSTM model as three class classifier. It is giving around 94% train and test accuracy. But on testing it on real data it is giving pretty inaccurate results.Its application is related to NLP.</p>

<p><strong>My raw dataset format is like</strong></p>

<pre><code>DATA        |  Class
1. String1  |    0
2. String2  |    1
3. String3  |    2
</code></pre>

<p>Accuracy, F1 Score and Confusion Matrix:-</p>

<pre><code>Accuracy =  91.07142857142857
F1 Score =  0.9104914905591774
[[39  3  0]
 [ 2 36  4]
 [ 0  1 27]]
</code></pre>

<p><a href=""https://i.sstatic.net/ziar9.png"" rel=""nofollow noreferrer"">Graph of training and validation acc</a></p>

<p>So what I want to achieve is that on feeding some string, if that string contain some common words of class 0 then it should predict that string is from class 0. Any help?</p>
",Training and Model Evaluation,rnn lstm model accuracy fails unseen data built rnn lstm model three class classifier giving around train test accuracy testing real data giving pretty inaccurate result application related nlp raw dataset format like accuracy f score confusion matrix graph training validation acc want achieve feeding string string contain common word class predict string class help
Why does OpenNLP&#39;s Document Categorizer train so fast?,"<p>OpenNLP is significantly underperforming other document classifiers I've tested, so before I give up on it, I decided to make sure I'm playing around will all the dials and knobs. One thing that stood out for me was OpenNLP is training my model extremely fast (around 1.2 seconds). Other NLP tools I've used can take minutes, if not hours to train. I have around 12k records in my training file.</p>

<p>I've tried to increase the iterations from 10 to 10000, unfortunately, it does not seem to have any impact on the training time or accuracy. </p>

<p>The odd thing is OpenNLP's docs state the following regarding the training duration: ""Now might be a good time to cruise over to Hulu or something, because this could take a while if you've got a large training set"". This makes me feel like I'm doing something wrong. </p>

<pre class=""lang-java prettyprint-override""><code>            int TRAINING_ITERATIONS = 10000;

            InputStreamFactory dataIn = new MarkableFileInputStreamFactory(new File(dataSetFileName));
            ObjectStream&lt;String&gt; lineStream = new PlainTextByLineStream(dataIn, ""UTF-8"");
            ObjectStream&lt;DocumentSample&gt; sampleStream = new DocumentSampleStream(lineStream);

            // define the training parameters
            TrainingParameters params = new TrainingParameters();
            params.put(TrainingParameters.ITERATIONS_PARAM, TRAINING_ITERATIONS+"""");
            params.put(TrainingParameters.CUTOFF_PARAM, 0+"""");
            params.put(AbstractTrainer.ALGORITHM_PARAM, NaiveBayesTrainer.NAIVE_BAYES_VALUE);

            FeatureGenerator[] featureGenerators = { new NGramFeatureGenerator(1,1),
                    new NGramFeatureGenerator(2,3) };
            DoccatFactory factory = new DoccatFactory(featureGenerators);


            // create a model from training data
            StopWatch stopWatch = new StopWatch();

            // Start the watch, do some task and stop the watch.
            stopWatch.start();


            model = DocumentCategorizerME.train(""en"", sampleStream, params, factory);
            stopWatch.stop();
            System.out.println(""Training Time: "" + stopWatch.getTime()+""ms""); // finishes in 1.2 second!!! 

</code></pre>

<p>Here is the output I'm getting</p>

<pre><code>Indexing events with TwoPass using cutoff of 0

    Computing event counts...  done. 2407 events
    Indexing...  done.
Collecting events... Done indexing in 0.84 s.
Incorporating indexed data for training...  
done.
    Number of Event Tokens: 2407
        Number of Outcomes: 12
      Number of Predicates: 44219
Computing model parameters...
Stats: (455/2407) 0.18903199002908183
...done.
Training Time: 1241ms
</code></pre>

<p>Does the iterations parameter even do anything? </p>
",Training and Model Evaluation,doe opennlp document categorizer train fast opennlp significantly underperforming document classifier tested give decided make sure playing around dial knob one thing stood wa opennlp training model extremely fast around second nlp tool used take minute hour train around k record training file tried increase iteration unfortunately doe seem impact training time accuracy odd thing opennlp doc state following regarding training duration might good time cruise hulu something could take got large training set make feel like something wrong output getting doe iteration parameter even anything
Difference between max length of word ngrams and size of context window,"<p>In the description of the fasttext library for python <a href=""https://github.com/facebookresearch/fastText/tree/master/python"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/fastText/tree/master/python</a> for training a supervised model there are different arguments, where among others are stated as:</p>

<ul>
<li><code>ws</code>: size of the context window</li>
<li><code>wordNgrams</code>: max length of word ngram.</li>
</ul>

<p>If I understand it right, both of them are responsible for taking into account the surrounding words of the word, but what is the clear difference between them?</p>
",Training and Model Evaluation,difference max length word ngrams size context window description fasttext library python training supervised model different argument among others stated size context window max length word ngram understand right responsible taking account surrounding word word clear difference
Spacy: Generate generic sentences and then train the model on top of that. Is it a good idea?,"<p>I am training a model from scratch to predict food items from the text. I have tagged around 500 sentences to train my model and the accuracy is pretty good. But, I am a bit worried about the unseen real-world data so I have come up with an interesting idea. So I wanted to know some experienced person thought in this interesting idea.</p>

<h2>So the idea is to convert the 500 sentences into maybe 10000 sentences. For that, I have first I replaced the actual entity with tag and then filled with possible entities. Example of this as follows:</h2>

<p>Original training Sentences: </p>

<ol>
<li>""Tesco sold fifty thousand pizza last year. "" ---  Food = pizza</li>
<li>""He loves to eat pudding when he is alone."" --- Food =  pudding
Generic Sentences:</li>
<li>""Tesco sold fifty thousand  last year. ""</li>
<li>""He loves to eat  when he is alone.""</li>
</ol>

<p>Food List:</p>

<ol>
<li>pizza</li>
<li>pudding</li>
</ol>

<p>New generated training sentences:</p>

<ol>
<li>""Tesco sold fifty thousand pizza last year. "" ---  Food = pizza</li>
<li>""Tesco sold fifty thousand pudding last year. "" ---  Food = pudding</li>
<li>""He loves to eat pizza when he is alone."" --- Food =  pizza</li>
<li>""He loves to eat pudding when he is alone."" --- Food =  pudding</li>
</ol>

<hr>

<p>So is this a good to generate training sentences like this.
Benefits which I think:</p>

<ol>
<li>More sentences.</li>
<li>The single entity will have more example instead of one or two.</li>
<li>May be high accuracy.</li>
</ol>

<p>Issues could be:</p>

<ul>
<li>Training data full of similar sentence pattern.</li>
</ul>

<p>Thanks, Please let me know thoughts in this approach.</p>
",Training and Model Evaluation,spacy generate generic sentence train model top good idea training model scratch predict food item text tagged around sentence train model accuracy pretty good bit worried unseen real world data come interesting idea wanted know experienced person thought interesting idea idea convert sentence maybe sentence first replaced actual entity tag filled possible entity example follows original training sentence tesco sold fifty thousand pizza last year food pizza love eat pudding alone food pudding generic sentence tesco sold fifty thousand last year love eat alone food list pizza pudding new generated training sentence tesco sold fifty thousand pizza last year food pizza tesco sold fifty thousand pudding last year food pudding love eat pizza alone food pizza love eat pudding alone food pudding good generate training sentence like benefit think sentence single entity example instead one two may high accuracy issue could training data full similar sentence pattern thanks please let know thought approach
Why is loss changing with dense and embedding layers sizes?,"<p>I have a simple model as follows:</p>

<pre><code>class Model(tf.keras.Model):
    def __init__(self, vocab_size, embedding_size):
        super(Model, self).__init__()

        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)
        self.out = tf.keras.layers.Dense(vocab_size)

    def call(self, x):
        return self.out(self.embedding(x))
</code></pre>

<p>My loss is calculated using softmax cross entropy and weights updated with the adam optimizer.</p>

<p>When vocab_size = 2 (for testing purposes), loss is this:
<a href=""https://i.sstatic.net/xFr71.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xFr71.png"" alt=""enter image description here""></a></p>

<p>When vocab_size = 20200 (full size of vocab), loss is this:
<a href=""https://i.sstatic.net/gPYg0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gPYg0.png"" alt=""enter image description here""></a></p>

<p>Why is the shape of the loss changing, when everything is the same except for dense and embedding layer sizes? And how do I get the same, nice shape for loss with the larger vocabulary size?</p>
",Training and Model Evaluation,loss changing dense embedding layer size simple model follows loss calculated using softmax cross entropy weight updated adam optimizer vocab size testing purpose loss vocab size full size vocab loss shape loss changing everything except dense embedding layer size get nice shape loss larger vocabulary size
Could I use BERT to Cluster phrases with pre-trained model,"<p>I found it was a failure that I had used Gensim with GoogleNews pre-trained model to cluster phrases like:</p>

<ul>
<li>knitting</li>
<li>knit loom</li>
<li>loom knitting</li>
<li>weaving loom</li>
<li>rainbow loom</li>
<li>home decoration accessories</li>
<li>loom knit/knitting loom</li>
<li>...</li>
</ul>

<p>I am advised that <a href=""https://stackoverflow.com/questions/57426745/how-to-cluster-words-and-phrases-with-pre-trained-model-on-gensim"">GoogleNews model does't have the phrases in it</a>. The phrases I have are a little specific to GoogleNews model while I don't have corpus to train a new model. I have only the phrases. And now I am considering to turn to BERT. But could BERT do that as I expected as above? Thank you.</p>
",Training and Model Evaluation,could use bert cluster phrase pre trained model found wa failure used gensim googlenews pre trained model cluster phrase like knitting knit loom loom knitting weaving loom rainbow loom home decoration accessory loom knit knitting loom advised href model doe phrase phrase little specific googlenews model corpus train new model phrase considering turn bert could bert expected thank
How to test a nlp model on a sample having less features (columns) than the training set?,"<p>I was trying to build a NLP model but i got a problem.  After one hot encoding(bag of words), my model has 1500 features but i want to test the model on a data having less features .
How to solve the issue. ?
How to match the presence of the test data words in training data so that i can put zero at other places in the test array and make it of length 1500.</p>
",Training and Model Evaluation,test nlp model sample le feature column training set wa trying build nlp model got problem one hot encoding bag word model ha feature want test model data le feature solve issue match presence test data word training data put zero place test array make length
"Incompatible shapes: [128,28] vs. [128,29] in validation","<p>I am training a seq2seq model.i have a training set with (x,1212)  shape and validation set with (x,29) shape. when i try to train my below model. it is throwing me an error in validation.</p>

<p>Error:</p>

<blockquote>
  <p>File ""C:\Users\X\eclipse-workspace\TextSummerization\main\process.py"",
  line 184, in 
      history=model.fit([X_train,Y_train], Y_train.reshape(Y_train.shape[0],Y_train.shape[1], 1)[:,1:]
  ,epochs=2,batch_size=128, validation_data=([X_test,Y_test], 
  Y_test.reshape(Y_test.shape[0],Y_test.shape[1], 1)[:,1:]))   File
  ""C:\Users\X\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"",
  line 1363, in fit
      validation_steps=validation_steps)   File ""C:\Users\X\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"",
  line 264, in fit_loop
      outs = f(ins_batch)   File ""C:\Users\X\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\backend.py"",
  line 2914, in <strong>call</strong>
      fetched = self._callable_fn(*array_vals)   File ""C:\Users\X\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\client\session.py"",
  line 1382, in <strong>call</strong>
      run_metadata_ptr)   File ""C:\Users\X\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py"",
  line 519, in <strong>exit</strong>
      c_api.TF_GetCode(self.status.status)) tensorflow.python.framework.errors_impl.InvalidArgumentError:
  Incompatible shapes: [128,28] vs. [128,29]     [[Node: metrics/acc/Equal
  = Equal[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](metrics/acc/Max,
  metrics/acc/Cast)]]</p>
</blockquote>

<p>when i change optimizer to </p>

<blockquote>
  <p>adam</p>
</blockquote>

<p>and loss to </p>

<blockquote>
  <p>categorical</p>
</blockquote>

<p>it is giving me another error: `</p>

<blockquote>
  <p>Error when checking target: expected time_distributed to have shape
  (None, 14847) but got array with shape (28, 1)</p>
</blockquote>

<p>`</p>

<pre><code># Encoder 
encoder_inputs = Input(shape=(X_train.shape[1],)) 
enc_emb = Embedding(x_voc_size, latent_dim,trainable=True)(encoder_inputs) 

#LSTM 1 
encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) 
encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) 

#LSTM 2 
encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) 
encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) 

#LSTM 3 
encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) 
encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) 

# Set up the decoder. 
decoder_inputs = Input(shape=(None,)) 
dec_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True) 
dec_emb = dec_emb_layer(decoder_inputs) 

#LSTM using encoder_states as initial state
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) 
decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) 

#Attention Layer
attn_layer = AttentionLayer(name='attention_layer') 
attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) 

# Concat attention output and decoder LSTM output 
decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])

#Dense layer
decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax')) 
decoder_outputs = decoder_dense(decoder_concat_input) 

# Define the model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs) 
model.summary()

model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',metrics=['accuracy'])

#monitoring loss here, if loss increases we will stop training.
#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)

history=model.fit([X_train,Y_train], Y_train.reshape(Y_train.shape[0],Y_train.shape[1], 1)[:,1:] ,epochs=2,batch_size=128, validation_data=([X_test,Y_test],  Y_test.reshape(Y_test.shape[0],Y_test.shape[1], 1)[:,1:]))
</code></pre>
",Training and Model Evaluation,incompatible shape v validation training seq seq model training set x shape validation set x shape try train model throwing error validation error file c user x eclipse workspace textsummerization main process py line history model fit x train train train reshape train shape train shape epoch batch size validation data x test test test reshape test shape test shape file c user x appdata local continuum anaconda lib site package tensorflow python kera engine training py line fit validation step validation step file c user x appdata local continuum anaconda lib site package tensorflow python kera engine training array py line fit loop f batch file c user x appdata local continuum anaconda lib site package tensorflow python kera backend py line call fetched self callable fn array vals file c user x appdata local continuum anaconda lib site package tensorflow python client session py line call run metadata ptr file c user x appdata local continuum anaconda lib site package tensorflow python framework error impl py line exit c api tf getcode self status status tensorflow python framework error impl invalidargumenterror incompatible shape v node metric acc equal equal dt float device job localhost replica task device cpu metric acc max metric acc cast change optimizer adam loss categorical giving another error
While training deep learning model using tensorflow library i am getting error: ResourceExhaustedError OOM on gpu(128 gb RAM) Kindly help me,"<p>C:\Users\CVL-Acoustics\Documents\bangla-sentence-correction-master>python train.py
Sit back and relax, it will take some time to train the model...
Vocabulary size 250000
WARNING:tensorflow:From C:\Users\CVL-Acoustics\Anaconda3\lib\site-packages\tensorflow\python\ops\rnn.py:417: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.
Instructions for updating:
seq_dim is deprecated, use seq_axis instead
WARNING:tensorflow:From C:\Users\CVL-Acoustics\Anaconda3\lib\site-packages\tensorflow\python\util\deprecation.py:432: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.
Instructions for updating:
batch_dim is deprecated, use batch_axis instead
WARNING:tensorflow:From train.py:228: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:</p>

<p>Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.</p>

<p>See @{tf.nn.softmax_cross_entropy_with_logits_v2}.</p>

<p>epoch 1
  training Traceback (most recent call last):
  File ""C:\Users\CVL-Acoustics\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\Users\CVL-Acoustics\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Users\CVL-Acoustics\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[6656,250000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Reshape, Variable_1/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.</p>

<pre><code>     [[Node: rnn/while/cond/Add/_87 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_421_rnn/while/cond/Add"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_clooprnn/while/cond/ArgMax/dimension/_1)]]
</code></pre>

<p>Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.</p>

<p>During handling of the above exception, another exception occurred:</p>

<p>Traceback (most recent call last):
  File ""train.py"", line 321, in 
    _, l = sess.run([train_op, loss], fd)
  File ""C:\Users\CVL-Acoustics\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\Users\CVL-Acoustics\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\CVL-Acoustics\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\Users\CVL-Acoustics\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[6656,250000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Reshape, Variable_1/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.</p>

<pre><code>     [[Node: rnn/while/cond/Add/_87 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_421_rnn/while/cond/Add"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_clooprnn/while/cond/ArgMax/dimension/_1)]]
</code></pre>

<p>Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.</p>

<p>Caused by op 'MatMul', defined at:
  File ""train.py"", line 218, in 
    decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)
  File ""C:\Users\CVL-Acoustics\Anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 2014, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""C:\Users\CVL-Acoustics\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 4278, in mat_mul
    name=name)
  File ""C:\Users\CVL-Acoustics\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\CVL-Acoustics\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""C:\Users\CVL-Acoustics\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1740, in <strong>init</strong>
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access</p>

<p>ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[6656,250000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Reshape, Variable_1/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.</p>

<pre><code>     [[Node: rnn/while/cond/Add/_87 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_421_rnn/while/cond/Add"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_clooprnn/while/cond/ArgMax/dimension/_1)]]
</code></pre>

<p>Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.</p>
",Training and Model Evaluation,training deep learning model using tensorflow library getting error resourceexhaustederror oom gpu gb ram kindly help c user cvl acoustic document bangla sentence correction master python train py sit back relax take time train model vocabulary size warning tensorflow c user cvl acoustic anaconda lib site package tensorflow python ops rnn py calling reverse sequence tensorflow python ops array ops seq dim deprecated removed future version instruction updating seq dim deprecated use seq axis instead warning tensorflow c user cvl acoustic anaconda lib site package tensorflow python util deprecation py calling reverse sequence tensorflow python ops array ops batch dim deprecated removed future version instruction updating batch dim deprecated use batch axis instead warning tensorflow train py softmax cross entropy logits tensorflow python ops nn ops deprecated removed future version instruction updating future major version tensorflow allow gradient flow label input backprop default see tf nn softmax cross entropy logits v epoch training traceback recent call last file c user cvl acoustic anaconda lib site package tensorflow python client session py line call return fn args file c user cvl acoustic anaconda lib site package tensorflow python client session py line run fn option feed dict fetch list target list run metadata file c user cvl acoustic anaconda lib site package tensorflow python client session py line call tf sessionrun run metadata tensorflow python framework error impl resourceexhaustederror oom allocating tensor shape type float job localhost replica task device gpu allocator gpu bfc node matmul matmul dt float transpose false transpose b false device job localhost replica task device gpu reshape variable read hint want see list allocated tensor oom happens add report tensor allocation upon oom runoptions current allocation info hint want see list allocated tensor oom happens add report tensor allocation upon oom runoptions current allocation info handling exception another exception occurred traceback recent call last file train py line l sess run train op loss fd file c user cvl acoustic anaconda lib site package tensorflow python client session py line run run metadata ptr file c user cvl acoustic anaconda lib site package tensorflow python client session py line run feed dict tensor option run metadata file c user cvl acoustic anaconda lib site package tensorflow python client session py line run run metadata file c user cvl acoustic anaconda lib site package tensorflow python client session py line call raise type e node def op message tensorflow python framework error impl resourceexhaustederror oom allocating tensor shape type float job localhost replica task device gpu allocator gpu bfc node matmul matmul dt float transpose false transpose b false device job localhost replica task device gpu reshape variable read hint want see list allocated tensor oom happens add report tensor allocation upon oom runoptions current allocation info hint want see list allocated tensor oom happens add report tensor allocation upon oom runoptions current allocation info caused op matmul defined file train py line decoder logits flat tf add tf matmul decoder output flat w b file c user cvl acoustic anaconda lib site package tensorflow python ops math ops py line matmul b transpose transpose transpose b transpose b name name file c user cvl acoustic anaconda lib site package tensorflow python ops gen math ops py line mat mul name name file c user cvl acoustic anaconda lib site package tensorflow python framework op def library py line apply op helper op def op def file c user cvl acoustic anaconda lib site package tensorflow python framework ops py line create op op def op def file c user cvl acoustic anaconda lib site package tensorflow python framework ops py line init self traceback self graph extract stack pylint disable protected access resourceexhaustederror see traceback oom allocating tensor shape type float job localhost replica task device gpu allocator gpu bfc node matmul matmul dt float transpose false transpose b false device job localhost replica task device gpu reshape variable read hint want see list allocated tensor oom happens add report tensor allocation upon oom runoptions current allocation info hint want see list allocated tensor oom happens add report tensor allocation upon oom runoptions current allocation info
How to get accuracy of an expanded query (User input an query which is expanded for better IR)?,"<p>Using an algorithm , I am taking an input user query and expanding it. Now I need to test accuracy for my algorithm that is I want to get accuracy ( precision and recall ) for my expanded query ?</p>

<p>I have used terrier and taking a Trec Dataset(having collection of documents), 
I took a random query and retrieved relevant documents using terrier, then I used my algorithm to get expanded query for the random query and retrieved relevant documents.</p>

<p>But I dont know how to get precision and recall using this method. </p>

<p>So how do i get accuracy for my expanded query ?</p>

<p>If any other tool can be used ?</p>

<p>Thanks</p>
",Training and Model Evaluation,get accuracy expanded query user input query expanded better ir using algorithm taking input user query expanding need test accuracy algorithm want get accuracy precision recall expanded query used terrier taking trec dataset collection document took random query retrieved relevant document using terrier used algorithm get expanded query random query retrieved relevant document dont know get precision recall using method get accuracy expanded query tool used thanks
How to test Stanford Sentiment model?,"<p>I know the following command can be used to train a Stanford sentiment model</p>

<pre><code>java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath dev.txt -train -model model.ser.gz
</code></pre>

<p>Now I want to how to test the model with the testing dataset.
I tried using <code>-trainPath</code> option, it didn't seem to work. I didn't find anything neither on the official documentation nor on the web.</p>
",Training and Model Evaluation,test stanford sentiment model know following command used train stanford sentiment model want test model testing dataset tried using option seem work find anything neither official documentation web
How to understand and add syllable break in this example?,"<p>I am new in machine learning and computing probabilities. This is <a href=""http://alias-i.com/lingpipe/demos/tutorial/hyphenation/read-me.html"" rel=""nofollow"">an example from Lingpipe</a> for adding syllabification in a word by training data.</p>

<pre><code>Given a source model p(h) for hyphenated words, and a channel model p(w|h) defined so that p(w|h) = 1 if w is equal to h with the hyphens removed and 0 otherwise. We then seek to find the most likely source message h to have produced message w by:

    ARGMAXh p(h|w) = ARGMAXh p(w|h) p(h) / p(w)
                   = ARGMAXh p(w|h) p(h)         
                   = ARGMAXh s.t. strip(h)=w p(h)

where we use strip(h) = w to mean that w is equal to h with the hyphenations stripped out (in Java terms, h.replaceAll("" "","""").equals(w)). Thus with a deterministic channel, we wind up looking for the most likely hyphenation h according to p(h), restricting our search to h that produce w when the hyphens are stripped out. 
</code></pre>

<p>I do not understand how to use it to build a syllabification model.</p>

<p>If there is a training set containing:</p>

<pre><code>a bid jan
a bide
a bie
a bil i ty
a bim e lech
</code></pre>

<p>How to have a model that will syllabify words? I mean what to be computed in order to find possible syllable breaks of a new word.</p>

<p>First compute what? then compute what? Can you please be specific with example?</p>

<p>Thanks a lot.</p>
",Training and Model Evaluation,understand add syllable break example new machine learning computing probability example lingpipe adding syllabification word training data understand use build syllabification model training set containing model syllabify word mean computed order find possible syllable break new word first compute compute please specific example thanks lot
How to solve sklearn Memory Error when fitting large data?,"<p>So I basically have a huge dataset to work with, its almost made up of 1,200,000 rows, and my target class count is about 20,000 labels.</p>

<p>I am performing text classifiaction on my data, so I first cleaned it, and then performed tfidf vectorzation on it.</p>

<p>The problem lies whenever I try to pick a model and fit the data, it gives me a Memory Error</p>

<p>My current PC is Core i7 with 16GB of RAM</p>

<pre><code>vectorizer = feature_extraction.text.TfidfVectorizer(ngram_range=(1, 1),
                         analyzer='word',
                         stop_words= fr_stopwords)

datavec = vectorizer.fit_transform(data.values.astype('U'))

X_train, X_test, y_train, y_test = train_test_split(datavec,target,test_size=0.2,random_state=0)


print(type(X_train))
print(X_train.shape)
</code></pre>

<p>Output: 
class 'scipy.sparse.csr.csr_matrix'
(963993, 125441)</p>

<pre><code>clf.fit(X_train, y_train)
</code></pre>

<p>This is where the Memory Error is happening</p>

<p>I have tried:
1 - to take a sample of the data, but the error is persisting.</p>

<p>2 - to fit many different models, but only the KNN model was working (but with a low accuracy score)</p>

<p>3- to convert datavec to an array, but this process is also causing a Memory Error</p>

<p>4- to use multi processing on different models</p>

<p>5 - I have been through every similar question on SO, but either an answer was unclear, or did not relate to my problem exactly</p>

<p>This is a part of my code:</p>

<pre><code>vectorizer = feature_extraction.text.TfidfVectorizer(ngram_range=(1, 1),
                         analyzer='word',
                         stop_words= fr_stopwords)



  df = pd.read_csv(""C:\\Users\\user\\Desktop\\CLEAN_ALL_DATA.csv"", encoding='latin-1')
classes = np.unique(df['BENEFITITEMCODEID'].str[1:])

vec = vectorizer.fit(df['NEWSERVICEITEMNAME'].values.astype('U'))

del df


clf = [KNeighborsClassifier(n_neighbors=5),
   MultinomialNB(),
   LogisticRegression(solver='lbfgs', multi_class='multinomial'),
   SGDClassifier(loss=""log"", n_jobs=-1),
   DecisionTreeClassifier(max_depth=5),
   RandomForestClassifier(n_jobs=-1),
   LinearDiscriminantAnalysis(),
   LinearSVC(multi_class='crammer_singer'),
   NearestCentroid(),
  ]

data = pd.Series([])

for chunk in pd.read_csv(datafile, chunksize=100000):

   data =  chunk['NEWSERVICEITEMNAME']
   target = chunk['BENEFITITEMCODEID'].str[1:]

   datavec = vectorizer.transform(data.values.astype('U'))

   clf[3].partial_fit(datavec, target,classes = classes)
   print(""**CHUNK DONE**"")

s = ""this is a testing sentence""
svec = vectorizer.transform([s])

clf[3].predict(svec)  --&gt; memory error
clf[3].predict(svec).todense()  --&gt; taking a lot of time to finish
clf[3].predict(svec).toarrray()  --&gt; taking a lot of time to finish as well
</code></pre>

<p>Anything else I could try?</p>
",Training and Model Evaluation,solve sklearn memory error fitting large data basically huge dataset work almost made row target class count label performing text classifiaction data first cleaned performed tfidf vectorzation problem lie whenever try pick model fit data give memory error current pc core gb ram output class scipy sparse csr csr matrix memory error happening tried take sample data error persisting fit many different model knn model wa working low accuracy score convert datavec array process also causing memory error use multi processing different model every similar question either answer wa unclear relate problem exactly part code anything else could try
How to recognize certain words from a sentence with machine learning?,"<p>So this is the problem: I have a list of names for various merchandise. The list(python 2.7) generally looks like: 
'''
['10 Apple phones','20W LED light bulb','Insignia™ - 450 Sq. Ft. Portable Air Conditioner','Jack Black Double-Duty Face Moisturizer SPF 20','apple']
'''<br>
All the items are strings. Items in the list are completely random and have no obvious connection to each other. </p>

<p>Now what I want to extract from each string is the item itself, without the descriptions. For example, ""10 Apple phones"" becomes ""phones""; ""Insignia™ - 450 Sq. Ft. Portable Air Conditioner"" becomes ""Air Conditioner"" and ""apple"" from the list is just ""apple"" (because that's exactly what it is). </p>

<p>The list after proper extraction looks like this (ideally): 
'''
['phones','light bulb','Air Conditioner','Face Moisturizer','apple']
'''</p>

<p>My frist approach was to find all the items that are similar and put them in one group (there are about 500k words in the dataframe). I then extracted the similar parts of the words in one group. For example, ""iphone XS Max"", ""3 iPhone 4"", ""two iPhone 7s"" and ""iPhone 3g"" would be put in one group, and the algorithm would extract the similar part, which is ""iPhone"" in this case. </p>

<p>This algorithm kind of worked in about 60% of the cases (I think it might get better if I optimize the algorithm a little bit more). But I'm looking for a different approach that will increase the accuracy. Any help will be greatly appreciated. Thanks guys! </p>
",Training and Model Evaluation,recognize certain word sentence machine learning problem list name various merchandise list python generally look like apple phone w led light bulb insignia sq ft portable air conditioner jack black double duty face moisturizer spf apple item string item list completely random obvious connection want extract string item without description example apple phone becomes phone insignia sq ft portable air conditioner becomes air conditioner apple list apple exactly list proper extraction look like ideally phone light bulb air conditioner face moisturizer apple frist approach wa find item similar put one group k word dataframe extracted similar part word one group example iphone x max iphone two iphone iphone g would put one group algorithm would extract similar part iphone case algorithm kind worked case think might get better optimize algorithm little bit looking different approach increase accuracy help greatly appreciated thanks guy
ngram character based spell checking,"<p>I  was developing the n gram spell check as per the mentioned example . Although the algorithmic approach will be   as follows:</p>

<p>Consider 2 strings “statistics” and “statistical”. If n is set to 2 (bi-grams are being extracted), then the similarity of the two strings is calculated as follows:</p>

<p>Initially, the two strings are split into bi-grams:</p>

<p>Statistics - st ta at ti is st ti ic cs 9 bigrams</p>

<p>Statistical - st ta at ti is st ti ic ca al 10 bigrams</p>

<p>Then find the unique bi-grams in each string</p>

<p>Statistics - st ta at is ti ic cs (7 unique bigrams)</p>

<p>Statistical - st ta at ti is ic ca al (8 unique bigrams)</p>

<p>Next, find the unique bi-grams that are shared with both the terms.</p>

<p>There are 6 such bi-grams: st ta at ic is ti.</p>

<p>The similarity measure is calculated using similarity coefficient with the following formula:</p>

<p><strong>Similarity coefficient = 2*C/A+B</strong></p>

<pre><code>A - unique n-grams in term 1.
B - unique n-grams in term 2.
C - unique n-grams appearing in term 1 and term 2.
</code></pre>

<p>The above example would produce the result (2*6) / (7+8) = 0.80. Higher the similarity measure is, more relevant is the word for correction.</p>

<p><strong>My sample output for the program looks like:</strong></p>

<pre><code>Enter a word: ttem
temp : 0.5
stem : 0.5
items : 0.4444444444444444
item : 0.5
</code></pre>

<p>How do i select the most probable candidate among them . i hope you can provide some sort of solutions to this. hope to see you guys.</p>
",Training and Model Evaluation,ngram character based spell checking wa developing n gram spell check per mentioned example although algorithmic approach follows consider string statistic statistical n set bi gram extracted similarity two string calculated follows initially two string split bi gram statistic st ta ti st ti ic c bigram statistical st ta ti st ti ic ca al bigram find unique bi gram string statistic st ta ti ic c unique bigram statistical st ta ti ic ca al unique bigram next find unique bi gram shared term bi gram st ta ic ti similarity measure calculated using similarity coefficient following formula similarity coefficient c b example would produce result higher similarity measure relevant word correction sample output program look like select probable candidate among hope provide sort solution hope see guy
Keras Dense Layer Dimension Issues,"<p>Currently the goal of my task is to be able to, given input of two sentences, 
output two labelings of the phrasal-synonyms of the two sentences.
e.g. I am [happy], I am [having a good time]</p>

<p>However, I can never get my dense layer to output the dimension I want, which is [batch size, max_sentence_length, num_classes].</p>

<p>I've looked at some other posts saying that dense layer only allows the ""units"" parameter to be 1-D, so I should flatten my labels before I get it through the dense layer, and reshape my label matrix accordingly for loss function calculation, but I don't know if this is actually the appropriate way to approach it.</p>

<p>My current model is something like this:</p>

<p>Two inputs: (None, 58)
GLoVE embedding: (None, 58, 300)
BiLSTM (without return sequences): (None, 256)
Dense: (None, 290)</p>

<p>I've tried getting this to run too, but the performance is abysmal so I am questioning myself if I did anything incorrectly...</p>

<p>My intended result was originally to have the dense layer output dimension (None, 58, 5), as I have sentence length 58 and 5 different labels, thus the 290 in dense: 58*5=290.</p>
",Training and Model Evaluation,kera dense layer dimension issue currently goal task able given input two sentence output two labelings phrasal synonym two sentence e g happy good time however never get dense layer output dimension want batch size max sentence length num class looked post saying dense layer allows unit parameter flatten label get dense layer reshape label matrix accordingly loss function calculation know actually appropriate way approach current model something like two input none glove embedding none bilstm without return sequence none dense none tried getting run performance abysmal questioning anything incorrectly intended result wa originally dense layer output dimension none sentence length different label thus dense
where can I get training data of part-of-speech tagger?,"<p>I want to implement a part-of-speech tagger,but I don't know where I can get a lot of training data?
Thanks!</p>
",Training and Model Evaluation,get training data part speech tagger want implement part speech tagger know get lot training data thanks
Tabular data using spacy,"<p>I'm using Spacy and need some help to train our model with custom entities given in tabular format in a word/pdf document.  </p>

<p>I'm able to train it with a custom entity based on an example of ANIMAL and it's working fine. In this case, we are providing the start and the end index of the aforementioned custom entity in a given text. </p>

<pre><code>(""Horses are too tall and they pretend to care about your feelings"", {
    'entities': [(0, 6, 'ANIMAL')]
}),
</code></pre>

<p>My question comes in case of Tabular format:<br>
How can I give indexes like ANIMAL example?<br>
Can anyone please guide and assist?</p>

<p><a href=""https://i.sstatic.net/80LbU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/80LbU.png"" alt=""enter image description here""></a></p>
",Training and Model Evaluation,tabular data using spacy using spacy need help train model custom entity given tabular format word pdf document able train custom entity based example animal working fine case providing start end index aforementioned custom entity given text question come case tabular format give index like animal example anyone please guide assist
Why do fit_transform and transform produce different results?,"<p>I was playing around with LDA in the <code>text2vec</code> package and was confused why the <code>fit_transfrom</code> and <code>transform</code> were different when using the same data. </p>

<p><a href=""http://text2vec.org/topic_modeling.html#apply_learned_model_to_new_data"" rel=""nofollow noreferrer"">The documentation</a> states that transform applys the learned model to new data but the result is a lot different than the one produced from <code>fit_transform</code></p>

<pre><code>data(""movie_review"")
library(stringr)
library(text2vec)
library(dpylr)

tokens = movie_review$review[1:4000] %&gt;% 
  tolower %&gt;% 
  word_tokenizer

it = itoken(tokens, ids = movie_review$id[1:4000], progressbar = FALSE)

v = create_vocabulary(it) %&gt;% 
  prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.2)

vectorizer = vocab_vectorizer(v)

dtm = create_dtm(it, vectorizer, type = ""dgTMatrix"")

lda_model = LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)

set.seed(123)

doc_topic_distr = 
  lda_model$fit_transform(x = dtm, n_iter = 1000, 
                          convergence_tol = 0.001, n_check_convergence = 25, 
                          progressbar = FALSE)

set.seed(123)

new_doc_topic_dist = 
  lda_model$transform(x = dtm, n_iter = 1000, 
                          convergence_tol = 0.001, n_check_convergence = 25, 
                          progressbar = FALSE)

head(doc_topic_distr)
head(new_doc_topic_dist)
</code></pre>

<p>I expected both <code>doc_topic_distr</code> and <code>new_doc_topic_distr</code> to be the same but they are quite different.</p>
",Training and Model Evaluation,fit transform transform produce different result wa playing around lda package wa confused different using data documentation state transform applys learned model new data result lot different one produced expected quite different
How to &quot;devectorize&quot; a vector array (nx100) into text using a pre-trained model in Word2Vec using Gensim?,"<p>So I created a Word2Vec model using Gensim using a document. Then I did some calculations on the vector array which I obtained by punching the code <code>model[model.wv.vocab]</code>. 
Now I have a new vector array with me and I want to ""devectorize"" this new vector array into text. How can this be done in python?</p>
",Training and Model Evaluation,devectorize vector array nx text using pre trained model word vec using gensim created word vec model using gensim using document calculation vector array obtained punching code new vector array want devectorize new vector array text done python
Predicting Missing Word in Sentence,"<p>How can I predict a word that's missing from a sentence?</p>

<p>I've seen many papers on predicting the <em>next word</em> in a sentence using an n-grams language model with frequency distributions from a set of training data. But instead I want to predict a missing word that's not necessarily at the end of the sentence. For example:</p>

<blockquote>
  <p>I took my ___ for a walk.</p>
</blockquote>

<p>I can't seem to find any algorithms that take advantage of the words after the blank; I guess I could ignore them, but they must add some value. And of course, a bi/trigram model doesn't work for predicting the first two words. </p>

<p>What algorithm/pattern should I use? Or is there no advantage to using the words after the blank?</p>
",Training and Model Evaluation,predicting missing word sentence predict word missing sentence seen many paper predicting next word sentence using n gram language model frequency distribution set training data instead want predict missing word necessarily end sentence example took walk seem find algorithm take advantage word blank guess could ignore must add value course bi trigram model work predicting first two word algorithm pattern use advantage using word blank
"Using training data to fine-tune BERT-base, the loss does not decrease","<p>I selected a total of 24478 scenic spot description data from 237 categories to fine-tune BERT-base, and the average loss remained about 5. Specifically, the amount of data for each category is less than or equal to 200, and learning rate: 10-6 or 10-7. Other hyper-parameters are consistent with the original experiment of BERT. What factors may cause losses to not converge?</p>

<p>I selected 800 news data from 10 categories from the public news data set as training set. After 10 epochs of training with learning rate of 10-5, the model achieves 97% accurance on 400 test samples. After verifying the validity of the method, I selected a total of 3050 scenic spot description data from 14 categories to fine-tune BERT-base. After 10 epochs of training with learning rate of 10-6, the training loss converges to 0.5 and the model achieves 83% accurance on 230 test samples(the loss does not decrease with learning rate of 10-5).</p>
",Training and Model Evaluation,using training data fine tune bert base loss doe decrease selected total scenic spot description data category fine tune bert base average loss remained specifically amount data category le equal learning rate hyper parameter consistent original experiment bert factor may cause loss converge selected news data category public news data set training set epoch training learning rate model achieves accurance test sample verifying validity method selected total scenic spot description data category fine tune bert base epoch training learning rate training loss model achieves accurance test sample loss doe decrease learning rate
Increasing training examples reduces accuracy for maximum entropy classifier,"<p>I am using MaxEnt part of speech tagger to pos tag classification of a language corpus. I know it from theory, that increasing training examples should generally improve the classification accuracy. But, I am observing that in my case, the tagger gives max f measure value if I take 3/4th data for training and rest for testing. If I increase the training data size by taking it to be 85 or 90℅ of the whole corpus, then the accuracy decreases. Even on reducing the training data size to 50℅ of full corpus, the accuracy decreases. </p>

<p>I would like to know the possible reason for this decrease in accuracy with increasing training examples.</p>
",Training and Model Evaluation,increasing training example reduces accuracy maximum entropy classifier using maxent part speech tagger po tag classification language corpus know theory increasing training example generally improve classification accuracy observing case tagger give max f measure value take th data training rest testing increase training data size taking whole corpus accuracy decrease even reducing training data size full corpus accuracy decrease would like know possible reason decrease accuracy increasing training example
How to extract features with different formats in a text file in python?,"<p>i have several text files of company invoices which have different kinds of Date formats</p>

<p><code>dd/mm/yyyy</code></p>

<p><code>mm:dd:yy</code></p>

<p><code>dd,monthname,yy</code></p>

<p><code>yearname,monthname,dd</code></p>

<p>and so on.
Theres lots of unique patterns that cant be listed here.</p>

<p>Problem is i have been using a mix of regex and (mostly) if else string matching to find out these dates but im sure there's a better way to identify them instead of hardcoding the program to find different patterns</p>

<p>I would also like to extract other features like 'TOTAL' amount which also has formats like</p>

<p>Total
$123 </p>

<p>Total $123</p>

<p>$123
Total</p>

<p>$123 Total</p>

<p>Here are some example text files</p>

<pre><code>Demo Company INVOICE
Demo Company Phone : 141-222-3333 Invoice# 1024
1234 Main Street Fax: 222-3984444 Account# C1000
Ashland, KY 41102 Email : sales@example.com 
Date 01-08-2009
Due By 02-05-2009
‘Subtotal $212.44
Tax $1.25
Total $213.69
Balance Due $213.69

</code></pre>

<pre><code>SAMPLE PURCHASE ORDER
ToNGE
Purchase Order Number 2
FROM: Purchase Order Date 6:15:2
Your Company
1122 Cherry Lane
San Diego, CA 92176
1 | MH1000 | MATHOIST STORAGE SYSTEMFOR (@)45°x $8920.00 | $8,920.00
MATS (see product description)
1 [NA Mat Hoist Voltage??? 208V, 230V or 460V Nec Nec
6 |cL7 CL-7 UNIT CLAMP
SA8.75 $292.50
</code></pre>

<pre><code>123 Anywhere St.
Some City, CA 91000
Phone (555) 555-1212 Fax (555) 555-5555
P.O. NUMBER: 1234
P.O. DATE: 4/15/13
SUBTOTAL $3000
SALES TAX $s 240
TOTAL $3240

</code></pre>

<p>Heres the code i am using for date extraction and total extraction</p>

<pre><code>def extractdate():
    with open(inpf, ""r"") as ifile:
        for line in ifile:
            if line.startswith(""DATE""):
                print(next(ifile, '').strip())
            elif line.startswith(""P.O. DATE""):
                if ""P.O. DATE"" in line:
                    # print(line)
                    print('')
            elif str(""Date"") in line:
                    # print(line)
                    print('')
            elif str(""date"")in line:
                    # print(line)
                    print('')

</code></pre>

<pre><code>def totalamount():
    with open(inpf, ""r"") as ifile:
        for line in ifile:
            if 'TOTAL' in line or 'Total' in line:
                s = """"
                for i in range(len(line)):
                    if line[i].isdigit():
                        s += line[i]
                    elif not line[i].isdigit() and line[i-1].isdigit():
                        break

</code></pre>

<p>Is there any way i can use machine learning and train a model to extract these features?</p>
",Training and Model Evaluation,extract feature different format text file python several text file company invoice different kind date format lot unique pattern cant listed problem using mix regex mostly else string matching find date im sure better way identify instead hardcoding program find different pattern would also like extract feature like total amount also ha format like total total total total example text file code using date extraction total extraction way use machine learning train model extract feature
Metric for ranked keyword identification,"<p>I am trying to determine which metric(s) to use to evaluate the ""coverage"" of a lexicon (list of words) with respect to a ranked list of significant keywords I have extracted from two different collections of texts. The two lists do not have any terms common. Importantly, only the top-n ranking words are to be considered ""correct"", while the bottom-m ranked words are to be considered ""incorrect"".</p>

<p>Below is a dummy example with a ranked keyword list and 4 different lexicons which each contain some of the words in the list.</p>

<pre><code>rank  keyword    lexicon_1  lexicon_2  lexicon_3  lexicon_4
1     apple      False      True       True       False
2     orange     False      True       True       False
3     banana     False      False      True       False
4     pear       False      False      True       False
5     kiwifruit  True       False      True       False
6     watermelon True       False      True       False
-----------------------------------------------------------
7     car        False      False      False      True
8     bus        False      False      False      True
9     truck      True       False      False      True
10    bike       False      True       False      True
</code></pre>

<p>So the intuition is that the more high-ranking keywords (1-6) are included in a lexicon the better, and the fewer lower ranking keywords (7-10) are included the better.</p>

<p>Calculating precision (P), recall (R) and F-score (F) is one way of quantifying and comparing the coverage. For this, true positives (TP) are when the lexicon includes a correct word, false positives (FP) when it includes an incorrect word, true negatives (TN) when it excludes an incorrect word and false negatives (FN) when it excludes a correct word. With this in mind, for <code>lexicon_1</code>, for example, this gives:</p>

<pre><code>TP=2, FP=1, TN=3, FN=4
P = 2 / (2 + 1)    R = 2 / (2 + 4)    F = 2 * 0.67 * 0.33 / (0.67 + 0.33)
  = 0.67             = 0.33             = 0.44
</code></pre>

<p>For <code>lexicon_2</code> we also get <code>F=0.44</code>. For <code>lexicon_3</code> we get <code>F=1.00</code> and for <code>lexicon_4</code> we get <code>F=0</code> (as undefined). So we can see that lexicons <code>1</code> and <code>2</code> are situated between <code>3</code> (best) and <code>4</code> (worst), but this does not take into account the fact that <code>lexicon_2</code> has more highly ranked words than <code>lexicon_1</code> and should therefore receive a more favourable evaluation.</p>

<p>Is there any metric that would take into account the ranking of words in these cases? I am aware of <code>average precision</code> and <code>mean average precision</code>, which account for ranking of retrieved results, but I'm not sure if those metrics are appropriate for this situation.</p>

<p><strong>EDIT 1:</strong></p>

<p>I have implemented my understanding of <a href=""https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision"" rel=""nofollow noreferrer"">average precision</a> for this situation:</p>

<pre class=""lang-py prettyprint-override""><code>def avep(df, lexicon_names):
    """"""
    Calculate average precision for each lexicon from 
    pre-calculated precision and recall scores stored
    in a DataFrame.
    """"""
    aveps = {}
    for l in lexicon_names:
        ap = 0.0
        for i in range(len(df)):
            p_i = df.iloc[i]['p_' + l]
            if i &gt; 0:
                dr_i = df.iloc[i]['r_' + l] - df.iloc[i - 1]['r_' + l]
            else:
                dr_i = 0.0
            ap += p_i * dr_i
        aveps[l] = ap
    return aveps
</code></pre>

<p>This gives me the following results:</p>

<pre><code>Lexicon     p                   r                   f                   AP                  
-------     -                   -                   -                   --                  
lexicon_1 : 0.6666666666666666  0.2857142857142857  0.4                 0.2857142857142857  
lexicon_2 : 0.6666666666666666  0.2857142857142857  0.4                 0.14285714285714285 
lexicon_3 : 1.0                 1.0                 1.0                 0.8571428571428571  
lexicon_4 : 0.0                 0.0                 0.0                 0.0     
</code></pre>

<p>But this gives a better result for <code>lexicon_1</code> than <code>lexicon_2</code> which is the opposite of what I want (and AP is suspiciously equal to recall for <code>lexicon_1</code> and <code>2 x recall</code> for <code>lexicon_2</code>). Also, not sure what to do with the first row where difference in recall is undefined (using 0). This yields a value less than 1.0 for the ""perfect"" <code>lexicon_3</code>.</p>

<p>Any better suggestions would be much appreciated!</p>

<p><strong>EDIT 2:</strong></p>

<p>Here is my Python implementation of the solution provided by RobertBaron:</p>

<pre class=""lang-py prettyprint-override""><code>def coverage_metric(df, lexicon_names):
    scores = {}

    max_score = int(''.join([str(x) for x in df.index]))

    for l in lexicon_names:
        correct_score = int(''.join([str(x) for x in df.loc[df[l] == True].index]))
        incorrect_score = int(''.join([str(x) for x in df.loc[df[l] == False].index]))
        scores[l] = (correct_score - incorrect_score) / max_score

    scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)

    return scores
</code></pre>

<p>This is the implementation of the number conversion required for RobertBaron's solution (as far as I understand it):</p>

<pre class=""lang-py prettyprint-override""><code>def int2base(n, b):
    """"""
    Implementation of the algorith; described at http://www.cs.trincoll.edu/~ram/cpsc110/inclass/conversions.html
    """"""
    x = ''
    while n &gt; 0:
        d = int(n / b)
        r = n % b
        x += str(r)
        n = d

    return int(x)
</code></pre>
",Training and Model Evaluation,metric ranked keyword identification trying determine metric use evaluate coverage lexicon list word respect ranked list significant keywords extracted two different collection text two list term common importantly top n ranking word considered correct bottom ranked word considered incorrect dummy example ranked keyword list different lexicon contain word list intuition high ranking keywords included lexicon better fewer lower ranking keywords included better calculating precision p recall r f score f one way quantifying comparing coverage true positive tp lexicon includes correct word false positive fp includes incorrect word true negative tn excludes incorrect word false negative fn excludes correct word mind example give also get get get undefined see lexicon situated best worst doe take account fact ha highly ranked word therefore receive favourable evaluation metric would take account ranking word case aware account ranking retrieved result sure metric appropriate situation edit implemented understanding average precision situation give following result give better result opposite want ap suspiciously equal recall also sure first row difference recall undefined using yield value le perfect better suggestion would much appreciated edit python implementation solution provided robertbaron implementation number conversion required robertbaron solution far understand
Spacy NER Model Training Data Improvement,"<p>Am newer to NLP, Try to create NER model with help of <a href=""https://spacy.io"" rel=""nofollow noreferrer"">spacy.io</a>, I just Create my own NER Model for ORG entity <a href=""https://spacy.io/usage/training#ner"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#ner</a>. Trained Data size was 100 and Trained data look like this.,</p>

<pre><code>TRAIN_DATA = [
    (""2003 -2005 Pergo Inc. Software Analyst\Database Administrator"", {""entities"": [(11, 20, ""ORG"")]}),
    (""PROFESSIONAL EXPERIENCE Client: WPS Health Solutions, Madison, WI                           Mar17 - Till Date Role: RPA Developer"", {""entities"": [(32, 52, ""ORG"")]}),
    (""Client: National Institutes of Health (NIH/NIAMS), Bethesda, MD             Jan15 - Feb17 Role: RPA Developer"", {""entities"": [(8, 36, ""ORG"")]}),
    (""Client: Wells Fargo, Fremont, CA                                                   July14 - Dec14 Role: .Net/SharePoint Developer"", {""entities"": [(8, 19, ""ORG"")]}),
]
</code></pre>

<p>Now I Test my sentence with my Trained Model. If am used trained data I got perfect company name.</p>

<pre><code>doc = nlp('Client: Ananth Technologies Limited, Hyderabad, India Feb11- July12 Role: QA Automation Tester')
print(""Organization"", [(ent.text, ent.label_) for ent in doc.ents])
</code></pre>

<p>Organization [(u'Ananth Technologies Limited', u'ORG')]</p>

<p>but I passed new sentence it partially detect.</p>

<pre><code>doc = nlp('Client: MOUNTAIN HIGH HOME BUILDERS, Loveland, CO Application Engineer 8/03-5/10')
print(""Organization"", [(ent.text, ent.label_) for ent in doc.ents])
</code></pre>

<p>Organization [(u'MOUNTAIN HIGH', u'ORG')]</p>

<p>Now I gradually increase my Trained data, accuracy increased at same time predict wrong word as ORG. My trained data(sentence) is look different with each like Date,Designation,location,etc..., in different places not in order you can see above(TRAIN_DATA). Now am Struck with here and My question is am in right way?</p>

<p>Can anyone please suggest me any idea to improve my model?</p>

<p>Thanks</p>
",Training and Model Evaluation,spacy ner model training data improvement newer nlp try create ner model help spacy io create ner model org entity trained data size wa trained data look like test sentence trained model used trained data got perfect company name organization u ananth technology limited u org passed new sentence partially detect organization u mountain high u org gradually increase trained data accuracy increased time predict wrong word org trained data sentence look different like date designation location etc different place order see train data struck question right way anyone please suggest idea improve model thanks
Finetuning BERT on custom data,"<p>I want to train a <strong>21 class</strong> text classification model using Bert. But I have very little training data, so a downloaded a similar dataset with <strong>5 classes</strong> with 2 million samples.t 
And finetuned downloaded data with uncased pretrained model provided by bert.
And got about 98% validation accuracy.
Now, I want to use this model as pretrained model for my small custom data.
But I am getting <code>shape mismatch with tensor output_bias from checkpoint reader</code> error as the check-point model has 5 classes and my custom data has 21 classes.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>NFO:tensorflow:Calling model_fn.
INFO:tensorflow:Running train on CPU
INFO:tensorflow:*** Features ***
INFO:tensorflow:  name = input_ids, shape = (32, 128)
INFO:tensorflow:  name = input_mask, shape = (32, 128)
INFO:tensorflow:  name = is_real_example, shape = (32,)
INFO:tensorflow:  name = label_ids, shape = (32, 21)
INFO:tensorflow:  name = segment_ids, shape = (32, 128)
Tensor(""IteratorGetNext:3"", shape=(32, 21), dtype=int32)
WARNING:tensorflow:From /home/user/Spine_NLP/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /home/user/Spine_NLP/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
INFO:tensorflow:num_labels:21;logits:Tensor(""loss/BiasAdd:0"", shape=(32, 21), dtype=float32);labels:Tensor(""loss/Cast:0"", shape=(32, 21), dtype=float32)
INFO:tensorflow:Error recorded from training_loop: Shape of variable output_bias:0 ((21,)) doesn't match with shape of tensor output_bias ([5]) from checkpoint reader.</code></pre>
</div>
</div>
</p>
",Training and Model Evaluation,finetuning bert custom data want train class text classification model using bert little training data downloaded similar dataset class million sample finetuned downloaded data uncased pretrained model provided bert got validation accuracy want use model pretrained model small custom data getting error check point model ha class custom data ha class
Pytorch minibatching keeps model from training,"<p>I am trying to classify sequences by a binary feature. I have a dataset of sequence/label pairs and am using a simple one-layer LSTM to classify each sequence. Before I implemented minibatching, I was getting reasonable accuracy on a test set (80%), and the training loss would go from 0.6 to 0.3 (averaged).</p>

<p>I implemented minibatching, using parts of this tutorial: <a href=""https://pytorch.org/tutorials/beginner/chatbot_tutorial.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/beginner/chatbot_tutorial.html</a></p>

<p>However, now my model won’t do better than 70-72% (70% of the data has one label) with batch size set to 1 and all other parameters exactly the same. Additionally, the loss starts out at 0.0106 and quickly gets really really small, with no significant change in results. I feel like the results between no batching and batching with size 1 should be the same, so I probably have a bug, but for the life of me I can’t find it. My code is below.</p>

<p>Training code (one epoch):</p>

<pre><code>for i in t:
    model.zero_grad()

    # prep inputs
    last = i+self.params['batch_size']
    last = last if last &lt; len(train_data) else len(train_data)
    batch_in, lengths, batch_targets = self.batch2TrainData(train_data[shuffled][i:last], word_to_ix, label_to_ix)

    iters += 1

    # forward pass.
    tag_scores = model(batch_in, lengths)

    # compute loss, then do backward pass, then update gradients
    loss = loss_function(tag_scores, batch_targets)
    loss.backward()

    # Clip gradients: gradients are modified in place
    nn.utils.clip_grad_norm_(model.parameters(), 50.0)

    optimizer.step()
</code></pre>

<p>Functions:</p>

<pre><code>def prep_sequence(self, seq, to_ix):
    idxs = [to_ix[w] for w in seq]
    return torch.tensor(idxs, dtype=torch.long)

# transposes batch_in
def zeroPadding(self, l, fillvalue=0):
    return list(itertools.zip_longest(*l, fillvalue=fillvalue))

# Returns padded input sequence tensor and lengths
def inputVar(self, batch_in, word_to_ix):
    idx_batch = [self.prep_sequence(seq, word_to_ix) for seq in batch_in]
    lengths = torch.tensor([len(idxs) for idxs in idx_batch])
    padList = self.zeroPadding(idx_batch)
    padVar = torch.LongTensor(padList)
    return padVar, lengths

# Returns all items for a given batch of pairs
def batch2TrainData(self, batch, word_to_ix, label_to_ix):
    # sort by dec length
    batch = batch[np.argsort([len(x['turn']) for x in batch])[::-1]]
    input_batch, output_batch = [], []
    for pair in batch:
        input_batch.append(pair['turn'])
        output_batch.append(pair['label'])
    inp, lengths = self.inputVar(input_batch, word_to_ix)
    output = self.prep_sequence(output_batch, label_to_ix)
    return inp, lengths, output
</code></pre>

<p>Model:</p>

<pre><code>class LSTMClassifier(nn.Module):

    def __init__(self, params, vocab_size, tagset_size, weights_matrix=None):
        super(LSTMClassifier, self).__init__()
        self.hidden_dim = params['hidden_dim']

        if weights_matrix is not None:
            self.word_embeddings = nn.Embedding.from_pretrained(weights_matrix)
        else:
            self.word_embeddings = nn.Embedding(vocab_size, params['embedding_dim'])

        self.lstm = nn.LSTM(params['embedding_dim'], self.hidden_dim, bidirectional=False)

        # The linear layer that maps from hidden state space to tag space
        self.hidden2tag = nn.Linear(self.hidden_dim, tagset_size)

    def forward(self, batch_in, lengths):
        embeds = self.word_embeddings(batch_in)
        packed = nn.utils.rnn.pack_padded_sequence(embeds, lengths)
        lstm_out, _ = self.lstm(packed)
        outputs, _ = nn.utils.rnn.pad_packed_sequence(lstm_out)
        tag_space = self.hidden2tag(outputs)
        tag_scores = F.log_softmax(tag_space, dim=0)
        return tag_scores[-1]
</code></pre>
",Training and Model Evaluation,pytorch minibatching keep model training trying classify sequence binary feature dataset sequence label pair using simple one layer lstm classify sequence implemented minibatching wa getting reasonable accuracy test set training loss would go averaged implemented minibatching using part tutorial however model better data ha one label batch size set parameter exactly additionally loss start quickly get really really small significant change result feel like result batching batching size probably bug life find code training code one epoch function model
More than one words(say phrase) vector in word2vec,"<p>Word2vec is really great. 
Now I have a pretrained model(glove.txt), I hava ""machine"" vector and ""learning"" vector, but how can I get ""machine learning"" vector?
Existing methods include adding ""macine_learning"" to corpus and training, which not works for me cause I cannot train the model with so large corpus;
Another method is get the mean of ""machine"" and ""learning"" vectors.
I wanna if there is other ways</p>
",Training and Model Evaluation,one word say phrase vector word vec word vec really great pretrained model glove txt hava machine vector learning vector get machine learning vector existing method include adding macine learning corpus training work cause train model large corpus another method get mean machine learning vector wan na way
Language model to score sentence?,"<p>I have a corpus with bad sentences and good ones. I need to train a RNN language model to give quality score to every sentence. I tried <a href=""https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb"" rel=""nofollow noreferrer"">tensorflow ptb</a> model. It works during training. But the project doesn't give much info about how to use the model. I ran into all sorts of problem when trying to modify it to score sentence.
Is there an example showing how to use.the model?</p>

<p>Also, another project  <a href=""https://www.tensorflow.org/tutorials/keras/basic_text_classification"" rel=""nofollow noreferrer"">IMDB comment classification</a> caught my eyes. Is it a good idea to use that model to classify good sentences and bad ones?</p>
",Training and Model Evaluation,language model score sentence corpus bad sentence good one need train rnn language model give quality score every sentence tried tensorflow ptb model work training project give much info use model ran sort problem trying modify score sentence example showing use model also another project imdb comment classification caught eye good idea use model classify good sentence bad one
How to automate the task of adding the training phrases to correct intent inside dialogflow using any nlp model,"<p>I have an agent on Dialogflow with some fixed number of intents. Currently whenever a phrase does not trigger an intent(i.e trigger a default intent), i manually add the phrase to some existing intent or create a new intent and add the phrase to it.
I want to automate this task of adding the new phrase to a particular intent using some machine learning, nlp classifier.</p>

<p>I have trained a intent classifier to classify the intent based on the phrase, but i am not really sure what my training data should be for this task.</p>

<p>Please refer to following diagram. The diagram shows the thing i want to achieve.
<a href=""https://drive.google.com/file/d/1-6VRHuxM7E5-7iBVu1uo5SmYZpf6y71l/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1-6VRHuxM7E5-7iBVu1uo5SmYZpf6y71l/view?usp=sharing</a></p>
",Training and Model Evaluation,automate task adding training phrase correct intent inside dialogflow using nlp model agent dialogflow fixed number intent currently whenever phrase doe trigger intent e trigger default intent manually add phrase existing intent create new intent add phrase want automate task adding new phrase particular intent using machine learning nlp classifier trained intent classifier classify intent based phrase really sure training data task please refer following diagram diagram show thing want achieve
RNN Keras model for NLP takes lot of time while training with no decrease in validation loss,"<p>I have built an RNN model for entity recognition. I used BERT embedding and then processed the results through a RNN Model. However, while training the model for 5 epochs, each epoch seems to take about 2 hours. Also, the validation loss does not seem to decrease at all. </p>

<p>I am running the process on a RTX 2080 GPU. I have tried manipulating the model but does not improve the model. The dataset I have is of about 400000 sentences.</p>

<p>This is my model:</p>

<pre><code>def build_model(max_seq_length, n_tags): 
    in_id = Input(shape=(max_seq_length,), name=""input_ids"")
    in_mask = Input(shape=(max_seq_length,), name=""input_masks"")
    in_segment = Input(shape=(max_seq_length,), name=""segment_ids"")

    bert_inputs = [in_id, in_mask, in_segment]   
    bert_output = BertLayer(n_fine_tune_layers=3, pooling=""first"")(bert_inputs)
    x = RepeatVector(max_seq_length)(bert_output)
    x = Bidirectional(LSTM(units=lstm_units, return_sequences=True,
                           recurrent_dropout=0.2, dropout=0.2))(x)
    x_rnn = Bidirectional(LSTM(units=lstm_units, return_sequences=True,
                               recurrent_dropout=0.2, dropout=0.2))(x)
    x = add([x, x_rnn])  # residual connection to the first biLSTM
    pred = TimeDistributed(Dense(n_tags, activation=""softmax""))(x)

    model = Model(inputs=bert_inputs, outputs=pred)
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    return model
</code></pre>

<p>This is the model summary:</p>

<pre><code>Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_ids (InputLayer)          (None, 30)           0                                            
__________________________________________________________________________________________________
input_masks (InputLayer)        (None, 30)           0                                            
__________________________________________________________________________________________________
segment_ids (InputLayer)        (None, 30)           0                                            
__________________________________________________________________________________________________
bert_layer_3 (BertLayer)        ((None, 30), 768)    110104890   input_ids[0][0]                  
                                                                 input_masks[0][0]                
                                                                 segment_ids[0][0]                
__________________________________________________________________________________________________
repeat_vector_2 (RepeatVector)  ((None, 30), 30, 768 0           bert_layer_3[0][0]               
__________________________________________________________________________________________________
bidirectional_2 (Bidirectional) ((None, 30), 30, 200 695200      repeat_vector_2[0][0]            
__________________________________________________________________________________________________
bidirectional_3 (Bidirectional) ((None, 30), 30, 200 240800      bidirectional_2[0][0]            
__________________________________________________________________________________________________
add_1 (Add)                     ((None, 30), 30, 200 0           bidirectional_2[0][0]            
                                                                 bidirectional_3[0][0]            
__________________________________________________________________________________________________
time_distributed_1 (TimeDistrib ((None, 30), 30, 3)  603         add_1[0][0]                      
==================================================================================================
Total params: 111,041,493
Trainable params: 22,790,811
Non-trainable params: 88,250,682
__________________________________________________________________________________________________

</code></pre>

<p>Logs:</p>

<pre><code> 32336/445607 [=&gt;............................] - ETA: 2:12:59 - loss: 0.3469 - acc: 0.9068
 32352/445607 [=&gt;............................] - ETA: 2:12:58 - loss: 0.3469 - acc: 0.9068
 32368/445607 [=&gt;............................] - ETA: 2:12:58 - loss: 0.3469 - acc: 0.9068
</code></pre>

<p>Can you help me find out where I am going wrong?</p>
",Training and Model Evaluation,rnn kera model nlp take lot time training decrease validation loss built rnn model entity recognition used bert embedding processed result rnn model however training model epoch epoch seems take hour also validation loss doe seem decrease running process rtx gpu tried manipulating model doe improve model dataset sentence model model summary log help find going wrong
How to increase accuracy of lstm training,"<p>I trained quora question pair detection with LSTM but training accuracy is very low and always changes when i train. I dont understand what mistake i did.</p>

<p>I tried changing loss and optimiser and with increased epoch.</p>

<pre><code>import numpy as np
from numpy import array
from keras.callbacks import ModelCheckpoint
import keras
from keras.optimizers import SGD
import tensorflow as tf
from sklearn import preprocessing
import xgboost as xgb
from keras import backend as K
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from keras.preprocessing.text import Tokenizer , text_to_word_sequence
from keras.preprocessing.sequence import pad_sequences
from keras.layers.embeddings import Embedding
from keras.models import Sequential, model_from_json, load_model
from keras.layers import LSTM, Dense, Input, concatenate, Concatenate,             Activation, Flatten
 from keras.models import Model
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import     TfidfVectorizer,CountVectorizer
import nltk

from nltk.stem.lancaster import LancasterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import pandas as pd
import scipy
import matplotlib.pyplot as plt
import pickle

df = pd.read_csv(""questions.csv"")
df.drop(['id','qid1', 'qid2'], axis=1, inplace=True)

df2 = pd.read_csv(""testmenew.csv"") 
</code></pre>

## TO filter the datset

<pre><code> SPECIAL_TOKENS = {
    'quoted': 'quoted_item',
    'non-ascii': 'non_ascii_word',
    'undefined': 'something'
}

def clean(text, stem_words=True):
    import re
    from string import punctuation
    from nltk.stem import SnowballStemmer
    from nltk.corpus import stopwords

    def pad_str(s):
        return ' '+s+' '

    if pd.isnull(text):
        return ''

    if type(text) != str or text=='':
        return ''

    text = re.sub(""\'s"", "" "", text) 
    text = re.sub("" whats "", "" what is "", text, flags=re.IGNORECASE)
    text = re.sub(""\'ve"", "" have "", text)
    text = re.sub(""can't"", ""can not"", text)
    text = re.sub(""n't"", "" not "", text)
    text = re.sub(""i'm"", ""i am"", text, flags=re.IGNORECASE)
    text = re.sub(""\'re"", "" are "", text)
    text = re.sub(""\'d"", "" would "", text)
    text = re.sub(""\'ll"", "" will "", text)
    text = re.sub(""e\.g\."", "" eg "", text, flags=re.IGNORECASE)
    text = re.sub(""b\.g\."", "" bg "", text, flags=re.IGNORECASE)
    text = re.sub(""(\d+)(kK)"", "" \g&lt;1&gt;000 "", text)
    text = re.sub(""e-mail"", "" email "", text, flags=re.IGNORECASE)
    text = re.sub(""(the[\s]+|The[\s]+)?U\.S\.A\."", "" America "", text,    flags=re.IGNORECASE)
    text = re.sub(""(the[\s]+|The[\s]+)?United State(s)?"", "" America "",  text, flags=re.IGNORECASE)
     text = re.sub(""\(s\)"", "" "", text, flags=re.IGNORECASE)
    text = re.sub(""[c-fC-F]\:\/"", "" disk "", text)

    text = re.sub('(?&lt;=[0-9])\,(?=[0-9])', """", text)
    text = re.sub('\$', "" dollar "", text)
    text = re.sub('\%', "" percent "", text)
    text = re.sub('\&amp;', "" and "", text)     
    text = re.sub('[^\x00-\x7F]+', pad_str(SPECIAL_TOKENS['non-ascii']), text)  
    text = re.sub(""(?&lt;=[0-9])rs "", "" rs "", text, flags=re.IGNORECASE)
    text = re.sub("" rs(?=[0-9])"", "" rs "", text, flags=re.IGNORECASE)
    text = re.sub(r"" (the[\s]+|The[\s]+)?US(A)? "", "" America "", text)
    text = re.sub(r"" UK "", "" England "", text, flags=re.IGNORECASE)
    text = re.sub(r"" india "", "" India "", text)
    text = re.sub(r"" switzerland "", "" Switzerland "", text)
    text = re.sub(r"" china "", "" China "", text)
    text = re.sub(r"" chinese "", "" Chinese "", text) 
    text = re.sub(r"" imrovement "", "" improvement "", text, flags=re.IGNORECASE)
    text = re.sub(r"" intially "", "" initially "", text, flags=re.IGNORECASE)
    text = re.sub(r"" quora "", "" Quora "", text, flags=re.IGNORECASE)
    text = re.sub(r"" dms "", "" direct messages "", text,   flags=re.IGNORECASE)  
    text = re.sub(r"" demonitization "", "" demonetization "", text, flags=re.IGNORECASE) 
    text = re.sub(r"" actived "", "" active "", text, flags=re.IGNORECASE)
    text = re.sub(r"" kms "", "" kilometers "", text, flags=re.IGNORECASE)
    text = re.sub(r"" cs "", "" computer science "", text, flags=re.IGNORECASE) 
     text = re.sub(r"" upvote"", "" up vote"", text, flags=re.IGNORECASE)
    text = re.sub(r"" iPhone "", "" phone "", text, flags=re.IGNORECASE)
    text = re.sub(r"" \0rs "", "" rs "", text, flags=re.IGNORECASE)
    text = re.sub(r"" calender "", "" calendar "", text, flags=re.IGNORECASE)
     text = re.sub(r"" ios "", "" operating system "", text, flags=re.IGNORECASE)
     text = re.sub(r"" gps "", "" GPS "", text, flags=re.IGNORECASE)
    text = re.sub(r"" gst "", "" GST "", text, flags=re.IGNORECASE)
    text = re.sub(r"" programing "", "" programming "", text, flags=re.IGNORECASE)
    text = re.sub(r"" bestfriend "", "" best friend "", text, flags=re.IGNORECASE)
    text = re.sub(r"" dna "", "" DNA "", text, flags=re.IGNORECASE)
    text = re.sub(r"" III "", "" 3 "", text)
    text = re.sub(r"" banglore "", "" Banglore "", text, flags=re.IGNORECASE)
    text = re.sub(r"" J K "", "" JK "", text, flags=re.IGNORECASE)
    text = re.sub(r"" J\.K\. "", "" JK "", text, flags=re.IGNORECASE)
    text = re.sub('[0-9]+\.[0-9]+', "" 87 "", text)
    text = ''.join([c for c in text if c not in punctuation]).lower()
    return text

    text = re.sub('(?&lt;=[0-9])\,(?=[0-9])', """", text)

 df['question1'] = df['question1'].apply(clean)
 df['question2'] = df['question2'].apply(clean)

df2['q1'] = df2['q1'].apply(clean)
df2['q2'] = df2['q2'].apply(clean)

main =df['is_duplicate'].values

main.shape
(404351,)


vocabularySize = 20000
 lstm_out = 200
embed_dim = 128

Rawdata=df['question1'].apply(word_tokenize)
Rawdata2=df['question2'].apply(word_tokenize)

testme = df2['q1'].apply(word_tokenize)
testme2=df2['q2'].apply(word_tokenize)

tokenizer2 = Tokenizer(num_words = vocabularySize )

tokenizer2.fit_on_texts(testme)
tokenizer2.fit_on_texts(testme2)

tokenizer = Tokenizer(num_words = vocabularySize )

tokenizer.fit_on_texts(Rawdata)
tokenizer.fit_on_texts(Rawdata2)

 sequences = tokenizer.texts_to_sequences(Rawdata)
sequences2 = tokenizer.texts_to_sequences(Rawdata2)

sequences3 = tokenizer2.texts_to_sequences(testme)
sequences4 = tokenizer2.texts_to_sequences(testme2)

data = pad_sequences(sequences, maxlen=2)
data2 = pad_sequences(sequences2, maxlen=2)

data3 = pad_sequences(sequences3, maxlen=2)
data4 = pad_sequences(sequences4, maxlen=2)

TestInput = np.array([data3,data4])
TestInput = TestInput.reshape(1,2,2)
Input = np.array([data,data2])
Input =  Input.reshape(404351,2,2)

#opt = SGD(lr = 0.001, momentum = 0.60)

model = Sequential()
#model.add(Embedding(1, 4,input_length = 2 , dropout = 0.4))
model.add(LSTM((1), input_shape = (2,2), return_sequences=False))
model.add(Activation ('sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=['accuracy'])
X_train,X_test,y_train,y_test = train_test_split(Input,main,test_size = 0.2,random_state = 4)

Input.shape
(404351, 2, 2)

history = model.fit(X_train,y_train,epochs = 10,validation_data=   (X_test,y_test) )
model.save_weights('newoutput2.h5') 
</code></pre>

<p>Train on 323480 samples, validate on 80871 samples
Epoch 1/10
323480/323480 [==============================] - 27s 83us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323
Epoch 2/10
323480/323480 [==============================] - 24s 73us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323
Epoch 3/10
323480/323480 [==============================] - 23s 71us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323
Epoch 4/10
323480/323480 [==============================] - 23s 71us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323
Epoch 5/10
323480/323480 [==============================] - 23s 72us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323
Epoch 6/10
323480/323480 [==============================] - 23s 71us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323
Epoch 7/10
323480/323480 [==============================] - 23s 71us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323
Epoch 8/10
323480/323480 [==============================] - 25s 76us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323
Epoch 9/10
323480/323480 [==============================] - 25s 78us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323
Epoch 10/10
323480/323480 [==============================] - 25s 78us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323
​</p>

<pre><code>filename = 'newoutput2.h5'
model.load_weights(filename)
new = model.predict(TestInput)
if new &gt; 0.6:
    print(""Duplication detected"")
else:
    print(""No duplicate"")
new 

giving output around 0.6567 but not atall increasing, Please help !!
</code></pre>

<p>I need to Increase accuracy of training</p>
",Training and Model Evaluation,increase accuracy lstm training trained quora question pair detection lstm training accuracy low always change train dont understand mistake tried changing loss optimiser increased epoch filter datset train sample validate sample epoch u step loss acc val loss val acc epoch u step loss acc val loss val acc epoch u step loss acc val loss val acc epoch u step loss acc val loss val acc epoch u step loss acc val loss val acc epoch u step loss acc val loss val acc epoch u step loss acc val loss val acc epoch u step loss acc val loss val acc epoch u step loss acc val loss val acc epoch u step loss acc val loss val acc need increase accuracy training
How can I evaluate out-of-domain question in a domain-specific Q&amp;A bot when I only have in-domain data?,"<p>I learned that some popular bots like RASA or LUIS will have ""confidence scores"" to evaluate the out-of-domain questions, but none of them provide documentation of how they calculate these scores. Also, information retrieval has some approaches to compute similarity, but I don't know what approaches it will use for out-of-domain classification. Could someone give me some ideas about which papers, directions, or codes I can work on?</p>
",Training and Model Evaluation,evaluate domain question domain specific q bot domain data learned popular bot like rasa luis confidence score evaluate domain question none provide documentation calculate score also information retrieval ha approach compute similarity know approach use domain classification could someone give idea paper direction code work
Efficient metrics evaluation in PyTorch,"<p>I am new to PyTorch and want to efficiently evaluate among others F1 during my Training and my Validation Loop.</p>

<p>So far, my approach was to calculate the predictions on GPU, then push them to CPU and append them to a vector for both Training and Validation. After Training and Validation, I would evaluate both for each epoch using sklearn. However, profiling my code it showed, that pushing to cpu is quite a bottleneck. </p>

<pre class=""lang-py prettyprint-override""><code>for epoch in range(n_epochs):
    model.train()
    avg_loss = 0
    avg_val_loss = 0
    train_pred = np.array([])
    val_pred = np.array([])
    # Training loop (transpose X_batch to fit pretrained (features, samples) style)
    for X_batch, y_batch in train_loader:
        scores = model(X_batch)
        y_pred = F.softmax(scores, dim=1)
        train_pred = np.append(train_pred, self.get_vector(y_pred.detach().cpu().numpy()))

        loss = loss_fn(scores, self.get_vector(y_batch))
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        avg_loss += loss.item() / len(train_loader)

    model.eval()
    # Validation loop
    for X_batch, y_batch in val_loader:
        with torch.no_grad():
            scores = model(X_batch)
            y_pred = F.softmax(scores, dim=1)
            val_pred = np.append(val_pred, self.get_vector(y_pred.detach().cpu().numpy()))
            loss = loss_fn(scores, self.get_vector(y_batch))
            avg_val_loss += loss.item() / len(val_loader)

    # Model Checkpoint for best validation f1
    val_f1 = self.calculate_metrics(train_targets[val_index], val_pred, f1_only=True)
    if val_f1 &gt; best_val_f1:
        prev_best_val_f1 = best_val_f1
        best_val_f1 = val_f1
        torch.save(model.state_dict(), self.PATHS['xlm'])
        evaluated_epoch = epoch

    # Calc the metrics
    self.save_metrics(train_targets[train_index], train_pred, avg_loss, 'train')
    self.save_metrics(train_targets[val_index], val_pred, avg_val_loss, 'val')
</code></pre>

<p>I am certain there is a more efficient way to 
a) store the predictions without having to push them to cpu each batch. b) calculate the metrics on GPU directly?</p>

<p>As I am new to PyTorch, I am very grateful for any hints and feedback :)</p>
",Training and Model Evaluation,efficient metric evaluation pytorch new pytorch want efficiently evaluate among others f training validation loop far approach wa calculate prediction gpu push cpu append vector training validation training validation would evaluate epoch using sklearn however profiling code showed pushing cpu quite bottleneck certain efficient way store prediction without push cpu batch b calculate metric gpu directly new pytorch grateful hint feedback
How to build my training data in my case to train a SVM in classifier in scikit-learn?,"<p>I have sentences, coming from research studies, and there manually extracted word phrases, which are the key words of the sentences I want to have. Now to build the train data for a SVM classifier I would like to vectorize the sentences together with each keywords.  See code</p>

<p>I was thinking about a dictionary and the applying DictVectorizer from the sklearn-Library. </p>

<pre><code>Code:

sklearn.feature_extraction import DictVectorizer

v = DictVectorizer()

D = [{""sentence"":""the laboratory information system was evaluated"", 
       ""keyword"":""laboratory information system""},
     {""sentence"":""the electronic health record system was evaluated"", 
      ""keyword"":""electronic health record system""}]

X = v.fit_transform(D)

print(X)

content = X.toarray()

print(content)

print(v.get_feature_names())

Results:

 (0, 1) 1.0
  (0, 3)    1.0
  (1, 0)    1.0
  (1, 2)    1.0

[[0. 1. 0. 1.]
 [1. 0. 1. 0.]]

['keyword=electronic health record system', 'keyword=laboratory information system', 'sentence=the electronic health record system was evaluated', 'sentence=the laboratory information system was evaluated']
</code></pre>

<p>Is this methodological correct or how can I bring together each sentence with the according manually extracted keyword for vectorizing to reveice the training data. Thanks a lot.</p>
",Training and Model Evaluation,build training data case train svm classifier scikit learn sentence coming research study manually extracted word phrase key word sentence want build train data svm classifier would like vectorize sentence together keywords see code wa thinking dictionary applying dictvectorizer sklearn library methodological correct bring together sentence according manually extracted keyword vectorizing reveice training data thanks lot
"Add training data(text, melspectrogram, spectrogram, label(.wav) files) to Tensorflow 2.0","<ol>
<li>How to add training data(text, melspectrogram, label(.wav)) in Tensorflow 2.0 like tutorials on Tensorflow website?</li>
<li>How to train that model with gradient tape in tf 2.0? I don't know what is the next step to do? I have built the model with layers with tf.keras.layers and tf.sequence_mask. I am replicating deep voice 3 model to do TTS.</li>
</ol>

<pre class=""lang-py prettyprint-override""><code>def model():
    def __init__(self):
        with tf.GradientTape() as tape:
            self.char2idx, self.idx2char = load_vocab()
            self.x, self.y1, self.y2, self.z, self.num_batch = get_batch()
            self.prev_max_attentions_li = tf.ones(shape=(hp.dec_layers, hp.batch_size), dtype=tf.int32)
            self.decoder_input = tf.concat((tf.zeros_like(self.y1[:, :1, -hp.n_mels:]), self.y1[:, :-1, -hp.n_mels:]), 1)
            self.keys, self.vals = Encoder(self.x)
            self.mel_logits, self.done_output, self.decoder_output, self.alignments_li, self.max_attentions_li = Decoder(self.decoder_input, self.keys, self.vals, self.prev_max_attentions_li)
            self.mel_output = tf.nn.sigmoid(self.mel_logits)
            self.converter_input = tf.reshape(self.decoder_output, (-1, hp.Ty, hp.char_embed//hp.r))
            self.converter_input = tf.keras.layers.Dense(hp.cchannel, activation = 'relu')(self.converter_input)
            self.mag_logits = Converter(self.converter_input)
            self.mag_output = tf.nn.sigmoid(self.mag_logits)
            self.global_step = tf.Variable(0, name='global_step', trainable=False)

            self.loss_mels = tf.reduce_mean(input_tensor=tf.abs(self.mel_output - self.y1))
            self.loss_dones = tf.reduce_mean(input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.done_output, labels=self.y2))
            self.loss_mags = tf.reduce_mean(input_tensor=tf.abs(self.mag_output - self.z))
            self.loss = self.loss_mels + self.loss_dones + self.loss_mags

            # Training Scheme
            self.optimizer = tf.keras.optimizer.Adam(lr=hp.lr)
            ## gradient clipping
            self.gvs = self.optimizer.compute_gradients(self.loss)
            self.clipped = []
            for grad, var in self.gvs:
                grad = tf.clip_by_value(grad, -1. * hp.max_grad_val, hp.max_grad_val)
                grad = tf.clip_by_norm(grad, hp.max_grad_norm)
                self.clipped.append((grad, var))
            self.train_op = self.optimizer.apply_gradients(self.clipped, global_step=self.global_step)

                # Summary
            tf.summary.scalar('Train_Loss/LOSS', self.loss)
            tf.summary.scalar('Train_Loss/mels', self.loss_mels)
            tf.summary.scalar('Train_Loss/dones', self.loss_dones)
            tf.summary.scalar('Train_Loss/mags', self.loss_mags)

            self.merged = tf.summary.merge_all()

</code></pre>
",Training and Model Evaluation,add training data text melspectrogram spectrogram label wav file tensorflow add training data text melspectrogram label wav tensorflow like tutorial tensorflow website train model gradient tape tf know next step built model layer tf kera layer tf sequence mask replicating deep voice model tt
Is there a way to evaluate losses on the test sample using spacy model,"<p>I am trying to create a binary classifier with spacy 2.1.3 and in order to perform an overfitting test I would like to evaluate losses on the test sample. In their tutorial losses are used as a parameter and somehow updated:</p>

<p><a href=""https://github.com/explosion/spaCy/blob/master/examples/training/train_textcat.py#L90"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/master/examples/training/train_textcat.py#L90</a> </p>

<p>I cannot find any example of how to evaluate it on my test sample. Ideally I would like to produce plots as show here:</p>

<p><a href=""https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/"" rel=""nofollow noreferrer"">https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/</a></p>

<p>I tried digging into their code but I didn't find anything useful. Has anyone tried to produce similar plots? </p>

<p>Thank you for your help and comments :) </p>
",Training and Model Evaluation,way evaluate loss test sample using spacy model trying create binary classifier spacy order perform overfitting test would like evaluate loss test sample tutorial loss used parameter somehow updated find example evaluate test sample ideally would like produce plot show tried digging code find anything useful ha anyone tried produce similar plot thank help comment
Identical results with different model parameters,"<p>I'm making a neural network model to assign sentiments to phrases. My problem is no matter what I change in my model, the accuracy score is always identical.</p>

<p>I have tried adding or removing layers, changing batch size, changing number of epochs, changing activators etc. </p>

<pre><code>tokenizer = Tokenizer()
total_phrases = phrases_train.append([phrases_valid,  phrases_test], 
                                     ignore_index = True) 

tokenizer.fit_on_texts(total_phrases)

maxlen = max([len(s.split()) for s in total_phrases])

vocab_size = len(tokenizer.word_index) + 1

phrases_train_tokens = tokenizer.texts_to_sequences(phrases_train)
phrases_valid_tokens = tokenizer.texts_to_sequences(phrases_valid)
phrases_test_tokens = tokenizer.texts_to_sequences(phrases_test)

pad_phrases_train = pad_sequences(phrases_train_tokens, maxlen=maxlen, 
                                  padding='post') 
pad_phrases_valid = pad_sequences(phrases_valid_tokens, maxlen=maxlen, 
                                  padding='post') 
pad_phrases_test = pad_sequences(phrases_valid_tokens, maxlen=maxlen, 
                                 padding='post')

model = Sequential()
model.add(Embedding(vocab_size, 100, input_length=maxlen))
model.add(Dense(50, kernel_initializer='truncated_normal', 
                activation='relu'))
model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='softmax'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics= 
              ['accuracy'])

model.fit(pad_phrases_train, sents_train, batch_size=250, epochs=50, 
          validation_data=(pad_phrases_valid, sents_valid), verbose=2)

model.evaluate(pad_phrases_test, sents_test)
</code></pre>

<p>Evaluate always shows exactly 0.2057 score. I want to achieve around 50% accuracy</p>
",Training and Model Evaluation,identical result different model parameter making neural network model assign sentiment phrase problem matter change model accuracy score always identical tried adding removing layer changing batch size changing number epoch changing activator etc evaluate always show exactly score want achieve around accuracy
is it possible to train spacy to identify any random organization name,"<p>is it possible to identify in spacy as an entity any random organization name in any language</p>

<p>i tried the pretrained model of spacy to identify the organization names but  but it fails on some places eg Rama works at Remote software</p>
",Training and Model Evaluation,possible train spacy identify random organization name possible identify spacy entity random organization name language tried pretrained model spacy identify organization name fails place eg rama work remote software
"Training model with CreateML MLTextClassifier, stopped by EXC_BAD_ACCESS (code=1, address=0x0)","<p>I'm trying to train my own NLP model with CreateML with Xcode playground, and going through the tutorial by Apple: <a href=""https://developer.apple.com/documentation/createml/creating_a_text_classifier_model"" rel=""nofollow noreferrer"">https://developer.apple.com/documentation/createml/creating_a_text_classifier_model</a></p>

<p>but the program terminated by EXC_BAD_ACCESS (code=1, address=0x0)</p>

<p>I found some solution from the Internet, they stated that the pointer is pointing to NULL when trying to access the variable</p>

<pre class=""lang-swift prettyprint-override""><code>import Foundation
import CreateML

let source = ""icecream""

let data = try MLDataTable(contentsOf: URL(fileURLWithPath: ""/path/to/\(source).csv""))

let (trainingData, testingData) = data.randomSplit(by: 0.8, seed: 0)

// program stopped here
let sentimentClassifier = try MLTextClassifier(trainingData: trainingData, textColumn: ""text"", labelColumn: ""sentiment"")
</code></pre>

<pre><code>// error
error: Execution was interrupted, reason: EXC_BAD_ACCESS (code=1, address=0x0).

// output
Finished parsing file /path/to/icecream.csv
Parsing completed. Parsed 100 lines in 0.03412 secs.
Finished parsing file /path/to/icecream.csv
Parsing completed. Parsed 188 lines in 0.008235 secs.
Automatically generating validation set from 10% of the data.
Tokenizing data and extracting features
Starting MaxEnt training with 146 samples
Iteration 1 training accuracy 0.650685
Iteration 2 training accuracy 0.869863
Iteration 3 training accuracy 0.945205
Iteration 4 training accuracy 0.986301
Iteration 5 training accuracy 0.993151
Finished MaxEnt training in 0.04 seconds
</code></pre>
",Training and Model Evaluation,training model createml mltextclassifier stopped exc bad access code address x trying train nlp model createml xcode playground going tutorial apple program terminated exc bad access code address x found solution internet stated pointer pointing null trying access variable
How can I test a word2vec over development data?,"<p>In a computer assignment, it's requested to implement word2vec algorithm to generate dense vectors for some words using a neural network. I implemented the neural network and trained it over training data. First, how can I test it over the test data? The question asks to draw a plot showing the perplexity for the training and test data during training (epochs). I can do that for the loss, which is something like this:</p>

<pre><code>EPOCH: 0 LOSS: 27030.09155006593
EPOCH: 0 P_LOSS: 24637.964948774144
EPOCH: 0 PP: inf
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:121: RuntimeWarning: overflow encountered in double_scalars
EPOCH: 1 LOSS: 25349.086587261085
EPOCH: 1 P_LOSS: 22956.95998596929
EPOCH: 1 PP: inf
EPOCH: 2 LOSS: 24245.455581381622
EPOCH: 2 P_LOSS: 21853.32898008983
EPOCH: 2 PP: inf
EPOCH: 3 LOSS: 23312.976009712416
EPOCH: 3 P_LOSS: 20920.849408420647
</code></pre>

<p>Which I gained by the following code:</p>

<pre><code>    # CYCLE THROUGH EACH EPOCH
    for i in range(0, self.epochs):

        self.loss = 0
        self.loss_prob = 0
        # CYCLE THROUGH EACH TRAINING SAMPLE
        for w_t, w_c in training_data:

            # FORWARD PASS
            y_pred, h, u = self.forward_pass(w_t)

            # CALCULATE ERROR
            EI = np.sum([np.subtract(y_pred, word) for word in w_c], axis=0)

            # BACKPROPAGATION
            self.backprop(EI, h, w_t)

            # CALCULATE LOSS
            self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))
            self.loss_prob += -2*np.log(len(w_c)) -np.sum([u[word.index(1)] for word in w_c]) + (len(w_c) * np.log(np.sum(np.exp(u))))

        print('EPOCH:',i, 'LOSS:', self.loss)
        print('EPOCH:',i, 'P_LOSS:', self.loss_prob)
        print('EPOCH:',i, 'PP:', 2**self.loss_prob)
</code></pre>

<p>However, I don't know how to find the perplexity for the training and development data in each epoch. Based on <a href=""https://stackoverflow.com/questions/53765598/calculate-perplexity-of-word2vec-model"">this question</a>, it's said perplexity is <code>2**loss</code>. However, as I tried this formula, I gained <code>INF</code>. Could you guide me how can I calculate perplexity? Can I do it in the current code, or should I apply a function over whole development data?</p>
",Training and Model Evaluation,test word vec development data computer assignment requested implement word vec algorithm generate dense vector word using neural network implemented neural network trained training data first test test data question asks draw plot showing perplexity training test data training epoch loss something like gained following code however know find perplexity training development data epoch based href question said perplexity however tried formula gained could guide calculate perplexity current code apply function whole development data
How to test a RASA model?,"<p>I'm trying to write my own chatbot with the RASA framework.<br>
Right now I'm just playing around with it and I have the following piece of code for training purposes.</p>

<pre><code>from rasa.nlu.training_data import load_data
from rasa.nlu.config import RasaNLUModelConfig
from rasa.nlu.model import Trainer
from rasa.nlu import config
training_data = load_data(""./data/nlu.md"")
trainer = Trainer(config.load(""config.yml""))
interpreter = trainer.train(training_data)
model_directory = trainer.persist(""./models/nlu"",fixed_model_name=""current"")
</code></pre>

<p>Now, I read that if I wanted to test it I should do something like this.</p>

<pre><code>from rasa.nlu.evaluate import run_evaluation
run_evaluation(""nlu.md"", model_directory)
</code></pre>

<p>But this code is not available anymore in rasa.nlu.evaluate nor in rasa.nlu.test! <br>
What's the way, then, of testing a RASA model?</p>
",Training and Model Evaluation,test rasa model trying write chatbot rasa framework right playing around following piece code training purpose read wanted test something like code available anymore rasa nlu evaluate rasa nlu test way testing rasa model
How to improve the confidence score of the intent in Rasa NLU?,"<p>I was working on Rasa NLU for intent classification, in <a href=""http://rasa.com/docs/nlu/master/quickstart/"" rel=""nofollow noreferrer"">link</a> how shall I improve the confidence score for a given intent. </p>

<p>I have tried to give more training data but still the confidence score isn't increasing. Can anyone please let me know which parameters \ hyperparameters I can tune in order to get good confidence score.
I did tried to all possible combinations provided in this <a href=""http://rasa.com/docs/nlu/master/choosing_pipeline/#choosing-pipeline"" rel=""nofollow noreferrer"">link</a> but still there was hardly any improvement.</p>

<p>I did checked the suggestion provided over here, but I am looking for granular tuning of the model such that it can perform better.</p>

<p>Thanks. </p>

<p><strong>Edit 1</strong>: Please provide a valid reason for down-voting the question. </p>
",Training and Model Evaluation,improve confidence score intent rasa nlu wa working rasa nlu intent classification link shall improve confidence score given intent tried give training data still confidence score increasing anyone please let know parameter hyperparameters tune order get good confidence score tried possible combination provided link still wa hardly improvement checked suggestion provided looking granular tuning model perform better thanks edit please provide valid reason voting question
Bad accuracy when prediction happens,"<p>After I trained my model for the toxic challenge at Keras the accuracy of the prediction is bad. I'm not sure if I'm doing something wrong, but the accuracy during the training period was pretty good ~0.98.</p>

<p><strong>How I trained</strong>  </p>

<pre><code>import sys, os, re, csv, codecs, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model
from keras import initializers, regularizers, constraints, optimizers, layers

train = pd.read_csv('train.csv')


list_classes = [""toxic"", ""severe_toxic"", ""obscene"", ""threat"", ""insult"", ""identity_hate""]
y = train[list_classes].values
list_sentences_train = train[""comment_text""]

max_features = 20000
tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(list(list_sentences_train))
list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)

maxlen = 200
X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)

inp = Input(shape=(maxlen, ))

embed_size = 128
x = Embedding(max_features, embed_size)(inp)
x = LSTM(60, return_sequences=True,name='lstm_layer')(x)
x = GlobalMaxPool1D()(x)
x = Dropout(0.1)(x)
x = Dense(50, activation=""relu"")(x)
x = Dropout(0.1)(x)
x = Dense(6, activation=""sigmoid"")(x)

model = Model(inputs=inp, outputs=x)
model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])

batch_size = 32
epochs = 2
print(X_t[0])
model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)

model.save(""m.hdf5"")
</code></pre>

<p><strong>This is how I predict</strong> </p>

<pre><code>model = load_model('m.hdf5')

list_sentences_train = np.array([""I love you Stackoverflow""])

max_features = 20000
tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(list(list_sentences_train))
list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)

maxlen = 200
X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)

print(X_t)

print(model.predict(X_t))
</code></pre>

<p><strong>Output</strong> </p>

<blockquote>
  <p>[[  1.97086316e-02   9.36032447e-05   3.93966911e-03   5.16672269e-04
      3.67353857e-03   1.28102733e-03]]</p>
</blockquote>
",Training and Model Evaluation,bad accuracy prediction happens trained model toxic challenge kera accuracy prediction bad sure something wrong accuracy training period wa pretty good trained predict output e e e e e e
IMDB dataset preprocessing unsuitable for GLoVe word embeddings?,"<p>I want to train a simple sentiment classifier on the IMDB dataset using pretrained GLoVe vectors, an LSTM and final dense layer with sigmoid activation.</p>

<p>The problem I have is that the obtained accuracy is relatively low: 78% . This is lower than the 82% accuracy when using a trainable embedding layer instead of GLoVe vectors.</p>

<p>I think the main reason for this is because only 67.9% of words in the dataset are found in the GLoVe file (I am using the 6B corpus).</p>

<p>I looked at some words which were not found in the GLoVe file and some examples are :</p>

<p>grandmother's 
twin's</p>

<p>Basically a lot of words that have a quote are not found in the GLoVe file.</p>

<p>I wonder if the data needs to be preprocessed differently. Currently, the preprocessing is taken care by the function <code>imdb.load_data()</code>.</p>

<p>I tried using the larger 42B words corpus, but that only resulted in 76.5% coverage.</p>

<p>I wonder if the data ought to be tokenized differently to get a good coverage.</p>

<p>The code is this:</p>

<p><strong>load_embeddings.py</strong></p>

<pre class=""lang-py prettyprint-override""><code>from numpy import asarray
import time

def load_embeddings(filename):
    start_time = time.time()
    embeddings_index = dict()
    f = open(filename, encoding = 'utf8')
    for line in f:
        values = line.split()
        word = values[0]
        embedding_vector = asarray(values[1:], dtype='float32')
        embeddings_index[word] = embedding_vector
    f.close()
    end_time = time.time()
    print('Loaded %s word vectors in %f seconds' % (len(embeddings_index), end_time- start_time))
    return embeddings_index
</code></pre>

<p><strong>train.py</strong></p>

<pre class=""lang-py prettyprint-override""><code>from __future__ import print_function
import numpy as np

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding
from keras.layers import LSTM
from keras.datasets import imdb
from load_embeddings import load_embeddings

maxlen = 80
batch_size = 32

print('Loading data...')
(x_train, y_train), (x_test, y_test) = imdb.load_data()
print(len(x_train), 'train sequences')
print(len(x_test), 'test sequences')

print('Pad sequences (samples x time)')
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)

word_to_index = imdb.get_word_index()
vocab_size = len(word_to_index)
print('Vocab size : ', vocab_size)


words_freq_list = []
for (k,v) in imdb.get_word_index().items():
  words_freq_list.append((k,v))

sorted_list = sorted(words_freq_list, key=lambda x: x[1])

print(""50 most common words: \n"")
print(sorted_list[0:50])


# dimensionality of word embeddings
EMBEDDING_DIM = 100

# Glove file
GLOVE_FILENAME = 'data/glove.6B.100d.txt'

# Word from this index are valid words. i.e  3 -&gt; 'the' which is the
# most frequent word
INDEX_FROM = 3

word_to_index = {k:(v+INDEX_FROM-1) for k,v in imdb.get_word_index().items()}
word_to_index[""&lt;PAD&gt;""] = 0
word_to_index[""&lt;START&gt;""] = 1
word_to_index[""&lt;UNK&gt;""] = 2

embeddings_index = load_embeddings(GLOVE_FILENAME)
# create a weight matrix for words in training docs
embedding_matrix = np.zeros((vocab_size+INDEX_FROM, EMBEDDING_DIM))
# unknown words are mapped to zero vector
embedding_matrix[0] = np.array(EMBEDDING_DIM*[0])
embedding_matrix[1] = np.array(EMBEDDING_DIM*[0])
embedding_matrix[2] = np.array(EMBEDDING_DIM*[0])

for word, i in word_to_index.items():
  embedding_vector = embeddings_index.get(word)
  if embedding_vector is not None:
    embedding_matrix[i] = embedding_vector
  # uncomment below to see which words were not found
  # else :
  #   print(word, ' not found in GLoVe file.')

nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))
coverage = nonzero_elements / vocab_size
print('Coverage = ',coverage)


# Build and train model

print('Build model...')
model = Sequential()
model.add(Embedding(vocab_size+INDEX_FROM, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False, name= 'embedding'))
model.add(LSTM(EMBEDDING_DIM, dropout=0.2, recurrent_dropout=0.2, name = 'lstm'))
model.add(Dense(1, activation='sigmoid', name='out'))

# try using different optimizers and different optimizer configs
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

print('Train...')
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=10,
          validation_data=(x_test, y_test))
score, acc = model.evaluate(x_test, y_test,
                            batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)
</code></pre>
",Training and Model Evaluation,imdb dataset preprocessing unsuitable glove word embeddings want train simple sentiment classifier imdb dataset using pretrained glove vector lstm final dense layer sigmoid activation problem obtained accuracy relatively low lower accuracy using trainable embedding layer instead glove vector think main reason word dataset found glove file using b corpus looked word found glove file example grandmother twin basically lot word quote found glove file wonder data need preprocessed differently currently preprocessing taken care function tried using larger b word corpus resulted coverage wonder data ought tokenized differently get good coverage code load embeddings py train py
How to Implement Perplexity in Keras?,"<p>I have been trying to evaluate language models and I need to keep track of perplexity metric.</p>

<p>What I tried is: since perplexity is 2^-J where J is the cross entropy:</p>

<pre><code>def perplexity(y_true, y_pred):
        oneoverlog2 = 1.442695
        return K.pow(2.0,K.mean(-K.log(y_pred)*oneoverlog2))
</code></pre>

<p>But this curiously goes to infinity during training within a few batches.</p>

<p>Is there some wrong with the implementation or any  other way to implement perplexity?</p>
",Training and Model Evaluation,implement perplexity kera trying evaluate language model need keep track perplexity metric tried since perplexity j j cross entropy curiously go infinity training within batch wrong implementation way implement perplexity
"Resource exhausted: OOM when allocating tensor with shape[845246,300]","<p>I am working with a sequence to sequence language model, and after changing the code to pass custom word embedding weights to the Embeddings layer, I am receiving a OOM error when I try to train on the gpu.</p>

<p>Here is the relevant code:</p>

<pre><code>def create_model(word_map, X_train, Y_train, vocab_size, max_length):
    # define model
    model = Sequential()
    # get custom embedding weights as matrix
    embedding_matrix = get_weights_matrix_from_word_map(word_map)
    model.add(Embedding(len(word_map)+1, 300, weights=[embedding_matrix], input_length=max_length-1))
    model.add(LSTM(50))
    model.add(Dense(vocab_size, activation='softmax'))
    print(model.summary())
    # compile network
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.fit(X_train, Y_train, epochs=100, verbose=2)
    return model
</code></pre>

<p>And here is the full error log from the server:</p>

<pre><code>    File ""/home2/slp24/thesis/UpdatedLanguageModel_7_31.py"", line 335, in create_model_2
    model.fit(X_train, Y_train, batch_size=32, epochs=1, verbose=2)  ## prev X, y
  File ""/opt/python-3.4.1/lib/python3.4/site-packages/keras/models.py"", line 963, in fit
    validation_steps=validation_steps)
  File ""/opt/python-3.4.1/lib/python3.4/site-packages/keras/engine/training.py"", line 1682, in fit
    self._make_train_function()
  File ""/opt/python-3.4.1/lib/python3.4/site-packages/keras/engine/training.py"", line 990, in _make_train_function
    loss=self.total_loss)
  File ""/opt/python-3.4.1/lib/python3.4/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
  File ""/opt/python-3.4.1/lib/python3.4/site-packages/keras/optimizers.py"", line 466, in get_updates
    m_t = (self.beta_1 * m) + (1. - self.beta_1) * g
  File ""/opt/python-3.4.1/lib/python3.4/site-packages/tensorflow/python/ops/math_ops.py"", line 898, in binary_op_wrapper
    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
  File ""/opt/python-3.4.1/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 932, in convert_to_tensor
    as_ref=False)
  File ""/opt/python-3.4.1/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 1022, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/opt/python-3.4.1/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.py"", line 100, in _IndexedSlicesToTensor
    value.values, value.indices, value.dense_shape[0], name=name)
  File ""/opt/python-3.4.1/lib/python3.4/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 5186, in unsorted_segment_sum
    num_segments=num_segments, name=name)
  File ""/opt/python-3.4.1/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/opt/python-3.4.1/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/opt/python-3.4.1/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[845246,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: training/Adam/mul_2/y = UnsortedSegmentSum[T=DT_FLOAT, Tindices=DT_INT32, Tnumsegments=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](training/Adam/gradients/embedding_1/Gather_grad/Reshape, training/Adam/gradients/embedding_1/Gather_grad/Reshape_1/_101, training/Adam/mul_2/strided_slice)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
</code></pre>

<p><strong>Edit:</strong></p>

<p>So far I have tried</p>

<ul>
<li>Adding batching, started with batch_size=32</li>
<li>I am currently working to decrease the number of output classes from 845,286. I think something went wrong when I calculated the custom embedding matrix, specifically when I was ""connecting"" the vocabulary token index's assigned during preprocessing and the y_categorical values assigned by Keras that the model uses...</li>
</ul>

<p>Any help or guidance is greatly appreciated! I have searched many similar issued but have not been able to apply those fixes to my code thus far. Thank you</p>
",Training and Model Evaluation,resource exhausted oom allocating tensor shape working sequence sequence language model changing code pas custom word embedding weight embeddings layer receiving oom error try train gpu relevant code full error log server edit far tried adding batching started batch size currently working decrease number output class think something went wrong calculated custom embedding matrix specifically wa connecting vocabulary token index assigned preprocessing categorical value assigned kera model us help guidance greatly appreciated searched many similar issued able apply fix code thus far thank
Is Naive Bayes biased?,"<p>I have  a use case where in text needs to be classified into one of the three categories. I started with Naive Bayes [Apache OpenNLP, Java] but i was informed that the algorithm is biased, meaning if my training data has 60% of data as classA and 30% as classB and 10% as classC then the algorithm tends to biased towards ClassA and thus predicting the other class texts to be of classA. </p>

<p>If this is true is there a way to overcome this issue? </p>

<p>There are other algorithm that i came across like SVM Classifier or logistic regression (maximum entropy model), however I am not sure which will be more suitable for my use case. Please advise.</p>
",Training and Model Evaluation,naive bayes biased use case text need classified one three category started naive bayes apache opennlp java wa informed algorithm biased meaning training data ha data classa classb classc algorithm tends biased towards classa thus predicting class text classa true way overcome issue algorithm came across like svm classifier logistic regression maximum entropy model however sure suitable use case please advise
gensim Word2Vec - how to apply stochastic gradient descent?,"<p>To my understanding, batch (vanilla) gradient descent makes one parameter update for all training data. Stochastic gradient descent (SGD) allows you to update parameter for each training sample, helping the model to converge faster, at the cost of high fluctuation in function loss. </p>

<p><a href=""https://i.sstatic.net/0LGA5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0LGA5.png"" alt=""enter image description here""></a></p>

<p>Batch (vanilla) gradient descent sets <code>batch_size=corpus_size</code>.</p>

<p>SGD sets <code>batch_size=1</code>.</p>

<p>And mini-batch gradient descent sets <code>batch_size=k</code>, in which <code>k</code> is usually 32, 64, 128...</p>

<p>How does gensim apply SGD or mini-batch gradient descent? It seems that <code>batch_words</code> is the equivalent of <code>batch_size</code>, but I want to be sure. </p>

<p>Is setting <code>batch_words=1</code> in gensim model equivalent to applying SGD?</p>
",Training and Model Evaluation,gensim word vec apply stochastic gradient descent understanding batch vanilla gradient descent make one parameter update training data stochastic gradient descent sgd allows update parameter training sample helping model converge faster cost high fluctuation function loss batch vanilla gradient descent set sgd set mini batch gradient descent set usually doe gensim apply sgd mini batch gradient descent seems equivalent want sure setting gensim model equivalent applying sgd
"Need to know how to properly regression test a Dialogflow agent - multiple, conflicting options","<p>I've been working with Dialogflow for several months now - really enjoy the tool.  However, it's very important to me to know if my changes (in training) are making intent mapping accuracy better or worse over time.  I've talked to numerous people about the best way to do this using Dialogflow and I've gotten at least 4 different answers.  Can you help me out here?  Here are the 4 answers I've received on how to properly train/test your agent.  Please note that ALL of these answers involve generating a confusion matrix...</p>

<ol>
<li><p>""Traditional"" - randomly split all your input data into 80/20% - use the 80% for training and the 20% for testing.  Every time you train your agent (because you've collected new input data), start the process all over again - meaning randomly split all your input data (old and new) into 80/20%.  In this model, a piece of training data for one agent iteration might be used as a piece of test data on another iteration - and vice-versa.  I've seen variations on this option (KFold and Monte Carlo).</p></li>
<li><p>""Golden Set"" - similar to the above except that the initial 80/20% training and testing sets that you use to create your first agent continue to grow over time as you add more input data.  In this model, once a piece of input data has been tagged as training data, it will NEVER be used as testing data - and once a piece of input data has been tagged as testing data, it will NEVER be used as training data.  The 2 initial sets of training and testing data just continue to grow over time as new inputs are randomly split into the existing sets.</p></li>
<li><p>""All Data is Training Data - Except for Fake Testing Data"" - In this model, we use all the input data as training data.  We then copy a portion of the input data (20%) and ""munge it"" - meaning that we inject characters or words into the 20% portion - and use that as test data.</p></li>
<li><p>""All Data is Training Data - and Some of it is Testing Data Also"" - In this model, we use all the input data as training data.  We then copy a portion of the input data (20%) and use it as testing data.  In other words, we are testing our agent using a portion of our (unmodified) training data.  A variation on this option is to still use all your inputs as training data but sort your inputs by ""popularity/usage"" and take the top 20% for testing data.</p></li>
</ol>

<p>If I were creating a bot from scratch, I'd simply go with option #1 above.  However, I'm using an off-the-shelf product (Dialogflow) and it isn't clear to me that traditional testing is required.  Golden Set seems like it will (mostly) get me to the same place as ""traditional"" so I don't have a big problem with it.  Option #3 seems bad - creating fake testing data sounds problematic on many levels.  And option #4 is using the same data to test as it uses to train - which scares me.</p>

<p>Anyway, would love to hear some thoughts on the above from the experts!</p>
",Training and Model Evaluation,need know properly regression test dialogflow agent multiple conflicting option working dialogflow several month really enjoy tool however important know change training making intent mapping accuracy better worse time talked numerous people best way using dialogflow gotten least different answer help answer received properly train test agent please note answer involve generating confusion matrix traditional randomly split input data use training testing every time train agent collected new input data start process meaning randomly split input data old new model piece training data one agent iteration might used piece test data another iteration vice versa seen variation option kfold monte carlo golden set similar except initial training testing set use create first agent continue grow time add input data model piece input data ha tagged training data never used testing data piece input data ha tagged testing data never used training data initial set training testing data continue grow time new input randomly split existing set data training data except fake testing data model use input data training data copy portion input data munge meaning inject character word portion use test data data training data testing data also model use input data training data copy portion input data use testing data word testing agent using portion unmodified training data variation option still use input training data sort input popularity usage take top testing data creating bot scratch simply go option however using shelf product dialogflow clear traditional testing required golden set seems like mostly get place traditional big problem option seems bad creating fake testing data sound problematic many level option using data test us train scare anyway would love hear thought expert
Can word2vec model be used for words also as training data instead of sentences,"<p>In Word2vec can we use words instead of sentences for model training</p>

<p>Like below code gberg_sents is sentence tokens
model = Word2Vec(sentences=gberg_sents,size=64,sg=1,window=10,min_count=5,seed=42,workers=8)</p>

<p>Like this can we use word tokens also</p>
",Training and Model Evaluation,word vec model used word also training data instead sentence word vec use word instead sentence model training like code gberg sent sentence token model word vec sentence gberg sent size sg window min count seed worker like use word token also
Where to find a pretrained doc2vec model on Wikipedia or large article dataset like Google news?,"<p>Am struggling with training wikipedia dump on doc2vec model, not experienced in setting up a server as a local machine is out of question due to the ram it requires to do the training. I couldnt find a pre trained model except outdated copies for python 2.</p>
",Training and Model Evaluation,find pretrained doc vec model wikipedia large article dataset like google news struggling training wikipedia dump doc vec model experienced setting server local machine question due ram requires training couldnt find pre trained model except outdated copy python
How to train a model that will result in the similarity score between two news titles?,"<p>I am trying to build a Fake news classifier and I am quite new in this field. I have a column ""title_1_en"" which has the title for fake news and another column called ""title_2_en"". There are 3 target labels; ""agreed"", ""disagreed"", and ""unrelated"" if the title of the news in column ""title_2_en"" agrees, disagrees or is unrelated to that in the first column. </p>

<p>I have tried calculating basic cosine similarity between the two titles after converting the words of the sentences into vectors. This has resulted in the the cosine similarity score but this needs a lot of improvement as synonyms and semantic relationship has not been considered at all. </p>

<pre><code>def L2(vector):
    norm_value = np.linalg.norm(vector)
    return norm_value

def Cosine(fr1, fr2):
    cos = np.dot(fr1, fr2)/(L2(fr1)*L2(fr2))
    return cos
</code></pre>
",Training and Model Evaluation,train model result similarity score two news title trying build fake news classifier quite new field column title en ha title fake news another column called title en target label agreed disagreed unrelated title news column title en disagrees unrelated first column tried calculating basic cosine similarity two title converting word sentence vector ha resulted cosine similarity score need lot improvement synonym semantic relationship ha considered
how to text classification using LSTM,"<p>I am doing text classification using LSTM model, I got 98% accuracy in validation data but when I  am submitting It gets 0 scores, please help me how to do, I am a beginner to  NLP.
 I have data like this  </p>

<pre><code>train.head()
    id  category    text
0   959 0   5573 1189 4017 1207 4768 8542 17 1189 5085 5773
1   994 0   6315 7507 6700 4742 1944 2692 3647 4413 6700
2   995 0   5015 8067 5335 1615 7957 5773
3   996 0   2925 7199 1994 4647 7455 5773 4518 2734 2807 8...
4   997 0   7136 1207 6781 237 4971 3669 6193
</code></pre>

<p>I am applying tokenizer here :</p>

<pre><code>from keras.preprocessing.text import Tokenizer
max_features = 1000
tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(list(X_train))
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)
</code></pre>

<p>I am applying  sequence padding here:</p>

<pre><code>from keras.preprocessing import sequence
max_words = 30
X_train = sequence.pad_sequences(X_train, maxlen=max_words)
X_test = sequence.pad_sequences(X_test, maxlen=max_words)
print(X_train.shape,X_test.shape)
</code></pre>

<p>Here my model:</p>

<pre><code>batch_size = 64
epochs = 5

max_features = 1000
embed_dim = 100
num_classes = train['category'].nunique()
  model = Sequential()
    model.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1]))
    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
    model.add(MaxPooling1D(pool_size=2))    
    model.add(LSTM(100, dropout=0.2))
    model.add(Dense(num_classes, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_2 (Embedding)      (None, 30, 100)           100000    
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 30, 32)            9632      
_________________________________________________________________
max_pooling1d_3 (MaxPooling1 (None, 15, 32)            0         
_________________________________________________________________
conv1d_4 (Conv1D)            (None, 15, 32)            3104      
_________________________________________________________________
max_pooling1d_4 (MaxPooling1 (None, 7, 32)             0         
_________________________________________________________________
lstm_2 (LSTM)                (None, 100)               53200     
_________________________________________________________________
dense_2 (Dense)              (None, 2)                 202       
=================================================================
Total params: 166,138
Trainable params: 166,138
Non-trainable params: 0
_________________________________________________________________
None
</code></pre>

<p>Here my epochs:</p>

<pre><code>model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=1)

Train on 2771 samples, validate on 693 samples
Epoch 1/5
2771/2771 [==============================] - 2s 619us/step - loss: 0.2816 - acc: 0.9590 - val_loss: 0.1340 - val_acc: 0.9668
Epoch 2/5
2771/2771 [==============================] - 1s 238us/step - loss: 0.1194 - acc: 0.9664 - val_loss: 0.0809 - val_acc: 0.9668
Epoch 3/5
2771/2771 [==============================] - 1s 244us/step - loss: 0.0434 - acc: 0.9843 - val_loss: 0.0258 - val_acc: 0.9899
Epoch 4/5
2771/2771 [==============================] - 1s 236us/step - loss: 0.0150 - acc: 0.9958 - val_loss: 0.0423 - val_acc: 0.9899
Epoch 5/5
2771/2771 [==============================] - 1s 250us/step - loss: 0.0064 - acc: 0.9984 - val_loss: 0.0532 - val_acc: 0.9899
</code></pre>

<p>after I will applied predict function to test data:
my submission file like this :</p>

<pre><code>   submission.head()





  id    category
0   3729    0.999434
1   3732    0.999128
2   3761    0.999358
3   5       0.996779
4   7       0.998702
</code></pre>

<p>my actual submission file like this :</p>

<pre><code>submission.head()
    id         category
0   3729    1
1   3732    1
2   3761    1
3   5       1
4   7       1
</code></pre>
",Training and Model Evaluation,text classification using lstm text classification using lstm model got accuracy validation data submitting get score please help beginner nlp data like applying tokenizer applying sequence padding model epoch applied predict function test data submission file like actual submission file like
Should i use the same epochs for each batch?,"<p>i need to understand how the epochs/iterations affect the training of a deep learning model.</p>

<p>I am training a NER model with Spacy 2.1.3, my documents are very long so i cannot train more than 200 documents per iteration. So basically i do</p>

<p>from the document 0 to the document 200 -> 20 epochs</p>

<p>from the document 201 to the document 400 -> 20 epochs</p>

<p>and so on.</p>

<p>Maybe, it is a stupid question but, should the epochs of the next batches be the same as the first 0-200? so if i chose 20 epochs i must train the next with 20 epochs too?</p>

<p>Thanks</p>
",Training and Model Evaluation,use epoch batch need understand epoch iteration affect training deep learning model training ner model spacy document long train document per iteration basically document document epoch document document epoch maybe stupid question epoch next batch first chose epoch must train next epoch thanks
What features could help to classify the end of sentence? Sequence classification,"<h1>Problem:</h1>

<p>I have pairs of sentences that lack a period and a capitalized letter in between them. Need to segment them from each other. I'm looking for some help in picking the good features to improve the model.</p>

<h1>Background:</h1>

<p>I'm using <code>pycrfsuite</code> to perform sequence classification and find the end of the first sentence, like so:</p>

<p>From brown corpus, I join every two sentences together and get their pos tags. Then, I label every token in the sentence with <code>'S'</code> if the space follows it and <code>'P'</code> if the period follows it in the sentence. Then I delete a period between the sentences, and lower the following token. I get something like this:</p>

<p>Input:</p>

<pre class=""lang-py prettyprint-override""><code>data = ['I love Harry Potter.', 'It is my favorite book.']
</code></pre>

<p>Output:</p>

<pre class=""lang-py prettyprint-override""><code>sent = [('I', 'PRP'), ('love', 'VBP'), ('Harry', 'NNP'), ('Potter', 'NNP'), ('it', 'PRP'), ('is', 'VBZ'), ('my', 'PRP$'), ('favorite', 'JJ'), ('book', 'NN')]
labels = ['S', 'S', 'S', 'P', 'S', 'S', 'S', 'S', 'S']
</code></pre>

<p>At the moment, I extract these general features:</p>

<pre class=""lang-py prettyprint-override""><code>def word2features2(sent, i):
    word = sent[i][0]
    postag = sent[i][1]

    # Common features for all words
    features = [
        'bias',
        'word.lower=' + word.lower(),
        'word[-3:]=' + word[-3:],
        'word[-2:]=' + word[-2:],
        'word.isupper=%s' % word.isupper(),
        'word.isdigit=%s' % word.isdigit(),
        'postag=' + postag
    ]

    # Features for words that are not
    # at the beginning of a document
    if i &gt; 0:
        word1 = sent[i-1][0]
        postag1 = sent[i-1][1]
        features.extend([
            '-1:word.lower=' + word1.lower(),
            '-1:word.isupper=%s' % word1.isupper(),
            '-1:word.isdigit=%s' % word1.isdigit(),
            '-1:postag=' + postag1
        ])
    else:
        # Indicate that it is the 'beginning of a sentence'
        features.append('BOS')

    # Features for words that are not
    # at the end of a document
    if i &lt; len(sent)-1:
        word1 = sent[i+1][0]
        postag1 = sent[i+1][1]
        features.extend([
            '+1:word.lower=' + word1.lower(),
            '+1:word.isupper=%s' % word1.isupper(),
            '+1:word.isdigit=%s' % word1.isdigit(),
            '+1:postag=' + postag1
        ])
    else:
        # Indicate that it is the 'end of a sentence'
        features.append('EOS')
</code></pre>

<p>And train crf with these parameters:</p>

<pre class=""lang-py prettyprint-override""><code>    trainer = pycrfsuite.Trainer(verbose=True)

    # Submit training data to the trainer
    for xseq, yseq in zip(X_train, y_train):
        trainer.append(xseq, yseq)

    # Set the parameters of the model
    trainer.set_params({
        # coefficient for L1 penalty
        'c1': 0.1,

        # coefficient for L2 penalty
        'c2': 0.01,

        # maximum number of iterations
        'max_iterations': 200,

        # whether to include transitions that
        # are possible, but not observed
        'feature.possible_transitions': True
    })

    trainer.train('crf.model')
</code></pre>

<h1>Results:</h1>

<p>Accuracy report shows:</p>

<pre><code>              precision    recall  f1-score   support

           S       0.99      1.00      0.99    214627
           P       0.81      0.57      0.67      5734

   micro avg       0.99      0.99      0.99    220361
   macro avg       0.90      0.79      0.83    220361
weighted avg       0.98      0.99      0.98    220361
</code></pre>

<p><strong>What are some ways I could edit <code>word2features2()</code> in order to improve the model?</strong> (or any other part)</p>

<p>Here is the <a href=""https://github.com/arlaptiev/sentence-segment/blob/master/split.py"" rel=""nofollow noreferrer"">link</a> to the full code as it is today.</p>

<p>Also, I am just a beginner in nlp so I would <strong>greatly</strong> appreciate any feedback overall, links to relevant or helpful sources, and rather simple explanations. Thank you very-very much!</p>
",Training and Model Evaluation,feature could help classify end sentence sequence classification problem pair sentence lack period capitalized letter need segment looking help picking good feature improve model background using perform sequence classification find end first sentence like brown corpus join every two sentence together get po tag label every token sentence space follows period follows sentence delete period sentence lower following token get something like input output moment extract general feature train crf parameter result accuracy report show way could edit order improve model part link full code today also beginner nlp would greatly appreciate feedback overall link relevant helpful source rather simple explanation thank much
CoreNLP: Can it tell whether a noun refers to a person?,"<p>Can CoreNLP determine whether a common noun (as opposed to a proper noun or proper name) refers to a person out-of-the-box? Or if I need to train a model for this task, how do I go about that? </p>

<p>First, I am <em>not</em> looking for coreference resolution, but rather a building block for it. Coreference by definition depends on the context, whereas I am trying to evaluate whether a word <em>in isolation</em> is a subset of ""person"" or ""human"". For example:</p>

<pre><code>is_human('effort') # False
is_human('dog') # False
is_human('engineer') # True
</code></pre>

<p>My naive attempt to use Gensim's and spaCy's pre-trained word vectors failed to rank ""engineer"" above the other two words.</p>

<pre><code>import gensim.downloader as api
word_vectors = api.load(""glove-wiki-gigaword-100"") 
for word in ('effort', 'dog', 'engineer'):
    print(word, word_vectors.similarity(word, 'person'))

# effort 0.42303842
# dog 0.46886832
# engineer 0.32456854
</code></pre>

<p>I found the following lists from <a href=""https://nlp.stanford.edu/software/dcoref.html"" rel=""nofollow noreferrer"">CoreNLP</a> promising. </p>

<pre><code>dcoref.demonym                   // The path for a file that includes a list of demonyms 
dcoref.animate                   // The list of animate/inanimate mentions (Ji and Lin, 2009)
dcoref.inanimate 
dcoref.male                      // The list of male/neutral/female mentions (Bergsma and Lin, 2006) 
dcoref.neutral                   // Neutral means a mention that is usually referred by 'it'
dcoref.female 
dcoref.plural                    // The list of plural/singular mentions (Bergsma and Lin, 2006)
dcoref.singular
</code></pre>

<p>Would these work for my task? And if so, how would I access them from the <a href=""https://pypi.org/project/corenlp-python/"" rel=""nofollow noreferrer"">Python wrapper</a>? Thank you.</p>
",Training and Model Evaluation,corenlp tell whether noun refers person corenlp determine whether common noun opposed proper noun proper name refers person box need train model task go first looking coreference resolution rather building block coreference definition depends context whereas trying evaluate whether word isolation subset person human example naive attempt use gensim spacy pre trained word vector failed rank engineer two word found following list corenlp promising would work task would access python wrapper thank
How to perform class balancing on an imbalanced data set,"<p>I have an imbalanced data set with more positive reviews(90%) than negative(10%), which options below should I follow.</p>

<ol>
<li>Use <code>class_weight='balanced'</code> in GridsearchCV(Training &amp; CV) and Logistic algo(Train &amp; Test)</li>
<li>Split data into train,CV &amp; test sets and then do oversampling on minority class by taking the majority class(my case its +ve class). Thereby both classes balanced</li>
<li>Undersampling- not considered due to information loss.</li>
</ol>
",Training and Model Evaluation,perform class balancing imbalanced data set imbalanced data set positive review negative option follow use gridsearchcv training cv logistic algo train test split data train cv test set oversampling minority class taking majority class case class thereby class balanced undersampling considered due information loss
Classifying Documents into Categories,"<p>I've got about 300k documents stored in a Postgres database that are tagged with topic categories (there are about 150 categories in total).  I have another 150k documents that don't yet have categories.  I'm trying to find the best way to programmaticly categorize them.</p>

<p>I've been exploring <a href=""http://www.nltk.org/"" rel=""noreferrer"">NLTK</a> and its Naive Bayes Classifier.  Seems like a good starting point (if you can suggest a better classification algorithm for this task, I'm all ears).</p>

<p>My problem is that I don't have enough RAM to train the NaiveBayesClassifier on all 150 categoies/300k documents at once (training on 5 categories used 8GB).  Furthermore, accuracy of the classifier seems to drop as I train on more categories (90% accuracy with 2 categories, 81% with 5, 61% with 10).</p>

<p>Should I just train a classifier on 5 categories at a time, and run all 150k documents through the classifier to see if there are matches?  It seems like this would work, except that there would be a lot of false positives where documents that don't really match any of the categories get shoe-horned into on by the classifier just because it's the best match available...  Is there a way to have a ""none of the above"" option for the classifier just in case the document doesn't fit into any of the categories?</p>

<p>Here is my test class <a href=""http://gist.github.com/451880"" rel=""noreferrer"">http://gist.github.com/451880</a></p>
",Training and Model Evaluation,classifying document category got k document stored postgres database tagged topic category category total another k document yet category trying find best way programmaticly categorize exploring nltk naive bayes classifier seems like good starting point suggest better classification algorithm task ear problem enough ram train naivebayesclassifier categoies k document training category used gb furthermore accuracy classifier seems drop train category accuracy category train classifier category time run k document classifier see match seems like would work except would lot false positive document really match category get shoe horned classifier best match available way none option classifier case document fit category test class
Naive Bayes in Python,"<p>I'm trying to do Laplace smoothing on my Naive Bayes code. It gives me 72.5% accuracy on 70% train 30% test set, which is kinda low. Does anyone see anything wrong?</p>

<pre><code>posTotal=len(pos)
negTotal=len(neg)

for w in larr:
  if (w not in pos) or (w not in neg):
    unk[w]+=1
    unkTotal=len(unk)
  else:
    if (w in pos):
      posP+=(math.log10(pos[w])-math.log10(posTotal))
    if (w in neg):
      negP+=(math.log10(neg[w])-math.log10(negTotal))
</code></pre>

<p><code>pos</code> and <code>neg</code> are a defaultdic.</p>
",Training and Model Evaluation,naive bayes python trying laplace smoothing naive bayes code give accuracy train test set kinda low doe anyone see anything wrong defaultdic
How can I train the semantic role labeling model in AllenNLP?,"<p>How can I train the <a href=""https://demo.allennlp.org/semantic-role-labeling/NjU2MjA3"" rel=""nofollow noreferrer"">semantic role labeling model in AllenNLP</a>?</p>

<p>I am aware of the <a href=""https://allenai.github.io/allennlp-docs/api/allennlp.training.trainer.html"" rel=""nofollow noreferrer""><code>allennlp.training.trainer</code></a> function but I don't know how to use it to train the semantic role labeling model.</p>

<p>Let's assume that the training samples are BIO tagged, e.g.:</p>

<pre><code>Remove B_O
the B_ARG1
fish I_ARG1
in B_LOC
the I_LOC 
background I_LOC 
</code></pre>
",Training and Model Evaluation,train semantic role labeling model allennlp train semantic role labeling model allennlp aware function know use train semantic role labeling model let assume training sample bio tagged e g
Problems with Naive Bayes implemented on Amazon fine food reviews dataset,"<p><a href=""https://i.sstatic.net/BizjX.png"" rel=""nofollow noreferrer"">cv accuracy</a> <a href=""https://i.sstatic.net/cUso0.png"" rel=""nofollow noreferrer"">cv accuracy graph</a> <a href=""https://i.sstatic.net/2HYAV.png"" rel=""nofollow noreferrer"">test accuracy</a></p>

<p>I am trying to implement Naive bayes on fine food reviews dataset of amazon. Can you review the code and tell why there is such a big difference between cross validation accuracy and test accuracy?</p>

<p>Conceptually is there anything wrong with the below code?</p>

<pre><code>#BOW()

from sklearn.feature_extraction.text import CountVectorizer
bow = CountVectorizer(ngram_range = (2,3))
bow_vect = bow.fit(X_train[""F_review""].values)
bow_sparse = bow_vect.transform(X_train[""F_review""].values)
X_bow = bow_sparse
y_bow = y_train



roc = []
accuracy = []
f1 = []
k_value = []
for i in range(1,50,2):
  BNB =BernoulliNB(alpha =i)

  print(""************* for alpha = "",i,""*************"")
  x = (cross_validate(BNB, X_bow,y_bow, scoring = ['accuracy','f1','roc_auc'], return_train_score = False, cv = 10))
  print(x[""test_roc_auc""].mean())
  print(""-----c------break------c-------break-------c-----------"")
  roc.append(x['test_roc_auc'].mean())#This is the ROC metric
  accuracy.append(x['test_accuracy'].mean())#This is the accuracy metric
  f1.append(x['test_f1'].mean())#This is the F1 score

  k_value.append(i)


#BOW Test prediction
BNB =BernoulliNB(alpha= 1)
BNB.fit(X_bow, y_bow)
y_pred = BNB.predict(bow_vect.transform(X_test[""F_review""]))
print(""Accuracy Score: "",accuracy_score(y_test,y_pred))
print(""ROC: "", roc_auc_score(y_test,y_pred))
print(""Confusion Matrix: "", confusion_matrix(y_test,y_pred))
</code></pre>
",Training and Model Evaluation,problem naive bayes implemented amazon fine food review dataset cv accuracy cv accuracy graph test accuracy trying implement naive bayes fine food review dataset amazon review code tell big difference cross validation accuracy test accuracy conceptually anything wrong code
Is there a way to increase dimensionality of pre-trained Word Embeddings?,"<p>I am almost newly exposed to NLP research, struggling with NLP and Machine Learning techniques that are used in NLP.</p>

<p>The question that I'm dealing with now is if there is some method to increase the dimensionality of pre-trained word embeddings (like GloVe embeddings) from a fixed size 100 to let's say 512? </p>

<p>The reason I'm asking such question is that I used these embeddings to train RNN network with a pre-defined dimension of 100. Now, I have switched to self-attention mechanism (Transformers) where the model is highly sensitive to training parameters. So, I was wondering if I could somehow (for example, using perceptron, or maybe MLP) to transform 100d embeddings to a new space with 512d. </p>

<p>I googled this before asking here, but ended up with finding no reliable source in the end.</p>
",Training and Model Evaluation,way increase dimensionality pre trained word embeddings almost newly exposed nlp research struggling nlp machine learning technique used nlp question dealing method increase dimensionality pre trained word embeddings like glove embeddings fixed size let say reason asking question used embeddings train rnn network pre defined dimension switched self attention mechanism transformer model highly sensitive training parameter wa wondering could somehow example using perceptron maybe mlp transform embeddings new space googled asking ended finding reliable source end
How to combine different sets of word embeddings?,"<p>I am building a semantic relations classifier.</p>

<p>Example of ""cause-effect"" relation: ""<strong>Zinc</strong> is essential for <strong>growth</strong> and cell division.""</p>

<p>For this I am using pretrained word embeddings from Spacy as I don´t have anough data to train my own. I took two approaches:</p>

<ol>
<li>took the two words that are related (or not) and calculated their cosine similarity using their word embeddings and used that as feature vector for a MLP classifier.</li>
<li>took the mean vector of the entire sentence and used it as feature vector for the MLP classifier.
But the results on test data are not very good (aprox 50% precision and recall). I think the results could improve by combinig both approaches, but I don´t really know how to do that.</li>
</ol>

<p>I am using a multilayer perceptron with one hidden layer of 150 nodes.</p>

<p>Here is the code of the first approach:</p>

<pre><code>nlp=spacy.load('en_core_web_md') #these are the embeddings

entities_vec=np.zeros((len(entity1), 300))
for i in range(len(entity1)):
    doc1=nlp(entity1[i])
    doc2=nlp(entity2[i])
    entities_vec[i,:]=(sum(doc1.vector, doc2.vector))*0.5
</code></pre>

<p>Here´s the code of the second approach:</p>

<pre><code>sentences_vec=np.zeros((len(sentences), 300))
for i in range(len(sentences)):
    doc=nlp(sentences[i])
    sentences_vec[i,:]=doc.vector

</code></pre>

<p>I tried adding sentences_vec and entities_vec but that threw even worse results on test data. What would be a way to improve the model?</p>
",Training and Model Evaluation,combine different set word embeddings building semantic relation classifier example cause effect relation zinc essential growth cell division using pretrained word embeddings spacy anough data train took two approach took two word related calculated cosine similarity using word embeddings used feature vector mlp classifier took mean vector entire sentence used feature vector mlp classifier result test data good aprox precision recall think result could improve combinig approach really know using multilayer perceptron one hidden layer node code first approach code second approach tried adding sentence vec entity vec threw even worse result test data would way improve model
How to implement a multi-label text classifier in Keras?,"<p>I've been trying to create a multi-label text classifier that uses GloVe embeddings using Keras. I'm currently experimenting with the TREC-6 data set available at <a href=""http://cogcomp.org/Data/QA/QC/"" rel=""nofollow noreferrer"">http://cogcomp.org/Data/QA/QC/</a>. I'm only considering the 5 broad labels for my classification problem and am ignoring the sub-labels. </p>

<p>Since it's a multi-label classification problem, given a sentence, my neural network should output all labels with a probability greater than 0.1. Issue is that the network almost always classifies a question with only 1 label, which is fine when I'm asking a question that belongs to only one category. When I combine questions from different categories however, it still gives only one label with a high confidence most of the time, although I want all relevant labels to be identified.
I'm absolutely sure the pre-processing steps are correct. I get the feeling that there's some issue in my model.</p>

<p>I started experimenting with only CNNs in the beginning by referring to the paper ""Convolutional Neural Networks for Sentence Classification"" at <a href=""https://www.aclweb.org/anthology/D14-1181.pdf"" rel=""nofollow noreferrer"">https://www.aclweb.org/anthology/D14-1181.pdf</a>, but on a teacher's advice and after seeing that they fail for long questions with different relevant topics, I tried experimenting with LSTMs and BiLSTMS. I started with this approach <a href=""https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80568"" rel=""nofollow noreferrer"">https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80568</a> and kept modifying some parameters / adding and removing layers hoping to get a good result but I've failed so far.
I tried copy pasting some code for Attention mechanism and adding that after my LSTM layers as well but it doesn't help.</p>

<p>My current model looks somewhat like this. I'll paste most of the rest of my code for clarity. The model and training code is present in the sentence_classifier() function.</p>

<pre><code>class SentenceClassifier:
    def __init__(self):
        self.MAX_SEQUENCE_LENGTH = 200
        self.EMBEDDING_DIM = 100
        self.LABEL_COUNT = 0
        self.WORD_INDEX = dict()
        self.LABEL_ENCODER = None

    def clean_str(self, string):
        """"""
        Cleans each string and convert to lower case.
        """"""

        string = re.sub(r""\'s"", """", string)
        string = re.sub(r""\'ve"", """", string)
        string = re.sub(r""n\'t"", "" not"", string)
        string = re.sub(r""\'re"", """", string)
        string = re.sub(r""\'d"", """", string)
        string = re.sub(r""\'ll"", """", string)
        string = re.sub(r""[^A-Za-z0-9]"", "" "", string)
        string = re.sub(r""\s{2,}"", "" "", string)

        return string.strip().lower()

    def loader_encoder(self, table, type=""json""):
        """"""
        Load and encode data from dataset.

        type = ""sql"" means get data from MySQL database.
        type = ""json"" means get data from .json file.        """"""

        if type == ""json"":
            with open('data/' + table + '.json', 'r', encoding='utf8') as f:
                datastore = json.load(f)
                questions = []
                tags = []
                for row in datastore:
                    questions.append(row['question'])
                    tags.append(row['tags'].split(','))

        tokenizer = Tokenizer(lower=True, char_level=False)
        tokenizer.fit_on_texts(questions)
        self.WORD_INDEX = tokenizer.word_index

        questions_encoded = tokenizer.texts_to_sequences(questions)
        questions_encoded_padded = pad_sequences(questions_encoded, maxlen=self.MAX_SEQUENCE_LENGTH, padding='post')


        for i, ele in enumerate(tags):
            for j, tag in enumerate(ele):
                if len(tag) == 0 or tag == ',':
                    del tags[i][j]


        encoder = MultiLabelBinarizer()
        encoder.fit(tags)
        self.LABEL_ENCODER = encoder
        tags_encoded = encoder.fit_transform(tags)
        self.LABEL_COUNT = len(tags_encoded[0]) #No. of labels
        print(""\tUnique Tokens in Training Data: "", len(self.WORD_INDEX))


        return questions_encoded_padded, tags_encoded

    def load_embeddings(self, EMBED_PATH='./embeddings/glove.6B.100d.txt'):
        """"""
        Load pre-trained embeddings into memory.
        """"""
        embeddings_index = {}
        try:
            f = open(EMBED_PATH, encoding='utf-8')
        except FileNotFoundError:
            print(""Embeddings missing."")
            sys.exit()
        for line in f:
            values = line.rstrip().rsplit(' ')
            word = values[0]
            vec = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = vec
        f.close()
        print(""\tNumber of tokens in embeddings file: "", len(embeddings_index))
        return embeddings_index

    def create_embedding_matrix(self, embeddings_index):
        """"""
        Creates an embedding matrix for all the words(vocab) in the training data with shape (vocab, EMBEDDING_DIM).
        Out-of-vocab words will be randomly initialized to values between +0.25 and -0.25.
        """"""
        words_not_found = []
        vocab = len(self.WORD_INDEX) + 1
        embedding_matrix = np.random.uniform(-0.25, 0.25, size=(vocab, self.EMBEDDING_DIM))
        for word, i in self.WORD_INDEX.items():
            if i &gt;= vocab:
                continue
            embedding_vector = embeddings_index.get(word)
            if (embedding_vector is not None) and len(embedding_vector) &gt; 0:
                embedding_matrix[i] = embedding_vector
            else:
                words_not_found.append(word)
        # print('Number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))
        print(""\tShape of embedding matrix: "", str(embedding_matrix.shape))
        print(""\tNo. of words not found in pre-trained embeddings: "", len(words_not_found))
        return embedding_matrix

    def sentence_classifier_cnn(self, embedding_matrix, x, y, table, load_saved=0):
        """"""
        A static CNN model.
        Makes uses of Keras functional API for constructing the model.

        If load_saved=1, THEN load old model, ELSE train new model


        model_name = table + "".model.h5""
        if load_saved == 1 and os.path.exists('./saved/' + model_name):
            print(""\nLoading saved model..."")
            model = load_model('./saved/' + model_name)
            print(""Model Summary"")
            print(model.summary())
        """"""


        print(""\nTraining model..."")
        inputs = Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32')
        embedding = Embedding(input_dim=(len(self.WORD_INDEX) + 1), output_dim=self.EMBEDDING_DIM,
                                weights=[embedding_matrix],
                                input_length=self.MAX_SEQUENCE_LENGTH)(inputs)

        X = keras.layers.SpatialDropout1D(0.3)(embedding)

        X = keras.layers.Bidirectional(keras.layers.CuDNNLSTM(64, return_sequences = True))(X)  

        #X2 = keras.layers.Bidirectional(keras.layers.CuDNNGRU(128, return_sequences = False))(X)  

        X = keras.layers.Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='normal')(X)           
        X = keras.layers.GlobalMaxPooling1D()(X)       
        #X = Attention(self.MAX_SEQUENCE_LENGTH)(X)

        X = Dropout(0.5)(X)
        X = keras.layers.Dense(16, activation=""relu"")(X)
        X = Dropout(0.5)(X)
        X = keras.layers.BatchNormalization()(X)

        output = Dense(units=self.LABEL_COUNT, activation='sigmoid')(X)

        model = Model(inputs=inputs, outputs=output, name='intent_classifier')
        print(""Model Summary"")
        print(model.summary())

        cbk = OutputObserver(model, classifier)
        model.compile(loss='binary_crossentropy',
                          optimizer='adam',
                          metrics=['accuracy'])
        model.fit(x, y,
                      batch_size=30,
                      epochs=23,
                      verbose=2,
                      callbacks = [cbk])


        #keras.utils.vis_utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

        return model

    def tag_question(self, model, question):

        question = self.clean_str(question)
        question_encoded = [[self.WORD_INDEX[w] for w in question.split(' ') if w in self.WORD_INDEX]]
        question_encoded_padded = pad_sequences(question_encoded, maxlen=self.MAX_SEQUENCE_LENGTH, padding='post')
        predictions = model.predict(question_encoded_padded)

        possible_tags = []
        for i, probability in enumerate(predictions[0]):
            if probability &gt;= 0.01:
                possible_tags.append([self.LABEL_ENCODER.classes_[i], probability])

        possible_tags.sort(reverse=True, key=lambda x:x[1]) #sort in place on the basis of the probability in each sub-list in descending order
        print(possible_tags)


    def setup_classifier(self, table):
        '''

        '''
        print(""Loading Data Set..."")
        x, y = self.loader_encoder(table)

        embeddings_index = self.load_embeddings()

        print(""\nGenerating embedding matrix..."")
        embedding_matrix = self.create_embedding_matrix(embeddings_index)

        #Loading / Training model
        model = self.sentence_classifier_cnn(embedding_matrix, x, y, table, load_saved=1)

        return model, embeddings_index

    def connect_to_db(self):
        mydb = mysql.connector.connect(host=""localhost"", user=""root"", passwd=""root"", database=""questiondb"")
        cursor = mydb.cursor()
        return mydb, cursor
</code></pre>

<p>As you can see, I've used a callback to print my predictions after each step.
I've tried to predict labels for all kinds of questions but I get good results only for questions that fall under one category. </p>

<p>For instance,</p>

<pre><code>classifier.tag_question(model, ""how many days before new year?"")
</code></pre>

<p>gives</p>

<pre><code>[['numeric', 0.99226487]]
</code></pre>

<p>as the output. But a more complex question like</p>

<pre><code>classifier.tag_question(model, ""who is the prophet of the muslim people and  where is india located and how much do fruits costs there?"")
</code></pre>

<p>gives something like </p>

<pre><code>[['human', 0.9990531]]
</code></pre>

<p>as the output although labels like 'location' and 'numeric' are also relevant.
I used a callback to predict the prediction for that question after every epoch and I see something like this.</p>

<pre><code>Epoch 1/23
 - 19s - loss: 0.6581 - acc: 0.6365
[['human', 0.69752634], ['location', 0.40014982], ['entity', 0.32047516], ['abbreviation', 0.23877779], ['numeric', 0.23324837], ['description', 0.15995058]]
Epoch 2/23
 - 12s - loss: 0.4525 - acc: 0.8264
[['human', 0.7437608], ['location', 0.18141672], ['entity', 0.14474556], ['numeric', 0.09171515], ['description', 0.053900182], ['abbreviation', 0.05283475]]
Epoch 3/23
 - 12s - loss: 0.3854 - acc: 0.8478
[['human', 0.86335427], ['location', 0.12673976], ['entity', 0.09847507], ['numeric', 0.064431995], ['description', 0.035599917], ['abbreviation', 0.02441895]]
Epoch 4/23
 - 12s - loss: 0.3634 - acc: 0.8509
[['human', 0.90795004], ['location', 0.10085008], ['entity', 0.09804481], ['numeric', 0.050411616], ['description', 0.032810867], ['abbreviation', 0.014970899]]
Epoch 5/23
 - 13s - loss: 0.3356 - acc: 0.8582
[['human', 0.8365586], ['entity', 0.1130701], ['location', 0.10253032], ['numeric', 0.039931685], ['description', 0.02874279]]
Epoch 6/23
 - 13s - loss: 0.3142 - acc: 0.8657
[['human', 0.95577633], ['entity', 0.088555306], ['location', 0.055004593], ['numeric', 0.015950901], ['description', 0.01428318]]
Epoch 7/23
 - 13s - loss: 0.2942 - acc: 0.8750
[['human', 0.89538944], ['entity', 0.130977], ['location', 0.06350105], ['description', 0.023014158], ['numeric', 0.019377537]]
Epoch 8/23
 - 13s - loss: 0.2739 - acc: 0.8802
[['human', 0.9725125], ['entity', 0.061141968], ['location', 0.026945814], ['description', 0.010931551]]
Epoch 9/23
 - 13s - loss: 0.2579 - acc: 0.8914
[['human', 0.9797143], ['entity', 0.042518377], ['location', 0.027904237]]
Epoch 10/23
 - 13s - loss: 0.2380 - acc: 0.9020
[['human', 0.7897601], ['entity', 0.14315197], ['location', 0.07439863], ['description', 0.019453615], ['numeric', 0.010681627]]
Epoch 11/23
 - 13s - loss: 0.2250 - acc: 0.9104
[['human', 0.9886158], ['entity', 0.024878502], ['location', 0.015951043]]
Epoch 12/23
 - 13s - loss: 0.2131 - acc: 0.9178
[['human', 0.9677731], ['entity', 0.03698206], ['location', 0.026153017]]
Epoch 13/23
 - 13s - loss: 0.2029 - acc: 0.9204
[['human', 0.9514474], ['entity', 0.053581357], ['location', 0.029657435]]
Epoch 14/23
 - 13s - loss: 0.1915 - acc: 0.9285
[['human', 0.9706739], ['entity', 0.0328649], ['location', 0.013876333]]
Epoch 15/23
 - 13s - loss: 0.1856 - acc: 0.9300
[['human', 0.9328136], ['location', 0.05573874], ['entity', 0.025918543]]
Epoch 16/23
 - 13s - loss: 0.1802 - acc: 0.9318
[['human', 0.9895527], ['entity', 0.014941782], ['location', 0.011972391]]
Epoch 17/23
 - 13s - loss: 0.1717 - acc: 0.9373
[['human', 0.9426272], ['entity', 0.03754583], ['location', 0.023379702]]
Epoch 18/23
 - 13s - loss: 0.1614 - acc: 0.9406
[['human', 0.99186605]]
Epoch 19/23
 - 13s - loss: 0.1573 - acc: 0.9432
[['human', 0.9926062]]
Epoch 20/23
 - 13s - loss: 0.1511 - acc: 0.9448
[['human', 0.9993554]]
Epoch 21/23
 - 13s - loss: 0.1591 - acc: 0.9426
[['human', 0.9964465]]
Epoch 22/23
 - 13s - loss: 0.1507 - acc: 0.9451
[['human', 0.999688]]
Epoch 23/23
 - 13s - loss: 0.1524 - acc: 0.9436
[['human', 0.9990531]]
</code></pre>

<p>I've tried varying my parameters hundreds of times, especially my network size, batch size and epochs to try and avoid over-fitting.</p>

<p>I know my question is ridiculously long but I'm running out of patience and any help would be appreciated.</p>

<p>Here's the link to my colab notebook - <a href=""https://colab.research.google.com/drive/1EOklUw7efOv69HvWKpuKVy1LSzcvTTCk"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1EOklUw7efOv69HvWKpuKVy1LSzcvTTCk</a>.</p>
",Training and Model Evaluation,implement multi label text classifier kera trying create multi label text classifier us glove embeddings using kera currently experimenting trec data set available considering broad label classification problem ignoring sub label since multi label classification problem given sentence neural network output label probability greater issue network almost always classifies question label fine asking question belongs one category combine question different category however still give one label high confidence time although want relevant label identified absolutely sure pre processing step correct get feeling issue model started experimenting cnns beginning referring paper convolutional neural network sentence classification teacher advice seeing fail long question different relevant topic tried experimenting lstms bilstms started approach kept modifying parameter adding removing layer hoping get good result failed far tried copy pasting code attention mechanism adding lstm layer well help current model look somewhat like paste rest code clarity model training code present sentence classifier function see used callback print prediction step tried predict label kind question get good result question fall one category instance give output complex question like give something like output although label like location numeric also relevant used callback predict prediction question every epoch see something like tried varying parameter hundred time especially network size batch size epoch try avoid fitting know question ridiculously long running patience help would appreciated link colab notebook
TextBlob and NLTK POS tagging accuracy,"<p>So far, I have this code below</p>

<pre class=""lang-py prettyprint-override""><code>from textblob import TextBlob
class BrinBot:

    def __init__(self, message): #Accepts the message from the user as the argument
        parse(message)

class parse:
    def __init__(self, message):
        self.message = message
        blob = TextBlob(self.message)
        print(blob.tags)

BrinBot(""Handsome Bob's dog is a beautiful Chihuahua"")
</code></pre>

<p>This is the output:</p>

<pre class=""lang-py prettyprint-override""><code>[('Handsome', 'NNP'), ('Bob', 'NNP'), (""'s"", 'POS'), ('dog', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('beautiful', 'JJ'), ('Chihuahua', 'NNP')]
</code></pre>

<p>My question is that apparently TextBlob thinks ""Handsome"" is a singular proper noun, which is not correct as ""Handsome"" is supposed to be an adjective. Is there a way to fix that, I tried this on NLTK also but got the same results.</p>
",Training and Model Evaluation,textblob nltk po tagging accuracy far code output question apparently textblob think handsome singular proper noun correct handsome supposed adjective way fix tried nltk also got result
Pytorch RNN HTML Generation,"<p>I’ m stuck for a couple of days trying to make and <strong>RNN network</strong> to <strong>learn</strong> a <strong>basic HTML template</strong>.
I tried different approaches and I even <strong>overfit</strong> on the following data:</p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Page Title&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;

&lt;h1&gt;This is a Heading&lt;/h1&gt;
&lt;p&gt;This is a paragraph.&lt;/p&gt;

&lt;/body&gt;
&lt;/html&gt; 
</code></pre>

<p>Obtaining <strong>100% accuracy on training and validation</strong> using <strong>Adam</strong> Optimizer and <strong>CrossEntropyLoss</strong>.</p>

<p>The problem is that when I try to sample from the network, the <strong>results</strong> are completely <strong>random</strong> and I don’t know whats the <strong>problem</strong>:</p>

<pre><code>..&lt;a&lt;a&lt;a&lt;a&lt;aa&lt;ttp11111b11111b11111111b11b1bbbb&lt;btttn111
</code></pre>

<p>My <strong>sampling function</strong> is the following:</p>

<pre><code>def sample_sentence():
    words = list()
    count = 0
    modelOne.eval()
    with torch.no_grad():
        # Setup initial input state, and input word (we use ""the"").
        previousWord = torch.LongTensor(1, 1).fill_(trainData.vocabulary['letter2id']['[START]'])
        hidden =  Variable(torch.zeros(6, 1, 100).to(device))


        while True:
            # Predict the next word based on the previous hidden state and previous word.
            inputWord = torch.autograd.Variable(previousWord.to(device))


            predictions, newHidden = modelOne(inputWord, hidden)

            hidden = newHidden


            pred = torch.nn.functional.softmax(predictions.squeeze()).data.cpu().numpy().astype('float64')

            pred = pred/np.sum(pred)


            nextWordId = np.random.multinomial(1, pred, 1).argmax()


            if nextWordId == 0:
                continue

            words.append(trainData.vocabulary['id2letter'][nextWordId])
            # Setup the inputs for the next round.
            previousWord.fill_(nextWordId)


            # Keep adding words until the [END] token is generated.
            if nextWordId == trainData.vocabulary['letter2id']['[END]']:
                break

            if count&gt;20000:
                break
            count += 1
        words.insert(0, '[START]')


        return words
</code></pre>

<p>And my network architecture is here:</p>

<pre><code>class ModelOne(Model) :
    def __init__(self,
                vocabulary_size,
                hidden_size,
                num_layers,
                rnn_dropout,
                embedding_size,
                dropout,
                num_directions):
        super(Model, self).__init__()

        self.vocabulary_size = vocabulary_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn_dropout = rnn_dropout
        self.dropout = dropout
        self.num_directions = num_directions
        self.embedding_size = embedding_size


        self.embeddings = nn.Embedding(self.vocabulary_size, self.embedding_size)
        self.rnn = nn.GRU(self.embedding_size,
                          self.hidden_size,
                          num_layers=self.num_layers,
                          bidirectional=True if self.num_directions==2 else False,
                          dropout=self.rnn_dropout,
                          batch_first=True)
        self.linear = nn.Linear(self.hidden_size*self.num_directions, self.vocabulary_size)





    def forward(self, paddedSeqs, hidden):

        batchSequenceLength = paddedSeqs.size(1)

        batchSize = paddedSeqs.size(0)

        lengths = paddedSeqs.ne(0).sum(dim=1)

        embeddingVectors = self.embeddings(paddedSeqs)

        x = torch.nn.utils.rnn.pack_padded_sequence(embeddingVectors, lengths, batch_first=True)

        self.rnn.flatten_parameters()

        x,hid = self.rnn(x, hidden)

        output, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0, total_length=batchSequenceLength)

        predictions = self.linear(output)


        return predictions.view(batchSize, self.vocabulary_size, batchSequenceLength), hid

    def init_hidden(self, paddedSeqs):
        hidden = Variable(torch.zeros(self.num_layers*self.num_directions,
                                    1,
                                    self.hidden_size).to(device))
        return hidden



modelOne =ModelOne(vocabulary_size=vocabularySize,
                 hidden_size=100,
                 embedding_size=50,
                 num_layers=3,
                 rnn_dropout=0.0,
                 dropout=0,
                 num_directions=2).to(device)
</code></pre>

<p>If you have any idea of what needs to be changed, please let me know.
I added all the code to github repository here: <a href=""https://github.com/OverclockRo/HTMLGeneration/blob/SamplingTestTemplate/Untitled.ipynb"" rel=""nofollow noreferrer"">https://github.com/OverclockRo/HTMLGeneration/blob/SamplingTestTemplate/Untitled.ipynb</a></p>
",Training and Model Evaluation,pytorch rnn html generation stuck couple day trying make rnn network learn basic html template tried different approach even overfit following data obtaining accuracy training validation using adam optimizer crossentropyloss problem try sample network result completely random know whats problem sampling function following network architecture idea need changed please let know added code github repository
Using SentencePiece as a command,"<p>I need to use Google's SentencePiece from </p>

<p><a href=""https://github.com/google/sentencepiece"" rel=""nofollow noreferrer"">SentencePiece Github</a></p>

<p>I have installed it via pip and I would like to run the example command to train a model like </p>

<pre><code>spm_train --input=&lt;input&gt; --model_prefix=&lt;model_name&gt; --vocab_size=8000 --character_coverage=1.0 --model_type=&lt;type&gt;
</code></pre>

<p>However I get <code>spm_train: command not found</code> does that mean I need to install C++ version from the github ?</p>

<p>Also, how can I know when a pip install will allow me to use the terminal command described in README ? </p>

<p>Because when I install a very similar program <a href=""https://github.com/rsennrich/subword-nmt"" rel=""nofollow noreferrer"">Sennrich BPE</a> I just need to install via pip and then run the command in my terminal</p>
",Training and Model Evaluation,using sentencepiece command need use google sentencepiece sentencepiece github installed via pip would like run example command train model like however get doe mean need install c version github also know pip install allow use terminal command described readme install similar program sennrich bpe need install via pip run command terminal
Javascript word tokenizer library with support for multiple languages (as many as possible),"<p>I am looking for a word tokenizer library for node.js, that supports as many languages as possible. I'd like to pass in a string like: <code>tokenize('Hello, world!', 'en')</code> and have it return <code>['Hello', 'world']</code>. The number of supported languages is more important than precision.</p>
",Training and Model Evaluation,javascript word tokenizer library support multiple language many possible looking word tokenizer library node j support many language possible like pas string like return number supported language important precision
What Java libraries can I use to pull text and font styles?,"<p>I am working on a machine learning project (NLP), where I would like to use the text content and font styles to train the model to identify parts of a document. I found apache tika &amp; POI, but I am struggling to figure out how to parse more than text.</p>
",Training and Model Evaluation,java library use pull text font style working machine learning project nlp would like use text content font style train model identify part document found apache tika poi struggling figure parse text
GRU Language Model not Training Properly,"<p>I’ve tried reimplementing a simple GRU language model using just a GRU and a linear layer (the full code is also at <a href=""https://www.kaggle.com/alvations/gru-language-model-not-training-properly"" rel=""nofollow noreferrer"">https://www.kaggle.com/alvations/gru-language-model-not-training-properly</a>):</p>

<pre><code>class Generator(nn.Module):
    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers):
        super(Generator, self).__init__()

        # Initialize the embedding layer with the 
        # - size of input (i.e. no. of words in input vocab)
        # - no. of hidden nodes in the embedding layer
        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)

        # Initialize the GRU with the 
        # - size of the input (i.e. embedding layer)
        # - size of the hidden layer 
        self.gru = nn.GRU(embedding_size, hidden_size, num_layers)

        # Initialize the ""classifier"" layer to map the RNN outputs
        # to the vocabulary. Remember we need to -1 because the 
        # vectorized sentence we left out one token for both x and y:
        # - size of hidden_size of the GRU output.
        # - size of vocabulary
        self.classifier = nn.Linear(hidden_size, vocab_size)

    def forward(self, inputs, use_softmax=False, hidden=None):
        # Look up for the embeddings for the input word indices.
        embedded = self.embedding(inputs)
        # Put the embedded inputs into the GRU.
        output, hidden = self.gru(embedded, hidden)

        # Matrix manipulation magic.
        batch_size, sequence_len, hidden_size = output.shape
        # Technically, linear layer takes a 2-D matrix as input, so more manipulation...
        output = output.contiguous().view(batch_size * sequence_len, hidden_size)
        # Put it through the classifier
        # And reshape it to [batch_size x sequence_len x vocab_size]
        output = self.classifier(output).view(batch_size, sequence_len, -1)

        return (F.softmax(output,dim=2), hidden) if use_softmax else (output, hidden)


    def generate(self, max_len, temperature=1.0):
        pass
</code></pre>

<p>And the training routine:</p>

<pre><code>device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Set the hidden_size of the GRU 
embed_size = 100
hidden_size = 100
num_layers = 1

# Setup the data.
batch_size=50
kilgariff_data = KilgariffDataset(tokenized_text)
dataloader = DataLoader(dataset=kilgariff_data, batch_size=batch_size, shuffle=True)

criterion = nn.CrossEntropyLoss(ignore_index=kilgariff_data.vocab.token2id['&lt;pad&gt;'], size_average=True)
model = Generator(len(kilgariff_data.vocab), embed_size, hidden_size, num_layers).to(device)

learning_rate = 0.003
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

#model = nn.DataParallel(model)

losses = []

def train(num_epochs, dataloader, model, criterion, optimizer):
    plt.ion()
    for _e in range(num_epochs):
        for batch in tqdm(dataloader):
            x = batch['x'].to(device)
            x_len = batch['x_len'].to(device)
            y = batch['y'].to(device)
            # Zero gradient.
            optimizer.zero_grad()
            # Feed forward. 
            output, hidden = model(x, use_softmax=True)
            # Compute loss:
            # Shape of the `output` is [batch_size x sequence_len x vocab_size]
            # Shape of `y` is [batch_size x sequence_len]
            # CrossEntropyLoss expects `output` to be [batch_size x vocab_size x sequence_len]

            _, prediction = torch.max(output, dim=2)
            loss = criterion(output.permute(0, 2, 1), y)
            loss.backward()
            optimizer.step()
            losses.append(loss.float().data)

            clear_output(wait=True)
            plt.plot(losses)
            plt.pause(0.05)


train(50, dataloader, model, criterion, optimizer)

#learning_rate = 0.05
#optimizer = optim.SGD(model.parameters(), lr=learning_rate)
#train(4, dataloader, model, criterion, optimizer)
</code></pre>

<p>But when the model is predicting, we see that it’s only predicting “the” and comma “,”.</p>

<p><strong>Anyone spot something wrong with my code? Or hyperparameters?</strong></p>

<p>The full code:</p>

<pre><code># coding: utf-8

# In[1]:


# IPython candies...
from IPython.display import Image
from IPython.core.display import HTML

from IPython.display import clear_output


# In[2]:


import numpy as np
from tqdm import tqdm

import pandas as pd

from gensim.corpora import Dictionary

import torch
from torch import nn, optim, tensor, autograd
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader

from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

device = 'cuda' if torch.cuda.is_available() else 'cpu'


# In[3]:


import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style(""darkgrid"")
sns.set(rc={'figure.figsize':(12, 8)})


torch.manual_seed(42)


# In[4]:


try: # Use the default NLTK tokenizer.
    from nltk import word_tokenize, sent_tokenize 
    # Testing whether it works. 
    # Sometimes it doesn't work on some machines because of setup issues.
    word_tokenize(sent_tokenize(""This is a foobar sentence. Yes it is."")[0])
except: # Use a naive sentence tokenizer and toktok.
    import re
    from nltk.tokenize import ToktokTokenizer
    # See https://stackoverflow.com/a/25736515/610569
    sent_tokenize = lambda x: re.split(r'(?&lt;=[^A-Z].[.?]) +(?=[A-Z])', x)
    # Use the toktok tokenizer that requires no dependencies.
    toktok = ToktokTokenizer()
    word_tokenize = word_tokenize = toktok.tokenize


# In[5]:


import os
import requests
import io #codecs


# Text version of https://kilgarriff.co.uk/Publications/2005-K-lineer.pdf
if os.path.isfile('language-never-random.txt'):
    with io.open('language-never-random.txt', encoding='utf8') as fin:
        text = fin.read()
else:
    url = ""https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt""
    text = requests.get(url).content.decode('utf8')
    with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:
        fout.write(text)


# In[6]:


# Tokenize the text.
tokenized_text = [list(map(str.lower, word_tokenize(sent))) 
                  for sent in sent_tokenize(text)]


# In[7]:


class KilgariffDataset(nn.Module):
    def __init__(self, texts):
        self.texts = texts

        # Initialize the vocab 
        special_tokens = {'&lt;pad&gt;': 0, '&lt;unk&gt;':1, '&lt;s&gt;':2, '&lt;/s&gt;':3}
        self.vocab = Dictionary(texts)
        self.vocab.patch_with_special_tokens(special_tokens)

        # Keep track of the vocab size.
        self.vocab_size = len(self.vocab)

        # Keep track of how many data points.
        self._len = len(texts)

        # Find the longest text in the data.
        self.max_len = max(len(txt) for txt in texts) 

    def __getitem__(self, index):
        vectorized_sent = self.vectorize(self.texts[index])
        x_len = len(vectorized_sent)
        # To pad the sentence:
        # Pad left = 0; Pad right = max_len - len of sent.
        pad_dim = (0, self.max_len - len(vectorized_sent))
        vectorized_sent = F.pad(vectorized_sent, pad_dim, 'constant')
        return {'x':vectorized_sent[:-1], 
                'y':vectorized_sent[1:], 
                'x_len':x_len}

    def __len__(self):
        return self._len

    def vectorize(self, tokens, start_idx=2, end_idx=3):
        """"""
        :param tokens: Tokens that should be vectorized. 
        :type tokens: list(str)
        """"""
        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx 
        # Lets just cast list of indices into torch tensors directly =)

        vectorized_sent = [start_idx] + self.vocab.doc2idx(tokens) + [end_idx]
        return torch.tensor(vectorized_sent)

    def unvectorize(self, indices):
        """"""
        :param indices: Converts the indices back to tokens.
        :type tokens: list(int)
        """"""
        return [self.vocab[i] for i in indices]


# In[8]:


kilgariff_data = KilgariffDataset(tokenized_text)
len(kilgariff_data.vocab)


# In[9]:


batch_size = 10
dataloader = DataLoader(dataset=kilgariff_data, batch_size=batch_size, shuffle=True)

for data_dict in dataloader:
    # Sort indices of data in batch by lengths.
    sorted_indices = np.array(data_dict['x_len']).argsort()[::-1].tolist()
    data_batch = {name:_tensor[sorted_indices]
                  for name, _tensor in data_dict.items()}
    print(data_batch)
    break


# In[97]:


class Generator(nn.Module):
    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers):
        super(Generator, self).__init__()

        # Initialize the embedding layer with the 
        # - size of input (i.e. no. of words in input vocab)
        # - no. of hidden nodes in the embedding layer
        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)

        # Initialize the GRU with the 
        # - size of the input (i.e. embedding layer)
        # - size of the hidden layer 
        self.gru = nn.GRU(embedding_size, hidden_size, num_layers)

        # Initialize the ""classifier"" layer to map the RNN outputs
        # to the vocabulary. Remember we need to -1 because the 
        # vectorized sentence we left out one token for both x and y:
        # - size of hidden_size of the GRU output.
        # - size of vocabulary
        self.classifier = nn.Linear(hidden_size, vocab_size)

    def forward(self, inputs, use_softmax=False, hidden=None):
        # Look up for the embeddings for the input word indices.
        embedded = self.embedding(inputs)
        # Put the embedded inputs into the GRU.
        output, hidden = self.gru(embedded, hidden)

        # Matrix manipulation magic.
        batch_size, sequence_len, hidden_size = output.shape
        # Technically, linear layer takes a 2-D matrix as input, so more manipulation...
        output = output.contiguous().view(batch_size * sequence_len, hidden_size)
        # Put it through the classifier
        # And reshape it to [batch_size x sequence_len x vocab_size]
        output = self.classifier(output).view(batch_size, sequence_len, -1)

        return (F.softmax(output,dim=2), hidden) if use_softmax else (output, hidden)


    def generate(self, max_len, temperature=1.0):
        pass


# In[98]:


# Set the hidden_size of the GRU 
embed_size = 12
hidden_size = 10
num_layers = 4

_encoder = Generator(len(kilgariff_data.vocab), embed_size, hidden_size, num_layers)


# In[99]:


# Take a batch.
_batch = next(iter(dataloader))
_inputs, _lengths = _batch['x'], _batch['x_len']
_targets = _batch['y']
max(_lengths)


# In[100]:


_output, _hidden = _encoder(_inputs)
print('Output sizes:\t', _output.shape)
print('Input sizes:\t', batch_size, kilgariff_data.max_len -1, len(kilgariff_data.vocab))
print('Target sizes:\t', _targets.shape)


# In[101]:


_, predicted_indices = torch.max(_output, dim=2)
print(predicted_indices.shape)
predicted_indices


# In[103]:


device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Set the hidden_size of the GRU 
embed_size = 100
hidden_size = 100
num_layers = 1

# Setup the data.
batch_size=50
kilgariff_data = KilgariffDataset(tokenized_text)
dataloader = DataLoader(dataset=kilgariff_data, batch_size=batch_size, shuffle=True)

criterion = nn.CrossEntropyLoss(ignore_index=kilgariff_data.vocab.token2id['&lt;pad&gt;'], size_average=True)
model = Generator(len(kilgariff_data.vocab), embed_size, hidden_size, num_layers).to(device)

learning_rate = 0.003
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

#model = nn.DataParallel(model)

losses = []

def train(num_epochs, dataloader, model, criterion, optimizer):
    plt.ion()
    for _e in range(num_epochs):
        for batch in tqdm(dataloader):
            x = batch['x'].to(device)
            x_len = batch['x_len'].to(device)
            y = batch['y'].to(device)
            # Zero gradient.
            optimizer.zero_grad()
            # Feed forward. 
            output, hidden = model(x, use_softmax=True)
            # Compute loss:
            # Shape of the `output` is [batch_size x sequence_len x vocab_size]
            # Shape of `y` is [batch_size x sequence_len]
            # CrossEntropyLoss expects `output` to be [batch_size x vocab_size x sequence_len]

            _, prediction = torch.max(output, dim=2)
            loss = criterion(output.permute(0, 2, 1), y)
            loss.backward()
            optimizer.step()
            losses.append(loss.float().data)

            clear_output(wait=True)
            plt.plot(losses)
            plt.pause(0.05)


train(50, dataloader, model, criterion, optimizer)

#learning_rate = 0.05
#optimizer = optim.SGD(model.parameters(), lr=learning_rate)
#train(4, dataloader, model, criterion, optimizer)


# In[ ]:


list(kilgariff_data.vocab.items())


# In[105]:


start_token = '&lt;s&gt;'
hidden_state = None
max_len = 20
temperature=0.8

i = 0

while start_token not in ['&lt;/s&gt;', '&lt;pad&gt;'] and i &lt; max_len:
    i += 1
    start_state = torch.tensor(kilgariff_data.vocab.token2id[start_token]).unsqueeze(0).unsqueeze(0).to(device)
    model.embedding(start_state)
    output, hidden_state = model.gru(model.embedding(start_state), hidden_state)

    batch_size, sequence_len, hidden_size = output.shape
    output = output.contiguous().view(batch_size * sequence_len, hidden_size)

    output = model.classifier(output).view(batch_size, sequence_len, -1)
    _, prediction = torch.max(F.softmax(output, dim=2), dim=2)

    start_token = kilgariff_data.vocab[int(prediction.squeeze(0).squeeze(0))]

    print(start_token, end=' ')
</code></pre>
",Training and Model Evaluation,gru language model training properly tried reimplementing simple gru language model using gru linear layer full code also training routine model predicting see predicting comma anyone spot something wrong code hyperparameters full code
Inconsistent results between predict() and predict_proba() usin scikit-learn&#39;s multi-class text classification packages,"<p>I am working on a multi-class text classification problem that has to provide the top 5 matches as opposed to just the best match. Therefore, “success” is defined as at least one of the top 5 matches being a correct classification.  The algorithm must achieve at least a 95% success rate rate given how we have defined success above. We will of course train our model on a subset of the data and test on the remaining subset in order to validate the success of our model. </p>

<p>I have been using python’s scikit-learn’s predict_proba() function in order to select the top 5 matches and calculating the success rates below using a custom script which seems to run fine on my sample data, however, I noticed that the top 5 rate of success was less than that from the top 1 success rate using .predict() on my own custom data, which is mathematically impossible. This is because the top result will automatically be included in the top 5 results, the success rate must therefore be, at the very least, equal to the top 1 success rate if not more. In order to trouble shoot, I am comparing the top 1 success rate using predict() vs predict_proba() to make sure they are equal, and making sure that the success rate on the top 5 is greater than the top 1. </p>

<p>I have set up the script below to walk you through my logic to see if am making an incorrect assumption somewhere, or if there might a problem with my data that needs to be fixed. I am testing many classifiers and features, but just for the sake of simplicity you will see that I am just using count vectors as features and Logistic Regression as the classifier since I do not believe (to my knowledge, that this is part of the issue). 
I would very much appreciate any insight that anyone may have to explain why I am finding this discrepancy. </p>

<p>Code:</p>

<pre><code># Set up environment
from sklearn.datasets import fetch_20newsgroups
from sklearn.linear_model import LogisticRegression
from sklearn import metrics, model_selection
from sklearn.feature_extraction.text import CountVectorizer

import pandas as pd
import numpy as np

#Read in data and do just a bit of preprocessing

# User's Location of git repository
Git_Location = 'C:/Documents'

# Set Data Location:
data = Git_Location + 'Data.csv'

# load the data
df = pd.read_csv(data,low_memory=False,thousands=',', encoding='latin-1')
df = df[['CODE','Description']] #select only these columns
df = df.rename(index=float, columns={""CODE"": ""label"", ""Description"": ""text""})

#Convert label to float so you don't need to encode for processing later on
df['label']=df['label'].str.replace('-', '',regex=True, case = False).str.strip()
df['label'].astype('float64', raise_on_error = True)

# drop any labels with count LT 500 to build a strong model and make our testing run faster -- we will get more data later
df = df.groupby('label').filter(lambda x : len(x)&gt;500)

#split data into testing and training
train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df.text, df.label,test_size=0.33, random_state=6,stratify=df.label)

# Other examples online use the following data types... we will do the same to remain consistent
train_y_npar = pd.Series(train_y).values
train_x_list = pd.Series.tolist(train_x)
valid_x_list = pd.Series.tolist(valid_x)

# cast validation datasets to dataframes to allow to merging later on
valid_x_df = pd.DataFrame(valid_x)
valid_y_df = pd.DataFrame(valid_y)


# Extracting features from data
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(train_x_list)
X_test_counts = count_vect.transform(valid_x_list)

# Define the model training and validation function
def TV_model(classifier, feature_vector_train, label, feature_vector_valid, valid_y, valid_x, is_neural_net=False):

    # fit the training dataset on the classifier
    classifier.fit(feature_vector_train, label)

    # predict the top n labels on validation dataset
    n = 5
    #classifier.probability = True
    probas = classifier.predict_proba(feature_vector_valid)
    predictions = classifier.predict(feature_vector_valid)

    #Identify the indexes of the top predictions
    top_n_predictions = np.argsort(probas, axis = 1)[:,-n:]

    #then find the associated SOC code for each prediction
    top_class = classifier.classes_[top_n_predictions]

    #cast to a new dataframe
    top_class_df = pd.DataFrame(data=top_class)

    #merge it up with the validation labels and descriptions
    results = pd.merge(valid_y, valid_x, left_index=True, right_index=True)
    results = pd.merge(results, top_class_df, left_index=True, right_index=True)


    top5_conditions = [
        (results.iloc[:,0] == results[0]),
        (results.iloc[:,0] == results[1]),
        (results.iloc[:,0] == results[2]),
        (results.iloc[:,0] == results[3]),
        (results.iloc[:,0] == results[4])]
    top5_choices = [1, 1, 1, 1, 1]

    #Top 1 Result
    #top1_conditions = [(results['0_x'] == results[4])]
    top1_conditions = [(results.iloc[:,0] == results[4])]
    top1_choices = [1]

    # Create the success columns
    results['Top 5 Successes'] = np.select(top5_conditions, top5_choices, default=0)
    results['Top 1 Successes'] = np.select(top1_conditions, top1_choices, default=0)

    print(""Are Top 5 Results greater than Top 1 Result?: "", (sum(results['Top 5 Successes'])/results.shape[0])&gt;(metrics.accuracy_score(valid_y, predictions)))
   print(""Are Top 1 Results equal from predict() and predict_proba()?: "", (sum(results['Top 1 Successes'])/results.shape[0])==(metrics.accuracy_score(valid_y, predictions)))

    print("" "")
    print(""Details: "")
    print(""Top 5 Accuracy Rate (predict_proba)= "", sum(results['Top 5 Successes'])/results.shape[0])
    print(""Top 1 Accuracy Rate (predict_proba)= "", sum(results['Top 1 Successes'])/results.shape[0])
    print(""Top 1 Accuracy Rate = (predict)="", metrics.accuracy_score(valid_y, predictions))
</code></pre>

<p><strong>Example of Output using scikit learn’s built in twentynewsgroups dataset (this is my goal):</strong> 
Note: I ran this exact code on another dataset and was able to produce these results which tells me that the function and it's dependencies work therefore the issue must be in the data somehow. </p>

<pre><code>Are Top 5 Results greater than Top 1 Result?:  True 
Are Top 1 Results equal from predict() and predict_proba()?:  True  
</code></pre>

<p><strong>Details:</strong>  </p>

<pre><code>Top 5 Accuracy Rate (predict_proba)=  0.9583112055231015 
Top 1 Accuracy Rate (predict_proba)=  0.8069569835369091 
Top 1 Accuracy Rate = (predict)= 0.8069569835369091
</code></pre>

<p>Now run on my data: </p>

<pre><code>TV_model(LogisticRegression(), X_train_counts, train_y_npar, X_test_counts, valid_y_df, valid_x_df)
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>Are Top 5 Results greater than Top 1 Result?:  False 
Are Top 1 Results equal from predict() and predict_proba()?:  False   
</code></pre>

<p><strong>Details:</strong>  </p>

<ul>
<li>Top 5 Accuracy Rate (predict_proba)=  0.6581632653061225</li>
<li>Top 1 Accuracy Rate (predict_proba)=  0.2010204081632653  </li>
<li>Top 1 Accuracy Rate = (predict)= 0.8091187478734263</li>
</ul>
",Training and Model Evaluation,inconsistent result predict predict proba usin scikit learn multi class text classification package working multi class text classification problem ha provide top match opposed best match therefore success defined least one top match correct classification algorithm must achieve least success rate rate given defined success course train model subset data test remaining subset order validate success model using python scikit learn predict proba function order select top match calculating success rate using custom script seems run fine sample data however noticed top rate success wa le top success rate using predict custom data mathematically impossible top result automatically included top result success rate must therefore least equal top success rate order trouble shoot comparing top success rate using predict v predict proba make sure equal making sure success rate top greater top set script walk logic see making incorrect assumption somewhere might problem data need fixed testing many classifier feature sake simplicity see using count vector feature logistic regression classifier since believe knowledge part issue would much appreciate insight anyone may explain finding discrepancy code example output using scikit learn built twentynewsgroups dataset goal note ran exact code another dataset wa able produce result tell function dependency work therefore issue must data somehow detail run data output detail top accuracy rate predict proba top accuracy rate predict proba top accuracy rate predict
LSTM skews predictions towards one value,"<p>I am trying to train an LSTM, followed by a Dense layer in keras with numerical input sequences of different lengths. the numbers range is [1,13]. each one of those sequences ends with the same number, 13 in my case. </p>

<p>I train the controller on a few sequences, use the trained model to generate a few more sequences with the same properties, add them to the training set and train the LSTM again. As this loop goes on, the LSTM predictions start converging towards the final value of each sequence. </p>

<p>The sequences are padded to a certain maximum length. as a result the x_train data is of the size (None, max_len-1) and y_train data is categorical data of the last element of each input sequence. in this case, every element in the y_train data is the same (one hot encoded vector for the number 13). </p>

<ol>
<li>Is the way input and output data is structured the reason for this skewing of predictions?</li>
<li>Is there a way to work around it?</li>
</ol>
",Training and Model Evaluation,lstm skews prediction towards one value trying train lstm followed dense layer kera numerical input sequence different length number range one sequence end number case train controller sequence use trained model generate sequence property add training set train lstm loop go lstm prediction start converging towards final value sequence sequence padded certain maximum length result x train data size none max len train data categorical data last element input sequence case every element train data one hot encoded vector number way input output data structured reason skewing prediction way work around
TextLMDataBunch Memory issue Language Model Fastai,"<p>I have a dataset with 45 million rows of data. I have three 6gb ram gpu. I am trying to train a language model on the data.</p>

<p>For that, I am trying to load the data as the fastai data bunch. But this part always fails because of the memory issue. </p>

<pre><code>data_lm = TextLMDataBunch.from_df('./', train_df=df_trn, 
valid_df=df_val, bs=10)
</code></pre>

<p>How do I handle this issue?</p>
",Training and Model Evaluation,textlmdatabunch memory issue language model fastai dataset million row data three gb ram gpu trying train language model data trying load data fastai data bunch part always fails memory issue handle issue
Is there any way to calculate gensim WmdSimilarity faster,"<p>I am working with gensim WmdSimilarity. I followed <a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html"" rel=""nofollow noreferrer"">this tutorial</a>. But for a pre-trained model, it took 5-6 seconds for each output. I found <a href=""https://github.com/src-d/wmd-relax"" rel=""nofollow noreferrer"">some other implementation</a> of calculating WMD (Word Mover Distance) but I'm not sure about using it with pre-trained W2V </p>
",Training and Model Evaluation,way calculate gensim wmdsimilarity faster working gensim wmdsimilarity followed tutorial pre trained model took second output found implementation calculating wmd word mover distance sure using pre trained w v
is patternmatching function in python relevant for answer evaluating system?,"<p>I'm developing a system for conducting online examinations that evaluate students answers compared to the teacher's reference answer. Currently, I have pattern matching in python compares two texts and generates a value between <code>0</code> and <code>1</code>. is there any other efficient method to do this like NLP functions???</p>
",Training and Model Evaluation,patternmatching function python relevant answer evaluating system developing system conducting online examination evaluate student answer compared teacher reference answer currently pattern matching python compare two text generates value efficient method like nlp function
"Temporal logic (e.g., LTL) repository","<p>I am currently dealing with the problem of formalizing the content of natural language texts by means of temporal logic, for instance LTL.</p>

<p>An example would be the phrase </p>

<blockquote>
  <p>""When a train is approaching, a train will eventually cross"",</p>
</blockquote>

<p>that should correspond to an LTL specification like</p>

<blockquote>
  <p>G(train_approaching ---> F(train_crossing))</p>
</blockquote>

<p>I would like to consider the problem as a translation task, from natural language to LTL. In order to train a translation model, among other things, I'd need a proper training set, in which each instance should be represented by the natural language text, paired with the corresponding LTL (or another kind of temporal logic) formula.</p>

<p>I have already found these resources:</p>

<ul>
<li><a href=""http://patterns.projects.cs.ksu.edu/documentation/patterns/ltl.shtml"" rel=""nofollow noreferrer"">http://patterns.projects.cs.ksu.edu/documentation/patterns/ltl.shtml</a> A repository of property patterns (based on LTL), for a total of around 100 instances</li>
<li><a href=""https://gitlab.lrz.de/i7/ltlstore"" rel=""nofollow noreferrer"">https://gitlab.lrz.de/i7/ltlstore</a> LTL Store, which is a repository of LTL formulas, however without a paired natural language text (it could still be useful as ""monolingual"" training data)</li>
</ul>

<p>Of course, the overall goal would be that of handling full-logic, however I may also focus on a subset of temporal logic-based patterns to begin with.</p>

<p>Do you know any other dataset that I could use as training data for this purpose?</p>
",Training and Model Evaluation,temporal logic e g ltl repository currently dealing problem formalizing content natural language text mean temporal logic instance ltl example would phrase train approaching train eventually cross correspond ltl specification like g train approaching f train crossing would like consider problem translation task natural language ltl order train translation model among thing need proper training set instance represented natural language text paired corresponding ltl another kind temporal logic formula already found resource repository property pattern based ltl total around instance ltl store repository ltl formula however without paired natural language text could still useful monolingual training data course overall goal would handling full logic however may also focus subset temporal logic based pattern begin know dataset could use training data purpose
SpaCy Multiple TextCategorizer&#39;s in one pipeline,"<p>I'm working with SpaCy and I want to know if there is way to include multiple text categorizers within one pipeline and keeps is predictions separated. I already was able to train the two categorizers separately, but I can't figure out if there is a way to merge both classifiers in one pipeline. My current implementation looks like the following snippet:</p>

<pre><code>nlp.add_pipe(text_categorizer_1, last=True)
text_categorizer_1.add_label(""MyLabelforCategorizer1"")

nlp.add_pipe(text_categorizer_2, name=""textcat2"",last=True)
text_categorizer_2.add_label(""MyLabelforCategorizer2"")

# Train and validation code here

doc = nlp(""My sample text"")
print(doc.cats)
# doc.cats will contain an object with the categories and its  associated probabilities
# but this object mix the result of both classifiers
# {""MyLabelforCategorizer1"":0.1, ""MyLabelforCategorizer2"":0.9}
</code></pre>

<p>I want to know if add two categorizers to a single pipeline is supported? Doing that can have a negative impact in the underlying model (The CNN neural network of SpaCy)? If there is not impact how can I get the results from each categorizer? is enough to take the max value for each label set?.</p>
",Training and Model Evaluation,spacy multiple textcategorizer one pipeline working spacy want know way include multiple text categorizers within one pipeline keep prediction separated already wa able train two categorizers separately figure way merge classifier one pipeline current implementation look like following snippet want know add two categorizers single pipeline supported negative impact underlying model cnn neural network spacy impact get result categorizer enough take max value label set
"LSTM high loss, doesn&#39;t decreasing with each epoch","<p>I have a block of lines with some text, and for each line I want to predict the next word (word_0 -> word_1, than by word_0 and word_1 -> word_2 and so on for each line). Great tutorial and code here: <a href=""https://stackoverflow.com/questions/42064690/using-pre-trained-word2vec-with-lstm-for-word-generation?answertab=active#tab-top"">Predict next word</a>
<a href=""https://gist.github.com/maxim5/c35ef2238ae708ccb0e55624e9e0252b"" rel=""nofollow noreferrer"">Source code</a></p>

<p>But in my way the loss doesn't decrease:</p>

<pre><code>....
Epoch 42/50
2668/2668 [==============] - 1777s 666ms/step - loss: 4.6435 - acc: 0.1361
Epoch 43/50
2668/2668 [==============] - 1791s 671ms/step - loss: 4.6429 - acc: 0.1361
Epoch 44/50
2668/2668 [==============] - 1773s 665ms/step - loss: 4.6431 - acc: 0.1361
Epoch 45/50
2668/2668 [==============] - 1770s 664ms/step - loss: 4.6417 - acc: 0.1361
Epoch 46/50
2668/2668 [==============] - 1774s 665ms/step - loss: 4.6436 - acc: 0.1361
....
</code></pre>

<p>My LSTM NN setting:</p>

<pre><code>nn_model = Sequential()
nn_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, 
weights=[pretrained_weights]))
nn_model.add(LSTM(units=embedding_size, return_sequences=True))
nn_model.add(LSTM(units=embedding_size))
nn_model.add(Dense(units=vocab_size))
nn_model.add(Activation('softmax'))
nn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', 
metrics=['accuracy'])
</code></pre>

<p>where:</p>

<pre><code>pretrained_weights = model.wv.syn0 (model is Word2Vec model)
vocab_size, embedding_size = pretrained_weights.shape   
</code></pre>

<p>I tried to change batch_size (128, 64, 20, 10); tried to add any LSTM layers, but all doesn't help me. 
What's wrong and how can I fix this problem?</p>
",Training and Model Evaluation,lstm high loss decreasing epoch block line text line want predict next word word word word word word line great tutorial code source code way loss decrease lstm nn setting tried change batch size tried add lstm layer help wrong fix problem
PyTorch n-to-1 LSTM does not learn anything,"<p>I am new to PyTorch and LSTMs and I am trying to train a classification model that takes a sentences where each word is encoded via word2vec (pre-trained vectors) and outputs one class after it saw the full sentence. I have four different classes. The sentences have variable length.</p>

<p>My code is running without errors, but it always predicts the same class, no matter how many epochs I train my model. So I think the gradients are not properly backpropagated. Here is my code:</p>

<pre><code>class LSTM(nn.Module):
    def __init__(self, embedding_dim, hidden_dim, tagset_size):
        super(LSTM, self).__init__()
        self.hidden_dim = hidden_dim
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)
        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)
        self.hidden = self.init_hidden()

    def init_hidden(self):
        # The axes semantics are (num_layers, minibatch_size, hidden_dim)
        return (torch.zeros(1, 1, self.hidden_dim).to(device),
                torch.zeros(1, 1, self.hidden_dim).to(device))

    def forward(self, sentence):
        lstm_out, self.hidden = self.lstm(sentence.view(len(sentence), 1, -1), self.hidden)
        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))
        tag_scores = F.log_softmax(tag_space, dim=1)
        return tag_scores

EMBEDDING_DIM = len(training_data[0][0][0])
HIDDEN_DIM = 256

model = LSTM(EMBEDDING_DIM, HIDDEN_DIM, 4)
model.to(device)
loss_function = nn.NLLLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

for epoch in tqdm(range(n_epochs)):
    for sentence, tag in tqdm(training_data):
        model.zero_grad()

        model.hidden = model.init_hidden()

        sentence_in = torch.tensor(sentence, dtype=torch.float).to(device)
        targets = torch.tensor([label_to_idx[tag]], dtype=torch.long).to(device)

        tag_scores = model(sentence_in)

        res = torch.tensor(tag_scores[-1], dtype=torch.float).view(1,-1).to(device)
        # I THINK THIS IS WRONG???
        print(res)     # tensor([[-10.6328, -10.6783, -10.6667,  -0.0001]], device='cuda:0', grad_fn=&lt;CopyBackwards&gt;)
        print(targets) # tensor([3], device='cuda:0')

        loss = loss_function(res, targets)

        loss.backward()
        optimizer.step()
</code></pre>

<p>The code is largely inspired by <a href=""https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html</a>
The difference is that they have a sequence-to-sequence model and I have a sequence-to-ONE model.</p>

<p>I am not sure what the problem is, but I guess that the scores returned by the model contain a score for each tag and my ground truth only contains the index of the correct class? How would this be handled correctly?</p>

<p>Or is the loss function maybe not the correct one for my use case? Also I am not sure if this is done correctly:</p>

<pre><code>res = torch.tensor(tag_scores[-1], dtype=torch.float).view(1,-1).to(device)
</code></pre>

<p>By taking <code>tag_scores[-1]</code> I want to get the scores after the last word has been given to the network because tag_scores contains the scores after each step, if I understand correctly.</p>

<p>And this is how I evaluate:</p>

<pre><code>with torch.no_grad():
    preds = []
    gts = []

    for sentence, tag in tqdm(test_data):
        inputs = torch.tensor(sentence, dtype=torch.float).to(device)

        tag_scores = model(inputs)

        # find index with max value (this is the class to be predicted)
        pred = [j for j,v in enumerate(tag_scores[-1]) if v == max(tag_scores[-1])][0]

        print(pred, idx_to_label[pred], tag)
        preds.append(pred)
        gts.append(label_to_idx[tag])

print(f1_score(gts, preds, average='micro'))
print(classification_report(gts, preds))
</code></pre>

<p><strong>EDIT</strong>:</p>

<p>When shuffling the data before training it seems to work. But why?</p>

<p><strong>EDIT 2</strong>:</p>

<p>I think the reason why shuffling is needed is that my training data contains samples for each class in groups. So when training them each after the other, the model will only see the same class in the last N iterations and therefore it will only predict this class. Another reason might also be that I am currently using mini-batches of only one sample because I haven't figured out yet how to use other sizes.</p>
",Training and Model Evaluation,pytorch n lstm doe learn anything new pytorch lstms trying train classification model take sentence word encoded via word vec pre trained vector output one class saw full sentence four different class sentence variable length code running without error always predicts class matter many epoch train model think gradient properly backpropagated code code largely inspired difference sequence sequence model sequence one model sure problem guess score returned model contain score tag ground truth contains index correct class would handled correctly loss function maybe correct one use case also sure done correctly taking want get score last word ha given network tag score contains score step understand correctly evaluate edit shuffling data training seems work edit think reason shuffling needed training data contains sample class group training model see class last n iteration therefore predict class another reason might also currently using mini batch one sample figured yet use size
how to make correct dimension of training and test test to fit in the model for elmo embedding,"<p>i have got error while fitting the elmo embedding model with training set of dimension x_tr=(43163, 50),and y_tr=
(43163, 50, 1) as :</p>

<pre><code>InvalidArgumentError: Incompatible shapes: [1600] vs. [32,50]
     [[{{node metrics/acc/Equal}} = Equal[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](metrics/acc/Reshape, metrics/acc/Cast)]].
</code></pre>

<p>how to solve this error ?</p>

<p>i tried to solve by making the training sample divisible by the batch size .</p>

<h1>training set for fitting the model:</h1>

<pre><code>X_tr=np.array(X_tr)
print(X_tr.shape)
y_tr = np.array(y_tr).reshape(len(y_tr), max_len, 1)
print(y_tr.shape)
(43163, 50)
(43163, 50, 1)
</code></pre>

<h1>making the model :</h1>

<pre><code>input_text = Input(shape=(max_len,), dtype=tf.string)
embedding = Lambda(ElmoEmbedding, output_shape=(None, 1024))(input_text)
x = Bidirectional(LSTM(units=512, return_sequences=True,
                       recurrent_dropout=0.2, dropout=0.2))(embedding)
x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,
                           recurrent_dropout=0.2, dropout=0.2))(x)
x = add([x, x_rnn])  # residual connection to the first biLSTM
out = TimeDistributed(Dense(n_tags, activation=""softmax""))(x)
model = Model(input_text, out)
</code></pre>

<h1>compiling the model:</h1>

<pre><code>model.compile(optimizer=""adam"", loss=""sparse_categorical_crossentropy"", metrics=[""accuracy""])
</code></pre>

<h1>fitting the model:</h1>

<pre><code>fit_model = model.fit(np.array(X_tr), np.array(y_tr).reshape(len(y_tr), max_len, 1), validation_split=0.1,
                    batch_size=batch_size, epochs=5, verbose=1)
</code></pre>

<p>ERROR:</p>

<pre><code>    InvalidArgumentError: Incompatible shapes: [1600] vs. [32,50]
     [[{{node metrics/acc/Equal}} = Equal[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](metrics/acc/Reshape, metrics/acc/Cast)]]

Expected result could be:
Train on 38816 samples, validate on 4320 samples
Epoch 1/5
38816/38816 [==============================] - 433s 11ms/step - loss: 0.0625 - acc: 0.9818 - val_loss: 0.0459 - val_acc: 0.9858
Epoch 2/5
38816/38816 [==============================] - 430s 11ms/step - loss: 0.0404 - acc: 0.9869 - val_loss: 0.0421 - val_acc: 0.9865
Epoch 3/5
38816/38816 [==============================] - 429s 11ms/step - loss: 0.0334 - acc: 0.9886 - val_loss: 0.0426 - val_acc: 0.9868
Epoch 4/5
38816/38816 [==============================] - 429s 11ms/step - loss: 0.0275 - acc: 0.9904 - val_loss: 0.0431 - val_acc: 0.9868
Epoch 5/5
38816/38816 [==============================] - 430s 11ms/step - loss: 0.0227 - acc: 0.9920 - val_loss: 0.0461 - val_acc: 0.9867
</code></pre>
",Training and Model Evaluation,make correct dimension training test test fit model elmo embedding got error fitting elmo embedding model training set dimension x tr tr solve error tried solve making training sample batch size training set fitting model making model compiling model fitting model error
Computing a similarity score for a set of sentences,"<p>My team does a lot of chatbot training, and I'm trying to come up with some tools to improve the quality of our work.  In chatbot training, it is really important to train intents with diverse utterances that phrase the same intent in very different ways.  Ideally, there would be very little similarity in the syntax of the utterances in the set.</p>

<p><strong>Here's an example for an intent inquiring about medical insurance coverage</strong></p>

<p><em>Bad set of utterances</em></p>

<ul>
<li>Is my daughter covered by insurance?</li>
<li>Is my son covered by medical insurance?</li>
<li>Will my son be covered by insurance?</li>
</ul>

<p><em>Decent set of utterances</em></p>

<ul>
<li><p>How can I look up whether we have insurance coverage for the whole family?</p></li>
<li><p>Seeking details on eligibility for medical coverage</p></li>
<li><p>Is there a document that details who is protected under our medical insurance policy?</p></li>
</ul>

<p>I want to be able to take all of the utterances associated to an intent and analyze them for similarity.  I would expect my set of bad utterances to have a high similarity score and my set of decent utterances to have a low similarity score.</p>

<p>I've tried playing around with a few doc2vec tutorials, but I feel like I'm missing something.  I keep seeing stuff like this:</p>

<ul>
<li>Train a set of data and then measure the similarity of a new sentence to your set of data</li>
<li>Measure the similarity between two sentences</li>
</ul>

<p>I need to have an array of sentences and understand how similar they are to each other.</p>

<p>Any advice on achieving this?</p>

<p>Answering some questions:</p>

<ol>
<li>What makes the bad utterances bad?The utterances themselves are not bad, it is the lack of variety between them. If most of the training had been like the “bad” set, then real user utterances of greater variety will not be recognized correctly.</li>
<li>Are you trying to discover new intents? No, this is for prerelease training, trying to improve the effectiveness of it.</li>
<li>Why do bad utterances have high similarity scores and decent utterances have low similarity scores?  This is a hypothesis. I know how varied real user utterances are, and I have found my trainers fall into ruts when training, asking things the same way, and not seeing good accuracy results. Improving the variety in the utterances tends to result in better accuracy.</li>
<li>What will I do with this info? I’ll use it to assess the training quality of an intent, to determine if more training is likely necessary. In the future we might build real time tools as utterances are being added to let trainers know if they’re being too repetitive.</li>
</ol>
",Training and Model Evaluation,computing similarity score set sentence team doe lot chatbot training trying come tool improve quality work chatbot training really important train intent diverse utterance phrase intent different way ideally would little similarity syntax utterance set example intent inquiring medical insurance coverage bad set utterance daughter covered insurance son covered medical insurance son covered insurance decent set utterance look whether insurance coverage whole family seeking detail eligibility medical coverage document detail protected medical insurance policy want able take utterance associated intent analyze similarity would expect set bad utterance high similarity score set decent utterance low similarity score tried playing around doc vec tutorial feel like missing something keep seeing stuff like train set data measure similarity new sentence set data measure similarity two sentence need array sentence understand similar advice achieving answering question make bad utterance bad utterance bad lack variety training like bad set real user utterance greater variety recognized correctly trying discover new intent prerelease training trying improve effectiveness bad utterance high similarity score decent utterance low similarity score hypothesis know varied real user utterance found trainer fall rut training asking thing way seeing good accuracy result improving variety utterance tends result better accuracy info use ass training quality intent determine training likely necessary future might build real time tool utterance added let trainer know repetitive
Train Fastext on non-english data set,"<p>I'm into a new project which I desire to represent words as vectors, I read about Fasttext library and I saw that they have pre-trained models for language which is not English. The purpose is to predict closeness between different words </p>

<blockquote>
  <p><a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a></p>
</blockquote>

<p>what I want to know is can I train a Fasttext model on non-English data and like articles of news sites, to achieve better results for specific genres like politics and nowadays topics, and so.</p>

<ol>
<li>Can I train it on non-English data sets?</li>
<li>How long does it take to train a model for 10 GB of text? is it big enough?</li>
<li>There are any better solutions?</li>
</ol>

<p><strong>Thanks in advance!</strong></p>
",Training and Model Evaluation,train fastext non english data set new project desire represent word vector read fasttext library saw pre trained model language english purpose predict closeness different word want know train fasttext model non english data like article news site achieve better result specific genre like politics nowadays topic train non english data set long doe take train model gb text big enough better solution thanks advance
Supervised Extractive Text Summarization,"<p>I want to extract potential sentences from news articles which can be part of article summary.</p>

<p>Upon spending some time, I found out that this can be achieved in two ways,</p>

<ol>
<li>Extractive Summarization (Extracting sentences from text and clubbing them)</li>
<li>Abstractive Summarization (internal language representation to generate more human-like summaries)</li>
</ol>

<p>Reference: <a href=""https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/"" rel=""nofollow noreferrer"">rare-technologies.com</a></p>

<p>I followed <a href=""http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html"" rel=""nofollow noreferrer"">abigailsee's Get To The Point: Summarization with Pointer-Generator Networks</a> for summarization which was producing good results with the pre-trained model but it was abstractive.</p>

<p><strong>The Problem:</strong>
Most of the extractive summarizers that I have looked so far(PyTeaser, PyTextRank and Gensim) are not based on Supervised learning but on Naive Bayes classifier, tf–idf, POS-tagging, sentence ranking based on keyword-frequency, position etc., which don't require any training. </p>

<p>Few things that I have tried so far to extract potential summary sentences.</p>

<ul>
<li>Get all sentences of articles and label summary sentences as 1 and 0 for all others</li>
<li>Clean up the text and apply stop word filters</li>
<li>Vectorize a text corpus using Tokenizer <code>from keras.preprocessing.text import Tokenizer</code> with Vocabulary size of 20000 and pad all sequences to average length of all sentences.</li>
<li>Build a Sqequential keras model a train it.</li>
</ul>

<pre class=""lang-py prettyprint-override""><code>model_lstm = Sequential()
model_lstm.add(Embedding(20000, 100, input_length=sentence_avg_length))
model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model_lstm.add(Dense(1, activation='sigmoid'))
model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
</code></pre>

<p>This is giving very low accuracy ~0.2</p>

<p>I think this is because the above model is more suitable for positive/negative sentences rather than summary/non-summary sentences classification.   </p>

<p>Any guidance on approach to solve this problem would be appreciated.</p>
",Training and Model Evaluation,supervised extractive text summarization want extract potential sentence news article part article summary upon spending time found achieved two way extractive summarization extracting sentence text clubbing abstractive summarization internal language representation generate human like summary reference rare technology com followed abigailsee get point summarization pointer generator network summarization wa producing good result pre trained model wa abstractive problem extractive summarizers looked far pyteaser pytextrank gensim based supervised learning naive bayes classifier tf idf po tagging sentence ranking based keyword frequency position etc require training thing tried far extract potential summary sentence get sentence article label summary sentence others clean text apply stop word filter vectorize text corpus using tokenizer vocabulary size pad sequence average length sentence build sqequential kera model train giving low accuracy think model suitable positive negative sentence rather summary non summary sentence classification guidance approach solve problem would appreciated
Missing words when training word2vec model,"<p>I am trying to train a word2vec model using gensim. This is the line I am using:</p>

<pre><code>model = Word2Vec(training_texts, size=50, window=5, min_count=1, workers=4, max_vocab_size=20000)
</code></pre>

<p>Where training_texts is a list of lists of strings representing words. The corpora I am using has 8924372 sentences with 141,985,244 words and 1,531,477 unique words. After training, only 15642 words are present in the model:</p>

<pre><code>len(list(model.wv.vocab))
# returns 15642
</code></pre>

<p>Shouldn't the model have 20,000 words, as specified max_vocab_size? Why is it missing most of the training words?</p>

<p>Thanks!!</p>
",Training and Model Evaluation,missing word training word vec model trying train word vec model using gensim line using training text list list string representing word corpus using ha sentence word unique word training word present model model word specified max vocab size missing training word thanks
How to implement pairwise hinge loss in Keras?,"<p>I'm trying to implement a pairwise hinge loss for two tensors which are both 200 dimensional. The goal is to use the cosine similarity of that two tensors as a scoring function and train the model with the pairwise hinge loss. </p>

<p>The model is accepting two text inputs and they have been converted to two 200 dimensional vectors. (The second text input is the correct label for the first text input).</p>

<p>Can anyone show me how to implement this in Keras?</p>

<pre><code>def cosine_distance(vests):
    jd, jt = vests
    jd = K.l2_normalize(jd, axis=-1)
    jt = K.l2_normalize(jt, axis=-1)
    return -K.mean(jd * jt, axis=-1, keepdims=True)

def cos_dist_output_shape(shapes):
    shape1, shape2 = shapes
    return (shape1[0],1)
</code></pre>

<p>Above are the code I have for the lambda layer to calculate the cosine similarity and jd is the first text input and jt is the second.</p>

<p>Thanks</p>
",Training and Model Evaluation,implement pairwise hinge loss kera trying implement pairwise hinge loss two tensor dimensional goal use cosine similarity two tensor scoring function train model pairwise hinge loss model accepting two text input converted two dimensional vector second text input correct label first text input anyone show implement kera code lambda layer calculate cosine similarity jd first text input jt second thanks
Gensim pretrained model similarity,"<p>Problem :</p>

<p>Im using glove pre-trained model with vectors to retrain my model with a specific domain say #cars, after training I want to find similar words within my domain but I got words not in my domain corpus, I believe it's from glove's vectors. </p>

<pre><code>model_2.most_similar(positive=['spacious'],    topn=10)

[('bedrooms', 0.6275501251220703),
 ('roomy', 0.6149100065231323),
 ('luxurious', 0.6105825901031494),
 ('rooms', 0.5935696363449097),
 ('furnished', 0.5897485613822937),
 ('cramped', 0.5892841219902039),
 ('courtyard', 0.5721820592880249),
 ('bathrooms', 0.5618442893028259),
 ('opulent', 0.5592212677001953),
 ('expansive', 0.555268406867981)]
</code></pre>

<p>Here I expect something like leg-room, car's spacious features mentioned in the domain's corpus. How can we exclude the glove vectors while having similar vectors?</p>

<p>Thanks  </p>
",Training and Model Evaluation,gensim pretrained model similarity problem im using glove pre trained model vector retrain model specific domain say car training want find similar word within domain got word domain corpus believe glove vector expect something like leg room car spacious feature mentioned domain corpus exclude glove vector similar vector thanks
What is the stochastic aspect of Word2Vec?,"<p>I'm vectorizing words on a few different corpora with Gensim and am getting results that are making me rethink how Word2Vec functions. My understanding was that Word2Vec was deterministic, and that the position of a word in a vector space would not change from training to training. If ""My cat is running"" and ""your dog can't be running"" are the two sentences in the corpus, then the value of ""running"" (or its stem) seems necessarily fixed.</p>

<p>However, I've found that that value indeed does vary across models, and words keep changing where they are on a vector space when I train the model. The differences are not always hugely meaningful, but they do indicate the existence of some random process. What am I missing here?</p>
",Training and Model Evaluation,stochastic aspect word vec vectorizing word different corpus gensim getting result making rethink word vec function understanding wa word vec wa deterministic position word vector space would change training training cat running dog running two sentence corpus value running stem seems necessarily fixed however found value indeed doe vary across model word keep changing vector space train model difference always hugely meaningful indicate existence random process missing
Using Keras for predicting next word,"<p>I have a sequence prediction problem that I approach as a language model. 
My data contains 4 choices (1-4) and a reward (1-100) .
I started using Keras but I'm not sure it has the flexibility I need.</p>

<ol>
<li>This is how the model's architecture looks :
<img src=""https://cdn-images-1.medium.com/max/800/1*jdObA6XD2tSVvOCZKf92wQ.png"" alt=""Train""></li>
</ol>

<p>I'm not sure about the test phase. One option is sampling:</p>

<p><img src=""https://cdn-images-1.medium.com/max/800/1*XRkki7DNXXlKCkC2JMsONw.png"" alt=""2""></p>

<p>And I'm not sure how to evaluate the output of this option vs my test set.</p>

<p>Another option is to give the trained model a sequence and let it plot the last timestep value (like giving a sentence and predicting last word) - but still having x = t_hat. </p>

<p>is it possible in Keras ? I can't find examples like this.</p>

<ol start=""2"">
<li>Besides passing the previous choice (or previous word) as an input , I need to pass the second feature, which is a reward value. The choice are one-hot encoded , how can I add a single number with an encoded vector?</li>
</ol>

<p>EDIT :
This is the training phase (haven't done the sampling yet) :</p>

<pre><code>model = Sequential()
model.add(LSTM(64, input_shape=(seq_length, X_train.shape[2]) , return_sequences=True))
model.add(Dense(y_cat_train.shape[2], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_cat_train, epochs=100, batch_size=10, verbose=2)
</code></pre>
",Training and Model Evaluation,using kera predicting next word sequence prediction problem approach language model data contains choice reward started using kera sure ha flexibility need model architecture look sure test phase one option sampling sure evaluate output option v test set another option give trained model sequence let plot last timestep value like giving sentence predicting last word still x hat possible kera find example like besides passing previous choice previous word input need pas second feature reward value choice one hot encoded add single number encoded vector edit training phase done sampling yet
TF-IDF + Multiple Regression Prediction Problem,"<p>I have a dataset of ~10,000 rows of vehicles sold on a portal similar to Craigslist. The columns include price, mileage, no. of previous owners, how soon the car gets sold (in days), and most importantly a body of text that describes the vehicle (e.g. ""accident free, serviced regularly""). </p>

<p>I would like to find out which keywords, when included, will result in the car getting sold sooner. However I understand how soon a car gets sold also depends on the other factors especially price and mileage. </p>

<p>Running a TfidfVectorizer in scikit-learn resulted in very poor prediction accuracy. Not sure if I should try including price, mileage, etc. in the regression model as well, as it seems pretty complicated. Currently am considering repeating the TF-IDF regression on a particular segment of the data that is sufficiently huge (perhaps Toyotas priced at $10k-$20k).</p>

<p>The last resort is to plot two histograms, one of vehicle listings containing a specific word/phrase and another for those that do not. The limitation here would be that the words that I choose to plot will be based on my subjective opinion.</p>

<p>Are there other ways to find out which keywords could potentially be important? Thanks in advance.</p>
",Training and Model Evaluation,tf idf multiple regression prediction problem dataset row vehicle sold portal similar craigslist column include price mileage previous owner soon car get sold day importantly body text describes vehicle e g accident free serviced regularly would like find keywords included result car getting sold sooner however understand soon car get sold also depends factor especially price mileage running tfidfvectorizer scikit learn resulted poor prediction accuracy sure try including price mileage etc regression model well seems pretty complicated currently considering repeating tf idf regression particular segment data sufficiently huge perhaps toyota priced k k last resort plot two histogram one vehicle listing containing specific word phrase another limitation would word choose plot based subjective opinion way find keywords could potentially important thanks advance
Gensim doc2vec file stream training worse performance,"<p>Recently I switched to gensim 3.6 and the main reason was the optimized training process, which streams the training data directly from file, thus avoiding the GIL performance penalties.</p>

<p>This is how I used to trin my doc2vec:</p>

<pre><code>training_iterations = 20
d2v = Doc2Vec(vector_size=200, workers=cpu_count(), alpha=0.025, min_alpha=0.00025, dm=0)
d2v.build_vocab(corpus)

for epoch in range(training_iterations):
    d2v.train(corpus, total_examples=d2v.corpus_count, epochs=d2v.iter)
    d2v.alpha -= 0.0002
    d2v.min_alpha = d2v.alpha
</code></pre>

<p>And it is classifying documents quite well, only draw back is that when it is trained CPUs are utilized at 70%</p>

<p>So the new way:</p>

<pre><code>corpus_fname = ""spped.data""
save_as_line_sentence(corpus, corpus_fname)

# Choose num of cores that you want to use (let's use all, models scale linearly now!)
num_cores = cpu_count()

# Train models using all cores
d2v_model = Doc2Vec(corpus_file=corpus_fname, workers=num_cores, dm=0, vector_size=200, epochs=50)
</code></pre>

<p><strong>Now all CPUs are utilized at 100%</strong></p>

<p>but the model is performing very poorly.
According to the documentation, I should not use the train method also, I should use only epoch count and not iterations, also the min_aplpha and aplha values should not be touched.</p>

<p>The configuration of both Doc2Vec looks the same to me so is there an issue with my new set up or configuration, or there is something wrong with the new version of gensim?</p>

<p>P.S I am using the same corpus in both cases, also I tried epoch count = 100, also with smaller numbers like 5-20, but I had no luck</p>

<p><strong>EDIT</strong>: First model was doing 20 iterations 5 epoch each, second was doing 50 epoch, so having the second model make 100 epochs made it perform even better, since I was no longer managing the alpha by myself.</p>

<p>About the second issue that popped up: when providing file with line documents, the doc ids were not always corresponding to the lines, I didn't manage to figure out what could be causing this, it seems to work fine for small corpus, If I find out what I am doing wrong I will update this answer.</p>

<p>The final configuration for corpus of size 4GB looks like this</p>

<pre><code>    d2v = Doc2Vec(vector_size=200, workers=cpu_count(), alpha=0.025, min_alpha=0.00025, dm=0)
    d2v.build_vocab(corpus)
    d2v.train(corpus, total_examples=d2v.corpus_count, epochs=100)
</code></pre>
",Training and Model Evaluation,gensim doc vec file stream training worse performance recently switched gensim main reason wa optimized training process stream training data directly file thus avoiding gil performance penalty used trin doc vec classifying document quite well draw back trained cpu utilized new way cpu utilized model performing poorly according documentation use train method also use epoch count iteration also min aplpha aplha value touched configuration doc vec look issue new set configuration something wrong new version gensim p using corpus case also tried epoch count also smaller number like luck edit first model wa iteration epoch second wa epoch second model make epoch made perform even better since wa longer managing alpha second issue popped providing file line document doc id always corresponding line manage figure could causing seems work fine small corpus find wrong update answer final configuration corpus size gb look like
"Can tesseract work with languages such as bengali? If so, with how much accuracy and what steps should I follow to implement it for bengali language?","<p>I want to implement an offline program which can detect bengali text from an image (white background black text). I need to know how to approach my work to begin with</p>
",Training and Model Evaluation,tesseract work language bengali much accuracy step follow implement bengali language want implement offline program detect bengali text image white background black text need know approach work begin
How can I solve a classification problem with a dependent variable with more than two values,"<p>I have a simple NLP problem, where I have some written reviews that have a simple binary positive or negative judgement. In this case I am able to train and test as independent variables the columns of X that contain the ""bags of words"", namely the single  words in a sparse matrix.</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features = 300)
#indipendent
X = cv.fit_transform(corpus).toarray()
#dependent
y = dataset.iloc[:, 1].values
</code></pre>

<p>..and the  dependent variable y, that is represented by the column 1 that assume values as 0 and 1( so basically positive and negative review).</p>

<p>if instead of 0 and 1, I have reviews that can be voted from 1 to 5 stars should I proceed having an y variable column with values from 0 to 4?In other words I would lie to know how differ the model if instead of a binary good/bad review, the user has the possibility after his or her review to give a rating from 1 to 5.
How is called this kind of problem in machine learning?</p>
",Training and Model Evaluation,solve classification problem dependent variable two value simple nlp problem written review simple binary positive negative judgement case able train test independent variable column x contain bag word namely single word sparse matrix dependent variable represented column assume value basically positive negative review instead review voted star proceed variable column value word would lie know differ model instead binary good bad review user ha possibility review give rating called kind problem machine learning
Customizg loss function in Word2vec,"<p>I have list of co-occurences and I want to train word2vec model with my own customized loss_function. </p>

<p>What is the best way to approach this?</p>

<ol>
<li>Is it possible to set gensim Word2Vec model with my own function? </li>
<li>If not, is there an example to an implementation with keras?</li>
<li>If not, must I define everything totally from scratch?</li>
</ol>

<p>Thanks!</p>
",Training and Model Evaluation,customizg loss function word vec list co occurences want train word vec model customized loss function best way approach possible set gensim word vec model function example implementation kera must define everything totally scratch thanks
How to convert pretrained fastText vectors to gensim model,"<p>How to convert pretrained fastText vectors to gensim model?
I need predict_output_word method.</p>

<p>import gensim
from gensim.models import Word2Vec 
from gensim.models.wrappers import FastText</p>

<p>model_wiki = gensim.models.KeyedVectors.load_word2vec_format(""wiki.ru.vec"")
model3 = Word2Vec(sentences=model_wiki)</p>

<blockquote>
  <p>TypeError                                 Traceback (most recent call
  last)  in 
  ----> 1 model3 = Word2Vec(sentences=model_wiki)  # train a model from the corpus</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/word2vec.py
  in <strong>init</strong>(self, sentences, corpus_file, size, alpha, window,
  min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs,
  negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule,
  sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)
      765             callbacks=callbacks, batch_words=batch_words, trim_rule=trim_rule, sg=sg, alpha=alpha, window=window,
      766             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss,
  --> 767             fast_version=FAST_VERSION)
      768 
      769     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/base_any2vec.py
  in <strong>init</strong>(self, sentences, corpus_file, workers, vector_size,
  epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed,
  hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss,
  fast_version, **kwargs)
      757                 raise TypeError(""You can't pass a generator as the sentences argument. Try an iterator."")
      758 
  --> 759             self.build_vocab(sentences=sentences, corpus_file=corpus_file, trim_rule=trim_rule)
      760             self.train(
      761                 sentences=sentences, corpus_file=corpus_file, total_examples=self.corpus_count,</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/base_any2vec.py
  in build_vocab(self, sentences, corpus_file, update, progress_per,
  keep_raw_vocab, trim_rule, **kwargs)
      934         """"""
      935         total_words, corpus_count = self.vocabulary.scan_vocab(
  --> 936             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)
      937         self.corpus_count = corpus_count
      938         self.corpus_total_words = total_words</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/word2vec.py
  in scan_vocab(self, sentences, corpus_file, progress_per, workers,
  trim_rule)    1569             sentences = LineSentence(corpus_file)<br>
  1570 
  -> 1571         total_words, corpus_count = self._scan_vocab(sentences, progress_per, trim_rule)    1572     1573 
  logger.info(</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/word2vec.py
  in _scan_vocab(self, sentences, progress_per, trim_rule)    1538<br>
  vocab = defaultdict(int)    1539         checked_string_types = 0
  -> 1540         for sentence_no, sentence in enumerate(sentences):    1541             if not checked_string_types:    1542<br>
  if isinstance(sentence, string_types):</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/keyedvectors.py
  in <strong>getitem</strong>(self, entities)
      337             return self.get_vector(entities)
      338 
  --> 339         return vstack([self.get_vector(entity) for entity in entities])
      340 
      341     def <strong>contains</strong>(self, entity):</p>
  
  <p>TypeError: 'int' object is not iterable</p>
</blockquote>
",Training and Model Evaluation,convert pretrained fasttext vector gensim model convert pretrained fasttext vector gensim model need predict output word method import gensim gensim model import word vec gensim model wrapper import fasttext model wiki gensim model keyedvectors load word vec format wiki ru vec model word vec sentence model wiki typeerror traceback recent call last model word vec sentence model wiki train model corpus anaconda envs pym lib python site package gensim model word vec py init self sentence corpus file size alpha window min count max vocab size sample seed worker min alpha sg h negative n exponent cbow mean hashfxn iter null word trim rule sorted vocab batch word compute loss callback max final vocab callback callback batch word batch word trim rule trim rule sg sg alpha alpha window window seed seed h h negative negative cbow mean cbow mean min alpha min alpha compute loss compute loss fast version fast version def train epoch self corpus file thread id offset cython vocab thread private mem cur epoch anaconda envs pym lib python site package gensim model base vec py init self sentence corpus file worker vector size epoch callback batch word trim rule sg alpha window seed h negative n exponent cbow mean min alpha compute loss fast version kwargs raise typeerror pas generator sentence argument try iterator self build vocab sentence sentence corpus file corpus file trim rule trim rule self train sentence sentence corpus file corpus file total example self corpus count anaconda envs pym lib python site package gensim model base vec py build vocab self sentence corpus file update progress per keep raw vocab trim rule kwargs total word corpus count self vocabulary scan vocab sentence sentence corpus file corpus file progress per progress per trim rule trim rule self corpus count corpus count self corpus total word total word anaconda envs pym lib python site package gensim model word vec py scan vocab self sentence corpus file progress per worker trim rule sentence linesentence corpus file total word corpus count self scan vocab sentence progress per trim rule logger info anaconda envs pym lib python site package gensim model word vec py scan vocab self sentence progress per trim rule vocab defaultdict int checked string type sentence sentence enumerate sentence checked string type isinstance sentence string type anaconda envs pym lib python site package gensim model keyedvectors py getitem self entity return self get vector entity return vstack self get vector entity entity entity def contains self entity typeerror int object iterable
Adding metadata to words in word2vec,"<p>I have a word2vec model and I want to change it by adding some additional data beside the occurrence of the word itself.</p>

<p>For example:</p>

<p>Category (out of predefined 50), POS etc.</p>

<p>I thought of two ways to do it:</p>

<ol>
<li>Just concat the metadata to the word. (so that the word ""desk"" will be coded as ""desk-furniture-Noun""</li>
<li>The better way in my opinion: Create a new loss function that will be a function of the co-occurrences of the word itself, the co-occurrences of the category, the co-occurrences POS, etc.</li>
</ol>

<p>So my questions are:
1. What will be a better way?
2. How can I create a new loss function and optimize it in Word2Vec? Can I just pass a parameter to Gensim's Word2Vec or do I need to build a new Word2vec model from scratch?</p>
",Training and Model Evaluation,adding metadata word word vec word vec model want change adding additional data beside occurrence word example category predefined po etc thought two way concat metadata word word desk coded desk furniture noun better way opinion create new loss function function co occurrence word co occurrence category co occurrence po etc question better way create new loss function optimize word vec pas parameter gensim word vec need build new word vec model scratch
How to train spacy to recognize a label without specifying the other ones?,"<p>In order to extract the ""PERSON"" labels of some sentences, I'm training spacy with some sentences like ""John Doe likes London and Berlin"".</p>

<p>For this example, the training data would look like this :</p>

<pre><code>TRAIN_DATA = [
('John Doe likes London and Berlin.', {
    'entities': [(0, 8, 'PERSON'), (15, 21, 'LOC'), (26, 32, 'LOC')]
})]
</code></pre>

<p>But I don't want to specify the other labels like London = LOC and Berlin = Loc like I did in this example.</p>

<p>Is it possible or do I always have to specify the other labels ?</p>
",Training and Model Evaluation,train spacy recognize label without specifying one order extract person label sentence training spacy sentence like john doe like london berlin example training data would look like want specify label like london loc berlin loc like example possible always specify label
Gensim-python: Is there a simple way to get the number of times a given token arise in all documents?,"<p>My gensim model is like this:</p>

<pre><code>class MyCorpus(object):
    parametersList = []
    def __init__(self,dictionary):
       self.dictionary=dictionary
    def __iter__(self):
        #for line in open('mycorpus.txt'):
        for line in texts:
            # assume there's one document per line, tokens separated by whitespace
            yield self.dictionary.doc2bow(line[0].lower().split())




if __name__==""__main__"":
    texts=[['human human interface computer'],
             ['survey user user computer system system system response time'],
             ['eps user interface system'],
             ['system human system eps'],
             ['user response time'],
             ['trees'],
             ['graph trees'],
             ['graph minors trees'],
             ['graph minors minors survey survey survey']]


    dictionary = corpora.Dictionary(line[0].lower().split() for line in texts)

    corpus= MyCorpus(dictionary)
</code></pre>

<p>The frequency of each token in each document is automatically evaluated.</p>

<p>I also can define the tf-idf model and access the tf-idf statistic for each token in each document.</p>

<pre><code>model = TfidfModel(corpus)
</code></pre>

<p><strong>However, I have no clue how to count (memory-friendly) the number of documents that a given word arise. How can I do that?</strong> [Sure... I can use the values of tf-idf and document frequency to evaluate it... However, I would like to evaluate it directly from some counting process]</p>

<p>For instance, for the first document, I would like to get somenthing like</p>

<pre><code>[('human',2), ('interface',2), ('computer',2)]
</code></pre>

<p>since each token above arises twice in each document.</p>

<p>For the second.</p>

<pre><code>[('survey',2), ('user',3), ('computer',2),('system',3), ('response',2),('time',2)]
</code></pre>
",Training and Model Evaluation,gensim python simple way get number time given token arise document gensim model like frequency token document automatically evaluated also define tf idf model access tf idf statistic token document however clue count memory friendly number document given word arise sure use value tf idf document frequency evaluate however would like evaluate directly counting process instance first document would like get somenthing like since token arises twice document second
Extracting texts from pdf files for building a model with Gensim,"<p>I would like to train a model with Gensim using news texts from electronic newspapers (in pdf format). What is the best way to extract texts from pdf files and to process the texts ready for training? Any sample codes?</p>
",Training and Model Evaluation,extracting text pdf file building model gensim would like train model gensim using news text electronic newspaper pdf format best way extract text pdf file process text ready training sample code
How to evaluate auto summary generated with gold summaries with Rouge metric?,"<p>I'm working on a auto summarization system and I want to evaluate my output summary with my gold summaries. I have multiple summaries with different length for each case. So I'm a little confused in here. 
my question is that how should I evaluate my summary with these gold summaries. should I evaluate mine with each gold summary then average the results or assume union of gold summaries as gold summary then evaluate mine with that?</p>

<p>Thank you in advance </p>
",Training and Model Evaluation,evaluate auto summary generated gold summary rouge metric working auto summarization system want evaluate output summary gold summary multiple summary different length case little confused question evaluate summary gold summary evaluate mine gold summary average result assume union gold summary gold summary evaluate mine thank advance
Doc2Vec online training,"<p>I train my doc2vec model:</p>

<pre><code>data = [""Sentence 1"",
        ""Sentence 2"",
        ""Sentence 3"",
        ""Sentence 4""]

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags[str(i)]) 
                              for i, _d in enumerate(data)]
</code></pre>

<p>training part:</p>

<pre><code>model = Doc2Vec(size=100, window=10, min_count=1, workers=11, alpha=0.025, 
                min_alpha=0.025, iter=20)

model.build_vocab(tagged_data, update=False)

model.train(tagged_data,epochs=model.iter,total_examples=model.corpus_count)
</code></pre>

<p>Save model:</p>

<pre><code>model.save(""d2v.model"")
</code></pre>

<p>And it's work. Than I want to add some sentence to my vocabulary and model. E.x.:</p>

<pre><code>new_data = [""Sentence 5"",
            ""Sentence 6"",
            ""Sentence 7""]
new_tagged_data= 
[TaggedDocument(words=word_tokenize(_d.lower()),tags[str(i+len(data))]) 
                for i,_d in enumerate(new_data)]
</code></pre>

<p>And than update model:</p>

<pre><code>model.build_vocab(new_tagged_data, update=True)

model.train(new_tagged_data, 
            epochs=model.iter,total_examples=model.corpus_count)
</code></pre>

<p>But it doesn't work. Jupiter urgently shut down and no answer. I use the same way with word2vec model and it works!</p>

<p>What can be a problem with this?</p>
",Training and Model Evaluation,doc vec online training train doc vec model training part save model work want add sentence vocabulary model e x update model work jupiter urgently shut answer use way word vec model work problem
How to train ngram model on my own corpus,"<p>I have a corpus of list of strings:</p>

<pre><code>corpus = [""Hello I am Sam"", ""This is a white desk"",""I ate cereals"", ...]
</code></pre>

<p>I want to build a language model (preferably using nltk) on this corpus, to get the probability of a word in a sentence. 
So, my later usage will be to get</p>

<blockquote>
  <p>P(""Sam""| ""I am"")</p>
</blockquote>

<p>in this corpus.
I couldn't find - what is the best way to do so? How to train an ngram model and later get such probabilities?</p>

<p>Thanks!</p>
",Training and Model Evaluation,train ngram model corpus corpus list string want build language model preferably using nltk corpus get probability word sentence later usage get p sam corpus find best way train ngram model later get probability thanks
Make fixed timestep length LSTM Keras model free timestep length,"<p>I have a Keras LSTM multitask model that performs two tasks. One is a sequence tagging task (so I predict a label per token). The other is a global classification task over the whole sequence using a CNN that is stacked on the hidden states of the LSTM.</p>

<p>In my setup (don't ask why) I only need the CNN task during training, but the labels it predicts have no use on the final product. So, on Keras, one can train a LSTM model without especifiying the input sequence lenght. like this:</p>

<pre><code>l_input = Input(shape=(None,), dtype=""int32"", name=input_name)
</code></pre>

<p>However, if I add the CNN stacked on the LSTM hidden states I need to set a fixed sequence length for the model.</p>

<pre><code>l_input = Input(shape=(timesteps_size,), dtype=""int32"", name=input_name)
</code></pre>

<p>The problem is that once I have trained the model with a fixed timestep_size I can no longer use it to predict longer sequences.</p>

<p>In other frameworks this is not a problem. But in Keras, I cannot get rid of the CNN and change the expected input shape of the model once it has been trained.</p>

<p>Here is a simplified version of the model</p>

<pre><code>l_input = Input(shape=(timesteps_size,), dtype=""int32"")
l_embs  = Embedding(len(input.keys()), 100)(l_input)
l_blstm = Bidirectional(GRU(300, return_sequences=True))(l_embs)

# Sequential output
l_out1  = TimeDistributed(Dense(len(labels.keys()),
                                activation=""softmax""))(l_blstm)


# Global output
conv1  = Conv1D( filters=5 , kernel_size=10 )( l_embs )
conv1  = Flatten()(MaxPooling1D(pool_size=2)( conv1 ))

conv2  = Conv1D( filters=5 , kernel_size=8 )( l_embs )
conv2  = Flatten()(MaxPooling1D(pool_size=2)( conv2 ))

conv   = Concatenate()( [conv1,conv2] )
conv   = Dense(50, activation=""relu"")(conv)

l_out2 = Dense( len(global_labels.keys()) ,activation='softmax')(conv)

model  = Model(input=input, output=[l_out1, l_out2])
optimizer = Adam()

model.compile(optimizer=optimizer,
              loss=""categorical_crossentropy"",
              metrics=[""accuracy""])
</code></pre>

<p>I would like to know if anyone here has faced this issue, and if there are any solutions to delete layers from a model after training and, more important, how to reshape input layer sizes after training.</p>

<p>Thanks </p>
",Training and Model Evaluation,make fixed timestep length lstm kera model free timestep length kera lstm multitask model performs two task one sequence tagging task predict label per token global classification task whole sequence using cnn stacked hidden state lstm setup ask need cnn task training label predicts use final product kera one train lstm model without especifiying input sequence lenght like however add cnn stacked lstm hidden state need set fixed sequence length model problem trained model fixed timestep size longer use predict longer sequence framework problem kera get rid cnn change expected input shape model ha trained simplified version model would like know anyone ha faced issue solution delete layer model training important reshape input layer size training thanks
Sklearn OneVsRestClassifier - get probabilities for all possibilities of target class,"<p>I have a pipeline that performs feature engineering and model selection. </p>

<p><em>Feature engineering and model selection</em></p>

<pre><code>from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
</code></pre>

<p><em>Pipeline of feature engineering and model</em></p>

<pre><code>model = Pipeline([('vectorizer', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LinearSVC(class_weight=""balanced"")))])
</code></pre>

<p><em>Paramater selection</em></p>

<pre><code>from sklearn.model_selection import GridSearchCV
parameters = {'vectorizer__ngram_range': [(1, 1), (1, 2),(2,2)],
               'tfidf__use_idf': (True, False)}

gs_clf_svm = GridSearchCV(model, parameters, n_jobs=-1)
gs_clf_svm = gs_clf_svm.fit(X, y)
print(gs_clf_svm.best_score_)
print(gs_clf_svm.best_params_)
</code></pre>

<p><em>Preparing the final pipeline using the selected parameters</em></p>

<pre><code>model = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,2))),
    ('tfidf', TfidfTransformer(use_idf=True)),
    ('clf', OneVsRestClassifier(LinearSVC(class_weight=""balanced"")))])
</code></pre>

<p><em>Fit model with training data</em>
    model.fit(X_train, y_train)</p>

<p><em>Save the model</em></p>

<pre><code>from sklearn.externals import joblib
joblib.dump(model, 'model_question_topic.pkl', compress=1)
</code></pre>

<p>NOW in another file, I am loading model and predicting</p>

<pre><code>from sklearn.externals import joblib
model = joblib.load('model_question_topic.pkl')
</code></pre>

<p>Now it is predicting the classes properly as class 1</p>

<pre><code>question = ""apply leave""
model.predict([question])[0]
</code></pre>

<p>BUT the problem is I need the confidence rate or percentage like</p>

<blockquote>
  <p>Class1 = 0.8 -- Class2 = 0.05 -- Class3 = 0.05 -- Class4 = 0.1</p>
</blockquote>

<pre><code>model.predict_proba([question])[0]
</code></pre>

<p>How do I do this in python3?</p>
",Training and Model Evaluation,sklearn onevsrestclassifier get probability possibility target class pipeline performs feature engineering model selection feature engineering model selection pipeline feature engineering model paramater selection preparing final pipeline using selected parameter fit model training data model fit x train train save model another file loading model predicting predicting class properly class problem need confidence rate percentage like class class class class python
Getting vector obtained in the last layer of CNN before softmax layer,"<p>I am trying to implement a system by encoding inputs using CNN. After CNN, I need to get a vector and use it in another deep learning method.</p>

<pre><code>  def get_input_representation(self):
    # get word vectors from embedding
    inputs = tf.nn.embedding_lookup(self.embeddings, self.input_placeholder)


    sequence_length = inputs.shape[1] # 56
    vocabulary_size = 160 # 18765
    embedding_dim = 256
    filter_sizes = [3,4,5]
    num_filters = 3
    drop = 0.5

    epochs = 10
    batch_size = 30

    # this returns a tensor
    print(""Creating Model..."")
    inputs = Input(shape=(sequence_length,), dtype='int32')
    embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)
    reshape = Reshape((sequence_length,embedding_dim,1))(embedding)

    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)
    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)
    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)

    maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)
    maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)
    maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)

    concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])
    flatten = Flatten()(concatenated_tensor)
    dropout = Dropout(drop)(flatten)
    output = Dense(units=2, activation='softmax')(dropout)
    model = Model(inputs=inputs, outputs=output)
    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])
    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])
    print(""Traning Model..."")
    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(X_test, y_test))  # starts training


    return ??
</code></pre>

<p>The above code, trains the model using <code>X_train</code> and <code>Y_train</code> and then tests it. However in my system I do not have <code>Y_train</code> or <code>Y_test</code>, I only need the vector in the last hidden layer before softmax layer. How can I obtain it?</p>
",Training and Model Evaluation,getting vector obtained last layer cnn softmax layer trying implement system encoding input using cnn cnn need get vector use another deep learning method code train model using test however system need vector last hidden layer softmax layer obtain
RF model loses accuracy when I remove it from Pipeline,"<p>Hoping I'm overlooking something stupid here or maybe I don't understand how this is working...</p>

<p>I have an nlp pipeline that does basically the following:</p>

<pre><code>rf_pipeline = Pipeline([
('vect', TfidfVectorizer(tokenizer = spacy_tokenizer)),
('fit', RandomForestClassifier())
])
</code></pre>

<p>I run it:</p>

<pre><code>clf = rf_pipeline.fit(X_train, y_train)
preds = clf.predict(X_test)
</code></pre>

<p>When I optimize I get accuracy in the high 90's with the following:</p>

<pre><code>confusion_matrix(y_test, preds)
accuracy_score(y_test, preds)
precision_score(y_test, preds)
</code></pre>

<p>the TfidfVectorizer is the bottleneck in my computations, so I wanted to break out the pipeline. run the vectorizer, and then do a grid search on the classifier rather than running it on the whole pipeline. Here's how I broke it out:</p>

<pre><code># initialize
tfidf = TfidfVectorizer(tokenizer = spacy_tokenizer)
# transform and fit
vect = tfidf.fit_transform(X_train)
clf = rf_class.fit(vect, y_train)
# predict
clf.predict(tfidf.fit_transform(X_test))
</code></pre>

<p>When I took a look at the accuracy before I ran a full grid search it had plummeted to just over 50%. When I tried increasing the number of trees the score dropped almost 10%.</p>

<p>Any ideas?</p>
",Training and Model Evaluation,rf model loses accuracy remove pipeline hoping overlooking something stupid maybe understand working nlp pipeline doe basically following run optimize get accuracy high following tfidfvectorizer bottleneck computation wanted break pipeline run vectorizer grid search classifier rather running whole pipeline broke took look accuracy ran full grid search plummeted tried increasing number tree score dropped almost idea
Gensim worker thread stuck,"<p>I am training document embeddings on a ~20 million sentences and using parallel processing in gensim. I'm creating my model and training with the following code</p>

<pre><code>class read_corpus(object):

    def __init__(self, fname, n):
        self.fname = fname
        self.n = n

    def __iter__(self):
        num_notes = 0
        with open(self.fname, 'r') as f:
            while num_notes &lt; n:
                note = next(f)
                sentence_id, sentence = note.split('\t')

                # remove the newline character after each line and split into words
                sentence = sentence[:-1].split(' ')

                # some processing


                yield TaggedDocument(sentence, [sentence_id])
                num_notes += 1


def model(fname, vector_size, min_count,
          n_epochs, model_name,
          n, prev_model_name=None):


    data = read_corpus(fname, n)

    if prev_model_name is not None:
        model = Doc2Vec.load(prev_model_name)
    else:
        model = Doc2Vec(vector_size=vector_size,
                        min_count=min_count,
                        workers=4,
                        window=8,
                        alpha=0.1,
                        min_alpha=0.0001)

        model.build_vocab(data)

    model.train(data, total_examples=model.corpus_count, epochs=n_epochs)
    model.save(model_name)
</code></pre>

<p>After 6 - 8 epochs, the logging information shows that the training gets stuck waiting for a worker thread.
Note: the logging information says ""EPOCH 1"" because I'm training in a for loop. </p>

<p><code>... 
INFO : EPOCH 1 - PROGRESS: at 99.71% examples, 162493 words/s, in_qsize 8, out_qsize 0
INFO : EPOCH 1 - PROGRESS: at 99.81% examples, 162528 words/s, in_qsize 7, out_qsize 0
INFO : EPOCH 1 - PROGRESS: at 99.91% examples, 162560 words/s, in_qsize 7, out_qsize 0
INFO : worker thread finished; awaiting finish of 3 more threads
INFO : worker thread finished; awaiting finish of 2 more threads</code></p>

<p>It's been stuck here for several hours.</p>

<p>I had a similar output on a previous run. But the logging stopped at <code>INFO : worker thread finished; awaiting finish of 3 more threads</code></p>
",Training and Model Evaluation,gensim worker thread stuck training document embeddings million sentence using parallel processing gensim creating model training following code epoch logging information show training get stuck waiting worker thread note logging information say epoch training loop stuck several hour similar output previous run logging stopped
how to find a next word from a &#39;dataset&#39; using ngram with a given test set.,"<p><strong>given a corpus and test set.</strong>
corpus contains 10000 complete sentences. 
The test set contains 100 incomplete sentence,where each sentence has 3 consecutive words.
I want to train the corpus using ngrams and predict the next word for the Test Set.</p>

<pre><code>text = 'dataset.txt'
# Order of the grams
n = 2


ngrams = {}


words = nltk.word_tokenize(text)
for i in range(len(words)-n):
    gram = ' '.join(words[i:i+n])
    if gram not in ngrams.keys():
        ngrams[gram] = []
    ngrams[gram].append(words[i+n])


currentGram = ' '.join(words[0:n])
result = currentGram
for i in range(30):
    if currentGram not in ngrams.keys():
        break
    possibilities = ngrams[currentGram]
    nextItem = possibilities[random.randrange(len(possibilities))]
    result += ' '+nextItem
    rWords = nltk.word_tokenize(result)
    currentGram = ' '.join(rWords[len(rWords)-n:len(rWords)])
</code></pre>

<hr>

<p>test set is in .csv format
<a href=""https://i.sstatic.net/OPTUN.jpg"" rel=""nofollow noreferrer"">Top five lines of a test set</a></p>
",Training and Model Evaluation,find next word dataset using ngram given test set given corpus test set corpus contains complete sentence test set contains incomplete sentence sentence ha consecutive word want train corpus using ngrams predict next word test set test set csv format top five line test set
Unable to train model in Naive Bayes,"<p>I am trying to classify email as spam/ham using NLTK</p>

<p>Below are the steps followed :</p>

<ol>
<li><p>Trying to extract all the tokens</p></li>
<li><p>Fetching all the features</p></li>
<li><p>Extracting features from the corpus of all unique words and mapping
True/false</p></li>
<li>Training the data in Naive Bayes classifier</li>
</ol>

<hr>

<pre><code>from nltk.classify.util import apply_features
from nltk import NaiveBayesClassifier
import pandas as pd
import collections
from sklearn.model_selection import train_test_split
from collections import Counter
data = pd.read_csv('https://raw.githubusercontent.com/venkat1017/Data/master/emails.csv')

""""""fetch array of tuples where each tuple is defined by (tokenized_text, label)
""""""

processed_tokens=data['text'].apply(lambda x:([x for x in x.split() if x.isalpha()]))
processed_tokens=processed_tokens.apply(lambda x:([x for x in x if len(x)&gt;3]))

processed_tokens = [(i,j) for i,j in zip(processed_tokens,data['spam'])]



""""""
 dictword return a Set of unique words in complete corpus.
""""""

list = zip(*processed_tokens)

dictionary = Counter(word for i, j in processed_tokens for word in i)

dictword = [word for word, count in dictionary.items() if count == 1]


""""""maps each input text into feature vector""""""

y_dict = ( [ (word, True) for word in dictword] )
feature_vec=dict(y_dict)

""""""Training""""""

training_set, testing_set = train_test_split(y_dict, train_size=0.7)

classifier = NaiveBayesClassifier.train(training_set)
</code></pre>

<hr>

<pre><code>    ~\AppData\Local\Continuum\anaconda3\lib\site-packages\nltk\classify\naivebayes.py in train(cls, labeled_featuresets, estimator)
    197         for featureset, label in labeled_featuresets:
    198             label_freqdist[label] += 1
--&gt; 199             for fname, fval in featureset.items():
    200                 # Increment freq(fval|label, fname)
    201                 feature_freqdist[label, fname][fval] += 1

AttributeError: 'str' object has no attribute 'items'
</code></pre>

<p>I am facing with the following error when trying to train the corpus of unique words</p>
",Training and Model Evaluation,unable train model naive bayes trying classify email spam ham using nltk step followed trying extract token fetching feature extracting feature corpus unique word mapping true false training data naive bayes classifier facing following error trying train corpus unique word
Gensim HDP topic model: How to train on multiple passes of corpus?,"<p>Gensim's HDP model for topic modeling (gensim.models.hdpmodel.HdpModel) has a constructor that takes an argument called <code>max_chunks</code>.</p>

<p>On the documentation, it says <code>max_chunks</code> is the number of chunks the model will go over, and if that is larger than the number of chunks in supplied corpus, the training will wrap around the corpus.</p>

<p>Since I was warned by INFO logs that the likelihood function has been decreasing, I figure I may need multiple passes on corpus to converge.</p>

<p>LDA model provides with the <code>passes</code> argument the functionality to train on corpus for multiple iterations. I have difficulty figuring out how <code>max_chunks</code> in HDP maps to <code>passes</code> in LDA.</p>

<p>For example, let say my corpus has 1000000 documents. what <code>max_chunks</code> needs to be exactly in order to train, say, 3 passes on my corpus.</p>

<p>Any suggestion? Many many thanks</p>
",Training and Model Evaluation,gensim hdp topic model train multiple pass corpus gensim hdp model topic modeling gensim model hdpmodel hdpmodel ha constructor take argument called documentation say number chunk model go larger number chunk supplied corpus training wrap around corpus since wa warned info log likelihood function ha decreasing figure may need multiple pass corpus converge lda model provides argument functionality train corpus multiple iteration difficulty figuring hdp map lda example let say corpus ha document need exactly order train say pass corpus suggestion many many thanks
grade multiple responses from different users,"<p>I want to grade/score the response of different users inputs. For this I have used <code>Multinomial navie bayes</code>. The below my code.</p>

<pre><code># use natural language toolkit
import nltk
from nltk.stem.lancaster import LancasterStemmer
import os
import json
import datetime
stemmer = LancasterStemmer()   
# 3 classes of training data
training_data = []
# capture unique stemmed words in the training corpus
class_words={}
corpus_words = {}
classes = list(set([a['class'] for a in training_data]))
for c in classes:
    class_words[c] = []

for data in training_data:
    # tokenize each sentence into words
    for word in nltk.word_tokenize(data['sentence']):
        # ignore a few things
        if word not in [""?"", ""'s""]:
            # stem and lowercase each word
            stemmed_word = stemmer.stem(word.lower())
            if stemmed_word not in corpus_words:
                corpus_words[stemmed_word] = 1
            else:
                corpus_words[stemmed_word] += 1

            class_words[data['class']].extend([stemmed_word])

# we now have each word and the number of occurances of the word in our training corpus (the word's commonality)
print (""Corpus words and counts: %s"" % corpus_words)
# also we have all words in each class
print (""Class words: %s"" % class_words)
sentence=""The biggest advantages to a JavaScript having a ability to support all modern browser and produce the same result.""
def calculate_class_score(sentence, class_name):
    score = 0
    for word in nltk.word_tokenize(sentence):
        if word in class_words[class_name]:
            score += 1
    return score
for c in class_words.keys():
    print (""Class: %s  Score: %s"" % (c, calculate_class_score(sentence, c)))
# calculate a score for a given class taking into account word commonality
def calculate_class_score_commonality(sentence, class_name):
    score = 0
    for word in nltk.word_tokenize(sentence):
        if word in class_words[class_name]:
            score += (1 / corpus_words[word])
    return score
# now we can find the class with the highest score
for c in class_words.keys():
    print (""Class: %s  Score: %s"" % (c, calculate_class_score_commonality(sentence, c)))
def find_class(sentence):
    high_class = None
    high_score = 0
    for c in class_words.keys():
        score = calculate_class_score_commonality(sentence, c)
        if score &gt; high_score:
            high_class = c
            high_score = score
    return high_class, high_score
</code></pre>

<p><strong>Note:</strong> I haven't added any training data.</p>

<p>When I give the input as </p>

<pre><code>find_class(""the biggest advantages to a JavaScript having a ability to
 support all modern browser and produce the same result.JavaScript
 small bit of code you can test"")
</code></pre>

<p>I'm getting the output as </p>

<pre><code>('Advantages', 5.07037037037037)
</code></pre>

<p>But when I give the input as </p>

<pre><code>find_class(""JavaScript can be executed within the user's browser
without having to communicate with the server, saving on bandwidth"")
</code></pre>

<p>I'm getting the response/output as </p>

<pre><code>('Advantages', 2.0454545)
</code></pre>

<p>I'm building it for the JavaScript Interview/viva questions.
When a user type the same answer in the different way as I mentioned above I'm getting I'm getting different scores. I want the scores to be precise. How can I do it. </p>
",Training and Model Evaluation,grade multiple response different user want grade score response different user input used code note added training data give input getting output give input getting response output building javascript interview viva question user type answer different way mentioned getting getting different score want score precise
NLP - What to do when unigram is not present in corpus while doing stupid backoff smoothing,"<p>In stupid backoff for smoothing for trigrams, if trigram is not found then we backoff to bigram , if bigram is also not found we backoff to unigram. But what if unigram is not present in the corpus. In the <a href=""http://www.aclweb.org/anthology/D07-1090.pdf"" rel=""nofollow noreferrer"">paper</a> under stupid backoff section it is mentioned that </p>

<blockquote>
  <p>The
  recursion ends at unigrams</p>
</blockquote>

<p>So what probability should be assigned to a completely new unigram, which is not present in training dataset.</p>
",Training and Model Evaluation,nlp unigram present corpus stupid backoff smoothing stupid backoff smoothing trigram trigram found backoff bigram bigram also found backoff unigram unigram present corpus paper stupid backoff section mentioned recursion end unigrams probability assigned completely new unigram present training dataset
How to use Parts-of-Speech to evaluate semantic text similarity?,"<p>I'm trying to write a program to evaluate semantic similarity between texts. I have already compared n-gram frequencies between texts (a lexical measure). I wanted something a bit less shallow than this, and I figured that looking at similarity in sentence construction would be one way to evaluate text similarity.</p>

<p>However, all I can figure out how to do is to count the POS (for example, 4 nouns per text, 2 verbs, etc.). This is then similar to just counting n-grams (and actually works less well than the ngrams).</p>

<pre><code>postags = nltk.pos_tag(tokens)
self.pos_freq_dist = Counter(tag for word,tag in postags)
for pos, freq in self.pos_freq_dist.iteritems():
    self.pos_freq_dist_relative[pos] = freq/self.token_count    #normalise pos freq by token counts             
</code></pre>

<p>Lots of people (Pearsons, ETS Research, IBM, academics, etc.) use Parts-of-Speech for deeper measures, but no one says how they have done it. How can Parts-of-Speech be used for a 'deeper' measure of semantic text similarity?</p>
",Training and Model Evaluation,use part speech evaluate semantic text similarity trying write program evaluate semantic similarity text already compared n gram frequency text lexical measure wanted something bit le shallow figured looking similarity sentence construction would one way evaluate text similarity however figure count po example noun per text verb etc similar counting n gram actually work le well ngrams lot people pearsons ets research ibm academic etc use part speech deeper measure one say done part speech used deeper measure semantic text similarity
Find words and combinations of words that can be spoken the quickest,"<p>I'm a big fan of discovering sentences that can be rapped very quickly.  For example, ""gotta read a little bit of Wikipedia"" or ""don't wanna wind up in the gutter with a bottle of malt."" (George Watsky) </p>

<p>I wanted to write a program in Python that would enable me to find words (or combinations of words) that can be articulated such that it sounds very fast when spoken.  </p>

<p>I initially thought that words that had a high syllable to letter ratio would be the best, but upon writing a Python program to do find those words, I retrieved only very simple words that didn't really sound fast (e.g. ""iowa"").  </p>

<p>So I'm at a loss at what actually makes words sound fast.  Is it the morpheme to letter ratio?  Is it the number of alternating vowel-consonant pairs?  </p>

<p>How would you guys go about devising a python program to resolve this problem?</p>
",Training and Model Evaluation,find word combination word spoken quickest big fan discovering sentence rapped quickly example got ta read little bit wikipedia wan na wind gutter bottle malt george watsky wanted write program python would enable find word combination word articulated sound fast spoken initially thought word high syllable letter ratio would best upon writing python program find word retrieved simple word really sound fast e g iowa loss actually make word sound fast morpheme letter ratio number alternating vowel consonant pair would guy go devising python program resolve problem
Is Word2vec memory and time consuming?,"<p>I am trying to train a Word2Vec model using neural Networks. My question is as follows:</p>

<p>Correct me if I am wrong: word2vec uses as input text, which doesn't have the same order of magnitude as images ( with respect to memory) ? Does this imply that there is no need to use GPU for training a word2vec model , and by the way a 64Go Virtual cloud machine is enough to make training? Text uses for training can't exced 5-10Go?</p>
",Training and Model Evaluation,word vec memory time consuming trying train word vec model using neural network question follows correct wrong word vec us input text order magnitude image respect memory doe imply need use gpu training word vec model way go virtual cloud machine enough make training text us training exced go
Using RNN tensorflow language model to predict the probabilities of test sentences,"<p>I was able to train a language model using the <a href=""https://www.tensorflow.org/versions/master/tutorials/recurrent/index.html#language-modeling"" rel=""nofollow noreferrer"">tensorflow tutorials</a> , the models are saved as checkpoint files as per the <a href=""https://stackoverflow.com/questions/33980496/tensorflow-rnn-model-path/33981028?noredirect=1#comment55719523_33981028"">code given here</a>.</p>

<pre><code>save_path = saver.save(sess, ""/tmp/model.epoch.%03d.ckpt"" % (i + 1))
</code></pre>

<p>Now I need to restore the checkpoint and use it in the following code:</p>

<pre><code>    def run_epoch(session, m, data, eval_op, verbose=False):
  """"""Runs the model on the given data.""""""
  epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps
  start_time = time.time()
  costs = 0.0
  iters = 0
  state = m.initial_state.eval()
  for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size,
                                                    m.num_steps)):
    cost, state, _ = session.run([m.cost, m.final_state, eval_op],
                                 {m.input_data: x,
                                  m.targets: y,
                                  m.initial_state: state})
    costs += cost
    iters += m.num_steps

    if verbose and step % (epoch_size // 10) == 10:
      print(""%.3f perplexity: %.3f speed: %.0f wps"" %
            (step * 1.0 / epoch_size, np.exp(costs / iters),
             iters * m.batch_size / (time.time() - start_time)))

  return np.exp(costs / iters)
</code></pre>

<p>I cannot find any way of encoding the test sentences and getting sentence probability output from the trained checkpoint model.</p>

<p>The tutorials mention following code:</p>

<pre><code> probabilities = tf.nn.softmax(logits)
</code></pre>

<p>but that it is for training and I cannot figure out how do I get the actual probabilities.
I should Ideally get something like :</p>

<pre><code>&gt;&gt;getprob('this is a temp sentence')
&gt;&gt;0.322
</code></pre>
",Training and Model Evaluation,using rnn tensorflow language model predict probability test sentence wa able train language model using tensorflow tutorial model saved checkpoint file per href given need restore checkpoint use following code find way encoding test sentence getting sentence probability output trained checkpoint model tutorial mention following code training figure get actual probability ideally get something like
Multinomial Naive Bayes + neg_log_loss + Machine Learning + Python : How to use neg_log_loss with cross_val_score(),"<p>I am finding the optimal value of hyperparameter alpha for my Multinpmial Naive Bayes model which uses cross validation and neg_log_loss as metric. I wrote thie code:</p>

<pre><code>alphas = list(range(1, 500))

#perform k fold cross validation for different metrics
def cross_val(metric):

    MSE = []
    cv_scores = []
    training_scores = []

    for alpha in alphas:
        naive_bayes = MultinomialNB(alpha=alpha)
        scores = cross_val_score(naive_bayes, x_train_counts, y_train, cv=20, scoring='neg_log_loss')                           

        #score() returns the mean accuracy on the given test data and labels
        scores_training = naive_bayes.fit(x_train_counts, y_train).score(x_train_counts, y_train)

        cv_scores.append(scores.mean())
        training_scores.append(scores_training)


    #changing to misclassification error
    MSE = [1 - x for x in cv_scores]  

    #determining best alpha
    optimal_alpha = alphas[MSE.index(min(MSE))]
    print('\nThe optimal value of alpha for %s is %f' % (metric, optimal_alpha))
    return optimal_alpha


optimal_alpha = cross_val('neg_log_loss')   
</code></pre>

<p>The above code was initially working. Now it's throwing following error:</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-43-facbaa3537ca&gt; in &lt;module&gt;()
----&gt; 1 optimal_alpha = cross_val('neg_log_loss')
      2 prediction(optimal_alpha, 'neg_log_loss')

&lt;ipython-input-41-ff0a9191d45c&gt; in cross_val(metric)
     13     for alpha in alphas:
     14         naive_bayes = MultinomialNB(alpha=alpha)
---&gt; 15         scores = cross_val_score(naive_bayes, x_train_counts, y_train, cv=20, scoring='neg_log_loss')
     16 
     17         #score() returns the mean accuracy on the given test data and labels

~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/cross_validation.py in cross_val_score(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)
   1579                                               train, test, verbose, None,
   1580                                               fit_params)
-&gt; 1581                       for train, test in cv)
   1582     return np.array(scores)[:, 0]
   1583 

~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py in __call__(self, iterable)
    777             # was dispatched. In particular this covers the edge
    778             # case of Parallel used with an exhausted iterator.
--&gt; 779             while self.dispatch_one_batch(iterator):
    780                 self._iterating = True
    781             else:

~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self, iterator)
    623                 return False
    624             else:
--&gt; 625                 self._dispatch(tasks)
    626                 return True
    627 

~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self, batch)
    586         dispatch_timestamp = time.time()
    587         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
--&gt; 588         job = self._backend.apply_async(batch, callback=cb)
    589         self._jobs.append(job)
    590 

~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self, func, callback)
    109     def apply_async(self, func, callback=None):
    110         """"""Schedule a func to be run""""""
--&gt; 111         result = ImmediateResult(func)
    112         if callback:
    113             callback(result)

~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py in __init__(self, batch)
    330         # Don't delay the application, to avoid keeping the input
    331         # arguments in memory
--&gt; 332         self.results = batch()
    333 
    334     def get(self):

~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py in __call__(self)
    129 
    130     def __call__(self):
--&gt; 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
    132 
    133     def __len__(self):

~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py in &lt;listcomp&gt;(.0)
    129 
    130     def __call__(self):
--&gt; 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
    132 
    133     def __len__(self):

~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/cross_validation.py in _fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)
   1692 
   1693     else:
-&gt; 1694         test_score = _score(estimator, X_test, y_test, scorer)
   1695         if return_train_score:
   1696             train_score = _score(estimator, X_train, y_train, scorer)

~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/cross_validation.py in _score(estimator, X_test, y_test, scorer)
   1749         score = scorer(estimator, X_test)
   1750     else:
-&gt; 1751         score = scorer(estimator, X_test, y_test)
   1752     if hasattr(score, 'item'):
   1753         try:

~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/metrics/scorer.py in __call__(self, clf, X, y, sample_weight)
    142                                                  **self._kwargs)
    143         else:
--&gt; 144             return self._sign * self._score_func(y, y_pred, **self._kwargs)
    145 
    146     def _factory_args(self):

~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/metrics/classification.py in log_loss(y_true, y_pred, eps, normalize, sample_weight, labels)
   1684                              ""y_true: {2}"".format(transformed_labels.shape[1],
   1685                                                   y_pred.shape[1],
-&gt; 1686                                                   lb.classes_))
   1687         else:
   1688             raise ValueError('The number of classes in labels is different '

ValueError: y_true and y_pred contain different number of classes 26, 27. Please provide the true labels explicitly through the labels argument. Classes found in y_true: [ 2  4  5  6  7  8  9 10 11 12 14 15 16 17 19 21 22 23 24 27 29 30 31 32
 33 35]
</code></pre>

<p>This code worked few times initially. Suddenly, it stopped working. How can I make it work?</p>
",Training and Model Evaluation,multinomial naive bayes neg log loss machine learning python use neg log loss cross val score finding optimal value hyperparameter alpha multinpmial naive bayes model us cross validation neg log loss metric wrote thie code code wa initially working throwing following error code worked time initially suddenly stopped working make work
Cannot align graph because multiple tag doc2vec returning more items in doctag_syn0 than there are in the training data,"<p>I am training a doc2vec model with multiple tags, so it includes the typical doc ""ID"" tag and then it also contains a label tag ""Category 1."" I'm trying to graph the results such that I get the doc distribution in a 2d (using LargeVis) but am able to color different tags. My problem is that the vectors the model returns exceed the number of training observations by 5 making difficult to align the original tags with the vectors: </p>

<pre><code>In[1]: data.shape 
Out[1]: (17717,5)
</code></pre>

<p>Training the model on 100 parameters  </p>

<pre><code>In[2]: model.docvecs.doctag_syn0.shape
Out[2]: (17722,100) 
</code></pre>

<p>I have no idea whether the 5 additional observations shift the order of the vectors or whether they're just appended to the end. I want to avoid using string tags for the doc IDs because I am preparing this code to use on a much larger dataset.
I found an explanation in a google group <a href=""https://groups.google.com/forum/#!topic/gensim/OdvQkwuADl0"" rel=""nofollow noreferrer"">https://groups.google.com/forum/#!topic/gensim/OdvQkwuADl0</a>
which explained that using multiple tags per doc can result in this type of output. However, I haven't been able to find a way to avoid or correct it in any forum or documentation. </p>
",Training and Model Evaluation,align graph multiple tag doc vec returning item doctag syn training data training doc vec model multiple tag includes typical doc id tag also contains label tag category trying graph result get doc distribution using largevis able color different tag problem vector model return exceed number training observation making difficult align original tag vector training model parameter idea whether additional observation shift order vector whether appended end want avoid using string tag doc id preparing code use much larger dataset found explanation google group explained using multiple tag per doc result type output however able find way avoid correct forum documentation
"Text classification + NLP + Python : Warning: The least populated class in y has only 23 members, which is too few","<p>I am working on a text classification problem for which I am using 30 fold cross validation. Before starting experiment I made sure that each class had at least 30 members. Then I did necessary text processing and split my dataset into test and train sets.</p>

<pre><code>x_train, x_test, y_train, y_test = cross_validation.train_test_split(data['event_name_description'], data['category_id'], test_size=0.2, random_state=42)
</code></pre>

<p>Test set consists of 20% of total data. Now when I run my model for training, I get this warning:</p>

<pre><code>/home/hp/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 23 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=30.
</code></pre>

<p>Apparently, it seems <strong>after splitting my data into test set and train set</strong>, I have at least one class in my <strong>train set</strong> which has as few as 23 members. Am I correct?</p>
",Training and Model Evaluation,text classification nlp python warning least populated class ha member working text classification problem using fold cross validation starting experiment made sure class least member necessary text processing split dataset test train set test set consists total data run model training get warning apparently seems splitting data test set train set least one class train set ha member correct
What is the meaning of negative empirical Likelihood in HLDA Mallet?,"<p>I am using mallet to train a hierarchical LDA model. However when calculating the empirical Likelihood using:</p>

<pre><code>double empiricalLikelihood = hlda.empiricalLikelihood(1000, testing);
</code></pre>

<p>I am getting a negative number.
How can I interpret the meaning of that negative number?</p>

<p>Regards</p>
",Training and Model Evaluation,meaning negative empirical likelihood hlda mallet using mallet train hierarchical lda model however calculating empirical likelihood using getting negative number interpret meaning negative number regard
Improve detecting words like &quot;she&quot; and &quot;her&quot; from sentences and return &quot;Female&quot; as a result,"<p>I have a variable ""bio_sentences"" and as the name of the variable suggests, it has four to five bio sentences of individuals (extracted and split into sentences from ""bio"" variable). I am trying to determine what gender an individual is using this logic...</p>

<pre><code>Femalew &lt;- c(""She"", ""Her"")
Check &lt;- str_extract_all(bio,Femalew)
Check &lt;- Check[Check != ""character(0)""]
Gender &lt;- vector(""character"")
if(length(Check) &gt; 0){
  Gender[1] &lt;- ""Female""
}else{
  Gender[1] &lt;- ""Male""
}
for(i in 1:length(bio_sentences)){
  Gender[i] &lt;- Gender[1]
} 
</code></pre>

<p>I am getting a good result (majority in my dataset are male), however there are few misses (some females aren't detected) despite the fact the sentences have ""she"" or ""her"" in them. Is there anyway, I can improve the accuracy of the logic or deploy some new function like grepl?</p>

<p>EDIT:</p>

<pre><code>    data1.Gender    A B C D E   data1.Description
1   Female  0   0   0   0   0   Ranjit Singh President of Boparan Holdings Limited Ranjit is President of Boparan Holdings Limited.
2   Female  0   0   0   NA  NA  He founded the business in 1993 and has more than 25 years’ experience in the food industry.
3   Female  0   0   0   NA  NA  Ranjit is particularly skilled at growing businesses, both organically and through acquisition.
4   Female  0   0   0   NA  NA  Notable acquisitions include Northern Foods and Brookes Avana in 2011.
5   Female  0   0   0   NA  NA  Ranjit and his wife Baljinder Boparan are the sole shareholders of Boparan Holdings, the holding company for 2 Sisters Food Group.
6   Female  0   0   0   NA  NA  s
</code></pre>

<p>The above is a person from the data, My requirement is that the code reads all the lines in the ""data1.description"" (in my code this is in a for loop, so it reads all sentences for each individual) and as you can see the person is Male and there is clearly a ""He"" in one of the sentences, however I get it as ""Female"" by applying the above logic I have written before.</p>
",Training and Model Evaluation,improve detecting word like sentence return female result variable bio sentence name variable suggests ha four five bio sentence individual extracted split sentence bio variable trying determine gender individual using logic getting good result majority dataset male however miss female detected despite fact sentence anyway improve accuracy logic deploy new function like grepl edit person data requirement code read line data description code loop read sentence individual see person male clearly one sentence however get female applying logic written
Causal Sentences Extraction Using NLTK python,"<p>I am extracting causal sentences from the accident reports on water. I am using NLTK as a tool here. I manually created my regExp grammar by taking 20 causal sentence structures [see examples below]. The constructed grammar is of the type </p>

<pre><code>grammar = r'''Cause: {&lt;DT|IN|JJ&gt;?&lt;NN.*|PRP|EX&gt;&lt;VBD&gt;&lt;NN.*|PRP|VBD&gt;?&lt;.*&gt;+&lt;VBD|VBN&gt;?&lt;.*&gt;+}'''
</code></pre>

<p>Now the grammar has 100% recall on the test set ( I built my own toy dataset with 50 causal and 50 non causal sentences) but a low precision. I would like to ask about: </p>

<ol>
<li>How to train NLTK to build the regexp grammar automatically for
extracting particular type of sentences.</li>
<li><p>Has any one ever tried to extract causal sentences. Example
causal sentences are:  </p>

<ul>
<li><p>There was poor sanitation in the village, as a consequence, she had
health problems.</p></li>
<li><p>The water was impure in her village, For this reason, she suffered
from parasites.</p></li>
<li><p>She had health problems because of poor sanitation in the village.
I would want to extract only the above  type of sentences from a
large text.</p></li>
</ul></li>
</ol>
",Training and Model Evaluation,causal sentence extraction using nltk python extracting causal sentence accident report water using nltk tool manually created regexp grammar taking causal sentence structure see example constructed grammar type grammar ha recall test set built toy dataset causal non causal sentence low precision would like ask train nltk build regexp grammar automatically extracting particular type sentence ha one ever tried extract causal sentence example causal sentence wa poor sanitation village consequence health problem water wa impure village reason suffered parasite health problem poor sanitation village would want extract type sentence large text
Why Gensim most similar in doc2vec gives the same vector as the output?,"<p>I am using the following code to get the ordered list of user posts.</p>

<pre><code>model = doc2vec.Doc2Vec.load(doc2vec_model_name)
doc_vectors = model.docvecs.doctag_syn0
doc_tags = model.docvecs.offset2doctag

for w, sim in model.docvecs.most_similar(positive=[model.infer_vector('phone_comments')], topn=4000):
        print(w, sim)
        fw.write(w)
        fw.write("" ("")
        fw.write(str(sim))
        fw.write("")"")
        fw.write(""\n"")

fw.close()
</code></pre>

<p>However, I am also getting the vector <code>""phone comments""</code> (that I use to find nearest neighbours) in like 6th place of the list. Is there any mistake I do in the code? or is it a issue in Gensim (becuase the vector cannot be a neighbour of itself)?</p>

<p><strong>EDIT</strong></p>

<p>Doc2vec model training code</p>

<pre><code>######Preprocessing
docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')
for key, value in my_d.items():
    value = re.sub(""[^1-9a-zA-Z]"","" "", value)
    words = value.lower().split()
    tags = key.replace(' ', '_')
    docs.append(analyzedDocument(words, tags.split(' ')))

sentences = []  # Initialize an empty list of sentences
######Get n-grams
#Get list of lists of tokenised words. 1 sentence = 1 list
for item in docs:
    sentences.append(item.words)

#identify bigrams and trigrams (trigram_sentences_project) 
trigram_sentences_project = []
bigram = Phrases(sentences, min_count=5, delimiter=b' ')
trigram = Phrases(bigram[sentences], min_count=5, delimiter=b' ')

for sent in sentences:
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigram[sent]]
    trigram_sentences_project.append(trigrams_)

paper_count = 0
for item in trigram_sentences_project:
    docs[paper_count] = docs[paper_count]._replace(words=item)
    paper_count = paper_count+1

# Train model
model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 5, workers = 4, iter = 20)

#Save the trained model for later use to take the similarity values
model_name = user_defined_doc2vec_model_name
model.save(model_name)
</code></pre>
",Training and Model Evaluation,gensim similar doc vec give vector output using following code get ordered list user post however also getting vector use find nearest neighbour like th place list mistake code issue gensim becuase vector neighbour edit doc vec model training code
Anomaly detection in Text Classification,"<p>I have built a text classifier using OneClassSVM.</p>

<p>I have the training set which corresponds to only one label i.e(""Yes"") and I don't have the other(""NO"") label data. My task is to build a classifier which classifies the new unseen sentence(test data) as 1 if it is very similar to the training data. Else, it classifies as -1 i.e,(anomaly).</p>

<p>I have used Word2Vec to build the word embeddings for my training data. Then, I am using word-vector averaging with OneClassSVM to build a anomaly detector classifier.</p>

<p>This classifier is currently giving accuracy of about 50%-55%. I have to enhance this further to build a robust classifier.</p>

<p>Any suggestions to this problem would be helpful...</p>
",Training and Model Evaluation,anomaly detection text classification built text classifier using oneclasssvm training set corresponds one label e yes label data task build classifier classifies new unseen sentence test data similar training data else classifies e anomaly used word vec build word embeddings training data using word vector averaging oneclasssvm build anomaly detector classifier classifier currently giving accuracy enhance build robust classifier suggestion problem would helpful
How do we analyse a loss vs epochs graph?,"<p>I'm training a language model and the loss vs epochs is plotted each time of training. I'm attaching two samples from it. </p>

<p><a href=""https://i.sstatic.net/LZp7x.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/LZp7x.png"" alt=""image 1""></a></p>

<p><a href=""https://i.sstatic.net/DJrhv.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/DJrhv.png"" alt=""image 2""></a></p>

<p>Obviously, the second one is showing better performance. But, from these graphs, when do we take a decision to stop training (early stopping)?</p>

<p>Can we understand overfitting and underfitting from these graphs or do I need to plot additional learning curves?  </p>

<p>What are the additional inferences that can be made from these plots? </p>
",Training and Model Evaluation,analyse loss v epoch graph training language model loss v epoch plotted time training attaching two sample obviously second one showing better performance graph take decision stop training early stopping understand overfitting underfitting graph need plot additional learning curve additional inference made plot
Wrong length for Gensim Word2Vec&#39;s vocabulary,"<p>I am trying to train the <code>Gensim Word2Vec</code> model by:</p>

<pre><code>X = train['text']    

model_word2vec = models.Word2Vec(X.values, size=150)
model_word2vec.train(X.values, total_examples=len(X.values), epochs=10)
</code></pre>

<p>after the training, I get a small vocabulary (<code>model_word2vec.wv.vocab</code>) of length <code>74</code> containing only the alphabet's letters.</p>

<p>How could I get the right vocabulary?</p>

<p><strong>Update</strong></p>

<p>I tried this before:</p>

<pre><code>tokenizer = Tokenizer(lower=True)
tokenized_text = tokenizer.fit_on_texts(X)
sequence = tokenizer.texts_to_sequences(X)

model_word2vec.train(sequence, total_examples=len(X.values), epochs=10
</code></pre>

<p>but I got the same wrong vocabulary size.</p>
",Training and Model Evaluation,wrong length gensim word vec vocabulary trying train model training get small vocabulary length containing alphabet letter could get right vocabulary update tried got wrong vocabulary size
Using Doc2Vec to find salience score for resumes based on job description,"<p>Here is my use case: </p>

<p>HR department provide <code>job description</code>(free text) and set of <code>resumes</code>(plain text), and the ask is to come up with salience score based on job description relevance.</p>

<p>The <code>job description</code> consists of skills required and minimum qualification. I was considering <strong>Doc2Vec</strong> but bit confused <strong>how should I train the model</strong>? </p>

<ol>
<li>If I collate all job descriptions, and create corpus, querying profile text will give incorrect results. </li>
<li>Moreover, job requisitions are transient in nature and a profile might match with a expired requisition.</li>
</ol>

<p>Since each job description is exclusive, shall I create a trained model for each job? Or if there's any better framework, please advise.</p>

<p>Please see the code below:</p>

<pre><code>'
import os
from gensim import corpora, models, similarities

path = &lt;working_dir&gt;

os.chdir(path)

with open('Dir int Strategy.txt') as f:
    job_desc_text = f.read()

with open('Jeannine.txt') as f:
    candidate1_text = f.read()

with open('Penny.txt') as t:
    candidate2_text = t.read()

with open('Omar.txt') as z:
    candidate3_text = z.read()


with open('Kyle.txt') as p:
    candidate4_text = p.read()

documents = [candidate1_text, candidate2_text, candidate3_text,candidate4_text]   
stoplist = set('for a of the and to in'.split())   
documents_split_texts = [[word for word in document.lower().split() if  word not in stoplist]
    for document in documents]   
dictionary = corpora.Dictionary(documents_split_texts)   
dictionary.save('/tmp/deerwester.dict')        

corpus = [dictionary.doc2bow(text) for text in documents_split_texts]   
corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)     

tfidf = models.TfidfModel(corpus)      

query_vector = job_desc_text    
query_vector = dictionary.doc2bow(query_vector.lower().split())

index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=699)     

sims = index[tfidf[query_vector]]            
'
</code></pre>
",Training and Model Evaluation,using doc vec find salience score resume based job description use case hr department provide free text set plain text ask come salience score based job description relevance consists skill required minimum qualification wa considering doc vec bit confused train model collate job description create corpus querying profile text give incorrect result moreover job requisition transient nature profile might match expired requisition since job description exclusive shall create trained model job better framework please advise please see code
remove intent ranking from output of a rasa model,"<p>i have developed a rasa intent clasification model which is showing correct intents and entities from training data but in addition it is also showing intent ranking with all the other intents and i dont want that to be shown can anyone help me in removing that from my output thanks for the help.......
code for the model is.......</p>

<pre><code>from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

# from rasa_nlu.converters import load_data
from rasa_nlu.training_data import load_data

from rasa_nlu.config import RasaNLUModelConfig
#from rasa_nlu.config import RasaNLUConfig
from rasa_nlu.model import Trainer, Metadata, Interpreter
from rasa_nlu import config

def train (data, config_file, model_dir):
     training_data = load_data(data)
     configuration = config.load(config_file)
     trainer = Trainer(configuration)
     trainer.train(training_data)
     model_directory = trainer.persist(model_dir, fixed_model_name = 'chat')

def run():
    interpreter = Interpreter.load('./models/nlu/default/chat')
    print(interpreter.parse('buy a pendrive from amazon'))
    #print(interpreter.parse(u'What is the reivew for the movie Die Hard?'))

if __name__ == '__main__':
     #train('./data/testData.json', './config/config.yml', './models/nlu')
     run()
</code></pre>

<p>for tainning the data remove comment before train and comment run() and to for running vice versa
<a href=""https://i.sstatic.net/N370B.png"" rel=""nofollow noreferrer"">output after running</a></p>

<p>just want to remove intent ranking</p>
",Training and Model Evaluation,remove intent ranking output rasa model developed rasa intent clasification model showing correct intent entity training data addition also showing intent ranking intent dont want shown anyone help removing output thanks help code model tainning data remove comment train comment run running vice versa output running want remove intent ranking
Training multiclass NN in Keras using binary cross-entropy gives higher score than using categorical cross-entropy,"<p>By mistake, I used the binary cross-entropy loss function instead of categorical cross-entropy when training my NN for predicting Named Entities in text. I have a multiclass problem with 3 classes, which is heavily imbalanced. The model is developed using Keras.</p>

<p>I evaluated my model using the evaluation script for CoNLL-2002 (not Keras build-in evaluation methods). However, I achieve higher scores using binary cross-entropy than using categorical cross-entropy and can't understand why? Any suggestions would be helpful :) </p>

<p>Thanks! </p>
",Training and Model Evaluation,training multiclass nn kera using binary cross entropy give higher score using categorical cross entropy mistake used binary cross entropy loss function instead categorical cross entropy training nn predicting named entity text multiclass problem class heavily imbalanced model developed using kera evaluated model using evaluation script conll kera build evaluation method however achieve higher score using binary cross entropy using categorical cross entropy understand suggestion would helpful thanks
How to use Ontonotes LDC corpus with Stanford Parser,"<p>How different is the format for ontonotes from penn treebank? Could I train a model of ontonotes on the Stanford Parser? If not, how could develop a ontonotes tree parser for the Stanford Parser.</p>
",Training and Model Evaluation,use ontonotes ldc corpus stanford parser different format ontonotes penn treebank could train model ontonotes stanford parser could develop ontonotes tree parser stanford parser
Using the CRFBiasedClassifier,"<p>I came across a class in Stanford's CoreNLP, the CRFBiasedClassifier. I'm wondering if there is any documentation on its use.</p>

<p>What I am trying to do is bias my classifier to be able to pick up more entities of my label A. After setting my <code>classBias</code> property to A:100, I am unable to get entities of label A picked up.</p>

<p>I have three labels: A, B &amp; C
Currently, as I am checking it out, A has 40 occurrences, B has 70 and C has 100.</p>

<p>I am serializing the classifier as below:</p>

<pre><code>SeqClassifierFlags flags = new SeqClassifierFlags(props);
CRFBiasedClassifier&lt;CoreLabel&gt; crf = new CRFBiasedClassifier&lt;&gt;(flags);
crf.train();

crf.serializeClassifier(modelOutPath);
</code></pre>

<p>And deserializing like this:</p>

<pre><code>CRFClassifier&lt;CoreLabel&gt; model = CRFClassifier.getClassifier(MODELFILE);
</code></pre>

<p>If I try</p>

<pre><code>CRFBiasedClassifier&lt;CoreLabel&gt; model = (CRFBiasedClassifier&lt;CoreLabel&gt;)CRFClassifier.getClassifier(MODELFILE);
</code></pre>

<p>I get</p>

<pre><code>java.lang.ClassCastException: edu.stanford.nlp.ie.crf.CRFClassifier cannot be cast to edu.stanford.nlp.ie.crf.CRFBiasedClassifier
</code></pre>

<p>Is there a better way to improve recall rate of my entities?</p>
",Training and Model Evaluation,using crfbiasedclassifier came across class stanford corenlp crfbiasedclassifier wondering documentation use trying bias classifier able pick entity label setting property unable get entity label picked three label b c currently checking ha occurrence b ha c ha serializing classifier deserializing like try get better way improve recall rate entity
"Accuracy testing a word2vec model using COSADD, COSMUL and euclidean distance","<p>I have trained a model in word2vec and want to use googles analogy test set to test its accuracy. I want to use COSADD, COSMUL and hopefully euclidean distance. </p>

<p>To use COSADD i simply use the code:
model.wv.accuracy(‘questions-words.txt’).</p>

<p>I’m not sure how to use the others. The accuracy method has the following optional parameters
accuracy(.txt file, restrict_vocab=..., most_similar=...)</p>

<p>where I feel like I should be able to write most_similar = COSMUL</p>

<p>but this does not work :( </p>

<p>Does anyone know how to do the accuracy test with COSMUL or euclidean distance (or both)?</p>
",Training and Model Evaluation,accuracy testing word vec model using cosadd cosmul euclidean distance trained model word vec want use google analogy test set test accuracy want use cosadd cosmul hopefully euclidean distance use cosadd simply use code model wv accuracy question word txt sure use others accuracy method ha following optional parameter accuracy txt file restrict vocab similar feel like able write similar cosmul doe work doe anyone know accuracy test cosmul euclidean distance
Mnemonic Generation Using LSTM&#39;s | How do I make sure my Model Generates Meaningful Sentence Using a Loss Function?,"<p>I'm working on project that generates mnemonics. I have a problem with my Model.</p>

<h1>My question is ,How do I make sure my Model Generates Meaningful Sentence Using a Loss Function?</h1>

<h1><strong>Aim of the project</strong></h1>

<p>To generate Mnemonics for a list of words. Given a list of words user wants to remember, the model will Output a meaningful, simple and easy to remember sentence which encapsulates the one-two first letters of the words that the user wants to remember in the words of Mnemonic to be generated. My model will receive only the first two letters of the words the user wants to remember as that is it carries all the information for the mnemonic to be generated.
<a href=""https://i.sstatic.net/OPSW4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OPSW4.jpg"" alt=""enter image description here""></a></p>

<h1>Dataset</h1>

<p>I’m Using Kaggle’s 55000+ song lyrics data and the sentences in those lyrics contain 5 to 10 words and Mnemonic I want to generate also contain the same number of words.</p>

<h1>Input/Output Preprocessing.</h1>

<p>I am iterating through all the sentences after removing punctuation and numbers and extracting first 2 letters from each word in a sentence and assigning a unique number to those pair of letters from a predefined dictionary which contains pairs of keys a key and a unique number as value. 
List of these unique number assigned while act as input and Glove vectors of those words will act as the output. At each time step LSTM model will take these unique numbers assigned to these words and will output the corresponding word’s GloVe vector.</p>

<p><a href=""https://i.sstatic.net/VcAsL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VcAsL.jpg"" alt=""enter image description here""></a></p>

<h1>Model Architecture</h1>

<p>I'm using LSTM's with 10 time steps.
At each time step the unique number associated with the pair of letters will be fed and the output will be the GloVe vector of the corresponding word.</p>

<p><a href=""https://i.sstatic.net/BqVXj.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BqVXj.jpg"" alt=""enter image description here""></a></p>

<pre><code>optimizer=rmsprop(lr=0.0008)
model=Sequential()
model.add(Embedding(input_dim=733,output_dim=40,input_length=12))
model.add(Bidirectional(LSTM(250,return_sequences=True,activation='relu'),merge_mode='concat'))
Dropout(0.4)

model.add(Bidirectional(LSTM(350,return_sequences=True,activation='relu'),merge_mode='concat'))
Dropout(0.4)
model.add(TimeDistributed(Dense(332, activation='tanh')))
model.compile(loss='cosine_proximity',optimizer=optimizer,metrics=['acc'])
</code></pre>

<h1>Results:</h1>

<p>My model is outputting Mnemonics which match the first two letters of each word in the input. But the mnemonic generated carries little to no meaning.
I have realized this problem is caused because of the way I’m training. The order of letter extracts from words is already ready for sentence formation. But this is not the same in case of while testing. The order with which I’m feeding the letter extracts of words may not have a high probability of sentence formation.
So I built a bigram for my data and feed that permutation that had the highest probability of sentence formation into my mnemonic generator model. Though there were some improvements, the sentence as a whole didn’t make any sense.
I’m stuck at this point.</p>

<h3>Input</h3>

<p><a href=""https://i.sstatic.net/Dn4gj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Dn4gj.png"" alt=""Input""></a></p>

<h3>Output</h3>

<p><a href=""https://i.sstatic.net/WrD2x.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WrD2x.png"" alt=""Output""></a></p>

<h1>My question is,</h1>

<h1>How do I make sure my Model Generates Meaningful Sentence? Using a Loss Function</h1>
",Training and Model Evaluation,mnemonic generation using lstm make sure model generates meaningful sentence using loss function working project generates mnemonic problem model question make sure model generates meaningful sentence using loss function aim project generate mnemonic list word given list word user want remember model output meaningful simple easy remember sentence encapsulates one two first letter word user want remember word mnemonic generated model receive first two letter word user want remember carry information mnemonic generated dataset using kaggle song lyric data sentence lyric contain word mnemonic want generate also contain number word input output preprocessing iterating sentence removing punctuation number extracting first letter word sentence assigning unique number pair letter predefined dictionary contains pair key key unique number value list unique number assigned act input glove vector word act output time step lstm model take unique number assigned word output corresponding word glove vector model architecture using lstm time step time step unique number associated pair letter fed output glove vector corresponding word result model outputting mnemonic match first two letter word input mnemonic generated carry little meaning realized problem caused way training order letter extract word already ready sentence formation case testing order feeding letter extract word may high probability sentence formation built bigram data feed permutation highest probability sentence formation mnemonic generator model though improvement sentence whole make sense stuck point input output question make sure model generates meaningful sentence using loss function
Can TimeDistributed Layer used for many-to-one LSTM?,"<p>In Keras, I found that many people specify ""return sequences"" to False when they train a many-to-one LSTM model. 
I wonder can I use a TimeDistributed Layer for each timestep cell and then use a dense layer above to get the output?</p>
",Training and Model Evaluation,timedistributed layer used many one lstm kera found many people specify return sequence false train many one lstm model wonder use timedistributed layer timestep cell use dense layer get output
How to train a pretrained binary file on my own corpus using gensim?,"<p>Hey guys I have a pretrained binary file and I want to train it on my corpus. </p>

<p><strong>Approach I tried :</strong></p>

<p>I tried to extract the txt file from the bin file I had and use this as a word2vec file at time of loading and further trained it on my own corpus and saved the model but the model is performing badly for the words which are there in the pre-trained bin file (I used intersect_word2vec_format command for this.) </p>

<p><a href=""https://drive.google.com/open?id=1AdBYWP1lWrg9QsX4BZ63rpUihyR4BpvS"" rel=""nofollow noreferrer"">Here</a> is the script I used.</p>

<p>What should be my approach for my model to perform well on words from both the pre-trained file and my corpus?</p>
",Training and Model Evaluation,train pretrained binary file corpus using gensim hey guy pretrained binary file want train corpus approach tried tried extract txt file bin file use word vec file time loading trained corpus saved model model performing badly word pre trained bin file used intersect word vec format command script used approach model perform well word pre trained file corpus
Is there a better approach for personality detection from twitter data?,"<p>I have tried different approaches like multinomialNB, SVM, MLPClassifier, CNN as well as LSTM network to train the dataset that consists of tweets and labels (big 5 classes - openness, conscientiousness, extraversion, agreeable, neuroticism). But the accuracy is at around 60% even after using word2vec, NRC features &amp; MRC features. Is there something that I can do to improve the accuracy?</p>
",Training and Model Evaluation,better approach personality detection twitter data tried different approach like multinomialnb svm mlpclassifier cnn well lstm network train dataset consists tweet label big class openness conscientiousness extraversion agreeable neuroticism accuracy around even using word vec nrc feature mrc feature something improve accuracy
When to stop training neural networks?,"<p>I'm trying to carry out a domain-specific classification research using RNN and have accumulated tens of millions of texts. Since it takes days and even months to run the whole dataset over, I only picked a small portion of it for testing, say 1M texts (80% for training, 20% for validation). I pre-trained the whole corpus with word vectorization and I also applied Dropout to the model to avoid over-fitting. When it trained 60000 text within 12 hrs, the loss had already dropped to a fairly low level with the accuracy 97%. Should I continue or not? Does it help continue with the training?</p>

<p>It is still running the first epoch and I'm afraid if I stopped right now, the model wouldn't cover the whole...</p>
",Training and Model Evaluation,stop training neural network trying carry domain specific classification research using rnn accumulated ten million text since take day even month run whole dataset picked small portion testing say text training validation pre trained whole corpus word vectorization also applied dropout model avoid fitting trained text within hr loss already dropped fairly low level accuracy continue doe help continue training still running first epoch afraid stopped right model cover whole
Convert a log.txt file to JSON file,"<p>I have to convert a log file into a json file to train a unsupervised model.
The log file is in format - </p>

<pre><code>40.77.167.191, 172.16.30.15 - - [08/May/2018:03:29:15 +0530] ""GET /speedwav-full-chrome-side-beading-for-tata-indigo-cs-46901.html HTTP/1.1"" 403 162 &lt;0.000&gt; &lt;-&gt; ""-"" ""Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)""
66.249.79.25, 172.16.30.15 - - [08/May/2018:03:29:17 +0530] ""GET /schneider-dc-control-relays-ca4kn31-t008000721.html HTTP/1.1"" 200 14443 &lt;0.445&gt; &lt;0.445&gt; ""-"" ""Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.96 Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)""
66.249.79.25, 172.16.30.15 - - [08/May/2018:03:29:19 +0530] ""GET /ajax/pdp/recentlyviewed/1184932 HTTP/1.1"" 200 2 &lt;0.089&gt; &lt;0.089&gt; ""https://www.tolexo.com/orient-18w-eternal-surface-panel-square-led-light-18w01-t14ori0043.html"" ""Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)""
</code></pre>

<p>I want to get the file in format - </p>

<pre><code>40.77.167.191, 172.16.30.15 - - [08/May/2018:03:29:15 +0530] ""GET /speedwav-full-chrome-side-beading-for-tata-indigo-cs-46901.html HTTP/1.1"" 403 162 &lt;0.000&gt; &lt;-&gt; ""-"" ""Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)""

66.249.79.25, 172.16.30.15 - - [08/May/2018:03:29:17 +0530] ""GET /schneider-dc-control-relays-ca4kn31-t008000721.html HTTP/1.1"" 200 14443 &lt;0.445&gt; &lt;0.445&gt; ""-"" ""Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.96 Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)""

66.249.79.25, 172.16.30.15 - - [08/May/2018:03:29:19 +0530] ""GET /ajax/pdp/recentlyviewed/1184932 HTTP/1.1"" 200 2 &lt;0.089&gt; &lt;0.089&gt; ""https://www.tolexo.com/orient-18w-eternal-surface-panel-square-led-light-18w01-t14ori0043.html"" ""Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)""
</code></pre>

<p>and then create a json file for it.</p>
",Training and Model Evaluation,convert log txt file json file convert log file json file train unsupervised model log file format want get file format create json file
n-gram markov chain transition table,"<p>I'm trying to build an n-gram markov model from a given piece of text, and then access the transition table for it so I can calculate the conditional entropy for each sequence of words of length n (the grams). 
For example, in a 2-gram model, after reading in a corpus of text</p>

<p>""dogs chase cats dogs chase cats dogs chase cats
    dogs chase cats dogs chase cats dogs chase cats
    dogs chase cats dogs chase cats dogs chase cats
    dogs chase people""</p>

<p>and building an internal transition table, the state ""dogs chase"" may transition to the state ""chase cats"" with probability 0.9, and to state ""chase people"" with probability 0.1. If I know of the possible transitions, I can calculate the conditional entropy.</p>

<p>Are there any good python libraries for doing this? I've checked NLTK, SRILM, and others but haven't found much.</p>
",Training and Model Evaluation,n gram markov chain transition table trying build n gram markov model given piece text access transition table calculate conditional entropy sequence word length n gram example gram model reading corpus text dog chase cat dog chase cat dog chase cat dog chase cat dog chase cat dog chase cat dog chase cat dog chase cat dog chase cat dog chase people building internal transition table state dog chase may transition state chase cat probability state chase people probability know possible transition calculate conditional entropy good python library checked nltk srilm others found much
NCE loss in word2vec vs a common Neural Network,"<p>As I was trying to learn more about NLP, I realized that people use NCE loss without defining a connected NN. To be more specific:  </p>

<pre><code># Define Embeddings:
embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))

# NCE loss parameters
nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],
                                               stddev=1.0 / np.sqrt(embedding_size)))
nce_biases = tf.Variable(tf.zeros([vocabulary_size]))

# Create data/target placeholders
x_inputs = tf.placeholder(tf.int32, shape=[batch_size])
y_target = tf.placeholder(tf.int32, shape=[batch_size, 1])
valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

# Lookup the word embedding:
embed = tf.nn.embedding_lookup(embeddings, x_inputs)

# Get loss from prediction
loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,
                                     biases=nce_biases,
                                     labels=y_target,
                                     inputs=embed,
                                     num_sampled=num_sampled,
                                     num_classes=vocabulary_size))
</code></pre>

<p>Instead, why do people not define a connected NN, and a with sigmoid as the activation function in the final layer? Would my suggestion be considered as doable if we were to use one-hot encoding, and not relevant in most-frequent value representation? I am sorry if my question is a bit dumb since I am not so much familiar with NLP yet.   </p>

<p>Thanks</p>
",Training and Model Evaluation,nce loss word vec v common neural network wa trying learn nlp realized people use nce loss without defining connected nn specific instead people define connected nn sigmoid activation function final layer would suggestion considered doable use one hot encoding relevant frequent value representation sorry question bit dumb since much familiar nlp yet thanks
Why my tensorflow model outputs become NaN after x epochs?,"<p>After 85 epochs the loss (a cosine distance) of my model (a RNN with 3 LSTM layers) become NaN. Why does it happen and how can I fix it? Outputs of my model also become NaN.</p>

<p>My model :</p>

<pre><code>tf.reset_default_graph()

seqlen = tf.placeholder(tf.int32, [None])
x_id = tf.placeholder(tf.int32, [None, None])
y_id = tf.placeholder(tf.int32, [None, None])

embeddings_matrix = tf.placeholder(np.float32, [vocabulary_size, embedding_size])
x_emb = tf.nn.embedding_lookup(embeddings_matrix, x_id)
y_emb = tf.nn.embedding_lookup(embeddings_matrix, y_id)

cells = [tf.contrib.rnn.LSTMCell(s, activation=a) for s, a in [(400, tf.nn.relu), (400, tf.nn.relu), (400, tf.nn.tanh)]]
cell = tf.contrib.rnn.MultiRNNCell(cells)

outputs, _ = tf.nn.dynamic_rnn(cell, x_emb, dtype=tf.float32, sequence_length=seqlen)

loss = tf.losses.cosine_distance(tf.nn.l2_normalize(outputs, 2), tf.nn.l2_normalize(y_emb, 2), 1)
tf.summary.scalar('loss', loss)
opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)
merged = tf.summary.merge_all()
</code></pre>

<p>The output of the training :</p>

<pre><code>Epoch 80/100
    Time : 499 s    Loss : 0.972911523852701    Val Loss : 0.9729658
Epoch 81/100
    Time : 499 s    Loss : 0.9723407568655597   Val Loss : 0.9718646
Epoch 82/100
    Time : 499 s    Loss : 0.9718870568505438   Val Loss : 0.971976
Epoch 83/100
    Time : 499 s    Loss : 0.9913996352643445   Val Loss : 0.990693
Epoch 84/100
    Time : 499 s    Loss : 0.9901496524596137   Val Loss : 0.98957264
Epoch 85/100
    Time : 499 s    Loss : nan  Val Loss : nan
Epoch 86/100
    Time : 498 s    Loss : nan  Val Loss : nan
Epoch 87/100
    Time : 498 s    Loss : nan  Val Loss : nan
Epoch 88/100
    Time : 499 s    Loss : nan  Val Loss : nan
Epoch 89/100
    Time : 498 s    Loss : nan  Val Loss : nan
Epoch 90/100
    Time : 498 s    Loss : nan  Val Loss : nan
</code></pre>

<p>And here sis the curve of the loos during the entire training :
<a href=""https://i.sstatic.net/E6QGm.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/E6QGm.png"" alt=""Loss""></a></p>

<p>The blue curve is the loss on training data and the orange one in the loss on validation data.</p>

<p>The learning rate used for ADAM is 0.001.</p>

<p>My x and y got the following shape : [batch size, maximum sequence length], they're both set to None, because the last batch of each epoch is smaller, and the maximal sequence length change at each batch.</p>

<p>x and y go through an embedding lookup and become of shape [batch size, maximum sequence length, embedding size], the embedding for the padding word is a vector of 0.</p>

<p>The dynamic rnn take the length of each sequence (seqlen in the code, with a shape of [batch size]) so it will only make predictions for the exact length of each sequence and the rest of the output will be padded with vectors of zero, as for y.</p>

<p>My guess is the values of the output become so close of zero, that once they're squared to compute the cosine distance they become 0 so it leads to a division by zero.</p>

<p>Cosine distance formula :<br>
<img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=1%20-%20%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20O_%7Bi%7D%20Y_%7Bi%7D%7D%7B%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20O_%7Bi%7D%5E%7B2%7D%7D%20%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20Y_%7Bi%7D%5E%7B2%7D%7D%7D"" alt=""cosine distance""></p>

<p>I don't know if I'm right, neither how to prevent this.</p>

<p><strong>EDIT:</strong><br>
I just checked weights of every layers and they're all NaN</p>

<p><strong>SOLVED:</strong><br>
Using a l2 regularization worked.</p>

<pre><code>tf.reset_default_graph()

seqlen = tf.placeholder(tf.int32, [None])
x_id = tf.placeholder(tf.int32, [None, None])
y_id = tf.placeholder(tf.int32, [None, None])

embeddings_matrix = tf.placeholder(np.float32, [vocabulary_size, embedding_size])
x_emb = tf.nn.embedding_lookup(embeddings_matrix, x_id)
y_emb = tf.nn.embedding_lookup(embeddings_matrix, y_id)

cells = [tf.contrib.rnn.LSTMCell(s, activation=a) for s, a in [(400, tf.nn.relu), (400, tf.nn.relu), (400, tf.nn.tanh)]]
cell = tf.contrib.rnn.MultiRNNCell(cells)

outputs, _ = tf.nn.dynamic_rnn(cell, x_emb, dtype=tf.float32, sequence_length=seqlen)

regularizer = tf.reduce_sum([tf.nn.l2_loss(v) for v in tf.trainable_variables()])
cos_distance = tf.losses.cosine_distance(tf.nn.l2_normalize(outputs, 2), tf.nn.l2_normalize(y_emb, 2), 1)
loss = cos_distance + beta * regularizer

opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)

tf.summary.scalar('loss', loss)
tf.summary.scalar('regularizer', regularizer)
tf.summary.scalar('cos_distance', cos_distance)
merged = tf.summary.merge_all()
</code></pre>
",Training and Model Evaluation,tensorflow model output become nan x epoch epoch loss cosine distance model rnn lstm layer become nan doe happen fix output model also become nan model output training si curve loo entire training blue curve loss training data orange one loss validation data learning rate used adam x got following shape batch size maximum sequence length set none last batch epoch smaller maximal sequence length change batch x go embedding lookup become shape batch size maximum sequence length embedding size embedding padding word vector dynamic rnn take length sequence seqlen code shape batch size make prediction exact length sequence rest output padded vector zero guess value output become close zero squared compute cosine distance become lead division zero cosine distance formula know right neither prevent edit checked weight every layer nan solved using l regularization worked
Do we need to use Stopwords filtering before POS Tagging?,"<p>I am new to Text mining and NLP related stuffs.I am working on a small project where I am trying to extract information out of a few documents.I am basically doing a pos tagging and then using a chunker to find out the pattern based on the tagged words.Do I need to use Stopwords before doing this POS tagging?will using stopwords affect my POS tagger's accuracy?</p>
",Training and Model Evaluation,need use stopwords filtering po tagging new text mining nlp related stuff working small project trying extract information document basically po tagging using chunker find pattern based tagged word need use stopwords po tagging using stopwords affect po tagger accuracy
Using AI/ML in PDF text mining,"<p>I am newbie when it comes to NLP since I am just starting to learn about it. So, if the question seems simplistic, please bear with me :)</p>

<p>I have bunch of PDF files (a lot) and the task is : whenever someone ask a question and answer is available within one of those PDFs, either that section containing answer is extracted or the page number of that particular PDF is shown as the answer. You can think of each PDF as a manual for a certain product; and in total there are hundreds of PDF files.</p>

<p>I know that this problem can be easily solved using a PDF search engine; but is there any potential approach from text mining and AI/ML perspective that could solve the problem ? May be I train the model on few PDFs and it works for rest ?</p>

<p>Recently I have been trying to do some research and what I have got so far is: Whenever someone asks a question, I can extract keywords from the question using nltk python (plenty of resources available online). But what is really bothering me is the next part, where pdf text/information comes into play.</p>

<p>Thanks in advance :)   </p>
",Training and Model Evaluation,using ai ml pdf text mining newbie come nlp since starting learn question seems simplistic please bear bunch pdf file lot task whenever someone ask question answer available within one pdfs either section containing answer extracted page number particular pdf shown answer think pdf manual certain product total hundred pdf file know problem easily solved using pdf search engine potential approach text mining ai ml perspective could solve problem may train model pdfs work rest recently trying research got far whenever someone asks question extract keywords question using nltk python plenty resource available online really bothering next part pdf text information come play thanks advance
Is there a &quot;best&quot; tokenization for NER training in OpenNLP?,"<p>Is there a ""best"" tokenization for NER training in OpenNLP? I noticed that OpenNLP provides a max-entropy tokenizer that allows you to tokenize based on a trained model. I also noticed that OpenNLP provides a simple tokenizer. If I use the same tokenizer during runtime that I used to train my model, does it matter which tokenizer I use?</p>

<p>I would rather use the simple tokenizer for my application. </p>
",Training and Model Evaluation,best tokenization ner training opennlp best tokenization ner training opennlp noticed opennlp provides max entropy tokenizer allows tokenize based trained model also noticed opennlp provides simple tokenizer use tokenizer runtime used train model doe matter tokenizer use would rather use simple tokenizer application
Train a custom BIO Tagger with NTLK,"<p>I have been searching for a while, but I didn't find anything about this.</p>

<p>I've got the following problem:
I want to train a model where for a input i get a custom BIO tag. For instance, for the input ""My dad lives in Manhattan, his name is Anthony Clark"", and the classes LOC and PER, the output has to be:</p>

<pre><code>[(My, O),(dad,O), (lives, O), (in,O), (Manhattan, B-LOC), (, , O), (his,O), (name,O), (is,O), (Anthony, B-PER), (Clark,I-PER)]
</code></pre>

<p>Is it possible to do with NTLK? Which features should I include?</p>
",Training and Model Evaluation,train custom bio tagger ntlk searching find anything got following problem want train model input get custom bio tag instance input dad life manhattan name anthony clark class loc per output ha possible ntlk feature include
Gensim Word2Vec &#39;you must first build vocabulary before training the model&#39;,"<p>I am trying to fit a Word2Vec model. According to the documentation for Gensim's Word2Vec we do not need to call <code>model.build_vocabulary</code> before using it. 
But yet it is asking for me to do it. I have tried calling this function and it has not worked. I also fitted a Word2Vec model before without needing to call <code>model.build_vocabulary</code> . </p>

<p>Am I doing something wrong? Here is my code:</p>

<pre><code>from gensim.models import Word2Vec
dataset = pd.read_table('genemap_copy.txt',delimiter='\t', lineterminator='\n')

def row_to_sentences(dataframe):
    columns = dataframe.columns.values
    corpus = []
    for index,row in dataframe.iterrows():
        if index == 1000:
            break
        sentence = ''
        for column in columns:
            sentence += ' '+str(row[column])
        corpus.append([sentence])
    return corpus

corpus = row_to_sentences(dataset)
clean_corpus = [[sentence[0].lower()] for sentence in corpus ]


# model = Word2Vec()
# model.build_vocab(clean_corpus)
model = Word2Vec(clean_corpus, size=100, window=5, min_count=5, workers=4)
</code></pre>

<p>Help is greatly appreciated!
Also I am using macOS Sierra.
There is not much support online for using Gensim with Mac D: . </p>
",Training and Model Evaluation,gensim word vec must first build vocabulary training model trying fit word vec model according documentation gensim word vec need call using yet asking tried calling function ha worked also fitted word vec model without needing call something wrong code help greatly appreciated also using macos sierra much support online using gensim mac
RNN not training when batch size &gt; 1 with variable length data,"<p>I'm implementing a simple RNN network which predicts 1/0 for some variable length 
 time-series data. The network would first feed the training data into an LSTM cell, and then use a linear layer for classification. </p>

<p>Usually, we would use mini-batches to train the network. But, the problem is that this simple RNN network is not training when I use <code>batch_size</code> > 1.</p>

<p>I manage to create a minimal code sample which can reproduce the problem. If you set <code>batch_size=1</code> at line 95, the network trains successfully, but if you set <code>batch_size=2</code>, the network is not training at all, the losses just bouncing around. (requires python3, pytorch >= 0.4.0)</p>



<pre class=""lang-python prettyprint-override""><code>import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence


class ToyDataLoader(object):

    def __init__(self, batch_size):
        self.batch_size = batch_size
        self.index = 0
        self.dataset_size = 10

        # generate 10 random variable length training samples,
        # each time step has 1 feature dimension
        self.X = [
            [[1], [1], [1], [1], [0], [0], [1], [1], [1]],
            [[1], [1], [1], [1]],
            [[0], [0], [1], [1]],
            [[1], [1], [1], [1], [1], [1], [1]],
            [[1], [1]],
            [[0]],
            [[0], [0], [0], [0], [0], [0], [0]],
            [[1]],
            [[0], [1]],
            [[1], [0]]
        ]

        # assign labels for the toy traning set
        self.y = torch.LongTensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

    def __len__(self):
        return self.dataset_size // self.batch_size

    def __iter__(self):
        return self

    def __next__(self):
        if self.index + self.batch_size &gt; self.dataset_size:
            self.index = 0
            raise StopIteration()
        if self.index == 0:  # shufle the dataset
            tmp = list(zip(self.X, self.y))
            random.shuffle(tmp)
            self.X, self.y = zip(*tmp)
            self.y = torch.LongTensor(self.y)
        X = self.X[self.index: self.index + self.batch_size]
        y = self.y[self.index: self.index + self.batch_size]
        self.index += self.batch_size
        return X, y


class NaiveRNN(nn.Module):
    def __init__(self):
        super(NaiveRNN, self).__init__()
        self.lstm = nn.LSTM(1, 128)
        self.linear = nn.Linear(128, 2)

    def forward(self, X):
        '''
        Parameter:
            X: list containing variable length training data
        '''

        # get the length of each seq in the batch
        seq_lengths = [len(x) for x in X]

        # convert to torch.Tensor
        seq_tensor = [torch.Tensor(seq) for seq in X]

        # sort seq_lengths and seq_tensor based on seq_lengths, required by torch.nn.utils.rnn.pad_sequence
        pairs = sorted(zip(seq_lengths, seq_tensor),
                       key=lambda pair: pair[0], reverse=True)
        seq_lengths = torch.LongTensor([pair[0] for pair in pairs])
        seq_tensor = [pair[1] for pair in pairs]

        # padded_seq shape: (seq_len, batch_size, feature_size)
        padded_seq = pad_sequence(seq_tensor)

        # pack them up
        packed_seq = pack_padded_sequence(padded_seq, seq_lengths.numpy())

        # feed to rnn
        packed_output, (ht, ct) = self.lstm(packed_seq)

        # linear classification layer
        y_pred = self.linear(ht[-1])

        return y_pred


def main():
    trainloader = ToyDataLoader(batch_size=2)  # not training at all! !!
    # trainloader = ToyDataLoader(batch_size=1) # it converges !!!

    model = NaiveRNN()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adadelta(model.parameters(), lr=1.0)

    for epoch in range(30):
        # switch to train mode
        model.train()

        for i, (X, labels) in enumerate(trainloader):

            # compute output
            outputs = model(X)
            loss = criterion(outputs, labels)

            # measure accuracy and record loss
            _, predicted = torch.max(outputs, 1)
            accu = (predicted == labels).sum().item() / labels.shape[0]

            # compute gradient and do SGD step
            optimizer.zero_grad()
            loss.backward()

            optimizer.step()

            print('Epoch: [{}][{}/{}]\tLoss {:.4f}\tAccu {:.3f}'.format(
                epoch, i, len(trainloader), loss, accu))


if __name__ == '__main__':
    main()
</code></pre>

<p>Sample output when <code>batch_size=1</code>:</p>

<pre class=""lang-python prettyprint-override""><code>...
Epoch: [28][7/10]       Loss 0.1582     Accu 1.000
Epoch: [28][8/10]       Loss 0.2718     Accu 1.000
Epoch: [28][9/10]       Loss 0.0000     Accu 1.000
Epoch: [29][0/10]       Loss 0.2808     Accu 1.000
Epoch: [29][1/10]       Loss 0.0000     Accu 1.000
Epoch: [29][2/10]       Loss 0.0001     Accu 1.000
Epoch: [29][3/10]       Loss 0.0149     Accu 1.000
Epoch: [29][4/10]       Loss 0.1445     Accu 1.000
Epoch: [29][5/10]       Loss 0.2866     Accu 1.000
Epoch: [29][6/10]       Loss 0.0170     Accu 1.000
Epoch: [29][7/10]       Loss 0.0869     Accu 1.000
Epoch: [29][8/10]       Loss 0.0000     Accu 1.000
Epoch: [29][9/10]       Loss 0.0498     Accu 1.000
</code></pre>

<p>Sample output when <code>batch_size=2</code>:</p>

<pre class=""lang-python prettyprint-override""><code>...
Epoch: [27][2/5]        Loss 0.8051     Accu 0.000
Epoch: [27][3/5]        Loss 1.2835     Accu 0.000
Epoch: [27][4/5]        Loss 1.0782     Accu 0.000
Epoch: [28][0/5]        Loss 0.5201     Accu 1.000
Epoch: [28][1/5]        Loss 0.6587     Accu 0.500
Epoch: [28][2/5]        Loss 0.3488     Accu 1.000
Epoch: [28][3/5]        Loss 0.5413     Accu 0.500
Epoch: [28][4/5]        Loss 0.6769     Accu 0.500
Epoch: [29][0/5]        Loss 1.0434     Accu 0.000
Epoch: [29][1/5]        Loss 0.4460     Accu 1.000
Epoch: [29][2/5]        Loss 0.9879     Accu 0.000
Epoch: [29][3/5]        Loss 1.0784     Accu 0.500
Epoch: [29][4/5]        Loss 0.6051     Accu 1.000
</code></pre>

<p>I've searched a lot of materials and still can't figure out why.</p>
",Training and Model Evaluation,rnn training batch size variable length data implementing simple rnn network predicts variable length time series data network would first feed training data lstm cell use linear layer classification usually would use mini batch train network problem simple rnn network training use manage create minimal code sample reproduce problem set line network train successfully set network training loss bouncing around requires python pytorch sample output sample output searched lot material still figure
Entity Type Recogition : Finding an Entity&#39;s Dominant Type from its Description,"<p>I've been working on a research project. I have a database of Wikipedia descriptions of a large number of entities, including sportspersons, politicians, actors, etc. The aim is to determine the type of entity using the descriptions. I have access to some data with the predicted type of entity which is quite accurate. <strong>This will be my training data.</strong> What I would like to do is train a model to predict the dominant type of entity for rest of the data. </p>

<p>What I've done till now:</p>

<ul>
<li>Extracted the first paragraph, H1, H2 headers of Wiki description of the entity.</li>
<li>Extracted the category list of the entity on the wiki page (The bottom 'Categories' section present on any page <a href=""https://en.wikipedia.org/wiki/Pulp_Fiction#mw-normal-catlinks"" rel=""nofollow noreferrer"">like here.</a></li>
</ul>

<p>Finding the type of entity can be difficult for entities that are associated with two or more concepts, like an <strong><em>actor who later became a politician.</em></strong></p>

<p><strong>I want to ask as to how I create a model out of the raw data that I have?</strong> <strong>What are the variables that I should use to train the model?</strong> 
Also are there any Natural Language Processing techniques that can be helpful for this purpose? I know POS taggers can be helpful in this case.</p>

<p>My search over the internet has not been much successful. I've stumbled across research papers and blogs like <a href=""https://www.searchtechnologies.com/blog/natural-language-processing-techniques"" rel=""nofollow noreferrer"">this one</a>, but none of them have relevant information for this purpose. Any ideas would be appreciated. Thanks in advance!</p>

<hr>

<p>EDIT 1: </p>

<p>The input data is the first paragraph of the Wikipedia page of the entity. For example, for <a href=""https://en.wikipedia.org/wiki/Al_Franken"" rel=""nofollow noreferrer"">this page</a>, my input would be: </p>

<p>Alan Stuart Franken (born May 21, 1951) is an American comedian, writer, producer, author, and politician who served as a United States Senator from Minnesota from 2009 to 2018. He became well known in the 1970s and 1980s as a performer on the television comedy show Saturday Night Live (SNL). After decades as a comedic actor and writer, he became a prominent liberal political activist, hosting The Al Franken Show on Air America Radio.</p>

<p>My extracted information is, the first paragraph of the page, the string of all the 'Categories' (bottom part of the page), and all the headers of the page.</p>
",Training and Model Evaluation,entity type recogition finding entity dominant type description working research project database wikipedia description large number entity including sportspersons politician actor etc aim determine type entity using description access data predicted type entity quite accurate training data would like train model predict dominant type entity rest data done till extracted first paragraph h h header wiki description entity extracted category list entity wiki page bottom category section present page like finding type entity difficult entity associated two concept like actor later became politician want ask create model raw data variable use train model also natural language processing technique helpful purpose know po tagger helpful case search internet ha much successful stumbled across research paper blog like one none relevant information purpose idea would appreciated thanks advance edit input data first paragraph wikipedia page entity example page input would alan stuart franken born may american comedian writer producer author politician served united state senator minnesota became well known performer television comedy show saturday night live snl decade comedic actor writer became prominent liberal political activist hosting al franken show air america radio extracted information first paragraph page string category bottom part page header page
Python interface to ARPA files,"<p>I'm looking for a pythonic interface to load ARPA files (back-off language models) and use them to evaluate some text, e.g. get its log-probability, perplexity etc.</p>

<p>I don't need to generate the ARPA file in Python, only to use it for querying.</p>

<p>Does anybody have a recommended package?
I already saw <a href=""https://github.com/kpu/kenlm"">kenlm</a> and <a href=""https://github.com/desilinguist/swig-srilm"">swig-srilm</a>, but the first is very hard to set up in Windows and the second seems un-maintained anymore.</p>
",Training and Model Evaluation,python interface arpa file looking pythonic interface load arpa file back language model use evaluate text e g get log probability perplexity etc need generate arpa file python use querying doe anybody recommended package already saw
How can using more n-gram orders decrease accuracy for Multinomial NaiveBayes classifier?,"<p>I'm building a model for text classification with nltk, and <code>sklearn</code>, and training it on the 20newsgroups dataset from <code>sklearn</code> (each document is approximately 130 words).</p>

<p>My preprocessing includes removing stopwords and lemmatizing tokens. </p>

<p>Next, in my pipeline I pass it to the <code>tfidfVectorizer()</code> and want to manipulate some of the input parameters of the vectorizer to improve accuracy. I've read that n-grams (generally, with n less than improves accuracy, but when I classify the vectorizer outputs with the <code>multinomialNB()</code> classifier, using <code>ngram_range=(1,2)</code> and <code>ngram_range=(1,3)</code> in tfidf, it worsens the accuracy. Can someone help explain why?</p>

<p><strong>EDIT</strong>:
Here's a sample datum as requested, with the code I used to fetch it and strip the header:</p>

<pre><code>from sklearn.datasets import fetch_20newsgroups
news = fetch_20newsgroups(subset='all', remove=""headers"")
#example of data text (no header)
print(news.data[0])

I am sure some bashers of Pens fans are pretty confused about the lack
of any kind of posts about the recent Pens massacre of the Devils. Actually,
I am  bit puzzled too and a bit relieved. However, I am going to put an end
to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they
are killing those Devils worse than I thought. Jagr just showed you why
he is much better than his regular season stats. He is also a lot
fo fun to watch in the playoffs. Bowman should let JAgr have a lot of
fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final regular season game.          PENS RULE!!!
</code></pre>

<p>This is my pipeline, the code run to train the model, and print accuracy:</p>

<pre><code>    test1_pipeline=Pipeline([('clean', clean()),
                         ('vectorizer', TfidfVectorizer(ngram_range=(1,2))), 
                         ('classifier', MultinomialNB())])

train(test1_pipeline, news_group_train.data, news_group_train.target)
</code></pre>
",Training and Model Evaluation,using n gram order decrease accuracy multinomial naivebayes classifier building model text classification nltk training newsgroups dataset document approximately word preprocessing includes removing stopwords lemmatizing token next pipeline pas want manipulate input parameter vectorizer improve accuracy read n gram generally n le improves accuracy classify vectorizer output classifier using tfidf worsens accuracy someone help explain edit sample datum requested code used fetch strip header pipeline code run train model print accuracy
How to use spaCy to create a new entity and learn only from keyword list,"<p>I am trying to use <a href=""https://spacy.io/"" rel=""noreferrer"">spaCy</a> to create a new entity categorization 'Species' with a list of species names, example can he found <a href=""https://a-z-animals.com/animals/scientific/"" rel=""noreferrer"">here</a>.</p>
<p>I found a tutorial for training new entity type from <a href=""https://spacy.io/usage/training#example-new-entity-type"" rel=""noreferrer"">this spaCy tutorial</a> (Github code <a href=""https://github.com/explosion/spaCy/blob/master/examples/training/train_new_entity_type.py"" rel=""noreferrer"">here</a>). However, the problem is, I don't want to manually create a sentence for each species name as it would be very time consuming.</p>
<p>I created below training data, which looks like this:</p>
<pre><code>TRAIN_DATA = [('Bombina',{'entities':[(0,6,'SPECIES')]}),
 ('Dermaptera',{'entities':[(0,9,'SPECIES')]}),
  .... 
]
</code></pre>
<p>The way I created the training set is: instead of providing a full sentence and the location of the matched entity, I only provide the name of each species, and the start and end index are programmatically generated:</p>
<blockquote>
<p>[( 0, 6, 'SPECIES' )]</p>
<p>[( 0, 9, 'SPECIES' )]</p>
</blockquote>
<p>Below training code is what I used to train the model. (Code copied from above hyperlink)</p>
<pre><code>nlp = spacy.blank('en')  # create blank Language class

 # Add entity recognizer to model if it's not in the pipeline 
 # nlp.create_pipe works for built-ins that are registered with spaCy 
 if 'ner' not in nlp.pipe_names: 
     ner = nlp.create_pipe('ner') 
     nlp.add_pipe(ner) 
 # otherwise, get it, so we can add labels to it 
 else: 
     ner = nlp.get_pipe('ner') 

 ner.add_label(LABEL)   # add new entity label to entity recognizer


  if model is None: 
      optimizer = nlp.begin_training() 
  else: 
      # Note that 'begin_training' initializes the models, so it'll zero out 
      # existing entity types. 
      optimizer = nlp.entity.create_optimizer() 

     # get names of other pipes to disable them during training 
     other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner'] 
     with nlp.disable_pipes(*other_pipes):  # only train NER 
         for itn in range(n_iter): 
             random.shuffle(TRAIN_DATA) 
             losses = {} 
             for text, annotations in TRAIN_DATA: 
                 nlp.update([text], [annotations], sgd=optimizer, drop=0.35,  losses=losses) 
             print(losses) 
</code></pre>
<p>I'm new to NLP and spaCy please let me know if I did it correctly or not. And why my attempt failed the training (when I ran it, it throws an error).</p>
<hr />
<p>[UPDATE]</p>
<p>The reason I want to feed keyword only to the training model is that, ideally, I would hope the model to learn those key words first, and once it identifies a context which contains the keyword, it will learn the associated context, and therefore, enhance the current model.</p>
<p>At the first glance, it is more like regex expression. But with more and more data feeding in, the model will continuous learn, and finally being able to identify new species names that previously not exists in the original training set.</p>
<hr />
<p>Thanks,
Katie</p>
",Training and Model Evaluation,use spacy create new entity learn keyword list trying use spacy create new entity categorization specie list specie name example found found tutorial training new entity type spacy tutorial github code however problem want manually create sentence specie name would time consuming created training data look like way created training set instead providing full sentence location matched entity provide name specie start end index programmatically generated specie specie training code used train model code copied hyperlink new nlp spacy please let know correctly attempt failed training ran throw error update reason want feed keyword training model ideally would hope model learn key word first identifies context contains keyword learn associated context therefore enhance current model first glance like regex expression data feeding model continuous learn finally able identify new specie name previously exists original training set thanks katie
CPU memory allocation failed in Dynet,"<p>I am not sure why I am running out of memory. Take the <a href=""https://github.com/elikip/bist-parser/blob/master/bmstparser/src/mstlstm.py"" rel=""nofollow noreferrer"">parser</a> of Goldberg, all I do is to change <a href=""https://github.com/elikip/bist-parser/blob/4cd6a29d6c737ff29cb61c0e51532a9d60c4818c/bmstparser/src/mstlstm.py#L185"" rel=""nofollow noreferrer"">this</a> line:</p>

<blockquote>
  <p>scores, exprs = self.__evaluate(conll_sentence, True)</p>
</blockquote>

<p>and add a for loop around it to repeat it K times:</p>

<pre><code>for k in xrange(K):
    scores, exprs = self.__evaluate(conll_sentence, True)
    # do something
</code></pre>

<p>Then in the <a href=""https://github.com/elikip/bist-parser/blob/4cd6a29d6c737ff29cb61c0e51532a9d60c4818c/bmstparser/src/mstlstm.py#L98"" rel=""nofollow noreferrer"">getExpr</a>, i do the following:</p>

<pre><code>samples_out = np.random.normal(0,0.001, (1, self.hidden_units))
samples_FOH = np.random.normal(0,0.001,(self.hidden_units, self.ldims * 2))
samples_FOM = np.random.normal(0,0.001,(self.hidden_units, self.ldims * 2))
samples_Bias = np.random.normal(0,0.001, (self.hidden_units))

XoutLayer = self.outLayer.expr()+inputTensor(samples_out)
XhidLayerFOH = self.hidLayerFOH.expr()+inputTensor(samples_FOH)
XhidLayerFOM = self.hidLayerFOM.expr()+inputTensor(samples_FOM)
XhidBias = self.hidBias.expr()+inputTensor(samples_Bias)

if sentence[i].headfov is None:
    sentence[i].headfov = XhidLayerFOH * concatenate([sentence[i].lstms[0], sentence[i].lstms[1]])
if sentence[j].modfov is None:
    sentence[j].modfov  = XhidLayerFOM * concatenate([sentence[j].lstms[0], sentence[j].lstms[1]])

output = XoutLayer * self.activation(sentence[i].headfov + sentence[j].modfov + XhidBias)
return output
</code></pre>

<p>Essentially what is happening in the above block is to first generate normally distributed noise, then add it to the trained values. But it seems somewhere along the way, all the generated values stay in the memory and it just runs out of memory. Any one knows why?</p>
",Training and Model Evaluation,cpu memory allocation failed dynet sure running memory take parser goldberg change line score exprs self evaluate conll sentence true add loop around repeat k time getexpr following essentially happening block first generate normally distributed noise add trained value seems somewhere along way generated value stay memory run memory one know
NLP - Bag of words classification,"<p><strong>The issue:</strong></p>

<p>I am confused as to why we transform our test data using the CountVectorizer fitted on our train data for bag of words classification.</p>

<p>Why would we not create a new CountVectorizer and fit the test data to this and have the classifier predict on the test CountVectorizer?</p>

<p>Looking here: <a href=""https://stackoverflow.com/questions/44978782/how-to-standardize-the-bag-of-words-for-train-and-test/44984242#44984242"">How to standardize the bag of words for train and test?</a></p>

<p><strong>Ripped from the answer:</strong></p>

<pre><code>LabeledWords=pd.DataFrame(columns=['word','label'])

LabeledWords.append({'word':'Church','label':'Religion'} )

vectorizer = CountVectorizer()

Xtrain,yTrain=vectorizer.fit_transform(LabeledWords['word']).toarray(),vectorizer.fit_transform(LabeledWords['label']).toarray()
forest = RandomForestClassifier(n_estimators = 100) 
clf=forest.fit(Xtrain,yTrain)

for each_word,label in Preprocessed_list:
    test_featuresX.append(vectorizer.transform(each_word),toarray())
    test_featuresY.append(label.toarray())

clf.score(test_featuresX,test_featuresY) 
</code></pre>

<p>We can see the user created a CountVectorizer and fit it to the training data. Then fit the classifier to this CountVectorizer. Afterwards the user just transformed the test data using the CountVectorizer which was fit to the train data, and fed this into the classifier. Why is that?</p>

<p><strong>What I am trying to accomplish:</strong>
I am trying to implement bag of visual words. It uses the same concept, but I am unsure how should create my train and test sets for classification.</p>
",Training and Model Evaluation,nlp bag word classification issue confused transform test data using countvectorizer fitted train data bag word classification would create new countvectorizer fit test data classifier predict test countvectorizer looking href standardize bag word train test ripped answer see user created countvectorizer fit training data fit classifier countvectorizer afterwards user transformed test data using countvectorizer wa fit train data fed classifier trying accomplish trying implement bag visual word us concept unsure create train test set classification
Microsoft LUIS doesn&#39;t recognize an Entity,"<p>I've created model for MS LUIS.
It recognizes intents sharply, but no any entity recognized or recognized partly.
What can be wrong?</p>

<p>My steps are: 
import new version in LUIS app -> Train -> Publish -> Test.</p>

<p>When I type utterance from model example - it finds.</p>

<p>Also e.g. when I type: ""create task check why it doesn't show all"", I am getting this (small part of taskName):</p>

<pre><code> ""entities"": [
{
  ""entity"": ""check why"",
  ""type"": ""taskName"",
  ""startIndex"": 12,
  ""endIndex"": 20,
  ""score"": 0.8236943,
  ""role"": """"
}
]
</code></pre>

<p>here is  JSON model:</p>

<pre><code>{
 ""luis_schema_version"": ""3.0.0"",
 ""versionId"": ""0.12"",
 ""name"": ""Todo-Agent 0.4"",
 ""desc"": ""This LUIS app has a  AddTask and None intents."",
 ""culture"": ""en-us"",
 ""intents"": [
{
  ""name"": ""AddTask""
},
],
 ""entities"": [
{
  ""name"": ""taskName"",
  ""roles"": []
}
 ],
 ""composites"": [],
 ""closedLists"": [],
 ""patternAnyEntities"": [],
 ""regex_entities"": [],
 ""prebuiltEntities"": [],
 ""model_features"": [],
 ""regex_features"": [],
 ""patterns"": [
   {
  ""pattern"": ""add task - {taskName}"",
  ""intent"": ""AddTask""
},
{
  ""pattern"": ""create task {taskName}"",
  ""intent"": ""AddTask""
}
 ],
 ""utterances"": [
{
  ""text"": ""2"",
  ""intent"": ""None"",
  ""entities"": []
},
{
  ""text"": ""5"",
  ""intent"": ""None"",
  ""entities"": []
},
{
  ""text"": ""add the task - do something"",
  ""intent"": ""AddTask"",
  ""entities"": [
    {
      ""entity"": ""taskName"",
      ""startPos"": 15,
      ""endPos"": 26
    }
  ]
},
{
  ""text"": ""add the task test text"",
  ""intent"": ""AddTask"",
  ""entities"": [
    {
      ""entity"": ""taskName"",
      ""startPos"": 13,
      ""endPos"": 21
    }
  ]
},
{
  ""text"": ""bla bla"",
  ""intent"": ""None"",
  ""entities"": []
},
{
  ""text"": ""create task bad response"",
  ""intent"": ""AddTask"",
  ""entities"": [
    {
      ""entity"": ""taskName"",
      ""startPos"": 12,
      ""endPos"": 23
    }
  ]
},
{
  ""text"": ""create task check again"",
  ""intent"": ""AddTask"",
  ""entities"": [
    {
      ""entity"": ""taskName"",
      ""startPos"": 12,
      ""endPos"": 22
    }
  ]
},
{
  ""text"": ""create task check entity"",
  ""intent"": ""AddTask"",
  ""entities"": [
    {
      ""entity"": ""taskName"",
      ""startPos"": 12,
      ""endPos"": 23
    }
  ]
},
{
  ""text"": ""create task do something wrong"",
  ""intent"": ""AddTask"",
  ""entities"": [
    {
      ""entity"": ""taskName"",
      ""startPos"": 12,
      ""endPos"": 29
    }
  ]
},
{
  ""text"": ""create task go home"",
  ""intent"": ""AddTask"",
  ""entities"": [
    {
      ""entity"": ""taskName"",
      ""startPos"": 11,
      ""endPos"": 18
    }
  ]
},
{
  ""text"": ""create task testone"",
  ""intent"": ""AddTask"",
  ""entities"": [
    {
      ""entity"": ""taskName"",
      ""startPos"": 12,
      ""endPos"": 18
    }
  ]
}
 ]
}
</code></pre>

<p>Thank you for help in advance.</p>
",Training and Model Evaluation,microsoft luis recognize entity created model luis recognizes intent sharply entity recognized recognized partly wrong step import new version luis app train publish test type utterance model example find also e g type create task check show getting small part taskname json model thank help advance
Paraphrase evaluation corpora,"<p>Is there a corpus other than MSRPC (Microsoft Research Paraphrase Corpus) for evaluating Paraphrase recognition approaches? I'm using MSRPC but I'm in need of other corpora for evaluating my approach. </p>
",Training and Model Evaluation,paraphrase evaluation corpus corpus msrpc microsoft research paraphrase corpus evaluating paraphrase recognition approach using msrpc need corpus evaluating approach
Retraining and updating an existing Rasa NLU model,"<p>I've been using Rasa NLU for a project which involves making sense of structured text. My use case requires me to keep updating my training set by adding new examples of text corpus entities. However, this means that I have to keep retraining my model every few days, thereby taking more time for the same owing to increased training set size.</p>

<p>Is there a way in Rasa NLU to update an already trained model by only training it with the new training set data instead of retraining the entire model again using the entire previous training data set and the new training data set? </p>

<p>I'm trying to look for an approach where I can simply update my existing trained model by training it with incremental additional training data set every few days.</p>
",Training and Model Evaluation,retraining updating existing rasa nlu model using rasa nlu project involves making sense structured text use case requires keep updating training set adding new example text corpus entity however mean keep retraining model every day thereby taking time owing increased training set size way rasa nlu update already trained model training new training set data instead retraining entire model using entire previous training data set new training data set trying look approach simply update existing trained model training incremental additional training data set every day
How to conceptually think about relationship between tokenized words and word embeddings?,"<p>I have been using JJ Allaire's guide to using word embeddings in neural network model for text processing (<a href=""https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/6.1-using-word-embeddings.nb.html"" rel=""noreferrer"">https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/6.1-using-word-embeddings.nb.html</a>). I am confused as to how the model relates the tokenized sequences of words (x_train) back to the word embeddings that are defined using the whole dataset (instead of just the training data). Is there a way to conceptualize how the word tokens are mapped to word embeddings? Otherwise, how does a word like 'king' map to the word embedding (obtained using Glove for example). I am speaking to the relation between these chunks of code:</p>

<pre><code>#building model 
history &lt;- model %&gt;% fit(
 x_train, y_train,
 epochs = 20,
 batch_size = 32,
 validation_data = list(x_val, y_val)
)

#relating model to word embeddings
model &lt;- keras_model_sequential() %&gt;% 
layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
              input_length = maxlen) %&gt;% 
layer_flatten() %&gt;% 
layer_dense(units = 32, activation = ""relu"") %&gt;% 
layer_dense(units = 1, activation = ""sigmoid"")

get_layer(model, index = 1) %&gt;% 
 set_weights(list(embedding_matrix)) %&gt;% 
 freeze_weights()
</code></pre>

<p>How is a tokenized word from the x_train linked back to a word in the embedding_matrix (especially if the embedding layer is trained on all data)?</p>
",Training and Model Evaluation,conceptually think relationship tokenized word word embeddings using jj allaire guide using word embeddings neural network model text processing confused model relates tokenized sequence word x train back word embeddings defined using whole dataset instead training data way conceptualize word token mapped word embeddings otherwise doe word like king map word embedding obtained using glove example speaking relation chunk code tokenized word x train linked back word embedding matrix especially embedding layer trained data
i am trying to evaluate the confusion matrix and ROC curve in this code in NLTK.,"<h1>    This is code i am working on</h1>

<p>Here is the code of short reviews of movies. </p>

<pre><code>documents = []
all_words = []
allowed_words_types = ['J']
for p in short_pos.split('\n'):
    documents.append((p,""pos""))
    words = word_tokenize(p)
    pos = nltk.pos_tag(words)
    for w in pos:
        if w[1][0] in allowed_words_types:
            all_words.append(w[0].lower())
for p in short_neg.split('\n'):
    documents.append((p,""neg""))
    words = word_tokenize(p)
    pos = nltk.pos_tag(words)
    for w in pos:
        if w[1][0] in allowed_words_types:
            all_words.append(w[0].lower())
all_words = nltk.FreqDist(all_words)
words_features = list(all_words.keys())[:5000]
def find_features(document):
    words = word_tokenize(document)
    features = {}
    for w in words_features:
        features[w] = (w in words)
    return features
featuresets = [(find_features(rev),category) for (rev,category) in documents]
random.shuffle(featuresets)
print(len(featuresets))
training_set = featuresets[:100] 
testing_set = featuresets[100:]
classifier = nltk.NaiveBayesClassifier.train(training_set)
</code></pre>

<h2>    Here i want to calculate the confusion  Mattrix and ROC</h2>

<p>i just find the accuracy but i'm unable to find the roc and the confusion matrix, it will very helpfull anyone can help me out. thanks.</p>

<pre><code>print("" Original Naive Bayes Algo accuracy percent : "",(nltk.classify.accuracy(classifier,testing_set))*100)
MNB_classifier = SklearnClassifier(MultinomialNB())
MNB_classifier.train(training_set)
print(""MNB_classifier accuracy percent : "",(nltk.classify.accuracy(MNB_classifier,testing_set))*100)
voted_classifier = VoteClassifier(classifier,
                                  MNB_classifier)
def sentiment(text):
    feats = find_features(text)
    return voted_classifier.classify(feats),voted_classifier.confidence(feats)
</code></pre>
",Training and Model Evaluation,trying evaluate confusion matrix roc curve code nltk code working code short review movie want calculate confusion mattrix roc find accuracy unable find roc confusion matrix helpfull anyone help thanks
"Programatically create, train and publish a LUIS model","<p>Can we programatically create, train and publish a LUIS model from C#. Do we have any SDK available? If we want to add a new intent or entity or create a Utterance and book mark this we need to do this via luis.ai can we get this done via C# code?</p>
",Training and Model Evaluation,programatically create train publish luis model programatically create train publish luis model c sdk available want add new intent entity create utterance book mark need via luis ai get done via c code
Difference between adequacy and fluency in ngram,"<blockquote>
  <p>""When 1-gram precision is high, the reference tends to satisfy
  adequacy.</p>
  
  <p>When longer n-gram precision is high, the reference tends to account
  for fluency.""</p>
</blockquote>

<p>What does this mean?</p>
",Training and Model Evaluation,difference adequacy fluency ngram gram precision high reference tends satisfy adequacy longer n gram precision high reference tends account fluency doe mean
Google Colab upload word embeddings,"<p>I'm using Google Colab for my DL model (NLP), I uploaded and imported my training data (screenshot) and now I'd like to pre-train on GloVe word embeddings. If I upload the same way, it will take hours I guess, and even then I'm not sure if it works.</p>

<p>Did anyone come across the same problem?</p>

<p>Thanks</p>

<p><strong>uploading training data</strong>
<a href=""https://i.sstatic.net/W7Sad.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/W7Sad.png"" alt=""screenshot: uploading training data""></a></p>
",Training and Model Evaluation,google colab upload word embeddings using google colab dl model nlp uploaded imported training data screenshot like pre train glove word embeddings upload way take hour guess even sure work anyone come across problem thanks uploading training data
Natural language query preprocessing,"<p>I am trying to implement a natural language query preprocessing module which would, given a query formulated in natural language, extract the keywords from that query and submit it to an Information Retrieval (IR) system.</p>

<p>At first, I thought about using some training set to compute tf-idf values of terms and use these values for estimating the importance of single words. But on second thought, this does not make any sense in this scenario - I only have a training collection but I dont have access to index the IR data. Would it be reasonable to only use the idf value for such estimation? Or maybe another weighting approach?</p>

<p>Could you suggest how to tackle this problem? Usually, the articles about NLP processing that I read address training and test data sets. But what if I only have the query and training data?</p>
",Training and Model Evaluation,natural language query preprocessing trying implement natural language query preprocessing module would given query formulated natural language extract keywords query submit information retrieval ir system first thought using training set compute tf idf value term use value estimating importance single word second thought doe make sense scenario training collection dont access index ir data would reasonable use idf value estimation maybe another weighting approach could suggest tackle problem usually article nlp processing read address training test data set query training data
how to change parameters of fasttext api in a python script,"<p>We have <a href=""https://github.com/facebookresearch/fastText/blob/master/README.md#models"" rel=""nofollow noreferrer"">fasttext commands</a> to run in command prompt</p>

<p>I have cloned the github repository and for example to change parameters of the network for a supervised learning in the command I used are like</p>

<pre><code> ./fasttext supervised -input FT_Race_data.txt -output race_model  -lr 0.4 -epoch 30 -loss hs
</code></pre>

<p>I am changing lr and epoch and loss. I can train and fetch the required output. </p>

<p>For programming in python script, I installed the fasttext library and I tried like </p>

<pre><code>classifier = fasttext.supervised('FT_Race_data.txt','race_model') 
</code></pre>

<p>The model gets trained but the results are not good, In this case, I didn't define any parameters. So I tried like </p>

<pre><code>classifier = fasttext.supervised('FT_Race_data.txt','race_model', 0.4, 30, 'hs')
</code></pre>

<p>The programs run with no error but don't give any result. So I tried like</p>

<pre><code>classifier = fasttext.supervised(input = 'FT_Race_data.txt',output ='race_model', lr = 0.4,epoch= 30,loss = 'hs')
</code></pre>

<p>it gives an error that fasttext takes only two arguments.</p>

<p>How to change parameters in python script like in command prompt to fine tune the supervised learning ? </p>
",Training and Model Evaluation,change parameter fasttext api python script fasttext command run command prompt cloned github repository example change parameter network supervised learning command used like changing lr epoch loss train fetch required output programming python script installed fasttext library tried like model get trained result good case define parameter tried like program run error give result tried like give error fasttext take two argument change parameter python script like command prompt fine tune supervised learning
How to collect RDF triples for a simple knowledge graph?,"<p>When building a knowledge graph, the first step (if I understand it correctly), is to collect structured data, mainly RDF triples written by using some ontology, for example, Schema.org. <strong>Now, what is the best way to collect these RDF triples?</strong></p>

<p>Seems two things we can do. </p>

<ol>
<li><p>Use a crawler to crawls the web content, and for a specific page, search for RDF triples on this page. If we find them, collect them. If not, move on to the next page. </p></li>
<li><p>For the current page, instead of looking for existing RDF triples, use some NLP tools to understand the page content (such as using NELL, see <a href=""http://rtw.ml.cmu.edu/rtw/"" rel=""nofollow noreferrer"">http://rtw.ml.cmu.edu/rtw/</a>).</p></li>
</ol>

<p>Now, is my understanding above (basically/almost) correct? If so, why do we use NLP? why not just rely on the existing RDF triples? Seems like NLP is not as good/reliable as we are hoping… I could be completely wrong.</p>

<h3>Here is another try of asking the same question</h3>

<p>Let us say we want to create RDF triples by using the 3rd method mentioned by @AKSW, i.e., extract RDF triples from some web pages (text). </p>

<p><a href=""https://6thfloor.blogs.nytimes.com/2013/09/10/why-roger-federer-is-the-greatest-of-all-time/"" rel=""nofollow noreferrer"">For example, this page</a>. If you open it and use ""view source"", you can see quite some semantic mark-ups there (using OGP and Schema.org). So my crawler can simply do this: ONLY crawl/parse these mark-ups, and easily change these mark-ups into RDF triples, then declare success, move on to the next page.</p>

<p>So what the crawler has done on this text page is very simple: only collect semantic markups and create RDF triples from these markup. It is simple and efficient.</p>

<p>The other choice, is to use NLP tools to automatically extract structured semantic data from this same text (maybe we are not satisfied with the existing markups). Once we extract the structured information, we then create RDF triples from them. This is obviously a much harder thing to do, and we are not sure about its accuracy either (?).</p>

<p><strong>What is the best practice here, what is the pros/cons here?</strong> I would prefer the easy/simple way - simply collect the existing markup and change that into RDF content, instead of using NLP tools.</p>

<p>And I am not sure how many people would agree with this? And is this the best practice? Or, it is simply a question of how far our requirements lead us?</p>
",Training and Model Evaluation,collect rdf triple simple knowledge graph building knowledge graph first step understand correctly collect structured data mainly rdf triple written using ontology example schema org best way collect rdf triple seems two thing use crawler crawl web content specific page search rdf triple page find collect move next page current page instead looking existing rdf triple use nlp tool understand page content using nell see understanding basically almost correct use nlp rely existing rdf triple seems like nlp good reliable hoping could completely wrong another try asking question let u say want create rdf triple using rd method mentioned aksw e extract rdf triple web page text example page open use view source see quite semantic mark ups using ogp schema org crawler simply crawl parse mark ups easily change mark ups rdf triple declare success move next page crawler ha done text page simple collect semantic markup create rdf triple markup simple efficient choice use nlp tool automatically extract structured semantic data text maybe satisfied existing markup extract structured information create rdf triple obviously much harder thing sure accuracy either best practice pro con would prefer easy simple way simply collect existing markup change rdf content instead using nlp tool sure many people would agree best practice simply question far requirement lead u
How does CountVectorizer max_features process ngrams with the same frequencies?,"<p>I have a question about the CountVectorizer and TfidfVectorizer.</p>

<p>It is unclear to me how the ngrams are selected with the same frequencies in max_features. If we say max_features = 10000 and 100 ngrams in a corpus with the same frequencies on the boarder, how does CountVectorizer separate what ngram will be in the features and what ones will not? The toy example, we have a corpus with eight unique words. Words ""jeans"" and ""cat"" have the same freq 1. We take max_features=7. Why ""cat"" is appear in the features and ""jeans"" is not, but not vice versa?  </p>

<pre><code>data = ['gpu processor cpu performance',
        'gpu performance ram computer computer',
        'cpu computer ram processor jeans processor cat']

cv = CountVectorizer(ngram_range=(1, 1), max_features=7)
cv_fit = cv.fit_transform(data).toarray()
cv.vocabulary_

out:
{'cat': 0,
 'computer': 1,
 'cpu': 2,
 'gpu': 3,
 'performance': 4,
 'processor': 5,
 'ram': 6}
</code></pre>
",Training and Model Evaluation,doe countvectorizer max feature process ngrams frequency question countvectorizer tfidfvectorizer unclear ngrams selected frequency max feature say max feature ngrams corpus frequency boarder doe countvectorizer separate ngram feature one toy example corpus eight unique word word jean cat freq take max feature cat appear feature jean vice versa
Error in reshaping input tokenized text predicting the sentiments in a lstm rnn,"<p>I am new to neural network and have been learning it's application in the field of text analytics, so i have used a lstm rnn for the application in python.</p>

<p>After training the model on a dataset of dimension 20,000*1 (2000-being the text and ,1-being the sentiment of the text) i got a good accuracy of 99%, after which i validated the model which was working fine(using the model.predict()function).</p>

<p>Now just to test my model i have been trying to give random text inputs either from a dataframe or variables containing some text but i always landup with the error of reshaping the array , where it is required that the input to the rnn model be of the dimension (1,30).</p>

<p>But when i re-input the training data into the model for prediction , the model works absolutely fine , why is this happening?</p>

<p><a href=""https://i.sstatic.net/yv7r2.jpg"" rel=""nofollow noreferrer"">link for the screenshot of error</a></p>

<p><img src=""https://i.sstatic.net/yv7r2.jpg""></p>

<p><a href=""https://i.sstatic.net/OusDm.jpg"" rel=""nofollow noreferrer"">link for image of model summary</a></p>

<p><img src=""https://i.sstatic.net/OusDm.jpg""></p>

<p><a href=""https://i.sstatic.net/dHG42.jpg"" rel=""nofollow noreferrer"">training data</a></p>

<p><img src=""https://i.sstatic.net/dHG42.jpg""></p>

<p>I am just stuck here and any kind of suggestion  will help me learning more about rnn, i am attaching the error and the rnn model code with this request.</p>

<p>Thank You</p>

<p>Regards</p>

<p>Tushar Upadhyay</p>

<pre><code>    import numpy as np 
    import pandas as pd 
    import keras
    import sklearn

    from sklearn.feature_extraction.text import CountVectorizer
    from keras.preprocessing.text import Tokenizer
    from keras.preprocessing.sequence import pad_sequences
    from keras.models import Sequential
    from keras.layers import Dense, Embedding, LSTM
     from sklearn.model_selection import train_test_split
    from keras.utils.np_utils import to_categorical
    import re


    data=pd.read_csv('..../twitter_tushar_data.csv')
    max_fatures = 4000
    tokenizer = Tokenizer(num_words=max_fatures, split=' ')
    tokenizer.fit_on_texts(data['tweetText'].values)
    X = tokenizer.texts_to_sequences(data['tweetText'].values)
    X = pad_sequences(X)


    embed_dim = 128
    lstm_out = 196
    model = Sequential()
    keras.layers.core.SpatialDropout1D(0.2) #used to avoid overfitting
    model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))
    model.add(LSTM(196, recurrent_dropout=0.2, dropout=0.2))
   model.add(Dense(2,activation='softmax'))
   model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics 
   = ['accuracy'])
   print(model.summary())
   #splitting data in training and testing parts

   Y = pd.get_dummies(data['SA']).values
   X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 
   0.30, random_state = 42)
   print(X_train.shape,Y_train.shape)
   print(X_test.shape,Y_test.shape)
   batch_size = 128
   model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 
   2)


   validation_size = 3500

   X_validate = X_test[-validation_size:]
   Y_validate = Y_test[-validation_size:]
   X_test = X_test[:-validation_size]
   Y_test = Y_test[:-validation_size]
   score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = 128)
   print(""score: %.2f"" % (score))
   print(""acc: %.2f"" % (acc))


   pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0
   for x in range(len(X_validate)):
   result = 
 model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose 
 = 2)[0]
 if np.argmax(result) == np.argmax(Y_validate[x]):
    if np.argmax(Y_validate[x]) == 0:
        neg_correct += 1
    else:
        pos_correct += 1

if np.argmax(Y_validate[x]) == 0:
    neg_cnt += 1
else:
    pos_cnt += 1
print(""pos_acc"", pos_correct/pos_cnt*100, ""%"")
print(""neg_acc"", neg_correct/neg_cnt*100, ""%"")
</code></pre>
",Training and Model Evaluation,error reshaping input tokenized text predicting sentiment lstm rnn new neural network learning application field text analytics used lstm rnn application python training model dataset dimension text sentiment text got good accuracy validated model wa working fine using model predict function test model trying give random text input either dataframe variable containing text always landup error reshaping array required input rnn model dimension input training data model prediction model work absolutely fine happening link screenshot error link image model summary training data img src p stuck kind suggestion help learning rnn attaching error rnn model code request thank regard tushar upadhyay
How to combine two pre-trained Word2Vec models?,"<p>I successfully followed deeplearning4j.org tutorial on Word2Vec, so I am able to load already trained model or train a new one based on some raw text (more specifically, I am using <code>GoogleNews-vectors-negative300</code> and <code>Emoji2Vec</code> pre-trained model). </p>

<p>However, I would like to combine these two above models for the following reason: Having a sentence (for example, a comment from Instagram or Twitter, which consists of emoji), I want to identify the emoji in the sentence and then map it to the word it is related to. In order to do that, I was planning to iterate over all the words in the sentence and calculate the closeness (how near the emoji and the word are located in the vector space).</p>

<p>I <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/word2vec/Word2VecUptrainingExample.java"" rel=""nofollow noreferrer"">found the code</a> how to uptrain the already existing model. However, it is mentioned that new words are not added in this case and only weights for the existing words will be updated based on a new text corpus. </p>

<p>I would appreciate any help or ideas on the problem I have. Thanks in advance!</p>
",Training and Model Evaluation,combine two pre trained word vec model successfully followed deeplearning j org tutorial word vec able load already trained model train new one based raw text specifically using pre trained model however would like combine two model following reason sentence example comment instagram twitter consists emoji want identify emoji sentence map word related order wa planning iterate word sentence calculate closeness near emoji word located vector space found code uptrain already existing model however mentioned new word added case weight existing word updated based new text corpus would appreciate help idea problem thanks advance
Updating the scikit multinomial classifier,"<p>I am trying to update the scikit multinomial classifier with the new training data. Here is what i had tried</p>

<pre><code>from sklearn.feature_extraction.text import HashingVectorizer
import numpy as np
from sklearn.naive_bayes import MultinomialNB

# Training with first training set
targets = ['education','film','sports','laptops','phones']
x = [""football is the sport"",""gravity is the movie"", ""education is imporatant"",""lenovo is a laptop"",""android phones""]
y = np.array([2,1,0,3,4])
clf = MultinomialNB()
vectorizer = HashingVectorizer(stop_words='english', non_negative=True,
                                   n_features=32*2)
X_train = vectorizer.transform(x)
clf.partial_fit(X_train, y, classes=[0,1,2,3,4])

#Testing with First training set
test_data = [""android"",""lenovo"",""Transformers""]
X_test = vectorizer.transform(test_data)
print ""Using Initial classifier""
pred = clf.predict(X_test)
for doc, category in zip(test_data, pred):
    print('%r =&gt; %s' % (doc, targets[category]))

# Training with updated training set
x = [""cricket"", ""Transformers is a film"",""which college""]
y = np.array([2,1,0])
X_train = vectorizer.transform(x)
clf.partial_fit(X_train, y)

# Testing with the updated trainign set
test_data = [""android"",""lenovo"",""Transformers""]
X_test = vectorizer.transform(test_data)
print ""\nUsing Updatable classifiers""
pred = clf.predict(X_test)
for doc, category in zip(test_data, pred):
    print('%r =&gt; %s' % (doc, targets[category]))
</code></pre>

<p>The Output to this is</p>

<pre><code>Using Initial classifier
'android' =&gt; phones
'lenovo' =&gt; laptops
'Transformers' =&gt; education

Using Updatable classifiers
'android' =&gt; sports
'lenovo' =&gt; education
'Transformers' =&gt; film
</code></pre>

<p>I have two questions onto this -></p>

<p>1) the category for ""lenovo"" is coming wrong because training data for that category is not included while updating classifier. Is there any solution to avoid this. As I dont want to provide training data for each category every time I update the classifier. So It should work even if I provide the data for single category while updating. </p>

<p>2) how can I add new categories to the existing classifier. Like if I want a new category like ""health"" to the existing classifier. Then is there any way to do that.</p>

<p>Help is appreciated. Thanks</p>
",Training and Model Evaluation,updating scikit multinomial classifier trying update scikit multinomial classifier new training data tried output two question onto category lenovo coming wrong training data category included updating classifier solution avoid dont want provide training data category every time update classifier work even provide data single category updating add new category existing classifier like want new category like health existing classifier way help appreciated thanks
Use pretrained models to further train current corpus,"<p>Is it possible to leverage the pretrained model e.g. GLOVE and use it to further train a corpus. </p>

<p>Any example will be very helpful.</p>
",Training and Model Evaluation,use pretrained model train current corpus possible leverage pretrained model e g glove use train corpus example helpful
Use pretrained models to further train current corpus,"<p>Is it possible to leverage the pretrained model e.g. GLOVE and use it to further train a corpus. </p>

<p>Any example will be very helpful.</p>
",Training and Model Evaluation,use pretrained model train current corpus possible leverage pretrained model e g glove use train corpus example helpful
What happens if I just &#39;vectorizer.transform(phrase)&#39; without fitting it?,"<p>Some things are confusing to me when it comes to vectorizing phrases and put them into a matrix form.</p>

<p>When you import either CountVectorizer or TfidfVectorizer, </p>

<p>what are the differences of .fit &amp; .transform &amp; .fit_transform  functions?</p>

<p><strong>I know '.fit' will learn the ngrams, split into ngrams.</strong></p>

<p><strong>'.transform' will put it into a phrase x ngram matrix.</strong></p>

<p><strong>'.fit_transform' works as a combination of .fit &amp; .transform</strong></p>

<p>If this is the case, <strong>what happens if I just vectorize.transform(phrase) without fitting it?</strong></p>

<p>I saw this tutorial set up which fit &amp; transform the train data but for the test data, it only does 'transform' operation for the prediction. </p>

<p>Thanks in advance everyone.</p>
",Training and Model Evaluation,happens vectorizer transform phrase without fitting thing confusing come vectorizing phrase put matrix form import either countvectorizer tfidfvectorizer difference fit transform fit transform function know fit learn ngrams split ngrams transform put phrase x ngram matrix fit transform work combination fit transform case happens vectorize transform phrase without fitting saw tutorial set fit transform train data test data doe transform operation prediction thanks advance everyone
Doc2vec: Only 10 docvecs in gensim doc2vec model?,"<p>I used gensim fit a doc2vec model, with tagged document (length>10) as training data. The target is to get doc vectors of all training docs, but only 10 vectors can be found in model.docvecs.</p>

<p>The example of training data (length>10)</p>

<pre><code>docs = ['This is a sentence', 'This is another sentence', ....]
</code></pre>

<p>with some pre-treatment</p>

<pre><code>doc_=[d.strip().split("" "") for d in doc]
doc_tagged = []
for i in range(len(doc_)):
  tagd = TaggedDocument(doc_[i],str(i))
  doc_tagged.append(tagd)
</code></pre>

<p>tagged docs</p>

<pre><code>TaggedDocument(words=array(['a', 'b', 'c', ..., ],
  dtype='&lt;U32'), tags='117')
</code></pre>

<p>fit a doc2vec model</p>

<pre><code>model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)
model.build_vocab(doc_tagged)
model.train(doc_tagged, total_examples= model.corpus_count, epochs= model.iter)
</code></pre>

<p>then i get the final model</p>

<pre><code>len(model.docvecs)
</code></pre>

<p>the result is 10...</p>

<p>I tried other datasets (length>100, 1000) and got same result of <code>len(model.docvecs)</code>.
So, my question is:
How to use model.docvecs to get full vectors? (without using <code>model.infer_vector</code>)
Is <code>model.docvecs</code> designed to provide all training docvecs?</p>
",Training and Model Evaluation,doc vec docvecs gensim doc vec model used gensim fit doc vec model tagged document length training data target get doc vector training doc vector found model docvecs example training data length pre treatment tagged doc fit doc vec model get final model result tried datasets length got result question use model docvecs get full vector without using designed provide training docvecs
Reducing false positive in CNN (Conv1D) text classification model,"<p>I created a char-based CNN model for text classification on <code>keras</code> + <code>tensorflow</code> - mainly using Conv1D, mainly based on:</p>

<p><a href=""http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"" rel=""nofollow noreferrer"">http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/</a></p>

<p>The model is performing very good with 80%+ accuracy on test data set. However I'm having problem with false positive. One of the reason could be that the final layer is a <code>Dense</code> layer with <code>softmax</code> activation function. </p>

<p>To give an idea of how the model is performing, I train the model with data set with 31 classes with 1021 samples, the performance is ~85% on 25% test data set</p>

<p>However if you include false negative the performance is pretty bad (I didn't run another test data with false negative since it's pretty obvious just testing by hand) - every input has a corresponding prediction. For example a sentence <code>acasklncasdjsandjas</code> can result in a class <code>ask_promotion</code>.</p>

<p>Are there any best practice on how to deal with false positive in this case?
My idea is to:</p>

<ol>
<li>Implement a <code>noise</code> class where samples are just a set of totally random text. However this doesn't seem to help since the noise doesn't contain any pattern thus it would be difficult to train the model</li>
<li>Replace softmax with something that doesn't require all output probability to 1 so small values can stay small regardless of other values. I did some research on this but there's not much information on changing the activation function for this specific case</li>
</ol>
",Training and Model Evaluation,reducing false positive cnn conv text classification model created char based cnn model text classification mainly using conv mainly based model performing good accuracy test data set however problem false positive one reason could final layer layer activation function give idea model performing train model data set class sample performance test data set however include false negative performance pretty bad run another test data false negative since pretty obvious testing hand every input ha corresponding prediction example sentence result class best practice deal false positive case idea implement class sample set totally random text however seem help since noise contain pattern thus would difficult train model replace softmax something require output probability small value stay small regardless value research much information changing activation function specific case
Labeled dataset of product review spam,"<p>I've edited my question below after comment from @Tchotchke:
I am trying to train an NLP model to classify fake and authentic product reviews. For the training, I need a labeled dataset of such reviews. There are product review datasets published by researchers, but they aren't labeled. And some datasets (like the one in <a href=""https://stackoverflow.com/questions/10588912/fake-reviews-datasets"">Fake reviews datasets</a>) is for hotel reviews, and thus does not represent the wide range of language features that can exist for reviews of products like shoes, clothes, furniture, electronics, etc. that are sold on typical shopping portals like Amazon, Walmart, etc. Can someone please suggest a way to generate such a labeled dataset rather quickly or find one already available?</p>
",Training and Model Evaluation,labeled dataset product review spam edited question comment tchotchke trying train nlp model classify fake authentic product review training need labeled dataset review product review datasets published researcher labeled datasets like one href review datasets hotel review thus doe represent wide range language feature exist review product like shoe clothes furniture electronics etc sold typical shopping portal like amazon walmart etc someone please suggest way generate labeled dataset rather quickly find one already available
Inner workings of Keras LSTM,"<p>I am working on a multi-class classification task: the goal is to identify what is the correct language of origin of a certain surname. For this, I am using a Keras LSTM. 
So far, I have only worked with PyTorch and I am very surprised by the ""black box"" character of Keras. For this classification task, my understanding is that I need to retrieve the output of the last time step for a given input sequence in the LSTM and then apply the softmax on it to get the probability distribution over all classes.
Interestingly, without me explicitly defining to do so, the LSTM seems to automatically do the right thing and chooses the last time step's output and not e.g. the hidden state to apply the softmax on (good training &amp; validation results so far). How is that possible? Does the choice of the appropriate loss function categorical_crossentropy indicate to the model that is should use the last time step's output to do the classification?</p>

<p>Code:</p>

<pre><code>model = Sequential()

model.add(Dense(100, input_shape=(max_len, len(alphabet)), kernel_regularizer=regularizers.l2(0.00001)))

model.add(Dropout(0.85))

model.add(LSTM(100, input_shape=(100,))) 

model.add(Dropout(0.85))

model.add(Dense(num_output_classes, activation='softmax'))

adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=1e-6)

model.compile(loss='categorical_crossentropy',
              optimizer=adam,
              metrics=['accuracy'])

history = model.fit(train_data, train_labels,
          epochs=5000,
          batch_size=num_train_examples,
          validation_data = (valid_data, valid_labels))
</code></pre>
",Training and Model Evaluation,inner working kera lstm working multi class classification task goal identify correct language origin certain surname using kera lstm far worked pytorch surprised black box character kera classification task understanding need retrieve output last time step given input sequence lstm apply softmax get probability distribution class interestingly without explicitly defining lstm seems automatically right thing chooses last time step output e g hidden state apply softmax good training validation result far possible doe choice appropriate loss function categorical crossentropy indicate model use last time step output classification code
Reduce Google&#39;s Word2Vec model with Gensim,"<p>Loading the complete pre-trained word2vec model by <a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"" rel=""noreferrer"">Google</a> is time intensive and tedious, therefore I was wondering if there is a chance to remove words below a certain frequency to bring the <code>vocab</code> count down to e.g. 200k words.</p>

<p>I found Word2Vec methods in the <code>gensim</code> package to determine the word frequency and to re-save the model again, but I am not sure how to <code>pop</code>/<code>remove</code> vocab from the pre-trained model before saving it again. I couldn't find any hint in the <code>KeyedVector class</code> and the <code>Word2Vec class</code> for such an operation?</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py</a>
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py</a></p>

<p><strong>How can I select a subset of the vocabulary of the pre-trained word2vec model?</strong></p>
",Training and Model Evaluation,reduce google word vec model gensim loading complete pre trained word vec model google time intensive tedious therefore wa wondering chance remove word certain frequency bring count e g k word found word vec method package determine word frequency save model sure vocab pre trained model saving find hint operation select subset vocabulary pre trained word vec model
How to detect multi set words OpenNLP,"<p>I'm doing NER using Java OpenNLP and I'm not sure how can I detect multiple words (eg. New York, Bruno Mars, Hong Kong) by using the custom model I have trained.</p>

<p>My training data do cover multi-word spans:</p>

<pre><code>&lt;START:place&gt; Hong Kong &lt;END&gt; ... &lt;START:person&gt; Putin &lt;END&gt;
</code></pre>

<p>I'm pretty sure my trained model and training data are working good. It's just that I do not know how to get the multi-word set. Here is what I did </p>

<pre><code>    // testing the model
    NameFinderME nameFinder = new NameFinderME(nameFinderModel);

    String sentence = ""India may US to Japan France so Putin should Hong Kong review Trump"";
    WhitespaceTokenizer whitespaceTokenizer = WhitespaceTokenizer.INSTANCE;

    // Tokenizing the given paragraph
    String tokens[] = whitespaceTokenizer.tokenize(sentence);
    Span nameSpans[] = nameFinder.find(tokens);
    for (Span s : nameSpans)
        System.out.println(s.toString() + ""  "" + tokens[s.getStart()]);
</code></pre>

<p>And here is what I get:</p>

<pre><code>[0..1) place  India
[0..1) place  US
[0..1) place  Japan
[0..1) place  France
[0..1) person  Putin
[0..1) place  Hong
[0..1) person  Trump
</code></pre>

<p>But I want to get [0..1) place  Hong Kong instead of splitting them into two categories.</p>

<p>Thanks.</p>
",Training and Model Evaluation,detect multi set word opennlp ner using java opennlp sure detect multiple word eg new york bruno mar hong kong using custom model trained training data cover multi word span pretty sure trained model training data working good know get multi word set get want get place hong kong instead splitting two category thanks
load Doc2Vec model and get new sentence&#39;s vectors for test,"<p>I have read lots of examples regarding doc2vec, but I couldn't find any answer. Like a real example, I want to build a model with doc2vec and then train it with some ML models. after that, how can I get the vector of a raw string with the exact trained Doc2vec model? because I need to predict with my ML model with the same size and logical vector</p>
",Training and Model Evaluation,load doc vec model get new sentence vector test read lot example regarding doc vec find answer like real example want build model doc vec train ml model get vector raw string exact trained doc vec model need predict ml model size logical vector
Is this a correct reimplementation of Pytorch Seq2Seq model?,"<p>I made a code that sort of change the tutorial script of seq2seq provided by Pytorch. Here’s the model:</p>

<pre><code>class Seq2Seq(nn.Module):
def __init__(self, encoder, batch_size, vocab_size, input_size, output_size, hidden_dim, embedding_dim, n_layers=2, dropout_p=0.5):
    super(Seq2Seq, self).__init__()

    self.hidden_dim = hidden_dim
    self.batch_size = batch_size
    self.input_length = input_size
    self.output_length = output_size
    self.vocab_size = vocab_size

    self.encoder = encoder
    self.dropout = nn.Dropout(dropout_p)
    self.selu = nn.SELU()
    self.decoder_embeddings = nn.Embedding(vocab_size, hidden_dim)
    self.decoder_gru = nn.GRU(hidden_dim, hidden_dim)
    self.out = nn.Linear(hidden_dim, vocab_size)
    self.softmax = nn.LogSoftmax()

def decode(self, SOS_token, encoder_hidden, target_output, teacher_forcing_ratio=0.8):
    decoder_output_full = autograd.Variable(torch.zeros(self.output_length, self.batch_size, self.vocab_size))
    decoder_output_full = decoder_output_full.cuda() if use_cuda else decoder_output_full
    target = target_output.permute(1,0)

    use_teacher_forcing = True if random.random() &lt; teacher_forcing_ratio else False

    for idx in range(self.output_length):
        if idx == 0:
            decoder_input = SOS_token
            decoder_hidden = encoder_hidden.unsqueeze(0)
        output = self.decoder_embeddings(decoder_input).view(1, self.batch_size, -1)
        output = self.dropout(output)

        output = self.selu(output)

        if use_teacher_forcing:
            decoder_output, decoder_hidden = self.decoder_gru(output, decoder_hidden)
            temp = 1
            out = self.out(decoder_output[0])
            out = out + sample_gumbel(out.shape)
            decoder_output = F.softmax(out / temp, dim=1)
            # decoder_output = (self.decoder_embeddings.weight * decoder_output.unsqueeze(1)).sum(0).view(1, 1, -1)
            decoder_output_full[idx, :, :] = decoder_output
            decoder_input = target[idx-1]  # Teacher forcing

        else:
            decoder_output, decoder_hidden = self.decoder_gru(output, decoder_hidden)
            temp = 1
            out = self.out(decoder_output[0])
            out = out + sample_gumbel(out.shape)
            decoder_output = F.softmax(out / temp, dim=1)
            # decoder_output = (self.decoder_embeddings.weight * decoder_output.unsqueeze(1)).sum(0).view(1, 1, -1)
            topv, topi = decoder_output.data.topk(1)
            # print topi
            ni = topi
            # decoder_input_v = autograd.Variable(torch.LongTensor([[ni]]))
            decoder_input = autograd.Variable(ni)
            # decoder_input = decoder_input.cuda() if use_cuda else decoder_input
            # print decoder_input
            decoder_output_full[idx, :, :] = decoder_output

    decoder_output_full = decoder_output_full.permute(1,0,2)

    # gen_output = self.softmax(self.out(decoder_output_full))

    return decoder_output_full

def forward(self, input, target_output, teacher_forcing_ratio=0.8):
    encoder_feat, _ = self.encoder(input)

    SOS_token = np.zeros((self.batch_size,1), dtype=np.int32)
    SOS_token = torch.LongTensor(SOS_token.tolist())
    SOS_token = autograd.Variable(SOS_token)
    if use_cuda:
        SOS_token = SOS_token.cuda(gpu)

    gen_output = self.decode(SOS_token, encoder_feat, target_output, teacher_forcing_ratio)

    return gen_output

def initHidden(self):
    result = autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))
    if use_cuda:
        return result.cuda()
    else:
        return result
</code></pre>

<p>The way I calculate the NLL loss is by creating one whole sequence of output first and compare it with the target output. Here’s the loss function:</p>

<pre><code>class batchNLLLoss(nn.Module):
def __init__(self):
    super(batchNLLLoss, self).__init__()

def forward(self, synt, target, claim_length=20):
    loss_fn = nn.NLLLoss()

    loss = 0

    for i in range(synt.shape[0]):
        for j in range(claim_length):
            loss += loss_fn(synt[i][j].unsqueeze(0), target[i][j])

    return loss
</code></pre>

<p>The current problem is the loss value is really small and seems like the network learns nothing (the output is the same word repeated again and again). Any thought about this? Thanks in advance!</p>
",Training and Model Evaluation,correct reimplementation pytorch seq seq model made code sort change tutorial script seq seq provided pytorch model way calculate nll loss creating one whole sequence output first compare target output loss function current problem loss value really small seems like network learns nothing output word repeated thought thanks advance
Date Extraction from Text,"<p>I am trying to use Stanford NLP tool to extract dates ( 8/11/2012 ) form text.</p>

<p>Here's <a href=""http://nlp.stanford.edu:8080/ner/process/"" rel=""noreferrer"">a link</a>! for the demo of this tool </p>

<p>Can u help me in how to train the classifier to identify date ( 8/11/2012 ).</p>

<p>I tried using training data as</p>

<p>Woodhouse   PERS
8/18/2012 Date
,   O
handsome    O </p>

<p>but does not work for same test data .</p>
",Training and Model Evaluation,date extraction text trying use stanford nlp tool extract date form text link demo tool u help train classifier identify date tried using training data woodhouse pers date handsome doe work test data
How to define the loss function or how to optimize if the target is a set?,"<p>I use a full-connected network to get the whole words distribution from the last state of an encoder.</p>

<p>For example, there are 5 words in the vocabulary.</p>

<pre><code>P = [0.1, 0.1, 0.2, 0.2, 0,4]
</code></pre>

<p>And the ground truth is a words' set for this train data.</p>

<p>I sample 3 words from the 5 words and if the target set contains the 3 words , then I want the probability of the 3 words in <code>P</code> increase, for this state. </p>

<p>If one of the 3 word is not in the target set, then I want the probability of the word in <code>P</code> decrease, for this state.</p>

<p>So I wrote these code:</p>

<pre><code>reward = [0,0,0]
</code></pre>

<p>Suppose the first 3 words are sampled from <code>P</code>, and only the first 2 of the 3 words are in the target set. And the third word is not in the target set. Then</p>

<pre><code>reward = [1,1,-1]
</code></pre>

<p>Then I compute the negative sum and dot product of <code>reward</code> and sampled 3 <code>P2=[0.1, 0.1, 0.2]</code> as the loss</p>

<pre><code>loss = -sum(reward * P2.log())
</code></pre>

<p>But I fail to get the result: The top probability words can be selected from the vocabulary for every state.</p>
",Training and Model Evaluation,define loss function optimize target set use full connected network get whole word distribution last state encoder example word vocabulary ground truth word set train data sample word word target set contains word want probability word increase state one word target set want probability word decrease state wrote code suppose first word sampled first word target set third word target set compute negative sum dot product sampled loss fail get result top probability word selected vocabulary every state
R: find ngram using dfm when there are multiple sentences in one document,"<p>I have a big dataset (>1 million rows) and each row is a multi-sentence  text. For example following is a sample of 2 rows:</p>

<pre><code>mydat &lt;- data.frame(text=c('I like apple. Me too','One two. Thank you'),stringsAsFactors = F)
</code></pre>

<p>What I was trying to do is extracting the bigram terms in each row (the ""."" will be able to separate ngram terms). If I simply use the dfm function:</p>

<pre><code>mydfm  = dfm(mydat$text,toLower = T,removePunct = F,ngrams=2)
dtm = as.DocumentTermMatrix(mydfm)
txt_data = as.data.frame(as.matrix(dtm))
</code></pre>

<p>These are the terms I got:</p>

<pre><code>""i_like""     ""like_apple"" ""apple_.""    ""._me""       ""me_too""     ""one_two""    ""two_.""      ""._thank""    ""thank_you"" 
</code></pre>

<p>These are What I expect, basically ""."" is skipped and used to separate the terms:</p>

<pre><code>""i_like""     ""like_apple""  ""me_too""     ""one_two""    ""thank_you"" 
</code></pre>

<p>Believe writing slow loops can solve this as well but given it is a huge dataset I would prefer efficient ways similar to the dfm() in quanteda to solve this. Any suggestions would be appreciated!</p>
",Training and Model Evaluation,r find ngram using dfm multiple sentence one document big dataset million row row multi sentence text example following sample row wa trying extracting bigram term row able separate ngram term simply use dfm function term got expect basically skipped used separate term believe writing slow loop solve well given huge dataset would prefer efficient way similar dfm quanteda solve suggestion would appreciated
How to handle Naive Bayes Classifier when keywords are not present in training set,"<p>I am trying to implement a simple <code>Naive Bayes Classifier</code>, On training I observed that if keywords (prediction) belongs to both class equally then classifier assigns equal probability to both the classes and if the keywords (prediction) are not present in the training data then also it assigns same probability to both the class.</p>

<p>It is difficult to distinguish between these 2 scenarios. I believe this is happening because of Laplace smoothing of 1 and the probability 0.5 in case 3 is because of a probability of class but I am not sure. Can I do a certain trick to ensure that if keywords are not present in the training data then classifier assigns a probability of zero. As the training data is less is it possible or I should look for some another option for this scenario.</p>

<blockquote>
  <p>Fruit probability: 0.50 Veggie probability: 0.50</p>
  
  <p>Fruit probability: 0.50 Veggie probability: 0.50</p>
  
  <p>Fruit probability: 0.50 Veggie probability: 0.50</p>
</blockquote>

<p>Code</p>

<pre><code>from nltk.classify import NaiveBayesClassifier, accuracy

dataFruits = ['Apple', 'Banana', 'Cherry', 'Grape', 'Guava', 
              'Lemon', 'Mangos', 'Orange', 'Strawberry', 'Watermelon']

dataVeggies = ['Potato', 'Spinach', 'Carrot', 'Onion', 'Cabbage', 
               'Broccoli', 'Tomato', 'Pea', 'Cucumber', 'Eggplant']

def basket_features(basket): 
    basket_items = set(basket) 
    features = {}
    for item in allFeatures:
        features['contains({})'.format(item)] = (item in basket_items)
    return features

def test(basket):
    lst = basket_features(basket)
    prob_dist = classifier.prob_classify(lst)
    print('\nFruit probability: {:.2f}\tVeggie probability: {:.2f}'.format(prob_dist.prob('fruit'), prob_dist.prob('veggie')))

allFeatures = dataFruits + dataVeggies

class1= [(basket_features([item]), 'fruit') for item in dataFruits]
class2 = [(basket_features([item]), 'veggie') for item in dataVeggies]

train_set = class1[:] + class2

# Train
classifier = NaiveBayesClassifier.train(train_set)

# Predict
test(['Apple', 'Banana', 'Potato', 'Spinach'])
test(['Apple', 'Banana', 'Potato', 'Spinach', 'Strawberry', 'Pea'])
test(['Hello', 'World'])
</code></pre>
",Training and Model Evaluation,handle naive bayes classifier keywords present training set trying implement simple training observed keywords prediction belongs class equally classifier assigns equal probability class keywords prediction present training data also assigns probability class difficult distinguish scenario believe happening laplace smoothing probability case probability class sure certain trick ensure keywords present training data classifier assigns probability zero training data le possible look another option scenario fruit probability veggie probability fruit probability veggie probability fruit probability veggie probability code
openNLP - Name Finder Training for Addresses,"<p>I am trying to isolate postal addresses from CVs (curriculum vitae).  The CVs are from many different countries so have no standard layout, formats, rhyme or reason to the addresses.</p>

<p>I have my raw data which has been segmented into sentences and tokens and is ready for markup.</p>

<p>Questions:</p>

<p>Whist City/town is of primary interest to me, should I mark up the entire address for best results?</p>

<pre><code>eg blah blah blah &lt;START:location&gt;1 Stack Avenue, London, SE1 KTB&lt;END&gt; blah blah
eg blah blah blah &lt;START:location&gt;Hoch Strasse 21, Berlin 17009, Germany&lt;END&gt; blah blah
</code></pre>

<p>Given that the address I seek mainly appear in the top quarter of a CV, should I trim the training data to that 25% and do the same with the live data or will I get better results by keeping the documents as a whole and just tagging the bit I need?</p>

<p>Finally any ideas on the level of success I'm likely to have finding addresses from none structured documents?</p>

<p>Advice, help and alternative methods greatly appreciated.  </p>
",Training and Model Evaluation,opennlp name finder training address trying isolate postal address cv curriculum vitae cv many different country standard layout format rhyme reason address raw data ha segmented sentence token ready markup question whist city town primary interest mark entire address best result given address seek mainly appear top quarter cv trim training data live data get better result keeping document whole tagging bit need finally idea level success likely finding address none structured document advice help alternative method greatly appreciated
K-fold cross validation to python pandas dataframe - NLTK classification,"<p>i want to Use 10-fold cross validation to evaluate a nltk classification model. this is the pandas data framework named: data (there are 10k rows and 10 classes)</p>

<blockquote>
  <p>features: hello_variant, goodbye_variant,wh_question,yesNo_question,
  conjuction_start, No_of_tokens</p>
</blockquote>

<p><a href=""https://i.sstatic.net/YarQK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YarQK.jpg"" alt=""enter image description here""></a></p>

<p>i tried below code. but it gives an error</p>

<pre><code>extract_features = data.drop(['class'],axis=1)
documents = data['class']

import nltk
from sklearn import cross_validation
training_set = nltk.classify.apply_features(extract_features, documents)
cv = cross_validation.KFold(len(training_set), n_folds=10,  shuffle=False, random_state=None)

for traincv, testcv in cv:
    classifier = nltk.NaiveBayesClassifier.train(training_set[traincv[0]:traincv[len(traincv)-1]])
    print 'accuracy:', nltk.classify.util.accuracy(classifier, training_set[testcv[0]:testcv[len(testcv)-1]])
</code></pre>

<p>error:</p>

<pre><code>&gt; --------------------------------------------------------------------------- ValueError                                Traceback (most recent call
&gt; last) &lt;ipython-input-253-2ddaf7264527&gt; in &lt;module&gt;()
&gt;       1 import nltk
&gt;       2 from sklearn import cross_validation
&gt; ----&gt; 3 training_set = nltk.classify.apply_features(extract_features, documents)
&gt;       4 cv = cross_validation.KFold(len(training_set), n_folds=10,  shuffle=False, random_state=None)
&gt;       5 
&gt; 
&gt; C:\Users\SampathR\Anaconda2\envs\dato-env\lib\site-packages\nltk\classify\util.pyc
&gt; in apply_features(feature_func, toks, labeled)
&gt;      60     """"""
&gt;      61     if labeled is None:
&gt; ---&gt; 62         labeled = toks and isinstance(toks[0], (tuple, list))
&gt;      63     if labeled:
&gt;      64         def lazy_func(labeled_token):
&gt; 
&gt; C:\Users\SampathR\Anaconda2\envs\dato-env\lib\site-packages\pandas\core\generic.pyc
&gt; in __nonzero__(self)
&gt;     712         raise ValueError(""The truth value of a {0} is ambiguous. ""
&gt;     713                          ""Use a.empty, a.bool(), a.item(), a.any() or a.all().""
&gt; --&gt; 714                          .format(self.__class__.__name__))
&gt;     715 
&gt;     716     __bool__ = __nonzero__
&gt; 
&gt; ValueError: The truth value of a Series is ambiguous. Use a.empty,
&gt; a.bool(), a.item(), a.any() or a.all().
</code></pre>

<p>further i want to get precision, recall, and F-score of each of the dialog act in the corpus(class), and the accuracy and the confusion matrix of the classifier . is there any method available in NLTK to calculate those? (other than sklearn)</p>
",Training and Model Evaluation,k fold cross validation python panda dataframe nltk classification want use fold cross validation evaluate nltk classification model panda data framework named data k row class feature hello variant variant wh question yesno question conjuction start token tried code give error error want get precision recall f score dialog act corpus class accuracy confusion matrix classifier method available nltk calculate sklearn
Update Word2vec Vectors,"<p>I have a corpus that contains several documents, for example 10 documents. The idea is to compute the similarity between them and combine the most similar ones into one document. So the result may be 4 documents. What I have done so far is that I iterate over the documents and calculate the most two similar documents and combine them into one document and so on until a threshold. I used Word2vec vectors by taking the mean vector for the whole document. The problem is that as i proceed with the iteration, the longer the document the more similar even if its not that similar due to the presence of more words. Any idea on how to approach this problem? </p>

<p>I used google Word2vec model. The reason: The corpus is not big to train a model. </p>

<p>Note : I do not want to use topic modeling for some specifications. Also the documents are really short, more than half of them may be one sentence. </p>

<p>I really appreciate your suggestions. </p>
",Training and Model Evaluation,update word vec vector corpus contains several document example document idea compute similarity combine similar one one document result may document done far iterate document calculate two similar document combine one document threshold used word vec vector taking mean vector whole document problem proceed iteration longer document similar even similar due presence word idea approach problem used google word vec model reason corpus big train model note want use topic modeling specification also document really short half may one sentence really appreciate suggestion
Getting word embeddings for your dataset using training data in glove,"<p>I recently installed gensim and glove in my mac and am trying to get word embedding for textual data I have. However, I'm having trouble finding the right function for it. I've only come across methods to get similarity metrics between two words. How do I train a glove object with data present in the library and use it to obtain embeddings for words in my dataset? Or is there any other library in python to do this? Thanks!</p>
",Training and Model Evaluation,getting word embeddings dataset using training data glove recently installed gensim glove mac trying get word embedding textual data however trouble finding right function come across method get similarity metric two word train glove object data present library use obtain embeddings word dataset library python thanks
how can I find news article dataset for text summarization?,"<p>I want to summarize some news article.I need a dataset. 
There is BBC dataset but the problem is that I cant evaluate my output with others.</p>
",Training and Model Evaluation,find news article dataset text summarization want summarize news article need dataset bbc dataset problem cant evaluate output others
Tensorflow num_classes parameter of nce_loss(),"<p>My understanding of noise contrastive estimation is that we sample some vectors from our word embeddings (the negative sample), and then calculate the log-likelihood of each. Then we want to maximize the difference between the probability of the target word and the log-likelihood of each of the negative sample words (So if I'm correct about this, we want to optimize the loss function so that it gets as close to 1 as possible).</p>

<p>My question is this:</p>

<p>What is the purpose of the <code>num_classes</code> parameters to the <code>nce_loss</code> function? My best guess is that the number of classes is passed in so that Tensorflow knows the size of the distribution from which the negative samples our drawn, but this might not make sense, since we could just infer the size of the distribution from the variable itself. Otherwise, I can't think of a reason for why we would need to know the total possible number of classes, especially if the language model is only outputting k + 1 predictions (negative sample size + 1 for the target word). </p>
",Training and Model Evaluation,tensorflow num class parameter nce loss understanding noise contrastive estimation sample vector word embeddings negative sample calculate log likelihood want maximize difference probability target word log likelihood negative sample word correct want optimize loss function get close possible question purpose parameter function best guess number class passed tensorflow know size distribution negative sample drawn might make sense since could infer size distribution variable otherwise think reason would need know total possible number class especially language model outputting k prediction negative sample size target word
Implement word2vec in Keras,"<p>I would like to implement word2vec algorithm in keras, Is this possible? 
How can I fit the model? Should I use custom loss function?</p>
",Training and Model Evaluation,implement word vec kera would like implement word vec algorithm kera possible fit model use custom loss function
Can I train my classifier multiple times?,"<p>I am building a basic NLP program using <code>nltk</code> and <code>sklearn</code>. I have a large dataset in a database and I am wondering what the best way to train the classifier is. </p>

<p>Is it advisable to download the training data in chunks and pass each chunk to the classifier? Is that even possible, or would I be overwriting what was learned from the previous chunk?</p>

<pre><code>from nltk.classify.scikitlearn import SklearnClassifier
from sklearn.naive_bayes import MultinomialNB

while True:
    training_set, proceed = download_chunk()  # pseudo
    trained = SklearnClassifier(MultinomialNB()).train(training_set)
    if not proceed:
        break
</code></pre>

<p>How is this normally done? I want to avoid keeping the database connection open for too long.</p>
",Training and Model Evaluation,train classifier multiple time building basic nlp program using large dataset database wondering best way train classifier advisable download training data chunk pas chunk classifier even possible would overwriting wa learned previous chunk normally done want avoid keeping database connection open long
Weka - Classifier returns the same distribution for any input,"<p>I'm trying to build a naive bayes classifier for classifying text between two classes. Everything works great in the GUI explorer, but when I try to recreate it in code, I get the same output no matter what input I try to  classify.</p>

<p>Within the code, I get the same evaluation metrics I get within the GUI (81% accuracy), but whenever I try to create a new instance and classify that, I get the same distributions for both classes no matter what input I use.</p>

<p>Below is my code - its in scala, but is pretty straightforward:</p>

<pre><code>//Building the classifier: 
val instances = new Instances(new DataSource(""/my/dataset.arff"").getDataSet)
instances.setClassIndex(3)

val filter = new StringToWordVector
filter.setAttributeIndicesArray( (0 to 2).toArray )
val classifier = new FilteredClassifier
classifier.setFilter(new StringToWordVector(1000000))
classifier.setClassifier(new NaiveBayesMultinomial)
classifier.buildClassifier(trainingSet)

//Evaluation (this prints about 80% accuracy)
val eval = new Evaluation(trainingSet)
eval.evaluateModel(classifier, trainingSet)

println(eval.toSummaryString)

//Attempting to use the classifier:

val atts = new util.ArrayList[Attribute]
atts.add(new Attribute(""sentence"", true))
atts.add(new Attribute(""parts_of_speech"", true))
atts.add(new Attribute(""dependency_graph"", true))
atts.add(new Attribute(""the_shizzle_clazz"", SentenceType.values().map(_.name()).toSeq.asJava ))

val unlabeledInstances = new Instances(""unlabeled"", atts, 1)
unlabeledInstances.setClassIndex( 3 )

val instance = new DenseInstance(4)

unlabeledInstances.add(instance)
instance.setDataset(unlabeledInstances)

instance.setValue(0, parsed.sentence)
instance.setValue(1, parsed.posTagsStr)
instance.setValue(2, parsed.depsGraphStr)

val distrib = classifier.distributionForInstance(unlabeledInstance.firstInstance())

distrib.foreach(println)
</code></pre>

<p>No matter what input I give, the output of distrib is always:</p>

<pre><code>0.44556173367704455
0.5544382663229555
</code></pre>

<p>Any ideas what I'm doing wrong? Would greatly appreciate any help.</p>
",Training and Model Evaluation,weka classifier return distribution input trying build naive bayes classifier classifying text two class everything work great gui explorer try recreate code get output matter input try classify within code get evaluation metric get within gui accuracy whenever try create new instance classify get distribution class matter input use code scala pretty straightforward matter input give output distrib always idea wrong would greatly appreciate help
How does precision and recall work in this situation?,"<p>Consider the following text, which has been manually annotated using IO-style annotation for persons (PER), locations (LOC) and organizations (ORG).</p>

<blockquote>
  <p>Chicago/LOC Mayor/O Rahm/PER Emanuel/PER, a/O former/O White/ORG
  House/ORG aide/O to/O US/LOC Presidents/O Barack/PER Obama/PER and/O
  Bill/PER Clinton/PER, on/O Friday/O joined/O the/O Ready/O For/O
  Hillary/PER group/O that/O is/O urging/O former/O US/LOC Secretary/O
  of/O State/O Hillary/PER Clinton/PER to/O run/O for/O president/O in/O
  2016/O.</p>
</blockquote>

<p>Consider the following feature assignments f(FEATURES,LABEL), which indicate assigning LABEL upon observing FEATURE, where w is the current token, and w-1 is the previous token.</p>

<pre><code>f1(isCapitalized(w), PER)

f2(label(w-1) = PER, PER)

f3(isCapitalized(w), LOC)

f4(lemma(w-1) = ""president"" OR ""mayor"", PER)
</code></pre>

<p>Based on the observed data, compute precision and recall for each of the features above, assuming that each of them is used in isolation in order to assign labels, and each starts with an unlabelled text.</p>

<p>How should I calculate the precision/ recall, in this situation?
Should I consider, for example, Rahm Emanuel as one True Positive feature? Or each token is a true positive feature? Or each token is a false positive as Rahm Emanuel is actually a single true positive feature?</p>
",Training and Model Evaluation,doe precision recall work situation consider following text ha manually annotated using io style annotation person per location loc organization org chicago loc mayor rahm per emanuel per former white org house org aide u loc president barack per obama per bill per clinton per friday joined ready hillary per group urging former u loc secretary state hillary per clinton per run president consider following feature assignment f feature label indicate assigning label upon observing feature w current token w previous token based observed data compute precision recall feature assuming used isolation order assign label start unlabelled text calculate precision recall situation consider example rahm emanuel one true positive feature token true positive feature token false positive rahm emanuel actually single true positive feature
Removing an &quot;empty&quot; character item from a corpus of documents in R?,"<p>I am using the <code>tm</code> and <code>lda</code> packages in R to topic model a corpus of news articles. However, I am getting a ""non-character"" problem represented as <code>""""</code> that is messing up my topics. Here is my workflow:</p>

<pre><code>text &lt;- Corpus(VectorSource(d$text))
newtext &lt;- lapply(text, tolower)
sw &lt;- c(stopwords(""english""), ""ahram"", ""online"", ""egypt"", ""egypts"", ""egyptian"")
newtext &lt;- lapply(newtext, function(x) removePunctuation(x))
newtext &lt;- lapply(newtext, function(x) removeWords(x, sw))
newtext &lt;- lapply(newtext, function(x) removeNumbers(x))
newtext &lt;- lapply(newtext, function(x) stripWhitespace(x))
d$processed &lt;- unlist(newtext)
corpus &lt;- lexicalize(d$processed)
k &lt;- 40
result &lt;-lda.collapsed.gibbs.sampler(corpus$documents, k, corpus$vocab, 500, .02, .05,
compute.log.likelihood = TRUE, trace = 2L)
</code></pre>

<p>Unfortunately, when I train the lda model, everything looks great except the most frequently occurring word is """". I try to remedy this by removing it from the vocab as given below and reestimating the model just as above: </p>

<pre><code>newtext &lt;- lapply(newtext, function(x) removeWords(x, """"))
</code></pre>

<p>But, it's still there, as evidenced by:</p>

<pre><code>str_split(newtext[[1]], "" "")

[[1]]
 [1] """"              ""body""          ""mohamed""       ""hassan""       
 [5] ""cook""          ""found""         ""turkish""       ""search""       
 [9] ""rescue""        ""teams""         ""rescued""       ""hospital""     
[13] ""rescue""        ""teams""         ""continued""     ""search""       
[17] ""missing""       ""body""          ""cook""          ""crew""         
[21] ""wereegyptians"" ""sudanese""      ""syrians""       ""hassan""       
[25] ""cook""          ""cargo""         ""ship""          ""sea""          
[29] ""bright""        ""crashed""       ""thursday""      ""port""         
[33] ""antalya""       ""southern""      ""turkey""        ""vessel""       
[37] ""collided""      ""rocks""         ""port""          ""thursday""     
[41] ""night""         ""result""        ""heavy""         ""winds""        
[45] ""waves""         ""crew""          """"             
</code></pre>

<p>Any suggestions on how to go about removing this? Adding <code>""""</code> to my list of stopwords doesn't help, either.</p>
",Training and Model Evaluation,removing empty character item corpus document r using package r topic model corpus news article however getting non character problem represented messing topic workflow unfortunately train lda model everything look great except frequently occurring word try remedy removing vocab given reestimating model still evidenced suggestion go removing adding list stopwords help either
How to write UD Pipe tagger output to file?,"<p>I have been using <a href=""https://ufal.mff.cuni.cz/udpipe"" rel=""nofollow noreferrer"">UD Pipe</a> to train and tag data in the Hindi Language. </p>

<p>I run the tagger using </p>

<blockquote>
  <p>udpipe --tag model.output hi-ud-test.conllu</p>
</blockquote>

<p>which works perfectly fine and displays the output in command line. How do I write this output in a file? </p>
",Training and Model Evaluation,write ud pipe tagger output file using ud pipe train tag data hindi language run tagger using udpipe tag model output hi ud test conllu work perfectly fine display output command line write output file
How do I calculate Gradient of ranking loss?,"<p>I am trying to understand ranking loss(a.k.a, Maximum Margin Objective Function, MarginRankingLoss ...) based on <a href=""http://cs224d.stanford.edu/lecture_notes/notes3.pdf"" rel=""nofollow noreferrer"">CS 224D: Deep Learning for NLP lecture note</a>.</p>

<p>In this note, the cost is defined as follows: J = (1 + sc − s) </p>

<p>s= f(θ,x),  sc = f(θ,xc), 
x is the input of the correct, and xc is the input of the wrong.</p>

<p>So, s is score of good thing, sc is score of bad thing.</p>

<p>My question is this:
To update the weights, do I have to get  ∂J/ ∂θ or ∂s/∂θ?</p>

<p>I thought I had to do ∂J / ∂θ to update θ.</p>

<p>Therefore, since J = 1 + sc-s, ∂J / ∂θ = ∂sc / ∂θ - ∂s / ∂θ.</p>

<p>So I thought that ∂sc / ∂θ and ∂s / ∂θ should be obtained, respectively.</p>

<p>In a lecture note, however, calculate ∂J / ∂s = -1 and use this value to update the network.</p>

<p>What am I doing wrong?</p>
",Training and Model Evaluation,calculate gradient ranking loss trying understand ranking loss k maximum margin objective function marginrankingloss based c deep learning nlp lecture note note cost defined follows j sc f x sc f xc x input correct xc input wrong score good thing sc score bad thing question update weight get j thought j update therefore since j sc j sc thought sc obtained respectively lecture note however calculate j use value update network wrong
how to learn language model?,"<ol>
<li><p>I'm trying to train a language model with LSTM based on Penn Treebank (PTB) corpus.</p>

<p>I was thinking that I should simply train with every bigram in the corpus so that it could predict the next word given previous words, but then it wouldn't be able to predict next word based on multiple preceding words.    </p>

<p>So what exactly is it to train a language model?    </p></li>
<li><p>In my current implementation, I have batch size=20 and the vocabulary size is 10000, so I have 20 resulting matrices of 10k entries (parameters?) and the loss is calculated by making comparison to 20 ground-truth matrices of 10k entries, where only the index for actual next word is 1 and other entries are zero. Is this a right implementation? I'm getting perplexity of around 2 that hardly changes over iterations, which is definitely not in a right range of what it usually is, say around 100.  </p></li>
</ol>
",Training and Model Evaluation,learn language model trying train language model lstm based penn treebank ptb corpus wa thinking simply train every bigram corpus could predict next word given previous word able predict next word based multiple preceding word exactly train language model current implementation batch size vocabulary size resulting matrix k entry parameter loss calculated making comparison ground truth matrix k entry index actual next word entry zero right implementation getting perplexity around hardly change iteration definitely right range usually say around
Sentence Classification on SNLI dataset,"<p>I am working on SNLI dataset <a href=""https://nlp.stanford.edu/projects/snli/"" rel=""nofollow noreferrer"">here</a> for sentence classification task. This dataset contains two sentences and need to tell whether 2nd sentence <strong>entails</strong>, <strong>contradicts</strong> or is <strong>neutral</strong> to the first sentence.</p>

<p>For classification purpose, I have implemented coccurrence matrix based SVM classification which gives around 70% accuracy. </p>

<p>Can someone please suggest any other NLP related classification strategy i.e using bigrams, or trigrams as I need to come up with some baseline. </p>
",Training and Model Evaluation,sentence classification snli dataset working snli dataset sentence classification task dataset contains two sentence need tell whether nd sentence entail contradicts neutral first sentence classification purpose implemented coccurrence matrix based svm classification give around accuracy someone please suggest nlp related classification strategy e using bigram trigram need come baseline
Stanford tagger making unexpected results,"<p>I'm trying to use Stanford CoreNLP POS tagger on my data.</p>

<p>I have used the automatic generated prop file. I have only changed the Open classes.</p>

<p>I want to know if there is a complete description about other fields in this property like the ""arch"" and it possible values, ""closedClassTagThreshold"", ""minFeatureThresh"", ""curWordMinFeatureThresh"", ""rareWordMinFeatureThresh"", ...</p>

<p>When I run the code to tag a text,It chooses the tag which has the minimus amount in the training data set. To make it more clear, travel is tagged as a /verb/ 10 times, but as a /noun/ 20 times. It always chooses the tag which has been repeated less. </p>
",Training and Model Evaluation,stanford tagger making unexpected result trying use stanford corenlp po tagger data used automatic generated prop file changed open class want know complete description field property like arch possible value closedclasstagthreshold minfeaturethresh curwordminfeaturethresh rarewordminfeaturethresh run code tag text chooses tag ha minimus amount training data set make clear travel tagged verb time noun time always chooses tag ha repeated le
How to avoid time lag in BRAT while editing annotations?,"<p>I'm using the Brat software (<a href=""http://brat.nlplab.org"" rel=""nofollow noreferrer"">http://brat.nlplab.org</a>) to annotate my data. I have my annotated files and want to manually change them, for eg. delete some entity, edit the entity type etc.</p>

<p>Everytime a change is made, the page reloads which takes about 5-10 seconds. This is because brat works on a web server(<a href=""http://brat.nlplab.org/installation.html"" rel=""nofollow noreferrer"">http://brat.nlplab.org/installation.html</a>), so when a change is made it uploads the changed file on server then reloads.</p>

<p>I need to do a number of changes and this lag is irritating. Is there a way to avoid the lag? Maybe make a local cache, make all changes in it and then upload it on the server.</p>

<p>This is needed to create a training data which will be done by other people, so I want it to be fast and easy for them.</p>
",Training and Model Evaluation,avoid time lag brat editing annotation using brat software annotate data annotated file want manually change eg delete entity edit entity type etc everytime change made page reloads take second brat work web server change made uploads changed file server reloads need number change lag irritating way avoid lag maybe make local cache make change upload server needed create training data done people want fast easy
"language modeling - model loss and accuracy not improving, model is underfitting","<p>I am trying to build a word-level language model in TensorFlow. My inputs are batches with word id's of shape <code>(batch_size, seq_length)</code>, my targets are the inputs shifted one time step to the left (so for each word, the target is the next word in the sequence).</p>

<p>The model receives word embeddings as an input (word embeddings were pre-trained using gensim word2vec). I manually checked that the word embeddings are read in correctly and that they correspond to the right word id's.</p>

<p>Although I have tried out a lot of things, my model is not improving. Even when training for 100 epochs over the full training set, the accuracy remains the same.</p>

<p>What I have tried (without any success):</p>

<ul>
<li>Removing dropout. My first goal is to get rid of underfitting</li>
<li>Different vocabulary size (100, 1000, 10000)</li>
<li>Using gradient clipping/ not using gradient clipping</li>
<li>Changing the initialization of the weights </li>
<li>Data shuffling</li>
<li>different optimizer (RSMProp, Adam and Gradient Descent)</li>
<li>larger/smaller model (2-4 hidden layers with 128-256 hidden units)</li>
<li>different batch size (10, 20, 128)</li>
<li>different learning rate (0.01, 0.001, 0.1)</li>
<li>different loss function (sparse_softmax_cross_entropy_with_logits or tf.contrib.seq2seq.sequence_loss)</li>
<li>refeeding/not refeeding the final state of the LSTM during training*</li>
</ul>

<p>In the beginning, both loss and accuracy are improving. Also, the model is adapting its predictions. But then, after some epochs over the full training set, loss and accuracy stay constant. Also, the model predictions aren't changing anymore and it gets stuck.
Here is an example that shows the development of loss and accuracy for the same input sequence. After epoch 30, nothing is changing anymore:</p>

<pre><code>2017-11-08 06:59:24,298 - DEBUG - Targets: [  91    4    9  116  237 1953  240    3    2    1    0    2    1    9  144 351   29  299   24  453]
 2017-11-08 06:59:24,299 - DEBUG - Predicted sequence: [0 0 0 0 0 0 0 0 2 1 0 0 1 0 0 0 0 0 0 0]
 2017-11-08 06:59:24,299 - INFO - Current epoch: 1
 2017-11-08 06:59:24,299 - INFO - Current training step: 2000
 2017-11-08 06:59:24,299 - INFO - Current loss: 107.67147064208984
 2017-11-08 06:59:24,299 - INFO - Current accuracy: 0.1599999964237213


 2017-11-08 07:04:09,559 - DEBUG - Targets: [  91    4    9  116  237 1953  240    3    2    1    0    2    1    9  144 351   29  299   24  453]
 2017-11-08 07:04:09,560 - DEBUG - Predicted sequence: [ 4  4  6  6 16  0  0  3  2  1  9  2  1  0  0  4  0  0  4  8]
 2017-11-08 07:04:09,560 - INFO - Current epoch: 5
 2017-11-08 07:04:09,560 - INFO - Current training step: 2000
 2017-11-08 07:04:09,560 - INFO - Current loss: 97.8116455078125
 2017-11-08 07:04:09,560 - INFO - Current accuracy: 0.2150000035762787


2017-11-08 07:43:03,875 - DEBUG - Targets: [  91    4    9  116  237 1953  240    3    2    1    0    2    1    9  144 351   29  299   24  453]
 2017-11-08 07:43:03,875 - DEBUG - Predicted sequence: [ 6  4  9 55 47  0  5  3  2  1  9  2  1  0 55 24  0  0  3  6]
 2017-11-08 07:43:03,876 - INFO - Current epoch: 30
 2017-11-08 07:43:03,876 - INFO - Current training step: 2000
 2017-11-08 07:43:03,876 - INFO - Current loss: 84.75357055664062
 2017-11-08 07:43:03,876 - INFO - Current accuracy: 0.2549999952316284
</code></pre>

<p>I have been working on this for a week already and I don't know what I can try out anymore. I would be super grateful for any tips or ideas.</p>

<p>The important parts of the code are here:</p>

<pre><code>    def build_graph(self, graph):
    with graph.as_default():
        tf.set_random_seed(self.random_seed)

        with tf.variable_scope('embedding'):
            embedding_matrix = tf.get_variable(name='embedding_matrix', shape=self.embds.shape, initializer=tf.constant_initializer(self.embds), trainable=False)

        with tf.name_scope('input'):
            self.input_batch = tf.placeholder(tf.int64, shape=(None, self.seq_length))
            self.inputs = tf.nn.embedding_lookup(embedding_matrix, self.input_batch)
            self.label_batch = tf.placeholder(tf.int64, shape=(None, self.seq_length))

        with tf.name_scope('rnn'):
            # Set up the RNN architecture
            cells = []

            for i in range(self.n_layers):
                cell = tf.contrib.rnn.LSTMCell(self.n_hidden, initializer=tf.contrib.layers.xavier_initializer())#use_peepholes=True,

                # Add dropout (only used during training)
                # cell = tf.contrib.rnn.DropoutWrapper(
                #     cell,
                #     output_keep_prob=(1.0 if not self.config['train'] else
                #                       self.dropout_keep_prob))
                cells.append(cell)


            cell = tf.contrib.rnn.MultiRNNCell(
                cells, state_is_tuple=True)

            # Create a zero-filled state tensor as an initial state
            self.init_state = cell.zero_state(self.batch_size, tf.float32)

            # Create a recurrent neural network
            output, self.final_state = tf.nn.dynamic_rnn(
                cell,
                inputs=self.inputs,
                initial_state=self.init_state)

            # OLD VERSION
            # self.logits = tf.contrib.layers.fully_connected(outputs, self.vocab_size, activation_fn=None)

            # NEW VERSION
            # Try out part of tensorflow tutorial

            self.output_flat = tf.reshape(output, [-1, cell.output_size])
            softmax_w = tf.get_variable(""softmax_w"", [self.n_hidden, self.vocab_size], dtype=tf.float32)

            softmax_b = tf.get_variable(""softmax_b"", [self.vocab_size], dtype=tf.float32)
            logits = tf.nn.xw_plus_b(self.output_flat, softmax_w, softmax_b)
            # Reshape logits to be a 3-D tensor for sequence loss
            self.logits = tf.reshape(logits, [self.batch_size, self.seq_length, self.vocab_size])

            # Use the contrib sequence loss and average over the batches
            loss = tf.contrib.seq2seq.sequence_loss(
                self.logits,
                self.label_batch,
                tf.ones([self.batch_size, self.seq_length], dtype=tf.float32),
                average_across_timesteps=False, average_across_batch=True)

            self.loss = tf.reduce_sum(loss)


        with tf.name_scope('prediction'):

            # Compute real-valued predictions of the network
            self.predictions = tf.argmax(self.logits, axis=2)

            # Compute the softmax                
            # softmax_ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.label_batch, logits=self.logits)

        #with tf.name_scope(""loss""):
            # Compute the loss (cross-entropy)
            # self.loss = tf.reduce_mean(softmax_ce)

        with tf.name_scope(""metrics""):
            # Compute accuracy and perplexity for evaluation

            correct_predictions = tf.to_float(tf.equal(self.label_batch, self.predictions))

            self.perplexity = tf.reduce_mean(tf.exp(softmax_ce))
            self.accuracy = tf.reduce_mean(correct_predictions)

        with tf.name_scope('train'):
            # Create a global step variable
            self.global_step = tf.Variable(
                0,
                trainable=False,
                name=""global_step"",
                collections=[ tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES ])

            # Get all variables created with trainable=True
            parameters = tf.trainable_variables()
            # Compute the gradient of the loss w.r.t to the params
            gradients = tf.gradients(self.loss, parameters)
            # Clip the gradients. How this works: Given a tensor t, and a maximum
            # clip value clip_norm the op normalizes t so that its L2-norm is less
            # than or equal to clip_norm
            clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.clip_norm)

            self.optimizer =  tf.train.AdamOptimizer(learning_rate=self.lr, epsilon=0.1)
            # Apply the optimizer              
            self.train_step = self.optimizer.apply_gradients(zip(clipped_gradients, parameters), global_step=self.global_step)

            # If not clipping the gradients, minimize the loss directly
            # self.train_step = tf.train.AdamOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)
            # self.train_step = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)

        self._create_summaries()

    return graph


def train(self, save_every=20):
    with self.graph.as_default():

        # Initialize the state of the network
        feed2 = np.zeros((self.batch_size, self.n_hidden))
        t = tuple((feed2, feed2))
        _current_state = np.array([t, t])
        training_step = 0

        for epoch_id in range(0, self.n_epochs):     
            m, n = self.x_train.shape
            self.n_batches = int(m//self.batch_size)

            for batch_number in range(0, self.n_batches):
                training_step += 1
                from_index = batch_number*self.batch_size
                to_index = (batch_number+1)*self.batch_size
                _inputs = self.x_train[from_index:to_index,:]
                _labels = self.y_train[from_index:to_index,:]

                # Run training step
                # The final state of the net is fed back into the net 
                _logits, _predictions, _train_step, _current_state, _loss, _acc, summary = self.sess.run(
                        [self.logits,
                        self.predictions,
                        self.train_step,
                        self.final_state,
                        self.loss,
                        self.accuracy,
                        #self.perplexity,
                        self.merged],
                        feed_dict={
                            self.input_batch: _inputs,
                            self.label_batch: _labels,
                            self.init_state[0][0]: _current_state[0][0],
                            self.init_state[0][1]: _current_state[0][1],
                            self.init_state[1][0]: _current_state[1][0],
                            self.init_state[1][1]: _current_state[1][1],
                           })

                pred = _predictions[0]

                if batch_number % 2000 == 0:
                    self.sw.add_summary(summary, training_step)
                    tf.logging.debug(""Targets: {}"".format(_labels[0]))
                    tf.logging.debug(""Predicted sequence: {}"".format(pred))
                    tf.logging.info(""Current epoch: {}"".format(epoch_id))
                    tf.logging.info(""Current training step: {}"".format(batch_number))
                    tf.logging.info(""Current loss: {}"".format(_loss))
                    tf.logging.info(""Current accuracy: {}"".format(_acc))
                    tf.logging.info(""Current perplexity: {}"".format(_perpl))

            self.save(epoch_id)
</code></pre>
",Training and Model Evaluation,language modeling model loss accuracy improving model underfitting trying build word level language model tensorflow input batch word id shape target input shifted one time step left word target next word sequence model receives word embeddings input word embeddings pre trained using gensim word vec manually checked word embeddings read correctly correspond right word id although tried lot thing model improving even training epoch full training set accuracy remains tried without success removing dropout first goal get rid underfitting different vocabulary size using gradient clipping using gradient clipping changing initialization weight data shuffling different optimizer rsmprop adam gradient descent larger smaller model hidden layer hidden unit different batch size different learning rate different loss function sparse softmax cross entropy logits tf contrib seq seq sequence loss refeeding refeeding final state lstm training beginning loss accuracy improving also model adapting prediction epoch full training set loss accuracy stay constant also model prediction changing anymore get stuck example show development loss accuracy input sequence epoch nothing changing anymore working week already know try anymore would super grateful tip idea important part code
Why acc of char-level cnn for text classification stay unchanged,"<p><strong>I misused binary cross-entropy for softmax, changed to categorical cross-entropy. And did some reviewing about details of the problem below in my own answer</strong></p>

<hr>

<p>I am trying to using open source data: sogou_news_csv(converted to pinyin using <em>jieba</em> from for text classification following <a href=""https://arxiv.org/abs/1502.01710"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1502.01710</a> ""Text understanding from scratch"" by Xiang Zhang and Yann LeCun. (mainly follow the idea of using character level CNN, but the structure proposed in the paper).</p>

<p>I did the preprocessing by using one-hot encoding according to a <em>alphabet collection</em> and filling all those not in the <em>alphabet collection</em> with 0s.
As a result, I got the training data with the shape of (450000, 1000, 70),(data_size, sequence_length, alphabet_size).</p>

<p>Then I feed the data into a cnn structure following <a href=""http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"" rel=""nofollow noreferrer"">http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/</a>.</p>

<p><strong>Problem is</strong> 
<strong>During the training, the loss and acc merely change, I tried preprocessing again for the data, and <code>tried different learning rate settings</code>, but not helpful, So what went wrong?</strong> </p>

<p>Below is one-hot encoding:</p>

<pre><code>import numpy as np

all_letters = ""abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\""/\\|_@#$%^&amp;*~`+-=&lt;&gt;()[]{}\n""
n_letters = len(all_letters)

def letterToIndex(letter):
    """"""
    'c' -&gt; 2
    """"""
    return all_letters.find(letter)


def sets2tensors(clean_train, n_letters=n_letters, MAX_SEQUENCE_LENGTH=1000):
    """"""
    From lists of cleaned passages to np.array with shape(len(train), 
        max_sequence_length, len(dict))
    Arg: 
        obviously
    """"""
    m = len(clean_train)
    x_data = np.zeros((m, MAX_SEQUENCE_LENGTH, n_letters))
    for ix in range(m):
        for no, letter in enumerate(clean_train[ix]):
            if no &gt;= 1000:
                break
            letter_index = letterToIndex(letter)
            if letter != -1:
                x_data[ix][no][letter_index]  = 1
            else:
                continue            
    return x_data
</code></pre>

<p>This is the Model:</p>

<pre><code>num_classes = 5
from keras.models import Sequential
from keras.layers import Activation, GlobalMaxPool1D, Merge, concatenate, Conv1D, Dense, Dropout
from keras.callbacks import EarlyStopping
from keras.optimizers import SGD
submodels = []
for kw in (3, 4, 5):    # kernel sizes
    submodel = Sequential()
    submodel.add(Conv1D(32,
                        kw,
                        padding='valid',
                        activation='relu',
                        strides=1, input_shape=(1000, n_letters)))
    submodel.add(GlobalMaxPool1D())
    submodels.append(submodel)
big_model = Sequential()
big_model.add(Merge(submodels, mode=""concat""))
big_model.add(Dense(64))
big_model.add(Dropout(0.5))
big_model.add(Activation('relu'))
big_model.add(Dense(num_classes))
big_model.add(Activation('softmax'))
print('Compiling model')
opt = SGD(lr=1e-6)  # tried different learning rate from 1e-6 to 1e-1
# changed from binary crossentropy to categorical_crossentropy
big_model.compile(loss='categorical_crossentropy', 
                  optimizer=opt,
                  metrics=['accuracy'])
</code></pre>

<p>Some results</p>

<pre><code>Train on 5000 samples, validate on 5000 samples
Epoch 1/5
5000/5000 [==============================] - 54s - loss: 0.5198 - acc: 0.7960 - val_loss: 0.5001 - val_acc: 0.8000
Epoch 2/5
5000/5000 [==============================] - 56s - loss: 0.5172 - acc: 0.7959 - val_loss: 0.5000 - val_acc: 0.8000
Epoch 3/5
5000/5000 [==============================] - 56s - loss: 0.5198 - acc: 0.7965 - val_loss: 0.5000 - val_acc: 0.8000
Epoch 4/5
5000/5000 [==============================] - 57s - loss: 0.5222 - acc: 0.7950 - val_loss: 0.4999 - val_acc: 0.8000
Epoch 5/5
5000/5000 [==============================] - 59s - loss: 0.5179 - acc: 0.7960 - val_loss: 0.4999 - val_acc: 0.8000
</code></pre>
",Training and Model Evaluation,acc char level cnn text classification stay unchanged misused binary cross entropy softmax changed categorical cross entropy reviewing detail problem answer trying using open source data sogou news csv converted pinyin using jieba text classification following text understanding scratch xiang zhang yann lecun mainly follow idea using character level cnn structure proposed paper preprocessing using one hot encoding according alphabet collection filling alphabet collection result got training data shape data size sequence length alphabet size feed data cnn structure following problem training loss acc merely change tried preprocessing data helpful went wrong one hot encoding model result
How to choose label/target for RNN models?,"<p>When training a character RNN, if we have an input X = (x_1, x_2, ..., x_t), we split it as two parts: X_train = (x_1, x_2, ..., x_(t-1)), y_train = (x_2, x_3, ..., x_t).</p>

<p>Why would we do that? Why not set y_train = (x_(t+1)), i.e. the next character that we want to predict? </p>

<p>I'm trying to predict other time series data with RNN, and it really confuses me how to choose the label/target for the model.</p>
",Training and Model Evaluation,choose label target rnn model training character rnn input x x x x split two part x train x x x train x x x would set train x e next character want predict trying predict time series data rnn really confuses choose label target model
"How to automatically transcribe a Skype meeting, correctly attributed to each participant?","<p>Assuming each participant agrees to the recording and transcription of the Skype call, is there a way to transcribe the meeting (either live or offline or both) such that it produces a text transcript where each spoken text is correctly attributed to the speaker.  The transcript could then be input to any variety of search or NLP algorithms.  </p>

<p>The top 3 Google search hits of ""automatically transcribe Skype"" refer to apps which make <strong>manual</strong> transcription easier:</p>

<p>(1) <a href=""http://www.dummies.com/how-to/content/how-to-convert-skype-audio-to-text-with-transcribe.html"" rel=""noreferrer"">http://www.dummies.com/how-to/content/how-to-convert-skype-audio-to-text-with-transcribe.html</a></p>

<p>(2) <a href=""http://ask.metafilter.com/231400/How-to-record-and-transcribe-Skype-conversation"" rel=""noreferrer"">http://ask.metafilter.com/231400/How-to-record-and-transcribe-Skype-conversation</a></p>

<p>(3) <a href=""https://www.ttetranscripts.com/blog/how-to-record-and-transcribe-your-skype-conversations"" rel=""noreferrer"">https://www.ttetranscripts.com/blog/how-to-record-and-transcribe-your-skype-conversations</a></p>

<p>While it would be trivial to record the audio and send it to a speech-to-text engine, I doubt it would be very high quality because the best results are usually speaker dependent models (else we wouldn't have to take time to train Dragon Naturally Speaking).  </p>

<p>But, before we can choose speaker dependent transcription models, we need to know which segment of the audio belongs to which speaker.  There's 2 ways that this is solved:</p>

<ol>
<li><p>There is an easy way to retrieve all the audio that came from each participant, e.g. you just record all the audio from each speaker's microphone during the call, and you don't have to do any segmentation.  </p></li>
<li><p>In case the first option isn't feasible or prohibitive in some way, we have to use a Speaker Diarization algorithm, which segments the audio into N clusters/speakers (most algorithms allow for being told how many speakers in the audio, but some can figure this out on their own).  For real-time transcript as the call goes on, I imagine we'd need some fancy Real Time Speaker Diarization algorithm.  </p></li>
</ol>

<p>In any case, once the segmentation is solved, each participant has their trained speaker model, which is then applied to their portions of the audio.  At the end of the day, everyone gets a nice conversation transcript and later one we can do fancy things like topic analysis or maybe Big Brother wants to sift over everyone's project meetings without having to listen to hours of audio.  </p>

<p>My question is, what would be a way to implement this in practice?  </p>
",Training and Model Evaluation,automatically transcribe skype meeting correctly attributed participant assuming participant recording transcription skype call way transcribe meeting either live offline produce text transcript spoken text correctly attributed speaker transcript could input variety search nlp algorithm top google search hit automatically transcribe skype refer apps make manual transcription easier would trivial record audio send speech text engine doubt would high quality best result usually speaker dependent model else take time train dragon naturally speaking choose speaker dependent transcription model need know segment audio belongs speaker way solved easy way retrieve audio came participant e g record audio speaker microphone call segmentation case first option feasible prohibitive way use speaker diarization algorithm segment audio n cluster speaker algorithm allow told many speaker audio figure real time transcript call go imagine need fancy real time speaker diarization algorithm case segmentation solved participant ha trained speaker model applied portion audio end day everyone get nice conversation transcript later one fancy thing like topic analysis maybe big brother want sift everyone project meeting without listen hour audio question would way implement practice
"when training data using IBM Bluemix natural language classifier api, return data too small","<p>When I follow ""Getting started with the Natural Language Classifier service"" guide line, I meet problem at Stage 2: Create and train a classifier:</p>

<pre><code>$ curl -i -u ""&lt;username&gt;"":""&lt;password&gt;"" \
-F training_data=@&lt;path_to_file&gt;/weather_data_train.csv \
-F training_metadata=""{\""language\"":\""en\"",\""name\"":\""TutorialClassifier\""}"" \
""https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers""
</code></pre>

<p>It returns:</p>

<pre><code>{
  ""code"" : 400,
  ""error"" : ""Data too small"",
  ""description"" : ""The number of training entries received = 1, which is smaller
 than the required minimum of 5""
}
</code></pre>

<p>Any one could kindly help how to solve this problem. Thanks a lot~</p>

<p>Here is the guide line link:
<a href=""http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/doc/nl-classifier/get_start.shtml#create"" rel=""nofollow"">http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/doc/nl-classifier/get_start.shtml#create</a></p>
",Training and Model Evaluation,training data using ibm bluemix natural language classifier api return data small follow getting started natural language classifier service guide line meet problem stage create train classifier return one could kindly help solve problem thanks lot guide line link
Evaluating POS tagger in NLTK,"<p>I want to evaluate different POS tags in NLTK using a text file as an input. </p>

<p>For an example, I will take Unigram tagger. I have found how to evaluate Unigram tag using brown corpus.</p>

<pre><code>from nltk.corpus import brown
import nltk

brown_tagged_sents = brown.tagged_sents(categories='news')
brown_sents = brown.sents(categories='news')
# We train a UnigramTagger by specifying tagged sentence data as a parameter
# when we initialize the tagger.
unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)
print(unigram_tagger.tag(brown_sents[2007]))
print(unigram_tagger.evaluate(brown_tagged_sents))
</code></pre>

<p>It produces an output like below. </p>

<pre><code>[('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'), ('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'), ('type', 'NN'), (',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'), ('ground', 'NN'), ('floor', 'NN'), ('so', 'QL'), ('that', 'CS'), ('entrance', 'NN'), ('is', 'BEZ'), ('direct', 'JJ'), ('.', '.')]
0.9349006503968017
</code></pre>

<p>In a similar manner, I want to read text from a text file and evaluate the accuracy of different POS taggers. </p>

<p>I figured out how to read a text file and how to apply pos tags for the tokens. </p>

<pre><code>import nltk
from nltk.corpus import brown
from nltk.corpus import state_union

brown_tagged_sents = brown.tagged_sents(categories='news')

sample_text = state_union.raw(
    r""C:\pythonprojects\tagger_nlt\new-testing.txt"")
tokens = nltk.word_tokenize(sample_text)

default_tagger = nltk.UnigramTagger(brown_tagged_sents)

default_tagger.tag(tokens)

print(default_tagger.tag(tokens))
[('Honestly', None), ('last', 'AP'), ('seven', 'CD'), ('lectures', None), ('are', 'BER'), ('good', 'JJ'), ('.', '.'), ('Lectures', None), ('are', 'BER'), ('understandable', 'JJ')
</code></pre>

<p>What I wanted to have is a score like <strong>default_tagger.evaluate()</strong>, so that I can compare different POS taggers in NLTK using the same input file to identify the most suited POS tagger for a given file. </p>

<p>Any help will be appreciated. </p>
",Training and Model Evaluation,evaluating po tagger nltk want evaluate different po tag nltk using text file input example take unigram tagger found evaluate unigram tag using brown corpus produce output like similar manner want read text text file evaluate accuracy different po tagger figured read text file apply po tag token wanted score like default tagger evaluate compare different po tagger nltk using input file identify suited po tagger given file help appreciated
"How to calculate Precision, Recall and F-score using python?","<p>The labelTrainData.csv is used to train the classifier for predicting sentiments of Testdata.csv. Finally i got BagOfCentroids.csv. </p>

<p><strong>labelTrainData.csv</strong></p>

<pre><code>id   sentiment    Tweet
1    0            tweet_1
2    1            tweet_2
3    0            tweet_3
</code></pre>

<p><strong>Testdata.csv</strong></p>

<pre><code>id      Tweet
1       tweet_1
2       tweet_2
3       tweet_3
</code></pre>

<p><strong>BagOfCentroids.csv</strong></p>

<pre><code>id      sentiment
1       0
2       1
3       1
</code></pre>

<p>To calculate above metrics, I am trying this,</p>

<pre><code>print 'Sentiment precision:'

nltk.metrics.precision(BagOfCentroids['sentiment'], Testdata['sentiment'])

print 'sentiment recall:'

nltk.metrics.recall(BagOfCentroids['sentiment'], Testdata['sentiment'])

print 'sentiment F-measure:'

nltk.metrics.f_measure(BagOfCentroids['sentiment'], Testdata['sentiment'])  
</code></pre>

<p>Is there any way to calculate Precision, Recall and F-score?</p>
",Training and Model Evaluation,calculate precision recall f score using python labeltraindata csv used train classifier predicting sentiment testdata csv finally got bagofcentroids csv labeltraindata csv testdata csv bagofcentroids csv calculate metric trying way calculate precision recall f score
Stupid Backoff implementation clarification,"<p>Hello people I'm implementing the <a href=""http://www.aclweb.org/anthology/D07-1090.pdf"" rel=""noreferrer"">Stupid Backoff</a> (page 2, equation 5) smoothing technique for a project I'm working on and I have a question on its implementation. This is a smoothing algorithm used in NLP, Good-Turing is I guess the most well known similar algorithm.</p>

<p>A brief description of the algorithm is:
When trying to find the probability of word appearing in a sentence it will first look for context for the word at the n-gram level and if there is no n-gram of that size it will recurse to the (n-1)-gram and multiply its score with 0.4. The recursion stops at unigrams.</p>

<p>So if I want to find the probability of ""day"" in the context of ""a sunny day"" it would first look to see if the tri-gram ""a sunny day"" exists in the corpus, if not it would try the same with the bigram ""sunny day"" and finally it would just get the frequency for ""day"" divided by the corpus size (total number of words in the training data).</p>

<p>My question is: Do I multiply the score with 0.4 every time I reduce the size of the n-gram?</p>

<p>So in the above example if we are not able to find a tri-gram or bi-gram the final score would be:</p>

<p>0.4 * 0.4 * frequency(day) / corpus_size?</p>

<p>or do I just multiply once at the final level so regardless of how many backoffs I have to make I just multiply the final score with 0.4?</p>
",Training and Model Evaluation,stupid backoff implementation clarification hello people implementing stupid backoff page equation smoothing technique project working question implementation smoothing algorithm used nlp good turing guess well known similar algorithm brief description algorithm trying find probability word appearing sentence first look context word n gram level n gram size recurse n gram multiply score recursion stop unigrams want find probability day context sunny day would first look see tri gram sunny day exists corpus would try bigram sunny day finally would get frequency day divided corpus size total number word training data question multiply score every time reduce size n gram example able find tri gram bi gram final score would frequency day corpus size multiply final level regardless many backoffs make multiply final score
What evaluation metric should I use to compare knowledge based approach with generative model?,"<p>My team and I are working on developing a chatbot for technical support and we have two models- Generative model using rnn algorithm and Knowledge based approach using NLP. I would like to know if I want to compare these two models, what evaluation metric should I use. For generative model, I can compare rnn, tf-idf and lstm algorithms using recall@k metric. Is there any industrial standard to compare rnn algorithm with knowledge based approach?</p>
",Training and Model Evaluation,evaluation metric use compare knowledge based approach generative model team working developing chatbot technical support two model generative model using rnn algorithm knowledge based approach using nlp would like know want compare two model evaluation metric use generative model compare rnn tf idf lstm algorithm using recall k metric industrial standard compare rnn algorithm knowledge based approach
Is there a minimum data size required for BigramTagger to work?,"<p>I am learning the BigramTagger class in the nltk library. I train a 'Part-of-Sentence' tagger using the brown corpus that comes with nltk. </p>

<p>I notice that if I train on this corpus, and then tag a few words from the first sentence of the corpus, it works great. </p>

<pre><code>from nltk.corpus import brown
from nltk.tag import BigramTagger 
from nltk import word_tokenize

# Works completely fine:
brown_train = brown.tagged_sents(categories='news')
bigram_tagger = BigramTagger(brown_train)
print(bigram_tagger.tag(word_tokenize(""that any irregularities took place"")))
</code></pre>

<p>We get the expected output:</p>

<blockquote>
  <p>[('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN')]</p>
</blockquote>

<p>But if I only train with 100 sentences, it fails. </p>

<pre><code># Fails to work: 
brown_train = brown.tagged_sents(categories='news')[:100]
bigram_tagger = BigramTagger(brown_train)
print(bigram_tagger.tag(word_tokenize(""that any irregularities took place"")))
</code></pre>

<p>It fails to tag these words, so it gives them the None tag:</p>

<blockquote>
  <p>[('that', None), ('any', None), ('irregularities', None), ('took', None), ('place', None)]</p>
</blockquote>

<p>Is there a minimum corpus required by the class? Or is there some other parameter I am forgetting that makes the model fail in the second case?</p>

<p>I've looked at the documentation here: <a href=""http://www.nltk.org/api/nltk.tag.html#nltk.tag.sequential.BigramTagger"" rel=""nofollow noreferrer"">http://www.nltk.org/api/nltk.tag.html#nltk.tag.sequential.BigramTagger</a> and it looks like there is a cutoff parameter, but that is set to 0 by default. </p>
",Training and Model Evaluation,minimum data size required bigramtagger work learning bigramtagger class nltk library train part sentence tagger using brown corpus come nltk notice train corpus tag word first sentence corpus work great get expected output c dti irregularity nns took vbd place nn train sentence fails fails tag word give none tag none none irregularity none took none place none minimum corpus required class parameter forgetting make model fail second case looked documentation look like cutoff parameter set default
Training and evaluating spaCy model by sentences or paragraphs,"<h2>Observation:</h2>

<p>Paragraph: <code>I love apple. I eat one banana a day</code><br>
Sentence: <code>I love apple.</code>, <code>I eat one banana a day</code><br>
There are two sentences in this paragraph, <code>I love apple</code> and <code>I eat one banana a day</code>. If I put the whole paragraph into spaCy, it'll recognize only one entity, for example, <code>apple</code>, but if I put the sentences in paragraph one by one, spaCy can recognize two entities, <code>apple</code> and <code>banana</code>.(<strong>This is just an example to show my point, the actual recognition result could be different</strong>)  </p>

<h2>Situation:</h2>

<p>After having trained a model by myself, I want to evaluate the recognizing accuracy of my model, there are two ways to pass the text into the spaCy model:<br>
1. split the paragraph into sentences and pass the sentence one by one    <code>
for sentence in paragraph:
    doc = nlp(sentence)
    # retrieve the parsing result
</code>
2. pass the paragraph at once
<code>
  doc = nlp(paragraph)
  # retrieve the parsing result
</code></p>

<h2>Question:</h2>

<ol>
<li>I'm wondering which way would be better to test the performance of the model? Since I'm sure passing by sentence can always recognize more entities than passing by paragraph.  </li>
<li>If the second one is better, do I also need to change the way that I trained the model? Currently, I train the spacy model sentence by sentence rather than a paragraph.</li>
</ol>

<h2>The goal of my project:</h2>

<p>After getting a document, recognize all the entities that I'm interested in the document.</p>

<p>Thanks!</p>
",Training and Model Evaluation,training evaluating spacy model sentence paragraph observation paragraph sentence two sentence paragraph put whole paragraph spacy recognize one entity example put sentence paragraph one one spacy recognize two entity example show point actual recognition result could different situation trained model want evaluate recognizing accuracy model two way pas text spacy model split paragraph sentence pas sentence one one pas paragraph question wondering way would better test performance model since sure passing sentence always recognize entity passing paragraph second one better also need change way trained model currently train spacy model sentence sentence rather paragraph goal project getting document recognize entity interested document thanks
Speeding up model training using MITIE with Rasa,"<p>I'm training a model for recognizing short, one to three sentence strings of text using the MITIE back-end in Rasa. The model trains and works using spaCy, but it isn't quite as accurate as I'd like. Training on spaCy takes no more than five minutes, but training for MITIE ran for several days non-stop on my computer with 16GB of RAM. So I started training it on an Amazon EC2 r4.8xlarge instance with 255GB RAM and 32 threads, but it doesn't seem to be using all the resources available to it.</p>

<p>In the Rasa config file, I have <code>num_threads: 32</code> and set <code>max_training_processes: 1</code>, which I thought would help use all the memory and computing power available. But now that it has been running for a few hours, CPU usage is sitting at 3% (100% usage but only on one thread), and memory usage stays around 25GB, one tenth of what it could be.</p>

<p>Do any of you have any experience with trying to accelerate MITIE training? My model has 175 intents and a total of 6000 intent examples. Is there something to tweak in the Rasa config files? </p>
",Training and Model Evaluation,speeding model training using mitie rasa training model recognizing short one three sentence string text using mitie back end rasa model train work using spacy quite accurate like training spacy take five minute training mitie ran several day non stop computer gb ram started training amazon ec r xlarge instance gb ram thread seem using resource available rasa config file set thought would help use memory computing power available ha running hour cpu usage sitting usage one thread memory usage stay around gb one tenth could experience trying accelerate mitie training model ha intent total intent example something tweak rasa config file
wit.ai quantity not working,"<p>I am unable to setup the wit/quantity entity. There is no documentation regarding this except for the tooltip. </p>

<p>for example: </p>

<ol>
<li>I need to say, order two fries. </li>
<li>Order a lamb burger</li>
</ol>

<p>Entities: </p>

<ul>
<li>products [keyword] </li>
<li>with all the products (ex: fries, lamb burger,
etc.) units [keyword] </li>
<li>empty wit/quantity.</li>
</ul>

<p>I tried to train the model. It was only able to recognize product. It does not learn when I teach it to recognize the quantity. </p>

<p>Any help is appreciated. </p>
",Training and Model Evaluation,wit ai quantity working unable setup wit quantity entity documentation regarding except tooltip example need say order two fry order lamb burger entity product keyword product ex fry lamb burger etc unit keyword empty wit quantity tried train model wa able recognize product doe learn teach recognize quantity help appreciated
How can you train GATE (General Architecture for Text Enginnering) Developer with some training data or data that already annotated?,"<p>I am looking for ways to train my GATE. Not just running the application, but training it with like data that already annotated (not just plain document). I really appreciate if anybody help me. Thanks :) </p>
",Training and Model Evaluation,train gate general architecture text enginnering developer training data data already annotated looking way train gate running application training like data already annotated plain document really appreciate anybody help thanks
Multiple tags for single document in doc2vec. TaggedDocument,"<p>Is it possible to to train a doc2vec model where a single document has multiple tags?
For example, in movie reviews,</p>

<pre><code>doc0 = doc2vec.TaggedDocument(words=review0,tags=['UID_0','horror','action'])
doc1 = doc2vec.TaggedDocument(words=review1,tags=['UID_1','drama','action','romance'])
</code></pre>

<p>In such case where each document has a unique tag (UID) and multiple categorical tags, how do I access the vector after the training? For example, what would be the most proper syntax to call</p>

<pre><code>model['UID_1']
</code></pre>
",Training and Model Evaluation,multiple tag single document doc vec taggeddocument possible train doc vec model single document ha multiple tag example movie review case document ha unique tag uid multiple categorical tag access vector training example would proper syntax call
How to get Plural form from Singular form?,"<p>I want to get plural of the given noun. I have tried JAVA INFLECTOR. But it has very poor accuracy for nouns not following the regular rules.</p>

<p><em>Examples from JAVA INFLECTOR:</em></p>

<ol>
<li>paparazzo -> paparazzos</li>
<li>criterion -> criterions</li>
<li>tooth -> tooths</li>
<li>thief -> thiefs</li>
<li>loaf -> loafs</li>
</ol>

<p>Stanford coreNLP lemmatizer is very good at plural to singular conversion. It takes care of many exceptional cases. As stated below:</p>

<p><em>Plural to singular from STANFORD LEMMATIZER</em>:</p>

<ol>
<li>vertices -> vertex</li>
<li>spectra -> spectrum</li>
<li>alumni -> alumnus</li>
<li>criteria -> criterion</li>
<li>thieves -> thief</li>
<li>geese -> goose</li>
<li>fungi -> fungus</li>
<li>loaves -> loaf.</li>
</ol>

<p>But the problem is I don't know how to get plural from given singular using Stanford CoreNLP. The lemmatizer gives singular from plural.</p>

<p>So, basically I want to get plural from singular nouns using STANFORD NLP.</p>

<p>How can this be achieved?</p>
",Training and Model Evaluation,get plural form singular form want get plural given noun tried java inflector ha poor accuracy noun following regular rule example java inflector paparazzo paparazzo criterion criterion tooth tooth thief thief loaf loaf stanford corenlp lemmatizer good plural singular conversion take care many exceptional case stated plural singular stanford lemmatizer vertex vertex spectrum spectrum alumnus alumnus criterion criterion thief thief goose goose fungi fungus loaf loaf problem know get plural given singular using stanford corenlp lemmatizer give singular plural basically want get plural singular noun using stanford nlp achieved
Why does MITIE get stuck on segment classifier?,"<p>I'm building a model using <code>MITIE</code> with a training dataset of 1,400 sentences, between 3-10 words long, paired to around 120 intents. My model training get stuck at <code>Part II: train segment classifier</code>. I've let it run for 14 hours before terminating.</p>

<p>My machines has <code>2.4 GHz Intel Core i7</code> and <code>8 GB 1600 MHz DDR3</code> and the <code>segment classifier</code> uses all available memory (around 7gb), eventually relying on compressed memory, and at the end of the last session activity monitor showed <code>32gb</code> used and <code>27gb</code> compressed. And <code>segment classifier</code> has never completed.</p>

<p>My current output is below:</p>

<pre><code>INFO:rasa_nlu.model:Starting to train component nlp_mitie
INFO:rasa_nlu.model:Finished training component.
INFO:rasa_nlu.model:Starting to train component tokenizer_mitie
INFO:rasa_nlu.model:Finished training component.
INFO:rasa_nlu.model:Starting to train component ner_mitie
Training to recognize 20 labels: 'pet', 'room_number', 'broken_things', '@sys.ignore', 'climate', 'facility', 'gym', 'medicine', 'item', 'exercise_equipment
', 'service', 'number', 'electronic_device', 'charger', 'toiletries', 'time', 'date', 'facility_hours', 'cost_inquiry', 'tv channel'
Part I: train segmenter
words in dictionary: 200000
num features: 271

now do training
C:           20
epsilon:     0.01
num threads: 1
cache size:  5
max iterations: 2000
loss per missed segment:  3
C: 20   loss: 3         0.669591
C: 35   loss: 3         0.690058
C: 20   loss: 4.5       0.701754
C: 5   loss: 3  0.616959
C: 20   loss: 1.5       0.634503
C: 28.3003   loss: 5.74942      0.71345
C: 25.9529   loss: 5.72171      0.707602
C: 27.7407   loss: 5.97907      0.707602
C: 30.2561   loss: 5.61669      0.701754
C: 27.747   loss: 5.66612       0.710526
C: 28.9754   loss: 5.82319      0.707602
best C: 28.3003
best loss: 5.74942
num feats in chunker model: 4095
train: precision, recall, f1-score: 0.805851 0.885965 0.844011
Part I: elapsed time: 180 seconds.

Part II: train segment classifier
now do training
num training samples: 415
</code></pre>

<p>I understand this could be an issue caused by redundant labels (as explained <a href=""https://github.com/mit-nlp/MITIE/issues/11"" rel=""nofollow noreferrer"">here</a>); however, all of my labels are unique. My understanding is that training shouldn't take this long or use this much memory. I've seen others posting similar issues with no solution provided yet. What is causing this high memory usage and insane training time? How is it fixed?</p>
",Training and Model Evaluation,doe mitie get stuck segment classifier building model using training dataset sentence word long paired around intent model training get stuck let run hour terminating machine ha us available memory around gb eventually relying compressed memory end last session activity monitor showed used compressed ha never completed current output understand could issue caused redundant label explained however label unique understanding training take long use much memory seen others posting similar issue solution provided yet causing high memory usage insane training time fixed
Make feature vector of words in documents using bag of word model,"<p>I am trying to implement BoW model for which I need BoW feature vector. I have several train folders which contain several text files. I created the vocab of the whole training data. I now pass the whole vocabulary and document one by one. But I don't know how to proceed. I followed one source and implemented this code:</p>

<pre><code>corpus = dict(((term, index) for index, term in    enumerate(sorted(total_vocab)))
fvs = [[0]*num_words for _ in input]
print(fvs)
for i, doc_terms in enumerate(input):
    fv = fvs[i]
    for term in doc_terms:
        fv[corpus[term]] += 1
</code></pre>

<p>It gives me some 90,000 sized vector with all zeros. </p>

<p>What should I do in order to get the feature vector?</p>

<p>Here is the snippet of the class:</p>

<pre><code> class FeatureVector(object):
        def __init__(self,vocabsize,numdata):
                self.vocabsize = vocabsize
                self.X =  np.zeros((numdata,self.vocabsize), dtype=np.int)
                self.Y =  np.zeros((numdata,), dtype=np.int)

        def make_featurevector(self, input, classid,total_vocab):
                corpus = dict(((term, index) for index, term in enumerate(sorted(total_vocab)))
                #fvs = [[0]*num_words for _ in input]
                #print(fvs)
                #for i, doc_terms in enumerate(input):
                    #fv = fvs[i]
                    #for term in doc_terms:
                        #fv[corpus[term]] += 1
                #var = np.zeros((len(total_vocab),1),dtype=np.int)
                #for raw_word in input:
                  #  for eachkey in total_vocab.keys():
                   #    if eachkey in raw_word:
                    #       var[raw_word] += 1
                #print fvs
                """"""
                Takes input tihe documents and outputs the feature vectors as X and classids as Y.
                """"""
</code></pre>
",Training and Model Evaluation,make feature vector word document using bag word model trying implement bow model need bow feature vector several train folder contain several text file created vocab whole training data pas whole vocabulary document one one know proceed followed one source implemented code give sized vector zero order get feature vector snippet class
train word2vec with pretrained vectors,"<p>I am training word vectors on particular text corpus using fast text.
Fasttext provides all the necessary mechanics and options for training word vectors and when looked with tsne, the vectors are amazing. I notice gensim has a wrapper for fasttext which is good for accessing vectors. </p>

<p>for my task, I have many text corpuses. I need to use the above trained vectors again with new corpus and use the trained vectors again on new discovered corpuses. fasttext doesnot provide this function. I donot see any package that achieves this or may be I am lost. I see in google <a href=""https://groups.google.com/forum/#!topic/gensim/Y_WmJST9xx8"" rel=""nofollow noreferrer"">forum</a> gensim provides intersect_word2vec_format, but cannot understand or find usage tutorial for this. There is another <a href=""https://stackoverflow.com/questions/41096821/word2vec-model-intersect-word2vec-format"">question</a> open similar to this with no answer.</p>

<p>So apart from gensim, is there any other way to train the models like above.</p>
",Training and Model Evaluation,train word vec pretrained vector training word vector particular text corpus using fast text fasttext provides necessary mechanic option training word vector looked tsne vector amazing notice gensim ha wrapper fasttext good accessing vector task many text corpus need use trained vector new corpus use trained vector new discovered corpus fasttext doesnot provide function donot see package achieves may lost see google forum gensim provides intersect word vec format understand find usage tutorial another href open similar answer p apart gensim way train model like
CRF++: anybody understand what does the float number mean in CRF model file,"<p>When you build you model file with -t option by crf_learn:
        crf_learn template train_data -t model</p>

<p>It will then generate two model file, one of them is model.txt.</p>

<p>Can anybody tell what does the float numbers mean?</p>

<p>See the following example:</p>

<p>version: 100 
cost-factor: 1
maxid: 40
xsize: 1</p>

<p>B
I</p>

<p>U00:%x[0,0]
B</p>

<p>36 B
20 U00:、
26 U00:か
18 U00:が
22 U00:こ
8 U00:た
10 U00:ち
2 U00:っ
4 U00:て
34 U00:に
12 U00:の
0 U00:よ
28 U00:ら
24 U00:れ
32 U00:上
14 U00:世
16 U00:代
30 U00:地
6 U00:私</p>

<p>-0.3022268562246992
0.3022268562246989
-0.3629407244093161
0.3629407244093156
-0.3327259487028221
0.3327259487028215
0.3462799099537973
-0.3462799099537980
0.3452020097664334
-0.3452020097664336
-0.3218750203631590
0.3218750203631575
0.0376944272290242
-0.0376944272290280
0.3329631783491211
-0.3329631783491230
-0.3092967308014029
0.3092967308014015
0.3413769126433928
-0.3413769126433950
0.3786782765859961
-0.3786782765859980
0.5208645073272351
-0.5208645073272384
-0.3261580548802839
0.3261580548802814
-0.3615756495615902
0.3615756495615884
-0.3248593224319323
0.3248593224319312
0.3281895709166696
-0.3281895709166719
-0.3040331359589971
0.3040331359589951
0.2836939567332580
-0.2836939567332600
-0.1530917919770705
-0.1613508585854637
0.4245699543724943
-0.1101273038099901</p>

<p>My understanding is:
each float number should correspond to each template， for instance:
first float number ""-0.3022268562246992"" should correspond to ""36 B"".
But why the number of float number double the number of template?
what does those float number mean?</p>

<p>Many thanks,</p>

<p>Shuai Hua</p>
",Training and Model Evaluation,crf anybody understand doe float number mean crf model file build model file option crf learn crf learn template train data model generate two model file one model txt anybody tell doe float number mean see following example version cost factor maxid xsize b u x b b u u u u u u u u u u u u u u u u u u understanding float number correspond template instance first float number correspond b number float number double number template doe float number mean many thanks shuai hua
How to store variable in loss function into instance variable,"<p>I am using Keras with Tensorflow.
Since I want to create <a href=""https://arxiv.org/abs/1603.01360"" rel=""nofollow noreferrer"">LSTM-CRF model</a>, I defined my own loss function using <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/crf/crf_log_likelihood"" rel=""nofollow noreferrer"">tf.contrib.crf.crf_log_likelihood</a>:</p>

<pre><code>def loss(self, y_true, y_pred):
    sequence_lengths = ... # calc from y_true
    log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(y_pred, y_true, sequence_lengths)
    loss = tf.reduce_mean(-log_likelihood)
    self.transition_params = transition_params

    return loss
</code></pre>

<p>As you know, CRF needs transition params on prediction phase. So I stored <strong>transition_params</strong> into instance variables, <strong>self.transition_params</strong>.</p>

<p>The problem is that self.transition_params has never been updated during minibatch. According to my observation, it seems to be stored only once when compiling the model.</p>

<p>Is there any way to store variable in loss function into instance variable in Keras?</p>
",Training and Model Evaluation,store variable loss function instance variable using kera tensorflow since want create lstm crf model defined loss function using tf contrib crf crf log likelihood know crf need transition params prediction phase stored transition params instance variable self transition params problem self transition params ha never updated minibatch according observation seems stored compiling model way store variable loss function instance variable kera
What disk image should I choose for my Google Cloud VM so that pandas will work just as it does on my Mac?,"<p>I followed a <a href=""https://jeffdelaney.me/blog/running-jupyter-notebook-google-cloud-platform/"" rel=""nofollow noreferrer"">handy tutorial</a> to setup a Google Compute Engine VM instance with data science libraries and Debian GNU/Linux 9 disk image. I ran a data exploration notebook I had put together on my local machine, and found <code>pandas.read_csv()</code> to screw up the import of my training data.</p>

<p>Correctly imported, the dataset is a pandas dataframe with one column ('text'). Each of 3000 entries in that column is an article from a biomedical literature corpus. What happens on the VM though is that some length threshold is applied and pandas shunts part of a given article to a new row of the dataframe. It does this to most but not all of the articles and the dataframe ends up with close to 6000 entries. More importantly, it's useless to try to train a model on.</p>

<p>I cloned my local environment using Vagrant but it looks like it might be difficult to get my disk image into Google Cloud and optimized. So, I thought I would check here first if anyone knows a simpler solution, like perhaps choosing a different machine type than Debian/Linux to set up my Compute Engine instance so that pandas functions work properly. Thanks for your input!</p>
",Training and Model Evaluation,disk image choose google cloud vm panda work doe mac followed handy tutorial setup google compute engine vm instance data science library debian gnu linux disk image ran data exploration notebook put together local machine found screw import training data correctly imported dataset panda dataframe one column text entry column article biomedical literature corpus happens vm though length threshold applied panda shunt part given article new row dataframe doe article dataframe end close entry importantly useless try train model cloned local environment using vagrant look like might difficult get disk image google cloud optimized thought would check first anyone know simpler solution like perhaps choosing different machine type debian linux set compute engine instance panda function work properly thanks input
zero frequency in Naive Bayes,"<p>Can we handle a class which is not a part of training data but present in test data and our predictor classify that data as ‘Other’ label class? </p>

<pre><code>pred &lt;- predict(classifier, data$test)
</code></pre>
",Training and Model Evaluation,zero frequency naive bayes handle class part training data present test data predictor classify data label class
Setting max length of char n-grams for fastText,"<p>I want to compare word2vec and fasttext model based on this comparison tutorial. 
<a href=""https://github.com/jayantj/gensim/blob/fast_text_notebook/docs/notebooks/Word2Vec_FastText_Comparison.ipynb"" rel=""nofollow noreferrer"">https://github.com/jayantj/gensim/blob/fast_text_notebook/docs/notebooks/Word2Vec_FastText_Comparison.ipynb</a></p>

<p>According to this, the semantic accuracy of fastText model increase when we set the max length of char n-grams to zero, such that fastText starts to behave almost like to word2vec. It ignores the ngrams. </p>

<p>However, I can not find any formation on how to set this parameter while loading a fastText model. Any ideas on how to do this?</p>
",Training and Model Evaluation,setting max length char n gram fasttext want compare word vec fasttext model based comparison tutorial according semantic accuracy fasttext model increase set max length char n gram zero fasttext start behave almost like word vec ignores ngrams however find formation set parameter loading fasttext model idea
NLP Aspect Mining approach,"<p>I'm trying to implement as aspect miner based on consumer reviews in amazon for durable- washing machine, refrigerator. The idea is to output sentiment polarity for aspects instead of the entire sentence. For eg: 'Food was good but service was bad' review must output food to be positive and service to be negative. I read through Richard Socher's paper on RNTN model for fine grained sentiment classifier but I guess I'll need to manually tag sentiment for phrases for a different domain and create my own treebank for better accuracy.</p>

<p>Here's an alternate approach I'd thought of. Could someone pls validate/guide me with your feedback
Break the approach into 2 sub tasks. 1) Identify aspects 2) Identify sentiment</p>

<p><strong>Identify aspects</strong></p>

<ol>
<li>Use POS tagger to identify all nouns. This should shortlist
potentially all aspects in the reviews. </li>
<li>Use word2vec of these nouns to determine similar nouns and reduce the dataset size</li>
</ol>

<p><strong>Identify sentiments</strong></p>

<ol start=""3"">
<li>Train a CNN or dense net model on reviews with rating 1,2,4,5(ignore
3 as we need data that has polarity) </li>
<li>Breakdown the test set reviews into phrases(eg 'Food was good') and then score them using the above model </li>
<li>Find the aspects identified in the 1st sub task and tag them to
their respective phrases.</li>
</ol>
",Training and Model Evaluation,nlp aspect mining approach trying implement aspect miner based consumer review amazon durable washing machine refrigerator idea output sentiment polarity aspect instead entire sentence eg food wa good service wa bad review must output food positive service negative read richard socher paper rntn model fine grained sentiment classifier guess need manually tag sentiment phrase different domain create treebank better accuracy alternate approach thought could someone pls validate guide feedback break approach sub task identify aspect identify sentiment identify aspect use po tagger identify noun shortlist potentially aspect review use word vec noun determine similar noun reduce dataset size identify sentiment train cnn dense net model review rating ignore need data ha polarity test set review phrase eg food wa good score using model find aspect identified st sub task tag respective phrase
Where to download a trained LDA model for Gensim?,"<p>I want to use an LDA(Latent Dirichlet Allocation) model for an NLP purpose. 
To train a such a model from Wikipedia corpus takes about 5 to 6 hours and wiki corpus is about 8GB. Check the <a href=""http://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation"" rel=""nofollow noreferrer"">Tutorial</a> </p>

<p>Rather than doing so, is there a place to download a built LDA model and use it directly with Gensim?</p>
",Training and Model Evaluation,download trained lda model gensim want use lda latent dirichlet allocation model nlp purpose train model wikipedia corpus take hour wiki corpus gb check tutorial rather place download built lda model use directly gensim
Classification of Text into multiple categories,"<p>I am working on a project which needs to determine if a word is a fruit. I have tried several approaches but not satisfied with any of the results. Any suggestions?</p>

<p>My training set looks like this</p>

<ul>
<li><strong>Input</strong>: Apple is a fruit. <strong>Output</strong>: Apple.</li>
<li><strong>Input</strong>: Guava is also a fruit <strong>Output</strong>: Guava.</li>
<li><strong>Input</strong>: Pineapple is a seasonal fruit <strong>Output</strong>: Pineapple.</li>
</ul>

<p>Example when running outside training data:</p>

<ul>
<li><strong>Input</strong>: I love all fruits but favorites are guava and apple. <strong>Output</strong>: Guava, Apple</li>
</ul>
",Training and Model Evaluation,classification text multiple category working project need determine word fruit tried several approach satisfied result suggestion training set look like input apple fruit output apple input guava also fruit output guava input pineapple seasonal fruit output pineapple example running outside training data input love fruit favorite guava apple output guava apple
How to rank features by their importance in a Weka classifier?,"<p>I use Weka to successfully build a classifier. I would now like to evaluate how effective or important my features are. Fot this I use AttributeSelection. But I don't know how to ouput the different features with their corresponding importance. I want simply list the features in decreasing order of their information gain scores! </p>
",Training and Model Evaluation,rank feature importance weka classifier use weka successfully build classifier would like evaluate effective important feature fot use attributeselection know ouput different feature corresponding importance want simply list feature decreasing order information gain score
PPDB paraphases searching,"<p>There is a well known lexical resources of paraphrases <a href=""http://paraphrase.org/"" rel=""nofollow"">PPDB</a>.</p>

<p>It comes with several forms from the biggest precision to the biggest recall. The biggest set XXXL for paraphrases contains ~5Gb of data.</p>

<p>I want PPDB for my research and I wounder what is the best engine to perform searching in such a big resources. I didn't try but I think to use it as is in file is not a good idea.</p>

<p>I was thinking about to export all the data to mongo, but I am not sure if this the best solution.</p>

<p>Please if you have some ideas share them with us.</p>

<p>Thank you.</p>
",Training and Model Evaluation,ppdb paraphases searching well known lexical resource paraphrase ppdb come several form biggest precision biggest recall biggest set xxxl paraphrase contains gb data want ppdb research wounder best engine perform searching big resource try think use file good idea wa thinking export data mongo sure best solution please idea share u thank
User Review - Topic modeling or Intent detection in R,"<p>I am doing social media analysis in R - something like, reviewing user feedback on a particular business and trying to distinguish a user review to a category/topic(s). 
For example: Find if the user review talks about Neighborhood or Crime etc.. 
How do I find the intent of a given text? To train a model, I don’t have any pre-determined topics with titles. I am performing a un-known topic analysis.
Topic modeling (LDA) can give us several topics (or high frequent terms for each topic) but to identify the topics that were mentioned in the review is hard. I mean, there can be several words/vocabulary that relates to a topic but how do I understand a user review is exactly talking about a topic called ""Neighborhood"". Any thoughts? Thanks!</p>
",Training and Model Evaluation,user review topic modeling intent detection r social medium analysis r something like reviewing user feedback particular business trying distinguish user review category topic example find user review talk neighborhood crime etc find intent given text train model pre determined topic title performing un known topic analysis topic modeling lda give u several topic high frequent term topic identify topic mentioned review hard mean several word vocabulary relates topic understand user review exactly talking topic called neighborhood thought thanks
low loss with low accuracy in deep neural network,"<p>I recently built a model for POS tagging. I tried an LSTM model and it works well, but I still want to add a CNN layer which rebuilds the original word's vector. The main problem is the flexible length of the sequence, which can be solved by a masking layer when in RNN, but that's not supported by the CNN. I still zero-pad the origin sequence to the MAXLEN and use it as the input of the CNN because the output of these extra words are still mostly zero, and can be solved by the masking layer. </p>

<p>But it seems very bad with low loss and low acc(0.342,0.298) compared with LSTM(0.478,0.871). What is the main reason for this? How can I solve the flexible length problem?'</p>

<pre><code>input_seq = Input(shape=(None, input_dim), )
#conv，RELU
conv_out=Conv1D(
   filters=200,
   kernel_size=3,
   padding='same',
   activation='relu',
   use_bias=1,)(input_seq)
#zero pad 2 at head
pad_out=ZeroPadding1D(padding=(2,0))(conv_out)
#max_pool
pool_out=MaxPool1D(pool_size=3,strides=1,padding='valid')(pad_out)

# masking
mask_out = Masking(mask_value=0.0)(pool_out)
# LSTM
lstm_out = LSTM(units=hidden_unit, return_sequences=True)(mask_out)
# drop_out
drop_out = Dropout(drop_out_rate)(lstm_out)
# softmax
output_seq = TimeDistributed(Dense(output_dim, activation=""softmax""))(drop_out)

# compile
model = Model(inputs=input_seq, outputs=output_seq)
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
</code></pre>

<p>the padding sequences' shape is x(Samples,MAXLEN,200),y(Samples,MAXLEN,42),i use zero-pad for each sequence of x and y.</p>
",Training and Model Evaluation,low loss low accuracy deep neural network recently built model po tagging tried lstm model work well still want add cnn layer rebuilds original word vector main problem flexible length sequence solved masking layer rnn supported cnn still zero pad origin sequence maxlen use input cnn output extra word still mostly zero solved masking layer seems bad low loss low acc compared lstm main reason solve flexible length problem padding sequence shape x sample maxlen sample maxlen use zero pad sequence x
Can I create a topic model (such as LDA) from the output of doc2vec model?,"<p>I did document similarity on my corpus using Doc2Vec and it outputting not that good of similarities. I was wondering if I could do a topic model from what Doc2Vec is giving me to increase the accuracy of my model in order to get better similarities? </p>
",Training and Model Evaluation,create topic model lda output doc vec model document similarity corpus using doc vec outputting good similarity wa wondering could topic model doc vec giving increase accuracy model order get better similarity
Classification Algorithm for text using R,"<p>I wanted to predict class of new document using historical data of text ""description"" and ""class""</p>

<p>Below script I am using , but for new document which I want to predict I am not getting better accuracy , can anyone help me to know which algorithm can be used to increase accuracy. Please advice.</p>

<pre><code>library(plyr)
library(tm)
library(e1071)

setwd(""C:/Data"")

past &lt;- read.csv(""Past - Copy.csv"",header=T,na.strings=c(""""))
future &lt;- read.csv(""Future - Copy.csv"",header=T,na.strings=c(""""))

training &lt;- rbind.fill(past,future)

Res_Desc_Train &lt;- subset(training,select=c(""Class"",""Description""))

##Step 1 : Create Document Matrix of ticket Descriptions available past data

docs &lt;- Corpus(VectorSource(Res_Desc_Train$Description))
docs &lt;-tm_map(docs,content_transformer(tolower))

#remove potentially problematic symbols
toSpace &lt;- content_transformer(function(x, pattern) { return (gsub(pattern, "" "", x))})
removeSpecialChars &lt;- function(x) gsub(""[^a-zA-Z0-9 ]"","""",x)
docs &lt;- tm_map(docs, content_transformer(tolower))
docs &lt;- tm_map(docs, removeNumbers)
docs &lt;- tm_map(docs, removePunctuation)
docs &lt;- tm_map(docs, stripWhitespace)
docs &lt;- tm_map(docs, removeWords, stopwords('english'))


#inspect(docs[440])
dataframe&lt;-data.frame(text=unlist(sapply(docs, `[`, ""content"")), stringsAsFactors=F)

dtm &lt;- DocumentTermMatrix(docs,control=list(stopwords=FALSE,wordLengths =c(2,Inf)))

##Let's remove the variables which are 95% or more sparse.
dtm &lt;- removeSparseTerms(dtm,sparse = 0.95)

Weighteddtm &lt;- weightTfIdf(dtm,normalize=TRUE)
mat.df &lt;- as.data.frame(data.matrix(Weighteddtm), stringsAsfactors = FALSE)
mat.df &lt;- cbind(mat.df, Res_Desc_Train$Class)
colnames(mat.df)[ncol(mat.df)] &lt;- ""Class""
Assignment.Distribution &lt;- table(mat.df$Class)

Res_Desc_Train_Assign &lt;- mat.df$Class

Assignment.Distribution &lt;- table(mat.df$Class)

### Feature has different ranges, normalizing to bring ranges from 0 to 1
### Another way to standardize using z-scores

normalize &lt;- function(x) {
  y &lt;- min(x)
  z &lt;- max(x)
  temp &lt;- x - y
  temp1 &lt;- (z - y)
  temp2 &lt;- temp / temp1
  return(temp2)
}
#normalize(c(1,2,3,4,5))

num_col &lt;- ncol(mat.df)-1
mat.df_normalize &lt;- as.data.frame(lapply(mat.df[,1:num_col], normalize))
mat.df_normalize &lt;- cbind(mat.df_normalize, Res_Desc_Train_Assign)
colnames(mat.df_normalize)[ncol(mat.df_normalize)] &lt;- ""Class""

#names(mat.df)
outcomeName &lt;- ""Class""

train = mat.df_normalize[c(1:nrow(past)),]
test = mat.df_normalize[((nrow(past)+1):nrow(training)),]


train$Class &lt;- as.factor(train$Class) 

###SVM Model
x &lt;- subset(train, select = -Class)
y &lt;- train$Class
model &lt;- svm(x, y, probability = TRUE) 
test1 &lt;- subset(test, select = -Class)
svm.pred &lt;- predict(model, test1, decision.values = TRUE, probability = TRUE)
svm_prob &lt;- attr(svm.pred, ""probabilities"")

finalresult &lt;- cbind(test,svm.pred,svm_prob)
</code></pre>
",Training and Model Evaluation,classification algorithm text using r wanted predict class new document using historical data text description class script using new document want predict getting better accuracy anyone help know algorithm used increase accuracy please advice
causality identification in text with Python NLTK? (followed by subjectivity detection),"<p>I have been hoping to identify causal inferences manifest in short texts revolving around sporting events... For instance, if a sentences goes like ""Ugggh, no way my team lost a point because the referee sucks!"", ""Home base!! Awesome cooperation!!"", etc. I wish to first identify there is causal inference --- my team lost because the referee was not doing his job, or my team won because they cooperate perfectly, etc.</p>

<p>And on top of this identification task, I need extract objectivity/subjectivity score from the reasons/causes identified as above.</p>

<p>I was wondering, other than manually create a lexicon for such purposes, or train a simple classifier by gathering some appropriate training data, probably requiring human annotation, is there a better/quick workaround, preferably using Python (and NLTK?)?</p>

<p>Any input is greatly appreciated!</p>
",Training and Model Evaluation,causality identification text python nltk followed subjectivity detection hoping identify causal inference manifest short text revolving around sporting event instance sentence go like ugggh way team lost point referee suck home base awesome cooperation etc wish first identify causal inference team lost referee wa job team cooperate perfectly etc top identification task need extract objectivity subjectivity score reason cause identified wa wondering manually create lexicon purpose train simple classifier gathering appropriate training data probably requiring human annotation better quick workaround preferably using python nltk input greatly appreciated
train a language model using Google Ngrams,"<p>I want to find a conditional probability of a word given its previous set of words. I plan to use <code>Google N-grams</code> for the same. However, being such a huge resource as it is, I don't think it is computationally possible to do on my PC. ( To process all N-grams, to train a language model). </p>

<p>So is there any way I can train a language model using Google Ngrams ? (Even <code>python NLTK</code> library does not support <code>ngram</code> language model anymore)
Note - I know that a language model can be trained using ngrams, but given the vast size of Google N grams, how can a language model be trained using specifically Google ngrams?</p>
",Training and Model Evaluation,train language model using google ngrams want find conditional probability word given previous set word plan use however huge resource think computationally possible pc process n gram train language model way train language model using google ngrams even library doe support language model anymore note know language model trained using ngrams given vast size google n gram language model trained using specifically google ngrams
Train a text model to predict true or false,"<p>So I am new to the whole machine learning topic but I think I have an interesting problem to solve. I basically just want to know if a sentence conforms to <em>TRUE</em> or <em>FALSE</em></p>

<p>Here a some sample sentences:</p>

<ul>
<li>Yes this is me -> TRUE</li>
<li>This was me -> TRUE</li>
<li>Yes true -> TRUE</li>
<li>This was not me -> FALSE</li>
</ul>

<p>....</p>

<p>Now I would need some hints how I can successfully train a model in e.g. Keras, Caffe or other tools and what kind of principal I should follow.</p>

<p>Thanks for any hints</p>

<p><strong>Update</strong></p>

<p>So from what I understood I need to do natural language classification. I would need to create 2 classes and get the probability for each of the classes back.</p>

<p>Could something like <a href=""https://github.com/Russell91/nlpcaffe"" rel=""nofollow noreferrer"">https://github.com/Russell91/nlpcaffe</a> be useful?</p>
",Training and Model Evaluation,train text model predict true false new whole machine learning topic think interesting problem solve basically want know sentence conforms true false sample sentence yes true wa true yes true true wa false would need hint successfully train model e g kera caffe tool kind principal follow thanks hint update understood need natural language classification would need create class get probability class back could something like useful
NLP short text marking approach,"<p>I am working on a project to evaluate short answer questions for an educational institution.  Here is what I need to do:</p>

<p>Teacher has a sample answer (known to us in advance).
 Sample answer has 3-4 keywords.
 Student enters the answer.  The application should evaluate student's answer as below:</p>

<ul>
<li>Contextual meaning of those keywords should be present in the answer with same/similar relations as in sample answer.</li>
<li>Students are expected to use the synonyms of the keywords.   </li>
<li>Proper relationships of the synonyms are expected as well. </li>
<li>Students are not allowed to use the same        keywords (no marks if
they use the keywords in their answer).</li>
<li>Answers are not more that 2-3 sentences.</li>
</ul>

<p>Can someone guide me what is a good approach for this?  Looking for some starting point to accomplish this.  I am familiar with basics of the NLP, but not worked with much tools available there.  </p>
",Training and Model Evaluation,nlp short text marking approach working project evaluate short answer question educational institution need teacher ha sample answer known u advance sample answer ha keywords student enters answer application evaluate student answer contextual meaning keywords present answer similar relation sample answer student expected use synonym keywords proper relationship synonym expected well student allowed use keywords mark use keywords answer answer sentence someone guide good approach looking starting point accomplish familiar basic nlp worked much tool available
Mallet ML Library printing out same result for different instances,"<p>I was wondering why <a href=""http://mallet.cs.umass.edu/classification.php"" rel=""nofollow noreferrer"">Mallet Classification Model</a> gives  the same output even though my instances are completely different from one another. </p>

<p>I have changed the code in CSV2Classify so it only prints out the top 10 labels and their confidence score. I also made it print out the statistical data of each instance so I can be sure that it is working correctly. However, I don't think the code is the problem, because Mallet seems to classify most instances with the same labeling. Nevertheless, below is the code I changed in CSV2Classify:</p>

<p>1.Getting locations with top confidences:</p>

<pre><code>public static int[] getTopLocations(Labeling labeling, int numberOfCategories) {
    double[] values = new double[labeling.numLocations()];
    for (int location = 0; location &lt; labeling.numLocations(); location++) {
        values[location] = labeling.valueAtLocation(location);          
    }
    int[] outputLocations = indexesOfTopElements(values, numberOfCategories);
    return outputLocations;
}

private static int[] indexesOfTopElements(double[] orig, int nummax) {
    double[] copy = Arrays.copyOf(orig,orig.length);
    Arrays.sort(copy);
    double[] honey = Arrays.copyOfRange(copy,copy.length - nummax, copy.length);
    int[] result = new int[nummax];
    int resultPos = 0;
    for(int i = 0; i &lt; orig.length; i++) {
        double onTrial = orig[i];
        int index = Arrays.binarySearch(honey,onTrial);
        if(index &lt; 0) continue;
        result[resultPos++] = i;
    }
    return result;
}
</code></pre>

<p>2.Classify and printing out instance's data, in order to see if statistical model is working correctly:</p>

<pre><code>    public static void main (String[] args) throws FileNotFoundException, IOException {

        // Process the command-line options
        CommandOption.setSummary (Csv2Classify.class,
                                  ""A tool for classifying a stream of unlabeled instances"");
        CommandOption.process (Csv2Classify.class, args);

        // Print some helpful messages for error cases
        if (args.length == 0) {
            CommandOption.getList(Csv2Classify.class).printUsage(false);
            System.exit (-1);
        }
        if (inputFile == null) {
            throw new IllegalArgumentException (""You must include `--input FILE ...' in order to specify a""+
                                ""file containing the instances, one per line."");
        }

      // Read classifier from file
        Classifier classifier = null;
        try {
            ObjectInputStream ois =
                new ObjectInputStream (new BufferedInputStream(new FileInputStream (classifierFile.value)));

            classifier = (Classifier) ois.readObject();
            ois.close();
        } catch (Exception e) {
            throw new IllegalArgumentException(""Problem loading classifier from file "" + classifierFile.value +
                               "": "" + e.getMessage());
        }

        // Read instances from the file
        Reader fileReader;
        if (inputFile.value.toString().equals (""-"")) {
            fileReader = new InputStreamReader (System.in);
        }
        else {
            fileReader = new InputStreamReader(new FileInputStream(inputFile.value), encoding.value);
        }
        Iterator&lt;Instance&gt; csvIterator =
            new CsvIterator (fileReader, Pattern.compile(lineRegex.value),
            dataOption.value, 0, nameOption.value);
        Iterator&lt;Instance&gt; iterator =
            classifier.getInstancePipe().newIteratorFrom(csvIterator);

        // Write classifications to the output file
        PrintStream out = null;

        if (outputFile.value.toString().equals (""-"")) {
            out = System.out;
        }
        else {
            out = new PrintStream(outputFile.value, encoding.value);
        }

        // gdruck@cs.umass.edu
        // Stop growth on the alphabets. If this is not done and new
        // features are added, the feature and classifier parameter
        // indices will not match.
        classifier.getInstancePipe().getDataAlphabet().stopGrowth();
        classifier.getInstancePipe().getTargetAlphabet().stopGrowth();

        while (iterator.hasNext()) {
            Instance instance = iterator.next();

            Labeling labeling =
                classifier.classify(instance).getLabeling();

            StringBuilder output = new StringBuilder();
            output.append(instance.getName() + ""\n"");                       
            output.append(""\t"" + instance.getData() + ""\n"");
            int[] topLocations = Csv2Classify.getTopLocations(labeling, 10);
            for (int index = 0; index &lt; topLocations.length; index++) {
                int location = topLocations[index];
                System.out.print(""location printed:"" + location + ""\n"");
                output.append(""\t"" + labeling.labelAtLocation(location));
                output.append(""\t"" + labeling.valueAtLocation(location));
            }
            output.append(""\n"");
            out.println(output);
        }

        if (! outputFile.value.toString().equals (""-"")) {
            out.close();
        }
    }
}   
</code></pre>

<p>I trained using decision tree, at 50 trials. Then, the command I used to classify is:</p>

<pre><code>bin/mallet classify-file --input data --output classification.output --classifier decision_tree.classifier
</code></pre>

<p>Source file:</p>

<pre><code>10914642 sky room business office people young teamwork success professional monitor contemporary entrepreneur customer idea businesspeople window adult person pc indoor guy worker confident attractive operator male job career communication handsome book chair businessman manager work table meeting workplace cloud headset caucasian man lamp executive successful corporate occupation concept
13209539 performance industry panel results photovoltaic technology energy security_helmet nature running eco-friendly ecology sky android touchpad function environment businessman plant solar_panel architecture man analysis worker renewable_energy wireless business electronic_tablet solarium construction engineering cell operation electricity power light sensor electric collector durable setup senior alternative installation solarization checking touchscreen engineer
26375762 building hat occupation expert plan professional engineering confident architector business businessman designer helmet engineer man hardhat architect construction worker suit executive work builder
26780099 desk male sitting technology headphone arabian laptop flare office resting business startup morning job person work casual eastern men creative indoors businessman workplace relax play lifestyle worker phone beard sunset arab sunrise computer professional sun break effect handsome young leisure legs happy hipster people
26783548 lifestyle manager male elegance use one adult work executive cellphone intelligence notebook business laptop technology entrepreneur middle-aged modern smart busy communication job businessman contemporary occupation corporate urban smile man message senior mobile caucasian city computer hold expertise suit professional wireless restaurant internet look break worker phone cafe table smartphone sit
26783561 elegant intelligent caucasian read urban friendly coffee laptop table hold businessman folder sit adult cafeteria computer concentration senior informed lifestyle restaurant male smartphone confident drink look worker one entrepreneur break business middle-aged corporate job cup smart professional work cafe successful technology modern city executive document serious paper man contemporary beard
26958424 serious male formal connection vision city executive mature worker businessman professional phone glasses aspirations call confidence street confident communication corporate feminism successful outdoors solution device lifestyle guy calling standing smart adult caucasian business shirt technology success person suit outside entrepreneur tie urban building man gadget thoughtful smartphone
27207487 leisure window business resting mustache networking beard lonely rest hobby relaxation thinking break lifestyle pool_table mobile_phone sitting room style alone man businessman pool connection locker technology relax
27210236 information workplace development office plan organization research entrepreneur office_worker meeting discussion businessman business_people interaction corporate operations analysis busy strategic process strategy workspace cooperation communication white_collar_worker talking statistics enterpriser corporate_business motivation global_finance investment objective global_market working mission business collaboration thinking global_business planning solution vision tactics marketing
27344048 businessman brick alone management telecommunication planning plan connection working talking internet computer white_collar_worker workspace technology research startup mobile_phone office business window brick_wall laptop strategy place_of_work online on_the_phone man thinking wireless digital_device workplace business_person communication
</code></pre>

<p>Result file:</p>

<pre><code>10914642
    person(62)=1.0
job(121)=1.0
occupation(128)=1.0
work(159)=1.0
man(195)=1.0
male(203)=1.0
attractive(204)=1.0
handsome(206)=1.0
confident(209)=1.0
worker(210)=1.0
adult(220)=1.0
caucasian(222)=1.0
young(238)=1.0
people(239)=1.0
professional(327)=1.0
guy(354)=1.0
business(369)=1.0
successful(370)=1.0
executive(371)=1.0
success(376)=1.0
office(379)=1.0
entrepreneur(382)=1.0
corporate(390)=1.0
businesspeople(392)=1.0
businessman(395)=1.0
table(443)=1.0
manager(506)=1.0
meeting(520)=1.0
communication(560)=1.0
teamwork(579)=1.0
indoor(615)=1.0
room(622)=1.0
idea(729)=1.0
workplace(737)=1.0
contemporary(740)=1.0
career(798)=1.0
lamp(829)=1.0
concept(850)=1.0
window(924)=1.0
book(1216)=1.0
cloud(1318)=1.0
sky(1333)=1.0
headset(1595)=1.0
chair(1808)=1.0
customer(2171)=1.0
operator(2345)=1.0
monitor(2741)=1.0

    9466    0.20320855614973263 9467    0.10160427807486631 9505    0.0427807486631016  9514    0.016042780748663103    9468    0.053475935828877004    9462    0.13903743315508021 9463    0.13368983957219252 9460    0.22994652406417113 9486    0.0374331550802139  9506    0.016042780748663103

13209539
    nature(55)=1.0
man(195)=1.0
worker(210)=1.0
industry(218)=1.0
construction(232)=1.0
business(369)=1.0
businessman(395)=1.0
environment(550)=1.0
light(552)=1.0
power(567)=1.0
engineering(617)=1.0
engineer(630)=1.0
senior(653)=1.0
performance(669)=1.0
energy(790)=1.0
electric(1001)=1.0
electricity(1007)=1.0
checking(1295)=1.0
sky(1333)=1.0
installation(1350)=1.0
technology(1395)=1.0
wireless(1461)=1.0
analysis(1797)=1.0
plant(2419)=1.0
alternative(2693)=1.0
results(2748)=1.0
architecture(3206)=1.0
android(3253)=1.0
touchpad(3256)=1.0
running(3294)=1.0
panel(3598)=1.0
photovoltaic(3599)=1.0
security_helmet(3600)=1.0
eco-friendly(3601)=1.0
ecology(3602)=1.0
function(3603)=1.0
solar_panel(3604)=1.0
renewable_energy(3605)=1.0
electronic_tablet(3606)=1.0
solarium(3607)=1.0
cell(3608)=1.0
operation(3609)=1.0
sensor(3610)=1.0
collector(3611)=1.0
durable(3612)=1.0
setup(3613)=1.0
solarization(3614)=1.0
touchscreen(3615)=1.0

    9466    0.20320855614973263 9467    0.10160427807486631 9505    0.0427807486631016  9514    0.016042780748663103    9468    0.053475935828877004    9462    0.13903743315508021 9463    0.13368983957219252 9460    0.22994652406417113 9486    0.0374331550802139  9506    0.016042780748663103

26375762
    occupation(128)=1.0
work(159)=1.0
man(195)=1.0
hat(208)=1.0
confident(209)=1.0
worker(210)=1.0
hardhat(223)=1.0
construction(232)=1.0
professional(327)=1.0
plan(346)=1.0
business(369)=1.0
executive(371)=1.0
suit(377)=1.0
businessman(395)=1.0
building(542)=1.0
engineering(617)=1.0
engineer(630)=1.0
helmet(634)=1.0
builder(1017)=1.0
designer(2675)=1.0
expert(3458)=1.0
architect(4485)=1.0
architector(7523)=1.0

    9466    0.20320855614973263 9467    0.10160427807486631 9505    0.0427807486631016  9514    0.016042780748663103    9468    0.053475935828877004    9462    0.13903743315508021 9463    0.13368983957219252 9460    0.22994652406417113 9486    0.0374331550802139  9506    0.016042780748663103

26780099
    person(62)=1.0
job(121)=1.0
work(159)=1.0
male(203)=1.0
handsome(206)=1.0
worker(210)=1.0
lifestyle(230)=1.0
young(238)=1.0
people(239)=1.0
men(241)=1.0
eastern(320)=1.0
happy(323)=1.0
professional(327)=1.0
play(364)=1.0
business(369)=1.0
office(379)=1.0
indoors(389)=1.0
businessman(395)=1.0
laptop(512)=1.0
computer(517)=1.0
sitting(588)=1.0
casual(631)=1.0
workplace(737)=1.0
leisure(783)=1.0
phone(804)=1.0
beard(871)=1.0
desk(928)=1.0
sun(1329)=1.0
technology(1395)=1.0
headphone(1639)=1.0
creative(2040)=1.0
relax(2166)=1.0
break(2247)=1.0
legs(2256)=1.0
morning(2415)=1.0
sunset(2538)=1.0
hipster(3302)=1.0
sunrise(3325)=1.0
arabian(3671)=1.0
startup(3699)=1.0
effect(4314)=1.0
resting(5592)=1.0
flare(7660)=1.0
arab(7661)=1.0

    9466    0.20320855614973263 9467    0.10160427807486631 9505    0.0427807486631016  9514    0.016042780748663103    9468    0.053475935828877004    9462    0.13903743315508021 9463    0.13368983957219252 9460    0.22994652406417113 9486    0.0374331550802139  9506    0.016042780748663103

26783548
    job(121)=1.0
occupation(128)=1.0
smile(150)=1.0
work(159)=1.0
man(195)=1.0
one(199)=1.0
male(203)=1.0
worker(210)=1.0
adult(220)=1.0
caucasian(222)=1.0
lifestyle(230)=1.0
professional(327)=1.0
business(369)=1.0
executive(371)=1.0
suit(377)=1.0
entrepreneur(382)=1.0
corporate(390)=1.0
businessman(395)=1.0
table(443)=1.0
restaurant(448)=1.0
manager(506)=1.0
laptop(512)=1.0
computer(517)=1.0
modern(540)=1.0
look(557)=1.0
communication(560)=1.0
message(574)=1.0
elegance(644)=1.0
senior(653)=1.0
smart(732)=1.0
contemporary(740)=1.0
busy(746)=1.0
expertise(775)=1.0
phone(804)=1.0
mobile(881)=1.0
cellphone(1223)=1.0
hold(1237)=1.0
sit(1364)=1.0
technology(1395)=1.0
wireless(1461)=1.0
use(1645)=1.0
internet(1805)=1.0
notebook(1809)=1.0
city(1865)=1.0
smartphone(2216)=1.0
break(2247)=1.0
urban(3021)=1.0
middle-aged(3492)=1.0
cafe(4856)=1.0
intelligence(4943)=1.0

    9466    0.20320855614973263 9467    0.10160427807486631 9505    0.0427807486631016  9514    0.016042780748663103    9468    0.053475935828877004    9462    0.13903743315508021 9463    0.13368983957219252 9460    0.22994652406417113 9486    0.0374331550802139  9506    0.016042780748663103

26783561
    drink(109)=1.0
job(121)=1.0
work(159)=1.0
man(195)=1.0
one(199)=1.0
male(203)=1.0
confident(209)=1.0
worker(210)=1.0
adult(220)=1.0
caucasian(222)=1.0
serious(224)=1.0
lifestyle(230)=1.0
professional(327)=1.0
friendly(329)=1.0
business(369)=1.0
successful(370)=1.0
executive(371)=1.0
entrepreneur(382)=1.0
corporate(390)=1.0
businessman(395)=1.0
table(443)=1.0
restaurant(448)=1.0
elegant(455)=1.0
cup(485)=1.0
laptop(512)=1.0
computer(517)=1.0
folder(523)=1.0
modern(540)=1.0
look(557)=1.0
senior(653)=1.0
smart(732)=1.0
contemporary(740)=1.0
document(744)=1.0
paper(745)=1.0
beard(871)=1.0
coffee(947)=1.0
hold(1237)=1.0
sit(1364)=1.0
read(1372)=1.0
technology(1395)=1.0
concentration(1428)=1.0
city(1865)=1.0
smartphone(2216)=1.0
break(2247)=1.0
urban(3021)=1.0
middle-aged(3492)=1.0
cafeteria(4134)=1.0
cafe(4856)=1.0
intelligent(5655)=1.0
informed(7666)=1.0

    9466    0.20320855614973263 9467    0.10160427807486631 9505    0.0427807486631016  9514    0.016042780748663103    9468    0.053475935828877004    9462    0.13903743315508021 9463    0.13368983957219252 9460    0.22994652406417113 9486    0.0374331550802139  9506    0.016042780748663103

26958424
    person(62)=1.0
confidence(194)=1.0
man(195)=1.0
male(203)=1.0
confident(209)=1.0
worker(210)=1.0
adult(220)=1.0
caucasian(222)=1.0
serious(224)=1.0
outdoors(227)=1.0
lifestyle(230)=1.0
standing(236)=1.0
professional(327)=1.0
guy(354)=1.0
tie(367)=1.0
business(369)=1.0
successful(370)=1.0
executive(371)=1.0
success(376)=1.0
suit(377)=1.0
entrepreneur(382)=1.0
corporate(390)=1.0
businessman(395)=1.0
glasses(456)=1.0
building(542)=1.0
mature(553)=1.0
communication(560)=1.0
smart(732)=1.0
formal(733)=1.0
outside(784)=1.0
phone(804)=1.0
shirt(876)=1.0
solution(1021)=1.0
technology(1395)=1.0
connection(1466)=1.0
city(1865)=1.0
smartphone(2216)=1.0
vision(2313)=1.0
call(2347)=1.0
calling(2491)=1.0
device(2574)=1.0
urban(3021)=1.0
thoughtful(3358)=1.0
street(3428)=1.0
gadget(4981)=1.0
feminism(7584)=1.0
aspirations(7741)=1.0

    9466    0.20320855614973263 9467    0.10160427807486631 9505    0.0427807486631016  9514    0.016042780748663103    9468    0.053475935828877004    9462    0.13903743315508021 9463    0.13368983957219252 9460    0.22994652406417113 9486    0.0374331550802139  9506    0.016042780748663103

27207487
    man(195)=1.0
lifestyle(230)=1.0
style(306)=1.0
business(369)=1.0
businessman(395)=1.0
sitting(588)=1.0
room(622)=1.0
leisure(783)=1.0
beard(871)=1.0
window(924)=1.0
technology(1395)=1.0
connection(1466)=1.0
thinking(1487)=1.0
alone(1978)=1.0
relax(2166)=1.0
break(2247)=1.0
mobile_phone(2284)=1.0
mustache(2332)=1.0
hobby(2524)=1.0
relaxation(2702)=1.0
networking(3698)=1.0
pool(5219)=1.0
resting(5592)=1.0
rest(5768)=1.0
lonely(7803)=1.0
pool_table(7804)=1.0
locker(7805)=1.0

    9466    0.20320855614973263 9467    0.10160427807486631 9505    0.0427807486631016  9514    0.016042780748663103    9468    0.053475935828877004    9462    0.13903743315508021 9463    0.13368983957219252 9460    0.22994652406417113 9486    0.0374331550802139  9506    0.016042780748663103

27210236
    information(41)=1.0
working(205)=1.0
plan(346)=1.0
business(369)=1.0
office(379)=1.0
entrepreneur(382)=1.0
corporate(390)=1.0
businessman(395)=1.0
meeting(520)=1.0
cooperation(524)=1.0
communication(560)=1.0
discussion(600)=1.0
collaboration(718)=1.0
interaction(734)=1.0
workplace(737)=1.0
busy(746)=1.0
planning(759)=1.0
strategy(760)=1.0
talking(781)=1.0
solution(1021)=1.0
research(1081)=1.0
business_people(1222)=1.0
thinking(1487)=1.0
development(1596)=1.0
analysis(1797)=1.0
mission(1887)=1.0
global_business(2246)=1.0
office_worker(2274)=1.0
vision(2313)=1.0
tactics(2586)=1.0
investment(3004)=1.0
marketing(3197)=1.0
organization(3801)=1.0
motivation(3802)=1.0
operations(4660)=1.0
white_collar_worker(4840)=1.0
process(4983)=1.0
corporate_business(5542)=1.0
workspace(5706)=1.0
enterpriser(6814)=1.0
strategic(7806)=1.0
statistics(7807)=1.0
global_finance(7808)=1.0
objective(7809)=1.0
global_market(7810)=1.0

    9466    0.20320855614973263 9467    0.10160427807486631 9505    0.0427807486631016  9514    0.016042780748663103    9468    0.053475935828877004    9462    0.13903743315508021 9463    0.13368983957219252 9460    0.22994652406417113 9486    0.0374331550802139  9506    0.016042780748663103

27344048
    man(195)=1.0
working(205)=1.0
plan(346)=1.0
business(369)=1.0
office(379)=1.0
businessman(395)=1.0
business_person(507)=1.0
place_of_work(510)=1.0
laptop(512)=1.0
computer(517)=1.0
communication(560)=1.0
workplace(737)=1.0
planning(759)=1.0
strategy(760)=1.0
talking(781)=1.0
window(924)=1.0
research(1081)=1.0
technology(1395)=1.0
wireless(1461)=1.0
connection(1466)=1.0
thinking(1487)=1.0
online(1804)=1.0
internet(1805)=1.0
alone(1978)=1.0
mobile_phone(2284)=1.0
management(2985)=1.0
startup(3699)=1.0
digital_device(4572)=1.0
white_collar_worker(4840)=1.0
brick(5553)=1.0
workspace(5706)=1.0
telecommunication(7883)=1.0
brick_wall(7884)=1.0
on_the_phone(7885)=1.0

    9466    0.20320855614973263 9467    0.10160427807486631 9505    0.0427807486631016  9514    0.016042780748663103    9468    0.053475935828877004    9462    0.13903743315508021 9463    0.13368983957219252 9460    0.22994652406417113 9486    0.0374331550802139  9506    0.016042780748663103



    9480    0.03642671292281006 9496    0.033824804856895055    9481    0.03469210754553339 9499    0.03555941023417172 9491    0.03295750216825672 9517    0.03816131830008673 9501    0.03469210754553339 9492    0.03469210754553339 9478    0.03469210754553339 9506    0.03469210754553339
</code></pre>

<p>I will also include links to my training data and classifier file, just in case I made a mistake with parsing training data or training process:</p>

<ul>
<li><a href=""https://drive.google.com/open?id=0B9_0IQ9zpJH2Nk8wU1hScVhKbzg"" rel=""nofollow noreferrer"">Classifier File</a></li>
<li><a href=""https://drive.google.com/open?id=0B9_0IQ9zpJH2ZkU3b0NfY3NMMVU"" rel=""nofollow noreferrer"">Checkpoint File</a></li>
</ul>

<p>Any help would be much appreciated.</p>
",Training and Model Evaluation,mallet ml library printing result different instance wa wondering mallet classification model give output even though instance completely different one another changed code csv classify print top label confidence score also made print statistical data instance sure working correctly however think code problem mallet seems classify instance labeling nevertheless code changed csv classify getting location top confidence classify printing instance data order see statistical model working correctly trained using decision tree trial command used classify source file result file also include link training data classifier file case made mistake parsing training data training process classifier file checkpoint file help would much appreciated
Mallet CRF Sequence Classification Training Data Format,"<p>I am trying to train a CRF sequence model using the Mallet library but I am missing some important information.  I found a an example in the library itself at <a href=""https://github.com/mimno/Mallet/blob/master/src/cc/mallet/examples/TrainCRF.java"" rel=""nofollow noreferrer"">https://github.com/mimno/Mallet/blob/master/src/cc/mallet/examples/TrainCRF.java</a>
however the example does not state the format of the input training data so I do not know how to recreate it.</p>

<p>Mallet does have a data import example at <a href=""http://mallet.cs.umass.edu/import-devel.php"" rel=""nofollow noreferrer"">http://mallet.cs.umass.edu/import-devel.php</a> but the particular example seems to be for document classification and not CRF sequence models which is my use case.  </p>

<p>I tried putting the input training data in the form used at <a href=""http://mallet.cs.umass.edu/sequences.php"" rel=""nofollow noreferrer"">http://mallet.cs.umass.edu/sequences.php</a> i.e.</p>

<pre><code>Bill CAPITALIZED noun
slept non-noun
here LOWERCASE STOPWORD non-noun
</code></pre>

<p>and test data in the form</p>

<pre><code>CAPITAL Al
        slept
        here
</code></pre>

<p>however based on the output logs it does not seem to be the correct format.  For example one line in the log is <code>INFO: testing label slept P � R 0 F1 �</code> but <code>slept</code> is not a label - the labels should be <code>noun</code> or <code>non-noun</code>.  </p>

<p>So if someone could tell me what format the training data should be in that would be great.</p>
",Training and Model Evaluation,mallet crf sequence classification training data format trying train crf sequence model using mallet library missing important information found example library however example doe state format input training data know recreate mallet doe data import example particular example seems document classification crf sequence model use case tried putting input training data form used e test data form however based output log doe seem correct format example one line log label label someone could tell format training data would great
Python NLTK Shakespeare corpus,"<p>I am trying to import sentences from Shakespeare's NLTK corpus – following <a href=""http://www.nltk.org/howto/corpus.html#shakespeare"" rel=""nofollow noreferrer"">this</a> help site – but I am having trouble getting access to the sentences (in order to train a word2vec model) :</p>

<pre><code>from nltk.corpus import shakespeare #XMLCorpusreader
shakespeare.fileids()
['a_and_c.xml', 'dream.xml', 'hamlet.xml', 'j_caesar.xml', ...]

play = shakespeare.xml('dream.xml') #ElementTree object
print(play)
&lt;Element 'PLAY' at ...&gt;

for i in range(9):
    print('%s: %s' % (play[i].tag, play[i].text))
</code></pre>

<p>Returns the following :</p>

<pre><code>TITLE: A Midsummer Night's Dream
PERSONAE: 

SCNDESCR: SCENE  Athens, and a wood near it.
PLAYSUBT: A MIDSUMMER NIGHT'S DREAM
ACT: None
ACT: None
ACT: None
ACT: None
ACT: None
</code></pre>

<p>Why are all the acts None ?</p>

<p>None of the methods defined here (<a href=""http://www.nltk.org/howto/corpus.html#data-access-methods"" rel=""nofollow noreferrer"">http://www.nltk.org/howto/corpus.html#data-access-methods</a>) (.sents(), tagged_sents(), chunked_sents(), parsed_sents()) seem to work when applied to the shakespeare XMLCorpusReader</p>

<p>I'd like to understand :<br>
1/ how to get the sentences </p>

<p>2/ how to know how to look for them in an ElementTree object</p>
",Training and Model Evaluation,python nltk shakespeare corpus trying import sentence shakespeare nltk corpus following help site trouble getting access sentence order train word vec model return following act none none method defined sent tagged sent chunked sent parsed sent seem work applied shakespeare xmlcorpusreader like understand get sentence know look elementtree object
supervised tag suggestion for documents,"<p>I have thousands of documents with associated tag information. However i also have many documents without tags.</p>

<p>I want to train a model on the documents WITH tags and then apply the trained classifier to the UNTAGGED documents; the classifier will then suggest the most appropriate tags for each UNTAGGED document.</p>

<p>I have done quite a lot of research and there doesn't seem to be a SUPERVISED implementation to document tag classification.</p>

<p>I know NLTK, gensim, word2vec and other libraries will be useful for this problem.</p>

<p>I will be coding the project in Python.</p>

<p>Any help would be greatly appreciated.</p>
",Training and Model Evaluation,supervised tag suggestion document thousand document associated tag information however also many document without tag want train model document tag apply trained classifier untagged document classifier suggest appropriate tag untagged document done quite lot research seem supervised implementation document tag classification know nltk gensim word vec library useful problem coding project python help would greatly appreciated
converting from a list to dictionary for a word2vec model,"<p>i have a huge corpus of data in my my text file that i want to train for skip gram model.
i have split  the data from file into list
now i want to count the words with their number of occurrence and make a dictionary ,give the word as key to the dictionary and frequency as the value.here is a snippet of my code</p>

<pre><code>with open(""enwik8"",""r"") as data:
    words=data.read().split()   

vocabulary_size = 5000


  count = [['UNK', -1]]
  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
</code></pre>

<p>i have succesfully made a list with the words and their frequency upto first most common 50000 words,now i need to feed them to dictionary,key as a word and value as freq.</p>

<pre><code>dictionary = dict()
for word, _ in count:
</code></pre>

<p>can anyone help me through??</p>
",Training and Model Evaluation,converting list dictionary word vec model huge corpus data text file want train skip gram model split data file list want count word number occurrence make dictionary give word key dictionary frequency value snippet code succesfully made list word frequency upto first common word need feed dictionary key word value freq anyone help
Identify the person referred to in an email using ML/NLP,"<p>I am working on an NLP project, wherein I have a list of emails all related to appreciation. I am trying to determine from the email content, who is being appreciated. This in turn will help the organization in our performance evaluation program.</p>

<p>Apart from identifying who is being appreciated, I am also trying to identify the type of work a person has done and score it. I am using open NLP (max entropy/logistic regression) for classification of the email and use some form of heuristics to identify the person being appreciated. </p>

<p>The approach for person identification is as follows:</p>

<ol>
<li>Determine if an email is related to appreciation</li>
<li>Get the list of people in the ""To:"" list</li>
<li>Check if that person is being referred to in the email</li>
<li>Tag that person as the receiver of appreciation</li>
</ol>

<p>However, this approach is very simple and does not work for complex emails we generally see. An email can consist of many email ids or people being referred to and they are not the receivers of the appreciation. The context of the person is not available and hence the accuracy is not very good.</p>

<p>I am thinking of using HMM and word2vec to solve the person issue. I would appreciate if anyone has come across this problem or has any suggestion.</p>
",Training and Model Evaluation,identify person referred email using ml nlp working nlp project wherein list email related appreciation trying determine email content appreciated turn help organization performance evaluation program apart identifying appreciated also trying identify type work person ha done score using open nlp max entropy logistic regression classification email use form heuristic identify person appreciated approach person identification follows determine email related appreciation get list people list check person referred email tag person receiver appreciation however approach simple doe work complex email generally see email consist many email id people referred receiver appreciation context person available hence accuracy good thinking using hmm word vec solve person issue would appreciate anyone ha come across problem ha suggestion
Use word2vec and seq2seq model in Keras,"<p>My purpose is build something like Q&amp;A bot that can generate sentences according to the input sentences of user. I use pre-trained word2vec in gensim for my input of model.<br>
My words are chinese, but I think it doesn't matter to word2vec. 
I first turned each sentence into a 3d-array. The shape is</p>

<p>(sample_n, time_step, word_dim)</p>

<pre><code>x = [sentence_1, sentence_2, ... , sentence_n]
sentence = [word_1, word_2, ...]
word = [250 dimensions array]
</code></pre>

<p>time_step is equal to the length of sequence.The sequences have already done the zero padding. Thus, the length is fixed.</p>

<p>Next, I build a simple seq2seq model like that:<br>
(I said ""simple"", because it doesn't have any attention layer and feed last output to current input in decoder.)</p>

<pre><code>model = Sequential

# Encoder
model.add(LSTM(units=HIDDEN_SIZE, input_shape=(X_TIME_STEP, WORD_DIM), return_sequences=True))
model.add(Dropout(0.1))
model.add(LSTM(units=HIDDEN_SIZE, input_shape=(X_TIME_STEP, WORD_DIM), return_sequences=False))
model.add(Dense(HIDDEN_SIZE, activation=""relu""))
model.add(RepeatVector(Y_TIME_STEP))

# Decoder
model.add(LSTM(units=WORD_DIM, return_sequences=True))
model.add(Dropout(0.1))
model.add(LSTM(units=WORD_DIM, return_sequences=True))
model.add(TimeDistributed(Dense(WORD_DIM, activation=""linear"")))

optimizer = optimizers.Adam()
model.compile(loss=""categorical_crossentropy"", optimizer=optimizer, metrics=['acc'])

model.fit(train_x, train_y, batch_size=BATCH_SIZE, epochs=EPOCHS)
</code></pre>

<p>The loss value was negative during training.
Afterward, I just use training data to do prediction and turn the output vector back to the sentence.<br>
However, the words in sentence are all the same or something weird.  </p>

<p>I changed loss function to ""mse"". It seems that it improved a little bit and at least the loss wasn't negative, but I think that is not a correct way to solve this.<br>
What I can figure out that is the value of word2vec in each dimension is <strong>not between 0 and 1</strong>.</p>

<ul>
<li><p>In order to use ""categorical_crossentropy"", It need to do normalization with word vector before training and use softmax for activation function of output layer.<br>
After prediction, convert the normalized vector back to origin(called denormalized?) and then turn it into sentences.<br>
If this is fine, what method that I can use to normalized?</p></li>
<li><p>just change the loss function or layer that can deal with the unbound value.</p></li>
</ul>

<p>I am new to NN and Keras, is there any way to solve it? Thanks!</p>
",Training and Model Evaluation,use word vec seq seq model kera purpose build something like q bot generate sentence according input sentence user use pre trained word vec gensim input model word chinese think matter word vec first turned sentence array shape sample n time step word dim time step equal length sequence sequence already done zero padding thus length fixed next build simple seq seq model like said simple attention layer feed last output current input decoder loss value wa negative training afterward use training data prediction turn output vector back sentence however word sentence something weird changed loss function mse seems improved little bit least loss negative think correct way solve figure value word vec dimension order use categorical crossentropy need normalization word vector training use softmax activation function output layer prediction convert normalized vector back origin called denormalized turn sentence fine method use normalized change loss function layer deal unbound value new nn kera way solve thanks
A Binary Classification of determining whether a given word is disease or not,"<p>I wrote a code for predicting disease in a given sentence.
The following are the features used</p>

<pre><code>POS Tags
IOB tagger
</code></pre>

<p>Output was simple either the given word is disease or not a disease.
I have made a LSTM model with following specification.</p>

<pre><code>256 hidden units (Single Layered)
20% droupout
20 epochs
1 batch size
</code></pre>

<p>Astonishingly I got a 100% accuracy on  private test cases (i.e. On documents which were not trained), is this even possible ?</p>
",Training and Model Evaluation,binary classification determining whether given word disease wrote code predicting disease given sentence following feature used output wa simple either given word disease disease made lstm model following specification astonishingly got accuracy private test case e document trained even possible
Different accuracy in model.fit() and predict_classes(),"<p>Can Someone please explain why there are different accuracy in model.fit() and predict_classes() ?</p>

<p>model.evaluate gave ('Test score:', 0.5457103276905948) and ('Test accuracy:', 0.94977169167505548) using:</p>

<pre><code>score = model.evaluate(x_val, y_val, batch_size=batch_size, verbose=0)
# print ('Raw test score:', score)
print('Test score:', score[0])
print('Test accuracy:', score[1])
</code></pre>

<p>and the accuracy_score of model.predict_classes on Train and test data gave 
Training accuracy:94.17% and 
Testing accuracy: 88.61% </p>

<p>Code is given here: <a href=""https://gist.github.com/dirko/375397bc942d134a3c82d0dd514f3fea"" rel=""nofollow noreferrer"">https://gist.github.com/dirko/375397bc942d134a3c82d0dd514f3fea</a></p>
",Training and Model Evaluation,different accuracy model fit predict class someone please explain different accuracy model fit predict class model evaluate gave test score test accuracy using accuracy score model predict class train test data gave training accuracy testing accuracy code given
Best Option to Search with autocomplete with suggestions?,"<p>I hope to develop a search function like google for my website with auto complete. scenarios are below </p>

<ul>
<li>User Can input keywords with spelling mistakes and get the correct suggestion only for items in our site </li>
<li>the user must get autocomplete suggestions if they are typing a word</li>
<li>Must be able to input keywords and to train a model for suggestions </li>
</ul>

<p>After little research, i found below products are capable of doing such a task </p>

<ul>
<li>Apache Solr</li>
<li>Elasticsearch</li>
</ul>

<p>can someone explain about these two services and tell me what can be developed with a minimum development effort and maximum efficiency? and if there are any other products match the scenario mentions above suggest me, please.  </p>
",Training and Model Evaluation,best option search autocomplete suggestion hope develop search function like google website auto complete scenario user input keywords spelling mistake get correct suggestion item site user must get autocomplete suggestion typing word must able input keywords train model suggestion little research found product capable task apache solr elasticsearch someone explain two service tell developed minimum development effort maximum efficiency product match scenario mention suggest please
Specifying the # of hidden units in Facebook fasttext,"<p>In the <a href=""https://arxiv.org/pdf/1607.01759v3.pdf"" rel=""nofollow noreferrer"">paper on fasttext</a> for supervised classification, the authors specified various quantities of hidden units by altering some parameter  (h is the one on pages 3,4 - In table 1 you see ""It has 10 hidden units and we evaluate it with and without bigrams."") But after reading <a href=""https://github.com/facebookresearch/fastText/blob/master/README.md#full-documentation"" rel=""nofollow noreferrer"">the documentation</a> it does not appear that there is a ""hidden unit"" parameter to alter.  Is there a way to specify the number of hidden units? Or is this the same as specifying the -dim option? </p>
",Training and Model Evaluation,specifying hidden unit facebook fasttext paper fasttext supervised classification author specified various quantity hidden unit altering parameter h one page table see ha hidden unit evaluate without bigram reading documentation doe appear hidden unit parameter alter way specify number hidden unit specifying dim option
Manipulate Nested Boolean Query String in Python,"<p>I have string boolean queries like this</p>

<pre><code>   queryString= """"""And(
                      OR(abc,xyz,wxy),
                      AND(AND(xyz,wxy),xzy),
                      XOR(x1,y1, AND(xy,zz))  
                      )""""""
</code></pre>

<p>At current it is hard for me to modify the above query string, as I want to </p>

<ol>
<li>Add another <code>OR(x3,y3)</code> in the last <code>XOR</code> </li>
<li>Remove entire  <code>OR(abc,xyz,wxy)</code></li>
</ol>

<p>with desired output</p>

<pre><code>   resultQueryString= """"""And(                        
                            AND(AND(xyz,wxy),xzy),
                            XOR(x1,y1, AND(xy,zz),OR(x3,y3))  
                            )""""""
</code></pre>

<p>I think I cannot easily do it unless I come up with a sophisticated regex for each different query.</p>

<p>I am trying to write a python function which would take above string boolean query as input and output a tree data structure.</p>

<p>So that I can traverse the tree and evaluate or change whatever portion of query I want to change.</p>

<p>In above example, if I had it as a tree, I can easily see the root is <code>AND</code> and traverse/modify other branches so on.</p>
",Training and Model Evaluation,manipulate nested boolean query string python string boolean query like current hard modify query string want add another last remove entire desired output think easily unless come sophisticated regex different query trying write python function would take string boolean query input output tree data structure traverse tree evaluate change whatever portion query want change example tree easily see root traverse modify branch
Text Analytics/ Text prediction approach in python,"<p>This is my  customer dataset </p>

<pre><code>ID    Address                    Location    Age
1    room no5,
     rose apt,hill street,       Nagpur      38
     nagpur-""500249""      
2    block C-4,kirti park,       Thane       26
     Thane-""400356""
3    Dream villa, noor nagar,                46
     Colaba-""400008"" 
5    Sita Apt,Room no 101,       Kalyan      55
     Kalyan- ""421509""
7    Rose Apt, room no 20,       Mumbai      43
     Antop hill, 
     Mumbai-""400007""
8   Dirk Villa,nouis park,       Mumbai      50
    Dadar-""400079""
9   Raj apt,room no-2,                       33
    powai,""400076""
</code></pre>

<p>As in above case I have <strong>Address column</strong> values and corresponding values for <strong>Location column</strong>. Now given the above training data(data which will have all Address and Location values present) and test data( data which will consist only Address values for which I would be wanting to predict Location for it) I want to forecast which Address will fall under what loaction.
For this problem I want to know which approach in <strong>Python</strong> would suit the best,here are some reference which I had came across, but do not know which to follow.
here are some links<a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow noreferrer"">link_1</a></p>

<p><a href=""https://www.analyticsvidhya.com/blog/2015/10/understaing-support-vector-machine-example-code/"" rel=""nofollow noreferrer"">link_2</a>
Any suggestion will be much more appreciated.Thanks</p>
",Training and Model Evaluation,text analytics text prediction approach python customer dataset case address column value corresponding value location column given training data data address location value present test data data consist address value would wanting predict location want forecast address fall loaction problem want know approach python would suit best reference came across know follow linkslink link suggestion much appreciated thanks
SGDClassifier giving different accuracy each time for text classification,"<p>I'm using the SVM Classifier for classifying text as good text and gibberish. I'm using python's scikit-learn and doing it as follows: </p>

<pre><code>'''
Created on May 5, 2017
'''

import re
import random
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn import metrics

# Prepare data

def prepare_data(data):
    """"""
    data is expected to be a list of tuples of category and texts.
    Returns a tuple of a list of lables and a list of texts
    """"""
    random.shuffle(data)
    return zip(*data)

# Format training data

training_data = [
    (""good"", ""rain a lot the packs maybe damage.""),
    (""good"", ""15107 Lane Pflugerville, TX customer called me and his phone number and my phone numbers were not masked. thank you customer has had a stroke and items were missing from his delivery the cleaning supplies for his wet vacuum steam cleaner.  he needs a call back from customer support ""),
    (""gibber"", ""wh. screen""),
    (""gibber"", ""How will I know if I""),
    (""good"", ""I have problems scheduling blocks they are never any available.  Can I do full time?  Can I get scheduled more than one day a month?""),
    (""good"", ""Suggestion: easier way to sign in due alleviate the tediousness of periodically having to sign back in to the app to check for blocks.""),
    (""good"", ""I am so glad to hear from you. ""),
    (""good"", ""loading on today's itinerary takes ages!!!!!! time consuming when you have 150+ packages to deliver!!!!!""),
    (""good"", ""due to the new update that makes hours available at 10 pm. if you worked 8 hours that day you can't see next day hours due to 8 hour limit. please fix this""),
    (""good"", ""omg, PLEASE make it so we don't have to sign in every time we need to go into the app. At least make it good for a week. Thanks.""),
    (""good"", ""Constantly being logged out of app, if we could have a continuous login so we could receive notifications if blocks are available that would be ideal.""),
    (""good"", ""I am having problems  with the App. Every time I exit the App and reopen it asks for my login info.""),
    (""good"", ""15 minute service time due to 33rd floor and  20l lbs of cargo""),
    (""good"", ""I have been sceduled 1 block in 3 weeks. I check for new block availability multiple times a day and have not seen 1 available in three weeks. is there any way to get more blocks.""),
    (""good"", ""When will delivery jobs be available? Everytime I open this app, it says nothing is available. Have deliveries in Cincinnati started yet?""),
    (""good"", ""During delivery had to call customer support and after 10 minutes support person couldn't find my pick up location Kirkland /Bellevue and told me to hang up and call different support team.  Support person were unprofessional and rude, which is not acceptable.""),
    (""good"", ""can you please remove the pick up from my phone""),
    (""good"", ""Dear friends: I'm very very happy it's a big oportunitt""),
    (""good"", ""THANK YOU so much for the block you assigned me for next week.   If you have an additional 5 blocks please go ahead and assign them to me for next week.  My availability is updated and current.  You guys are awesome!!!""),
    (""good"", ""after update every time I open app I have too log in! I used to be able to stay logged in unless I logged out, can you return stay logged in option.""),
    (""good"", ""It looks like my app is not installed properly on my android phone, Note 5. I cannot access or do not see the tab to swipe to start delivering and the map or help button that should be visible for me to work today 5/6 at rpm""),
    (""gibber"", ""AF0000""),
    (""good"", ""awesome app, awesome hiring process, awesome delivery warehouse , awesome team and help in the field! lets deliver I would like more more more delivers , looking forward to the future ! I just bought a new delivery vehical !""),
    (""good"", ""I will like to ask why I can't get more delivery's only one in two weeks""),
    (""good"", ""device too slow software crashing all day""),
    (""good"", ""it doesn't work sometimes.""),
    (""good"", ""can you please remove the old sprouts pick up from my phone""),
    (""good"", ""They ability to zoom in on text screens would be very helpful. Am example would be customer notes when viewing in certain lighting conditions can be difficult.""),
    (""good"", ""I missed out on a delivery day when I clicked check in and waited for my turn to get an order only to find out that not only did my check in not register but the gps showed me down the street. I encountered this issue again when one of the warehouse employees placed an order for that location and the app wanted me to drive in a big circle to get back to where I was standing.""),
    (""good"", ""i am a little concerned that i didn't receive any blocks of time for this coming week, even though i had a perfect delivery score from this past pay period. Did the Cincinnati market over hire drivers where there are many people being shut completely out of any delivery blocks for an entire week? i really enjoy this type of work and the app makes it quite convenient.""),
    (""good"", ""I've arrived at the pick up restaurant but the staff did not have the barecode for me to scan, however I pick up the package and deliver but my is still not let me move on""),
    (""good"", ""might want to check my assigned hours for next week.  5am to 1pm??""),
    (""good"", ""hi team--just want to give some positive feedback.  I have had nothing but positive feedback from customers. Great support when calling help line. Thank you for this opportunity and if there is ever a situation where you need drivers immediately I will drop what I'm doing and help. You guys are the best.""),
    (""good"", ""Allow days or blocks throughout the day to be modified after General availability is set up for time off like doctors appointments.""),
    (""gibber"", ""AL001234""),
    (""good"", ""Please, enlight me.""),
    (""good"", ""it only shows my schedule starting in two weeks. when will we be able to start work""),
    (""good"", ""include more packages for one block, if the packages can be fitted into the car, so driver don't have to come back and pickup every two hours. 25% of the time is wasted coming back for pick up.""),
    (""gibber"", ""BBB h""),
    (""gibber"", ""AG0003006033SDgCJ12344""),
    (""gibber"", ""How will I know if I""),
    (""good"", ""please bring back some sort of hours cap! or possibly stagger the hour drops from 1200 to 1203 so that people with slower internet/slower phone arent at a disadvantage!""),
    (""good"", ""when the hours released tonight all of the people who didn't have 40 hours could see them.    however the drivers that are capped at 40 were unable to see them due to a flawed system.  please fix the system so that we are not continually treated unfairly like all of the drivers that whined so much and got us in to this mess.  the cap system is unfair to people that want to work and it caused problems with a lack of drivers  to deliver today at the hub.  obviously this is not a good system and benefits no one.""),
    (""good"", ""You have seriously messed up the whole scheduling process. Why can't I get any blocks at 10 even if I wait exactly until 10? Midnight was much better. So now that scheduling is a huge random pain in the ass, why would people want to keep doing this? I haven't been able to schedule work for three days now, it's quite frustrating when I don't get a chance to sign up, even when I'm diligent with timing.""),
    (""good"", ""Seriously, that's all I'm going to get is one lousy day? Tell me again why you need drivers if all we get is one day. I'm not sure this is gonna work out for me. I waited forever to get my background check back and this is what I get? smh""),
    (""good"", ""doesn't save updated access codes""),
    (""good"", ""the scheduling of my route is nor done very accurately. it keeps me driving back and forth""),
    (""good"", ""can't understand how to pick up a block. my availability is wide open. when you guys send the alerts about blocks available I open it real quick and there is nothing there. I do it in a matter of seconds""),
    (""good"", ""My availability keeps disappearing from my calender.   I set my availability for three weeks in advance. The gray dots are visible  but disappear on Wednesday or Thursday.  This makes it impossible for me to see and choose available blocks for the upcoming week. How can I get it fix.   Mike""),
    (""good"", ""GPS blank screen""),
    (""gibber"", ""sea swq""),
    (""gibber"", ""hiw o""),
    (""gibber"", ""Dr a""),
    (""gibber"", ""quick to quick to u uhu wu just us""),
    (""gibber"", ""Awa what's""),
    (""gibber"", ""wxdfcs""),
    (""gibber"", ""7k9opu""),
    (""gibber"", ""o.m.day day""),
    (""gibber"", ""GGT part his h""),
    (""gibber"", ""aawfhg""),
    (""gibber"", ""seesaw 2s""),
    (""gibber"", ""wawaa""),
    (""gibber"", ""of ll""),
    (""gibber"", ""rewards""),
    (""gibber"", ""mmqqm5my""),
    (""gibber"", "".in w""),
    (""gibber"", ""play r""),
    (""gibber"", ""was wwnw www www n""),
    (""gibber"", ""wqq2fwqq2fz22""),
    (""gibber"", ""not""),
    (""gibber"", ""I by yu I""),
    (""gibber"", ""Hi just wanted to let you know that it's bee""),
    (""gibber"", ""I erroneously v""),
    (""gibber"", ""I find it""),
    (""gibber"", ""bqyyx I a""),
    (""gibber"", ""are are""),
    (""gibber"", ""wawi waarnnnkwn""),
    (""gibber"", ""t Petey ueteu he""),
    (""gibber"", ""ews ri""),
    (""gibber"", ""bd xd""),
    (""gibber"", ""hatpa""),
    (""gibber"", ""se wests tasgt""),
    (""gibber"", ""wa vgcx azc Jo of""),
    (""gibber"", ""2w222""),
    (""gibber"", ""her u t b""),
    (""gibber"", ""ddddedc""),
    (""gibber"", ""just juju in hiking""),
    (""gibber"", ""wew2ww2wwwew2i2wkkk""),
    (""gibber"", ""meleeee""),
    (""gibber"", ""Aaq wqXD""),


]
training_labels, training_texts = prepare_data(training_data)


# Format test data

test_data = [

(""gibber"", ""an quality""),
    (""good"", ""Can't check in.   Time was 4:06.  I didn't drive out here for no reason.""),
    (""good"", ""can you do view all full address including postal code how it's in old app that helps do correctly delivery and not waist customer time""),
    (""good"", ""i am available again starting at 10am to 10pm. thanks""),
    (""gibber"", ""Hello, I encountered""),
    (""good"", ""I want to know how we are notified if there is a block I have been signed in and haven't been given a block yet""),
    (""gibber"", ""aawaaw""),
    (""gibber"", ""eeeeeeeeene""),
    (""good"", ""I am not getting enough shifts""),
    (""gibber"", ""hey e75k""),
    (""good"", ""my screen had went black or inverted""),
    (""good"", ""maps packed up again in sr20ls""),
    (""good"", ""how to clear my itinerary from old pickup address ?""),
    (""good"", ""keep signing me out.""),
    (""good"", ""For alcohol delivery,  where does customer sign?""),
    (""gibber"", ""t Petey ueteu he""),
    (""good"", ""can't get blocks.  too many drivers ??""),
    (""good"", ""got a new phone how do i download to new phone"")



]
test_labels, test_texts = prepare_data(test_data)


# Create feature vectors

""""""
Convert a collection of text documents to a matrix of token counts.
See: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
""""""
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(training_texts)
y = training_labels


# Train the classifier


clf = SGDClassifier()
clf.fit(X, y)


# Test performance

X_test = vectorizer.transform(test_texts)
y_test = test_labels

# Generates a list of labels corresponding to the samples
test_predictions = clf.predict(X_test)

# Convert back to the usual format
annotated_test_data = list(zip(test_predictions, test_texts))
print(annotated_test_data)

# evaluate predictions
y_test = np.array(test_labels)
print(metrics.classification_report(y_test, test_predictions))
print(""Accuracy: %0.4f"" % metrics.accuracy_score(y_test, test_predictions))
</code></pre>

<p>But, I keep getting different accuracy each time I run it. Why is this happening?</p>

<p>UPDATE:
So I moved the training_data to a text file and I'm reading it in the above code like this:</p>

<pre><code>lines = [line.rstrip('\n') for line in open(""file.txt"")]
training_data=[]
for i in lines:
    result = i.rstrip(',')
    l = literal_eval(result)
    training_data.append(l)

training_labels, training_texts = prepare_data(training_data)
</code></pre>

<p>And I also changed this in my above code:</p>

<pre><code>clf = SGDClassifier(random_state=5000)
</code></pre>

<p>So, now the random_state is not None. But, I'm still getting different accuracies each time!!</p>
",Training and Model Evaluation,sgdclassifier giving different accuracy time text classification using svm classifier classifying text good text gibberish using python scikit learn follows keep getting different accuracy time run happening update moved training data text file reading code like also changed code random state none still getting different accuracy time
Finding the accuracy of an HMM model for POS-Tagger,"<p>I am implementing the <code>Viterbi Algorithm</code> for <code>POS-Tagger</code> using the <code>Brown-corpus</code> as my data set. Now an important aspect of this <code>NLP</code> task is finding the accuracy of the model. So I really need help as what to implement. It's easier using the <code>nltk toolkit</code> but since I am not using a toolkit, I am stuck on how to determine the accuracy of my model. Any help, code examples or referral links would be appreciated. Thanks</p>
",Training and Model Evaluation,finding accuracy hmm model po tagger implementing using data set important aspect task finding accuracy model really need help implement easier using since using toolkit stuck determine accuracy model help code example referral link would appreciated thanks
Tensorflow RNN: Perplexity per Epoch remains constant,"<p>I am training an RNN-based language-model using Tensorflow. The model is very similar to the PTB model example in the TF tutorials section. However, when I attempt to train the model on my own data, the perplexity of the model does not go down; it remains constant throughout multiple epochs. Could anyone let me know what I might be doing wrong.</p>

<p>I have a feeling that I am not handling the targets properly, but the gist of my code for the targets is:</p>

<pre><code>def batcher(batch_size,unroll_steps,data,pad):
    print(len(data))
    batches = len(data) / batch_size
    inp = []
    target = []
    for i in range(batches):
            #print(len(data[i*batch_size:(i+1)*batch_size]))
            x = data[i*batch_size:(i+1)*batch_size]
            y =  [ line[1:]+[pad] for line in x ]
            yield (x,y)
</code></pre>

<p>That is, I just shift the data by 1 and use that as the target for the next word in a sentence.</p>

<p>The training script and model (class) are seen below</p>

<p>Training script (excerpt):</p>

<pre><code>def train(session, model, folder,batch_size,unroll_steps,epoch):

    word_to_id, id_to_word, train, val = build_inputs(folder,unroll_steps)
    pad = word_to_id['&lt;pad&gt;']
    costs = 0
    iters = 0
    train_size = len(train)
    batch_size = model.batch_size
    batches = train_size / batch_size
    state = session.run(model._initial_state)
    print(""Running epoch %d"" % epoch)
    for i in range(batches):
            fetches = [model.cost, model._final_state, model.logits]
            feed_dict = {}
            x = train[i*batch_size:(i+1)*batch_size]
            y = [ line[1:] +[pad] for line in x ]
            feed_dict[model.input] = x
            feed_dict[model.targets] = y
            feed_dict[model._initial_state] = state
            #print(""Cell-state complete - Running"")
            cost, state, logits = session.run(fetches, feed_dict)
            #print(""Single Run complete"")
            costs += cost
            iters += model.unroll_steps
    print(""\tEpoch %d: Perplexity is %f"" % (epoch, np.exp(costs/iters)))

    return np.exp(costs/iters)
</code></pre>

<p>Model:</p>

<pre><code>import tensorflow as tf

class LM(object):

    def __init__(self, train, max_gradient, batch_size, unroll_steps, vocab, size, layers, learning_rate, init, prob):
            self.batch_size = batch_size
            self.max_gradient = max_gradient
            self.layers = layers
            self.learning_rate = learning_rate
            self.unroll_steps = unroll_steps
            self.init = init
            #with tf. name_scope(""Paramters""):

            with tf.device('/gpu:0'), tf.name_scope(""Input""):
                    self.input = tf.placeholder(tf.int64, shape=[batch_size, unroll_steps], name=""input"")
                    self.targets = tf.placeholder(tf.int64, shape=[batch_size, unroll_steps], name=""targets"")
                    #self.init = tf.placeholder(tf.float32, shape=[], name=""init"")

            with tf.device('/gpu:0'), tf.name_scope(""Embedding""):
                    embedding = tf.Variable(tf.random_uniform([vocab, size], -self.init, self.init), dtype=tf.float32, name=""embedding"")
                    embedded_input = tf.nn.embedding_lookup(embedding, self.input, name=""embedded_input"")

            with tf.device('/gpu:0'), tf.name_scope(""RNN""), tf.variable_scope(tf.get_variable_scope(), reuse = False) as scope:
                    lstm_cell = tf.contrib.rnn.BasicLSTMCell(size, forget_bias=0.0, state_is_tuple=True)
                    if train and prob &lt; 1.0:
                            lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=prob)
                    cell = tf.contrib.rnn.MultiRNNCell([lstm_cell for _ in range(layers)], state_is_tuple=True)

                    self._initial_state = cell.zero_state(batch_size, tf.float32)
                    outputs = []
                    state = self._initial_state
                    for step in range(unroll_steps):
                            if step &gt; 0: tf.get_variable_scope().reuse_variables()
                            (cell_output, state) = cell(embedded_input[:, step, :], state)
                            outputs.append(cell_output)

            with tf.device('/gpu:0'), tf.name_scope(""Cost""), tf.variable_scope(tf.get_variable_scope(), reuse = False) as scope:
                    output = tf.reshape(tf.concat(outputs,1), [-1,size])
                    softmax_w = tf.get_variable(""softmax_w"", [size, vocab], dtype=tf.float32)
                    softmax_b = tf.get_variable(""softmax_b"", [vocab], dtype=tf.float32)
                    logits = tf.matmul(output, softmax_w) + softmax_b
                    losses = []
                    for logit, target in zip([logits], [tf.reshape(self.targets,[-1])]):
                            target = tf.reshape(target, [-1])
                            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit,labels=target)
                            losses.append(loss)
                    self.cost = tf.reduce_sum(losses) / batch_size
                    self._final_state = state
                    self.logits = logits
                    scope.reuse_variables()

            if not train:
                    return

            with tf.device('/gpu:0'), tf.name_scope(""Train""), tf.variable_scope(tf.get_variable_scope(), reuse=False):
                    train_variables = tf.trainable_variables()
                    gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, train_variables),self.max_gradient)
                    optimizer = tf.train.AdamOptimizer(self.learning_rate)
                    self.training = optimizer.apply_gradients(zip(gradients, train_variables))
                    tf.get_variable_scope().reuse_variables()
</code></pre>
",Training and Model Evaluation,tensorflow rnn perplexity per epoch remains constant training rnn based language model using tensorflow model similar ptb model example tf tutorial section however attempt train model data perplexity model doe go remains constant throughout multiple epoch could anyone let know might wrong feeling handling target properly gist code target shift data use target next word sentence training script model class seen training script excerpt model
What&#39;s slowing down this piece of python code?,"<p>I have been trying to implement the Stupid Backoff language model (the description is available <a href=""http://www.aclweb.org/anthology/D07-1090.pdf"" rel=""nofollow noreferrer"">here</a>, though I believe the details are not relevant to the question).</p>

<p>The thing is, the code's working and producing the result that is expected, but works slower than I expected. I figured out the part that was slowing down everything is here (and NOT in the training part):</p>

<pre><code>def compute_score(self, sentence):
    length = len(sentence)
    assert length &lt;= self.n
    if length == 1:
        word = tuple(sentence)
        return float(self.ngrams[length][word]) / self.total_words
    else:
        words = tuple(sentence[::-1])
        count = self.ngrams[length][words]
        if count == 0:
            return self.alpha * self.compute_score(sentence[1:])
        else:
            return float(count) / self.ngrams[length - 1][words[:-1]]

def score(self, sentence):
"""""" Takes a list of strings as argument and returns the log-probability of the 
    sentence using your language model. Use whatever data you computed in train() here.
""""""
    output = 0.0
    length = len(sentence)
    for idx in range(length):
        if idx &lt; self.n - 1:
            current_score = self.compute_score(sentence[:idx+1])
        else:
            current_score = self.compute_score(sentence[idx-self.n+1:idx+1])
        output += math.log(current_score)
    return output
</code></pre>

<p>self.ngrams is a nested dictionary that has n entries. Each of these entries is a dictionary of form (word_i, word_i-1, word_i-2.... word_i-n) : the count of this combination.</p>

<p>self.alpha is a constant that defines the penalty for going n-1.</p>

<p>self.n is the maximum length of that tuple that the program is looking for in the dictionary self.ngrams. It is set to 3 (though setting it to 2 or even 1 doesn't anything). It's weird because the Unigram and Bigram models work just fine in fractions of a second.</p>

<p>The answer that I am looking for is not a refactored version of my own code, but rather a tip which part of it is the most computationally expensive (so that I could figure out myself how to rewrite it and get the most educational profit from solving this problem).</p>

<p>Please, be patient, I am but a beginner (two months into the world of programming). Thanks.</p>

<p>UPD:
I timed the running time with the same data using time.time(): </p>

<p>Unigram = 1.9</p>

<p><a href=""https://gist.github.com/anonymous/3abbdd1b20102e1bf233ccbfe3912d01"" rel=""nofollow noreferrer"">Bigram</a> = 3.2</p>

<p>Stupid Backoff (n=2) = 15.3</p>

<p>Stupid Backoff (n=3) = 21.6</p>

<p>(It's on some bigger data than originally because of time.time's bad precision.)</p>
",Training and Model Evaluation,slowing piece python code trying implement stupid backoff language model description available though believe detail relevant question thing code working producing result expected work slower expected figured part wa slowing everything training part self ngrams nested dictionary ha n entry entry dictionary form word word word word n count combination self alpha constant defines penalty going n self n maximum length tuple program looking dictionary self ngrams set though setting even anything weird unigram bigram model work fine fraction second answer looking refactored version code rather tip part computationally expensive could figure rewrite get educational profit solving problem please patient beginner two month world programming thanks upd timed running time data using time time unigram bigram stupid backoff n stupid backoff n bigger data originally time time bad precision
How to evaluate the transfer function f(h) for a set of two synsets,"<p>How to evaluate the transfer function f(h) for a set of two synsets(Synonym sets) which is used to evaluate the sentence similarity based on semantic nets and corpus.
Here what exactly does 'h' stands for?
<a href=""http://ants.iis.sinica.edu.tw/3BkMJ9lTeWXTSrrvNoKNFDxRm3zFwRR/55/Sentence%20Similarity%20Based%20on%20Semantic%20Nets%20and%20corpus%20statistics.pdf"" rel=""nofollow noreferrer"">http://ants.iis.sinica.edu.tw/3BkMJ9lTeWXTSrrvNoKNFDxRm3zFwRR/55/Sentence%20Similarity%20Based%20on%20Semantic%20Nets%20and%20corpus%20statistics.pdf</a></p>
",Training and Model Evaluation,evaluate transfer function f h set two synset evaluate transfer function f h set two synset synonym set used evaluate sentence similarity based semantic net corpus exactly doe h stand
train LUIS to understand the difference between &quot;I am interested&quot; and &quot;I am not interested&quot;,"<p>I have a scenario where I have to train LUIS models to understand the difference between ""I am interested"" and ""I am not interested"". How to achieve it without directly mentioning the utterances in Intents.</p>
",Training and Model Evaluation,train luis understand difference interested interested scenario train luis model understand difference interested interested achieve without directly mentioning utterance intent
Restore models in tensorflow1.0 from many steps,"<p>I'd like to train my model with many epoches using Tensorflow v1.0. And my idea is to save every model in every epoch. But soon i found the current model would replace the last one.(i mean the last one would vanish.) So i want to know how to get all of the models and restore them one by one. I think it's hard and haven't got a nice solution. Thanks for every suggestion!</p>
",Training and Model Evaluation,restore model tensorflow many step like train model many epoch using tensorflow v idea save every model every epoch soon found current model would replace last one mean last one would vanish want know get model restore one one think hard got nice solution thanks every suggestion
"When use BaumWelchLearner in Java, is there a limitation of the sequences&#39; length (These sequences are used to train the hmm model)","<p>Thank you in advance!</p>

<p>I used BaumWelchLeaner() in java like this:</p>

<p><code>BaumWelchLearner bwl = new BaumWelchLearner();
 hmm = bwl.learn(hmm, seqs);</code></p>

<p>where ""seqs"" are my preprocessed training data.</p>

<p>However, if the sequence in ""seqs"" has a length more than 107, even though the statistics in ""pi"",""statesTostates"" and ""statesToObser"" matrix are OK, all the test data will be predicted to have a hidden sequence like this:</p>

<p>""0 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0"". </p>

<p>But if I control all the sequences' length to be less than 107, the result of prediction will be OK. 
According to jahmm.learn.BaumWelchLearner, there only say the length should >= 2. So I don't know how to solve my problem.</p>
",Training and Model Evaluation,use baumwelchlearner java limitation sequence length sequence used train hmm model thank advance used baumwelchleaner java like seqs preprocessed training data however sequence seqs ha length even though statistic pi statestostates statestoobser matrix ok test data predicted hidden sequence like control sequence length le result prediction ok according jahmm learn baumwelchlearner say length know solve problem
Startegies to deal with new terms in test data set,"<p>I'm using word2vec model to build a classifier on training data set and wonder what are technics to deal with unseen terms (words) in the test data.</p>

<p>Removing new terms doesn't seem like a best approach.
My current thought is to recalculate word2vec on combined data set (training + test) and replace new terms with nearest word from training data set (or maybe some linear combination of 2-3 nearest). Sounds a bit tricky, but should be doable.</p>

<p>Have you come across similar problem? Any idea/suggestion how to deal with unseen terms?</p>
",Training and Model Evaluation,startegies deal new term test data set using word vec model build classifier training data set wonder technics deal unseen term word test data removing new term seem like best approach current thought recalculate word vec combined data set training test replace new term nearest word training data set maybe linear combination nearest sound bit tricky doable come across similar problem idea suggestion deal unseen term
word2vec: weighted How can I give negative training data?,"<p>Im reusing word2vec for products on my website and users. I would like to say that a user is NEGATIVELY associated to a product if he has visited the page &lt; 5 seconds and POSITIVELY if he spent > 30 seconds on the page. Is there a way to specify this in word2vec? Or is there some other tool that enables this?</p>
",Training and Model Evaluation,word vec weighted give negative training data im reusing word vec product website user would like say user negatively associated product ha visited page second positively spent second page way specify word vec tool enables
How to train Word2Vec model on Wikipedia page using gensim?,"<p>After reading <a href=""https://codesachin.wordpress.com/2015/10/09/generating-a-word2vec-model-from-a-block-of-text-using-gensim-python/"" rel=""nofollow noreferrer"">this article</a>, I start to train my own model. The problem is that the author does not make it clear what the <code>sentences</code> in  <code>Word2Vec</code> should be like. </p>

<p>I download the text from a Wikipedia page, as it is written is the article, and I make a list of sentences from it:</p>

<pre><code>sentences = [word for word in wikipage.content.split('.')]
</code></pre>

<p>So, for example, <code>sentences[0]</code> looks like:</p>

<pre><code>'Machine learning is the subfield of computer science that gives computers the ability to learn without being explicitly programmed'
</code></pre>

<p>Then I try to train a model with this list:</p>

<pre><code>model = Word2Vec(sentences, min_count=2, size=50, window=10,  workers=4)
</code></pre>

<p>But the dictionary of the model consists of letters! For example, the output of <code>model.wv.vocab.keys()</code> is:</p>

<pre><code>dict_keys([',', 'q', 'D', 'B', 'p', 't', 'o', '(', ')', '0', 'V', ':', 'j', 's', 'R', '{', 'g', '-', 'y', 'c', '9', 'I', '}', '1', 'M', ';', '`', '\n', 'i', 'r', 'a', 'm', '–', 'v', 'N', 'h', '/', 'P', 'F', '8', '""', '’', 'W', 'T', 'u', 'U', '?', ' ', 'n', '2', '=', 'w', 'C', 'O', '6', '&amp;', 'd', '4', 'S', 'J', 'E', 'b', 'L', '$', 'l', 'e', 'H', '≈', 'f', 'A', ""'"", 'x', '\\', 'K', 'G', '3', '%', 'k', 'z'])
</code></pre>

<p>What am I doing wrong? Thanks in advance!</p>
",Training and Model Evaluation,train word vec model wikipedia page using gensim reading article start train model problem author doe make clear like download text wikipedia page written article make list sentence example look like try train model list dictionary model consists letter example output wrong thanks advance
openNLP categorize content return always first category,"<p>I'm testing with openNLP library to implemented automation in categorizing content but i have trouble. I'm using this code and it returns always the first category that i have in my training data which i'm passing full article from any news site.</p>

<pre><code>    public void trainModel() {
        try {
            InputStreamFactory inputStreamFactory = new MarkableFileInputStreamFactory( new File(""C:\\Users\\emehm\\Desktop\\data\\training_data.txt"") );
            ObjectStream&lt;String&gt; lineStream = new PlainTextByLineStream(inputStreamFactory, ""UTF-8"");
            ObjectStream&lt;DocumentSample&gt; sampleStream = new DocumentSampleStream(lineStream);

            DoccatModel model = DocumentCategorizerME.train(""en"", sampleStream, TrainingParameters.defaultParams(), new DoccatFactory());
            DocumentCategorizerME myCategorizer = new DocumentCategorizerME(model);
            double[] outcomes = myCategorizer.categorize(  new String[]{ this.getFileContent() });
            String category = myCategorizer.getBestCategory(outcomes);
            Map&lt;String, Double&gt; map = myCategorizer.scoreMap(new String[]{ this.getFileContent() });
            System.out.println(category);
        } catch (IOException e) {
            // Failed to read or parse training data, training failed
            e.printStackTrace();
        }
    }

    public String getFileContent() throws IOException {
        InputStream is = new FileInputStream(""C:\\Users\\emehm\\Desktop\\data\\statija.txt"");
        BufferedReader buf = new BufferedReader(new InputStreamReader(is));
        String line = buf.readLine();
        StringBuilder sb = new StringBuilder();
        while (line != null) {
            sb.append(line).append(""\n"");
            line = buf.readLine();
        }
        buf.close();
        return sb.toString();
    }
</code></pre>

<p>Training data: <a href=""http://pastebin.com/ZhxswkvJ"" rel=""nofollow noreferrer"">http://pastebin.com/ZhxswkvJ</a></p>

<p>Article i'm using: <a href=""http://pastebin.com/xtABGcbh"" rel=""nofollow noreferrer"">http://pastebin.com/xtABGcbh</a></p>

<p>it always returns the the first category from the list and i want to know what i'm missing? when i debug it it returns 0.25 score for all of them and picks first of them for some reason. when i test one word it works i guess but it's not working with an article.</p>
",Training and Model Evaluation,opennlp categorize content return always first category testing opennlp library implemented automation categorizing content trouble using code return always first category training data passing full article news site training data article using always return first category list want know missing debug return score pick first reason test one word work guess working article
stanford corenlp sentiment training set,"<p>I am new to the area of NLP and sentiment analysis in particular. My goal is to train the Stanford CoreNLP sentiment model. I am aware that the sentences provided as training data should be in the following format.</p>

<pre><code>(3 (2 (2 The) (2 Rock)) (4 (3 (2 is) (4 (2 destined) (2 (2 (2 (2 (2 to) (2 (2 be) (2 (2 the) (2 (2 21st) (2 (2 (2 Century) (2 's)) (2 (3 new) (2 (2 ``) (2 Conan)))))))) (2 '')) (2 and)) (3 (2 that) (3 (2 he) (3 (2 's) (3 (2 going) (3 (2 to) (4 (3 (2 make) (3 (3 (2 a) (3 splash)) (2 (2 even) (3 greater)))) (2 (2 than) (2 (2 (2 (2 (1 (2 Arnold) (2 Schwarzenegger)) (2 ,)) (2 (2 Jean-Claud) (2 (2 Van) (2 Damme)))) (2 or)) (2 (2 Steven) (2 Segal))))))))))))) (2 .)))
</code></pre>

<p>I am also aware that I can create the sentiment training model with my own training data using the following command.</p>

<pre><code>java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath     dev.txt -train -model model.ser.gz
</code></pre>

<p>My question is, do I have access to the training data set that was used to train the model? If yes, then where can I find it? 
Also, is there a way I can append new sentences to the original training data set and create the train model?</p>
",Training and Model Evaluation,stanford corenlp sentiment training set new area nlp sentiment analysis particular goal train stanford corenlp sentiment model aware sentence provided training data following format also aware create sentiment training model training data using following command question access training data set wa used train model yes find also way append new sentence original training data set create train model
OpenNLP Custom POS Tagger : How to make Dictionary override input tags,"<p>I am using OpenNLP for creating my own POS Tagger as follows</p>

<pre><code>public Trainer(String trainingData, String modelSavePath, String dictionary){

        try {
            dataIn = new MarkableFileInputStreamFactory(
                    new File(classLoader.getResource(trainingData).getFile()));

            lineStream = new PlainTextByLineStream(dataIn, ""UTF-8"");
            ObjectStream&lt;POSSample&gt; sampleStream = new WordTagSampleStream(lineStream);

            POSTaggerFactory fac=new POSTaggerFactory();

        if(dictionary!=null &amp;&amp; dictionary.length()&gt;0)
        {
            fac.setDictionary(new Dictionary(new FileInputStream(classLoader.getResource(dictionary).getFile())));
        }
            model = POSTaggerME.train(""en"", sampleStream, TrainingParameters.defaultParams(), fac);

        } catch (IOException e) {
            // Failed to read or parse training data, training failed
            e.printStackTrace();
        } finally {
            if (lineStream != null) {
                try {
                    lineStream.close();
                } catch (IOException e) {
                    // Not an issue, training already finished.
                    // The exception should be logged and investigated
                    // if part of a production system.
                    e.printStackTrace();
                }
            }
        }

OutputStream modelOut = null;
        try {
            modelOut = new BufferedOutputStream(new FileOutputStream(modelSavePath));
            //modelOut = new BufferedOutputStream(new FileOutputStream(new File(getClass().getResource(modelSavePath).toURI())));
            model.serialize(modelOut);
        } catch (IOException e) {
            // Failed to save model
            e.printStackTrace();
        } finally {
            if (modelOut != null) {
                try {
                    modelOut.close();
                } catch (IOException e) {
                    // Failed to correctly save model.
                    // Written model might be invalid.
                    e.printStackTrace();
                }
            }

        }
    }
</code></pre>

<p>which works well and saves the newly created model as a bin file. I want the dictionary terms to overwrite the words in my input and i do not see this behavior. </p>

<p>So consider the input</p>

<pre><code>Mary_NNP had_VBD a_DT little_JJ lamb_NN
</code></pre>

<p>Now i want the tag to be </p>

<pre><code>lamb_LAMB
</code></pre>

<p>so  put this in the dictionary</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;dictionary&gt;
&lt;entry tags=""LAMB""&gt;
&lt;token&gt;lamb&lt;/token&gt;
&lt;/entry&gt;
&lt;/dictionary&gt;
</code></pre>

<p>But when i try out the newly trained tagger, i still see the tag as <code>NN</code> for <code>lamb</code></p>

<p>However, if my training data is</p>

<pre><code>Mary_NNP had_VBD a_DT little_JJ lamb_LAMB
</code></pre>

<p>then it works as expected. Also, if i do not have the word <code>lamb</code> in my training data at all, then the custom generated tagger uses the dictionary tag.</p>

<p>How can i make sure that the dictionary tag always overrides the training data tag? Do i have to modify the training in any way? </p>
",Training and Model Evaluation,opennlp custom po tagger make dictionary override input tag using opennlp creating po tagger follows work well save newly created model bin file want dictionary term overwrite word input see behavior consider input want tag put dictionary try newly trained tagger still see tag however training data work expected also word training data custom generated tagger us dictionary tag make sure dictionary tag always override training data tag modify training way
Stanford NER lowercase entities,"<p>I am facing problem to detect named entities which starts with lowercase letter. If I train the model with only lowercase words, then the accuracy is reasonable; however, when the model is trained with fully uppercase tokens or even mix of lowercase and uppercase, the result is very bad. I tried some features which presented by the Stanford NLP Group <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ie/NERFeatureFactory.html"" rel=""noreferrer"">Class NERFeatureFactory</a> as well as variety of sentences, but I could not get the results that I expected.
An example for the problem I am facing is as follow:</p>

<p>""ali studied at university of michigan and now he works for us navy.""</p>

<p>I expected the model to recognize entities as follow:</p>

<ul>
<li>""university"" : ""FACILITY"",</li>
<li>""of michigan"" : ""FACILITY"",</li>
<li>""ali"" : ""PERSON""</li>
<li>""us"" : ""ORGANIZATION""</li>
<li>""navy"" : ""ORGANIZATION""</li>
</ul>

<p>If the .TSV file, which used as training data, contains ONLY lowercase letters, then I can get the above result otherwise the result is surprising.</p>

<p>Any help is highly appreciated a head. </p>
",Training and Model Evaluation,stanford ner lowercase entity facing problem detect named entity start lowercase letter train model lowercase word accuracy reasonable however model trained fully uppercase token even mix lowercase uppercase result bad tried feature presented stanford nlp group class nerfeaturefactory well variety sentence could get result expected example problem facing follow ali studied university michigan work u navy expected model recognize entity follow university facility michigan facility ali person u organization navy organization tsv file used training data contains lowercase letter get result otherwise result surprising help highly appreciated head
Splitting raw text on sentence level,"<p>What would be the best way to split a text without punctuation in Java on sentence level? </p>

<p>The text may contain multiple sentences without punctuation, e.g.:</p>

<pre><code>String text = ""i ate cornflakes it is a sunny day i have to wash my car"";
String[] sentences = splitOnSentenceLevel(text);
System.out.print(Arrays.toString(sentences));
&gt;&gt;&gt;[""i ate cornflakes"", ""it is a sunny day"", ""i have to wash my car""]
</code></pre>

<p>The only solution I could find is to train an n-gram model that tells the probability of each position being the end of a sentence, trained on punctuated text data. But setting that up seems like a huge task.</p>

<pre><code>public String[] splitOnSentenceLevel(String text) {
    List&lt;String&gt; sentences = new ArrayList&lt;String&gt;();
    String currentSentence = """";
    for(String word: text.split("" "")) {
        currentSentence += "" "" + word;
        if(nGramClassifierIsLastWordOfSentence(word)) {
            sentences.add(currentSentence);
            currentSentence = """";
        }
    }
    String[] sentencesArray = new String[ sentences.size() ];
    sentences.toArray( sentencesArray );
    return sentencesArray;
}
</code></pre>

<p>The Stanford CoreNLP toolkit doesn't seem to have such a feature either. The task is obviously ambiguous, but is there a simpler way of at least approximating a solution? The text I would like to analyze would contain relatively simple, short sentences.</p>
",Training and Model Evaluation,splitting raw text sentence level would best way split text without punctuation java sentence level text may contain multiple sentence without punctuation e g solution could find train n gram model tell probability position end sentence trained punctuated text data setting seems like huge task stanford corenlp toolkit seem feature either task obviously ambiguous simpler way least approximating solution text would like analyze would contain relatively simple short sentence
Training Stanford POS tagger using multiple text files,"<p>I have a corpus of about 20000 text files and i want to train the tagger using these text files, which is better,to group these text files into one text file(i don't know if it will affect tagging accuracy or not) or to include all these text files in the props file?</p>
",Training and Model Evaluation,training stanford po tagger using multiple text file corpus text file want train tagger using text file better group text file one text file know affect tagging accuracy include text file prop file
Stanford Relation Extractor custom model selects only one token of relation entities,"<p>I've successfully trained a Relation Extractor model and created a .ser file.</p>

<p>However, I'm running into an issue where the model successfully finds a relation but if one of its entities consists of multiple tokens, <strong>only one token is selected</strong>.
For example, for a relation called <em>Friend_of</em>, and a sentence like:</p>

<blockquote>
  <p>Sam Tarly's best friend is Jon Snow.</p>
</blockquote>

<p>The model will find a relation of type Friend_of between the following entities:</p>

<ul>
<li>Tarly</li>
<li>Jon</li>
</ul>

<p>This causes my tests to mark this as a <strong>false positive</strong> and the model as a whole to get a bad score.</p>

<p>I've tried training a custom NER model using the same training data, and then using this custom NER model to train the RelationExtractor model with the following properties in my props file:</p>

<pre><code>trainUsePipelineNER=true
ner.model=path/to/custom-ner-model.ser.gz
</code></pre>

<p>But that didn't solve the problem.</p>

<p>Is this just a problem of not enough training data or is there something I'm missing here?</p>

<p>Here is the Java code I use to get the relations:</p>

<pre><code>Properties props = new Properties();
props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, depparse, relation"");
props.put(""sup.relation.model"", ""lib/custom-relation-model-pipeline.ser"");
props.put(""pos.ptb3Escaping"", ""false"");

StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

List&lt;Relation&gt; foundRelations = new ArrayList&lt;&gt;();

for (String doc : documents) {
    Annotation document = new Annotation(doc);
    pipeline.annotate(document);
    List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);

    for (CoreMap sentence : sentences) {

        List&lt;RelationMention&gt; relationMentions = sentence.get(MachineReadingAnnotations.RelationMentionsAnnotation.class);

        for (RelationMention relation : relationMentions) {
            foundRelations.add(new Relation(relation.getArg(0).getValue(), relation.getType(), relation.getArg(1).getValue()));
        }

    }
}
</code></pre>

<p>Thank you!</p>

<p>Simon.</p>
",Training and Model Evaluation,stanford relation extractor custom model selects one token relation entity successfully trained relation extractor model created ser file however running issue model successfully find relation one entity consists multiple token one token selected example relation called friend sentence like sam tarly best friend jon snow model find relation type friend following entity tarly jon cause test mark false positive model whole get bad score tried training custom ner model using training data using custom ner model train relationextractor model following property prop file solve problem problem enough training data something missing java code use get relation thank simon
Issues Regarding Training Maltparser Model,"<p>I am trying to train a Maltparser Model for Bangla. I have annotated a small Corpus in Conllu Format. But it it gives me null pointer error. So i tried it with some treebank collected from UD website. And it works on those dataset. My questions are</p>

<ol>
<li><p>Can i train Maltparser Model without XPOSTAG, i have annotated the UPOSTAG field and XPOSTAG field is just copies of UPOSTAG. Do i need to annotate XPOSTAG? This is the only difference between my treebank and UD treebank</p></li>
<li><p>As it is for evaluation purpose can i automatically convert UPOSTAG to XPOSTAG?</p></li>
</ol>

<p>ref: <a href=""http://universaldependencies.org/format.html"" rel=""nofollow noreferrer"">http://universaldependencies.org/format.html</a></p>

<p>For better understanding i am giving example of both my bank and UD bank</p>

<p>My Example Bank(There are mistakes and some empty fields)(Language is Bangla)</p>

<pre><code>1   Ajake   _   NOUN    NOUN    _   5   iobj    _   _
2   rAtera  _   NOUN    NOUN    _   1   nmod    _   _
3   AbahAoYA    _   NOUN    NOUN    _   5   nsubj   _   _
4   kemana  _   ADV ADV _   5   advmod  _   _
5   hate    _   VERB    VERB    _   0   root    _   _
6   pAre    _   AUX AUX _   5   aux _   SpaceAfter=No
7   ?   _   _   _   _   _   _   _   _

1   Ajake   _   NOUN    NOUN    _   5   iobj    _   _
2   bikAlera    _   NOUN    NOUN    _   1   nmod    _   _
3   paribesha   _   NOUN    NOUN    _   5   nsubj   _   _
4   kemana  _   ADV ADV _   5   advmod  _   _
5   hate    _   VERB    VERB    _   0   root    _   _
6   pAre    _   AUX AUX _   5   aux _   SpaceAfter=No
7   ?   _   _   _   _   _   _   _   _
</code></pre>

<p>UD Bank</p>

<pre><code>1   From    _   ADP IN  _   3   case    _   _
2   the _   DET DT  _   3   det _   _
3   AP  _   PROPN   NNP _   4   nmod    _   _
4   comes   _   VERB    VBZ _   0   root    _   _
5   this    _   DET DT  _   6   det _   _
6   story   _   NOUN    NN  _   4   nsubj   _   _
7   :   _   PUNCT   :   _   4   punct   _   _

1   President   _   PROPN   NNP _   2   compound    _   _
2   Bush    _   PROPN   NNP _   5   nsubj   _   _
3   on  _   ADP IN  _   4   case    _   _
4   Tuesday _   PROPN   NNP _   5   nmod    _   _
5   nominated   _   VERB    VBD _   0   root    _   _
6   two _   NUM CD  _   7   nummod  _   _
7   individuals _   NOUN    NNS _   5   dobj    _   _
8   to  _   PART    TO  _   9   mark    _   _
9   replace _   VERB    VB  _   5   advcl   _   _
10  retiring    _   VERB    VBG _   11  amod    _   _
11  jurists _   NOUN    NNS _   9   dobj    _   _
12  on  _   ADP IN  _   14  case    _   _
13  federal _   ADJ JJ  _   14  amod    _   _
14  courts  _   NOUN    NNS _   11  nmod    _   _
15  in  _   ADP IN  _   18  case    _   _
16  the _   DET DT  _   18  det _   _
17  Washington  _   PROPN   NNP _   18  compound    _   _
18  area    _   NOUN    NN  _   14  nmod    _   _
19  .   _   PUNCT   .   _   5   punct   _   _
</code></pre>
",Training and Model Evaluation,issue regarding training maltparser model trying train maltparser model bangla annotated small corpus conllu format give null pointer error tried treebank collected ud website work dataset question train maltparser model without xpostag annotated upostag field xpostag field copy upostag need annotate xpostag difference treebank ud treebank evaluation purpose automatically convert upostag xpostag ref better understanding giving example bank ud bank example bank mistake empty field language bangla ud bank
opennlp sample training data for disease,"<p>I'm using OpenNLP for data classification. I could not find TokenNameFinderModel for disease <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow noreferrer"">here</a>. I know I can create my own model but I was wondering is there any large sample training data available for disease? </p>
",Training and Model Evaluation,opennlp sample training data disease using opennlp data classification could find tokennamefindermodel disease know create model wa wondering large sample training data available disease
NLP &amp; ML Text Extraction,"<p>I have some user chat data and categorised in various categories, the problem is there are a lot of algorithm generated categories, please see example below:</p>

<pre><code>Message | Category
I want to play cricket | Play cricket
I wish to watch cricket | Watch cricket
I want to play cricket outside | Play cricket outside 
</code></pre>

<p>As you can see Categories (essentially phrases) are extracted from the text itself,
based on my data there are 10,000 messages with approx 4,500 unique catgories.
Is there any suitable algorithm which can give me good prediction accuracy in such cases. </p>
",Training and Model Evaluation,nlp ml text extraction user chat data categorised various category problem lot algorithm generated category please see example see category essentially phrase extracted text based data message approx unique catgories suitable algorithm give good prediction accuracy case
Compute unweighted bag-of-words based TCM using text2vec in R?,"<p>I am trying to compute a term-term co-occurrence matrix (or TCM) from a corpus using the <code>text2vec</code> package in <code>R</code> (since it has a nice parallel backend). I followed <a href=""http://text2vec.org/glove.html"" rel=""nofollow"">this tutorial</a>, but while inspecting some toy examples, I noticed the <code>create_tcm</code> function does some sort of scaling or weighting on the term-term co-occurrence values. I know it uses skip-grams internally, but the documentation does not mention how it scales them - clearly, more distant terms/unigrams are weighted lower.</p>

<p>Here is an example:</p>

<pre><code>tcmtest = function(sentences){
  tokens &lt;- space_tokenizer(sentences)
  it = itoken(tokens, progressbar = FALSE)
  vocab &lt;- create_vocabulary(it, ngram = c(ngram_min = 1L, ngram_max = 1L))
  vectorizer &lt;- vocab_vectorizer(vocab,  grow_dtm = FALSE, skip_grams_window = 5L)
  return(create_tcm(it, vectorizer))
}

&gt; tcmtest(c(""a b"", ""a b c""))
3 x 3 sparse Matrix of class ""dgTMatrix""
  b c   a
b . 1 2.0
c . . 0.5
a . . .  
&gt; tcmtest(c(""a b"", ""c a b""))
3 x 3 sparse Matrix of class ""dgTMatrix""
  b   c a
b . 0.5 2
c . .   1
a . .   .
&gt; tcmtest(c(""a b"", ""c a a a b""))
3 x 3 sparse Matrix of class ""dgTMatrix""
  b    c        a
b . 0.25 2.833333
c . .    1.833333
a . .    .  
</code></pre>

<p><strong>Question</strong>: is there any way to disable this behaviour, so that every term/unigram in the skip-gram window is treated equally? I.e., if a term occurs inside the context window of another term twice in a corpus, it should say ""2"" in the TCM matrix. </p>

<p>Bonus question: how does the default scaling thing work anyway? If you add more ""a""s to the last example, then the b-c value seems to linearly decrease, while the b-a value actually increases - although more occurrences or ""a"" appear further away from ""b"".</p>
",Training and Model Evaluation,compute unweighted bag word based tcm using text vec r trying compute term term co occurrence matrix tcm corpus using package since ha nice parallel backend followed tutorial inspecting toy example noticed function doe sort scaling weighting term term co occurrence value know us skip gram internally documentation doe mention scale clearly distant term unigrams weighted lower example question way disable behaviour every term unigram skip gram window treated equally e term occurs inside context window another term twice corpus say tcm matrix bonus question doe default scaling thing work anyway add last example b c value seems linearly decrease b value actually increase although occurrence appear away b
Training my chatbot,"<p>I am currently working on ChatterBot in Python. And while doing that I got stuck in how to train my model. Since I have a long list of faq's to feed in my bot, so can anyone help me giving a right code on how I can train my model with writing each question and answer manually.</p>
",Training and Model Evaluation,training chatbot currently working chatterbot python got stuck train model since long list faq feed bot anyone help giving right code train model writing question answer manually
Classifying news articles by most probable subjects,"<p>Say I have an API that returns summaries of news articles from over 70 news publications. If I wanted to parse each summary and deduce the core subject matter, what sort of algorithms should I be researching?</p>
<p>Here is an example of what I'm attempting to accomplish:</p>
<blockquote>
<p><strong>Loss of sea ice accelerates warming, threatens animals and peoples
living in the Arctic and raises global security concerns.</strong></p>
<p>Polar sea ice melts each summer and reforms each winter—a freeze-thaw
cycle that in the Arctic has been dramatically altered by global
warming. Not only is summer sea ice shrinking rapidly in the Arctic,
but so is the average thickness of sea ice. Where in the past, some
Arctic sea ice grew to 10 feet (3 meters) thick over multiple years,
now much of the ice has only one year of growth, making it much more
susceptible to melting in the summer. Scientists project that the
Arctic Ocean may be ice-free in summer in just a few decades.</p>
</blockquote>
<pre><code>Core subjects: [&quot;sea ice melt&quot;, &quot;arctic&quot;, &quot;global warming&quot;] 
</code></pre>
",Training and Model Evaluation,classifying news article probable subject say api return summary news article news publication wanted parse summary deduce core subject matter sort algorithm researching example attempting accomplish loss sea ice accelerates warming threatens animal people living arctic raise global security concern polar sea ice melt summer reform winter freeze thaw cycle arctic ha dramatically altered global warming summer sea ice shrinking rapidly arctic average thickness sea ice past arctic sea ice foot meter thick multiple year much ice ha one year growth making much susceptible melting summer scientist project arctic ocean may ice free summer decade
Downsampling text documents,"<p>I have two classes with 1000 documents in class one and 40000 documents in class two. The documents consist of texts.
I want to use these texts in a neural network. But of course there is a unbalanced-dataset-problem. Each classifier would through all documents into class two and say ""I have a great accuracy of 97.5%. </p>

<p>Do you know if there are any implementations that check the document-similarities in class two and kind of clusters them and then just drops documents of each cluster proportional to its cluster size?</p>

<p>Or do you know similar approaches with the same target?</p>
",Training and Model Evaluation,downsampling text document two class document class one document class two document consist text want use text neural network course unbalanced dataset problem classifier would document class two say great accuracy know implementation check document similarity class two kind cluster drop document cluster proportional cluster size know similar approach target
class index differ error in weka,"<p>I want to do text classification with weka. I have a train and a test file (Persian language). first I load the train file and then choose ""string to word vector"" in preprocess. And because of choosing that, the class position goes to the start. For moving the class to its index (which is 2 in the files), I can go either to ""Edit"" part and right click on the class column and choose ""attribute as class"" or just in classify menu, choose (NOM)class. (unless most of the algorithms would be inactive). I run SMO and save the model. The problem is, after opening the test file, and click on ""re-evaluate the model on current test set"", this error occurs that, ""...class index differ: 1!=2"". I know it is because after opening the test file, again the class column goes to the start. For train part I solved the problem as I described above. But how can I solve it for the test part, too?</p>

<p>sample train file:
<a href=""https://i.sstatic.net/oo8ZC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oo8ZC.png"" alt=""enter image description here""></a></p>

<p>sample test file:
<a href=""https://i.sstatic.net/7MU7D.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7MU7D.png"" alt=""enter image description here""></a></p>
",Training and Model Evaluation,class index differ error weka want text classification weka train test file persian language first load train file choose string word vector preprocess choosing class position go start moving class index file go either edit part right click class column choose attribute class classify menu choose nom class unless algorithm would inactive run smo save model problem opening test file click evaluate model current test set error occurs class index differ know opening test file class column go start train part solved problem described solve test part sample train file sample test file
RCV1 dataset classification with Keras,"<p>I'm having issues training a text classifier using LSTM on the RCV1-v2 dataset,and after a thorough search I still don't know how to input the data to my model.</p>

<p>I'm using the fetch_rcv1() function from sklearn (<a href=""http://scikit-learn.org/stable/datasets/rcv1.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/datasets/rcv1.html</a>), which states the following:</p>

<p>data: The feature matrix is a scipy CSR sparse matrix, with 804414 samples and 47236 features.</p>

<p>target: The target values are stored in a scipy CSR sparse matrix, with 804414 samples and 103 categories.</p>

<p>sample_id: Each sample can be identified by its ID, ranging (with gaps) from 2286 to 810596:</p>

<p>Note that a document can belong to multiple categories,thus making the problem multilabel.According to the above ,train shape is (23149,47236) =>23149 documents with 47236 TF-IDF  vectors</p>

<p>Unfortunately there's no clear explanation or any tutorial whatsoever(even after days of searching!!) on how to manipulate the data. I'm trying to insert the data to a LSTM layer to no success.The model keeps failing to compile,example given:</p>

<pre><code>Train on 23149 samples, validate on 781265 samples
Traceback (most recent call last):
File ""/home/laxius/PycharmProjects/rcv1/rcv1.py"", line 31, in &lt;module&gt;
validation_data=(X_test, y_test))
File ""/usr/local/lib/python3.5/dist-packages/keras/models.py"", line 672, in fit
initial_epoch=initial_epoch)
File ""/usr/local/lib/python3.5/dist-packages/keras/engine/training.py"", line 1192, in fit
initial_epoch=initial_epoch)
File ""/usr/local/lib/python3.5/dist-packages/keras/engine/training.py"", line 892, in _fit_loop
outs = f(ins_batch)
File ""/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py"", line 1900, in __call__
feed_dict=feed_dict)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 766, in run
run_metadata_ptr)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 937, in _run
np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)
File ""/usr/local/lib/python3.5/dist-packages/numpy/core/numeric.py"", line 531, in asarray
return array(a, dtype, copy=False, order=order)
ValueError: setting an array element with a sequence.
</code></pre>

<p>Can anyone guide me on how I should reshape the data/layers to get the model to compile?Here's my code attempt so far:</p>

<pre><code>from sklearn.datasets import fetch_rcv1
from keras.models import Sequential
from keras.layers import Dense, Activation, Embedding
from keras.layers import LSTM


train = fetch_rcv1(subset='train')
test = fetch_rcv1(subset='test')

X_train = train.data
X_test = test.data


y_train = train.target
y_test = test.target

model = Sequential()
model.add(Embedding(input_dim=(None,47236),output_dim=300,dropout=0.25))
model.add(LSTM(350, dropout_W=0.4, dropout_U=0.4))  
model.add(Dense(103))
model.add(Activation('sigmoid'))


model.compile(loss='binary_crossentropy',
          optimizer='adam',
          metrics=['accuracy'])

print('Train...')
model.fit(X_train, y_train, batch_size=32, nb_epoch=15,
      validation_data=(X_test, y_test))
score, acc = model.evaluate(X_test, y_test,
                        batch_size=32)
print('Test score:', score)
print('Test accuracy:', acc)
</code></pre>
",Training and Model Evaluation,rcv dataset classification kera issue training text classifier using lstm rcv v dataset thorough search still know input data model using fetch rcv function sklearn state following data feature matrix scipy csr sparse matrix sample feature target target value stored scipy csr sparse matrix sample category sample id sample identified id ranging gap note document belong multiple category thus making problem multilabel according train shape document tf idf vector unfortunately clear explanation tutorial whatsoever even day searching manipulate data trying insert data lstm layer success model keep failing compile example given anyone guide reshape data layer get model compile code attempt far
How to extract words used for Doc2Vec,"<p>I am preparing a Doc2Vec model using tweets. Each tweet's word array is considered as a separate document and is labeled as ""SENT_1"", SENT_2"" etc.</p>

<pre>
taggeddocs = []
for index,i in enumerate(cleaned_tweets):
    if len(i) > 2: # Non empty tweets
        sentence = TaggedDocument(words=gensim.utils.to_unicode(i).split(), tags=[u'SENT_{:d}'.format(index)])
        taggeddocs.append(sentence)

# build the model
model = gensim.models.Doc2Vec(taggeddocs, dm=0, alpha=0.025, size=20, min_alpha=0.025, min_count=0)

for epoch in range(200):
    if epoch % 20 == 0:
        print('Now training epoch %s' % epoch)
    model.train(taggeddocs)
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model.alpha  # fix the learning rate, no decay
</pre>

<p>I wish to find tweets similar to a given tweet, say ""SENT_2"". How?</p>

<p>I get labels for similar tweets as:</p>

<pre>
sims = model.docvecs.most_similar('SENT_2')
for label, score in sims:
    print(label)
</pre>

<p>It prints as:</p>

<pre>
SENT_4372
SENT_1143
SENT_4024
SENT_4759
SENT_3497
SENT_5749
SENT_3189
SENT_1581
SENT_5127
SENT_3798
</pre>

<p>But given a label, how do I get original tweet words/sentence? E.g. what are the tweet words of, say, ""SENT_3497"". Can I query this to Doc2Vec model?</p>
",Training and Model Evaluation,extract word used doc vec preparing doc vec model using tweet tweet word array considered separate document labeled sent sent etc taggeddocs index enumerate cleaned tweet len non empty tweet sentence taggeddocument word gensim utils unicode split tag u sent format index taggeddocs append sentence build model model gensim model doc vec taggeddocs dm alpha size min alpha min count epoch range epoch print training epoch epoch model train taggeddocs model alpha decrease learning rate model min alpha model alpha fix learning rate decay wish find tweet similar given tweet say sent get label similar tweet sims model docvecs similar sent label score sims print label print sent sent sent sent sent sent sent sent sent sent given label get original tweet word sentence e g tweet word say sent query doc vec model
What string distance algorithm is best for measuring typing accuracy?,"<p>I'm trying to write a function that detects how accurate the user typed a particular phrase/sentence/word/words. My objective is to build an app to train the user's typing accuracy of certain phrases.</p>

<p>My initial instinct is to use the basic levenshtein distance algorithm (mostly because that's the only algo I knew off the top of my head).</p>

<p>But after a bit more research, I saw that <a href=""https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance"" rel=""nofollow noreferrer"">Jaro-Winkler</a> is a slightly more interesting algorithm because of its consideration for transpositions.</p>

<p>I even found a link that talks about the differences between these algorithms:</p>

<p><a href=""https://stackoverflow.com/questions/25540581/difference-between-jaro-winkler-and-levenshtein-distance"">Difference between Jaro-Winkler and Levenshtein distance?</a></p>

<p>Having read all that, in addition to the respective Wikipedia posts, I am still a little clueless as to which algorithm fits my objective the best.</p>
",Training and Model Evaluation,string distance algorithm best measuring typing accuracy trying write function detects accurate user typed particular phrase sentence word word objective build app train user typing accuracy certain phrase initial instinct use basic levenshtein distance algorithm mostly algo knew top head bit research saw jaro winkler slightly interesting algorithm consideration transposition even found link talk difference algorithm href jaro winkler levenshtein distance read addition respective wikipedia post still little clueless algorithm fit objective best
How to train a sequence CRF model with Mallet,"<p>I am new Mallet user, I have started with the last stable version 2.0.8. My task is coding a sequence tagger.</p>

<p>This is the code:</p>

<pre><code>ArrayList&lt;Pipe&gt; pipes = new ArrayList&lt;&gt;();

pipes.add(new SaveDataInSource());
pipes.add(new CharSequence2TokenSequence());
pipes.add(new TokenTextCharPrefix(""prefix1="", 1));
pipes.add(new TokenTextCharPrefix(""prefix2="", 2));  
pipes.add(new TokenTextCharSuffix(""suffix1="", 1));
pipes.add(new TokenTextCharSuffix(""suffix2="", 2));  
pipes.add(new TokenText(""word=""));  
pipes.add(new RegexMatches(""CAPITALIZED"", Pattern.compile(""^\\p{Lu}.*"")));
pipes.add(new RegexMatches(""STARTSNUMBER"", Pattern.compile(""^[0-9].*"")));
pipes.add(new RegexMatches(""HYPHENATED"", Pattern.compile("".*\\-.*"")));                
pipes.add(new TokenTextCharNGrams(""bigram="", new int[] {2}));                
pipes.add(new TokenTextCharNGrams(""trigram="", new int[] {3}));                
pipes.add(new MyTargetTagger()); 
pipes.add(new PrintTokenSequenceFeatures()); 
pipes.add(new TokenSequence2FeatureVectorSequence()); 

String[] str = new String[] {
    ""this is the first sentence John how are you"",
    ""this is the second sentence Maria how are you"",
    ""this is the third sentence Will how are you""
};                

Pipe pipe = new SerialPipes(pipes);

InstanceList trainingInstances = new InstanceList(pipe);
trainingInstances.addThruPipe(new ArrayIterator(str));            

CRF crf = new CRF(pipe, null);
crf.addStatesForThreeQuarterLabelsConnectedAsIn(trainingInstances);
crf.addStartState();

Instance r = crf.transduce(new Instance(""this is a sentence Bruno how are you ?"",null,null,null));                
System.out.println(r.getData().toString());
</code></pre>

<p>As you can see i have used a new Pipe (<code>MyTargetTagger</code>) that has this code:</p>

<pre><code>public Instance pipe (Instance carrier)
{            
    TokenSequence ts = (TokenSequence) carrier.getData();           
    LabelSequence labelSeq = new LabelSequence(getTargetAlphabet());

    for (int i = 0; i &lt; ts.size(); i++) {       
        if (ts.get(i).getText().equals(""John"")) {
            labelSeq.add(""PERSON"");
        } else if (ts.get(i).getText().equals(""Maria"")) {
            labelSeq.add(""PERSON"");
        } else if (ts.get(i).getText().equals(""Will"")) {
            labelSeq.add(""PERSON"");
        } else {
            labelSeq.add(""O"");
        }
    }

    System.out.print(labelSeq.toString());

    carrier.setTarget(labelSeq);            
}
</code></pre>

<p>It is stupid, i know, but it is only a test to understand how the target labels will be interpreted.
The labels of the three sentences are equals (obviously):</p>

<pre><code>0: O (0)
1: O (0)
2: O (0)
3: O (0)
4: O (0)
5: PERSON (1)
6: O (0)
7: O (0)
8: O (0)
</code></pre>

<p>As you can see i also added <code>pipes.add(new PrintTokenSequenceFeatures());</code> this is the output:</p>

<p><strong>First sentence:</strong>         </p>

<pre><code>name: array:0
O trigram=his trigram=thi bigram=is bigram=hi bigram=th word=this suffix2=is suffix1=s prefix2=th prefix1=t 
O bigram=is word=is suffix1=s prefix1=i 
O trigram=the bigram=he bigram=th word=the suffix2=he suffix1=e prefix2=th prefix1=t 
O trigram=rst trigram=irs trigram=fir bigram=st bigram=rs bigram=ir bigram=fi word=first suffix2=st suffix1=t prefix2=fi prefix1=f 
O trigram=nce trigram=enc trigram=ten trigram=nte trigram=ent trigram=sen bigram=ce bigram=nc bigram=en bigram=te bigram=nt bigram=en bigram=se word=sentence suffix2=ce suffix1=e prefix2=se prefix1=s 
PERSON trigram=ohn trigram=Joh bigram=hn bigram=oh bigram=Jo CAPITALIZED word=John suffix2=hn suffix1=n prefix2=Jo prefix1=J 
O trigram=how bigram=ow bigram=ho word=how suffix2=ow suffix1=w prefix2=ho prefix1=h 
O trigram=are bigram=re bigram=ar word=are suffix2=re suffix1=e prefix2=ar prefix1=a 
O trigram=you bigram=ou bigram=yo word=you suffix2=ou suffix1=u prefix2=yo prefix1=y 
</code></pre>

<p><strong>Second sentence:</strong></p>

<pre><code>name: array:1
O trigram=his trigram=thi bigram=is bigram=hi bigram=th word=this suffix2=is suffix1=s prefix2=th prefix1=t 
O bigram=is word=is suffix1=s prefix1=i 
O trigram=the bigram=he bigram=th word=the suffix2=he suffix1=e prefix2=th prefix1=t 
O trigram=ond trigram=con trigram=eco trigram=sec bigram=nd bigram=on bigram=co bigram=ec bigram=se word=second suffix2=nd suffix1=d prefix2=se prefix1=s 
O trigram=nce trigram=enc trigram=ten trigram=nte trigram=ent trigram=sen bigram=ce bigram=nc bigram=en bigram=te bigram=nt bigram=en bigram=se word=sentence suffix2=ce suffix1=e prefix2=se prefix1=s 
PERSON trigram=ria trigram=ari trigram=Mar bigram=ia bigram=ri bigram=ar bigram=Ma CAPITALIZED word=Maria suffix2=ia suffix1=a prefix2=Ma prefix1=M 
O trigram=how bigram=ow bigram=ho word=how suffix2=ow suffix1=w prefix2=ho prefix1=h 
O trigram=are bigram=re bigram=ar word=are suffix2=re suffix1=e prefix2=ar prefix1=a 
O trigram=you bigram=ou bigram=yo word=you suffix2=ou suffix1=u prefix2=yo prefix1=y
</code></pre>

<p><strong>Third sentence:</strong></p>

<pre><code>name: array:2
O trigram=his trigram=thi bigram=is bigram=hi bigram=th word=this suffix2=is suffix1=s prefix2=th prefix1=t 
O bigram=is word=is suffix1=s prefix1=i 
O trigram=the bigram=he bigram=th word=the suffix2=he suffix1=e prefix2=th prefix1=t 
O trigram=ird trigram=hir trigram=thi bigram=rd bigram=ir bigram=hi bigram=th word=third suffix2=rd suffix1=d prefix2=th prefix1=t 
O trigram=nce trigram=enc trigram=ten trigram=nte trigram=ent trigram=sen bigram=ce bigram=nc bigram=en bigram=te bigram=nt bigram=en bigram=se word=sentence suffix2=ce suffix1=e prefix2=se prefix1=s 
PERSON trigram=ill trigram=Wil bigram=ll bigram=il bigram=Wi CAPITALIZED word=Will suffix2=ll suffix1=l prefix2=Wi prefix1=W 
O trigram=how bigram=ow bigram=ho word=how suffix2=ow suffix1=w prefix2=ho prefix1=h 
O trigram=are bigram=re bigram=ar word=are suffix2=re suffix1=e prefix2=ar prefix1=a 
O trigram=you bigram=ou bigram=yo word=you suffix2=ou suffix1=u prefix2=yo prefix1=y
</code></pre>

<p>When i do:</p>

<pre><code>Instance r = crf.transduce(new Instance(""this is a sentence Bruno how are you"",null,null,null));                
System.out.println(r.getData().toString()); 
</code></pre>

<p>to see the performance of a new instance, the output is:</p>

<p><strong>PERSON O PERSON O PERSON O PERSON O</strong> </p>

<p>Why this output ??</p>

<p>I know that i need a lot of data to train my model better. Of couse, but I would like to know if there are problems with my code.</p>

<p>Thank you so much!      </p>
",Training and Model Evaluation,train sequence crf model mallet new mallet user started last stable version task coding sequence tagger code see used new pipe ha code stupid know test understand target label interpreted label three sentence equal obviously see also added output first sentence second sentence third sentence see performance new instance output person person person person output know need lot data train model better couse would like know problem code thank much
"CRF model trained on plural, not working on singular","<p>I have made a CRF model. My data set has 24 classes and at this time I am in beginning so my training data has just 1200 tokens/corpus. I have train the model. In my training data I have used the plural of tokens like addresses, photos, states, countries etc.</p>

<p>Now at the time of testing if I give plural of tokens in sentence form to this model then it work good but if I enter my sentence in singular like photo, state etc then it does not assign any tag to it.</p>

<p>This behavior of crf is looking very strange. I have explore the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ie/NERFeatureFactory.html"" rel=""nofollow noreferrer"">NER Feature Factory</a> and used some lemma features but it also did not work. Sharing my <code>austen.prop</code> for the model formation.</p>

<pre><code># location of the training file
trainFile = training_data_for_ner.txt
# location where you would like to save (serialize) your
# classifier; adding .gz at the end automatically gzips the file,
# making it smaller, and faster to load
serializeTo = ner-model.ser.gz

# structure of your training file; this tells the classifier that
# the word is in column 0 and the correct answer is in column 1
map = word=0,answer=1,pos=2,lemma=3

# This specifies the order of the CRF: order 1 means that features
# apply at most to a class pair of previous class and current class
# or current class and next class.
maxLeft=1

# these are the features we'd like to train with
# some are discussed below, the rest can be
# understood by looking at NERFeatureFactory
useClassFeature=true
useWord=true
# word character ngrams will be included up to length 6 as prefixes
# and suffixes only 
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useDisjunctive=true
useSequences=true
usePrevSequences=true
# the last 4 properties deal with word shape features
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
# newly added features.
useLemmas=true
usePrevNextLemmas=true
useLemmaAsWord=true
useTags=true
</code></pre>

<p>Last four features were added by reading that <code>NER Feature Factory</code>. If anyone can help me to solve this problem then I will be thankful to you.</p>
",Training and Model Evaluation,crf model trained plural working singular made crf model data set ha class time beginning training data ha token corpus train model training data used plural token like address photo state country etc time testing give plural token sentence form model work good enter sentence singular like photo state etc doe assign tag behavior crf looking strange explore ner feature factory used lemma feature also work sharing model formation last four feature added reading anyone help solve problem thankful
Why isn&#39;t my classifier predicting any positive classes?,"<p>I'm doing sentiment analysis on tweets containing the word ""Trump"". I manually labeled the first 200 tweets, here are the first 13 observations:</p>

<pre><code>Date    SentimentText   Sentiment
Mon Nov 28 23:24:12 +0000 2016  ""@HillaryClinton Go ahead with your hypocritical recount. It's fun to watch Trump squirm.""  0
Mon Nov 28 23:39:06 +0000 2016  @SenSchumer &amp;amp; @SenGillibrand - Demand Trump rescind Steve Bannon's appointment. @MoveOn 0
Mon Nov 28 23:30:34 +0000 2016  Democrats Demand Trump's Tax Returns And An Investigation Into His Conflicts Of Interest via @politicususa  0
Mon Nov 28 23:54:43 +0000 2016  ""Oh my god, how has this only been one day?"" -@SaraMurray on covering a day on the Trump Trail #girlsonthebus @gupolitics   0
Mon Nov 28 23:18:16 +0000 2016  People are mad at GiGi for impersonating Melania Trump, saying ""it's rude to bully and immigrant"" OH?! THE FUCKING IRONY    0
Mon Nov 28 23:50:10 +0000 2016  @dosdelimas @FoxNews mt @resnikoff For those who don't understand why Trump would lie about voter fraud ..  0
Mon Nov 28 23:29:29 +0000 2016  @tanveerali Yo! Do you mind if I steal your awesome electoral map (giving credit where credit is due)?  1
Mon Nov 28 23:19:39 +0000 2016  ""Historic,"" as in lower 1/3 of all EV results in American History   1
Mon Nov 28 23:41:40 +0000 2016  i thought this was gonna say trump before i opened it   0
Mon Nov 28 23:13:31 +0000 2016  Hold on wait, I voted for trump is the new racial slur now? im dead 1
Mon Nov 28 23:22:01 +0000 2016  O.K., well, if a mass of stuff was then taught, it was set up for. #SubhumanCheeto #NMP 0
Mon Nov 28 23:44:13 +0000 2016  Woman goes on racist, pro-Trump tirade in Michaels store over $1 bag  Trumpmerica ladies &amp;amp; gents.   0
</code></pre>

<p>I tried labeling the tweets based on whether the user supports Trump or posted something positive about him.
This is the code that I'm using thus far:</p>

<pre><code>import numpy as np
import pandas as pd
import csv
from sklearn import linear_model, naive_bayes
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.preprocessing import FunctionTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.base import TransformerMixin
from sklearn import cross_validation



logistic_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),
                     ('ft_vec', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),
                     ('tfidf', TfidfTransformer()),
                     ('ft_tfid', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),
                     ('clf', linear_model.LogisticRegression(penalty='l2',solver='lbfgs',max_iter=1000, multi_class='ovr',warm_start=True)),
                    ])

gnb_clf = Pipeline([('vect', CountVectorizer()),
                     ('ft_vec', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),
                     ('tfidf', TfidfTransformer()),
                     ('ft_tfid', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),
                     ('clf', naive_bayes.GaussianNB()),
                    ])


import csv
from pandas import *

df = read_excel('trump_labeled.xlsx')

#Collect the output in y variable

y = df['Sentiment']
X = df['SentimentText']
from sklearn.cross_validation import train_test_split
#cross validation
X_train, X_test,y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=42)
X_train = np.array(X_train)
y_train = np.array(y_train)
X_test = np.array(X_test)
y_test = np.array(y_test)



log_clf = logistic_clf.fit(X_train, y_train)
gnb_clf = gnb_clf.fit(X_train, y_train)

log_predicted = logistic_clf.predict(X_test) # predict labels for test data with logistic regression classifier
gnb_predicted = gnb_clf.predict(X_test) # predict labels for test data with naive bayes classifier

# PRINT SOME RESULTS FOR THE DATASETS PART
print(""\nDATASET RESULTS"")
print('\nLogistic Regression Results:\n\tNegative tweets: %.2f\n\tPositive tweets: %.2f' %(np.mean(log_predicted == 0), np.mean(log_predicted == 1)))
print('\tAccuracy: %.2f'% (np.mean(log_predicted == y_test)))
print('\tPositive Precision: %.2f' %(precision_score(y_test, log_predicted,pos_label=1)))
print('\tPositive Recall: %.2f' %(recall_score(y_test, log_predicted,pos_label=1)))
print('\tPositive F-measure: %.2f' %(f1_score(y_test, log_predicted,pos_label=1)))
print('\tNegative Precision: %.2f' %(precision_score(y_test, log_predicted,pos_label=0)))
print('\tNegative Recall: %.2f' %(recall_score(y_test, log_predicted,pos_label=0)))
print('\tNegative F-measure: %.2f' %(f1_score(y_test, log_predicted,pos_label=0)))
</code></pre>

<p>this generated the following results</p>

<pre><code>DATASET RESULTS

Logistic Regression Results:
        Negative tweets: 1.00
        Positive tweets: 0.00
        Accuracy: 0.72
        Positive Precision: 0.00
        Positive Recall: 0.00
        Positive F-measure: 0.00
        Negative Precision: 0.72
        Negative Recall: 1.00
        Negative F-measure: 0.84
C:\Users\My\Anaconda2\lib\sitepackages\sklearn\metrics\classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.'precision', 'predicted', average, warn_for)
C:\Users\My\Anaconda2\lib\sitepackages\sklearn\metrics\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.'precision', 'predicted', average, warn_for)
</code></pre>

<p>My classifiers(logistic regression and Naive Bayes) fail to accurately classifying the positive labels, where positive = 1, which makes my evaluation metrics ill defined. Of the 200 tweets, 43 were positive, however my classifier is classifying all 200 of them negative. How could I fix this? Note, I still have not preprocessed my data. So I still need to convert url's to the token url, account for whitespaces, etc. Is it because I haven't preprocessed my tweets yet? Or perhaps the way I manually labeled the tweets, some of them were difficult to decide whether they were positive or not. I tried search for prelabeled tweets on Trump and had no luck</p>

<p>Also, I noticed accounting for L2 regularization and the BFGS optimization method for my logistic regression does nothing to change my accuracy, is that normal?</p>
",Training and Model Evaluation,classifier predicting positive class sentiment analysis tweet containing word trump manually labeled first tweet first observation tried labeling tweet based whether user support trump posted something positive code using thus far generated following result classifier logistic regression naive bayes fail accurately classifying positive label positive make evaluation metric ill defined tweet positive however classifier classifying negative could fix note still preprocessed data still need convert url token url account whitespaces etc preprocessed tweet yet perhaps way manually labeled tweet difficult decide whether positive tried search prelabeled tweet trump luck also noticed accounting l regularization bfgs optimization method logistic regression doe nothing change accuracy normal
Word2Vec - adding constraint to vector representation,"<p>I am trying to adapt the pre-trained Google News word2vec model to my specific domain. For the domain I am looking at, certain words are known to be similar to each other so in an ideal world, the Word2Vec representation of those words should represent that. I understand that I can train the pre-trained model on a corpus of domain-specific data to update the vectors. </p>

<p>However, if I know for certain that certain words are highly similar and should be together, is there a way for me to incorporate that constraint into the word2vec model? Mathematically, I would like to add a term to the loss function of word2vec that provides a penalty if two that I know to be similar are not positioned close to each other in the vector space. Does anyone have advice on how to implement this? Will this require me to unpack the word2vec model or is there a way for me to potentially add that additional term to the loss function?</p>
",Training and Model Evaluation,word vec adding constraint vector representation trying adapt pre trained google news word vec model specific domain domain looking certain word known similar ideal world word vec representation word represent understand train pre trained model corpus domain specific data update vector however know certain certain word highly similar together way incorporate constraint word vec model mathematically would like add term loss function word vec provides penalty two know similar positioned close vector space doe anyone advice implement require unpack word vec model way potentially add additional term loss function
Calculating topic distribution of an unseen document on GenSim,"<p>I am trying to use LDA module of GenSim to do the following task</p>

<p>""Train a LDA model with one big document and keep track of 10 latent topics. Given a new, unseen document, predict probability distribution of 10 latent topics"".</p>

<p>As per tutorial here: <a href=""http://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">http://radimrehurek.com/gensim/tut2.html</a>, this seems possible for a document in a corpus, but I am wondering if it it would be possible for an unseen document. </p>

<p>Thank you!  </p>
",Training and Model Evaluation,calculating topic distribution unseen document gensim trying use lda module gensim following task train lda model one big document keep track latent topic given new unseen document predict probability distribution latent topic per tutorial seems possible document corpus wondering would possible unseen document thank
Can multiple ngrams be used in the same classifier?,"<p>I'm new to NLP and have a very simple question I expected to be asked a lot, but honestly couldn't find anywhere: can multiple types of ngrams be used within the same classifier (e.g. unigrams + bigrams)?</p>

<p>I have tried doing so, and, for Naive Bayes, at least, it gives me a higher accuracy than bigrams only (though lower than unigrams), but I am not sure whether it is a legitimate practice at all. One concern that I have is that there is potential for multicollinearity, if that's even an applicable term, i.e. both 'luck' and 'good luck' are near the top of the most informative features list.</p>
",Training and Model Evaluation,multiple ngrams used classifier new nlp simple question expected asked lot honestly find anywhere multiple type ngrams used within classifier e g unigrams bigram tried naive bayes least give higher accuracy bigram though lower unigrams sure whether legitimate practice one concern potential multicollinearity even applicable term e luck good luck near top informative feature list
LUIS limits the number of intents to 20,"<p>I am trying to build a bot which talks to a LUIS model. The bot would have 35 scenarios, each corresponding to a LUIS intent. Currently, LUIS supports having maximum 20 intents. 
How can I scale this in my code? I am wondering if it is better to have a LUIS model hierarchy, with the parent model calling on to the specific child model. Or should I maintain a list of keywords in my database and call a specific model based on it. I need help to evaluate the pros and cons of both the approaches. Thanks!</p>
",Training and Model Evaluation,luis limit number intent trying build bot talk luis model bot would scenario corresponding luis intent currently luis support maximum intent scale code wondering better luis model hierarchy parent model calling specific child model maintain list keywords database call specific model based need help evaluate pro con approach thanks
make a confusion matrix for a classifier with 2 classes,"<p>I have a file with some sentences (A Persian sentence, a tab, a Persian word (tag), a tab, an English word (tag)). The English words show the class of each sentence. There are 2 classes in this file, ""passion"" and ""salty"". I classified the sentences with naive bayes algorithm and now I have to calculate precision and recall. For that I have to make a confusion matrix but I don't know how. I wrote a small code and assumed that ""passion"" is the positive group and ""salty"" is the negative group. The code returned the output for this case. But if I assume ""salty"" as positive and ""passion"" as negative, the numbers are totally different from the first case, and consequently when I want to calculate precision and recall, I don't have the correct answer. Should I calculate tp, tn, fp and fn separately for the 2 classes (once for passion and once for salty) and then calculate the average and then calculate precision and recall according to this average?</p>

<p>(hint1: argmax is the output of the NB algorithm and it is the tag that the code recognized it for the test sentences.
hint2: I have some other files with more than 2 classes, too)</p>

<pre><code>#t = line.strip().split(""\t"")
if t[2] == ""passion"" and argmax == ""passion"":
    tp += 1
elif t[2] == ""passion"" and argmax != ""passion"":
    fn += 1
elif t[2] == ""salty"" and argmax != ""salty"":
    fp += 1
elif t[2] == ""salty"" and argmax == ""salty"":
    tn += 1
print (""tp"", tp, ""tn"", tn, ""fp"", fp, ""fn"", fn)
</code></pre>
",Training and Model Evaluation,make confusion matrix classifier class file sentence persian sentence tab persian word tag tab english word tag english word show class sentence class file passion salty classified sentence naive bayes algorithm calculate precision recall make confusion matrix know wrote small code assumed passion positive group salty negative group code returned output case assume salty positive passion negative number totally different first case consequently want calculate precision recall correct answer calculate tp tn fp fn separately class passion salty calculate average calculate precision recall according average hint argmax output nb algorithm tag code recognized test sentence hint file class
How to align two GloVe models in text2vec?,"<p>Let's say I have trained two separate GloVe vector space models (using <code>text2vec</code> in <code>R</code>) based on two different corpora. There could be different reasons for doing so: the two base corpora may come from two different time periods, or two very different genres, for example. I would be interested in comparing the usage/meaning of words between these two corpora. If I simply concatenated the two corpora and their vocabularies, that would not work (the location in the vector space for word pairs with different usages would just be somewhere in the ""middle""). </p>

<p>My initial idea was to train just one model, but when preparing the texts, append a suffix (_x, _y) to each word (where x and y stand for the usage of word A in corpus x/y), as well as keep a separate copy of each corpus without the suffixes, so that the vocabulary of the final concatenated training corpus would consist of: A, A_x, A_y, B, B_x, B_y ... etc, e.g.:</p>

<pre><code>this is an example of corpus X
this be corpus Y yo
this_x is_x an_x example_x of_x corpus_x X_x
this_y be_y corpus_y Y_y yo_y
</code></pre>

<p>I figured the ""mean"" usages of A and B would serve as sort of ""coordinates"" of the space, and I could measure the distance between A_x and A_y in the same space. But then I realized since A_x and A_y never occur in the same context (due to the suffixation of all words, including the ones around them), this would probably distort the space and not work. I also know there is something called an orthogonal procrustes problem, which relates to aligning matrices, but I wouldn't know how to implement it for my case.</p>

<p><strong>What would be a reasonable way to fit two GloVe models (preferably in <code>R</code> and so that they work with <code>text2vec</code>) into a common vector space, if my final goal is to measure the cosine similarity of word pairs, which are orthographically identical, but occur in two different corpora?</strong></p>
",Training and Model Evaluation,align two glove model text vec let say trained two separate glove vector space model using based two different corpus could different reason two base corpus may come two different time period two different genre example would interested comparing usage meaning word two corpus simply concatenated two corpus vocabulary would work location vector space word pair different usage would somewhere middle initial idea wa train one model preparing text append suffix x word x stand usage word corpus x well keep separate copy corpus without suffix vocabulary final concatenated training corpus would consist x b b x b etc e g figured mean usage b would serve sort coordinate space could measure distance x space realized since x never occur context due suffixation word including one around would probably distort space work also know something called procrustes problem relates aligning matrix know implement case would reasonable way fit two glove model preferably work common vector space final goal measure cosine similarity word pair orthographically identical occur two different corpus
What are ngram counts and how to implement using nltk?,"<p>I've read a paper that uses ngram counts as feature for a classifier, and I was wondering what this exactly means.</p>

<p>Example text: ""Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam""</p>

<p>I can create unigrams, bigrams, trigrams, etc. out of this text, where I have to define on which ""level"" to create these unigrams. The ""level"" can be character, syllable, word, ...</p>

<p>So creating unigrams out of the sentence above would simply create a list of all words? </p>

<p>Creating bigrams would result in word pairs bringing together words that follow each other? </p>

<p>So if the paper talks about ngram counts, it simply creates unigrams, bigrams, trigrams, etc. out of the text, and counts how often which ngram occurs? </p>

<p>Is there an existing method in python's nltk package? Or do I have to implement a version of my own?</p>
",Training and Model Evaluation,ngram count implement using nltk read paper us ngram count feature classifier wa wondering exactly mean example text lorem ipsum dolor sit amet consetetur sadipscing elitr sed diam create unigrams bigram trigram etc text define level create unigrams level character syllable word creating unigrams sentence would simply create list word creating bigram would result word pair bringing together word follow paper talk ngram count simply creates unigrams bigram trigram etc text count often ngram occurs existing method python nltk package implement version
what is fp and tn in word sense diasiambiguation in calculating precision and recall,"<p>I want to calculate precision and recall for word sense disambiguation. Naturally, for that I need to calculate tp, tn, fp and fn. I know tp is the number of documents that the tag of the test sentences equals to the tag of the classifier and fn is the number of documents that the tag of test sentences is not equal to the tags that the classifier found. But what is tn and fp? I can't see any other case more than the two cases I described above. For example my test sentence is as like as below:</p>

<p>fist sentence: word1 word2 word3 word4 tag1
second sentence: word1 word2 word3 word4 tag2</p>

<p>If my classifier finds the correct tag which is equal to the tag in sentences, it's tp unless it's fn. So what is tn and fp?</p>
",Training and Model Evaluation,fp tn word sense diasiambiguation calculating precision recall want calculate precision recall word sense disambiguation naturally need calculate tp tn fp fn know tp number document tag test sentence equal tag classifier fn number document tag test sentence equal tag classifier found tn fp see case two case described example test sentence like fist sentence word word word word tag second sentence word word word word tag classifier find correct tag equal tag sentence tp unless fn tn fp
php sentence boundaries detection,"<p>I would like to divide a text into sentences in PHP. I'm currently using a regex, which brings ~95% accuracy and would like to improve by using a better approach. I've seen NLP tools that do that in Perl, Java, and C but didn't see anything that fits PHP. Do you know of such a tool? </p>
",Training and Model Evaluation,php sentence boundary detection would like divide text sentence php currently using regex brings accuracy would like improve using better approach seen nlp tool perl java c see anything fit php know tool
Is it possible to re-train a word2vec model (e.g. GoogleNews-vectors-negative300.bin) from a corpus of sentences in python?,"<p>I am using pre-trained Google news dataset for getting word vectors by using Gensim library in python</p>

<pre><code>model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>After loading the model I am converting training reviews sentence words into vectors</p>

<pre><code>#reading all sentences from training file
with open('restaurantSentences', 'r') as infile:
x_train = infile.readlines()
#cleaning sentences
x_train = [review_to_wordlist(review,remove_stopwords=True) for review in x_train]
train_vecs = np.concatenate([buildWordVector(z, n_dim) for z in x_train])
</code></pre>

<p>During word2Vec process i get a lot of errors for the words in my corpus, that are not in the model. Problem is how can i retrain already pre-trained model (e.g GoogleNews-vectors-negative300.bin'), in order to get word vectors for those missing words.</p>

<p>Following is what I have tried:
Trained a new model from training sentences that I had</p>

<pre><code># Set values for various parameters
num_features = 300    # Word vector dimensionality                      
min_word_count = 10   # Minimum word count                        
num_workers = 4       # Number of threads to run in parallel
context = 10          # Context window    size                                                                                    
downsampling = 1e-3   # Downsample setting for frequent words

sentences = gensim.models.word2vec.LineSentence(""restaurantSentences"")
# Initialize and train the model (this will take some time)
print ""Training model...""
model = gensim.models.Word2Vec(sentences, workers=num_workers,size=num_features, min_count = min_word_count, 
                      window = context, sample = downsampling)


model.build_vocab(sentences)
model.train(sentences)
model.n_similarity([""food""], [""rice""])
</code></pre>

<p>It worked! but the problem is that I have a really small dataset and less resources to train a large model.</p>

<p>Second way that I am looking at is to extend the already trained model such as GoogleNews-vectors-negative300.bin.</p>

<pre><code>model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
sentences = gensim.models.word2vec.LineSentence(""restaurantSentences"")
model.train(sentences)
</code></pre>

<p>Is it possible and is that a good way to use, please help me out</p>
",Training and Model Evaluation,possible train word vec model e g googlenews vector negative bin corpus sentence python using pre trained google news dataset getting word vector using gensim library python loading model converting training review sentence word vector word vec process get lot error word corpus model problem retrain already pre trained model e g googlenews vector negative bin order get word vector missing word following tried trained new model training sentence worked problem really small dataset le resource train large model second way looking extend already trained model googlenews vector negative bin possible good way use please help
RNNLM using theano,"<p>I asked the same question on theano user list, but got no reply, just wondering if anyone can help me here.</p>

<p>I am trying to re-implement the RNNLM of <a href=""http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf"" rel=""nofollow"">http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf</a> based on this nice post.
I tried a toy test case which training data is the first 100 sentences of PTB training data (downloaded from <a href=""http://rnnlm.org/"" rel=""nofollow"">http://rnnlm.org/</a> ), the same data also used for evaluation.</p>

<p><strong>Baseline:</strong></p>

<p>I trained the LM with 25 iterations, using rnnlm-0.4b from <a href=""http://rnnlm.org/"" rel=""nofollow"">http://rnnlm.org/</a>, I got
test log probability: -4107.323481
PPL net: 85.496622</p>

<p>The command lines that to produce the baseline are:</p>

<pre><code>$ rnnlm -train ../ptb/ptb.train.txt.toy -valid ../ptb/ptb.train.txt.toy -rnnlm rnn.model.toy -hidden 10 -class 1 -max-iter 25 -independent -beta 0 -direct-order 0
$ rnnlm -rnnlm rnn.model.toy -test ../ptb/ptb.train.txt.toy -independent
</code></pre>

<p><strong>Using my implementation</strong>, after 25 iterations, there is a large difference in PPL:</p>

<blockquote>
  <p>epoch=24: log probability=-5374.255371 ppl=337.187731</p>
</blockquote>

<p>I am still learning Theano, did i miss something in my implementation?</p>

<p>Thanks</p>

<p>My implementation can be found at <a href=""https://11350770138305416713.googlegroups.com/attach/179c4942cbf157/rnnlm.py?part=0.1&amp;view=1&amp;vt=ANaJVrGZjtk4NfhfR2gY8Xk3x5yDxT5FUczkzSB6PRIGVu7lN_Dr9XHUZcYNE3WSrEc6SCL7vf8O9Xg5oCuzST4Z5oxo0WgCAIjBz7X0SC_4ZnvpsKf4HII"" rel=""nofollow"">here</a>:</p>

<pre><code>#! /usr/bin/env python

import itertools
import codecs
import numpy as np
import nltk
import sys
import time
from datetime import datetime
import theano as theano
import theano.tensor as T

class RNNLM:

    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):
        # Assign instance variables
        self.word_dim = word_dim
        self.hidden_dim = hidden_dim
        self.bptt_truncate = bptt_truncate
        # Randomly initialize the network parameters
        U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))
        V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))
        W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))
        # Theano: Created shared variables
        self.U = theano.shared(name='U', value=U.astype(theano.config.floatX))  # @UndefinedVariable
        self.V = theano.shared(name='V', value=V.astype(theano.config.floatX))  # @UndefinedVariable
        self.W = theano.shared(name='W', value=W.astype(theano.config.floatX))      # @UndefinedVariable
        # We store the Theano graph here
        self.theano = {}
        self.__theano_build__()

    def __theano_build__(self):
        U, V, W = self.U, self.V, self.W
        x = T.ivector('x')
        y = T.ivector('y')
        def forward_prop_step(x_t, s_t_prev, U, V, W):
            s_t = T.tanh(U[:,x_t] + W.dot(s_t_prev))
            o_t = V.dot(s_t)

            return [o_t, s_t]
        [o,s], updates = theano.scan(
            forward_prop_step,
            sequences=x,
            outputs_info=[None, dict(initial=T.zeros(self.hidden_dim))],
            non_sequences=[U, V, W],
            truncate_gradient=self.bptt_truncate,
            strict=True)
        p_x_given_h = T.nnet.softmax(o)

        o_error = T.sum(T.nnet.categorical_crossentropy(p_x_given_h, y))
        logp = T.sum(T.log10(p_x_given_h)[T.arange(y.shape[0]), y])


        # Gradients
        dU = T.grad(o_error, U)
        dV = T.grad(o_error, V)
        dW = T.grad(o_error, W)

        # Assign functions
        self.forward_propagation = theano.function([x], p_x_given_h)
        self.ce_error = theano.function([x, y], o_error)
        self.logp = theano.function([x, y], logp)
        # SGD
        learning_rate = T.scalar('learning_rate')
        self.sgd_step = theano.function([x,y,learning_rate], [], 
                      updates=[(self.U, self.U - learning_rate * dU),
                              (self.V, self.V - learning_rate * dV),
                              (self.W, self.W - learning_rate * dW)])

    def calculate_total_loss(self, X, Y):
        return np.sum([self.ce_error(x,y) for x,y in zip(X,Y)])

    def calculate_loss(self, X, Y):
        # Divide calculate_loss by the number of words
        num_words = np.sum([len(y) for y in Y])
        return self.calculate_total_loss(X,Y)/float(num_words)   

    def calculate_ppl(self, X, Y):
        num_words = np.sum([len(y) for y in Y])
        #print ""word count: "" + str(num_words)
        logp = np.sum([self.logp(x,y) for x,y in zip(X,Y)])
        ppl = 10 ** (-logp/num_words)
        return ppl, logp


def train_with_sgd(model, X_train, y_train, X_valid, y_valid, learning_rate=0.005, nepoch=1, evaluate_loss_after=5):
    # We keep track of the losses so we can plot them later
    losses = []
    num_examples_seen = 0
    for epoch in range(nepoch):
        # For each training example...
        for i in range(len(y_train)):
            model.sgd_step(X_train[i], y_train[i], learning_rate)
            num_examples_seen += 1

        loss = model.calculate_loss(X_train, y_train)
        losses.append((num_examples_seen, loss))
        time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')

        loss = model.calculate_loss(X_valid, y_valid)
        ppl, logp = model.calculate_ppl(X_valid, y_valid)

        print ""epoch=%d: log probability=%f ppl=%f"" % (epoch,logp,ppl)
        # Adjust the learning rate if loss increases
        if (len(losses) &gt; 1 and losses[-1][1] &gt; losses[-2][1]):
            learning_rate = learning_rate * 0.5  
            print ""Setting learning rate to %f"" % learning_rate

def load_data():
    print ""load data...""

    train = [(""%s %s %s"" % (sentence_end_token, x.strip(), sentence_end_token)).split() for x in codecs.open(""../ptb/ptb.train.txt.toy"", ""r"", ""UTF-8"")]

    print ""Parsed %d sentences."" % (len(train))

    # Count the word frequencies
    word_freq = nltk.FreqDist(itertools.chain(*train))
    print ""Found %d unique words tokens."" % len(word_freq.items())

    vocab = word_freq.most_common()
    index_to_word = [x[0] for x in vocab]
    word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])

    X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in train])
    y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in train])


    vocabulary_size = len(word_to_index)

    return  X_train, y_train, vocabulary_size + 1 



hidden_dim = 10
sentence_end_token = ""eos""
learning_rate = 0.1
nepoc=25
bptt_truncate=100
model_file=None

# Read the data 
X_train, y_train, vocabulary_size = load_data()
print ""vocabulary_size: "" + str(vocabulary_size)
model = RNNLM(vocabulary_size, hidden_dim=hidden_dim)
train_with_sgd(model, X_train, y_train, X_train, y_train, nepoch=nepoc, learning_rate=learning_rate)
</code></pre>
",Training and Model Evaluation,rnnlm using theano asked question theano user list got reply wondering anyone help trying implement rnnlm based nice post tried toy test case training data first sentence ptb training data downloaded data also used evaluation baseline trained lm iteration using rnnlm b got test log probability ppl net command line produce baseline using implementation iteration large difference ppl epoch log probability ppl still learning theano miss something implementation thanks implementation found
Classification training with only positive sentences,"<p>I'm starting a project to build an automated fact checking classificator nad I have some doubts about the process to follow.</p>

<p>I've a database of ~1000 sentences, each one being a fact check positive. In order to build a supervised machine learning model I'll need to have a big set of tagged sentences with the true/false result depending if it's a fact check candidate sentence or not. That would require a lot of time and effort, so I'd like to first get results (with less accuracy I guess) without doing that.</p>

<p>My idea is to use the already tagged positive sentences and apply a PoS tagger to them. This would give me interesting information to spot some patterns like the most common words(e.g: raised, increase, won) and the post tags (e.g. verbs in past/present tense, time and numerals).</p>

<p>With this results I'm thinking about assigning weights in order to analyze new unclassified sentences. The problem is that the weight assignment would be done by me in an ""heuristical"" way. It'd be best to use the results of the PoS tagger to train some model which assigns probabilities in a more sophisticated way.</p>

<p>Could you give me some pointers if there's a way to accomplish this?</p>

<p>I read about Maximum Entropy Classifiers and statistical parsers but I really don't know if they're the right choice.</p>

<p>Edit (I think it'd be better to give more details):</p>

<p>Parsing the sentences with a PoS tagger will give me some useful information about each one of them, allowing me to filter them and weighting them using some custom metrics.</p>

<p>For example:</p>

<p>There are one million more people in poverty than five years ago -> indicatives of a fact check candidate sentence:  verb in present tense, numerals and dates, (than) comparison.</p>

<p>We will increase the GDP by 3% the following year -> indicatives of a NOT fact check candidate sentence: it's in the future tense (indicative of some sort of prediction)</p>
",Training and Model Evaluation,classification training positive sentence starting project build automated fact checking classificator nad doubt process follow database sentence one fact check positive order build supervised machine learning model need big set tagged sentence true false result depending fact check candidate sentence would require lot time effort like first get result le accuracy guess without idea use already tagged positive sentence apply po tagger would give interesting information spot pattern like common word e g raised increase post tag e g verb past present tense time numeral result thinking assigning weight order analyze new unclassified sentence problem weight assignment would done heuristical way best use result po tagger train model assigns probability sophisticated way could give pointer way accomplish read maximum entropy classifier statistical parser really know right choice edit think better give detail parsing sentence po tagger give useful information one allowing filter weighting using custom metric example one million people poverty five year ago indicative fact check candidate sentence verb present tense numeral date comparison increase gdp following year indicative fact check candidate sentence future tense indicative sort prediction
Estimating nlp neural net training data set size,"<p>I'm working on a simple question / answer solution where answers are returned given a question. I don't have a training set of question, answer pairs. I plan to take a subset of the data , approx 10 sentences and manually create 100 question answer pairs. How can I estimate how many questions / answer pairs will be required in order to answer questions from this data ?</p>
",Training and Model Evaluation,estimating nlp neural net training data set size working simple question answer solution answer returned given question training set question answer pair plan take subset data approx sentence manually create question answer pair estimate many question answer pair required order answer question data
How to recreate same DocumentTermMatrix with new (test) data,"<p>Suppose I have text based training data and testing data. To be more specific, I have two data sets - training and testing - and both of them have one column which contains text and is of interest for the job at hand.</p>

<p>I used tm package in R to process the text column in the training data set. After removing the white spaces, punctuation, and stop words, I stemmed the corpus and finally created a document term matrix of 1 grams containing the frequency/count of the words in each document. I then took a pre-determined cut-off of, say, 50 and kept only those terms that have a count of greater than 50.  </p>

<p>Following this, I train a, say, GLMNET model using the DTM and the dependent variable (which was present in the training data). Everything runs smooth and easy till now.</p>

<p>However, how do I proceed when I want to score/predict the model on the testing data or any new data that might come in the future?</p>

<p>Specifically, what I am trying to find out is that how do I create the exact DTM on new data? </p>

<p>If the new data set does not have any of the similar words as the original training data then all the terms should have a count of zero (which is fine). But I want to be able to replicate the exact same DTM (in terms of structure) on any new corpus.</p>

<p>Any ideas/thoughts?</p>
",Training and Model Evaluation,recreate documenttermmatrix new test data suppose text based training data testing data specific two data set training testing one column contains text interest job hand used tm package r process text column training data set removing white space punctuation stop word stemmed corpus finally created document term matrix gram containing frequency count word document took pre determined cut say kept term count greater following train say glmnet model using dtm dependent variable wa present training data everything run smooth easy till however proceed want score predict model testing data new data might come future specifically trying find create exact dtm new data new data set doe similar word original training data term count zero fine want able replicate exact dtm term structure new corpus idea thought
Precision Recall calculations in POS tagging,"<p>I am trying to find the precision and recall in POS tag.</p>

<p><strong>Case-1</strong> 
Say the Golden data in this case is </p>

<pre><code>Secretariat[NNP] is[VBZ] expected[VBN] to[TO] race[VB] tomorrow[NR]
</code></pre>

<p>and the Test data is </p>

<pre><code>Secretariat[NNP] is[VBZ] expected[VBN] to[TO] race[NN] tomorrow[NR] 
</code></pre>

<p>Here, </p>

<pre><code>5 is TruePsotive, 
1 FalsePositive [NN], 
1 FalseNegative [VB]  
Precision = TP/(TP+FP)=5/6 
Recall = TP/(TP+FN)=5/6
</code></pre>

<p><strong>Case-2</strong>
In this case, we have total 700 tokens and some of the Tokens are tagged as ""UNK"" tag by the tagger, which is not present in the Golden data as well as in Training Data.</p>

<p>In the training data, some of the tag was not given, so tester take those unknown tag as ""UNK"" and then provided the output tagged file.</p>

<p>In the output we have observed that, </p>

<pre><code>500 tokens tagged by the tagger correctly, (True Positive)
200 tokens tagged incorrectly (False Positive)
100 tokens tagged as UNK (False negative)
</code></pre>

<p>So in this case, </p>

<pre><code>Precision = 500/700 = 0.714285714 
Recall = 500/600 = 0.833333333
</code></pre>

<p>Is the calculation correct for the both cases? Please do let me know.</p>
",Training and Model Evaluation,precision recall calculation po tagging trying find precision recall po tag case say golden data case test data case case total token token tagged unk tag tagger present golden data well training data training data tag wa given tester take unknown tag unk provided output tagged file output observed case calculation correct case please let know
Can Artificial Neural Networks Learn Language Models? Paper 2000 Implementation,"<p>I am new to research field in NLP. I want to implement a paper <a href=""http://repository.cmu.edu/cgi/viewcontent.cgi?article=2405&amp;context=compsci"" rel=""nofollow"">Can Artificial Neural Networks Learn Language Models?</a> In this paper first time a step was taken so that Neural Network can learn Language Model. I have understood the paper, everything is understandable just some confusions in last section of paper.</p>

<p><strong>I did not found any of its code.</strong> Paper is too old (2000)  <strong>I did not even find the Training data (Communicator Telephone Air Travel Information System) which was used at that time</strong>.</p>

<p>I have also emailed about this to both professors of the paper but email id of one of them is expired and waiting for response from other one.</p>

<p>Can anyone help me in this situation? Your guidelines would be valuable for new comers in research field. I would be thankful to you.</p>
",Training and Model Evaluation,artificial neural network learn language model paper implementation new research field nlp want implement paper artificial neural network learn language model paper first time step wa taken neural network learn language model understood paper everything understandable confusion last section paper found code paper old even find training data communicator telephone air travel information system wa used time also emailed professor paper email id one expired waiting response one anyone help situation guideline would valuable new comer research field would thankful
How do I train my kneser-ney model for next word prediction to estimate my discount parameters,"<p>I am working on a project to predict the next word in a text. I have used the <code>quanteda</code> package in R to generate tri-grams and bi-grams. I am aware that we need to maximize the probability of the sentences that are in the held out test set. However I am not sure how to go about it. Any help would be great :)</p>
",Training and Model Evaluation,train kneser ney model next word prediction estimate discount parameter working project predict next word text used package r generate tri gram bi gram aware need maximize probability sentence held test set however sure go help would great
Optimize Convolution Neural Network,"<p>I am doing small project using Convolution Neural Network. I code is base on <a href=""https://github.com/dennybritz/cnn-text-classification-tf"" rel=""nofollow noreferrer"">dennybritz</a>. Here my <a href=""https://github.com/ngoduyvu/Convolution-text-classification/blob/master/One_hot_encoding.ipynb"" rel=""nofollow noreferrer"">Convect</a> code and here is two function <a href=""https://github.com/ngoduyvu/Convolution-text-classification/blob/master/load_data.py"" rel=""nofollow noreferrer"">load data</a> and <a href=""https://github.com/ngoduyvu/Convolution-text-classification/blob/master/text_cnn.py"" rel=""nofollow noreferrer"">CNN architect</a>. I stuck in the evaluation process, my evaluation is always around 0.42 (The red star lines)<a href=""https://i.sstatic.net/G0Y3O.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/G0Y3O.png"" alt=""training results""></a>. </p>

<p>The result should be above 0.6. I tried change parameter of the Network or change the size of data but nothing change. Could anyone suggest what is wrong with my code and how optimize to get a better results.</p>
",Training and Model Evaluation,optimize convolution neural network small project using convolution neural network code base dennybritz convect code two function load data cnn architect stuck evaluation process evaluation always around red star line result tried change parameter network change size data nothing change could anyone suggest wrong code optimize get better result
How do I map words to functionalities using machine learning?,"<p>I'm doing a project which involves getting inputs from the user and mapping them to functionalities provided by a system. For example, the user might say ""alert"" which has to be mapped to the ""alarm"" functionality. Similarly, ""wake up"" should be mapped to ""detect motion"" or ""detect sound"" and so on. </p>

<p>How do I do this using machine learning? 
How can I train for this type of purpose without having to create a synthetic dataset, like training from existing literature?
How can I incorporate active learning into this?</p>
",Training and Model Evaluation,map word functionality using machine learning project involves getting input user mapping functionality provided system example user might say alert ha mapped alarm functionality similarly wake mapped detect motion detect sound using machine learning train type purpose without create synthetic dataset like training existing literature incorporate active learning
How do I use PropBank for prepositional phrases in my semantic role analyzer code?,"<p>I am trying to improve the accuracy of my semantic role analyser by introducing labeling for prepositional phrases. So far the code does this for verb phrases. How do I integrate prop bank into my code for this? I looked around a bit but couldn't find much help on the web. </p>

<p>My code outputs the symantic as well as the syntactic tree. E.g. for the clause -</p>

<blockquote>
  <p>Later he became one of the central spirits of the Army Language
  Program and the language school of Washington Foreign Service
  Institute</p>
</blockquote>

<p>The following is the output:</p>

<pre><code>(ROOT
  (S
    (ADVP (RB Later))
    (, ,)
    (NP (PRP he))
    (VP (VBD became)
      (NP
        (NP (CD one))
        (PP (IN of)
          (NP
            (NP (DT the) (JJ central) (NNS spirits))
            (PP (IN of)
              (NP
                (NP (DT the) (NNP Army) (NNP Language) (NNP Program))
                (CC and)
                (NP
                  (NP (DT the) (NN language) (NN school))
                  (PP (IN of)
                    (NP
                      (NP (NNP Washington) (POS 's))
                      (NNP Foreign) (NNP Service) (NNP Institute))))))))))
    (. .)))  



DEP Tree0:became[3-VBD]
     (advmod) 1:Later[0-RB]
     (nsubj) 1:he[2-PRP]
     (xcomp) 1:one[4-CD]
          (prep_of) 2:spirits[8-NNS]
               (det) 3:the[6-DT]
               (amod) 3:central[7-JJ]
               (prep_of) 3:Program[13-NNP]
                    (det) 4:the[10-DT]
                    (nn) 4:Army[11-NNP]
                    (nn) 4:Language[12-NNP]
                    (conj_and) 4:school[17-NN]
                         (det) 5:the[15-DT]
                         (nn) 5:language[16-NN]
                         (prep_of) 5:Institute[23-NNP]
                              (poss) 6:Washington[19-NNP]
                              (nn) 6:Foreign[21-NNP]
                              (nn) 6:Service[22-NNP]
</code></pre>
",Training and Model Evaluation,use propbank prepositional phrase semantic role analyzer code trying improve accuracy semantic role analyser introducing labeling prepositional phrase far code doe verb phrase integrate prop bank code looked around bit find much help web code output symantic well syntactic tree e g clause later became one central spirit army language program language school washington foreign service institute following output
Applying MACHINE learning in biological text data,"<p>I am trying to solve the following question - Given a text file containing a bunch of biological information, find out the one gene which is {up/down}regulated. Now, for this I have many such (60K) files and have annotated some (1000) of them as to which gene is {up/down}regulated.</p>
<p>Conditions -</p>
<ul>
<li>Many sentences in the file have some gene name mention and some of them also have neighboring text that can help one decide if this is indeed the gene being modulated.</li>
<li>Some files also have NO gene modulated. But these still have gene mentions.</li>
</ul>
<p>Given this, I wanted to ask (having absolutely no background in ML), what sequence learning algorithm/tool do I use that can take in my annotated (training) data (after probably converting the text to vectors somehow!) and can build a good model on which I can then test more files?</p>
<p>Example data -</p>
<blockquote>
<p>Title:    Assessment of Thermotolerance in preshocked hsp70(-/-) and
(+/+) cells</p>
<p>Organism:  Mus musculus</p>
<p>Experiment type:   Expression profiling by array</p>
<p>Summary:   From preliminary experiments, HSP70  deficient MEF cells display moderate thermotolerance to a severe heatshock of 45.5 degrees after a mild preshock at 43 degrees, even in the absence of hsp70 protein. We would like to determine which genes in these cells are being activated to account for this thermotolerance. AQP has also been reported to be important.</p>
<p>Keywords: thermal stress, heat shock response, knockout, cell culture, hsp70</p>
<p>Overall design:    Two cell lines are analyzed - hsp70 knockout and hsp70 rescue cells. 6 microarrays from the (-/-)knockout cells are analyzed (3 Pretreated vs 3 unheated controls). For the (+/+) rescue cells, 4 microarrays are used (2 pretreated and 2 unheated controls). Cells were plated at 3k/well in a 96 well plate, covered with a gas permeable sealer and heat shocked at  43degrees for 30 minutes at the 20 hr time point. The RNA was harvested at 3hrs after heat treatment</p>
</blockquote>
<p>Here my <em>main</em> gene is <code>hsp70</code> and it is <code>down-regulated</code> (deducible from <code>hsp(-/-)</code> or <code>HSP70 deficient</code>). Many other gene names are also there like <code>AQP</code>.
There could be another file with no gene modified at all. In fact, more files have no actual gene modulation than those who do, and all contain gene name mentions.</p>
<p>Any idea would be great!!</p>
",Training and Model Evaluation,applying machine learning biological text data trying solve following question given text file containing bunch biological information find one gene regulated many k file annotated gene regulated condition many sentence file gene name mention also neighboring text help one decide indeed gene modulated file also gene modulated still gene mention given wanted ask absolutely background ml sequence learning algorithm tool use take annotated training data probably converting text vector somehow build good model test file example data title assessment thermotolerance preshocked hsp cell organism mu musculus experiment type expression profiling array summary preliminary experiment hsp deficient mef cell display moderate thermotolerance severe heatshock degree mild preshock degree even absence hsp protein would like determine gene cell activated account thermotolerance aqp ha also reported important keywords thermal stress heat shock response knockout cell culture hsp overall design two cell line analyzed hsp knockout hsp rescue cell microarrays knockout cell analyzed pretreated v unheated control rescue cell microarrays used pretreated unheated control cell plated k well well plate covered gas permeable sealer heat shocked degree minute hr time point rna wa harvested hr heat treatment main gene deducible many gene name also like could another file gene modified fact file actual gene modulation contain gene name mention idea would great
How to use doc2vec with phrases?,"<p>i want to have phrases in doc2vec and i use gensim.phrases. in doc2vec we need tagged document to train the model and i cannot tag the phrases. how i can do this?</p>

<p>here is my code</p>

<pre><code>text = phrases.Phrases(text)
for i in range(len(text)):
    string1 = ""SENT_"" + str(i)

    sentence = doc2vec.LabeledSentence(tags=string1, words=text[i])
    text[i]=sentence

print ""Training model...""
model = Doc2Vec(text, workers=num_workers, \
            size=num_features, min_count = min_word_count, \
            window = context, sample = downsampling)
</code></pre>
",Training and Model Evaluation,use doc vec phrase want phrase doc vec use gensim phrase doc vec need tagged document train model tag phrase code
Entities on my gazette are not recognized,"<p>I would like to create a custom NER model. That's what i did:</p>

<p><strong>TRAINING DATA</strong> (stanford-ner.tsv):</p>

<pre><code>Hello    O
!    O
My    O
name    O
is    O
Damiano    PERSON
.    O
</code></pre>

<p><strong>PROPERTIES</strong> (stanford-ner.prop):</p>

<pre><code>trainFile = stanford-ner.tsv
serializeTo = ner-model.ser.gz
map = word=0,answer=1
maxLeft=1
useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useDisjunctive=true
useSequences=true
usePrevSequences=true
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
useGazettes=true
gazette=gazzetta.txt
cleanGazette=true
</code></pre>

<p><strong>GAZZETTE</strong> gazzetta.txt):</p>

<pre><code>PERSON John
PERSON Andrea
</code></pre>

<p>I build the model via command line with:</p>

<pre><code>java -classpath ""stanford-ner.jar:lib/*"" edu.stanford.nlp.ie.crf.CRFClassifier  -prop stanford-ner.prop
</code></pre>

<p>And test with:</p>

<pre><code>java -classpath ""stanford-ner.jar:lib/*"" edu.stanford.nlp.ie.crf.CRFClassifier  -loadClassifier ner-model.ser.gz -textFile test.txt
</code></pre>

<p>I did two tests with the following texts:</p>

<p><strong>>>> TEST 1 &lt;&lt;&lt;</strong></p>

<ul>
<li><p>TEXT:
Hello! My name is Damiano and this is a fake text to test.</p></li>
<li><p>OUTPUT
<em>Hello/O !/O
My/O name/O is/O Damiano/PERSON and/O this/O is/O a/O fake/O text/O to/O test/O ./O</em></p></li>
</ul>

<p><strong>>>> TEST 2 &lt;&lt;&lt;</strong></p>

<ul>
<li><p>TEXT:
Hello! My name is John and this is a fake text to test.</p></li>
<li><p>OUTPUT
<em>Hello/O !/O
My/O name/O is/O John/O and/O this/O is/O a/O fake/O text/O to/O test/O ./O</em></p></li>
</ul>

<p>As you can see only ""Damiano"" entity is found. This entity is in my training data but ""John"" (second test) is inside the gazzette. So the question is.</p>

<p>Why does John entity is not recognized ?</p>

<p>Thank you so much in advance.</p>
",Training and Model Evaluation,entity gazette recognized would like create custom ner model training data stanford ner tsv property stanford ner prop gazzette gazzetta txt build model via command line test two test following text test text hello name damiano fake text test output hello name damiano person fake text test test text hello name john fake text test output hello name john fake text test see damiano entity found entity training data john second test inside gazzette question doe john entity recognized thank much advance
How to Add Training Data to Out-of-the-Box Parsey McParseFace Model,"<p>I am wondering how, if possible at all, one might train a new SyntaxNet model that uses the training data from the original, out-of-the-box ""ready to parse"" model included on the github page. What I want to do is add new training data to make a new model, but I don't want to make an entirely new and therefore entirely distinct model from the original Parsey McParseFace. So my new model would be trained on the data that the included model was trained on (Penn Treebank, OntoNotes, English Web Treebank), plus my new data. I don't have the money to buy from the LDC the treebanks the original model is trained on. Has anyone attempted this? Thanks very much. </p>
",Training and Model Evaluation,add training data box parsey mcparseface model wondering possible one might train new syntaxnet model us training data original box ready parse model included github page want add new training data make new model want make entirely new therefore entirely distinct model original parsey mcparseface new model would trained data included model wa trained penn treebank ontonotes english web treebank plus new data money buy ldc treebanks original model trained ha anyone attempted thanks much
How MLE is used to train a n-gram model?,"<p>I learned many documents about training a n-gram model using MLE, but as I noticed all the implementation is just to calculate the conditional probability by count the n-grams, my question is what is the relationship with MLE?</p>
",Training and Model Evaluation,mle used train n gram model learned many document training n gram model using mle noticed implementation calculate conditional probability count n gram question relationship mle
Methods to ignore missing word features on test data,"<p>I'm working on a text classification problem, and I have problems with missing values on some features.</p>

<p>I'm calculating class probabilities of words from labeled training data.</p>

<p>For example;</p>

<p>Let word foo belongs to class A for 100 times and belongs to class B for 200 times. In this case, i find class probability vector as [0.33,0.67] , and give it along with the word itself to classifier. </p>

<p>Problem is that, in the test set, there are some words that have not  been seen in training data, so they have no probability vectors. </p>

<p>What could i do for this problem?</p>

<p>I ve tried giving average class probability vector of all words for missing values, but it did not improve accuracy. </p>

<p>Is there a way to make classifier ignore some features during evaluation just for specific instances which does not have a value for giving feature?</p>

<p>Regards</p>
",Training and Model Evaluation,method ignore missing word feature test data working text classification problem problem missing value feature calculating class probability word labeled training data example let word foo belongs class time belongs class b time case find class probability vector give along word classifier problem test set word seen training data probability vector could problem tried giving average class probability vector word missing value improve accuracy way make classifier ignore feature evaluation specific instance doe value giving feature regard
CRF model making is taking too much time,"<p>I am following this <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""nofollow"">link</a> for making a <strong>CRF model</strong>. I am using following command for making model.</p>

<pre><code>java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop
</code></pre>

<p>Model is made successfully but my training data is very much and it is taking too much time. When I closely observe what is happening in the system. <strong>It is just using only one Core of my computer.</strong> <br></p>

<p>Can I run this command in a way that it should use many cores of my computer? It look like that it is implemented as a single thread. Is there is a support of multi-threading? If yes kindly share.</p>
",Training and Model Evaluation,crf model making taking much time following link making crf model using following command making model model made successfully training data much taking much time closely observe happening system using one core computer run command way use many core computer look like implemented single thread support multi threading yes kindly share
Using WEKA classifier model in Java for classifying real time text,"<p>I had used the GUI to train a classifier for some sample arff files . After training I saved the obtained model  .</p>

<p>Now that I need to use this model file in my java code to classify some text , could you please tell me how should I proceed ? I dont want to do an evaluation but would like to classify the input text given .</p>

<p>I had gone thru the <a href=""http://weka.wikispaces.com/Serialization"" rel=""nofollow"">http://weka.wikispaces.com/Serialization</a> &amp; <a href=""http://weka.wikispaces.com/Use+Weka+in+your+Java+code"" rel=""nofollow"">http://weka.wikispaces.com/Use+Weka+in+your+Java+code</a> .</p>

<p>But still couldn't find code for it .I just got an way to load a model file .But didn't get any clue on classifying text directly to classes .  Any help on this regard would be helpfull .</p>
",Training and Model Evaluation,using weka classifier model java classifying real time text used gui train classifier sample arff file training saved obtained model need use model file java code classify text could please tell proceed dont want evaluation would like classify input text given gone thru still find code got way load model file get clue classifying text directly class help regard would helpfull
Compare similarity between names,"<p>I have to make a cross-validation for some data based on names.</p>

<p>The problem I'm facing is that depending on the source, names have slight variations, for example:</p>

<pre><code>L &amp; L AIR CONDITIONING   vs L &amp; L AIR CONDITIONING Service

BEST ROOFING vs ROOFING INC
</code></pre>

<p>I have several thousands of records so do it manually will be very time demanding, I want to automate the process as much as possible.</p>

<p>Since there are additional words it wouldn't be enough to lowercase the names. </p>

<p>Which are good algorithms to handle this?</p>

<p>Maybe to calculate the correlation giving low weight to words like 'INC' or 'Service'</p>

<p>Edit:</p>

<p>I tried the difflib library</p>

<pre><code>difflib.SequenceMatcher(None,name_1.lower(),name_2.lower()).ratio()
</code></pre>

<p>I'm getting a decent result with it.</p>
",Training and Model Evaluation,compare similarity name make cross validation data based name problem facing depending source name slight variation example several thousand record manually time demanding want automate process much possible since additional word enough lowercase name good algorithm handle maybe calculate correlation giving low weight word like inc service edit tried difflib library getting decent result
Maximum entropy estimation for (only) one class in sklearn,"<p>I want to use the library to estimate coefficients of maximum entropy model.</p>

<p>The setup of my experiment is as follows:</p>

<p>I have a set of annotated data: sentence plus a forest of its possible parse trees, of which only one is labeled as correct. I want to train a maximum entropy model to be able to score parse forests for unseen sentences. The problem is in the fact that the forests can be of an exponential size, so I would like to implement a custom algorithm to compute scores (product/sum of feature coefficients, computed dynamically) for making predictions. I would like to use only parse trees labeled as correct to compute the coefficients. My argument for skipping the incorrect data is that I do not need the probability, just the best score (and its derivation).</p>

<p>The problem is that sklearn expects the data to be annotated with at least two classes.</p>

<p>My proposed solution: 
Add 'artificial' feature which is activated only for one additional class and add one record to the training data that has only this one activated. It should not affect the results where this artificial feature will never be activated.</p>

<p>Does it make any sense to you? Can you advise any other solution? I would like to avoid implementing it from scratch.</p>
",Training and Model Evaluation,maximum entropy estimation one class sklearn want use library estimate coefficient maximum entropy model setup experiment follows set annotated data sentence plus forest possible parse tree one labeled correct want train maximum entropy model able score parse forest unseen sentence problem fact forest exponential size would like implement custom algorithm compute score product sum feature coefficient computed dynamically making prediction would like use parse tree labeled correct compute coefficient argument skipping incorrect data need probability best score derivation problem sklearn expects data annotated least two class proposed solution add artificial feature activated one additional class add one record training data ha one activated affect result artificial feature never activated doe make sense advise solution would like avoid implementing scratch
Loss function for OneVsRestClassifier,"<p>I have a <code>OneVsRestClassifier</code> (<code>scikit-learn</code>) which has been trained. </p>

<pre><code>clf = OneVsRestClassifier(LogisticRegression(C=1.2, penalty='l1')).fit(X_train, y_train)
</code></pre>

<p>I want to find out the loss for my test data. I used <code>log_loss</code> function but it does not seem to work because I have multiple classes as outputs for each test case. What do I do? </p>
",Training and Model Evaluation,loss function onevsrestclassifier ha trained want find loss test data used function doe seem work multiple class output test case
What are some evaluations for a spelling correction model?,"<p>I've been looking into spelling correction models and I'm trying to find some evaluation metrics. If you consider false negatives to be trying to fix an already correct word and false positives to be missing an error, then you could calculate precision, recall, and accuracy. However, these metrics do not say anything about the quality of the correction model (whether or not it successfully corrected a wrong word into what the user meant to type) and only evaluates the spell <strong>checking</strong> capabilities rather than the correcting capabilities. </p>
",Training and Model Evaluation,evaluation spelling correction model looking spelling correction model trying find evaluation metric consider false negative trying fix already correct word false positive missing error could calculate precision recall accuracy however metric say anything quality correction model whether successfully corrected wrong word user meant type evaluates spell checking capability rather correcting capability
Does word2vec make sense for supervised learning?,"<p>I have a list of sentence/label pairs to train the model, how should I encode the sentences as input to, say an SVM?</p>
",Training and Model Evaluation,doe word vec make sense supervised learning list sentence label pair train model encode sentence input say svm
Evaluating vector distance measures,"<p>I am working with vectors of word frequencies and trying out some of the different distance measures available in <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html"" rel=""nofollow"">Scikit Learns Pairwise Distances</a>. I would like to use these distances for clustering and classification. </p>

<p>I usually have a feature matrix of ~ 30,000 x 100. My idea was to choose a distance metric that maximizes the pairwise distances by running pairwise differences over the same dataset with the distance metrics <a href=""http://docs.scipy.org/doc/scipy/reference/spatial.distance.html"" rel=""nofollow"">available in Scipy</a> (e.g. Euclidean, Cityblock, etc.) and for each metric</p>

<ul>
<li><p>convert distances computed for the dataset to zscores to normalize across metrics</p></li>
<li><p>get the range of these zscores, i.e. the spread of the distances</p></li>
<li><p>use the distance metric that gives me the widest range of distances as it apparently gives me the maximum spread over my dataset and the most variance to work with. (Cf. code below)</p></li>
</ul>

<p>My questions: </p>

<ul>
<li><p>Does this approach make sense?</p></li>
<li><p>Are there other evaluation procedures that one should try? I found these papers (<a href=""http://apcg.uoregon.edu/envchange/reprints/pdfs/Gavin_QR03.pdf"" rel=""nofollow"">Gavin</a>, <a href=""https://bib.dbvis.de/uploadedFiles/155.pdf"" rel=""nofollow"">Aggarwal</a>, but they don't apply 100 % here...)</p></li>
</ul>

<p>Any help is much appreciated!</p>

<p>My code:</p>

<pre><code>matrix=np.random.uniform(0, .1, size=(10,300)) #test data set

scipy_distances=['euclidean', 'minkowski', ...] #these are the distance metrics 

for d in scipy_distances: #iterate over distances
    distmatrix=sklearn.metrics.pairwise.pairwise_distances(matrix, metric=d)
    distzscores = scipy.stats.mstats.zscore(distmatrix, axis=0, ddof=1)
    diststats=basicstatsmaker(distzscores)
    range=np.ptp(distzscores, axis=0)
    print ""range of metric"", d, np.ptp(range)
</code></pre>
",Training and Model Evaluation,evaluating vector distance measure working vector word frequency trying different distance measure available scikit learns pairwise distance would like use distance clustering classification usually feature matrix x idea wa choose distance metric maximizes pairwise distance running pairwise difference dataset distance metric available scipy e g euclidean cityblock etc metric convert distance computed dataset zscores normalize across metric get range zscores e spread distance use distance metric give widest range distance apparently give maximum spread dataset variance work cf code question doe approach make sense evaluation procedure one try found paper gavin aggarwal apply help much appreciated code
OpenNLP - Is training still required for abbreviation even with abbreviation dictionary?,"<p>I just used <a href=""https://opennlp.apache.org/"" rel=""nofollow noreferrer""><code>OpenNLP</code></a> for a small program where I was supposed to segment a paragraph into sentences.</p>

<p>Although I was able to complete the task after reading some documentation and going through their test cases, I couldn't help but notice that I still have to train for all the abbreviations (example Yahoo!), even when I created a custom abbreviation dictionary, passed it to <code>SentenceDetectorFactory</code> and used it to train <code>SentenceDetectorME</code>.</p>

<p>I am using similar approach as used in this <a href=""http://svn.apache.org/viewvc/opennlp/tags/opennlp-1.5.3-rc3/opennlp-tools/src/test/java/opennlp/tools/sentdetect/SentenceDetectorFactoryTest.java?view=markup"" rel=""nofollow noreferrer"">test case</a>.</p>

<p>I couldn't find this behaviour in their documentation nor I could find any explanation. Is there something I am missing?</p>

<p>Edit: Explanation of my problem</p>

<p>Although I am still working on making a training set as suitable for the domain I am working in, my test data is coming from unstructured data from web. Sometimes it contains an abbreviation that none of my team members ever anticipated. E.g. </p>

<pre><code>Company (acq. by another company) is a good company.
</code></pre>

<p>In this case we never assumed the word <code>acquired</code> to occur like <code>acq.</code> which is clearly used as an abbreviation.</p>

<p>Now we can either add <code>acq.</code> as an abbreviation and let the model continue working, as advertised, or train the model for it. But, even after adding it in abbreviation dictionary, it was not being treated as an abbreviation and we ended up training model for this abbreviation. This seems like a deviation from the concept of abbreviation dictionary.</p>

<p>I tried a small example in <code>NLTK</code> with <code>PunktSentenceTokenizer</code> <a href=""https://stackoverflow.com/a/14103163/1054638"">like this one</a>, and it works perfectly.</p>

<p>I am not sure if I have a training set with even 25,000 sentences, it will make a difference if <code>OpenNLP</code> is ignoring abbreviation dictionary.</p>
",Training and Model Evaluation,opennlp training still required abbreviation even abbreviation dictionary used small program wa supposed segment paragraph sentence although wa able complete task reading documentation going test case help notice still train abbreviation example yahoo even created custom abbreviation dictionary passed used train using similar approach used test case find behaviour documentation could find explanation something missing edit explanation problem although still working making training set suitable domain working test data coming unstructured data web sometimes contains abbreviation none team member ever anticipated e g case never assumed word occur like clearly used abbreviation either add abbreviation let model continue working advertised train model even adding abbreviation dictionary wa treated abbreviation ended training model abbreviation seems like deviation concept abbreviation dictionary tried small example href one work perfectly sure training set even sentence make difference ignoring abbreviation dictionary
How to compute precision and recall for a system that generates questions?,"<p>My system generates questions from a set of sentences. Can generate multiple questions for a single sentence depending on the quality of the sentence. Humans are also given the same set of sentences to generate questions.</p>

<p>For example:</p>

<p>sentence: The capital of Russia is Moscow.</p>

<p>============# System-Generated Questions #=============</p>

<p>Question 1: What the capital of Russia is?</p>

<p>Question 2: What is Moscow?</p>

<p>============# Human-Generated Questions #=============</p>

<p>Question 1: What is the capital of Russia?</p>

<p>Question 2: What is Moscow?</p>

<p>Question 3: Is the capital of Russia Moscow?</p>

<p>I want to evaluate the precision, recall and accuracy of my system. But I do not know how to compute such measures for the Question generation system case.</p>

<p>Data</p>

<ol>
<li>Annotated set of system-generated questions (Acceptable, Unacceptable)</li>
<li>Human-generated questions (for the same set of sentences)</li>
</ol>

<p>Given this data, How do I compute these measures?</p>
",Training and Model Evaluation,compute precision recall system generates question system generates question set sentence generate multiple question single sentence depending quality sentence human also given set sentence generate question example sentence capital russia moscow system generated question question capital russia question moscow human generated question question capital russia question moscow question capital russia moscow want evaluate precision recall accuracy system know compute measure question generation system case data annotated set system generated question acceptable unacceptable human generated question set sentence given data compute measure
How to recombine split sentences?,"<p>I am processing PDFs that have been converted to text. The problem? Sometimes a sentence gets split due to wonky PDF formatting and/or PDF-to-text conversion.</p>

<p>So I'm looking for tools that help ""reassemble"" sentences that got split apart. Page headers or footers often are the culprits. Other elements, such as figures and charts, can come into play as well, but they are not my primary concern right now.</p>

<p>This problem can be tackled in a few ways:</p>

<ol>
<li><p>Removing headers and footers before doing NLP sentence detection would certainly help. I don't know of tools that do this. Do you know of tools or methods? (The general idea to remove page numbers is ""easy"" in theory: find consecutive increasing numbers that occur about once per page.)</p></li>
<li><p>Using NLP parsers that can judge the likelihood that a sentence is grammatically correct would help. That way I can compare the grammatical correctness of two sentences taken separately in comparison with the correctness of their amalgamation. (The Stanford Parser, as I understand it, does not evaluate grammatical correctness.) Do you know of tools that can help?</p></li>
</ol>

<p>Please let me know if you have suggestions, answers, or other ways to approach the problem.</p>
",Training and Model Evaluation,recombine split sentence processing pdfs converted text problem sometimes sentence get split due wonky pdf formatting pdf text conversion looking tool help reassemble sentence got split apart page header footer often culprit element figure chart come play well primary concern right problem tackled way removing header footer nlp sentence detection would certainly help know tool know tool method general idea remove page number easy theory find consecutive increasing number occur per page using nlp parser judge likelihood sentence grammatically correct would help way compare grammatical correctness two sentence taken separately comparison correctness amalgamation stanford parser understand doe evaluate grammatical correctness know tool help please let know suggestion answer way approach problem
What is the best way to compute string similarity?,"<p>Consider you are given with two strings S1 and S2. What are the different algorithms available to compute how similar these strings in terms of their context and which one of them is the most efficient in terms of accuracy?</p>
",Training and Model Evaluation,best way compute string similarity consider given two string different algorithm available compute similar string term context one efficient term accuracy
How can I effectively build a sentiment model training dataset using Stanford CoreNLP?,"<p>I’m interested in training a new sentiment model with my own dataset.  I know that I need to create a file with sentiment labeled for sentences and their component phrases and words.  </p>

<p>I figured out how to create a tree like the following for the sentence “I do not love you.” via the BuildBinarizedDataset:</p>

<pre><code>(1 (1 I) (1 (1 (1 (1 do) (1 not)) (1 (1 love) (1 you))) (1 .)))
</code></pre>

<p>However, this seems terribly difficult to add labels manually in this format, particularly for phrases within a longer sentence.  It would be far easier if I could generate the following for labeling purposes, then convert when I am ready to train the new model. </p>

<pre><code>sentiment_score pline1

sentiment_score  phrase1

sentiment_score  phrase2

...........................

sentiment_score  phraseN

BLANK ROW

sentiment_score pline2
</code></pre>

<p>The problem is that I can’t figure out how to generate this from a sentence with the parser.  If someone could provide guidance, or direct me to documentation that will explain this process, it would help me tremendously.</p>
",Training and Model Evaluation,effectively build sentiment model training dataset using stanford corenlp interested training new sentiment model dataset know need create file sentiment labeled sentence component phrase word figured create tree like following sentence love via buildbinarizeddataset however seems terribly difficult add label manually format particularly phrase within longer sentence would far easier could generate following labeling purpose convert ready train new model problem figure generate sentence parser someone could provide guidance direct documentation explain process would help tremendously
Convert GMM-UBM scores to equicalent accuracy percent,"<p>I have constructed a GMM-UBM model for the speaker recognition purpose. The output of models adapted for each speaker some scores calculated by log likelihood ratio. Now I want to convert these likelihood scores to equivalent number between 0 and 100. Can anybody guide me please?</p>
",Training and Model Evaluation,convert gmm ubm score equicalent accuracy percent constructed gmm ubm model speaker recognition purpose output model adapted speaker score calculated log likelihood ratio want convert likelihood score equivalent number anybody guide please
"Mallet, how to use ExpGain and GradientGain method to construct a FeatureSelector","<p>I want to test the accuracy of a text classifier built with <strong>Mallet</strong>,there are 4 feature selection methods available.(<em>FeatureCounts,InfoGain,ExpGain and GradientGain</em>).
i want to know how to use <strong>ExpGain</strong> and <strong>GradientGain</strong>.</p>

<p>Eg:
FeatureSelector fselector=new FeatureSelector
                        (new FeatureCounts.Factory(),numOfFeature);</p>
",Training and Model Evaluation,mallet use expgain gradientgain method construct featureselector want test accuracy text classifier built mallet feature selection method available featurecounts infogain expgain gradientgain want know use expgain gradientgain eg featureselector fselector new featureselector new featurecounts factory numoffeature
CoreNLP Neural Network Dependency Parser - Difference between evaluation during training versus testing,"<p>I am trying to train a new model with the Stanford CoreNLP implementation of the neural network parser of Chen and Manning (2014). During training, I use the <code>-devFile</code> option to do a UAS evaluation on the development set every 100 iterations. After a few thousand iterations I get a fairly good UAS (around 86 per cent). However, after the training is completed and I try to test it on the same development set, I get a UAS of around 15 per cent. I am using the English Universal Dependencies treebank. </p>

<p>Command line options for training:</p>

<pre><code>java edu.stanford.nlp.parser.nndep.DependencyParser -trainFile ~/Datasets/universal-dependencies-1.2/UD_English/en-ud-train.conllu -devFile ~/Datasets/universal-dependencies-1.2/UD_English/en-ud-dev.conllu -embedFile path/to/wordvecs -embeddingSize 100 -model nndep.model.txt.gz  -trainingThreads 2
</code></pre>

<p>Command line options for testing:</p>

<pre><code>java edu.stanford.nlp.parser.nndep.DependencyParser -model nndep.model.txt.gz -testFile ~/Datasets/universal-dependencies-1.2/UD_English/en-ud-dev.conllu
</code></pre>

<p>When I use the provided UD model for English everything works fine, and I get a UAS of around 80 per cent on the development set. This leads me to believe that my trained model is subpar, and that I might have missed some needed step or option. But since the evaluation during training gives pretty good results, I am a bit confused. From my understanding there should not be that big of a difference between these two evaluations.</p>

<p>So, what might be the cause of the large discrepancy between the evaluation during training versus that at testing time?</p>
",Training and Model Evaluation,corenlp neural network dependency parser difference evaluation training versus testing trying train new model stanford corenlp implementation neural network parser chen manning training use option uas evaluation development set every iteration thousand iteration get fairly good uas around per cent however training completed try test development set get uas around per cent using english universal dependency treebank command line option training command line option testing use provided ud model english everything work fine get uas around per cent development set lead believe trained model subpar might missed needed step option since evaluation training give pretty good result bit confused understanding big difference two evaluation might cause large discrepancy evaluation training versus testing time
Is it possible to augment the existing TextBlob classifier?,"<p>I want to use the majority of the out-of-the-box classifier than TextBlob offers, but I also wanted to add my own small set of training data. This is because the text I am analyzing has some niche words I want to make sure make it into the training set.</p>

<p>So, in TextBlob, they say you can augment an existing classifier like this</p>

<pre><code>&gt;&gt;&gt; new_data = [('She is my best friend.', 'pos'),
        (""I'm happy to have a new friend."", 'pos'),
        (""Stay thirsty, my friend."", 'pos'),
        (""He ain't from around here."", 'neg')]
&gt;&gt;&gt; cl.update(new_data)
True
&gt;&gt;&gt; cl.accuracy(test)
1.0
</code></pre>

<p>However, it doesn't say anything about adding this data to the default classifier. Does anyone know if this is possible?</p>

<p><strong>EDIT</strong></p>

<p>Alternatively, is there a place where I can get enough training data so that I can training my classifier the other way around?</p>
",Training and Model Evaluation,possible augment existing textblob classifier want use majority box classifier textblob offer also wanted add small set training data text analyzing ha niche word want make sure make training set textblob say augment existing classifier like however say anything adding data default classifier doe anyone know possible edit alternatively place get enough training data training classifier way around
Stanford classifier cross validation averaged or aggregate metrics,"<p>With <a href=""http://nlp.stanford.edu/software/classifier.shtml"" rel=""nofollow"">Stanford Classifier</a> it is possible to use cross validation by setting the options in the properties file, such as this for 10-fold cross validation:</p>

<pre><code>crossValidationFolds=10
printCrossValidationDecisions=true
shuffleTrainingData=true
shuffleSeed=1
</code></pre>

<p>Running this will output, per fold, the various metrics, such as precision, recall, Accuracy/micro-averaged F1 and Macro-averaged F1.</p>

<p>Is there an option to get an averaged or otherwise aggregated score of all 10 Accuracy/micro-averaged F1 or all 10 Macro-averaged F1 as part of the output?</p>

<p>In Weka, by default the output after 10-fold cross validation includes averaged metrics over all folds. Is such an option also available in Stanford Classifier? Having a final precision, recall or F1 score available and optimizing the parameters against it like in Weka is very useful, and I would like to do this with Stanford Classifier. How?</p>
",Training and Model Evaluation,stanford classifier cross validation averaged aggregate metric stanford classifier possible use cross validation setting option property file fold cross validation running output per fold various metric precision recall accuracy micro averaged f macro averaged f option get averaged otherwise aggregated score accuracy micro averaged f macro averaged f part output weka default output fold cross validation includes averaged metric fold option also available stanford classifier final precision recall f score available optimizing parameter like weka useful would like stanford classifier
Incremental language model training with lingpipe,"<p>I'm trying to train a <code>DynamicLMClassifier.createNGramProcess(categories,nGram)</code> on a big dataset > 20GB. I'm currently feeding the entire training file as a String to the training methods and for obvious reasons i'm getting a <code>java.lang.OutOfMemoryError: Java heap space</code></p>

<p>Although it might be possible to increase the JVM heap size to support such training i'm interested in finding an incremental method.</p>

<p>The training code looks like this:</p>

<pre><code>char[] csBuf = new char[numChars];
for (int i = 0; i &lt; categories.length; ++i) {
    String category = categories[i];
    File trainingFile = new File(new File(dataDir,category),
                                 category + "".txt"");
    FileInputStream fileIn
        = new FileInputStream(trainingFile);
    InputStreamReader reader
        = new InputStreamReader(fileIn,Strings.UTF8);
    reader.read(csBuf);
    String text = new String(csBuf,0,numChars);
    Classification c = new Classification(category);
    Classified&lt;CharSequence&gt; classified
        = new Classified&lt;CharSequence&gt;(text,c);
    classifier.handle(classified);
    reader.close();
}
</code></pre>

<p>The ideal solution would be to feed classifier.handle() in a loop of N subsets of the training set. Theoretically I think this should be possible since the model only needs to remember ngrams tuples with their respective counts to compute the MLE.</p>
",Training and Model Evaluation,incremental language model training lingpipe trying train big dataset gb currently feeding entire training file string training method obvious reason getting although might possible increase jvm heap size support training interested finding incremental method training code look like ideal solution would feed classifier handle loop n subset training set theoretically think possible since model need remember ngrams tuples respective count compute mle
Doc2Vec: Best practice for feedback loop for training the model,"<p>I am learning about Doc2Vec and the gensim library. I have been able to train my model by creating a corpus of documents such as</p>

<pre><code>LabeledSentence(['what', 'happens', 'when', 'an', 'army', 'of', 'wetbacks', 'towelheads', 'and', 'godless', 'eastern', 'european', 'commies', 'gather', 'their', 'forces', 'south', 'of', 'the', 'border', 'gary', 'busey', 'kicks', 'their', 'butts', 'of', 'course', 'another', 'laughable', 'example', 'of', 'reagan-era', 'cultural', 'fallout', 'bulletproof', 'wastes', 'a', 'decent', 'supporting', 'cast', 'headed', 'by', 'l', 'q', 'jones', 'and', 'thalmus', 'rasulala'], ['LABELED_10', '0'])`
</code></pre>

<p>note that this particular document has two tags, namely 'LABELED_10' and  '0'.</p>

<p>Now after i load my model and perform </p>

<pre><code>print(model.docvecs.most_similar(""LABELED_10""))
</code></pre>

<p>i get </p>

<pre><code>[('LABELED_107', 0.48432376980781555), ('LABELED_110', 0.4827481508255005), ('LABELED_214', 0.48039984703063965), ('LABELED_207', 0.479473352432251), ('LABELED_315', 0.47931796312332153), ('LABELED_307', 0.47898322343826294), ('LABELED_124', 0.4776897132396698), ('LABELED_222', 0.4768940210342407), ('LABELED_413', 0.47479286789894104), ('LABELED_735', 0.47462597489356995)]
</code></pre>

<p>which is perfect ! as i get all the tags most similar to LABELED_10. </p>

<p>Now i would like to have a feedback loop while training my model. So if i give my model a new document, i would like to know how good or bad the model's classification is before tagging and adding that document to my corpus. How would i do that using Doc2Vec? So how do i know whether the documents for LABELED_107 and LABELED_10 are actually similar or not. Here is one approach that i have in mind. Here is the code for my random forest classifier</p>

<pre><code>result = cfun.rfClassifer(n_estimators, trainingDataFV, train[""sentiment""],testDataFV)
</code></pre>

<p>and here is the function</p>

<pre><code>def rfClassifer(n_estimators, trainingSet, label, testSet):

    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

    forest = RandomForestClassifier(n_estimators)
    forest = forest.fit(trainingSet, label)
    result = forest.predict(testSet)

    return result
</code></pre>

<p>and finally i can do</p>

<pre><code>output = pd.DataFrame(data={""id"": test[""id""], ""sentiment"": result})

output.to_csv(""../../submits/Doc2Vec_AvgVecPredict.csv"", index=False, quoting=3)
</code></pre>

<p>Feedback process</p>

<ol>
<li><p>Keep a validation set which is tagged correctly.</p></li>
<li><p>Feed the tagged validation set to the classifier after removing the tags and save the result in a csv.</p></li>
<li><p>Compare the result with another csv that has the correct tags.</p></li>
<li><p>For every mismatch, add those documents to the labeled training set and train the model again.</p></li>
<li><p>Repeat for more validation sets.</p></li>
</ol>

<p>Is this approach correct? Also, can i incrementally train the doc2vec model? Lets say that initially i trained my doc2vec model with 100k tagged docs. Now after the validation step, i need my model to be trained on a further 10k documents. Will i have to train my model from the very beginning ? Meaning will i need to train my model on the initial 100k tagged docs again?</p>

<p>I would really appreciate your insights.</p>

<p>Thanks </p>
",Training and Model Evaluation,doc vec best practice feedback loop training model learning doc vec gensim library able train model creating corpus document note particular document ha two tag namely labeled load model perform get perfect get tag similar labeled would like feedback loop training model give model new document would like know good bad model classification tagging adding document corpus would using doc vec know whether document labeled labeled actually similar one approach mind code random forest classifier function finally feedback process keep validation set tagged correctly feed tagged validation set classifier removing tag save result csv compare result another csv ha correct tag every mismatch add document labeled training set train model repeat validation set approach correct also incrementally train doc vec model let say initially trained doc vec model k tagged doc validation step need model trained k document train model beginning meaning need train model initial k tagged doc would really appreciate insight thanks
Extracting relations from dependency tree,"<p>I am trying to extract relations (triples) from sentences and have been trying to manually sift through the dependency parse from Stanford's CoreNLP and pull out subject - verb - object relations that way.</p>

<p>Problem is the moment you go beyond a simple sentence like ""I am happy"", appositive phrases, <code>ccomp</code> and <code>xcomp</code> compound verbs and <code>conj</code> conjunctions, finding relations becomes more complicated.</p>

<p>Example: ""My teacher, Bob is a great teacher"" (my teacher, is, great teacher) &amp; (My teacher, is, Bob)</p>

<p>""My friends and I don't like running or jumping."" (my friends, don't like, running) &amp; (my friends, don't like, jumping) &amp; (I, don't like, running) &amp; (I, don't like, jumping)</p>

<p>Stanford's OpenIE really doesn't work well for these scenarios (it resolves the first example fairly well, but doesn't get any relations for the second).</p>

<p>My question is: <em>Are there any open source libraries for Java that could perform this relation extraction - either directly from the text or from a dependency parse?</em></p>

<p>I did come accross: <a href=""https://github.com/knowitall/ollie"" rel=""nofollow"">https://github.com/knowitall/ollie</a> which looks very promising - however Ollie is strictly prohibited for commercial use and I need to be able to use the library for commercial use in the future.</p>

<hr>

<p><strong>Another idea:</strong> I'm not very familiar with machine learning techniques - but I was thinking, could I somehow pass a dependency parse of a sentence to some ML model training algorithm with my desired outputs as shown in the examples above and train a model that could do this relation extraction for me?</p>
",Training and Model Evaluation,extracting relation dependency tree trying extract relation triple sentence trying manually sift dependency parse stanford corenlp pull subject verb object relation way problem moment go beyond simple sentence like happy appositive phrase compound verb conjunction finding relation becomes complicated example teacher bob great teacher teacher great teacher teacher bob friend like running jumping friend like running friend like jumping like running like jumping stanford openie really work well scenario resolve first example fairly well get relation second question open source library java could perform relation extraction either directly text dependency parse come accross look promising however ollie strictly prohibited commercial use need able use library commercial use future another idea familiar machine learning technique wa thinking could somehow pas dependency parse sentence ml model training algorithm desired output shown example train model could relation extraction
"moses train-model.perl script error, --lm factor:order:filename requied","<p>when I run:</p>

<pre><code>$MOSES/scripts/training/train-model.perl -hierarchical -ghkm -external-bin-dir /home/zhanwang/giza-pp/tools/ -root-dir . --corpus corpus/nl2mr --f mr --e nl
</code></pre>

<p>showed:</p>

<blockquote>
  <p>ERROR: use --lm factor:order:filename to specify at least one language
  model at /home/zhanwang/mosesdecoder/scripts/training/train-model.perl
  line 597.</p>
  
  <p>root@zhanwang-virtual-machine:/home/zhanwang/mosesmodel/corpus3#
  $MOSES/scripts/training/train-model.perl</p>
</blockquote>

<p>But I don't want to use factor model.</p>

<p>try this too, show me the same thing. whatever parameter I try ,it just ask me to use --<code>lm factor:order:filename</code>.</p>

<p>For an standard phrase model, you will typically run the training script as follows.</p>

<pre><code> train-model.perl -root-dir . --corpus corpus/euro --f de --e en
</code></pre>

<p>I want to build syntax-base translation model, what should I do?</p>

<pre><code>$MOSES/scripts/training/train-model.perl -ghkm -external-bin-dir /home/zhanwang/giza-pp/tools/ -root-dir . --corpus corpus/nl2mr --f mr --e nl
</code></pre>

<p>here is my corpus:</p>

<pre><code>root@zhanwang-virtual-machine:/home/zhanwang/mosesmodel/corpus3/corpus# head nl2mr.nl nl2mr.mr

==&gt; nl2mr.nl &lt;==

Give me the cities in Virginia .

What are the high points of states surrounding Mississippi ?

==&gt; nl2mr.mr &lt;==

answer city loc_2 stateid 'virginia'

answer high_point_1 state next_to_2 stateid 'mississippi'
</code></pre>

<p>I want to extract ghkm rules and build a model that can translate ""Give me the cities in Virginia ."" to ""answer city loc_2 stateid 'virginia'""</p>
",Training and Model Evaluation,moses train model perl script error lm factor order filename requied run showed error use lm factor order filename specify least one language model home zhanwang mosesdecoder script training train model perl line root zhanwang virtual machine home zhanwang mosesmodel corpus moses script training train model perl want use factor model try show thing whatever parameter try ask use standard phrase model typically run training script follows want build syntax base translation model corpus want extract ghkm rule build model translate give city virginia answer city loc stateid virginia
Re titling a article/document based on the content in R/Python,"<p>I am trying to title an article based on the content and unable to get it right. I am using LDA package in R </p>

<p>Basically it fits a generative topic model which accounts for both the words which occur in a collection of documents as well as the links between the documents.</p>

<pre><code>rtm.collapsed.gibbs.sampler(documents, links, K, vocab, num.iterations,
alpha, eta, beta, trace = 0L, test.start = length(documents) + 1L)
rtm.em(documents, links, K, vocab, num.e.iterations, num.m.iterations,
alpha, eta, lambda = sum(sapply(links, length))/(length(links) *(length(links) -1)/2), initial.beta = rep(3, K), trace = 0L,
test.start = length(documents) + 1L, tempering = 0.0)
</code></pre>

<p>This isn't working great! Can Somebody help me on this please ?</p>
",Training and Model Evaluation,titling article document based content r python trying title article based content unable get right using lda package r basically fit generative topic model account word occur collection document well link document working great somebody help please
NLP POS Tagger For A New Domain,"<p>I am using NLTK and would like to use an existing pos-tagger which has been pre-trained and further train it for a new domain. </p>

<p>What I understand from Perceptron tagger operation is that it looks up the word in a file and if it cannot find the word, it predicts the POS for that word. Ideally I would like the tagger to predict the domain specific word. But I do not have regular annotated text for the new domain (I can comb through something like a medical dictionary to get domain specific words).There are two things I can do with the domain specific words:</p>

<p>1.I can write simple sentences like ""I had angioplasty"" and annotate them. This will allow the tagger to predict any unseen domain specific words with more accuracy. Will writing and annotating simple sentences bias the model compared to annotation from regular text?</p>

<p>2.I can directly include the word in the file. Not my first choice.</p>

<p>I also came across a document which used HMM for domain specific training but I am not sure it will work. </p>

<p>How should I go about it?</p>
",Training and Model Evaluation,nlp po tagger new domain using nltk would like use existing po tagger ha pre trained train new domain understand perceptron tagger operation look word file find word predicts po word ideally would like tagger predict domain specific word regular annotated text new domain comb something like medical dictionary get domain specific word two thing domain specific word write simple sentence like angioplasty annotate allow tagger predict unseen domain specific word accuracy writing annotating simple sentence bias model compared annotation regular text directly include word file first choice also came across document used hmm domain specific training sure work go
Using NLTK to perform document classification on website content issues with BeautifulSoup and NaiveBayes,"<p>I have a Python 2.7 project where I want to classify websites based on their content. I have a database in which I numerous website URLs and their associated category. There are many categories (= labels), and I wish to classify new sites into the corresponding category based on their content. I've been following the NLTK classification tutorial/example listed <a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow"">here</a>, but have run into some problems I cannot explain.</p>

<p>Here is an outline of the process that I use:</p>

<ol>
<li>Use MySQLdb to retrieve the category associated with a given website URL. 
   This will be used when extracting data (content) from the URL to pair it with the
   category (= label) of the site.</li>
<li>Use a getSiteContent(site) function to extract the content from a website</li>
</ol>

<p>The above function looks like this:</p>

<pre><code>def getSiteContent(site):
    try:
        response = urllib2.urlopen(site, timeout = 1)
        htmlSource = response.read()
    except Exception as e: # &lt;=== some websites may be inaccessible as list isn't up-to-date
        global errors
        errors += 1
        return ''

    soup = BeautifulSoup(htmlSource)
    for script in soup.find_all('script'):
        script.extract()

    commonWords = set(stopwords.words('english'))
    commonWords.update(['function', 'document', 'window', 'functions',     'getElementsByTagName', 'parentNode', 'getDocumentById', 'javascript', 'createElement',     'Copyright', 'Copyrights', 'respective', 'owners', 'Contact Us', 'Mobile Version', 'FAQ',     'Privacy Policy', 'Terms of Service', 'Legal Disclaimer' ])

    text = soup.get_text()

    # Remove ',', '/', '%', ':'
    re.sub(r'(\\d+[,/%:]?\\d*)', '', text)
    # Remove digits
    re.sub(r'\d+', '', text)
    # Remove non-ASCII
    re.sub(r'[^\x00-\x7F]',' ', text)
    # Remove stopwords
    for word in commonWords :
        text = text.replace(' '+word+' ', ' ')

    # Tokenize the site content using NLTK
    tokens = word_tokenize(text)

    # We collect some word statistics, i.e. how many times a given word appears in the     text
    counts = defaultdict(int)
    for token in tokens:
        counts[token] += 1

    features = {}
    # Get rid of words that appear less than 3 times
    for word in tokens:
        if counts[word] &gt;= 3 :
            features['count(%s)' % word] = counts[word]

    return features 
</code></pre>

<p>When all the above is done, I do the following:</p>

<pre><code>train = getTrainingSet(n)
random.shuffle(train)
</code></pre>

<p>Where n is the number of sites I wish to train my model against. </p>

<p>Afterwards, I do:</p>

<pre><code>feature_set = []
count = 0
for (site, category) in train:
    result = getSiteContent(site)
    count += 1
    if result != '':
        print ""%d. Got content for %s"" % (count, site)
        feature_set.append((result, category))
    else  :
        print ""%d. Failed to get content for %s"" % (count, site)
</code></pre>

<p>The print statements are mainly for debugging purposes at this time. After I do the above, <code>feature_set</code> contains something similar to the following:</p>

<pre><code>print feature_set
[({u'count(import)': 22, u'count(maxim)': 22, u'count(Maxim)': 5, u'count(css)': 22, u'count(//www)': 22, u'count(;)': 22, u'count(url)': 22, u'count(Gift)': 3, u""count('')"": 44, u'count(http)': 22, u'count(&amp;)': 3, u'count(ng16ub)': 22, u'count(STYLEThe)': 3, u'count(com/modules/system/system)': 4, u'count(@)': 22, u'count(?)': 22}, 'Arts &amp; Entertainment'), ({u'count(import)': 3, u'count(css)': 3, u'count(\u05d4\u05d9\u05d5\u05dd)': 4, u'count(\u05de\u05d9\u05dc\u05d5\u05df)': 6, u'count(;)': 3, u'count(\u05e2\u05d1\u05e8\u05d9)': 4, u'count(\u05d0\u05ea)': 3, u'count(\u05de\u05d5\u05e8\u05e4\u05d9\u05e7\u05e1)': 6, u""count('')"": 6, u'count(\u05d4\u05d5\u05d0)': 3, u'count(\u05e8\u05d1\u05de\u05d9\u05dc\u05d9\u05dd)': 3, u'count(ver=01122014_4)': 3, u'count(|)': 4, u'count(``)': 4, u'count(@)': 3, u'count(?)': 7}, 'Miscellaneous')]
</code></pre>

<p>Afterwards, I try to train my classifier and then run it against the test data that I extract from <code>feature_set</code></p>

<pre><code>train_set, test_set = feature_set[len(train)/2:], feature_set[:len(train)/2]
print ""Num in train_set: %d"" % len(train_set)
print ""Num in test_set: %d"" % len(test_set)
classifier = nltk.NaiveBayesClassifier.train(train_set) # &lt;=== classified declared on train_set
print classifier.show_most_informative_features(5)
print ""=== Classifying a site ===""
print classifier.classify(getSiteContent(""http://www.mangaspoiler.com""))
print ""Non-working sites: %d"" % errors
print ""Classifier accuracy: %d"" % nltk.classify.accuracy(classifier, test_set)
</code></pre>

<p>This is pretty much exactly how the tutorial on the NLTK documentation website does it. However, the results are the following (given a set of 100 websites):</p>

<pre><code>$ python classify.py
Num in train_set: 23
Num in test_set: 50
Most Informative Features
            count(Pizza) = None           Arts &amp; : Techno =      1.0 : 1.0
None
=== Classifying a site ===
Technology &amp; Computing
Non-working sites: 27
Classifier accuracy: 0
</code></pre>

<hr>

<p>Now, there are obviously a few problems with this:</p>

<ol>
<li><p>The word tokens contain unicode characters such as <code>\u05e2\u05d1\u05e8\u05d9</code>, as it seems that the regex for removing them only works if they are standalone. This is a minor problem.</p></li>
<li><p>A bigger problem is that even when I <code>print</code> the <code>feature_set</code>, the word tokens are displayed as <code>u'count(...)' = #</code> as opposed to <code>'count(...)' = #</code>. I think this may be a bigger issue and part of why my classifier is failing.</p></li>
<li><p>The classifier is, obviously, failing catastrophically as some point. The accuracy is listed as <code>0</code> even if I feed my entire dataset into the classifier, which seems extremely unlikely.</p></li>
<li><p>The <code>Most Informative Features</code> function says that <code>count(Pizza) = None</code>. The code where I declare <code>defaultdict(int)</code>, however, requires that every entry be associated with the number of appearances in the text.</p></li>
</ol>

<hr>

<p>I am at quite a loss as to why this happens. As far as I can tell, my data is structured identically to the data that the NLTK documentation uses in its tutorial on the website I linked at the top of this question. If anyone who has worked with NLTK has seen this behaviour before, I would greatly appreciate any tips as to what I could be doing wrong.</p>
",Training and Model Evaluation,using nltk perform document classification website content issue beautifulsoup naivebayes python project want classify website based content database numerous website url associated category many category label wish classify new site corresponding category based content following nltk classification tutorial example listed run problem explain outline process use use mysqldb retrieve category associated given website url used extracting data content url pair category label site use getsitecontent site function extract content website function look like done following n number site wish train model afterwards print statement mainly debugging purpose time contains something similar following afterwards try train classifier run test data extract pretty much exactly tutorial nltk documentation website doe however result following given set website obviously problem word token contain unicode character seems regex removing work standalone minor problem bigger problem even word token displayed opposed think may bigger issue part classifier failing classifier obviously failing catastrophically point accuracy listed even feed entire dataset classifier seems extremely unlikely function say code declare however requires every entry associated number appearance text quite loss happens far tell data structured identically data nltk documentation us tutorial website linked top question anyone ha worked nltk ha seen behaviour would greatly appreciate tip could wrong
Scikit Learn multiclass classification (perfect results),"<p>Hello i'm very new to scikit learn and i'm trying to do some text multiclass classification, i'm following <a href=""http://marcobonzanini.com/2015/01/19/sentiment-analysis-with-python-and-scikit-learn/"" rel=""nofollow noreferrer"">this</a> tutorial.<br />
My dataset has 4 classes <code>'fipdl', 'lna','m5s','pd'</code> , so i got 4 folder(one for class) each folder contains 120 txt files with about 25 rows of text(facebook statuses).
I use 90% of it for training , 10% for testing.<br />
10% of my txt files names starts with 'ts' and i'm using these for testing.<br />
so my code is :</p>
<pre><code>import sys
import os
import time

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import svm
from sklearn.metrics import classification_report
from sklearn.preprocessing import MultiLabelBinarizer

def usage():
    print(&quot;Usage:&quot;)
    print(&quot;python %s &lt;data_dir&gt;&quot; % sys.argv[0])

if __name__ == '__main__':

if len(sys.argv) &lt; 2:
    usage()
    sys.exit(1)

data_dir = sys.argv[1]
classes = ['fipdl', 'lna','m5s','pd']

# Read the data
train_data = []
train_labels = []
test_data = []
test_labels = []

for curr_class in classes:
    dirname = os.path.join(data_dir, curr_class)
    for fname in os.listdir(dirname):
        with open(os.path.join(dirname, fname), 'r') as f:
            content = f.read()
            if fname.startswith('ts'):
                test_data.append(content)
                test_labels.append(curr_class)
                
            else:
                train_data.append(content)
                train_labels.append(curr_class)



# Create feature vectors
vectorizer = TfidfVectorizer(min_df=5,
                             max_df = 0.8,
                             sublinear_tf=True,
                             use_idf=True)
train_vectors = vectorizer.fit_transform(train_data)
test_vectors = vectorizer.transform(test_data)
# Perform classification with SVM, kernel=rbf
classifier_rbf = svm.SVC()
t0 = time.time()
classifier_rbf.fit(train_vectors, train_labels)
t1 = time.time()
prediction_rbf = classifier_rbf.predict(test_vectors)
t2 = time.time()
time_rbf_train = t1-t0
time_rbf_predict = t2-t1

# Perform classification with SVM, kernel=linear
classifier_linear = svm.SVC(kernel='linear')
t0 = time.time()
classifier_linear.fit(train_vectors, train_labels)
t1 = time.time()
prediction_linear = classifier_linear.predict(test_vectors)
t2 = time.time()
time_linear_train = t1-t0
time_linear_predict = t2-t1

# Perform classification with SVM, kernel=linear
classifier_liblinear = svm.LinearSVC()
t0 = time.time()
classifier_liblinear.fit(train_vectors, train_labels)
t1 = time.time()
prediction_liblinear = classifier_liblinear.predict(test_vectors)
t2 = time.time()
time_liblinear_train = t1-t0
time_liblinear_predict = t2-t1

# Print results in a nice table
print(&quot;Results for SVC(kernel=rbf)&quot;)
print(&quot;Training time: %fs; Prediction time: %fs&quot; % (time_rbf_train, time_rbf_predict))
print(classification_report(test_labels, prediction_rbf))
print(&quot;Results for SVC(kernel=linear)&quot;)
print(&quot;Training time: %fs; Prediction time: %fs&quot; % (time_linear_train, time_linear_predict))
print(classification_report(test_labels, prediction_linear))
print(&quot;Results for LinearSVC()&quot;)
print(&quot;Training time: %fs; Prediction time: %fs&quot; % (time_liblinear_train, time_liblinear_predict))
print(classification_report(test_labels, prediction_liblinear))
</code></pre>
<p>output :</p>
<pre><code>Results for SVC(kernel=rbf)
Training time: 0.940005s; Prediction time: 0.055970s
             precision    recall  f1-score   support

      fipdl       1.00      1.00      1.00        11
        lna       1.00      1.00      1.00        11
        m5s       1.00      1.00      1.00        11
         pd       1.00      1.00      1.00        11

avg / total       1.00      1.00      1.00        44

Results for SVC(kernel=linear)
Training time: 0.941262s; Prediction time: 0.056382s
             precision    recall  f1-score   support

      fipdl       1.00      1.00      1.00        11
        lna       1.00      1.00      1.00        11
        m5s       1.00      1.00      1.00        11
         pd       1.00      1.00      1.00        11

avg / total       1.00      1.00      1.00        44

Results for LinearSVC()
Training time: 0.034038s; Prediction time: 0.000323s
             precision    recall  f1-score   support

      fipdl       1.00      1.00      1.00        11
        lna       1.00      1.00      1.00        11
        m5s       1.00      1.00      1.00        11
         pd       1.00      1.00      1.00        11

avg / total       1.00      1.00      1.00        44
</code></pre>
<p>Now the result seem too good to be true since every method gave me 1 of precision.<br />
I think also would be nice to try to predict string passed by me instead of a test set,for do more tests, so i change the original code to this:</p>
<pre><code>import sys
import os
import time

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import svm
from sklearn.metrics import classification_report
from sklearn.preprocessing import MultiLabelBinarizer

def usage():
    print(&quot;Usage:&quot;)
    print(&quot;python %s &lt;data_dir&gt;&quot; % sys.argv[0])

if __name__ == '__main__':

    if len(sys.argv) &lt; 2:
        usage()
        sys.exit(1)

    data_dir = sys.argv[1]
    classes = ['fipdl', 'lna','m5s','pd']

    # Read the data
    train_data = []
    train_labels = []
    test_data = []
    test_labels = []
    
    for curr_class in classes:
        dirname = os.path.join(data_dir, curr_class)
        for fname in os.listdir(dirname):
            with open(os.path.join(dirname, fname), 'r') as f:
                content = f.read()
                if fname.startswith('ts'):
                    test_data.append(content)
                    test_labels.append(curr_class)
                    
                else:
                    train_data.append(content)
                    train_labels.append(curr_class)


    
    # Create feature vectors
    vectorizer = TfidfVectorizer(min_df=5,
                                 max_df = 0.8,
                                 sublinear_tf=True,
                                 use_idf=True)
    string = ['string to predict'] #my string
    vector = vectorizer.transform(string) #convert
    train_vectors = vectorizer.fit_transform(train_data)
    
    test_vectors = vectorizer.transform(test_data)
    # Perform classification with SVM, kernel=rbf
    classifier_rbf = svm.SVC()
    t0 = time.time()
    classifier_rbf.fit(train_vectors, train_labels)
    t1 = time.time()
    prediction_rbf = classifier_rbf.predict(vector) #predict
    t2 = time.time()
    time_rbf_train = t1-t0
    time_rbf_predict = t2-t1

    # Perform classification with SVM, kernel=linear
    classifier_linear = svm.SVC(kernel='linear')
    t0 = time.time()
    classifier_linear.fit(train_vectors, train_labels)
    t1 = time.time()
    prediction_linear = classifier_linear.predict(test_vectors)
    t2 = time.time()
    time_linear_train = t1-t0
    time_linear_predict = t2-t1

    # Perform classification with SVM, kernel=linear
    classifier_liblinear = svm.LinearSVC()
    t0 = time.time()
    classifier_liblinear.fit(train_vectors, train_labels)
    t1 = time.time()
    prediction_liblinear = classifier_liblinear.predict(test_vectors)
    t2 = time.time()
    time_liblinear_train = t1-t0
    time_liblinear_predict = t2-t1

    # Print results in a nice table
    print(&quot;Results for SVC(kernel=rbf)&quot;)
    print(&quot;Training time: %fs; Prediction time: %fs&quot; % (time_rbf_train, time_rbf_predict))
    print(classification_report(test_labels, prediction_rbf))
    print(&quot;Results for SVC(kernel=linear)&quot;)
    print(&quot;Training time: %fs; Prediction time: %fs&quot; % (time_linear_train, time_linear_predict))
    print(classification_report(test_labels, prediction_linear))
    print(&quot;Results for LinearSVC()&quot;)
    print(&quot;Training time: %fs; Prediction time: %fs&quot; % (time_liblinear_train, time_liblinear_predict))
    print(classification_report(test_labels, prediction_liblinear))
</code></pre>
<p>but it fails with</p>
<pre><code>ValueError: Found arrays with inconsistent numbers of samples: [18 44]
</code></pre>
<p>i'm missing something? or maybe this is a totally wrong approach?<br />
any help would be really appreciated,<br />
thanks in advance Nico.</p>
",Training and Model Evaluation,scikit learn multiclass classification perfect result hello new scikit learn trying text multiclass classification following tutorial dataset ha class got folder one class folder contains txt file row text facebook status use training testing txt file name start using testing code output result seem good true since every method gave precision think also would nice try predict string passed instead test set test change original code fails missing something maybe totally wrong approach help would really appreciated thanks advance nico
How could I identify a sentence disclosing some specific information in a paragraph?,"<p>For example, I have such a paragraph as below:
The first sentence (bold and italic) is what I hope to identify out. 
The identification goal includes: 
1. whether this paragraph contain such disclosure. 
2. what this disclosure is.</p>

<p>The possible problems are :
1. this sentence may not be in the begin of the text string. it could be in any place of the given paragraph.
2. this sentence may vary with words but with same meaning. For example, it could also be expressed as: ""Sample provided for review"" or ""They sent to me an item for evaluation"" or something like this.</p>

<p>So how could I identify such disclosures ? Anyone's idea would be greatly appreciated. Thanks.</p>

<p>The paragraph:</p>

<p><strong><em>I was sent this Earbuds Audiophile headphones to review.</em></strong> I am just going to copy here the information from the site: ""High Definition Stereo Earphones with microphone Equipped with two 9mm high fidelity drivers, unique sound performance, well-balanced bass, mids and trebble. Designed specially for those who enjoy classic music, rock music, pop music, or gaming with superb quality sound. Let COR3 be your in ear sports earbuds. Replaceable Back Caps, inline controller and mic
Extreme flexible tangle free flat TPE cable including inline controller with universal microphone. Play/Pause your music or Answer/Hang up a call with a touch of a button right next to your hands, feature available depending on your device capability. COR3 should be your best gaming earbuds.
Extremely Comfortable</p>

<p>Methods I have tried: 
Up to now, my processing is very naive: 1) humanly labeled 1000 pieces of reviews as a binary variable (1 represents including the disclosure text, 0 otherwise). 2) Collect all the disclosure texts as a corpus denoted by DisclosureCor; 3) Based on these DisclosureCor, I discovered some basic regular regression rules, like "" review.* evaluation|test|opinion"". 4) Using these summarized rules to label new data. 5) The problem is that rules may not be complete, since they are just my own subject summarizations. Besides,  theses rules may not only occur in the disclosure text, but also some other parts in the review paragraphs, thus generating lots of noises (i.e. low precision); 6) I tried to use classification based association rules to train some rules from the labeled data. As keywords number is huge, long long time is needed to train the rule, crashed often. 7) I also tried to compare the similarity the review paragraph with the DisclosureCorp, but it's difficult to find a threshold to cut whether a review paragraph contains disclosure.  These are all the efforts I have tried, could you please give me some hints ? Thanks.</p>
",Training and Model Evaluation,could identify sentence disclosing specific information paragraph example paragraph first sentence bold italic hope identify identification goal includes whether paragraph contain disclosure disclosure possible problem sentence may begin text string could place given paragraph sentence may vary word meaning example could also expressed sample provided review sent item evaluation something like could identify disclosure anyone idea would greatly appreciated thanks paragraph wa sent earbuds audiophile headphone review going copy information site high definition stereo earphone microphone equipped two mm high fidelity driver unique sound performance well balanced bass mids trebble designed specially enjoy classic music rock music pop music gaming superb quality sound let cor ear sport earbuds replaceable back cap inline controller mic extreme flexible tangle free flat tpe cable including inline controller universal microphone play pause music answer hang call touch button right next hand feature available depending device capability cor best gaming earbuds extremely comfortable method tried processing naive humanly labeled piece review binary variable represents including disclosure text otherwise collect disclosure text corpus denoted disclosurecor based disclosurecor discovered basic regular regression rule like review evaluation test opinion using summarized rule label new data problem rule may complete since subject summarization besides thesis rule may occur disclosure text also part review paragraph thus generating lot noise e low precision tried use classification based association rule train rule labeled data keywords number huge long long time needed train rule crashed often also tried compare similarity review paragraph disclosurecorp difficult find threshold cut whether review paragraph contains disclosure effort tried could please give hint thanks
Using GATE Batch Learning PR to identify parts of emails,"<p>Can the GATE Batch Leaning PR be used to reliably identify the 'composed' part of an Email i.e. just the part that was written by the sender excluding</p>

<ul>
<li>quoted previous conversation,</li>
<li>signature</li>
<li>and header (From, To,  Date).</li>
</ul>

<p>At the moment I consider the greeting (""Dear Garry,"") and the closing (""Kind Regards, John"") as being part of the composed text.</p>

<p>I tried training with just a small hand-annotated set of emails and with the configuration shown below. As attributes I tried various features of annotation types Token, SpaceToken, Split, Date, Address and Lookup which were produced by a default ANNIE. I tried all of them together and each individually but with limited success (recall~25%, precision~50).</p>

<p>Can someone recommend which annotation types and features should give a good outcome? I could also use a suggestion for the other configuration parameters for example INSTANCE-TYPE.</p>

<p>The workings of the batch learner are still very opaque to me. For instance I couldn't find a source that explains how attributes of instances and their position are converted into a machine learning feature vector. So I wonder if the presence of a certain attribute and/or it's relative position affect the placement of the learned annotation.</p>

<p>Every help is appreciated. Thanks</p>

<p><br></p>

<p>more less interesting details:<br>
The documentation isn't very clear on how to use the INSTANCE-TYPE option but tests show me that I can only use annotation types as attributes if they have the same range as the annotation type selected for INSTANCE-TYPE. So in order to be able to also include spaces and line breaks in the learning I slightly changed the tokenizer rules of ANNIE to create Token annotations instead of SpaceToken annotations for them. Unfortunately this didn't really improve the outcome.</p>

<p><br></p>

<p>batch learning config:</p>

<pre><code>&lt;?xml version=""1.0""?&gt;
    &lt;ML-CONFIG&gt;
        &lt;SURROUND value=""true""/&gt;
        &lt;FILTERING ratio=""0.1"" dis=""near""/&gt;
        &lt;PARAMETER name=""thresholdProbabilityEntity"" value=""0.2""/&gt;
        &lt;PARAMETER name=""thresholdProbabilityBoundary"" value=""0.4""/&gt;
        &lt;PARAMETER name=""thresholdProbabilityClassification"" value=""0.5""/&gt;
        &lt;multiClassification2Binary method=""one-vs-others""/&gt;
        &lt;EVALUATION method=""kfold"" runs=""2""/&gt;
        &lt;ENGINE nickname=""SVM"" implementationName=""SVMLibSvmJava""
    options="" -c 0.7 -t 0 -m 100 -tau 0.4 ""/&gt;
        &lt;DATASET&gt;
            &lt;INSTANCE-TYPE&gt;Token&lt;/INSTANCE-TYPE&gt;


            &lt;ATTRIBUTELIST&gt;
                &lt;NAME&gt;TokenCategory&lt;/NAME&gt;
                &lt;SEMTYPE&gt;NOMINAL&lt;/SEMTYPE&gt;
                &lt;TYPE&gt;Token&lt;/TYPE&gt;
                &lt;FEATURE&gt;category&lt;/FEATURE&gt;
                &lt;RANGE from=""-5"" to=""5""/&gt;
            &lt;/ATTRIBUTELIST&gt;

            &lt;ATTRIBUTELIST&gt;
                &lt;NAME&gt;TokenKind&lt;/NAME&gt;
                &lt;SEMTYPE&gt;NOMINAL&lt;/SEMTYPE&gt;
                &lt;TYPE&gt;Token&lt;/TYPE&gt;
                &lt;FEATURE&gt;kind&lt;/FEATURE&gt;
                &lt;RANGE from=""-5"" to=""5""/&gt;
            &lt;/ATTRIBUTELIST&gt;

            &lt;ATTRIBUTELIST&gt;
                &lt;NAME&gt;TokenLength&lt;/NAME&gt;
                &lt;SEMTYPE&gt;NOMINAL&lt;/SEMTYPE&gt;
                &lt;TYPE&gt;Token&lt;/TYPE&gt;
                &lt;FEATURE&gt;length&lt;/FEATURE&gt;
                &lt;RANGE from=""-5"" to=""5""/&gt;
            &lt;/ATTRIBUTELIST&gt;

            &lt;ATTRIBUTELIST&gt;
                &lt;NAME&gt;TokenString&lt;/NAME&gt;
                &lt;SEMTYPE&gt;NOMINAL&lt;/SEMTYPE&gt;
                &lt;TYPE&gt;Token&lt;/TYPE&gt;
                &lt;FEATURE&gt;string&lt;/FEATURE&gt;
                &lt;RANGE from=""-5"" to=""5""/&gt;
            &lt;/ATTRIBUTELIST&gt;  


            &lt;ATTRIBUTELIST&gt;
                &lt;NAME&gt;SpaceTokenKind&lt;/NAME&gt;
                &lt;SEMTYPE&gt;NOMINAL&lt;/SEMTYPE&gt;
                &lt;TYPE&gt;SpaceToken&lt;/TYPE&gt;
                &lt;FEATURE&gt;kind&lt;/FEATURE&gt;
                &lt;RANGE from=""-5"" to=""5""/&gt;
            &lt;/ATTRIBUTELIST&gt;


            &lt;ATTRIBUTELIST&gt;
                &lt;NAME&gt;SplitKind&lt;/NAME&gt;
                &lt;SEMTYPE&gt;NOMINAL&lt;/SEMTYPE&gt;
                &lt;TYPE&gt;Split&lt;/TYPE&gt;
                &lt;FEATURE&gt;kind&lt;/FEATURE&gt;
                &lt;RANGE from=""-5"" to=""5""/&gt;
            &lt;/ATTRIBUTELIST&gt;


            &lt;ATTRIBUTELIST&gt;
                &lt;NAME&gt;DateKind&lt;/NAME&gt;
                &lt;SEMTYPE&gt;NOMINAL&lt;/SEMTYPE&gt;
                &lt;TYPE&gt;Date&lt;/TYPE&gt;
                &lt;FEATURE&gt;kind&lt;/FEATURE&gt;
                &lt;RANGE from=""-5"" to=""5""/&gt;
            &lt;/ATTRIBUTELIST&gt;

            &lt;ATTRIBUTELIST&gt;
                &lt;NAME&gt;DateRuleFinal&lt;/NAME&gt;
                &lt;SEMTYPE&gt;NOMINAL&lt;/SEMTYPE&gt;
                &lt;TYPE&gt;Date&lt;/TYPE&gt;
                &lt;FEATURE&gt;ruleFinal&lt;/FEATURE&gt;
                &lt;RANGE from=""-5"" to=""5""/&gt;
            &lt;/ATTRIBUTELIST&gt;


            &lt;ATTRIBUTELIST&gt;
                &lt;NAME&gt;AddressKind&lt;/NAME&gt;
                &lt;SEMTYPE&gt;NOMINAL&lt;/SEMTYPE&gt;
                &lt;TYPE&gt;Address&lt;/TYPE&gt;
                &lt;FEATURE&gt;kind&lt;/FEATURE&gt;
                &lt;RANGE from=""-5"" to=""5""/&gt;
            &lt;/ATTRIBUTELIST&gt;

            &lt;ATTRIBUTELIST&gt;
                &lt;NAME&gt;AddressRule&lt;/NAME&gt;
                &lt;SEMTYPE&gt;NOMINAL&lt;/SEMTYPE&gt;
                &lt;TYPE&gt;Address&lt;/TYPE&gt;
                &lt;FEATURE&gt;rule&lt;/FEATURE&gt;
                &lt;RANGE from=""-5"" to=""5""/&gt;
            &lt;/ATTRIBUTELIST&gt;

            &lt;ATTRIBUTELIST&gt;
                &lt;NAME&gt;AddressRuleFinal&lt;/NAME&gt;
                &lt;SEMTYPE&gt;NOMINAL&lt;/SEMTYPE&gt;
                &lt;TYPE&gt;Address&lt;/TYPE&gt;
                &lt;FEATURE&gt;ruleFinal&lt;/FEATURE&gt;
                &lt;RANGE from=""-5"" to=""5""/&gt;
            &lt;/ATTRIBUTELIST&gt;


            &lt;ATTRIBUTELIST&gt;
                &lt;NAME&gt;SplitKind&lt;/NAME&gt;
                &lt;SEMTYPE&gt;NOMINAL&lt;/SEMTYPE&gt;
                &lt;TYPE&gt;Split&lt;/TYPE&gt;
                &lt;FEATURE&gt;kind&lt;/FEATURE&gt;
                &lt;RANGE from=""-5"" to=""5""/&gt;
            &lt;/ATTRIBUTELIST&gt;

            &lt;ATTRIBUTELIST&gt;
                &lt;NAME&gt;Orthography&lt;/NAME&gt;
                &lt;SEMTYPE&gt;NOMINAL&lt;/SEMTYPE&gt;
                &lt;TYPE&gt;Token&lt;/TYPE&gt;
                &lt;FEATURE&gt;orth&lt;/FEATURE&gt;
                &lt;RANGE from=""-5"" to=""5""/&gt;
            &lt;/ATTRIBUTELIST&gt;

            &lt;ATTRIBUTELIST&gt;
                &lt;NAME&gt;Tokenkind&lt;/NAME&gt;
                &lt;SEMTYPE&gt;NOMINAL&lt;/SEMTYPE&gt;
                &lt;TYPE&gt;Token&lt;/TYPE&gt;
                &lt;FEATURE&gt;kind&lt;/FEATURE&gt;
                &lt;RANGE from=""-5"" to=""5""/&gt;
            &lt;/ATTRIBUTELIST&gt;

            &lt;ATTRIBUTELIST&gt;
                &lt;NAME&gt;Gaz&lt;/NAME&gt;
                &lt;SEMTYPE&gt;NOMINAL&lt;/SEMTYPE&gt;
                &lt;TYPE&gt;Lookup&lt;/TYPE&gt;
                &lt;FEATURE&gt;majorType&lt;/FEATURE&gt;
                &lt;RANGE from=""-5"" to=""5""/&gt;
            &lt;/ATTRIBUTELIST&gt;


            &lt;ATTRIBUTE&gt;
                &lt;NAME&gt;Class&lt;/NAME&gt;
                &lt;SEMTYPE&gt;NOMINAL&lt;/SEMTYPE&gt;
                &lt;TYPE&gt;Mention&lt;/TYPE&gt;
                &lt;FEATURE&gt;type&lt;/FEATURE&gt;
                &lt;POSITION&gt;0&lt;/POSITION&gt;
                &lt;CLASS/&gt;
            &lt;/ATTRIBUTE&gt;
        &lt;/DATASET&gt;
    &lt;/ML-CONFIG&gt;
</code></pre>
",Training and Model Evaluation,using gate batch learning pr identify part email gate batch leaning pr used reliably identify composed part email e part wa written sender excluding quoted previous conversation signature header date moment consider greeting dear garry closing kind regard john part composed text tried training small hand annotated set email configuration shown attribute tried various feature annotation type token spacetoken split date address lookup produced default annie tried together individually limited success recall precision someone recommend annotation type feature give good outcome could also use suggestion configuration parameter example instance type working batch learner still opaque instance find source explains attribute instance position converted machine learning feature vector wonder presence certain attribute relative position affect placement learned annotation every help appreciated thanks le interesting detail documentation clear use instance type option test show use annotation type attribute range annotation type selected instance type order able also include space line break learning slightly changed tokenizer rule annie create token annotation instead spacetoken annotation unfortunately really improve outcome batch learning config
"ISO Java implementation of NLP tasks: normalization, IBM Model 1 and Okapi BM25","<p>I've written a prototype in Python that uses the <code>NLTK</code> package to perform 3 NLP tasks:</p>

<ol>
<li>text normalization (split text into words, remove punctuation and other crud, convert words to base forms)</li>
<li>train and use IBM Translation Model 1</li>
<li>train and use Okapi BM25 model to evaluate relevance of queries</li>
</ol>

<p>I now need to port this into Java and am looking for existing implementations of the 3 tasks. </p>

<p>For the base form conversion subtask of #1, I would like to be able to supply a specialized dictionary to help better process text from a specialized domain that I am dealing with. But if that's not possible, using whatever default is fine too.</p>

<p>Performance is important. The python version is a prototype but the Java port has to work in production. The main requirement is scalability in terms of speed for large volumes of data. The prod machines have lots of RAM, so that's less of a concern.</p>

<p>Any recommendations? I can use <code>CoreNLP</code> or <code>OpenNLP</code> for #1 but what about #2 and 3? </p>
",Training and Model Evaluation,iso java implementation nlp task normalization ibm model okapi bm written prototype python us package perform nlp task text normalization split text word remove punctuation crud convert word base form train use ibm translation model train use okapi bm model evaluate relevance query need port java looking existing implementation task base form conversion subtask would like able supply specialized dictionary help better process text specialized domain dealing possible using whatever default fine performance important python version prototype java port ha work production main requirement scalability term speed large volume data prod machine lot ram le concern recommendation use
Precision and Recall computation for different group sizes,"<p>I didn't find an answer for this question anywhere, so I hope someone here could help me and also other people with the same problem.</p>

<p>Suppose that I have <strong><em>1000 Positive samples</em></strong> and <strong><em>1500 Negative samples</em></strong>.</p>

<p>Now, suppose that there are <strong><em>950 True Positives</em></strong> (positive samples correctly classified as positive) and <strong><em>100 False Positives</em></strong> (negative samples incorrectly classified as positive).</p>

<p>Should I use these raw numbers to compute the <strong><em>Precision</em></strong>, or should I consider the different group sizes?</p>

<p>In other words, should my precision be:</p>

<p><strong><em>TruePositive / (TruePositive + FalsePositive)</em></strong> = 950 / (950 + 100) = 90.476%</p>

<p><strong>OR</strong> should it be:</p>

<p><strong><em>(TruePositive / 1000) / [(TruePositive / 1000) + (FalsePositive / 1500)]</em></strong> = 0.95 / (0.95 + 0.067) = 93.44%</p>

<p>In the first calculation, I took the raw numbers without any consideration to the amount of samples in each group, while in the second calculation, I used the proportions of each measure to its corresponding group, to remove the bias caused by the groups' different size</p>
",Training and Model Evaluation,precision recall computation different group size find answer question anywhere hope someone could help also people problem suppose positive sample negative sample suppose true positive positive sample correctly classified positive false positive negative sample incorrectly classified positive use raw number compute precision consider different group size word precision truepositive truepositive falsepositive truepositive truepositive falsepositive first calculation took raw number without consideration amount sample group second calculation used proportion measure corresponding group remove bias caused group different size
How to extract the word features in trained model of MultinomialNB Pipeline in scikit-learn?,"<pre><code># Note: The runnable code example is at the end of this question ####
# Assume X_train contains cleaned sentence text as input data. Y_train are class labels. 
# parameters stores the parameter to be tried by GridSearchCV

text_clf_Pipline_MultinomialNB = Pipeline([('vect', CountVectorizer()),
                                           ('tfidf', TfidfTransformer()),
                                           ('clf', MultinomialNB()),                     
                                          ])
gs_clf = GridSearchCV(text_clf_Pipline_MultinomialNB, parameters, n_jobs=-1)   
gs_classifier = gs_clf.fit(X_train, y_train)
</code></pre>

<p>Now I can get the feature_log_prob_ from the gs_classifier based on <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"" rel=""nofollow"">sklearn.naive_bayes.MultinomialNB documentation</a>. Here is an example. </p>

<p>My question is how to get the word correspond to each log probability? 
The CountVectorizer() and TfidfTransformer() both did feature selection. 
Where do the GridSearchCV Object stores the selected word/ngram features? How to get match them back to the probabilities? </p>

<p>I have inspected the members of gs_classifier, while have not found the selected features. Thanks. </p>

<p>The following is a runnable example:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.grid_search import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from inspect import getmembers

X_train = ['qwe rtyuiop', 'asd fghj kl', 'zx cv bnm', 'qw erty ui op', 'as df ghj kl', 'zxc vb nm', 'qwe rt yu iop', 'asdfg hj kl', 'zx cvb nm',
          'qwe rt yui op', 'asd fghj kl', 'zx cvb nm', 'qwer tyui op', 'asd fg hjk l', 'zx cv b nm', 'qw ert yu iop', 'as df gh jkl', 'zx cvb nm',
           'qwe rty uiop', 'asd fghj kl', 'zx cvbnm', 'qw erty ui op', 'as df ghj kl', 'zxc vb nm', 'qwe rtyu iop', 'as dfg hj kl', 'zx cvb nm',
          'qwe rt yui op', 'asd fg hj kl', 'zx cvb nm', 'qwer tyuiop', 'asd fghjk l', 'zx cv b nm', 'qw ert yu iop', 'as df gh jkl', 'zx cvb nm']    

y_train = ['1', '2', '3', '1', '1', '3', '1', '2', '3',
          '1', '2', '3', '1', '4', '1', '2', '2', '4', 
          '1', '2', '3', '1', '1', '3', '1', '2', '3',
          '1', '2', '3', '1', '4', '1', '2', '2', '4']    


parameters = {  
                'clf__alpha': (1e-1, 1e-2),
                 'vect__ngram_range': [(1,2),(1,3)],
                 'vect__max_df': (0.9, 0.98)
            }

text_clf_Pipline_MultinomialNB = Pipeline([('vect', CountVectorizer()),
                                           ('tfidf', TfidfTransformer()),
                                           ('clf', MultinomialNB()),                     
                                          ])
gs_clf = GridSearchCV(text_clf_Pipline_MultinomialNB, parameters, n_jobs=-1)   

gs_classifier = gs_clf.fit(X_train, y_train)

nbclf = getmembers(gs_classifier.best_estimator_)[2][1]['named_steps']['clf']
nbclf.feature_log_prob_ 
</code></pre>

<p>Then the question are: How can I get the word feature list in trained model corresponds to the log probabilities? Also, for example, which probability in _log_prob_ output corresponds to the word 'qwe' for class '1'? </p>

<hr>

<p>Edit after get the answer:
Andreas's answer works:</p>

<pre><code>gs_classifier.best_estimator_.named_steps['vect'].get_feature_names() 
</code></pre>

<p>Similar to this, there is a better way to index into the GridSearchCV to get the trained classifier</p>

<pre><code>nbclf = gs_classifier.best_estimator_.named_steps['clf']
</code></pre>
",Training and Model Evaluation,extract word feature trained model multinomialnb pipeline scikit learn get feature log prob g classifier based sklearn naive bayes multinomialnb documentation example question get word correspond log probability countvectorizer tfidftransformer feature selection gridsearchcv object store selected word ngram feature get match back probability inspected member g classifier found selected feature thanks following runnable example question get word feature list trained model corresponds log probability also example probability log prob output corresponds word qwe class edit get answer andreas answer work similar better way index gridsearchcv get trained classifier
"Extracting &lt;subject, predicate, object&gt; triplet from unstructured text","<p>I need to extract simple triplets from unstructured text. Usually it is of the form noun- verb- noun, so I have tried POS tagging and then extracting nouns and verbs from neighbourhood.
However it leads to lot of cases and gives low accuracy.
Will Syntactic/semantic parsing help in this scenario?</p>

<p>Will ontology based information extraction be more useful?</p>
",Training and Model Evaluation,extracting subject predicate object triplet unstructured text need extract simple triplet unstructured text usually form noun verb noun tried po tagging extracting noun verb neighbourhood however lead lot case give low accuracy syntactic semantic parsing help scenario ontology based information extraction useful
Using language model in CMU sphnix4 1.0 beta6,"<p>I am a newbie in Java application development and I am trying to create a sample speech to text application for converting live speech. I tried to use Sphnix4-5prealpha and found it has issue with microphone (<a href=""http://sourceforge.net/p/cmusphinx/bugs/412/"" rel=""nofollow"">http://sourceforge.net/p/cmusphinx/bugs/412/</a>). So I switched back to 1.0 beta 6. I successfully ran the helloWorld and helloNgram programs. I am not sure helloNGram is the right one for me to start with, and even if this is the right one, I have very less idea how to proceed. I cannot find any way to move forward from helloNGram. Can any one please help me with the following two things:</p>

<ol>
<li>From which example should I start?</li>
<li>What will be the high level steps to achieve a generic English speech to text application with good accuracy.  </li>
</ol>
",Training and Model Evaluation,using language model cmu sphnix beta newbie java application development trying create sample speech text application converting live speech tried use sphnix prealpha found ha issue microphone switched back beta successfully ran helloworld hellongram program sure hellongram right one start even right one le idea proceed find way move forward hellongram one please help following two thing example start high level step achieve generic english speech text application good accuracy
Training a new Stanford part-of-speech tagger from within the NLTK,"<p>I've trained a part-of-speech tagger for an uncommon language (Uyghur) using the Stanford POS tagger and some self-collected training data. I've been using the NLTK's <code>nltk.tag.stanford.POSTagger</code> interface to tag individual sentences in Python. This works well for most of my purposes: (running from <code>/usr/share/stanford-postagger</code>)</p>

<pre><code>&gt;&gt;&gt; from nltk.tag.stanford import POSTagger
&gt;&gt;&gt; uy = POSTagger('models/uyghur.tagger', 'stanford-postagger.jar')
&gt;&gt;&gt; uy.tag('Men méning mantini yégenlikimni bilimen .'.split())
[[(u'Men', u'PN1s'), (u'm\xe9ning', u'PN1s.GEN'), (u'mantini', u'N-ACC'), (u'y\xe9genlikimni', u'Vt-PST.dir-1s2'), (u'bilimen', u'Vt-PRS-1s1'), (u'.', u'PUNCT')]]
</code></pre>

<p>I would like to do a ten-fold cross-validation to get a better sense of the accuracy of this tagger, i.e., use each tenth of my complete training data as test data for a tagger trained on the other nine-tenths of the data. Splitting the data set ten ways is no problem in Python, but I don't know if there's a way to train a new tagger from within Python. When I've done it, it's been from the command line using <code>java -mx1g -classpath /usr/share/stanford-postagger/stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -props uyghurtagger.props</code>.</p>

<p>Is it possible to train a new Stanford tagger from within Python using the NLTK interface, or will I need to create it manually via the command line each time?</p>
",Training and Model Evaluation,training new stanford part speech tagger within nltk trained part speech tagger uncommon language uyghur using stanford po tagger self collected training data using nltk interface tag individual sentence python work well purpose running would like ten fold cross validation get better sense accuracy tagger e use tenth complete training data test data tagger trained nine tenth data splitting data set ten way problem python know way train new tagger within python done command line using possible train new stanford tagger within python using nltk interface need create manually via command line time
Obtaining the complement of dictionary,"<p>I have a dictionary letters </p>

<pre><code>letterstoProbabilityMap={""aaa"":0.4,""bbb"":0.7,""ccc"":01}
</code></pre>

<p>for which I have three letter strings and their probability of occurring(I have shortened the dictionary).
I am assigning these probabilities based on some training data. But I also want to assign a probability to strings/keys I haven't seen. e.g ""aaa"".
Since all my keys are within the set aaa-zzz.
Is there a quick way for me to obtain the non assigned/complement and assign a value quickly. (I understand my question is quite abstract.) </p>

<p><strong>EDIT</strong>
The value is not fixed it is actually a la place probability.
Below is a code snippet I use to compute the probabilities I do know
The point is I reserve a probability mass which I will then assign to the three letter strings I haven't seen(because I know all strings are between aaa-zzz)</p>

<pre><code>for trigram in sorted(threeletter_counts.keys()):
        numerator=threeletter_counts[trigram]+1 
        denominator=twoletter_counts[trigram[:2]]+30
        prob=numerator/denominator
</code></pre>
",Training and Model Evaluation,obtaining complement dictionary dictionary letter three letter string probability occurring shortened dictionary assigning probability based training data also want assign probability string key seen e g aaa since key within set aaa zzz quick way obtain non assigned complement assign value quickly understand question quite abstract edit value fixed actually la place probability code snippet use compute probability know point reserve probability mass assign three letter string seen know string aaa zzz
"Making a Datum from a Data Set, Stanford NLP","<p>I was trying to run through examples for the Stanford NLP Classifier and had a question about classifying a new data set. I see that the "".test"" file contains the ""goldClass"" which is the right answer as well as the String which is supposed to be tested.</p>

<p>The example test set has the following format:</p>

<pre><code>&lt;label&gt; &lt;string&gt;
&lt;label&gt; &lt;String&gt;
...
....
</code></pre>

<p>This makes sense for evaluation of a model once we a model has been created from a hand classified data set. But now, once a model is created, how do I classify a completely new data set? I no longer have the associated Labels... I just have the new set of strings that I want to know the class for...</p>

<p>But to classify them, I will have to create a Datum object. To create a datum object, I will need to use makeDatumFromLine(), which requires a TSV line...  WHY does this have to be TSV? What is the use of specifying a goldClass when classifying new data?</p>

<p>I hope my question was clear.. </p>
",Training and Model Evaluation,making datum data set stanford nlp wa trying run example stanford nlp classifier question classifying new data set see test file contains goldclass right answer well string supposed tested example test set ha following format make sense evaluation model model ha created hand classified data set model created classify completely new data set longer associated label new set string want know class classify create datum object create datum object need use makedatumfromline requires tsv line doe tsv use specifying goldclass classifying new data hope question wa clear
Why does OpenNLP training tool require so much time in a (non-)multithreaded setup?,"<p>I have mined some training data from <code>Wikipedia/DBPedia</code> now (about 30MB of text).
I would like to train this model with opennlp but it takes like forever. My Mac has an i7 / 4 cores and it is running now for over 13h but still it is <code>computing the event counts</code>.
The problem could be, that it is not using <code>multithreading</code>.
Do you have any experience with that? How can i improve this process?</p>
",Training and Model Evaluation,doe opennlp training tool require much time non multithreaded setup mined training data mb text would like train model opennlp take like forever mac ha core running h still problem could using experience improve process
Is it possible to append words to an existing OpenNLP POS corpus/model?,"<p>Is there a way to train the existing Apache OpenNLP POS Tagger model? I need to add a few more proper nouns to the model that are specific to my application. When I try to use the below command:</p>

<pre><code>opennlp POSTaggerTrainer -type maxent -model en-pos-maxent.bin \
        -lang en -data en-pos.train -encoding UTF-8
</code></pre>

<p>the entire model is retrained. I'd only like to append a few new sentences to <code>en-pos-maxent.bin</code> </p>

<p>This is how my training file looks:</p>

<pre><code>Where_WRB is_VBZ the_DT Seven_DNNP Dwarfs_DNNP Mine_DNNP Train_DNNP ?_?
Where_WRB is_VBZ the_DT Astro_DNNP Orbiter_DNNP ?_?
Where_WRB is_VBZ the_DT Barnstormer_DNNP  ?_?
Where_WRB is_VBZ the_DT Big_DNNP Thunder_DNNP Mountain_DNNP Railroad_DNNP  ?_?
Where_WRB is_VBZ the_DT Buzz_DNNP Lightyears_DNNP Space_DNNP Ranger_DNNP Spin_DNNP  ?_?
Where_WRB is_VBZ the_DT Casey_DNNP Jr_DNNP Splash_DNNP N_DNNP Soak_DNNP Station_DNNP  ?_?
Where_WRB is_VBZ the_DT Cinderella_DNNP Castle_DNNP  ?_?
Where_WRB is_VBZ the_DT Country_DNNP Bear_DNNP Jamboree_DNNP  ?_?
Where_WRB is_VBZ the_DT Dumbo_DNNP the_DNNP Flying_DNNP Elephant_DNNP  ?_?
Where_WRB is_VBZ the_DT Enchanted_DNNP Tales_DNNP with_DNNP Belle_DNNP  ?_?
Where_WRB is_VBZ the_DT Frontierland_DNNP Shootin_DNNP Arcade_DNNP  ?_?
</code></pre>

<p>After training the model, all words except those in the training file are tagged as <code>DNNP</code>.
For example, if I ask for the word 'Where' (present in the training file) to be tagged, the answer is <code>WRB</code>, but if I ask the word 'hello' (not present in the training file) to be tagged, it is tagged as <code>DNNP</code>.  So I want to add a few words. How can I do that?</p>
",Training and Model Evaluation,possible append word existing opennlp po corpus model way train existing apache opennlp po tagger model need add proper noun model specific application try use command entire model retrained like append new sentence training file look training model word except training file tagged example ask word present training file tagged answer ask word hello present training file tagged tagged want add word
Classifier or heuristics?,"<p>I need to classify questions asking user to specify brand.
I has some set of samples featuring word ""brand"". </p>

<p><strong>Positives like:</strong> </p>

<ul>
<li><p>""What is your favorite cosmetic brand?"", </p></li>
<li><p>""Which fragrance brand (if any) do you think this advert is for?""... </p></li>
</ul>

<p><strong>and negatives like:</strong></p>

<ul>
<li>""Is there any particular reason why you chose this brand?""</li>
</ul>

<p>Of cause, it's possible to train 2-class classifier based on concrete samples. However precision and recall will be poor. Is there any way to construct something having good precision based on variety of positive samples?</p>
",Training and Model Evaluation,classifier heuristic need classify question asking user specify brand ha set sample featuring word brand positive like favorite cosmetic brand fragrance brand think advert negative like particular reason chose brand cause possible train class classifier based concrete sample however precision recall poor way construct something good precision based variety positive sample
Classifier or heuristics?,"<p>I need to classify questions asking user to specify brand.
I has some set of samples featuring word ""brand"". </p>

<p><strong>Positives like:</strong> </p>

<ul>
<li><p>""What is your favorite cosmetic brand?"", </p></li>
<li><p>""Which fragrance brand (if any) do you think this advert is for?""... </p></li>
</ul>

<p><strong>and negatives like:</strong></p>

<ul>
<li>""Is there any particular reason why you chose this brand?""</li>
</ul>

<p>Of cause, it's possible to train 2-class classifier based on concrete samples. However precision and recall will be poor. Is there any way to construct something having good precision based on variety of positive samples?</p>
",Training and Model Evaluation,classifier heuristic need classify question asking user specify brand ha set sample featuring word brand positive like favorite cosmetic brand fragrance brand think advert negative like particular reason chose brand cause possible train class classifier based concrete sample however precision recall poor way construct something good precision based variety positive sample
Using Dense Training Data for Prediction on Sparse Testing Data for SVD yields poor performance,"<p>I am implementing collaborative filtering(like netflix) using SVD and I am encountering an issue where my training data is very dense relative to the testing set. The algorithm returns no recommendations for most of my testing data. Any suggestions on how to fix this?</p>

<p>To make this more concrete the training data has long documents(wikipedia) but the testing data only has one or two non-zero entries (short phrases written by users).</p>
",Training and Model Evaluation,using dense training data prediction sparse testing data svd yield poor performance implementing collaborative filtering like netflix using svd encountering issue training data dense relative testing set algorithm return recommendation testing data suggestion fix make concrete training data ha long document wikipedia testing data ha one two non zero entry short phrase written user
"How to user Counter() to count the unigram, bigram, cooc and wordcount with a list traning_data?","<p>I want to know how to user Counter() to count the unigram, bigram, cooc and wordcount with a list traning_data.</p>

<p>I'm a python newcomer,please be patient to me.Thanks!</p>

<p>You need to implement two parts of the HMM postagger.</p>

<ol>
<li>A HMM model</li>
<li><p>viterbi decoding
This is the code:</p>

<pre><code>from collections import Counter
from math import log

class HMM(object):
    def __init__(self, epsilon=1e-5, training_data=None):
        self.epsilon = epsilon
        if training_data is not None:
            self.fit(training_data)
def fit(self, training_data):
'''
Counting the number of unigram, bigram, cooc and wordcount from the training
data.

Parameters
----------
training_data: list
    A list of training data, each element is a tuple with words and postags.
'''
self.unigram = Counter()    # The count of postag unigram, e.g. unigram['NN']=5
self.bigram = Counter()     # The count of postag bigram, e.g. bigram[('PRP', 'VV')]=1
self.cooc = Counter()       # The count of word, postag, e.g. cooc[('I', 'PRP')]=1
self.wordcount = Counter()  # The count of word, e.g. word['I']=1

print('building HMM model ...')
for words, tags in training_data:
    # Your code here! You need to implement the ngram counting part. Please count
    # - unigram
    # - bigram
    # - cooc
    # - wordcount

print('HMM model is built.')
self.postags = [k for k in self.unigram]
</code></pre></li>
</ol>

<p>This is the training_dataset and the expected result as below:</p>

<pre><code>    # The tiny example.
    training_dataset = [(['dog', 'chase', 'cat'], ['NN', 'VV', 'NN']),
                (['I', 'chase', 'dog'], ['PRP', 'VV', 'NN']),
                (['cat', 'chase', 'mouse'], ['NN', 'VV', 'NN'])
               ]

    hmm = HMM(training_data=training_dataset)

    # Testing if the parameter are correctly estimated.
    assert hmm.unigram['NN'] == 5
    assert hmm.bigram['VV', 'NN'] == 3
    assert hmm.bigram['NN', 'VV'] == 2
    assert hmm.cooc['dog', 'NN'] == 2
</code></pre>
",Training and Model Evaluation,user counter count unigram bigram cooc wordcount list traning data want know user counter count unigram bigram cooc wordcount list traning data python newcomer please patient thanks need implement two part hmm postagger hmm model viterbi decoding code training dataset expected result
Unable to make sense of how Theano works in RNN NLP for classification,"<pre><code>import os
import theano, numpy
from theano import tensor as T
from collections import OrderedDict

class RNNSLU(object):
 """""" Elman neural net""""""

 def __init__(self, nh, nc, ne, de, cs):
""""""
Hyperparameters used for initialization
nh : dimension of the hidden layer
nc : number of classes (labels)
ne : size of vocabulary
de : dimension of embedding
cs : word context window size
""""""
Parameter to be learnt : word embeddings
self.embeddings = theano.shared(name='embeddings',
    value = 0.2 * numpy.random.uniform(-1.0, 1.0, (ne + 1, de))
    .astype(theano.config.floatX))

# Parameter to be learnt : Weight matrix mapping input to the hidden layer (de*cs x nh)
self.wx = theano.shared(name='wx',
    value = 0.2 * numpy.random.uniform(-1.0, 1.0, (de * cs, nh))
    .astype(theano.config.floatX))

# Parameter to be learnt : Weight matrix mapping hidden layer from the
# previous time step to the current one
self.wh = theano.shared(name='wh',
    value = 0.2 * numpy.random.uniform(-1.0, 1.0, (nh, nh))
    .astype(theano.config.floatX))

# Parameter to be learnt : Weight matrix mapping hidden to output layer (nh x nc)
self.w = theano.shared(name='w',
    value = 0.2 * numpy.random.uniform(-1.0, 1.0, (nh, nc))
    .astype(theano.config.floatX))

# Parameter to be learnt : Bias at the hidden layer
self.bh = theano.shared(name='bh',
    value = numpy.zeros(nh,
      dtype=theano.config.floatX))

# Parameter to be learnt : The bias of the output layer
self.b = theano.shared(name='b',
    value = numpy.zeros(nc,
      dtype=theano.config.floatX))

# Parameter to be learnt : The hidden layer at time t=0
self.h0 = theano.shared(name='h0',
    value = numpy.zeros(nh,
      dtype=theano.config.floatX))

# Bundle the parameters
self.params = [self.embeddings, self.wx, self.wh, self.w, self.bh, self.b, self.h0]
self.names  = ['embeddings', 'Wx', 'Wh', 'W', 'bh', 'b', 'h0']

#Compile training function
self.prepare_train(de, cs)

def prepare_train(self, de, cs):
""""""
Trains the recurrent neural net
""""""
idxs = T.imatrix() # columns = no of words in window, rows = len of sentence
# Prepare to recieve input and output labels
x = self.embeddings[idxs].reshape((idxs.shape[0], de*cs))
y = T.iscalar('y')

 def recurrence(x_t, h_tm1):
  """"""
  x_t : Input at time t
  h_tm1 : Hidden state at time t-1
  """"""
  # Compute the hidden state at time time
  # h_t = g(x_t . w_x + h_tm1 . w_h + b_h)

  h_t = T.nnet.sigmoid(T.dot(x_t, self.wx) + T.dot(h_tm1, self.wh) + self.bh)
  # Compute the output layer
  # s_t = g(h_t . w + b)
  s_t = T.nnet.softmax(T.dot(h_t, self.w) + self.b)
  return [h_t, s_t]

[h,s], _ = theano.scan(fn=recurrence,
    sequences=x,
    outputs_info=[self.h0, None],
    n_steps=x.shape[0])

#print h.ndim
#print s.ndim

# TODO: What is the structure of s? What does the selection of axis do ?
p_y_given_sentence = s[:,0,:]
y_pred = T.argmax(p_y_given_sentence, axis=1)

# Learning rate
lr = T.scalar('lr')
# Sentence negative log-likelihood (The objective function)
sentence_nll = - T.mean(T.log(p_y_given_sentence)[T.arange(x.shape[0]), y])
# Compute paramter wise gradients
sentence_gradients = T.grad(sentence_nll, self.params)
# Compute updats
sentence_updates = OrderedDict((p, p - lr*g) for p,g in zip(self.params, sentence_gradients))

# Compile functions
self.classify = theano.function(inputs=[idxs], outputs=y_pred)
self.sentence_train = theano.function(inputs=[idxs, y, lr], outputs=sentence_nll, updates=sentence_updates)



#### Main Function from which we are calling class
rnn = RNNSLU(nh=s['nhidden'], nc=nClasses, ne=vocSize, de=s['emb_dimension'], cs=s['win'])

for word_batch, label_last_word in zip(words, labels):
  rnn.sentence_train(word_batch, label_last_word, s['clr'])
  rnn.normalize()
</code></pre>

<p>Explanation of code:</p>

<p>I know this will not be a good thing to do in stackoverflow. But I am struggling for more than a week to decode this code which is used to train  a Recurrent Neural Network. I am newbie to theano first of all.</p>

<p>word_batch = array([[ -1,  -1,  -1, 194, 358, 463, 208]], dtype=int32)
label_last_word = 126</p>

<p>Thw word_batch is an index for a sentence like the following:</p>

<p>'I am going to USA from England'</p>

<p>Here the word_batch is a context window associated with one particular word say USA. So, if the context windows is seven the middle ( 194 ) in the word batch represent the index of that word in the dataset. I want to know, when I am passing this as argument to rnn.sentence_train , how the training is happen inside the RNNSLU class. I am confused with the usage of variables like idx, x inside that class. I know how this happens in theory, but unable to decode the theano part explicitly. If my question doesn't make sense, please let me know.</p>

<p>Thanks.</p>
",Training and Model Evaluation,unable make sense theano work rnn nlp classification explanation code know good thing stackoverflow struggling week decode code used train recurrent neural network newbie theano first word batch array dtype int label last word thw word batch index sentence like following going usa england word batch context window associated one particular word say usa context window seven middle word batch represent index word dataset want know passing argument rnn sentence train training happen inside rnnslu class confused usage variable like idx x inside class know happens theory unable decode theano part explicitly question make sense please let know thanks
algorithm to extract simple sentences from complex(mixed) sentences?,"<p>Is there an algorithm that can be used to extract simple sentences from paragraphs?</p>

<p>My ultimate goal is to later run another algorithm on the resulted simple sentence to determine the author's sentiment.</p>

<p>I've researched this from sources such as Chae-Deug Park but none discuss preparing simple sentences as training data.</p>

<p>Thanks in advance</p>
",Training and Model Evaluation,algorithm extract simple sentence complex mixed sentence algorithm used extract simple sentence paragraph ultimate goal later run another algorithm resulted simple sentence determine author sentiment researched source chae deug park none discus preparing simple sentence training data thanks advance
OpenNLP: Training a custom NER Model for multiple entities,"<p>I am trying training a custom NER model for multiple entities. Here is the sample training data:</p>

<pre><code>count all &lt;START:item_type&gt; operating tables &lt;END&gt; on the &lt;START:location_id&gt; third &lt;END&gt; &lt;START:location_type&gt; floor &lt;END&gt;
count all &lt;START:item_type&gt; items &lt;END&gt; on the &lt;START:location_id&gt; third &lt;END&gt; &lt;START:location_type&gt; floor &lt;END&gt;
how many &lt;START:item_type&gt; beds &lt;END&gt; are in &lt;START:location_type&gt; room &lt;END&gt; &lt;START:location_id&gt; 2 &lt;END&gt;
</code></pre>

<p>The <code>NameFinderME.train(.)</code> method takes a string parameter <code>type</code>. What is the use of this parameter? And, how can I train a model for multiple entities (e.g. <code>item_type</code>, <code>location_type</code>, <code>location_id</code> in my case)</p>

<pre><code>public static void main(String[] args) {
    String trainingDataFile = ""/home/OpenNLPTest/lib/training_data.txt"";
    String outputModelFile = ""/tmp/model.bin"";
    String sentence = ""how many beds are in the hospital"";

    train(trainingDataFile, outputModelFile, ""location_type"");
    predict(sentence, outputModelFile);
}

private static void train(String trainingDataFile, String outputModelFile, String tagToFind) {
    File inFile = new File(trainingDataFile);
    NameSampleDataStream nss = null;
    try {
        nss = new NameSampleDataStream(new PlainTextByLineStream(new java.io.FileReader(inFile)));
    } catch (Exception e) {}

    TokenNameFinderModel model = null;
    int iterations = 100;
    int cutoff = 5;
    try {
        // Does the 'type' parameter mean the entity type that I am trying to train the model for?
        // What if I need to train for multiple entities?
        model = NameFinderME.train(""en"", tagToFind, nss, (AdaptiveFeatureGenerator) null, Collections.&lt;String,Object&gt;emptyMap(), iterations, cutoff); 
    } catch(Exception e) {}

    try {
        File outFile = new File(outputModelFile);           
        FileOutputStream outFileStream = new FileOutputStream(outFile);
        model.serialize(outFileStream);
    }
    catch (Exception ex) {}
}

private static void predict(String sentence, String modelFile) throws Exception {
    FileInputStream modelInToken = new FileInputStream(""/tmp/en-token.bin"");
    TokenizerModel modelToken = new TokenizerModel(modelInToken);
    Tokenizer tokenizer = new TokenizerME(modelToken); 
    String tokens[] = tokenizer.tokenize(sentence);

    FileInputStream modelIn = new FileInputStream(modelFile);

    TokenNameFinderModel model = new TokenNameFinderModel(modelIn);
    NameFinderME nameFinder = new NameFinderME(model);
    Span nameSpans[] = nameFinder.find(tokens);

    double[] spanProbs = nameFinder.probs(nameSpans);

    for( int i = 0; i&lt;nameSpans.length; i++) {
        System.out.println(nameSpans[i]);
    }
</code></pre>

<p>}</p>
",Training and Model Evaluation,opennlp training custom ner model multiple entity trying training custom ner model multiple entity sample training data method take string parameter use parameter train model multiple entity e g case
How to use crossValidate of stanford classifier?,"<p>I have downloaded stanford classifier and want to do crossvalidation. It has method crossValidate so I used this code : </p>

<pre><code>ColumnDataClassifier cdc = new ColumnDataClassifier(""examples/iris2007.prop"");
    Pair&lt;GeneralDataset&lt;String,String&gt; , List&lt;String[]&gt;&gt; cv = cdc.readTestExamples(""examples/iris.train"");
    Pair&lt;Double, Double&gt; result = cdc.crossValidate(cv.first(), cv.second());
</code></pre>

<p>but it report 0.00 for both accuracy and macro F . Am I missing something from my code to work fine? </p>
",Training and Model Evaluation,use crossvalidate stanford classifier downloaded stanford classifier want crossvalidation ha method crossvalidate used code report accuracy macro f missing something code work fine
Detect (predefined) topics in natural text,"<p>Is there a library or database out there that can detect the topics of natural text?</p>

<p>I'm not talking about generating topics from extracted keywords, but about analysing the used vocabulary and matching it with predefined topics. Like searching for words used in cooking or certain sports (like names of football clubs or technical terms).</p>

<p><strong>Update with clarification:</strong></p>

<p>Example text snippet: A sentence about football, then another sentence talking about catering at the event.</p>

<p>Library could assign categories ""sports"", ""football"", ""cooking"". </p>

<p>I'm looking for something that can assign these categories (or ""topics of interest"" maybe) without me having to train thousands of models with terabytes of manually classified documents. This could for example work by matching keywords instead of statistical analysis (that's why I mentioned database earlier).</p>

<p>I'm searching this because I don't have the manpower to build such a big database myself.</p>
",Training and Model Evaluation,detect predefined topic natural text library database detect topic natural text talking generating topic extracted keywords analysing used vocabulary matching predefined topic like searching word used cooking certain sport like name football club technical term update clarification example text snippet sentence football another sentence talking catering event library could assign category sport football cooking looking something assign category topic interest maybe without train thousand model terabyte manually classified document could example work matching keywords instead statistical analysis mentioned database earlier searching manpower build big database
How can the NamedEntityTag be used as EntityMention in RelationMention in the RelationExtractor?,"<p>I'm trying to train my own <code>NamedEntityRecognizer</code> and <code>RelationExtractor</code>. I've managed the <code>NER</code> model, but the integration with the <code>RelationExtractor</code> is a bit tricky. I get the right <code>NamedEntityTag</code>s, but the <code>RelationMention</code>s found by the are only one-term and with no extra <code>NamedEntity</code> than the default ones. I got input text: </p>

<blockquote>
  <p>America's President Nixon has passed a new law.</p>
</blockquote>

<p>and I got the following output:</p>

<pre><code>[Text=President CharacterOffsetBegin=10 CharacterOffsetEnd=19 PartOfSpeech=NNP Lemma=President NamedEntityTag=PRESIDENT]
[Text=Nixon CharacterOffsetBegin=20 CharacterOffsetEnd=25 PartOfSpeech=NNP Lemma=Nixon NamedEntityTag=PRESIDENT]

Extracted the following MachineReading relation mentions:
RelationMention [type=Live_In, start=0, end=4, {Live_In, 0.3719730539322602; OrgBased_In, 0.22490833335685645; _NR, 0.17474696244295865; Work_For, 0.11754788838692569; Located_In, 0.11082376188099895}
    EntityMention [type=O, objectId=EntityMention-2, hstart=3, hend=4, estart=2, eend=4, headPosition=3, value=""Nixon"", corefID=-1]
    EntityMention [type=LOCATION, objectId=EntityMention-1, hstart=0, hend=1, estart=0, eend=1, headPosition=0, value=""America"", corefID=-1]
]
</code></pre>
",Training and Model Evaluation,namedentitytag used entitymention relationmention relationextractor trying train managed model integration bit tricky get right found one term extra default one got input text america president nixon ha passed new law got following output
How to count number of words spoken using any method (SR or otherwise),"<p>I am having some trouble getting pointers to how to perform what appears to be a deceptively easy task:</p>

<p><strong>Given an audio stream, how do you count the number of words that have been spoken, in real-time?</strong></p>

<p>I don't need to recognize what the words are, but rather just have an accurate counter on words that have been uttered. The counter doesn't have to be too accurate and could even consider utterances and other ""grunts"" like coughs. </p>

<p>It appears that all Speech Recognition systems depend on a pre-defined grammar to be provided before they can analyze the phonemes that are spoken to convert to known words with some degree of accuracy. But I don't care about the accuracy at all, but rather the rate of words being spoken.</p>

<p>What is important is that this runs in real time, and allow the system to provide alerts after a certain number of words have been spoken. The system will encourage a visual cue to pause, and then the speaker can continue.</p>

<p>I've looked at CMU Sphinx FAQ and found that the idea of ""word spotting"" is not yet supported. I don't really need a real time search of particular words, but it approximates more closely to what I am looking for. Looking for very small silences in the waveform seems to be a very crude way of doing this and probably not very accurate at all, but that's all I have right now.</p>

<p>Any pointers on algorithms, research papers or any other insights would be appreciated!</p>
",Training and Model Evaluation,count number word spoken using method sr otherwise trouble getting pointer perform appears deceptively easy task given audio stream count number word spoken real time need recognize word rather accurate counter word uttered counter accurate could even consider utterance grunt like cough appears speech recognition system depend pre defined grammar provided analyze phoneme spoken convert known word degree accuracy care accuracy rather rate word spoken important run real time allow system provide alert certain number word spoken system encourage visual cue pause speaker continue looked cmu sphinx faq found idea word spotting yet supported really need real time search particular word approximates closely looking looking small silence waveform seems crude way probably accurate right pointer algorithm research paper insight would appreciated
NLTK/NLP buliding a many-to-many/multi-label subject classifier,"<p>I have a human tagged corpus of over 5000 subject indexed documents in XML. They vary in size from a few hundred kilobytes to a few hundred megabytes. Being short articles to manuscripts. They have all been subjected indexed as deep as the paragraph level. I am lucky to have such a corpus available, and I am trying to teach myself some NLP concepts. Admittedly, I've only begun. Thus far reading only the freely available NLTK book, <a href=""http://streamhacker.com"" rel=""nofollow"">streamhacker</a>, and skimming jacobs(?) NLTK cookbook. I like to experiment with some ideas.</p>

<p>It was suggested to me, that perhaps, I could take bi-grams and use naive Bayes classification to tag new documents. I feel as if this is the wrong approach. a Naive Bayes is proficient at a true/false sort of relationship, but to use it on my hierarchical tag set I would need to build a new classifier for each tag. Nearly a 1000 of them. I have the memory and processor power to undertake such a task, but am skeptical of the results. However, I will be trying this approach first, to appease someones request. I should likely have this accomplished in the next day or two, but I predict the accuracy to be low.</p>

<p>So my question is a bit open ended. Laregly becuase of the nature of the discipline and the general unfamilirity with my data it will likely be hard to give an exact answer. </p>

<ol>
<li><p>What sort of classifier would be appropriate for this task. Was I wrong can a Bayes be used for more than a true/false sort of operation.</p></li>
<li><p>what feature extraction should I pursue for such a task. I am not expecting much with the bigrams. </p></li>
</ol>

<p>Each document also contains some citational information including, author/s, an authors gender of m,f,mix(m&amp;f),and other (Gov't inst et al.), document type, published date(16th cent. to current), human analyst, and a few other general elements. I'd also appreciate some useful descriptive tasks to help investigate this data better for gender bias, analyst bias, etc. But realize that is a bit beyond the scope of this question. </p>
",Training and Model Evaluation,nltk nlp buliding many many multi label subject classifier human tagged corpus subject indexed document xml vary size hundred kilobyte hundred megabyte short article manuscript subjected indexed deep paragraph level lucky corpus available trying teach nlp concept admittedly begun thus far reading freely available nltk book streamhacker skimming jacob nltk cookbook like experiment idea wa suggested perhaps could take bi gram use naive bayes classification tag new document feel wrong approach naive bayes proficient true false sort relationship use hierarchical tag set would need build new classifier tag nearly memory processor power undertake task skeptical result however trying approach first appease someone request likely accomplished next day two predict accuracy low question bit open ended laregly becuase nature discipline general unfamilirity data likely hard give exact answer sort classifier would appropriate task wa wrong bayes used true false sort operation feature extraction pursue task expecting much bigram document also contains citational information including author author gender f mix f gov inst et al document type published date th cent current human analyst general element also appreciate useful descriptive task help investigate data better gender bias analyst bias etc realize bit beyond scope question
Word2Vec and Gensim parameters equivalence,"<p>Gensim is an optimized python port of Word2Vec (see <a href=""http://radimrehurek.com/2013/09/deep-learning-with-word2vec-and-gensim/"" rel=""nofollow"">http://radimrehurek.com/2013/09/deep-learning-with-word2vec-and-gensim/</a>)</p>

<p>I am currently using these vectors: <a href=""http://clic.cimec.unitn.it/composes/semantic-vectors.html"" rel=""nofollow"">http://clic.cimec.unitn.it/composes/semantic-vectors.html</a></p>

<p>I am going to rerun the model training with gensim because there was some noisy tokens in their models. So i would like to find out what are some equivalent parameters for <code>word2vec</code> in <code>gensim</code></p>

<p>And the parameters they used from <code>word2vec</code> are:</p>

<ul>
<li>2-word context window, PMI weighting, no compression, 300K dimensions</li>
</ul>

<p>What is the gensim equivalence when i train a Word2Vec model?</p>

<p>Is it:</p>

<pre><code>&gt;&gt;&gt; model = Word2Vec(sentences, size=300000, window=2, min_count=5, workers=4)
</code></pre>

<p><strong>Is there a PMI weight option in gensim?</strong></p>

<p><strong>What is the default min_count used in word2vec?</strong></p>

<p>There's another set of parameters from word2vec as such:</p>

<ul>
<li>5-word context window, 10 negative samples, subsampling, 400 dimensions.</li>
</ul>

<p><strong>Is there a negative samples parameter in gensim?</strong></p>

<p><strong>What is the parameter equivalence of subsampling in gensim?</strong></p>
",Training and Model Evaluation,word vec gensim parameter equivalence gensim optimized python port word vec see currently using vector going rerun model training gensim wa noisy token model would like find equivalent parameter parameter used word context window pmi weighting compression k dimension gensim equivalence train word vec model pmi weight option gensim default min count used word vec another set parameter word vec word context window negative sample subsampling dimension negative sample parameter gensim parameter equivalence subsampling gensim
Standard evaluation script for conll POS tagging,"<p>There is a standard perl script for chunking task for conll 2000. Is there any standard evalution script for POS tagging evaluation as well?
   Thank you!</p>
",Training and Model Evaluation,standard evaluation script conll po tagging standard perl script chunking task conll standard evalution script po tagging evaluation well thank
Standard evaluation script for conll POS tagging,"<p>There is a standard perl script for chunking task for conll 2000. Is there any standard evalution script for POS tagging evaluation as well?
   Thank you!</p>
",Training and Model Evaluation,standard evaluation script conll po tagging standard perl script chunking task conll standard evalution script po tagging evaluation well thank
automatic keyword generation evaluation,"<p>I have a simple text analyzer with generates keywords for a given input text. Until now I have been doing a manual evaluation of it, i.e., manually selecting keywords of a text and comparing them against the ones generated by the analyzer.</p>

<p>Is there any way in which I can automate this? I tried googling a lot for some free keyword generators which can help in this evaluation but have not found any till now. I will appreciate any suggestions on how to go about this.</p>
",Training and Model Evaluation,automatic keyword generation evaluation simple text analyzer generates keywords given input text manual evaluation e manually selecting keywords text comparing one generated analyzer way automate tried googling lot free keyword generator help evaluation found till appreciate suggestion go
Finding the probability with which an instance in classified in Weka,"<p>I am using Weka for classification using LibSVM classifier, and wanted some help related to the outputs that I get from the evaluation model. </p>

<p>In the below example, my test.arff file contains 1000 instances, and I want to know the probability with which each instance is classified as yes/ no (It's a simple two class problem). </p>

<p>For instance, for instance 1, if it is classified as 'yes', then with what probability is it classified so, is something which I am looking for. </p>

<p>Below is the code snippet that I have currently:</p>

<pre><code>            // Read and load the Training ARFF file 
        ArffLoader trainArffLoader = new ArffLoader();
        trainArffLoader.setFile(new File(""train_clusters.arff""));
        Instances train = trainArffLoader.getDataSet();
        train.setClassIndex(train.numAttributes() - 1);
        System.out.println(""Loaded Train File"");

        // Read and load the Test ARFF file 
        ArffLoader testArffLoader = new ArffLoader();
        testArffLoader.setFile(new File(""test_clusters.arff""));
        Instances test = testArffLoader.getDataSet();
        test.setClassIndex(test.numAttributes() - 1);
        System.out.println(""Loaded Test File"");


        LibSVM libsvm = new LibSVM();

        libsvm.buildClassifier(train);

        // Evaluation
        Evaluation evaluation = new Evaluation(train);
        evaluation.evaluateModel(libsvm, test);
        System.out.println(evaluation.toSummaryString(""\nPrinting the Results\n=====================\n"", true));
        System.out.println(evaluation.toClassDetailsString());
</code></pre>
",Training and Model Evaluation,finding probability instance classified weka using weka classification using libsvm classifier wanted help related output get evaluation model example test arff file contains instance want know probability instance classified yes simple two class problem instance instance classified yes probability classified something looking code snippet currently
Ngram model: Good-Turing Smoothing,"<p>I'm currently taking part of <a href=""http://www.kaggle.com/c/billion-word-imputation"" rel=""nofollow"">Kaggle's Billion Word Imputation</a> competition for an asignment at university. We are working with a simple 3-gram model. The thing is we had no choice but to ignore unigrams which ocurred 1,2,3 times, and bigrams which ocurred once on our training set, in order to save memory. So we forget about the words with frequencies 1,2,3; the bigrams that contained those words AND the bigrams with frequency 1; and also the 3grams that contained any of the previous deleted ngrams. </p>

<p>Now we've come to the point of finding where the missing word should go, and which word it is. The problem here is how do we calculate/estimate the probabilities of each ngram so that when when we find an unseen word or phrase the probability isn't 0? Well, before finding out that we wouldn't be able to handle the whole training set we thought of using Good-Turing Smoothing, which looked quite good and easy to implement.</p>

<p>For Good-Turing Smoothing we need the frequencies of each frequency, and now we don't have ANY  bigram  (w1, w2) with frequency 1, so what should we do? I thought of just storing that number for each w1, so we can use it in the calculations(*). I really don't know if this will make anything better or not...</p>

<p>On the other hand, regardless of the cut we did in our model, what if we had the following:</p>

<p>Freq  c ---> Freq of Freq Nc</p>

<p>1--->456</p>

<p>2--->123</p>

<p>3--->50</p>

<p>5--->23</p>

<p>To estimate the new c, c*, for the words with frequency c=3 we need both N4 and N2, but N4 is 0!!!</p>

<p>(*) so, although we wouldn't have any words w2 with c=1 stored, we would have N1 which is the most important to calculate the probability of unseen words.</p>

<p>Is there any simple way to solve this (specially this last part)? </p>
",Training and Model Evaluation,ngram model good turing smoothing currently taking part kaggle billion word imputation competition asignment university working simple gram model thing choice ignore unigrams ocurred time bigram ocurred training set order save memory forget word frequency bigram contained word bigram frequency also gram contained previous deleted ngrams come point finding missing word go word problem calculate estimate probability ngram find unseen word phrase probability well finding able handle whole training set thought using good turing smoothing looked quite good easy implement good turing smoothing need frequency frequency bigram w w frequency thought storing number w use calculation really know make anything better hand regardless cut model following freq c freq freq nc estimate new c c word frequency c need n n n although word w c stored would n important calculate probability unseen word simple way solve specially last part
How does OpenNLP calculate the false negatives when using the Sentence Detector Evaluation Tools?,"<p>I have found part of code of OpenNLP which shows me how to calculate the True Positive Value. I was wondering how does the false negative value been calculated when using OpenNLP Sentence Evaluation Tools. In another word, what does the Precision, Recall and F-Measure mean in terms of evaluation tool was used? Explain with examples would be wonderful.
(Sorry for my stupid question, i am totally new to NLP...)</p>

<pre><code>public static int countTruePositives(final Object[] references, final Object[] predictions) {



List&lt;Object&gt; predListSpans = new ArrayList&lt;Object&gt;(predictions.length);
Collections.addAll(predListSpans, predictions);
int truePositives = 0;
Object matchedItem = null;

for (int referenceIndex = 0; referenceIndex &lt; references.length; referenceIndex++) {
  Object referenceName = references[referenceIndex];

  for (int predIndex = 0; predIndex &lt; predListSpans.size(); predIndex++) {

    if (referenceName.equals(predListSpans.get(predIndex))) {
      matchedItem = predListSpans.get(predIndex);
      truePositives++;
    }
  }
  if (matchedItem != null) {
    predListSpans.remove(matchedItem);
  }
}
return truePositives;
</code></pre>
",Training and Model Evaluation,doe opennlp calculate false negative using sentence detector evaluation tool found part code opennlp show calculate true positive value wa wondering doe false negative value calculated using opennlp sentence evaluation tool another word doe precision recall f measure mean term evaluation tool wa used explain example would wonderful sorry stupid question totally new nlp
Weka - Naive Bayes always gives borderline results,"<p>I am trying to write a text classifier in Weka with Naive Bayes. I have a collection of Foursquare tips as training data with close to 500 of them marked as positive and approximately same marked as negative in an excel file. The input file has two columns with first one being the tip text and second one the marked polarity. I am using AFINN-111.txt to add an attribute to enhance the output. It calculates all the polar words in that tip and gives a final score of all the words. Here is my entire code:</p>

<pre><code>    public class DataReader {

    static Map&lt;String, Integer&gt; affinMap = new HashMap&lt;String, Integer&gt;();

    public List&lt;List&lt;Object&gt;&gt; createAttributeList() {
        ClassLoader classLoader = getClass().getClassLoader();
        initializeAFFINMap(classLoader);
        File inputWorkbook = new File(classLoader
                .getResource(""Tip_dataset2.xls"").getFile());
        Workbook w;
        Sheet sheet = null;
        try {
            w = Workbook.getWorkbook(inputWorkbook);
            // Get the first sheet
            sheet = w.getSheet(0);
        } catch (Exception e) {
            e.printStackTrace();
        }
        List&lt;List&lt;Object&gt;&gt; attributeList = new ArrayList&lt;List&lt;Object&gt;&gt;();
        for (int i = 1; i &lt; sheet.getRows(); i++) {
            String tip = sheet.getCell(0, i).getContents();

            tip = tip.replaceAll(""'"", """");
            tip = tip.replaceAll(""\"""", """");
            tip = tip.replaceAll(""%"", "" percent"");
            tip = tip.replaceAll(""@"", "" ATAUTHOR"");
            String polarity = getPolarity(sheet.getCell(1, i).getContents());
            int affinScore = 0;
            String[] arr = tip.split("" "");
            for (int j = 0; j &lt; arr.length; j++) {
                if (affinMap.containsKey(arr[j].toLowerCase())) {
                    affinScore = affinScore
                            + affinMap.get(arr[j].toLowerCase());
                }
            }
            List&lt;Object&gt; attrs = new ArrayList&lt;Object&gt;();
            attrs.add(tip);
            attrs.add(affinScore);
            attrs.add(polarity);

            attributeList.add(attrs);
        }
        return attributeList;
    }

    private String getPolarity(String cell) {
        if (cell.equalsIgnoreCase(""positive"")) {
            return ""positive"";
        } else {
            return ""negative"";
        }
    }

    private void initializeAFFINMap(ClassLoader classLoader) {
        try {
            InputStream stream = classLoader
                    .getResourceAsStream(""AFINN-111.txt"");
            DataInputStream in = new DataInputStream(stream);
            BufferedReader br = new BufferedReader(new InputStreamReader(in));
            String str;
            while ((str = br.readLine()) != null) {
                String[] array = str.split(""\t"");
                affinMap.put(array[0], Integer.parseInt(array[1]));
            }
            in.close();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args) throws Exception {
        List&lt;List&lt;Object&gt;&gt; attrList=new DataReader().createAttributeList();
        new CreateTrainedModel().createTrainingData(attrList);
    }

}
</code></pre>

<p>Here is the actual classifier class:</p>

<pre><code>public class CreateTrainedModel {

    public void createTrainingData(List&lt;List&lt;Object&gt;&gt; attrList)
            throws Exception {

        Attribute tip = new Attribute(""tip"", (FastVector) null);
        Attribute affin = new Attribute(""affinScore"");

        FastVector pol = new FastVector(2);
        pol.addElement(""positive"");
        pol.addElement(""negative"");
        Attribute polaritycl = new Attribute(""polarity"", pol);

        FastVector inputDataDesc = new FastVector(3);
        inputDataDesc.addElement(tip);
        inputDataDesc.addElement(affin);
        inputDataDesc.addElement(polaritycl);

        Instances dataSet = new Instances(""dataset"", inputDataDesc,
                attrList.size());
        // Set class index
        dataSet.setClassIndex(2);

        for (List&lt;Object&gt; onList : attrList) {
            Instance in = new Instance(3);
            in.setValue((Attribute) inputDataDesc.elementAt(0), onList.get(0)
                    .toString());
            in.setValue((Attribute) inputDataDesc.elementAt(1),
                    Integer.parseInt(onList.get(1).toString()));
            in.setValue((Attribute) inputDataDesc.elementAt(2), onList.get(2)
                    .toString());

            dataSet.add(in);
        }

        Filter f = new StringToWordVector();
        f.setInputFormat(dataSet);
        dataSet = Filter.useFilter(dataSet, f);

        Classifier model = (Classifier) new NaiveBayes();
        try {
            model.buildClassifier(dataSet);
        } catch (Exception e1) { // TODO Auto-generated catch block
            e1.printStackTrace();
        }

        ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(
                ""FS-TipsNaiveBayes.model""));
        oos.writeObject(model);
        oos.flush();
        oos.close();

        FastVector fvWekaAttributes1 = new FastVector(3);
        fvWekaAttributes1.addElement(tip);
        fvWekaAttributes1.addElement(affin);

        Instance in = new Instance(3);
        in.setValue((Attribute) fvWekaAttributes1.elementAt(0),
                ""burger here is good"");
        in.setValue((Attribute) fvWekaAttributes1.elementAt(1), 0);

        Instances testSet = new Instances(""dataset"", fvWekaAttributes1, 1);
        in.setDataset(testSet);

        double[] fDistribution = model.distributionForInstance(in);
        System.out.println(fDistribution);

    }

}
</code></pre>

<p>The problem I am facing is with any input the output distribution is always in the range of <code>[0.52314376998377, 0.47685623001622995]</code>. And it is always more towards the positive than the negative. These figures do not change drastically. Any idea what wrong am I doing? </p>
",Training and Model Evaluation,weka naive bayes always give borderline result trying write text classifier weka naive bayes collection foursquare tip training data close marked positive approximately marked negative excel file input file ha two column first one tip text second one marked polarity using afinn txt add attribute enhance output calculates polar word tip give final score word entire code actual classifier class problem facing input output distribution always range always towards positive negative figure change drastically idea wrong
How to pass in an estimator to NLTK&#39;s NgramModel?,"<p>I am using NLTK to train a bigram model using a Laplace estimator. The contructor for the NgramModel is:</p>

<pre><code>def __init__(self, n, train, pad_left=True, pad_right=False,
             estimator=None, *estimator_args, **estimator_kwargs):
</code></pre>

<p>After some research, I found that a syntax that works is the following:</p>

<pre><code>bigram_model = NgramModel(2, my_corpus, True, False, lambda f, b:LaplaceProbDist(f))
</code></pre>

<p>Although it seems to work correctly, I am confused about the last two arguments. Mainly, why is the 'estimator' argument a lambda function and how is interacting with the LaplaceProbDist? </p>
",Training and Model Evaluation,pas estimator nltk ngrammodel using nltk train bigram model using laplace estimator contructor ngrammodel research found syntax work following although seems work correctly confused last two argument mainly estimator argument lambda function interacting laplaceprobdist
Error using scipy.optimize.minimize / l-bfgs,"<p>I'm attempting to train a log-linear language model. In order to do that I need to maximize a vector parameter.
I'm using this Loss function: 
<a href=""https://i.sstatic.net/sEQ7t.png"" rel=""nofollow"">Loss function</a></p>

<p>This is my code: </p>

<pre><code>v0 = np.ones((len(tag_list), 1))

def first_argument(v, x, current_Tag):
    return np.dot(v, unigram_tag_feature_vector(current_Tag))

def second_argument(v, x, taglist):
    exp_ = 0
    for tag in taglist:
        exp_ += np.dot(v, unigram_tag_feature_vector(tag))
    return np.log(exp_)

def sum_func(ordered_text, taglist,  v):
    result = 0
    for (word, tag) in ordered_text:
        result += (-first_argument(v, 0, tag)- second_argument(v, 0, taglist))
    return result

def func(params, *args):
    ordered_text = args[0]
    taglist = args[1]
    v = params
    v_model= sum_func(ordered_text, taglist, v)
    print v_model
    return v

res = scipy.optimize.minimize(func, x0=v0, args=(ordered_text, tag_list))
</code></pre>

<p>And the error I get is: </p>

<pre><code>line 610, in approx_fprime
    grad[k] = (f(*((xk + d,) + args)) - f0) / d[k]
ValueError: setting an array element with a sequence.
</code></pre>

<p>The args are as follows:</p>

<pre><code>ordered_text=[('In', 'IN'), ('an', 'DT'), ('Oct.', 'NNP'), ('19', 'CD'), ....]
tag_list=['CC', 'CD', 'DT', 'EX', 'FW'....]
</code></pre>

<p>The function <code>unigram_tag_feature_vector</code> returns the following output:</p>

<pre><code> [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
</code></pre>

<p>(a sparse vector, for any input)</p>
",Training and Model Evaluation,error using scipy optimize minimize l bfgs attempting train log linear language model order need maximize vector parameter using loss function loss function code error get args follows function return following output sparse vector input
Algorithm to extract abbreviated word to its original word,"<p>I am building a program to do some text analysis.</p>

<p>I'm guessing that unpacking an abbreviated word to its original word will improve the accuracy of my analysis.
But I have no idea to implement it. I've google searched a little but can't find any article or paper discussing this. (Or maybe I just don't know the right keyword to search)</p>

<p>Basically what I need is: Given a word W, find a word with the highest probability to be the unabbreviated version of W from a dictionary (list of unabbreviated word). Optionally, I want the algorithm to be compatible with Indonesian language.</p>

<p>My question is somewhat similar to this SO question: <a href=""https://stackoverflow.com/questions/2304687/a-string-searching-algorithm-to-quickly-match-an-abbreviation-in-a-large-list-of"">A string searching algorithm to quickly match an abbreviation in a large list of unabbreviated strings?</a> , but that question hasn't been answered, despite being asked in 2010.</p>

<p>So, any idea? Thanks in advance!</p>
",Training and Model Evaluation,algorithm extract abbreviated word original word building program text analysis guessing unpacking abbreviated word original word improve accuracy analysis idea implement google searched little find article paper discussing maybe know right keyword search basically need given word w find word highest probability unabbreviated version w dictionary list unabbreviated word optionally want algorithm compatible indonesian language question somewhat similar question href string searching algorithm quickly match abbreviation large list unabbreviated string question answered despite asked idea thanks advance
KenLM perplexity weirdness,"<p>I have 96 files each containing ~10K lines of English text (tokenized, downcased).  If I loop through the files (essentially doing k-fold cross-validation with k=#files) and build a LM (using bin/lmplz) for 95 and run bin/query on the held out file against it, I see a PPL (including OOVs) of 1.0 every time.  But if I run a file against an LM built with all 96 files (so test doc is included in building the LM), I get a PPL of 27.8.</p>

<p>I have more experience with SRILM than KenLM, but I've never seen a perplexity score of 1.  Something feels wrong about that.  Even if I accepted that and attributed it to the test document's sentences occurring in other training data, it wouldn't explain why when I ensure that the test data is included in the training data I get a higher score.  What's going on? </p>

<p>=============================</p>

<p>this also seems strange:</p>

<pre><code>Perplexity including OOVs:  1
Perplexity excluding OOVs:  0.795685
OOVs:   0
</code></pre>
",Training and Model Evaluation,kenlm perplexity weirdness file containing k line english text tokenized downcased loop file essentially k fold cross validation k file build lm using bin lmplz run bin query held file see ppl including oovs every time run file lm built file test doc included building lm get ppl experience srilm kenlm never seen perplexity score something feel wrong even accepted attributed test document sentence occurring training data explain ensure test data included training data get higher score going also seems strange
What is the meaning of number behind every word in LDA model topic words?,"<p>When we train a model using LDA model we get an outcome of set of common topics which belong too LDA model. Each word in the topic have a number behind it. example</p>

<pre><code>topic - 0.004*great + 0.004*good + 0.004*like + 0.003*well + 0.003*best + 0.003*better 
</code></pre>

<p>What is the meaning of this number?</p>
",Training and Model Evaluation,meaning number behind every word lda model topic word train model using lda model get outcome set common topic belong lda model word topic number behind example meaning number
Topic Modeling Using Gensim in Python,"<p>I have a list of bag of words for two classes. Say <strong><em>n</em></strong> items in class <strong><em>A</em></strong> and <strong><em>m</em></strong> items in class <strong><em>B</em></strong>. I want to use the topic modeling with gensim package (for LDA) in python in order to train a model for class A vs class B. Meanwhile I am new to both <strong>Topic Modeling</strong> and <strong>Python</strong>. Does anyone know how should I do this? I mean, should I merge all the bags for each class and the use gensim or should I use bag for each item seperately? Thanks!</p>
",Training and Model Evaluation,topic modeling using gensim python list bag word two class say n item class item class b want use topic modeling gensim package lda python order train model class v class b meanwhile new topic modeling python doe anyone know mean merge bag class use gensim use bag item seperately thanks
News Articles Duplicate Detection,"<p>I want to perform near duplicate identification on crawled web news articles. ( I want to find articles on same news item and delete them) I have tried several generic methods such as simhash, shingles and clustering based approaches. but they did not produce reasonable amount of accuracy. Can anyone suggest a approach to do this?</p>
",Training and Model Evaluation,news article duplicate detection want perform near duplicate identification crawled web news article want find article news item delete tried several generic method simhash shingle clustering based approach produce reasonable amount accuracy anyone suggest approach
Text Feature Representation As Vectors for SVM,"<p>I am learning the Semantic Role Labeling (SRL) task. I have read a lot, and now I come to a problem for how to represent the text features as vectors. </p>

<p>For example, for the sentence: </p>

<p><code>We like StackOverflow very much</code></p>

<p>given the predicate verb: <code>like</code>, a few features are:</p>

<pre><code>the left 1st word: I
the right 1st word: StackOverflow
the POS tag of the left 1st word: Pronoun
The POS tag of the right 1st word: Adverbial
</code></pre>

<p><strong>What are the right ways to represent these features as vectors?</strong> </p>

<p>If possible, can you also give me some guidances for how to normalize these features please?</p>

<p>I basically want to train the data with these type of features using <code>SVM</code> models. </p>
",Training and Model Evaluation,text feature representation vector svm learning semantic role labeling srl task read lot come problem represent text feature vector example sentence given predicate verb feature right way represent feature vector possible also give guidance normalize feature please basically want train data type feature using model
Evaluating language identification methods,"<p>Part of my thesis work is to evaluate number of language detection methods that are already available and then finally implement one them. 
For this I have chosen the following methods,</p>

<ol>
<li>N-Gram-Based Text Categorization by Cavnar and Trenkle</li>
<li>Statistical Identification of Language by Ted Dunning</li>
<li>Using compression-based language models for text categorization by Teahan and Harper</li>
<li>Character Set Detection</li>
<li>A composite approach to language/encoding detection</li>
</ol>

<p>I have to first evaluate the methods and preferably present a table with accuracy for each of these methods. My question is that in order to find the accuracy of each of these methods, do I need to go ahead a build the language models using training data, then test them and record the accuracy or is there any other approach that I can follow here. Though most of the researches already include these accuracy tables, I am not sure if it's accepted in my education to simply grab it and present in the report.</p>

<p>Appreciate any thoughts on this. </p>
",Training and Model Evaluation,evaluating language identification method part thesis work evaluate number language detection method already available finally implement one chosen following method n gram based text categorization cavnar trenkle statistical identification language ted dunning using compression based language model text categorization teahan harper character set detection composite approach language encoding detection first evaluate method preferably present table accuracy method question order find accuracy method need go ahead build language model using training data test record accuracy approach follow though research already include accuracy table sure accepted education simply grab present report appreciate thought
Unable to update Open NLP model,"<p>I am using Apache OpenNLP for one of my projects.I am creating a new model to identify location since the pre-trained model (en-ner-location.bin) does not have this location.</p>

<p>Here is the code :</p>

<pre><code>package com.equinox.nlp;

import java.io.BufferedOutputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.util.Collections;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Map;

import opennlp.tools.namefind.NameFinderME;
import opennlp.tools.namefind.NameSample;
import opennlp.tools.namefind.NameSampleDataStream;
import opennlp.tools.namefind.TokenNameFinderModel;
import opennlp.tools.tokenize.SimpleTokenizer;
import opennlp.tools.tokenize.Tokenizer;
import opennlp.tools.util.InvalidFormatException;
import opennlp.tools.util.ObjectStream;
import opennlp.tools.util.PlainTextByLineStream;
import opennlp.tools.util.Span;

public class NlpTesting {
protected Map&lt;String, NameFinderME&gt; finders;
protected Tokenizer tokenizer;

public static void main(String[] args) throws InvalidFormatException,
        IOException {

    String bankura = ""In the 2011 census, Bankura municipality had a population of 138,036, out of which 70,734 were males and 67,302 were females."";
    String london = ""London is the capital city of England and the United Kingdom."";

    NlpTesting nlpTesting = new NlpTesting();
    NameFinderME nameFinderA = nlpTesting.createNameFinder(""./opennlp-models/en-ner-location.bin"");
    nlpTesting.findLocation(london, nameFinderA);
    System.out.println(""--------------------------"");
    nlpTesting.findLocation(bankura, nameFinderA);

    nlpTesting.train();

    NameFinderME nameFinderB = nlpTesting.createNameFinder(""./opennlp-models/en-ner-custom-location.bin"");

    nlpTesting.findLocation(bankura, nameFinderB);
}

public String findLocation(String str,NameFinderME nameFinder) throws InvalidFormatException,
        IOException {
    String commaSeparatedLocationNames = """";
    tokenizer = SimpleTokenizer.INSTANCE;

    String tokens[] = tokenizer.tokenize(str);
    Span nameSpans[] = nameFinder.find(tokens);
    HashSet&lt;String&gt; locationSet = new HashSet&lt;String&gt;();
    for (int i = 0; i &lt; nameSpans.length; i++) {
        locationSet.add(tokens[nameSpans[i].getStart()]);
    }
    for (Iterator&lt;String&gt; iterator = locationSet.iterator(); iterator
            .hasNext();) {
        String location = iterator.next();
        commaSeparatedLocationNames += location + "","";
    }
    System.out.println(commaSeparatedLocationNames);
    return commaSeparatedLocationNames;
}

public void train() throws IOException {
    File trainerFile = new File(""./train/train.txt"");
    File output = new File(""./opennlp-models/en-ner-custom-location.bin"");
    ObjectStream&lt;String&gt; lineStream = new PlainTextByLineStream(
            new FileInputStream(trainerFile), ""UTF-8"");
    ObjectStream&lt;NameSample&gt; sampleStream = new NameSampleDataStream(
            lineStream);
    System.out.println(""lineStream = "" + lineStream);
    TokenNameFinderModel model = NameFinderME.train(""en"", ""location"",
            sampleStream, Collections.&lt;String, Object&gt; emptyMap());
    BufferedOutputStream modelOut = null;
    try {
        modelOut = new BufferedOutputStream(new FileOutputStream(output));
        model.serialize(modelOut);
    } finally {
        if (modelOut != null)
            modelOut.close();
    }
}

public NameFinderME createNameFinder(String str) throws InvalidFormatException,
        FileNotFoundException, IOException {
    NameFinderME nameFinder = new NameFinderME(new TokenNameFinderModel(
            new FileInputStream(new File(str))));
    return nameFinder;
}
</code></pre>

<p>} </p>

<p>So far, it works fine.</p>

<p>The issue is I am unable to add another location to this custom model that I have created.
So, I went through the OpenNLP - README document.</p>

<p>There, it says, ""Note: In order to train a model you need all the training data. There is not currently a mechanism to update the models distributed with the project with additional data. ""</p>

<p>Does that mean I will not be able to update my custom models as well ? Is there any way to do this? It is quite possible that I may not have all the data while creating a model and an option to update the model should be there.Please help me.</p>
",Training and Model Evaluation,unable update open nlp model using apache opennlp one project creating new model identify location since pre trained model en ner location bin doe location code far work fine issue unable add another location custom model created went opennlp readme document say note order train model need training data currently mechanism update model distributed project additional data doe mean able update custom model well way quite possible may data creating model option update model please help
Matching an element from a set of abstracts to an element in set of titles,"<p>Suppose I have two sets, </p>

<pre><code>a = {""this is a title"", ...}
b = {""this is a short description of some title from a"", ...}
</code></pre>

<p>What is the best way to find the best match in set b for an element in set a, or vice versa. The approach I tried was to create a tf-idf bag of words vector space using the tokens of b, and then finding the cosine similarity. For given a, the pair (a,b) was selected if the cosine similarity was higher than any other element b. But it is not very accurate.</p>

<p>Are there any better methods to do this? How can I improve the accuracy?</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
# titles and abstracts are arrays of strings
tfidf = TfidfVectorizer(stop_words='english', analyzer='word')

vec = tfidf.fit_transform(abstracts)

def predict(title):
    titlevec = tfidf.transform([title])
    sim = cosine_similarity(titlevec,vec)
    return np.argmax(sim)

for i, title in titles:
    index = predict(title)
    print ""Title: {0}\nAbstracts:{1}"".format(title,abstracts[index])
</code></pre>
",Training and Model Evaluation,matching element set abstract element set title suppose two set best way find best match set b element set vice versa approach tried wa create tf idf bag word vector space using token b finding cosine similarity given pair b wa selected cosine similarity wa higher element b accurate better method improve accuracy
A Viable Solution for Word Splitting Khmer?,"<p>I am working on a solution to split long lines of Khmer (the Cambodian language) into individual words (in UTF-8).  Khmer does not use spaces between words.  There are a few solutions out there, but they are far from adequate (<a href=""http://sourceforge.net/projects/khmer/files/Khmer%20Word%20Breaking/"" rel=""nofollow noreferrer"">here</a> and <a href=""http://www.panl10n.net/english/Outputs%20Phase%202/CCs/Cambodia/MoEYS/Software/2008/Windows/KhmerLineBreaking.zip"" rel=""nofollow noreferrer"">here</a>), and those projects have fallen by the wayside.</p>

<p>Here is a sample line of Khmer that needs to be split (they can be longer than this):</p>

<blockquote>
  <p>ចូរសរសើរដល់ទ្រង់ដែលទ្រង់បានប្រទានការទាំងអស់នោះមកដល់រូបអ្នកដោយព្រោះអង្គព្រះយេស៊ូវ ហើយដែលអ្នកមិនអាចរកការទាំងអស់នោះដោយសារការប្រព្រឹត្តរបស់អ្នកឡើយ។</p>
</blockquote>

<p>The goal of creating a viable solution that splits Khmer words is twofold: it will encourage those who used Khmer legacy (non-Unicode) fonts to convert over to Unicode (which has many benefits), and it will enable legacy Khmer fonts to be imported into Unicode to be used with a spelling checker quickly (rather than manually going through and splitting words which, with a large document, can take a very long time).</p>

<p>I don't need 100% accuracy, but speed is important (especially since the line that needs to be split into Khmer words can be quite long).
I am open to suggestions, but currently I have a large corpus of Khmer words that are correctly split (with a non-breaking space), and I have created a word probability dictionary file (frequency.csv) to use as a dictionary for the word splitter.</p>

<p>I found this python code <a href=""https://stackoverflow.com/questions/195010/how-can-i-split-multiple-joined-words"">here</a> that uses the <a href=""http://en.wikipedia.org/wiki/Viterbi_algorithm"" rel=""nofollow noreferrer"">Viterbi algorithm</a> and it supposedly runs fast.  </p>

<pre><code>import re
from itertools import groupby

def viterbi_segment(text):
    probs, lasts = [1.0], [0]
    for i in range(1, len(text) + 1):
        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)
                        for j in range(max(0, i - max_word_length), i))
        probs.append(prob_k)
        lasts.append(k)
    words = []
    i = len(text)
    while 0 &lt; i:
        words.append(text[lasts[i]:i])
        i = lasts[i]
    words.reverse()
    return words, probs[-1]

def word_prob(word): return dictionary.get(word, 0) / total
def words(text): return re.findall('[a-z]+', text.lower()) 
dictionary = dict((w, len(list(ws)))
                  for w, ws in groupby(sorted(words(open('big.txt').read()))))
max_word_length = max(map(len, dictionary))
total = float(sum(dictionary.values()))
</code></pre>

<p>I also tried using the source java code from the author of this page: <a href=""https://stackoverflow.com/questions/4580877/text-segmentation-dictionary-based-word-splitting"">Text segmentation: dictionary-based word splitting</a> but it ran too slow to be of any use (because my word probability dictionary has over 100k terms...).</p>

<p>And here is another option in python from <a href=""https://stackoverflow.com/questions/2174093/python-word-splitting"">Detect most likely words from text without spaces / combined words</a>:</p>

<pre><code>WORD_FREQUENCIES = {
    'file': 0.00123,
    'files': 0.00124,
    'save': 0.002,
    'ave': 0.00001,
    'as': 0.00555
}

def split_text(text, word_frequencies, cache):
    if text in cache:
        return cache[text]
    if not text:
        return 1, []
    best_freq, best_split = 0, []
    for i in xrange(1, len(text) + 1):
        word, remainder = text[:i], text[i:]
        freq = word_frequencies.get(word, None)
        if freq:
            remainder_freq, remainder = split_text(
                    remainder, word_frequencies, cache)
            freq *= remainder_freq
            if freq &gt; best_freq:
                best_freq = freq
                best_split = [word] + remainder
    cache[text] = (best_freq, best_split)
    return cache[text]

print split_text('filesaveas', WORD_FREQUENCIES, {})

--&gt; (1.3653e-08, ['file', 'save', 'as'])
</code></pre>

<p>I am a newbee when it comes to python and I am really new to all real programming (outside of websites), so please bear with me.  Does anyone have any options that they feel would work well?</p>
",Training and Model Evaluation,viable solution word splitting khmer working solution split long line khmer cambodian language individual word utf khmer doe use space word solution far adequate project fallen wayside sample line khmer need split longer goal creating viable solution split khmer word encourage used khmer legacy non unicode font convert unicode ha many benefit enable legacy khmer font imported unicode used spelling checker quickly rather manually going splitting word large document take long time need accuracy speed important especially since line need split khmer word quite long open suggestion currently large corpus khmer word correctly split non breaking space created word probability dictionary file frequency csv use dictionary word splitter found python code viterbi algorithm supposedly run fast also tried using source java code author page newbee come python really new real programming outside website please bear doe anyone option feel would work well
Given 5 input words predict the &quot;most associated&quot; word,"<p>I have to solve this task for a NLP homework. The task is as general as I described it in the title. A set of 2000 examples, with the corresponding expected output, is provided and they look like:</p>

<pre><code>absence ~ away fonder illness leave presence
absent ~ away minded gone present ill
absurdity ~ stupid ridiculous mad stupidity clown
accents ~ dialects language foreign speech French
accordion ~ music piano play player instrument
</code></pre>

<p>I already solved the task using distributional semantics with a decent accuracy over this set, but the problem is that I have an additional constraint, that is: <strong>the size of the archive I deliver must be less than 50 MB</strong> (as far as I'm concerned this constraint is totally nonsense, but still I have to comply). Any distributional semantics approach will therefore not work, because the semantic space has to be built over a lot of data (thousands of Wikipedia pages, in my case) and its size can't be reduced so much to fit into 50 MB.</p>

<p>Can you suggest any other approaches that I can use to tackle this problem?</p>
",Training and Model Evaluation,given input word predict associated word solve task nlp homework task general described title set example corresponding expected output provided look like already solved task using distributional semantics decent accuracy set problem additional constraint size archive deliver must le mb far concerned constraint totally nonsense still comply distributional semantics approach therefore work semantic space ha built lot data thousand wikipedia page case size reduced much fit mb suggest approach use tackle problem
What is the accuracy of nltk pos_tagger?,"<p>I'm writing a dissertation, and using nltk.pos_tagger in my work. I can't find any information about what the accuracy of this algorithm. Does anybody know where can I find such information?</p>
",Training and Model Evaluation,accuracy nltk po tagger writing dissertation using nltk po tagger work find information accuracy algorithm doe anybody know find information
Ontological non-taxonomic relationship - Gold Standard evaluation ? - Genia ontology,"<p>I am working on extraction of non-taxonomic relationships between concepts in domain ontology learning. There is not exist a corpus and corresponding ontology for evaluation propose. In previews related works, in some work, use of some Gold standards such Genia, Tourism (loneyPlanet), Family law, ... ; but not exist this domain corpus and ontology on the web. Just exist Genia domain that is without reference non-taxonomic relationships. Other reference ontology such Cancer Chemoprevention (CANCO) are without having a corpus for learning. </p>

<p>There is exist some corpus and ontology for Gold standard evaluation of this ontological relationships?</p>
",Training and Model Evaluation,ontological non taxonomic relationship gold standard evaluation genia ontology working extraction non taxonomic relationship concept domain ontology learning exist corpus corresponding ontology evaluation propose preview related work work use gold standard genia tourism loneyplanet family law exist domain corpus ontology web exist genia domain without reference non taxonomic relationship reference ontology cancer chemoprevention canco without corpus learning exist corpus ontology gold standard evaluation ontological relationship
libSVM giving highly inaccurate predictions even for the file that was used to train it,"<p>here is the deal.
I am trying to make an SVM based POS tagger.
The feature vectors for the SVM was created with the help of format converters. 
Now here is a screenshot of the training file that I am using.
<a href=""http://tinypic.com/r/n4fn2r/8"" rel=""nofollow"">http://tinypic.com/r/n4fn2r/8</a></p>

<p>I have 25 labels for various POS tags. when i use the java implementation or the command line tools for prediction i get the following results.
<a href=""http://tinypic.com/r/2dtw5ky/8"" rel=""nofollow"">http://tinypic.com/r/2dtw5ky/8</a></p>

<p>I have tried with all the kernels available but it gave more or less the same results.
This is happening even when the training file is used as the testing file.
please help me out here..!! 
P.S. I cannot share more than two links. Thus here is a snippet of the model file</p>

<pre><code>    svm_type c_svc
    kernel_type rbf
gamma 0.000548546
nr_class 25
total_sv 431
rho -0.929467 1.01073 1.0531 1.03472 1.01585 0.953263 1.03027 -0.921365 0.984535 1.02796 1.01266 1.03374 0.949463 0.977925 0.986551 -0.920912 0.940926 -0.955562 0.975386 -0.981959 -0.884042 0.0516955 -0.980884 -0.966095 0.995091 1.023 1.01489 1.00308 0.948314 1.01137 -0.845876 0.968034 1.0076 1.00064 1.01335 0.942633 0.965703 0.979212 -0.861236 0.935055 -0.91739 0.970223 -0.97103 0.0743777 0.970321 -0.971215 -0.931582 0.972377 0.958193 0.931253 0.825797 0.954894 -0.972884 -0.941726 0.945077 0.922366 0.953999 -1.00503 0.840985 0.882229 -0.961742 0.791631 -0.984971 0.855911 -0.991528 -0.951211 -0.962096 -0.99213 -0.99708 -0.957557 -0.308987 -0.455442 -0.94881 -0.995319 -0.974945 -0.964637 -0.902152 -0.955258 -1.05287 -1.00614 -0.
</code></pre>

<p><em>update</em>
Just trained the SVM with svm type as c-SVC and kernel type as linear. Which gave a non-zero(although very poor) accuracy. </p>
",Training and Model Evaluation,libsvm giving highly inaccurate prediction even file wa used train deal trying make svm based po tagger feature vector svm wa created help format converter screenshot training file using label various po tag use java implementation command line tool prediction get following result tried kernel available gave le result happening even training file used testing file please help p share two link thus snippet model file update trained svm svm type c svc kernel type linear gave non zero although poor accuracy
Freebase evaluating which fields can be right for a popularity sort heuristic,"<p>I'm trying to fetch subsets of freebase. For a couple of types, I mostly want the most popular entities. For example  when I'm trying to fetch the most popular movies, I want the top 2k most popular movies. I see there is no popularity rank, but whenever I'm trying to limit/sort by things like estimated_budget or gross_revenue I get a very small result set. Is there any way to evaluate which fields might be actually relevant and not just a field that is used in a very low percentage of results? </p>

<p>An example of a simple movies query:</p>

<pre><code>[{
  ""type"": ""/film/film"",
  ""limit"": 10,
  ""name"": null,
  ""id"": null
}]
</code></pre>

<p>Question is which <code>""sort""</code> option should I consider, to make it sort by some sort of popularity heuristic?</p>
",Training and Model Evaluation,freebase evaluating field right popularity sort heuristic trying fetch subset freebase couple type mostly want popular entity example trying fetch popular movie want top k popular movie see popularity rank whenever trying limit sort thing like estimated budget gross revenue get small result set way evaluate field might actually relevant field used low percentage result example simple movie query question option consider make sort sort popularity heuristic
Using NLP for extracting domain-specific data from unstructured text,"<p>I'm looking for a way to automatically extract domain-specific knowledge from unstructured text in Java. We would have a manually annotated training set at our disposal that contains the following:</p>

<p>Text: The apartment contains 2 bedrooms and one bathroom.
Structured
- Type: apartment
- Bedrooms: 2
- Bathrooms: 1</p>

<p>Any idea what's the best way forward to train a model that would be able to perform this job? Possibly a POS-tagger that we extend with custom tags?</p>

<p>Thanks!</p>
",Training and Model Evaluation,using nlp extracting domain specific data unstructured text looking way automatically extract domain specific knowledge unstructured text java would manually annotated training set disposal contains following text apartment contains bedroom one bathroom structured type apartment bedroom bathroom idea best way forward train model would able perform job possibly po tagger extend custom tag thanks
Human name comparison: ways to approach this task,"<p>I'm not a Natural Language Programming student, yet I know it's not trivial strcmp(n1,n2). </p>

<p>Here's what i've learned so far:</p>

<ul>
<li>comparing Personal Names can't be solved 100%</li>
<li>there are ways to achieve certain degree of accuracy. </li>
<li>the answer will be locale-specific, that's OK. </li>
</ul>

<p>I'm not looking for spelling alternatives! The assumption is that the input's spelling is correct.</p>

<p>For example, all the names below can refer to the same person:</p>

<ul>
<li>Berry Tsakala</li>
<li>Bernard Tsakala</li>
<li>Berry J. Tsakala</li>
<li>Tsakala, Berry</li>
</ul>

<p>I'm trying to:</p>

<ol>
<li>build (or copy) an algorithm which grades the relationship 2 input names</li>
<li>find an indexing method (for names in my database, for hash tables, etc.)</li>
</ol>

<p>note:
My task isn't about finding names in text, but to compare 2 names. e.g. </p>

<pre><code>name_compare( ""James Brown"", ""Brown, James"", ""en-US"" ) ---&gt; 99.0%
</code></pre>
",Training and Model Evaluation,human name comparison way approach task natural language programming student yet know trivial strcmp n n learned far comparing personal name solved way achieve certain degree accuracy answer locale specific ok looking spelling alternative assumption input spelling correct example name refer person berry tsakala bernard tsakala berry j tsakala tsakala berry trying build copy algorithm grade relationship input name find indexing method name database hash table etc note task finding name text compare name e g
Are high values for c or gamma problematic when using an RBF kernel SVM?,"<p>I'm using WEKA/LibSVM to train a classifier for a term extraction system. My data is not linearly separable, so I used an RBF kernel instead of a linear one.<br>
I followed the <a href=""http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf"" rel=""nofollow"">guide from Hsu et al.</a> and iterated over several values for both c and gamma. The parameters which worked best for classifying known terms (test and training material differ of course) are rather high, c=2^10 and gamma=2^3.<br>
So far the high parameters seem to work ok, yet I wonder if they may cause any problems further on, especially regarding overfitting. I plan to do another evaluation by extracting new terms, yet those are costly as I need human judges.<br>
Could anything still be wrong with my parameters, even if both evaluation turns out positive? Do I perhaps need another kernel type?</p>

<p>Thank you very much!</p>
",Training and Model Evaluation,high value c gamma problematic using rbf kernel svm using weka libsvm train classifier term extraction system data linearly separable used rbf kernel instead linear one followed guide hsu et al iterated several value c gamma parameter worked best classifying known term test training material differ course rather high c gamma far high parameter seem work ok yet wonder may cause problem especially regarding overfitting plan another evaluation extracting new term yet costly need human judge could anything still wrong parameter even evaluation turn positive perhaps need another kernel type thank much
Is there a way to categorize technical words by their primary domain of usage,"<p>I have a very comprehensive list of technical words that I need to sort into broad categories like physics, chemistry, medicine, etc (I realize that these categories might overlap, which is ok). The words are very obscure and so no listed in many dictionaries (i.e. ""microcrith""). I'm at a loss as to how to proceed.</p>
",Training and Model Evaluation,way categorize technical word primary domain usage comprehensive list technical word need sort broad category like physic chemistry medicine etc realize category might overlap ok word obscure listed many dictionary e microcrith loss proceed
spell checker uses language model,"<p>I look for spell checker that could use language model.</p>

<p>I know there is a lot of good spell checkers such as <a href=""http://hunspell.sourceforge.net/"" rel=""noreferrer"">Hunspell</a>, however as I see it doesn't relate to context, so it only token-based spell checker.</p>

<p>for example,</p>

<p><code>I lick eating banana</code></p>

<p>so here at token-based level no misspellings at all, all words are correct, but there is no meaning in the sentence. However ""smart"" spell checker would recognize that ""lick"" is actually correctly written word, but may be the author meant ""like"" and then there is a meaning in the sentence.</p>

<p>I have a bunch of correctly written sentences in the specific domain, I want to train ""smart"" spell checker to recognize misspelling and to learn language model, such that it would recognize that even thought ""lick"" is written correctly, however the author meant ""like"".</p>

<p>I don't see that Hunspell has such feature, can you suggest any other spell checker, that could do so.</p>
",Training and Model Evaluation,spell checker us language model look spell checker could use language model know lot good spell checker hunspell however see relate context token based spell checker example token based level misspelling word correct meaning sentence however smart spell checker would recognize lick actually correctly written word may author meant like meaning sentence bunch correctly written sentence specific domain want train smart spell checker recognize misspelling learn language model would recognize even thought lick written correctly however author meant like see hunspell ha feature suggest spell checker could
Why can we use entropy to measure the quality of language model?,"<p>I am reading the &lt; <a href=""http://nlp.stanford.edu/fsnlp/"" rel=""nofollow"">Foundations of Statistical Natural Language Processing</a> >. It has the following statement about the relationship between information entropy and language model:</p>

<blockquote>
  <p>...The essential point here is that if a model captures more of the
  structure of a language, then the entropy of the model should be
  lower. In other words, we can sue entropy as a measure of the quality
  of our models...</p>
</blockquote>

<p>But how about this example:</p>

<p>Suppose we have a machine that spit 2 characters, A and B, one by one. And the designer of the machine makes A and B has the equal probability.</p>

<p>I am not the designer. And I try to model it through experiment.</p>

<p>During a initial experiment, I see the machine split the following character sequence:</p>

<blockquote>
  <p>A, B, A</p>
</blockquote>

<p>So I model the machine as P(A)=2/3 and P(B)=1/3. And we can calculate entropy of this model as :</p>

<pre><code>-2/3*Log(2/3)-1/3*Log(1/3)= 0.918 bit  (the base is 2)
</code></pre>

<p>But then, the designer tell me about his design, so I refined my model with this more information. The new model looks like this:</p>

<p>P(A)=1/2 P(B)=1/2</p>

<p>And the entropy of this new model is:</p>

<pre><code>-1/2*Log(1/2)-1/2*Log(1/2) = 1 bit
</code></pre>

<p>The second model is obviously better than the first one. But the entropy increased.</p>

<p>My point is, due to the arbitrariness of the model being tried, we cannot blindly say a smaller entropy indicates a better model.</p>

<p>Could anyone shed some light on this?</p>

<h2>ADD 1</h2>

<p>(Much thanks to Rob Neuhaus!)</p>

<p>Yes, after I re-digested the mentioned NLP book. I think I can explain it now.</p>

<p>What I calculated is actually the entropy of the language model distribution. It cannot be used to evaluate the effectiveness of a language model.</p>

<p>To evaluate a language model, we should measure how much <strong><em>surprise</em></strong> it gives us for real sequences in that language. For each real word encountered, the language model will give a probability <strong>p</strong>. And we use <strong>-log(p)</strong> to quantify the surprise. And we average the total surprise over a long enough sequence. So, in case of a 1000-letter sequence with 500 A and 500 B,
the surprise given by the 1/3-2/3 model will be:</p>

<p>[-500*log(1/3) - 500*log(2/3)]/1000 = 1/2 * Log(9/2)</p>

<p>While the correct 1/2-1/2 model will give:</p>

<p>[-500*log(1/2) - 500*log(1/2)]/1000 = 1/2 * Log(8/2)</p>

<p><strong>So, we can see, the 1/3, 2/3 model gives more surprise, which indicates it is worse than the correct model.</strong></p>

<p>Only when the sequence is long enough, the average effect will mimic the expectation over the 1/2-1/2 distribution. If the sequence is short, it won't give a convincing result.</p>

<p>I didn't mention the <strong>cross-entropy</strong> here since I think this jargon is too intimidating and not much helpful to reveal the root cause.</p>
",Training and Model Evaluation,use entropy measure quality language model reading foundation statistical natural language processing ha following statement relationship information entropy language model essential point model capture structure language entropy model lower word sue entropy measure quality model example suppose machine spit character b one one designer machine make b ha equal probability designer try model experiment initial experiment see machine split following character sequence b model machine p p b calculate entropy model designer tell design refined model information new model look like p p b entropy new model second model obviously better first one entropy increased point due arbitrariness model tried blindly say smaller entropy indicates better model could anyone shed light add much thanks rob neuhaus yes digested mentioned nlp book think explain calculated actually entropy language model distribution used evaluate effectiveness language model evaluate language model measure much surprise give u real sequence language real word encountered language model give probability p use log p quantify surprise average total surprise long enough sequence case letter sequence b surprise given model log log log correct model give log log log see model give surprise indicates worse correct model sequence long enough average effect mimic expectation distribution sequence short give convincing result mention cross entropy since think jargon intimidating much helpful reveal root cause
Looking for product reviews dataset,"<p>I'm working on a school project on product analysis which is based on sentimental analysis. I've been looking for a training dataset for quite a some time now and what I've been able to find so far is a dataset for movie reviews. My question is, can I use this dataset for training the classifier, i.e. will it have an effect on the accuracy of classification? If so, does anyone here know where I can get a free dataset for product reviews?</p>
",Training and Model Evaluation,looking product review dataset working school project product analysis based sentimental analysis looking training dataset quite time able find far dataset movie review question use dataset training classifier e effect accuracy classification doe anyone know get free dataset product review
Stanford CoreNLP model sentiment.ser.gz missing?,"<p>I am new stanford to corenlp and trying to use it. I was able to run sentimental analysis pipeline and corenlp software. While when I am trying to execute evaluate tool it is asking for model sentiment.ser.gz. </p>

<pre><code>java edu.stanford.nlp.sentiment.Evaluate edu/stanford/nlp/models/sentiment/sentiment.ser.gz test.txt
</code></pre>

<p>I could not find the model in the software that I downloaded from stanford site or anywhere on internet.</p>

<p>Can someone please guide if we can create our own model or if I can find anywhere on the internet.</p>

<p>Appreciate your help.</p>
",Training and Model Evaluation,stanford corenlp model sentiment ser gz missing new stanford corenlp trying use wa able run sentimental analysis pipeline corenlp software trying execute evaluate tool asking model sentiment ser gz could find model software downloaded stanford site anywhere internet someone please guide create model find anywhere internet appreciate help
How to test a text clustering application?,"<p>I am developing an application to cluster documents according to their topics. I am using the LDA (Latent Dirichlet Allocation) algorithm. Now the prototype is ready and there are some results. </p>

<p>I am looking for a reasonable way to test it. My current approach is to print out the topics and some of their related documents respectively. And manually evaluate them. I can think of the following test points:</p>

<ul>
<li>The documents within a topic are on that topic indeed.</li>
<li>The topics are substantially different from each other.</li>
</ul>

<p>Is there any best practice to do this? Is there any objective metric for this rather than my subjective evaluation?</p>
",Training and Model Evaluation,test text clustering application developing application cluster document according topic using lda latent dirichlet allocation algorithm prototype ready result looking reasonable way test current approach print topic related document respectively manually evaluate think following test point document within topic topic indeed topic substantially different best practice objective metric rather subjective evaluation
nltk pos tagger looks to incorporate &#39;.&#39;,"<p>I am new to python, nlp and and nltk, so please bear with me. I have a handful of articles (~200), that have been categorized by hand.  I am looking to develop a heuristic to assist/ recommend categories.  To start I was hoping to build a relationship between current categories and the words within the document.</p>

<p>My premise is that the nouns are more  important to the category than any other part of speech. For example the category ""Energy"" probably is driven nearly completely through the nouns: oil, battery, wind,  etc.  </p>

<p>The first thing I wanted to do was tag the parts and evaluate them.  On the first article I encountered some issues.  Some of the tokens are bound to punctuation.</p>

<pre><code>for articles in articles[1]:
    articles_id, content = articles
    clean = nltk.clean_html(content).replace('&amp;rsquo;', ""'"")
    tokens = nltk.word_tokenize(clean)
    pos_document = nltk.pos_tag(tokens)
    pos ={}
    for pos_word in pos_document:
        word, part = pos_word
        if pos.has_key(part):
            pos[part].append(word)
        else:
            pos[part] = [word]
</code></pre>

<p>Formatted output:</p>

<pre><code>{
'VBG': ['continuing', 'paying', 'falling', 'starting'], 
'VBD': ['made', 'ended'], 'VBN': ['been', 'leaned', 'been', 'been'], 
'VBP': ['know', 'hasn', 'have', 'continue', 'expect', 'take', 'see', 'have', 'are'], 
'WDT': ['which', 'which'], 'JJ': ['negative', 'positive', 'top', 'modest', 'negative', 'real', 'financial', 'isn', 'important', 'long', 'short', 'next'], 
'VBZ': ['is', 'has', 'is', 'leads', 'is', 'is'], 'DT': ['Another', 'the', 'the', 'any', 'any', 'the', 'the', 'a', 'the', 'the', 'the', 'the', 'a', 'the', 'a', 'a', 'the', 'a', 'the', 'any'], 
'RP': ['back'], 
'NN': [ 'listless', 'day', 'rsquo', 'll', 'progress', 'rsquo', 't', 'news', 'season', 'corner', 'surprise', 'stock', 'line', 'growth', 'question', 
        'stop', 'engineering', 'growth', 'isn', 'rsquo', 't', 'rsquo', 't', 'stock', 'market', 'look', 'junk', 'bond', 'market', 'turning', 'junk', 
        'rock', 'history', 'guide', 't', 'day', '%', '%', '%', 'level', 'move', 'isn', 'rsquo', 't', 'indication', 'way'], 
',': [',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ','], '.': ['.'], 
'TO': ['to', 'to', 'to', 'to', 'to', 'to', 'to'], 
'PRP': ['them', 'they', 'they', 'we', 'you', 'they', 'it'], 
'RB': ['then', 'there', 'just', 'just', 'always', 'so', 'so', 'only', 'there', 'right', 'there', 'much', 'typically', 'far', 'certainly'], 
':': [';', ';', ';', ';', ';', ';', ';'], 
'NNS': ['folks', 'companies', 'estimates', 'covers', 's', 'equities', 'bonds', 'equities', 'flats'], 
'NNP': ['drift.', 'We', 'Monday', 'DC', 'note.', 'Earnings', 'EPS', 'same.', 'The', 'Street', 'now.', 'Since', 'points.', 'What', 'behind.', 'We', 'flat.', 'The'], 
'VB': ['get', 'manufacture', 'buy', 'boost', 'look', 'see', 'say', 'let', 'rsquo', 'rsquo', 'be', 'build', 'accelerate', 'be'], 
'WRB': ['when', 'where'], 
'CC': ['&amp;', 'and', '&amp;', 'and', 'and', 'or', 'and', '&amp;', '&amp;', '&amp;', 'and', '&amp;', 'and', 'but', '&amp;'], 
'CD': ['47', '23', '30'], 
'EX': ['there'], 
'IN': ['on', 'if', 'until', 'of', 'around', 'as', 'on', 'down', 'since', 'of', 'for', 'under', 'that', 'about', 'at', 'at', 'that', 'like', 'if'], 
'MD': ['can', 'will', 'can', 'can', 'will'], 
'JJR': ['more']
}
</code></pre>

<p>notice under the NMP the word 'drift.' - shouldn't the period be removed?  Do I need to remove this on my own or am I missing something with the library?</p>
",Training and Model Evaluation,nltk po tagger look incorporate new python nlp nltk please bear handful article categorized hand looking develop heuristic assist recommend category start wa hoping build relationship current category word within document premise noun important category part speech example category energy probably driven nearly completely noun oil battery wind etc first thing wanted wa tag part evaluate first article encountered issue token bound punctuation formatted output notice nmp word drift period removed need remove missing something library
What are the different strategies for detecting noisy data in a pile of text?,"<p>I have around 10 GB of text from which I extract features based on bag of words model. The problem is that the feature space is very high dimensional(1 million words) and I can not discard words based on the count of each word as both the most and least occurring words are important of the model to perform better. What are the different strategies for reducing the size of the training data and number of features while still maintaining/improving the model performance?<br/>
<strong>Edit:</strong>
 I want to reduce the size of the training data both because of overfitting and training time. I am using FastRank(Boosted trees) as my ML model. My machine has a core i5 processor running with 8GB RAM. The number of training instances are of the order of 700-800 million. Along with processing it takes more than an hour for the model to train. I currently do random sampling of the training and test data so as to reduce the size to 700MB or so, so that the training of the model finishes in minutes.</p>
",Training and Model Evaluation,different strategy detecting noisy data pile text around gb text extract feature based bag word model problem feature space high dimensional million word discard word based count word least occurring word important model perform better different strategy reducing size training data number feature still maintaining improving model performance edit want reduce size training data overfitting training time using fastrank boosted tree ml model machine ha core processor running gb ram number training instance order million along processing take hour model train currently random sampling training test data reduce size mb training model finish minute
Best way to find document similarity,"<p>I'm new to NLP, i want to find the similarity between the two documents </p>

<p>I googled and found that there are some ways to do it e.g.</p>

<ul>
<li><a href=""http://www.hpl.hp.com/techreports/Compaq-DEC/SRC-TN-1997-015.pdf"" rel=""nofollow"">Shingling, and find text resemblance</a></li>
<li>Cosine similarity or lucene</li>
<li>tf-idf</li>
</ul>

<p>What is the best way to do this(I'm open for other methods too), in which we get high precision , If there is some API in java to do this please also let me know</p>
",Training and Model Evaluation,best way find document similarity new nlp want find similarity two document googled found way e g shingling find text resemblance cosine similarity lucene tf idf best way open method get high precision api java please also let know
Generate unigrams and bigrams from a trigram list,"<p>I am looking at potential ways of just storing the trigram frequencies in memory and calculating the unigram and bigram frequencies on the fly in the following way :</p>

<p>Given a trigram u , v , w :</p>

<p>count(v, w) = sum (.,v,w) i.e sum over all u</p>

<p>Similarly, count(w) = sum(.,w)</p>

<p>This sure does result in a few missing unigrams, for example the sentence begin marker , but does this sound like a valid approach to generating unigrams and bigrams ?</p>
",Training and Model Evaluation,generate unigrams bigram trigram list looking potential way storing trigram frequency memory calculating unigram bigram frequency fly following way given trigram u v w count v w sum v w e sum u similarly count w sum w sure doe result missing unigrams example sentence begin marker doe sound like valid approach generating unigrams bigram
How to extract dates/places in text?,"<p>What are the best packages/software for extracting times/dates/places in text? </p>

<p>Or is there any dataset that could be used as dataset? </p>

<p>For example: </p>

<blockquote>
  <blockquote>
    <p>-- PLEASE JOIN US FOR COOKIES AND COFFEE AT [2:30PM] BEFORE THE SEMINAR IN [ROOM 154 COORDINATED SCIENCE LABORATORY] </p>
    
    <p>-- Then we'll meet on [Friday 6 pm] ... </p>
    
    <p>-- This week's seminar is moved to Tuesday, from [11:00 am to 12:00pm].</p>
    
    <p>Time: [11-12pm], [Oct. 29 Tuesday] </p>
    
    <p>Place: [SC 0216]</p>
  </blockquote>
</blockquote>

<p>Title Statistical significance of combinatorial features with frequent itemset mining ....</p>

<p>I can train a machine learning model for such a task, but I don't know any labelled dataset for this. Anyone aware of any labelled dataset?  </p>
",Training and Model Evaluation,extract date place text best package software extracting time date place text dataset could used dataset example please join u cooky coffee pm seminar room coordinated science laboratory meet friday pm week seminar moved tuesday pm time pm oct tuesday place sc title statistical significance combinatorial feature frequent itemset mining train machine learning model task know labelled dataset anyone aware labelled dataset
What is the difference between evaluation metrics and features in relation to binary classification?,"<p>I'm having a hard time trying to separate these two concepts in my mind.  </p>

<p>I know that evaluation metrics such as <a href=""http://en.wikipedia.org/wiki/BLEU"" rel=""nofollow"">BLEU</a> can be used to measure the quality of a given input against a reference (as in Machine Translation).  But can this score be leveraged into classifying sentences into two categories? For example, a sentence with a certain evaluation metric score above 0.50 would be given a 'yes' while everything below 0.50 given a 'no'.  </p>

<p>Also, it this possibly related to features used in machine learning algorithms? For example, say the phrase, ""in the past"" is a possible feature of the data which then could be used to classify inputs into having this feature or not.</p>
",Training and Model Evaluation,difference evaluation metric feature relation binary classification hard time trying separate two concept mind know evaluation metric bleu used measure quality given input reference machine translation score leveraged classifying sentence two category example sentence certain evaluation metric score would given yes everything given also possibly related feature used machine learning algorithm example say phrase past possible feature data could used classify input feature
How to store and compare annotation (with Gold Standard) in GATE,"<p>I am very comfortable with <a href=""http://uima.apache.org/"" rel=""nofollow"">UIMA</a>, but my new work require me to use <a href=""http://gate.ac.uk/"" rel=""nofollow"">GATE</a> </p>

<p>So, I started learning GATE. My question is regarding how to calculate performance of my tagging engines (java based).</p>

<p>With UIMA, I generally dump all my system annotation into a xmi file and, then using a Java code compare that with a human annotated (gold standard) annotations to calculate Precision/Recall and F-score.</p>

<p>But, I am still struggling to find something similar with GATE.
After going through <a href=""http://gate.ac.uk/releases/gate-7.1-build4485-ALL/doc/tao/splitch10.html#sec%3aeval%3aannotationdiff"" rel=""nofollow"">Gate Annotation-Diff</a> and other info on that page, I can feel there has to be an easy way to do it in JAVA. But, I am not able to figure out how to do it using JAVA. Thought to put this question here, someone might have already figured this out.</p>

<ol>
<li>How to store system annotation into a xmi or any format file programmatically.</li>
<li>How to create one time gold standard data (i.e. human annotated data) for performance calculation.</li>
</ol>

<p>Let me know if you need more specific or details.</p>
",Training and Model Evaluation,store compare annotation gold standard gate comfortable uima new work require use gate started learning gate question regarding calculate performance tagging engine java based uima generally dump system annotation xmi file using java code compare human annotated gold standard annotation calculate precision recall f score still struggling find something similar gate going gate annotation diff info page feel ha easy way java able figure using java thought put question someone might already figured store system annotation xmi format file programmatically create one time gold standard data e human annotated data performance calculation let know need specific detail
How do I get the context of a sentence?,"<p>There is a questionnaire that we use to evaluate the student knowledge level (we do this manually, as in a test paper). It consists of the following parts:</p>

<ol>
<li>Multiple choice</li>
<li>Comprehension Questions (I.e: Is a spider an insect?)</li>
</ol>

<p>Now I have been given a task to make an expert system that will automate this. So basically we have a proper answer for this. But my problem is the ""comprehension questions"". I need to compare the context of their answer to the context of the correct answer.</p>

<p>I already initially searched for the answer, but it seems like it's really a big task to do. What I have search so far is I can do this through NLP which is really new to me. Also, if I'm not mistaken, it seems like that I have to find a dictionary of all words that is possible for the examiner to answer.</p>

<p>Am I on the right track? If no, please suggest of what should I do (study what?) or give me some links to the materials that I need. Also, should I make my own dictionary? Because the words that I will be using are in the Filipino language.</p>

<p><b> Update: Comprehension question</b></p>

<p>The comprehension section of the questionnaire contains one paragraph explaining a certain scenario. The questions are fairly simple. Here is an example:</p>

<p>Bonnie's uncle told her to pick apples from the tree. Picking up a stick, she poked the fruits so they would fall. In the middle of doing this, a strong gust of wind blew. Due to her fear of the fruits falling on top of her head, she stopped what she was doing. After this, though, she noticed that the wind had caused apples to fall from the tree. These fallen apples were what she brought home to her uncle.</p>

<p>The questions are:</p>

<ol>
<li>What did Bonnie's uncle tell her to do?</li>
<li>What caused Bonnie to stop picking apples from the tree?</li>
<li>Is Bonnie a good fruit picker? Please explain your answer.</li>
</ol>

<p>The possible answers that the answer key states are:</p>

<p>For number 1: <br>
1.1 Bonnie's uncle told her to pick apples from the tree <br>
1.2 Get apples <br></p>

<p>For number 2: <br>
2.1 A strong gust of wind blew <br>
2.2 She might get hit in the head by the fruits <br></p>

<p>For number 3: <br>
3.1 No, because the apples she got were already on the ground <br>
3.2 No, because the wind was what caused the fruits to fall <br>
3.3 Yes, because it is difficult to pick fruits when it's windy. <br>
3.4 Yes, because at least she tried</p>

<p>Now there are answers that were given to me. The job that the system shall be able to do is to compare the context of the student's answer to the context of the right answer in order for the system to successfully be able to grade the student's answer.</p>
",Training and Model Evaluation,get context sentence questionnaire use evaluate student knowledge level manually test paper consists following part multiple choice comprehension question e spider insect given task make expert system automate basically proper answer problem comprehension question need compare context answer context correct answer already initially searched answer seems like really big task search far nlp really new also mistaken seems like find dictionary word possible examiner answer right track please suggest study give link material need also make dictionary word using filipino language update comprehension question comprehension section questionnaire contains one paragraph explaining certain scenario question fairly simple example bonnie uncle told pick apple tree picking stick poked fruit would fall middle strong gust wind blew due fear fruit falling top head stopped wa though noticed wind caused apple fall tree fallen apple brought home uncle question bonnie uncle tell caused bonnie stop picking apple tree bonnie good fruit picker please explain answer possible answer answer key state number bonnie uncle told pick apple tree get apple number strong gust wind blew might get hit head fruit number apple got already ground wind wa caused fruit fall yes difficult pick fruit windy yes least tried answer given job system shall able compare context student answer context right answer order system successfully able grade student answer
Develop custom auto-complete plugin for MS Word using n-gram language model,"<p>Does anyone have any suggestions of how to implement a customization to Microsoft Word which will provide word prediction (auto-complete) option as the user types, based on n-gram language models built from a large corpus of training data.</p>

<p>I work in an office where we transcribe audio files. All the material is discourses of a single person speaking, and we have several 1000 transcriptions already done and several 1000 more to do. We have experimented with ASR solutions, but have found that its actually more effort to correct the auto-transcribed text, than it is to transcribe it from scratch.</p>

<p>I thought that we could come up with a solution using just the language model component, and use it to assist the transcribers as they type. The users could choose to type some of the words fully and just the first few letters of other words, and quickly scroll through the list of most probable completions using the space-bar, in such a way that they could transcribe as fast as the audio is being played back.</p>

<p>Would love to hear anybody's thoughts - specifically on how best to generate the LM and how to plug it in.</p>

<p>I also came across this awesome paper on incorporating topic related probabilities <a href=""http://noah.coccaro.com/publications/thesis.pdf"" rel=""nofollow"">http://noah.coccaro.com/publications/thesis.pdf</a> </p>
",Training and Model Evaluation,develop custom auto complete plugin word using n gram language model doe anyone suggestion implement customization microsoft word provide word prediction auto complete option user type based n gram language model built large corpus training data work office transcribe audio file material discourse single person speaking several transcription already done several experimented asr solution found actually effort correct auto transcribed text transcribe scratch thought could come solution using language model component use assist transcriber type user could choose type word fully first letter word quickly scroll list probable completion using space bar way could transcribe fast audio played back would love hear anybody thought specifically best generate lm plug also came across awesome paper incorporating topic related probability
OpenNLP parser training,"<p>I've tried asking this before on the OpenNLP sourceforge page, but it is still sadly languishing in the Help forums:</p>

<p>I have a treebank and I would like to train a model based on it. There was some code lying around using ParserME but that class doesn't seem to exist anymore. It looks like its been possibly replaced by TreebankParser but I can't seem to find any train tools in there. Is there a way of doing this? </p>

<p>Any hints welcome</p>
",Training and Model Evaluation,opennlp parser training tried asking opennlp sourceforge page still sadly languishing help forum treebank would like train model based wa code lying around using parserme class seem exist anymore look like possibly replaced treebankparser seem find train tool way hint welcome
Use NLP / Machine Learning to teach a machine how to detect if a string is mathematical?,"<p>I want to be able to detect if a string is mathematical.</p>

<p>Strings that would evaluate to true on being mathematical would be <code>""2""</code>, <code>""42000""</code>, <code>""-10""</code>, <code>""-55.22""</code>, <code>""forty-two""</code>, <code>""fifty six""</code>, <code>""negative ninety nine""</code>, and <code>""negative one point seven""</code>.</p>

<p>And since it is not numerical and mathematical something as complex as <code>""negative two times seven""</code>, or <code>""two plus two""</code>, or <code>""3 plus two""</code>, or <code>""two - 1""</code>, or <code>""2 ^ 7""</code> would pass.</p>

<p>Basically spelled out numbers, spelled out possessive numbers (first, thirteenth, thousandth) and the words <code>""plus""</code>, <code>""negative""</code>, <code>""positive""</code>, <code>""minus""</code>, <code>""subtracted""</code>, <code>""from""</code>, <code>""times""</code>, <code>""multiplied""</code>, <code>""by""</code> <code>""divided""</code>, <code>""over""</code>, <code>""point""</code>, <code>""to""</code>, <code>""the""</code>, <code>""power""</code>, <code>""of""</code>, and, <code>""and""</code>, <code>""raised""</code></p>

<p>And the function would return false if it is not like one of those examples.</p>

<p>Is it proper to use machine learning / NLP to do this? Is there a better way to do this than NLP / Machine Learning?</p>

<p>Are there any existing scripts or functions that can do this?</p>

<p>If not, how can I do this with <a href=""https://github.com/atrilla/nlptools"" rel=""nofollow"">NLPTools</a> or <a href=""https://github.com/angeloskath/php-nlp-tools"" rel=""nofollow"">PHP NLP tools</a> ?</p>
",Training and Model Evaluation,use nlp machine learning teach machine detect string mathematical want able detect string mathematical string would evaluate true mathematical would since numerical mathematical something complex would pas basically spelled number spelled possessive number first thirteenth thousandth word function would return false like one example proper use machine learning nlp better way nlp machine learning existing script function nlptools php nlp tool
Naive bayes text classification fails in one category. Why?,"<p>I am implementing Naive Bayes classifier for text category detection.
I have 37 categories and I've got accuracy about 36% on my test set.</p>

<p>I want to improve accuracy, so I decided to implement 37 two-way classifiers as suggested in many sources (<a href=""https://stackoverflow.com/questions/3473612/ways-to-improve-the-accuracy-of-a-naive-bayes-classifier"">Ways to improve the accuracy of a Naive Bayes Classifier?</a> is one of them), these classifiers would answer for a given text:</p>

<pre><code>specific_category OR everything_else
</code></pre>

<p>and I would determine text's category by applying them sequentally.</p>

<p>But I've got a problem with first classifier, it always fails in ""specific_category"" category.</p>

<p>I have training data - 37 categories, 100 documents for each category of the same size.
For each category I found list of 50 features I selected by mutual information criteria (features are just words). </p>

<p>For the sake of example, I use two categories ""agriculture"" and ""everything_else"" (except agriculture).</p>

<p>For category ""agriculture"":</p>

<pre><code>number of words in all documents of this class 
(first term in denominator in http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf, (13.7))
W_agriculture = 31649.

Size of vocabulary V_agriculture = 6951.
Log probability of Unknown word (UNK) P(UNK|agriculture) = -10.56
Log probability of class P(agriculture) = log(1/37) = -3.61 (we have 37 categories of same-size documents)
</code></pre>

<p>For category ""everything_else"":</p>

<pre><code>W_everything_else = 1030043
V_everything_else = 44221
P(UNK|everything_else) = -13.89
P(everything_else) = log(36/37) = -0.03
</code></pre>

<p>Then I have a text not related to agriculture, let it consist mostly of Unknown words (UNK). It has 270 words, they are mostly unknown for both categories ""agriculture"" and ""everything_else"". Let's assume 260 words are UNK for ""everything_else"", other 10 is known.</p>

<p>Then, when I calculate probabilities</p>

<pre><code>P(text|agriculture) = P(agriculture) + SUM(P(UNK|agriculture) for 270 times) 
P(text|everything_else) = P(everything_else) + SUM(P(UNK|everything_else) for 260 times) + SUM(P(word|everything_else) for 10 times)
</code></pre>

<p>In the last line we counted 260 words as UNK and 10 as known for a category.</p>

<p><strong>Main problem</strong>. As P(UNK|agriculture) >> P(everything_else) (for log it is much greater), the influence of those 270 terms P(UNK|agriculture) outweighs influence of sum for P(word|everything_else) for each word in text.
Because</p>

<pre><code>SUM(P(UNK|agriculture) for 270 times) = -2851.2
SUM(P(UNK|everything_else) for 260 times) = -3611.4
</code></pre>

<p>and first sum is much larger and can't be corrected not with P(agriculture) nor SUM(P(word|everything_else) for 10 words), because the difference is huge. Then I always fail in ""agriculture"" category though the text does not belong to it.</p>

<p><strong>The questions is</strong>: Am I missing something? Or how should I deal with big number of UNK words and their probability being significantly higher for small categories?</p>

<p><strong>UPD</strong>: Tried to enlarge tranining data for ""agriculture"" category (just concatenating the document 36 times) to be equal in number of documents. It helped for few categories, not much for others, I suspect due to fewer number of words and dictionary size, P(UNK|specific_category) gets bigger and outweighs P(UNK|everything_else) when summing 270 times.</p>

<p>So it seems such method is very sensitive on number of words in training data and vocabulary size. How to overcome this? Maybe bigrams/trigrams would help?</p>
",Training and Model Evaluation,naive bayes text classification fails one category implementing naive bayes classifier text category detection category got accuracy test set want improve accuracy decided implement two way classifier suggested many source training data category document category size category found list feature selected mutual information criterion feature word sake example use two category agriculture everything else except agriculture category agriculture category everything else text related agriculture let consist mostly unknown word unk ha word mostly unknown category agriculture everything else let assume word unk everything else known calculate probability last line counted word unk known category main problem p unk agriculture p everything else log much greater influence term p unk agriculture outweighs influence sum p word everything else word text first sum much larger corrected p agriculture sum p word everything else word difference huge always fail agriculture category though text doe belong question missing something deal big number unk word probability significantly higher small category upd tried enlarge tranining data agriculture category concatenating document time equal number document helped category much others suspect due fewer number word dictionary size p unk specific category get bigger outweighs p unk everything else summing time seems method sensitive number word training data vocabulary size overcome maybe bigram trigram would help
NLP model training,"<p>I just started with NLP (Natural Language Processing) and struggling to understand one important concept. How to train system for relation extraction on future inputs?</p>

<p>For example, I have few lines like:</p>

<ul>
<li><p>Tom is working for abc company </p></li>
<li><p>Jerry works at xyz  </p></li>
<li>organization is the  place where Person works.</li>
</ul>

<p>In all these cases relation ship is ""person"" ""Organization"" with relation ship type ""working""</p>

<p>Based on above examples and some NLP readings, I think we need to train system based on Part of Speech tag than real ""entity names"", to make it generic for other input data in field. This is the part I am really confused.</p>

<p>Please don't simply point me to some algorithms( SVM etc.,), because I know it is possible with them, but I am missing details on how algorithms process these lines to process other inputs. All the examples I see directly provides models and tells use them, due to which I am unable to construct few things I would like to.</p>

<p>Any example on how algorithms (any example algorithm is Ok) use above sentences to construct training model would be really helpful. </p>

<p>Thank you for your time and help.</p>

<p>Note: Any one of the programming language specified in tags section is Ok for me.</p>
",Training and Model Evaluation,nlp model training started nlp natural language processing struggling understand one important concept train system relation extraction future input example line like tom working abc company jerry work xyz organization place person work case relation ship person organization relation ship type working based example nlp reading think need train system based part speech tag real entity name make generic input data field part really confused please simply point algorithm svm etc know possible missing detail algorithm process line process input example see directly provides model tell use due unable construct thing would like example algorithm example algorithm ok use sentence construct training model would really helpful thank time help note one programming language specified tag section ok
Weka ignoring unlabeled data,"<p>I am working on an NLP classification project using Naive Bayes classifier in Weka. I intend to use semi-supervised machine learning, hence working with unlabeled data. When I test the model obtained from my labeled training data on an independent set of unlabeled test data, Weka ignores all the unlabeled instances. Can anybody please guide me how to solve this? Someone has already asked this question here before but there wasn't any appropriate solution provided. Here is a sample test file:</p>

<pre><code>@relation referents
@attribute feature1      NUMERIC
@attribute feature2      NUMERIC
@attribute feature3      NUMERIC
@attribute feature4      NUMERIC
@attribute class{1 -1}
@data
1, 7, 1, 0, ?
1, 5, 1, 0, ?
-1, 1, 1, 0, ?
1, 1, 1, 1, ?
-1, 1, 1, 1, ?
</code></pre>
",Training and Model Evaluation,weka ignoring unlabeled data working nlp classification project using naive bayes classifier weka intend use semi supervised machine learning hence working unlabeled data test model obtained labeled training data independent set unlabeled test data weka ignores unlabeled instance anybody please guide solve someone ha already asked question appropriate solution provided sample test file
Why is python&#39;s hstack used here for machine learning,"<p>I am trying to understand some python code that tries to predict prices based on an ad posting.</p>

<p>Before fitting the text vectorizers, a function does <code>hstack((des, titles))</code> to the ad description <code>des</code> and ad title <code>titles</code>.</p>

<p><strong>Question</strong>: What is the reason for doing the <code>hstack</code>? I dont see any difference between <code>des</code> and <code>merged</code> when printing it out. <code>merged</code> appears to be used as the training data instead of passing in <code>des</code> and <code>titles</code> seperately. How does this work?</p>

<p><strong>Vectorizing function</strong></p>

<pre><code>def fit(des, titles, sal, clf, alpha):
    tRidge = time()
    vect = TfidfVectorizer(min_df=1,ngram_range=(1,3),max_features=24000000)
    vect2 = TfidfVectorizer(min_df=1,ngram_range=(1,3),max_features=24000000)
    des = vect.fit_transform(des)
    titles = vect2.fit_transform(titles)
    merged = hstack((des, titles))
    print des, ""\n\n\n\n""
    print titles, ""\n\n\n\n""
    print merged

    rr = linear_model.Ridge(alpha= alpha)
    rr.fit(merged,sals)

    return vect, vect2, rr 
</code></pre>

<p><strong>Result</strong></p>

<pre><code>(0, 2991)   0.0923069427531
(0, 2989)   0.156938669001
(0, 2988)   0.183108029528
(0, 2984)   0.183108029528
(0, 2983)   0.0923069427531
(0, 2982)   0.0923069427531
(0, 2981)   0.0923069427531
(0, 2976)   0.0923069427531
(0, 2974)   0.0784693345005
(0, 2973)   0.1373027904
(0, 2968)   0.0923069427531
(0, 2967)   0.0923069427531
(0, 2966)   0.183108029528
(0, 2859)   0.0610360098426
(0, 2858)   0.0610360098426
(0, 2855)   0.0548137869472
(0, 2811)   0.0923069427531
(0, 2810)   0.0610360098426
(0, 2807)   0.0548137869472
(0, 2671)   0.0923069427531
(0, 2670)   0.0923069427531
(0, 2663)   0.0784693345005
(0, 2662)   0.0784693345005
(0, 2659)   0.0819523573892
(0, 2642)   0.0923069427531
:   :
(9, 225)    0.0518713890037
(9, 208)    0.105028746631
(9, 155)    0.0518713890037
(9, 154)    0.0518713890037
(9, 153)    0.0518713890037
(9, 152)    0.0518713890037
(9, 151)    0.0518713890037
(9, 149)    0.0440954196221
(9, 140)    0.0835380774247
(9, 135)    0.0518713890037
(9, 134)    0.0518713890037
(9, 132)    0.0881908392442
(9, 131)    0.0771565630894
(9, 122)    0.0518713890037
(9, 121)    0.0518713890037
(9, 118)    0.0518713890037
(9, 117)    0.0518713890037
(9, 116)    0.0771565630894
(9, 25) 0.0518713890037
(9, 8)  0.0518713890037
(9, 7)  0.0440954196221
(9, 6)  0.0440954196221
(9, 5)  0.0518713890037
(9, 4)  0.0518713890037
(9, 3)  0.0518713890037 




(0, 69) 0.42208707303
(0, 68) 0.42208707303
(0, 27) 0.42208707303
(0, 26) 0.42208707303
(0, 24) 0.379058050386
(0, 0)  0.379058050386
(1, 62) 0.42435658025
(1, 61) 0.42435658025
(1, 60) 0.42435658025
(1, 28) 0.42435658025
(1, 23) 0.42435658025
(1, 22) 0.315606501824
(2, 59) 0.346009923908
(2, 58) 0.346009923908
(2, 44) 0.346009923908
(2, 43) 0.346009923908
(2, 42) 0.346009923908
(2, 7)  0.346009923908
(2, 6)  0.346009923908
(2, 5)  0.346009923908
(2, 0)  0.205467906151
(3, 70) 0.343926205461
(3, 69) 0.227413915309
(3, 68) 0.227413915309
(3, 41) 0.343926205461
:   :
(7, 16) 0.231189334057
(7, 12) 0.271958221129
(7, 11) 0.271958221129
(7, 10) 0.271958221129
(8, 76) 0.265672282889
(8, 75) 0.265672282889
(8, 74) 0.265672282889
(8, 73) 0.265672282889
(8, 72) 0.265672282889
(8, 53) 0.265672282889
(8, 52) 0.22584571227
(8, 51) 0.22584571227
(8, 35) 0.265672282889
(8, 18) 0.265672282889
(8, 17) 0.265672282889
(8, 16) 0.22584571227
(8, 15) 0.265672282889
(8, 14) 0.265672282889
(8, 13) 0.265672282889
(9, 65) 0.435367791014
(9, 64) 0.435367791014
(9, 63) 0.370102397554
(9, 22) 0.323795863959
(9, 9)  0.435367791014
(9, 8)  0.435367791014 




(0, 2991)   0.0923069427531
(0, 2989)   0.156938669001
(0, 2988)   0.183108029528
(0, 2984)   0.183108029528
(0, 2983)   0.0923069427531
(0, 2982)   0.0923069427531
(0, 2981)   0.0923069427531
(0, 2976)   0.0923069427531
(0, 2974)   0.0784693345005
(0, 2973)   0.1373027904
(0, 2968)   0.0923069427531
(0, 2967)   0.0923069427531
(0, 2966)   0.183108029528
(0, 2859)   0.0610360098426
(0, 2858)   0.0610360098426
(0, 2855)   0.0548137869472
(0, 2811)   0.0923069427531
(0, 2810)   0.0610360098426
(0, 2807)   0.0548137869472
(0, 2671)   0.0923069427531
(0, 2670)   0.0923069427531
(0, 2663)   0.0784693345005
(0, 2662)   0.0784693345005
(0, 2659)   0.0819523573892
(0, 2642)   0.0923069427531
:   :
(7, 3669)   0.231189334057
(7, 3665)   0.271958221129
(7, 3664)   0.271958221129
(7, 3663)   0.271958221129
(8, 3729)   0.265672282889
(8, 3728)   0.265672282889
(8, 3727)   0.265672282889
(8, 3726)   0.265672282889
(8, 3725)   0.265672282889
(8, 3706)   0.265672282889
(8, 3705)   0.22584571227
(8, 3704)   0.22584571227
(8, 3688)   0.265672282889
(8, 3671)   0.265672282889
(8, 3670)   0.265672282889
(8, 3669)   0.22584571227
(8, 3668)   0.265672282889
(8, 3667)   0.265672282889
(8, 3666)   0.265672282889
(9, 3718)   0.435367791014
(9, 3717)   0.435367791014
(9, 3716)   0.370102397554
(9, 3675)   0.323795863959
(9, 3662)   0.435367791014
(9, 3661)   0.435367791014
</code></pre>
",Training and Model Evaluation,python hstack used machine learning trying understand python code try predict price based ad posting fitting text vectorizers function doe ad description ad title question reason dont see difference printing appears used training data instead passing seperately doe work vectorizing function result
Why getting different results with MALLET topic inference for single and batch of documents?,"<p>I'm trying to perform LDA topic modeling with Mallet 2.0.7.  I can train a LDA model and get good results, judging by the output from the training session.  Also, I can use the inferencer built in that process and get similar results when re-processing my training file.  However, if I take an individual file from the larger training set, and process it with the inferencer I get very different results, which are not good.</p>

<p>My understanding is that the inferencer should be using a fixed model, and only features local to that document, so I do not understand why I would get any different results while processing 1 file or the 1k from my training set.  I am not doing frequency cutoffs which would seem to be a global operation that would have this type of an effect.  You can see other parameters I'm using in the commands below, but they're mostly default. Changing # of iterations to 0 or 100 didn't help.</p>

<p>Import data:</p>

<pre><code>bin/mallet import-dir \
  --input trainingDataDir \
  --output train.data \
  --remove-stopwords TRUE \
  --keep-sequence TRUE \
  --gram-sizes 1,2 \
  --keep-sequence-bigrams TRUE
</code></pre>

<p>Train:</p>

<pre><code>time ../bin/mallet train-topics
  --input ../train.data \
  --inferencer-filename lda-inferencer-model.mallet \
  --num-top-words 50 \
  --num-topics 100 \
  --num-threads 3 \
  --num-iterations 100 \
  --doc-topics-threshold 0.1 \
  --output-topic-keys topic-keys.txt \
  --output-doc-topics doc-topics.txt
</code></pre>

<p>Topics assigned during training to one file in particular, #14 is about wine which is correct:</p>

<pre><code>998 file:/.../29708933509685249 14  0.31684981684981683 
&gt; grep ""^14\t"" topic-keys.txt 
14  0.5 wine spray cooking car climate top wines place live honey sticking ice prevent collection market hole climate_change winery tasting california moldova vegas horses converted paper key weather farmers_market farmers displayed wd freezing winter trouble mexico morning spring earth round mici torrey_pines barbara kinda nonstick grass slide tree exciting lots 
</code></pre>

<p>Run inference on entire train batch:</p>

<pre><code>../bin/mallet infer-topics \
  --input ../train.data \
  --inferencer lda-inferencer-model.mallet \
  --output-doc-topics inf-train.1 \
  --num-iterations 100
</code></pre>

<p>Inference score on train -- very similar:</p>

<pre><code>998 /.../29708933509685249 14 0.37505087505087503 
</code></pre>

<p>Run inference on another training data file comprised of only that 1 txt file:</p>

<pre><code>../bin/mallet infer-topics \
  --input ../one.data \
  --inferencer lda-inferencer-model.mallet \
  --output-doc-topics inf-one.2 \
  --num-iterations 100
</code></pre>

<p>Inference on one document produces topic 80 and 36, which are very different (14 is given near 0 score):</p>

<pre><code>0 /.../29708933509685249 80 0.3184778184778185 36 0.19067969067969068
&gt; grep ""^80\t"" topic-keys.txt 
80  0.5 tips dog care pet safety items read policy safe offer pay avoid stay important privacy services ebay selling terms person meeting warning poster message agree sellers animals public agree_terms follow pets payment fraud made privacy_policy send description puppy emailed clicking safety_tips read_safety safe_read stay_safe services_stay payment_services transaction_payment offer_transaction classifieds_offer 
</code></pre>
",Training and Model Evaluation,getting different result mallet topic inference single batch document trying perform lda topic modeling mallet train lda model get good result judging output training session also use inferencer built process get similar result processing training file however take individual file larger training set process inferencer get different result good understanding inferencer using fixed model feature local document understand would get different result processing file k training set frequency cutoff would seem global operation would type effect see parameter using command mostly default changing iteration help import data train topic assigned training one file particular wine correct run inference entire train batch inference score train similar run inference another training data file comprised txt file inference one document produce topic different given near score
Debugging the implementation of Baum Welch Algorithm (for POS tagging),"<p>I am working on a project, a part of which is to develop an unsupervised HMM trainer for POS tagging, which I now want to test for posible bugs.</p>

<p>I am using Baum-Welch algorithm to train the model. The inputs are sequence words (drawn from a corpus) and the outputs are sequence of hidden states from a set of states <code>(s1, s2, ... sn)</code>. 
I am now done with the coding, but I am not sure if it is bug free.</p>

<p>Can anyone suggest me some debugging ideas? As in what should I check in the outputs? How to check the accuracy of my algorithm ?</p>
",Training and Model Evaluation,debugging implementation baum welch algorithm po tagging working project part develop unsupervised hmm trainer po tagging want test posible bug using baum welch algorithm train model input sequence word drawn corpus output sequence hidden state set state done coding sure bug free anyone suggest debugging idea check output check accuracy algorithm
Implementing Bayes classifier (in PHP),"<p>I have a theoretical question about a Naive Bayes Classifier. Assume I have trained the classifier with the following training data:</p>

<pre><code>class word  count
-----------------
pos   good  1
      sun   1
neu   tree  1
neg   bad   1
      sad   1
</code></pre>

<p>Assume I now classify ""good sun great"". There are now two options:</p>

<p>1) classify against the trainingdata, which remains static. Meaning both ""good"" and ""sun"" come from the positive category, classifying this string as a positive. After classification, the training table remains unchanged. All strings are thus classified against the static set of training data.</p>

<p>2) You classify the string, but then update the training data, as in the table underneath. Thus, the next string will be classified against a more ""advanced"" set of training data than this one. By the end of (automatic) classification, the table that started out as a simple training set, will have grown in size, having been expanded with many words (and updated word counts)</p>

<pre><code>class word  count
-----------------
pos   good  2
      sun   2
      great 1
neu   tree  1
neg   bad   1
      sad   1
</code></pre>

<p>In my implementation of NMB I used the first method, but I'm now second-guessing I should have done the latter. Please enlighten me :-)</p>
",Training and Model Evaluation,implementing bayes classifier php theoretical question naive bayes classifier assume trained classifier following training data assume classify good sun great two option classify trainingdata remains static meaning good sun come positive category classifying string positive classification training table remains unchanged string thus classified static set training data classify string update training data table underneath thus next string classified advanced set training data one end automatic classification table started simple training set grown size expanded many word updated word count implementation nmb used first method second guessing done latter please enlighten
Bad Result And Evaluation From Giza++,"<p>I have tried to work with giza++ on window (using Cygwin compiler).
I used this code:</p>

<p>//Suppose source language is French and target language is English</p>

<pre><code>plain2snt.out  FrenchCorpus.f  EnglishCorpus.e

mkcls  -c30  -n20  -pFrenchCorpus.f  -VFrenchCorpus.f.vcb.classes  opt
mkcls  -c30  -n20  -pEnglishCorpus.e  -VEnglishCorpus.e.vcb.classes  opt
snt2cooc.out  FrenchCorpus.f.vcb  EnglishCorpus.e.vcb  FrenchCorpus.f_EnglishCorpus.e.snt &gt;courpuscooc.cooc

GIZA++  -S  FrenchCorpus.f.vcb  -T EnglishCorpus.e.vcb -C FrenchCorpus.f_EnglishCorpus.e.snt  -m1 100  -m2 30  -mh 30  -m3 30  -m4 30  -m5 30  -p1 o.95  -CoocurrenceFile  courpuscooc.cooc -o     dictionary
</code></pre>

<p>But the  after getting the output files from giza++  and evaluate output, I observed that the results were too bad.</p>

<p>My evaluation result  was:</p>

<p>RECALL =    0.0889</p>

<p>PRECISION = 0.0990</p>

<p>F_MEASURE = 0.0937</p>

<p>AER =       0.9035</p>

<p>Dose any body know the reason? Could the reason be that I have forgotten some parameters or I should change some of them?</p>

<p>in other word:</p>

<p>first I wanted train giza++ by huge amount of data and then test it by small corpus and compare its result by desired alignment(GOLD STANDARD) , but I don't find any document or useful page in web.</p>

<p>can you introduce useful document?</p>

<p>Therefore I ran it by small courpus (447 sentence) and compared result by desired alignment.do you think this is right way?</p>

<p>Also I changed my code as follows and got better result but It's still not good:</p>

<p>GIZA++  -S testlowsf.f.vcb  -T testlowde.e.vcb  -C testlowsf.f_testlowde.e.snt  -m1 5  -m2 0  -mh 5  -m3 5  -m4 0  -CoocurrenceFile inputcooc.cooc  -o dictionary -model1dumpfrequency 1  -model4smoothfactor 0.4  -nodumps 0  -nsmooth 4  -onlyaldumps 1  -p0 0.999 -diagonal yes  -final yes</p>

<p>result of evaluation :</p>

<p>// suppose A is result of GIZA++ and G is Gold standard. As and Gs is S link in A And G files. Ap and Gp is p link in A and G files.</p>

<p>RECALL = As intersect Gs/Gs = 0.6295</p>

<p>PRECISION = Ap intersect Gp/A = 0.1090</p>

<p>FMEASURE = (2*PRECISION*RECALL)/(RECALL + PRECISION) = 0.1859</p>

<p>AER = 1 - ((As intersect Gs + Ap intersect Gp)/(A + S)) = 0.7425</p>

<p>Do you know the reason?</p>
",Training and Model Evaluation,bad result evaluation giza tried work giza window using cygwin compiler used code suppose source language french target language english getting output file giza evaluate output observed result bad evaluation result wa recall precision f measure aer dose body know reason could reason forgotten parameter change word first wanted train giza huge amount data test small corpus compare result desired alignment gold standard find document useful page web introduce useful document therefore ran small courpus sentence compared result desired alignment think right way also changed code follows got better result still good giza testlowsf f vcb testlowde e vcb c testlowsf f testlowde e snt mh coocurrencefile inputcooc cooc dictionary model dumpfrequency model smoothfactor nodumps nsmooth onlyaldumps p diagonal yes final yes result evaluation suppose result giza g gold standard g link g file ap gp p link g file recall intersect g g precision ap intersect gp fmeasure precision recall recall precision aer intersect g ap intersect gp know reason
Using a support vector classifier with polynomial kernel in scikit-learn,"<p>I'm experimenting with different classifiers implemented in the scikit-learn package, to do some NLP task. The code I use to perform the classification is the following</p>

<pre><code>def train_classifier(self, argcands):
        # Extract the necessary features from the argument candidates
        train_argcands_feats = []
        train_argcands_target = []

        for argcand in argcands:
            train_argcands_feats.append(self.extract_features(argcand))
            train_argcands_target.append(argcand[""info""][""label""]) 

        # Transform the features to the format required by the classifier
        self.feat_vectorizer = DictVectorizer()
        train_argcands_feats = self.feat_vectorizer.fit_transform(train_argcands_feats)

        # Transform the target labels to the format required by the classifier
        self.target_names = list(set(train_argcands_target))
        train_argcands_target = [self.target_names.index(target) for target in train_argcands_target]

        # Train the appropriate supervised model
        self.classifier = LinearSVC()
        #self.classifier = SVC(kernel=""poly"", degree=2)

        self.classifier.fit(train_argcands_feats,train_argcands_target)

        return

def execute(self, argcands_test):
        # Extract features
        test_argcands_feats = [self.extract_features(argcand) for argcand in argcands_test]

        # Transform the features to the format required by the classifier
        test_argcands_feats = self.feat_vectorizer.transform(test_argcands_feats)

        # Classify the candidate arguments 
        test_argcands_targets = self.classifier.predict(test_argcands_feats)

        # Get the correct label names
        test_argcands_labels = [self.target_names[int(label_index)] for label_index in test_argcands_targets]

        return zip(argcands_test, test_argcands_labels)
</code></pre>

<p>As can be seen by the code, I'm testing two implementations of a Support Vectors Machine classifier: the LinearSVC and the SVC with a polynomial kernel. 
Now, for my ""problem"". When using the LinearSVC, I get a classification with no problems: the test instances are tagged with some labels. However, if I use the polynomial SVC, ALL test instances are tagged with the SAME label. 
I know that one possible explanation is that, simply, the polynomial SVC is not the appropriate classifier to use for my task, and that's fine. I just want to make sure that I'm using the polynomial SVC appropriately. </p>

<p>Thanks for all the help/advice you could give me.</p>

<p><strong>UPDATE</strong>
Following the recommendation given in the answers, I've changed the code that trains the classifier to do the following:</p>

<pre><code># Train the appropriate supervised model
parameters = [{'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['poly'], 'degree': [2]}]
self.classifier = GridSearchCV(SVC(C=1), parameters, score_func = f1_score)
</code></pre>

<p>Now I get the following message:</p>

<pre><code>ValueError: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than k=3.
</code></pre>

<p>This has something to do with the uneven distribution of class' instances in my training data, right? Or am I calling the procedure incorrectly?</p>
",Training and Model Evaluation,using support vector classifier polynomial kernel scikit learn experimenting different classifier implemented scikit learn package nlp task code use perform classification following seen code testing two implementation support vector machine classifier linearsvc svc polynomial kernel problem using linearsvc get classification problem test instance tagged label however use polynomial svc test instance tagged label know one possible explanation simply polynomial svc appropriate classifier use task fine want make sure using polynomial svc appropriately thanks help advice could give update following recommendation given answer changed code train classifier following get following message ha something uneven distribution class instance training data right calling procedure incorrectly
what kind of an approach should I use to extract semantics from a sentence,"<p>Say for an example there is a sentence ""Create a customer in the CRM and set his age to 25 and gender to male."". I would like to extract information like gender is male, age is 25, command is create a customer. How can I do this?</p>

<p>I've tried Semantic role labelers like mate-tools and it cannot extract things like age is 25, gender is male.
Is there a particular way or method or a pipeline I can do this successfully and what kind of an accuracy can I expect?</p>
",Training and Model Evaluation,kind approach use extract semantics sentence say example sentence create customer set age gender male would like extract information like gender male age command create customer tried semantic role labelers like mate tool extract thing like age gender male particular way method pipeline successfully kind accuracy expect
is MaltParser the &quot;Nivre&quot; parser mentioned in Parsing to Stanford Dependencies Trade-offs between speed and accuracy?,"<p>hard to find rich info about Nivre parser, google lead to Maltparser, is it the same?</p>

<p>is it the one mentioned in the paper Parsing to Stanford Dependencies Trade-offs between speed and accuracy?</p>

<p>it performs quite well in the paper, why is it so unpopular?</p>
",Training and Model Evaluation,maltparser nivre parser mentioned parsing stanford dependency trade offs speed accuracy hard find rich info nivre parser google lead maltparser one mentioned paper parsing stanford dependency trade offs speed accuracy performs quite well paper unpopular
Maximum Entropy classifier for big data sets,"<p>I have been looking for a maximum entropy classification implementation which can deal with an output size of 500 classes and 1000 features. My training data has around 30,000,000 lines.
I have tried using MegaM, the 64-bit R maxent package, the maxent tool from the University of Edinburgh but as expected, none of them can handle the size of data. However, the size of the data set doesn't seem too out of the world for nlp tasks of this nature. 
Are there any techniques that I should be employing? Or any suggestion for a toolkit which I may use?
I am trying to run this on a 64-bit Windows machine with 8GB of RAM,using Cygwin where required.</p>
",Training and Model Evaluation,maximum entropy classifier big data set looking maximum entropy classification implementation deal output size class feature training data ha around line tried using megam bit r maxent package maxent tool university edinburgh expected none handle size data however size data set seem world nlp task nature technique employing suggestion toolkit may use trying run bit window machine gb ram using cygwin required
How to evaluate and explain the trained model in this machine learning?,"<p>I am new in machine learning. I did a test but do not know how to explain and evaluate.</p>

<p>Case 1:</p>

<p>I first divide randomly the data (data A, about 8000 words) into 10 groups (a1..a10). Within each group, I use 90% of data to build ngram model. This ngram model is then tested on the other 10% data of the same group. The result is below 10% accuracy. Other 9 groups are done same way (respectively build model and respectively tested on the remained 10% data of that group). All results are about 10% accuracy. (Is this 10 fold cross-validation?)</p>

<p>Case 2:</p>

<p>I first build a ngram model based on <strong>entire</strong> data set (data A) of about 8000 words. Then I divide this A into 10 groups(a1,a2,a3..a10), randomly of course. I then use this ngram to test respectively a1,a2..a10. I found the model is almost 96% accuracy on all groups.</p>

<p>How to explain such situations.
Thanks in advance.</p>
",Training and Model Evaluation,evaluate explain trained model machine learning new machine learning test know explain evaluate case first divide randomly data data word group within group use data build ngram model ngram model tested data group result accuracy group done way respectively build model respectively tested remained data group result accuracy fold cross validation case first build ngram model based entire data set data word divide group randomly course use ngram test respectively found model almost accuracy group explain situation thanks advance
NLP - Improving Running Time and Recall of Fuzzy string matching,"<p>I have made a working algorithm but the running time is very horrible. Yes, I know from the start that it will be horrible but not that much. For just 200000 records, the program runs for more than an hour.</p>

<p>Basically what I am doing is:</p>

<pre><code>for each searchfield in search fields
    for each sample in samples
        do a q-gram matching
    if there are matches then return it
    else
        split the searchfield into uniwords
        for each sample in samples
            split sample into uniwords
            for each uniword in samples
                if the uniword is a known abbreviation
                    then search the dictionary for its full word or other known abbr
                else do a jaro-winkler matching
            average the distances of all the uniwords
            if the average is above threshold then make it as a match and break
        end for
        if there is a match make a comment that it matched one of the samples partially
    end else
end for
</code></pre>

<p>Yes, this code is very loop-happy. I am using brute-force because the recall is very important. So, I'm wondering how can I make it faster since I am not only running it for 200000 data for millions of data and the computers of the client are not high-end (1GB-2GB of Ram Pentium 4 or Dual-Core, the computer where I test this program is a Dual Core with 4GB of Ram). I came across TF/IDF but I do not know if it will be sufficient. And I wonder how can google make searches real time.</p>

<p>Thanks in advance!</p>

<p>Edit:
This program is a data filterer. From 200,000 dummy data (actual data is about 12M), I must filter data that is irrelevant to the samples (500 dummy samples, I still do not know how much the actual amount of samples).</p>

<p>With the given dummy data and samples, the running time is about 1 hour but after tinkering here and there, I have successfully lessen it to 10-15 minutes. I have lessen it by grouping the fields and samples that begin with the same character (discounting special and non-meaningful words e.g. the, a, an) and matching the fields to the sample with the same first character. I know there is a problem there. What if the field was misspelled at the first character? But I think the number of those are negligible. The samples are spelled correctly since it is always maintained.</p>
",Training and Model Evaluation,nlp improving running time recall fuzzy string matching made working algorithm running time horrible yes know start horrible much record program run hour basically yes code loop happy using brute force recall important wondering make faster since running data million data computer client high end gb gb ram pentium dual core computer test program dual core gb ram came across tf idf know sufficient wonder google make search real time thanks advance edit program data filterer dummy data actual data must filter data irrelevant sample dummy sample still know much actual amount sample given dummy data sample running time hour tinkering successfully lessen minute lessen grouping field sample begin character discounting special non meaningful word e g matching field sample first character know problem field wa misspelled first character think number sample spelled correctly since always maintained
Can OpenNLP use HTML tags as part of the training?,"<p>I'm creating a training set for the TokenNameFinder using html documents converted into plain text, but my precision is low and I want to use the HTML tags as part of the training. Like words in bold, and sentences in differents margin sizes. 
Will OpenNLP accept and use those tags to create rules?
Is there another way to make use of those tags to improve precision?</p>
",Training and Model Evaluation,opennlp use html tag part training creating training set tokennamefinder using html document converted plain text precision low want use html tag part training like word bold sentence differents margin size opennlp accept use tag create rule another way make use tag improve precision
OpenNLP HeadRules,"<p>I'm trying to train a parser for a new model using the openNLP tutorial <a href=""http://sourceforge.net/apps/mediawiki/opennlp/index.php?title=Parser#Training"" rel=""nofollow"">http://sourceforge.net/apps/mediawiki/opennlp/index.php?title=Parser#Training</a> . The only problem is that is requires a head_rules file. I can't seem to find any information anywhere on generating this file and the the only link to a head_rules file can be found here:
<a href=""http://opennlp.sourceforge.net/models/english/parser/head_rules"" rel=""nofollow"">http://opennlp.sourceforge.net/models/english/parser/head_rules</a> but I can't make sense of it. Does anyone know how to generate this from  training data?</p>
",Training and Model Evaluation,opennlp headrules trying train parser new model using opennlp tutorial problem requires head rule file seem find information anywhere generating file link head rule file found make sense doe anyone know generate training data
"Match trigrams, bigrams, and unigrams to a text; if unigram or bigram a substring of already matched trigram, pass; python","<p>main_text is a list of lists containing sentences that've been part-of-speech tagged:</p>

<pre><code> main_text = [[('the', 'DT'), ('mad', 'JJ'), ('hatter', 'NN'), ('likes','VB'),    
              ('tea','NN'), ('and','CC'), ('hats', 'NN')], [('the', 'DT'), ('red','JJ')                   
               ('queen', 'NN'), ('hates','VB'),('alice','NN')]]  
</code></pre>

<p>ngrams_to_match is a list of lists containing part-of-speech tagged trigrams:</p>

<pre><code> ngrams_to_match = [[('likes','VB'),('tea','NN'), ('and','CC')],
                    [('the', 'DT'), ('mad', 'JJ'), ('hatter', 'NN')],
                    [('hates', 'DT'), ('alice', 'JJ'), ('but', 'CC') ],
                    [('and', 'CC'), ('the', 'DT'), ('rabbit', 'NN')]]
</code></pre>

<p>(a) For each sentence in main_text, first check to see if a complete trigram in ngrams_to _match matches.  If the trigram matches, return the matched trigram and the sentence. </p>

<p>(b) Then, check to see if the the first tuple (a unigram) or the first two tuples (a bigram) of each of the trigrams match in main_text.   </p>

<p>(c) If the unigram or bigram forms a substring of an already matched trigram, don't return anything. Otherwise, return the bigram or unigram match and the sentence.</p>

<p>Here is what the output should be:</p>

<pre><code> trigram_match = [('the', 'DT'), ('mad', 'JJ'), ('hatter', 'NN')], sentence[0]
 trigram_match = [('likes','VB'),('tea','NN'), ('and','CC')], sentence[0]
 bigram_match = [('hates', 'DT'), ('alice', JJ')], sentence[1]
</code></pre>

<p>Condition (b) gives us the bigram_match.</p>

<p>The WRONG output would be:</p>

<pre><code> trigram_match = [('the', 'DT'), ('mad', 'JJ'), ('hatter', 'NN')], sentence[0]
 bigram_match =  [('the', 'DT'), ('mad', 'JJ')] #*bad by condition c
 unigram_match = [ [('the', 'DT')] #*bad by condition c
 trigram_match = [('likes','VB'),('tea','NN'), ('and','CC')], sentence[0]
 bigram_match = [('likes','VB'),('tea','NN')] #*bad by condition c
 unigram_match [('likes', 'VB')]# *bad by condition c
</code></pre>

<p>and so on.</p>

<p>The following, very ugly code works okay for this toy example. But I was wondering if anyone had a more streamlined approach.</p>

<pre><code> for ngram in ngrams_to_match:
  for sentence in main_text:
        for tup in sentence:

            #we can't be sure that our part-of-speech tagger will
            #tag an ngram word and a main_text word the same way, so 
            #we match the word in the tuple, not the whole tuple

        if ngram[0][0] == tup[0]: #if word in the first ngram matches...
            unigram_index = sentence.index(tup) #...then this is our index
            unigram = (sentence[unigram_index][0]) #save it as a unigram

            try:   
                        if sentence[unigram_index+2][0]==ngram[2][0]:
                 if sentence[unigram_index+2][0]==ngram[2][0]:  #match a trigram
                      trigram = (sentence[unigram_index][0],span[1][0], ngram[2][0])#save the match
                      print 'heres the trigram--&gt;', sentence,'\n', 'trigram---&gt;',trigram
            except IndexError:
            pass
            if ngram[0][0] == tup[0]:# == tup[0]:  #same as above
                unigram_index = sentence.index(tup)               
                if sentence[unigram_index+1][0]==span[1][0]:  #get bigram match     

                bigram = (sentence[unigram_index][0],span[1][0])#save the match
                if bigram[0] and bigram[1] in trigram:  #no substring matches
                                     pass                             
                else:
                    print 'heres a sentence--&gt;', sentence,'\n', 'bigram---&gt;', bigram
                if unigram in bigram or trigram:  #no substring matches
                    pass
                else:
                    print unigram 
</code></pre>
",Training and Model Evaluation,match trigram bigram unigrams text unigram bigram substring already matched trigram pas python main text list list containing sentence part speech tagged ngrams match list list containing part speech tagged trigram sentence main text first check see complete trigram ngrams match match trigram match return matched trigram sentence b check see first tuple unigram first two tuples bigram trigram match main text c unigram bigram form substring already matched trigram return anything otherwise return bigram unigram match sentence output condition b give u bigram match wrong output would following ugly code work okay toy example wa wondering anyone streamlined approach
Question about Latent Dirichlet Allocation (MALLET),"<p>Honestly, I'm not familiar with LDA, but am required to use MALLET's topic modeling for one of my projects.</p>

<p>My question is: given a set of documents within a specific timestamp as the training data for the topic model, how appropriate is it to use the model (using the inferencer) to track the topic trends, for documents + or -  the training data's timestamp. I mean, is the topic distributions being provided by MALLET a suitable metric to track the popularity of the topics over time if during the model building stage, we only provide a subset of the dataset I am required to analyze.</p>

<p>thanks.   </p>
",Training and Model Evaluation,question latent dirichlet allocation mallet honestly familiar lda required use mallet topic modeling one project question given set document within specific timestamp training data topic model appropriate use model using inferencer track topic trend document training data timestamp mean topic distribution provided mallet suitable metric track popularity topic time model building stage provide subset dataset required analyze thanks
Using Lingpipe for word-level language model,"<p>I have been trying to get a word-level language model to work on lingpipe. All the examples and tutorials I have come across show the character-n-gram model. How to I go about using lingpipe to train a word-level model and then use that model to test it on other documents?</p>

<p>Additionally, I noticed that TokenizedLM is not serializable. Is there no way I can save it and load it later without having to go through re-training every time?</p>

<p>Lastly, are there any other frameworks/tools that will allow me to do this without any coding on my part?</p>
",Training and Model Evaluation,using lingpipe word level language model trying get word level language model work lingpipe example tutorial come across show character n gram model go using lingpipe train word level model use model test document additionally noticed tokenizedlm serializable way save load later without go training every time lastly framework tool allow without coding part
Large scale na&#239;ve Bayes classifier with top-k output,"<p>I need a library for naïve Bayes large scale, with millions of training examples and +100k binary features. It must be an online version (updatable after training). I also need top-k output, that is multiple classifications for a single instance. Accuracy is not very important.</p>

<p>The purpose is an automatic text categorization application.</p>

<p>Any suggestions for a good library is very appreciated.</p>

<p>EDIT: The library should preferably be in Java.</p>
",Training and Model Evaluation,large scale na bayes classifier top k output need library na bayes large scale million training example k binary feature must online version updatable training also need top k output multiple classification single instance accuracy important purpose automatic text categorization application suggestion good library appreciated edit library preferably java
Is POS tagging deterministic?,"<p>I have been trying to wrap my head around why this is happening but am hoping someone can shed some light on this. I am trying to tag the following text:</p>

<pre><code>ae0.475      X  mod 
ae0.842      X  mod
ae0.842      X  mod 
ae0.775      X  mod 
</code></pre>

<p>using the following code:</p>

<pre><code>import nltk

file = open(""test"", ""r"")

for line in file:
        words = line.strip().split(' ')
        words = [word.strip() for word in words if word != '']
        tags = nltk.pos_tag(words)
        pos = [tags[x][1] for x in range(len(tags))]
        key = ' '.join(pos)
        print words, "" : "", key
</code></pre>

<p>and am getting the following result:</p>

<pre><code>['ae0.475', 'X', 'mod']  :  NN NNP NN
['ae0.842', 'X', 'mod']  :  -NONE- NNP NN
['ae0.842', 'X', 'mod']  :  -NONE- NNP NN
['ae0.775', 'X', 'mod']  :  NN NNP NN
</code></pre>

<p>And I don't get it. Does anyone know what is the reason for this inconsistency? I am not very particular about the accuracy about the pos tagging because I am attempting to extract some templates but it seems to be using different tags at different instances for a word that looks ""almost"" the same.</p>

<p>As a solution, I replaced all numbers with 1 and solved the problem:</p>

<pre><code>['ae1.111', 'X', 'mod']  :  NN NNP NN
['ae1.111', 'X', 'mod']  :  NN NNP NN
['ae1.111', 'X', 'mod']  :  NN NNP NN
['ae1.111', 'X', 'mod']  :  NN NNP NN
</code></pre>

<p>but am curious why it tagged the instance with different tags in my first case. Any suggestions?</p>
",Training and Model Evaluation,po tagging deterministic trying wrap head around happening hoping someone shed light trying tag following text using following code getting following result get doe anyone know reason inconsistency particular accuracy po tagging attempting extract template seems using different tag different instance word look almost solution replaced number solved problem curious tagged instance different tag first case suggestion
Python NLTK code snippet to train a classifier (naive bayes) using feature frequency,"<p>I was wondering if anyone could help me through a code snippet that demonstrates how to train Naive Bayes classifier using a feature frequency method as opposed to feature presence.</p>

<p>I presume the below as shown in Chap 6 <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch06.html#document-classify-all-words"" rel=""nofollow noreferrer"">link text</a> refers to creating a featureset using Feature Presence (FP) -</p>

<pre><code>def document_features(document): 
    document_words = set(document) 

    features = {}
    for word in word_features:
        features['contains(%s)' % word] = (word in document_words)

    return features
</code></pre>

<p>Please advice</p>
",Training and Model Evaluation,python nltk code snippet train classifier naive bayes using feature frequency wa wondering anyone could help code snippet demonstrates train naive bayes classifier using feature frequency method opposed feature presence presume shown chap link text refers creating featureset using feature presence fp please advice
"Inter-rater agreement (Fleiss&#39; Kappa, Krippendorff&#39;s Alpha etc) Java API?","<p>I am working on building a Question Classification/Answering corpus as a part of my masters thesis. I'm looking at evaluating my expected answer type taxonomy with respect to inter-rater agreement/reliability, and I was wondering: Does anybody know of any decent (preferably free) Java API(s) that can do this?</p>

<p>I'm reasonably certain all I need is Fleiss' Kappa and Krippendorff's Alpha at this point.</p>

<p>Weka provides a kappa statistic in it's evaluation package, but I think it can only evaluate a classifier and I'm not at that stage yet (because I'm still building the data set and classes).</p>

<p>Thanks.</p>
",Training and Model Evaluation,inter rater agreement fleiss kappa krippendorff alpha etc java api working building question classification answering corpus part master thesis looking evaluating expected answer type taxonomy respect inter rater agreement reliability wa wondering doe anybody know decent preferably free java api reasonably certain need fleiss kappa krippendorff alpha point weka provides kappa statistic evaluation package think evaluate classifier stage yet still building data set class thanks
"Difference between feature selection, feature extraction, feature weights ","<p>I am slightly confused as to what ""feature selection / extractor / weights"" mean and the difference between them. As I read the literature sometimes I feel lost as I find the term used quite loosely, my primary concerns are -- </p>

<ol>
<li><p>When people talk of Feature Frequency, Feature Presence - is it feature selection?</p></li>
<li><p>When people talk of algorithms such as Information Gain, Maximum Entropy - is it still feature selection. </p></li>
<li><p>If I train the classifier - with a feature set that asks the classifier to note the position of a word within a document as an example - would one still call this feature selection?</p></li>
</ol>

<p>Thanks
Rahul Dighe</p>
",Training and Model Evaluation,difference feature selection feature extraction feature weight slightly confused feature selection extractor weight mean difference read literature sometimes feel lost find term used quite loosely primary concern people talk feature frequency feature presence feature selection people talk algorithm information gain maximum entropy still feature selection train classifier feature set asks classifier note position word within document example would one still call feature selection thanks rahul dighe
understanding semcor corpus structure h,"<p>I'm learning NLP.  I currently playing with Word Sense Disambiguation.  I'm planning to use the semcor corpus as training data but I have trouble understanding the xml structure.  I tried googling but did not get any resource describing the content structure of semcor.</p>

<pre><code>&lt;s snum=""1""&gt;
&lt;wf cmd=""ignore"" pos=""DT""&gt;The&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""group"" lexsn=""1:03:00::"" pn=""group"" pos=""NNP"" rdf=""group"" wnsn=""1""&gt;Fulton_County_Grand_Jury&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""say"" lexsn=""2:32:00::"" pos=""VB"" wnsn=""1""&gt;said&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""friday"" lexsn=""1:28:00::"" pos=""NN"" wnsn=""1""&gt;Friday&lt;/wf&gt;
&lt;wf cmd=""ignore"" pos=""DT""&gt;an&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""investigation"" lexsn=""1:09:00::"" pos=""NN"" wnsn=""1""&gt;investigation&lt;/wf&gt;
&lt;wf cmd=""ignore"" pos=""IN""&gt;of&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""atlanta"" lexsn=""1:15:00::"" pos=""NN"" wnsn=""1""&gt;Atlanta&lt;/wf&gt;
&lt;wf cmd=""ignore"" pos=""POS""&gt;'s&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""recent"" lexsn=""5:00:00:past:00"" pos=""JJ"" wnsn=""2""&gt;recent&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""primary_election"" lexsn=""1:04:00::"" pos=""NN"" wnsn=""1""&gt;primary_election&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""produce"" lexsn=""2:39:01::"" pos=""VB"" wnsn=""4""&gt;produced&lt;/wf&gt;
&lt;punc&gt;``&lt;/punc&gt;
&lt;wf cmd=""ignore"" pos=""DT""&gt;no&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""evidence"" lexsn=""1:09:00::"" pos=""NN"" wnsn=""1""&gt;evidence&lt;/wf&gt;
&lt;punc&gt;''&lt;/punc&gt;
&lt;wf cmd=""ignore"" pos=""IN""&gt;that&lt;/wf&gt;
&lt;wf cmd=""ignore"" pos=""DT""&gt;any&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""irregularity"" lexsn=""1:04:00::"" pos=""NN"" wnsn=""1""&gt;irregularities&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""take_place"" lexsn=""2:30:00::"" pos=""VB"" wnsn=""1""&gt;took_place&lt;/wf&gt;
&lt;punc&gt;.&lt;/punc&gt;
&lt;/s&gt;
</code></pre>

<ul>
<li>I'm assuming wnsn is 'word sense'.  Is it correct?</li>
<li>What does the attribute lexsn mean? How does it map to wordnet?</li>
<li>What does the attribute pn refer to? (third line)</li>
<li>How is the rdf attribute assigned? (again third line)</li>
<li>In general, what are the possible attributes?</li>
</ul>
",Training and Model Evaluation,understanding semcor corpus structure h learning nlp currently playing word sense disambiguation planning use semcor corpus training data trouble understanding xml structure tried googling get resource describing content structure semcor assuming wnsn word sense correct doe attribute lexsn mean doe map wordnet doe attribute pn refer third line rdf attribute assigned third line general possible attribute
Better distance metrics besides Levenshtein for ordered word sets and subsequent clustering,"<p>I am trying to solve a problem that involves comparing large numbers of word sets , each of which contains a large, ordered number of words from a set of words (totaling around 600+, very high dimensionality!) for similarity and then clustering them into distinct groupings. The solution needs to be as unsupervised as possible.</p>

<p>The data looks like</p>

<p>[Apple, Banana, Orange...]<br>
[Apple, Banana, Grape...] <br>
[Jelly, Anise, Orange...]<br>
[Strawberry, Banana, Orange...]<br>
...etc</p>

<p>The order of the words in each set matters ([Apple, Banana, Orange] is distinct from [Apple, Orange, Banana]</p>

<p>The approach I have been using so far has been to use Levenshtein distance (limited by a distance threshold) as a metric calculated in a Python script with each word being the unique identifier, generate a similarity matrix from the distances, and throwing that matrix into k-Mediods in KNIME for the groupings.</p>

<p>My questions are:</p>

<ul>
<li>Is Levenshtein the most appropriate distance metric to use for this problem?</li>
<li>Is mean/medoid prototype clustering the best way to go about the groupings?</li>
<li>I haven't yet put much thought into validating the choice for 'k' in the clustering. Would evaluating an SSE curve of the clustering be the best way to go about this?</li>
<li>Are there any flaws in my methodology? </li>
<li>As an extension to the solution in the future, given training data, would anyone happen to have any ideas for going about assigning probabilities to cluster assignments? For example, set 1 has a 80% chance of being in cluster 1, etc. </li>
</ul>

<p>I hope my questions don't seem too silly or the answers painfully obvious, I'm relatively new to data mining.</p>

<p>Thanks!</p>
",Training and Model Evaluation,better distance metric besides levenshtein ordered word set subsequent clustering trying solve problem involves comparing large number word set contains large ordered number word set word totaling around high dimensionality similarity clustering distinct grouping solution need unsupervised possible data look like apple banana orange apple banana grape jelly anise orange strawberry banana orange etc order word set matter apple banana orange distinct apple orange banana approach using far ha use levenshtein distance limited distance threshold metric calculated python script word unique identifier generate similarity matrix distance throwing matrix k mediods knime grouping question levenshtein appropriate distance metric use problem mean medoid prototype clustering best way go grouping yet put much thought validating choice k clustering would evaluating sse curve clustering best way go flaw methodology extension solution future given training data would anyone happen idea going assigning probability cluster assignment example set ha chance cluster etc hope question seem silly answer painfully obvious relatively new data mining thanks
Evaluate the content of a paragraph,"<p>We are building a database of scientific papers and performing analysis on the abstracts. The goal is to be able to say ""Interest in this topic has gone up 20% from last year"". I've already tried key word analysis and haven't really liked the results. So now I am trying to move onto phrases and proximity of words to each other and realize I'm am in over my head. Can anyone point me to a better solution to this, or at very least give me a good term to google to learn more? </p>

<p>The language used is python but I don't think that really affects your answer. Thanks in advance for the help.</p>
",Training and Model Evaluation,evaluate content paragraph building database scientific paper performing analysis abstract goal able say interest topic ha gone last year already tried key word analysis really liked result trying move onto phrase proximity word realize head anyone point better solution least give good term google learn language used python think really affect answer thanks advance help
Person names disambiguation,"<p>I am currently doing a project on person name disambiguation. The idea behind the project, that it will be able to identify the correct person, when there are multiple people with the same name. I have used wikipedia for this. I want to evaluate my project on some standard data. I am looking for some testing data. I am not familiar with popular names in wikipedia. Any idea, where I can find this data? I am not looking for vast amounts of data. I am just looking for some 100-500 examples.</p>

<p>Thank you</p>

<p>Adding more information to the question.</p>

<p>What I am looking for is of people with same names but are actually different. For ex, Michael Jordon is a famous basketball player and there is also a statistician with that name. I am looking for examples like this.</p>

<p><a href=""http://en.wikipedia.org/wiki/Michael_Jordan"" rel=""nofollow"">http://en.wikipedia.org/wiki/Michael_Jordan</a>
http://en.wikipedia.org/wiki/Michael_I._Jordan</p>

<p>Hope, you understand the question now.</p>
",Training and Model Evaluation,person name disambiguation currently project person name disambiguation idea behind project able identify correct person multiple people name used wikipedia want evaluate project standard data looking testing data familiar popular name wikipedia idea find data looking vast amount data looking example thank adding information question looking people name actually different ex michael jordon famous basketball player also statistician name looking example like hope understand question
Looking for artificial intelligence (AI) cookbook reader research,"<p>I am looking for research (published) on AI techniques for reading cookbook recipes. Recipes are a very limited domain that might be doable in a natural language recognition engine with some degree of accuracy.</p>

<p>I have in mind writing a program that would allow copy/pasting a recipe from a web browser into the AI and having it determine the title, author, ingredients, instructions, nutritional information, etc. by ""reading"" the recipe. I would also like to be able to process PDF files (I have a large collection), maybe also just using copy/paste.</p>

<p>The output will be some kind of (standard) XML-based format that can be read by a recipe organizer.</p>

<p>I have in mind PhD or Masters-level work.</p>
",Training and Model Evaluation,looking artificial intelligence ai cookbook reader research looking research published ai technique reading cookbook recipe recipe limited domain might doable natural language recognition engine degree accuracy mind writing program would allow copy pasting recipe web browser ai determine title author ingredient instruction nutritional information etc reading recipe would also like able process pdf file large collection maybe also using copy paste output kind standard xml based format read recipe organizer mind phd master level work
Ngram IDF smoothing,"<p>I am trying to use IDF scores to find interesting phrases in my pretty huge corpus of documents.<br>
I basically need something like Amazon's Statistically Improbable Phrases, i.e. phrases that distinguish a document from all the others<br>
The problem that I am running into is that some (3,4)-grams in my data which have super-high idf actually consist of component unigrams and bigrams which have really low idf..<br>
For example, ""you've never tried"" has a very high idf, while each of the component unigrams have very low idf..<br>
I need to come up with a function that can take in document frequencies of an n-gram and all its component (n-k)-grams and return a more meaningful measure of how much this phrase will distinguish the parent document from the rest.<br>
If I were dealing with probabilities, I would try interpolation or backoff models.. I am not sure what assumptions/intuitions those models leverage to perform well, and so how well they would do for IDF scores.<br>
Anybody has any better ideas?</p>
",Training and Model Evaluation,ngram idf smoothing trying use idf score find interesting phrase pretty huge corpus document basically need something like amazon statistically improbable phrase e phrase distinguish document others problem running gram data super high idf actually consist component unigrams bigram really low idf example never tried ha high idf component unigrams low idf need come function take document frequency n gram component n k gram return meaningful measure much phrase distinguish parent document rest dealing probability would try interpolation backoff model sure assumption intuition model leverage perform well well would idf score anybody ha better idea
"The lines that stand out in a file, but aren&#39;t exact duplicates","<p>I'm combing a webapp's log file for statements that stand out.</p>

<p>Most of the lines are similar and uninteresting. I'd pass them through Unix <code>uniq</code>, however that filters nothing, as all the lines are slightly different: they all have a different timestamp, similar statements might print a different user ID, etc.</p>

<p>What's a way and/or tool to get just the lines that are notably different from any other? (But, again, not precise duplicates)</p>

<p>I was thinking about playing with Python's <a href=""http://docs.python.org/library/difflib.html"" rel=""nofollow noreferrer"">difflib</a> but that seems geared toward diffing two files, rather than all pairs of lines in the same file.</p>

<p>[EDIT]</p>

<p>I assumed the solution would give a uniqueness score for each line. So by ""notably different"" I meant, I choose a threshold that the uniqueness score must exceed for any line to be included in the output.</p>

<p>Based on that, if there are other viable ways to define it, please discuss. Also, the method doesn't have to have 100% accuracy and recall.</p>

<p>[/EDIT]</p>

<p>Examples:</p>

<p>I'd prefer answers that are as general purpose as possible. I know I can strip away the timestamp at the beginning. Stripping the end is more challenging, as its language may be absolutely unlike anything else in the file. These sorts of details are why I shied from concrete examples before, but because some people asked...</p>

<p>Similar 1:</p>

<pre><code>2009-04-20 00:03:57 INFO  com.foo.Bar - URL:/graph?id=1234
2009-04-20 00:04:02 INFO  com.foo.Bar - URL:/graph?id=asdfghjk
</code></pre>

<p>Similar 2:</p>

<pre><code>2009-04-20 00:05:59 INFO  com.baz.abc.Accessor - Cache /path/to/some/dir hits: 3466 / 16534, 0.102818% misses
2009-04-20 00:06:00 INFO  com.baz.abc.Accessor - Cache /path/to/some/different/dir hits: 4352685 / 271315, 0.004423% misses
</code></pre>

<p>Different 1:</p>

<pre><code>2009-04-20 00:03:57 INFO  com.foo.Bar - URL:/graph?id=1234
2009-04-20 00:05:59 INFO  com.baz.abc.Accessor - Cache /path/to/some/dir hits: 3466 / 16534, 0.102818% misses
</code></pre>

<p>In the Different 1 case, I'd like both lines returned but not other lines like them. In other words, those 2 lines are distinct types (then I can later ask for only statistically rare line types). The edit distance is much bigger between those two, for one thing.</p>
",Training and Model Evaluation,line stand file exact duplicate combing webapp log file statement stand line similar uninteresting pas unix however filter nothing line slightly different different timestamp similar statement might print different user id etc way tool get line notably different precise duplicate wa thinking playing python difflib seems toward diffing two file rather pair line file edit assumed solution would give uniqueness score line notably different meant choose threshold uniqueness score must exceed line included output based viable way define please discus also method accuracy recall edit example prefer answer general purpose possible know strip away timestamp beginning stripping end challenging language may absolutely unlike anything else file sort detail shied concrete example people asked similar similar different different case like line returned line like word line distinct type later ask statistically rare line type edit distance much bigger two one thing
