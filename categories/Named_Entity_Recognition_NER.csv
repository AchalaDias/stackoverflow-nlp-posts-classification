Title,Description,category,combined_text
Extracting Values from text using Python,"<p>I have long list of text from a pandas dataframe. However I am going to explain what I want to do just with one example. I have two pre-defined lists. These two-predefined list can be used for extracting the values I need from the text: </p>

<pre><code>ingredient_name_list = ['butter', 'butter oil', 'cheese', 'cheese food', 'cheese spread', 'cream', 'eggnog', 'sour dressing', 'milk', 'yogurt', 'cream substitute', 'dessert topping', 'sour cream', 'milk substitutes', 'milk shakes', 'whey', 'egg', 'egg substitute', 'cheese substitute', 'cheese sauce', 'parmesan cheese topping', 'reddi wip fat free whipped topping', 'cheese product', 'protein supplement', 'dulce de leche', 'ice cream', 'ice cream sandwich', 'ice cream cookie sandwich', 'ice cream cone', 'fat free ice cream', 'milk dessert bar', 'nutritional supplement for people with diabetes', 'ice cream bar', 'kefir', 'ice cream sundae cone', 'light ice cream', 'spices', 'basil', 'dill weed', 'mustard', 'salt', 'vinegar', 'thyme', 'vanilla extract', 'capers', 'horseradish', 'rosemary', 'peppermint', 'spearmint', 'seasoning mix', 'clif z bar', 'babyfood', 'zwieback', 'infant formula', 'toddler formula', 'toddler drink', 'child formula', 'fat', 'lard', 'salad dressing', 'sandwich spread', 'shortening', 'oil', 'margarine', 'vegetable oil', 'soybean', 'margarine-like', 'fish oil', 'animal fat', 'margarine-like spread with yogurt', 'margarine spread', 'margarine-like shortening', 'margarine-like spread', 'margarine-like vegetable-oil spread', 'dressing', 'mayonnaise', 'chicken', 'canada goose', 'duck', 'goose', 'guinea hen', 'pheasant', 'quail', 'squab', 'turkey', 'pate de foie gras', 'turkey and gravy', 'turkey breast', 'turkey thigh', 'turkey roast', 'turkey sticks', 'poultry', 'chicken patty', 'chicken breast tenders', 'ruffed grouse', 'emu', 'ostrich', 'turkey from whole', 'soup', ""campbell's"", 'sauce', 'gravy', 'split pea soup', 'split pea with ham soup', ""campbell's chunky"", 'smart soup', 'fish broth', 'potato soup', 'barbecue loaf', 'beerwurst', 'sausage', 'blood sausage', 'bockwurst', 'bologna', 'bratwurst', 'braunschweiger (a liver sausage)', 'cheesefurter', 'chicken spread', 'corned beef loaf', 'dutch brand loaf', 'frankfurter', 'ham', 'ham salad spread', 'headcheese', 'knackwurst', 'lebanon bologna', 'liver cheese', 'liver sausage', 'roast beef', 'luncheon meat', 'mortadella', 'olive loaf', 'pastrami', 'pate', 'peppered loaf', 'pepperoni', 'pickle and pimiento loaf', 'polish sausage', 'luxury loaf', ""mother's loaf"", 'picnic loaf', 'pork sausage', 'poultry salad sandwich spread', 'salami', 'thuringer', 'honey roll sausage', 'luncheon sausage', 'oscar mayer', 'bacon', 'liverwurst spread', 'roast beef spread', 'swisswurst', 'bacon and beef sticks', 'yachtwurst', 'chicken breast', 'kielbasa', 'macaroni and cheese loaf', 'scrapple', 'meatballs', 'cereals', 'cereals ready-to-eat', 'milk and cereal bar', 'rice and wheat cereal bar', 'incaparina', 'acerola', 'acerola juice', 'apples', 'apple juice', 'applesauce', 'apricots', 'apricot nectar', 'avocados', 'bananas', 'blackberries', 'blackberry juice', 'cherries', 'blueberries', 'boysenberries', 'breadfruit', 'carambola', 'carissa', 'cherimoya', 'crabapples', 'cranberries', 'cranberry sauce', 'cranberry-orange relish', 'currants', 'custard-apple', 'dates', 'elderberries', 'figs', 'fruit cocktail', 'fruit salad', 'gooseberries', 'goji berries', 'grapefruit', 'grapefruit juice', 'grapes', 'grape juice', 'groundcherries', 'guavas', 'guava sauce', 'jackfruit', 'java-plum', 'jujube', 'kiwifruit', 'kumquats', 'lemons', 'lemon juice', 'lemon juice from concentrate', 'lemon peel', 'limes', 'lime juice', 'litchis', 'loganberries', 'longans', 'loquats', 'mammy-apple', 'mangos', 'mangosteen', 'mango', 'melons', 'melon balls', 'mulberries', 'nectarines', 'oheloberries', 'olives', 'oranges', 'orange juice', 'orange peel', 'orange-grapefruit juice', 'tangerines', 'tangerine juice', 'papayas', 'papaya', 'papaya nectar', 'passion-fruit', 'passion-fruit juice', 'peaches', 'peach nectar', 'pears', 'pear nectar', 'persimmons', 'pineapple', 'pineapple juice', 'pitanga', 'plantains', 'plums', 'pomegranates', 'prickly pears', 'prunes', 'prune juice', 'pummelo', 'quinces', 'raisins', 'rambutan', 'raspberries', 'rhubarb', 'roselle', 'rose-apples', 'sapodilla', 'sapote', 'soursop', 'strawberries', 'sugar-apples', 'tamarinds', 'watermelon', 'maraschino cherries', 'feijoa', 'durian', 'prune puree', 'candied fruit', 'abiyuch', 'rowal', 'clementines', 'guanabana nectar', 'guava nectar', 'mango nectar', 'tamarind nectar', 'pomegranate juice', 'juice', 'nance', 'naranjilla (lulo) pulp', 'horned melon ', 'orange pineapple juice blend', 'fruit juice smoothie', 'cranberry juice blend', 'ruby red grapefruit juice blend (grapefruit', 'baobab powder', 'cherry juice', 'raspberry juice concentrate', 'pork', 'canadian bacon', 'hormel', 'hormel always tender', 'hormel canadian style bacon', 'pork loin', 'alfalfa seeds', 'amaranth leaves', 'arrowhead', 'artichokes', 'asparagus', 'balsam-pear (bitter gourd)', 'bamboo shoots', 'beans', 'lima beans', 'mung beans', 'beets', 'beet greens', 'broadbeans', 'broccoli', 'broccoli raab', 'brussels sprouts', 'burdock root', 'butterbur', 'cabbage', 'cardoon', 'carrots', 'cassava', 'cauliflower', 'celeriac', 'celery', 'celtuce', 'chard', 'chayote', 'chicory', 'chicory greens', 'chicory roots', 'chives', 'chrysanthemum', 'collards', 'coriander (cilantro) leaves', 'corn', 'corn with red and green peppers', 'cornsalad', 'cowpeas (blackeyes)', 'cowpeas', 'yardlong bean', 'cress', 'cucumber', 'dandelion greens', 'eggplant', 'edamame', 'endive', 'escarole', 'garlic', 'ginger root', 'gourd', 'drumstick leaves', 'hyacinth-beans', 'jerusalem-artichokes', ""jew's ear"", 'pepeao', 'jute', 'kale', 'kanpyo', 'mushrooms', 'kohlrabi', 'lambsquarters', 'leeks', 'lentils', 'lettuce', 'lotus root', 'mountain yam', 'mustard greens', 'mustard spinach', 'new zealand spinach', 'okra', 'onions', 'onion rings', 'parsley', 'parsnips', 'peas', 'peas and carrots', 'peas and onions', 'peppers', 'pigeonpeas', 'pokeberry shoots', 'potatoes', 'potato puffs', 'potato wedges', 'potato flour', 'potato salad', 'pumpkin flowers', 'pumpkin leaves', 'pumpkin', 'pumpkin pie mix', 'purslane', 'radishes', 'rutabagas', 'salsify', 'sauerkraut', 'seaweed', 'sesbania flower', 'soybeans', 'spinach', 'squash', 'succotash', 'water convolvulus', 'sweet potato leaves', 'sweet potato', 'taro', 'taro leaves', 'taro shoots', 'tomatoes', 'tomato juice', 'tomato products', 'tomato powder', 'tree fern', 'turnips', 'turnip greens', 'turnip greens and turnips', 'vegetable juice cocktail', 'vegetables', 'vinespinach', 'waterchestnuts', 'watercress', 'waxgourd', 'winged beans', 'winged bean leaves', 'winged bean tuber', 'yam', 'yambean (jicama)', 'borage', 'dock', 'eppaw', 'drumstick pods', 'shallots', 'carrot juice', 'corn pudding', 'spinach souffle', 'potato pancakes', 'radish seeds', 'carrot', 'arrowroot', 'chrysanthemum leaves', 'winged bean', 'ketchup', 'pickles', 'mushroom', 'pimento', 'pickle relish', 'catsup', 'radicchio', 'tomatillos', 'fennel', 'arugula', 'hearts of palm', 'nopales', 'lemon grass (citronella)', 'grape leaves', 'pepper', 'epazote', 'fireweed', 'malabar spinach', 'fungi', 'wasabi', 'yautia (tannier)', 'fiddlehead ferns', 'seeds', 'nuts', 'beef', 'alcoholic beverage', 'beverages', 'carbonated beverage', 'cocoa mix', 'cranberry juice cocktail', 'alcoholic beverages', 'lemonade', 'limeade', 'malt beverage', 'shake', 'strawberry-flavor beverage mix', 'water', 'whiskey sour mix', 'fish', 'crustaceans', 'mollusks', 'salmon nuggets', 'salmon', 'yokan', 'broadbeans (fava beans)', 'carob flour', 'chickpeas (garbanzo beans', 'chili with beans', 'hyacinth beans', 'lupins', 'mothbeans', 'noodles', 'mungo beans', 'peanuts', 'peanut butter', 'peanut flour', 'pigeon peas (red gram)', 'refried beans', 'meat extender', 'soy flour', 'soy meal', 'soymilk', 'soy protein ', 'tofu', 'okara', 'yardlong beans', 'hummus', 'falafel', 'veggie burgers or soyburgers', 'peanut spread', 'chickpea flour', 'mori-nu', 'frijoles rojos volteados (refried beans', 'tempeh', 'vitasoy usa', 'soymilk (all flavors)', 'silk plain', 'silk vanilla', 'silk chocolate', 'silk light plain', 'silk light vanilla', 'silk light chocolate', 'silk plus omega-3 dha', 'silk plus for bone health', 'silk plus fiber', 'silk unsweetened', 'silk very vanilla', 'silk nog', 'silk chai', 'silk mocha', 'silk coffee', 'silk vanilla soy yogurt', 'vitasoy usa organic nasoya', 'vitasoy usa nasoya', 'vitasoy usa organic nasoya sprouted', 'vitasoy usa azumaya', 'peanut butter with omega-3', 'soy protein concentrate', 'soy protein isolate', 'soy sauce made from soy and wheat (shoyu)', 'soy sauce', 'veal', 'lamb', 'game meat', 'bison', 'game meat ', 'bagels', 'biscuits', 'bread', 'cake', 'cookies', 'puff pastry', 'crackers', 'cracker', 'cream puff shell', 'croissants', 'croutons', 'danish pastry', 'doughnuts', 'muffins', 'french toast', 'hush puppies', 'ice cream cones', 'pancakes plain', 'pancakes', 'pie', 'pie crust', 'phyllo dough', 'popovers', 'rolls', 'strudel', 'sweet rolls', 'taco shells', 'toaster pastries', 'tortillas', 'waffles', 'wonton wrappers ', 'leavening agents', 'english muffins', 'tart', 'archway home style cookies', 'artificial blueberry muffin mix', 'kraft', 'george weston bakeries', 'keebler', 'continental mills', 'mckee baking', 'martha white foods', 'mission foods', 'nabisco', 'pillsbury', 'pillsbury grands', 'pillsbury golden layer buttermilk biscuits', 'kraft foods', 'heinz', 'interstate brands corp', 'waffle', 'muffin', 'tostada shells', 'keikitos (muffins)', 'pan dulce', 'pastry', 'garlic bread', 'cinnamon buns', 'cream puff', 'focaccia', 'schiff', 'candies', 'snacks', 'fruit syrup', 'topping', 'syrup', 'baking chocolate', 'ice creams', 'desserts', 'sherbet', 'syrups', 'puddings', 'chocolate-flavored hazelnut spread~^~chocolate-flavored hazelnut sprd~^^^~y~^^0^^6.25^^^', 'toppings', 'chewing gum~^~chewing gum~^~bubble gum~^^~y~^^0^^6.25^1.82^8.37^3.70', 'cocoa', 'egg custards', 'gelatin desserts', 'gelatins', 'flan', 'rennin', 'frozen novelties', 'frostings', 'frozen yogurts', 'fruit butters', 'honey', 'jams and preserves', 'jellies', 'marmalade', 'molasses', 'pectin', 'pie fillings', 'pudding', 'sugars', 'sweeteners', 'snack', 'tortilla chips', 'cheese puffs and twists', 'popcorn', 'potato chips', 'chocolate', 'sugar', 'sweetener', 'jams', 'amaranth grain', 'arrowroot flour', 'barley', 'buckwheat', 'buckwheat groats', 'buckwheat flour', 'bulgur', 'corn grain', 'corn bran', 'corn flour', 'cornmeal', 'cornstarch', 'couscous', 'hominy', 'millet', 'oat bran', 'quinoa', 'rice', 'oats', 'rice bran', 'rice flour', 'rye grain', 'rye flour', 'semolina', 'sorghum grain', 'tapioca', 'triticale', 'triticale flour', 'wheat', 'wheat bran', 'wheat germ', 'wheat flour', 'wild rice', 'pasta', 'macaroni', 'spaghetti', 'wheat flours', 'barley flour or meal', 'barley malt flour', 'oat flour', 'rice noodles', 'spelt', 'teff', 'millet flour', 'sorghum flour', 'fast foods', 'burger king', 'fast food', 'chick-fil-a', 'school lunch', 'subway', 'pizza', ""mcdonald's"", ""wendy's"", 'taco bell', 'bacon ranch salad with crispy chicken', 'popeyes', 'kfc', 'yogurt parfait', 'digiorno pizza', ""wend'ys"", 'pizza hut', ""arby's"", 'macaroni and cheese', 'spaghetti with meat sauce', 'beef macaroni with tomato sauce', 'pasta with sliced franks in tomato sauce', 'turkey pot pie', 'beef pot pie', 'hot pockets', ""hot pockets ham 'n cheese stuffed sandwich"", 'ravioli', 'tortellini', 'chili con carne with beans', 'beef stew', 'chicken pot pie', 'lasagna', 'chili', 'pasta with tomato sauce', 'lasagna with meat &amp; sauce', 'burrito', 'egg rolls', 'rice bowl with chicken', 'macaroni and cheese dinner with dry sauce mix', 'lean pockets', 'potato salad with egg~^~potato salad w/ egg~^^^~y~^^0^^^^^', 'pulled pork in barbecue sauce~^~pulled pork in barbecue sau~^^^^^0^^^^^', 'corn dogs', 'lasagna with meat sauce', 'chicken tenders', 'rice-a-roni', 'rice and vermicelli mix', 'beef composite', 'formulated bar', 'pretzels', 'rice crackers', 'dip', 'cookie', ""andrea's"", 'crunchmaster', 'glutino', ""mary's gone crackers"", 'pepperidge farm', ""rudi's"", 'sage valley', 'schar', ""udi's"", ""van's"", 'sweet potatoes', 'sweet potato puffs', 'vegetable juice', 'taquitos', 'pasta mix', 'yellow rice with seasoning', 'pizza rolls', 'potsticker or wonton', 'macaroni or noodles with cheese', 'turnover', 'spanish rice mix', 'rice mix', 'dumpling', 'salisbury steak with gravy', 'hungry man', 'banquet', 'jimmy dean', 'agutuk', 'ascidians ', 'bear', 'whale', 'caribou', 'stew/soup', 'chiton', 'cloudberries', 'cockles', 'cranberry', 'huckleberries', 'stew', 'moose', 'mashu roots', 'mouse nuts', 'octopus', 'seal', 'oopah (tunicate)', 'owl', 'sea cucumber', 'sourdock', 'squirrel', 'tea', 'walrus', 'deer (venison)', 'willow', 'mush', 'melon', 'chilchen ', 'mutton', 'frybread', 'tortilla', 'tamales', 'salmonberries', 'elk', 'buffalo', 'chokecherries', 'steelhead trout', 'acorn stew ', 'smelt', 'corned beef and potatoes in tortilla', 'tennis bread', 'agave', 'cattail', 'prairie turnips', 'rose hips', 'stinging nettles', 'pinon nuts', 'sea lion', 'wocas', 'hazelnuts', 'piki bread', ""applebee's"", ""t.g.i. friday's"", 'restaurant', 'cracker barrel', ""denny's"", 'olive garden', ""carrabba's italian grill"", 'on the border', 'creamy dressing', 'imitation cheese', 'milk dessert', 'whipped topping', 'granola bar', 'vegetable oil-butter spread', 'pork sausage rice links', 'papad', 'rice cake', 'gums', 'chewing gum', 'fluid replacement', 'breakfast bars', 'vermicelli', 'luncheon slices', 'vegetarian fillets', 'vegetarian meatloaf or patties', 'beverage', 'bacon bits', 'butter replacement', 'tomato sauce', 'whipped cream substitute', 'eggs', 'dove', 'tomato and vegetable juice', 'cranberry juice', 'yeast extract spread', 'tofu yogurt', 'jellyfish', 'breakfast bar', 'mayonnaise dressing', 'vital wheat gluten', 'frog legs', 'turtle', 'daikon', 'jicama', 'beatgreens', 'bokchoy', 'collard greens', 'microgreens', 'almonds', 'cashews', 'pecans', 'walnuts', 'blueberry', 'strawberry', 'barberries', 'raspberry', 'blackberry', 'boysenberry', 'lingonberries', 'lingonberry', 'elderberry', 'cloudberry', 'dewberry', 'goji berry', 'gooseberry']
quantity_list = ['cup','cups','teaspoon','teaspoons','tablespoon','tablespoons','lb','lbs']
</code></pre>

<p>The following is a text from that dataframe of a single row:</p>

<pre><code> 1 1/2  lbs    ground beef (a mixture of 1 pound ground beef and 1/2 pound ground pork will work well also)
1   large    egg
1/2  cup   grated parmesan cheese
1/3  cup    breadcrumbs (or use enough to hold the meat together (no dry breadcrumbs? just soak 3 slices of bread in the 1/3 )
1 -2   tablespoon   fresh minced garlic (or use 1 teaspoon garlic powder or to taste)
1 -2   teaspoon    salt (or to taste
1   teaspoon    fresh ground black pepper
1/3  cup    milk (can use up to 1/2 cup milk)
1/2  teaspoon    dried oregano (optional
1/4  cup   chopped fresh parsley (or 2 tablespoons dried parsley)
</code></pre>

<p>I obtained the above text using the following code:</p>

<pre><code>    import re
    text_value = df['ingredients']
    for i in text_value[0:1]:
        each_quantity_list = []
        each_unit_list = []
        each_ingradient_list = []
        my_list = i.split(',')
        rx = re.compile(r'\d+(?:/\d+)?')
        filtered = [item for item in my_list if rx.match(item)]
        for j in filtered:
            print (j)
            // Do stuff here
</code></pre>

<p>How can I extract the quantity, unit (cup,lb, etc.) and the name of the ingredient from the text so that I have the following:</p>

<pre><code>each_quantity_list  = ['1 1/2','1','1/2','1/3','1-2','1-2','1','1/3','1/2','1/4']
each_unit_list = ['lbs','cup','cup','cup','tablespoon','teaspoon','teaspoon','cup','teaspoon','cup']
each_ingradient_list = ['beef','egg','parmesan cheese','breadcrumbs','garlic','salt','black pepper','milk','oregano','parsley']
</code></pre>

<p>For clarity the below is the full code I used. But the problem of this code is it is not recognizing the quantity when it is something like 1 1/2, and also at times it is not getting the quantity like cup, cups, etc :</p>

<pre><code>for i in range(len(df)):
try:
    ingredient_list = []
    qunait_list = []
    value = df['ingredients'][i].split(',')
    rx = re.compile(r'\d+(?:/\d+)?')
    filtered = [item for item in value if rx.match(item)]
    for j in filtered:
        #print (j)
        each_ingredient = j.split()
        #print (each_ingredient)\\n
        for k in ingredient_name_list:
            if k in each_ingredient and each_ingredient[1] in quantity_list:
                quantity = each_ingredient[0]
                #print (quantity)
                #print (each_ingredient[1])
                #print (k)
                ingredient_list.append(k)
                qunait_list.append(str(quantity)+' '+str(each_ingredient[1]))
            elif k in each_ingredient and each_ingredient[1] not in quantity_list:
                int_quantity = each_ingredient[0]
                #print (int_quantity)\\n#print (each_ingredient[1])\\n#print (k)\\n
                ingredient_list.append(k)
                qunait_list.append(' '+str(quantity))
                #print (\\'----------------------------------------------------\\')\\n
    id_list.append(df['id'][i])
    #print(\""id\""+str(df[\\'id\\'][i]))\\n
    #print (\\'----------------------------------------------------\\')\\n
    ingre_list.append(ingredient_list)
    quan_list.append(qunait_list)
except:
    pass
</code></pre>

<p>I want to do this for all of them. Any help is really appreciated.</p>
",Named Entity Recognition (NER),extracting value text using python long list text panda dataframe however going explain want one example two pre defined list two predefined list used extracting value need text following text dataframe single row obtained text using following code extract quantity unit cup lb etc name ingredient text following clarity full code used problem code recognizing quantity something like also time getting quantity like cup cup etc want help really appreciated
Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does?,"<p>I'm using spaCy with the pl_core_news_lg model to extract named entities from Polish text. It correctly detects both organizations (ORG) and people's names (PER):</p>
<pre><code>import spacy

nlp = spacy.load(&quot;pl_core_news_lg&quot;)
text = &quot;Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google.&quot;

doc = nlp(text)
entities = [(ent.text, ent.label_) for ent in doc.ents]

print(entities)
</code></pre>
<p>Output:</p>
<pre><code>[('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')]
</code></pre>
<p>However, when I use Presidio with the pl_core_news_lg model and a configuration file, the recognizers do not correctly detect organizations (ORG) or PESEL numbers, even though they appear in the list of supported entities.</p>
<pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry
from presidio_analyzer.nlp_engine import NlpEngineProvider

provider = NlpEngineProvider(conf_file=&quot;path_to_my_file/nlp_config.yaml&quot;) 
nlp_engine = provider.create_engine()

print(f&quot;Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}&quot;)

supported_languages = list(nlp_engine.get_supported_languages())
registry = RecognizerRegistry(supported_languages=[&quot;pl&quot;])
registry.load_predefined_recognizers([&quot;pl&quot;])

print(f&quot;Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}&quot;)

analyzer = AnalyzerEngine(
    registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine
)

results = analyzer.analyze(text, &quot;pl&quot;)

for entity in results:
    print(f&quot;Found entity: {entity.entity_type} with score {entity.score}&quot;)
</code></pre>
<p>Output:</p>
<pre><code>Supported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION']
Supported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS']
</code></pre>
<p>Even though 'ORGANIZATION' and 'PL_PESEL' are listed (org should be listed in from NLP engine) as supported recognizers, Presidio does not detect them correctly in the text.</p>
<p>My config file:</p>
<pre><code>nlp_engine_name: spacy
models:
  - lang_code: pl
    model_name: pl_core_news_lg

ner_model_configuration:
  model_to_presidio_entity_mapping:
    persName: PERSON
    orgName: ORGANIZATION
#    orgName: ORG
    placeName: LOCATION
    geogName: LOCATION
    LOC: LOCATION
    GPE: LOCATION
    FAC: LOCATION
    DATE: DATE_TIME
    TIME: DATE_TIME
    NORP: NRP
    ID: ID
</code></pre>
<p>Why does Presidio fail to detect organizations (ORG) and PESEL numbers (PL_PESEL), while spaCy correctly detects them?</p>
",Named Entity Recognition (NER),doe presidio spacy nlp engine recognize organization pesel spacy doe using spacy pl core news lg model extract named entity polish text correctly detects organization org people name per output however use presidio pl core news lg model configuration file recognizers correctly detect organization org pesel number even though appear list supported entity output even though organization pl pesel listed org listed nlp engine supported recognizers presidio doe detect correctly text config file doe presidio fail detect organization org pesel number pl pesel spacy correctly detects
Spacy rules matching entities before text,"<p>I'm trying to write a spacy parser to extract the names and terms of a contract.
To do that, I've written a rule to extract the sellers and buyers, except it's extracting multiple times over a simple sentence.</p>
<p>Here's my rule</p>
<pre><code>[{&quot;label&quot;: &quot;seller&quot;, &quot;pattern&quot;: [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;:  &quot;{1,2}&quot;}, { &quot;OP&quot;: &quot;*&quot;}, {&quot;TEXT&quot;: &quot;seller&quot;}]},
{&quot;label&quot;: &quot;buyer&quot;, &quot;pattern&quot;: [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;: &quot;{1,2}&quot;}, { &quot;OP&quot;: &quot;*&quot;}, {&quot;TEXT&quot;: &quot;buyer&quot;}]},]
</code></pre>
<p>Which results in spans like this:</p>
<pre><code>span seller Text: john e. smith and wife judy c. smith, seller
span seller Text: e. smith and wife judy c. smith, seller
span seller Text: smith and wife judy c. smith, seller
span seller Text: judy c. smith, seller
span seller Text: c. smith, seller
span seller Text: smith, seller
</code></pre>
<p>It seems that spacy is chunking the person entities. How can I produce a rule that matches multiple sellers (or buyers), but doesn't cut them up like this example?</p>
<p>My code is below.</p>
<pre><code>#!/usr/bin/env python3

import spacy
from spacy.tokens import SpanGroup, DocBin, Span
from spacy import displacy
import bodytext
import sys
rules  = [{&quot;label&quot;: &quot;seller&quot;, &quot;pattern&quot;: [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;: &quot;{1,2}&quot;}, { &quot;OP&quot;: &quot;*&quot;}, {&quot;TEXT&quot;: &quot;seller&quot;}]},
        {&quot;label&quot;: &quot;buyer&quot;, &quot;pattern&quot;: [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;: &quot;{1,2}&quot;}, { &quot;OP&quot;: &quot;*&quot;}, {&quot;TEXT&quot;: &quot;buyer&quot;}]},]
nlp = spacy.load(&quot;en_core_web_lg&quot;)
ruler = nlp.add_pipe(&quot;span_ruler&quot;)
ruler.add_patterns(rules)

text = &quot;THIS AGREEMENT made on this 12 day of December, 2008, between John E. Smith and wife Judy C. Smith, Seller (whether one or more), whose address is: 1234 CRD 5000, midland, Texas, 79221-2016, and real estate investors, LLC, Buyer, whose address is: 4321 Harvard Ave, Midland, Texas 79701.  &quot;
doc = nlp(text.lower())

doc.spans[&quot;test&quot;] = SpanGroup(doc)
db = DocBin()

for sentence in doc.sents:
    for span in doc.spans[&quot;ruler&quot;]:
        print(&quot;span &quot;+ span.label_+&quot; Text: &quot;+span.text)
        if span.start &gt;= sentence.start and span.end &lt;= sentence.end:
            doc.spans[&quot;test&quot;] += [
                Span(doc, start=sentence.start, end=sentence.end, label=span.label_)
            ]
            doc.set_ents(entities=[span], default=&quot;unmodified&quot;)
</code></pre>
",Named Entity Recognition (NER),spacy rule matching entity text trying write spacy parser extract name term contract written rule extract seller buyer except extracting multiple time simple sentence rule result span like seems spacy chunking person entity produce rule match multiple seller buyer cut like example code
Converting data into spacy format &quot;convert_to_spacy_format&quot; in Name entity recognition Model,"<p><a href=""https://i.sstatic.net/TMDd85MJ.png"" rel=""nofollow noreferrer"">Dataset structure</a>Can somebody help me with the NER model in converting the data into spacy format.
The dataset format is shown in the screenshot here (<a href=""https://www.kaggle.com/datasets/naseralqaydeh/named-entity-recognition-ner-corpus"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/naseralqaydeh/named-entity-recognition-ner-corpus</a>)</p>
<p>Though i build but the model is not giving any output during test.</p>
<pre><code>#Convert data to spaCy format
def convert_to_spacy_format(data):
    nlp = spacy.blank(&quot;en&quot;)  # Creating blank English language model
    db = DocBin()  # document bin object
    
    for _, row in tqdm(data.iterrows(), total=len(data)):
        sentence = row[&quot;CleanSentence&quot;]
        pos_tags = row[&quot;POS&quot;]
        ner_tags = row[&quot;Tag&quot;]
        
        # Create a doc object
        doc = nlp.make_doc(sentence)
        
        # Split the sentence into words (tokens)
        words = sentence.split()
        
        # Check if lengths match
        if len(words) != len(ner_tags) or len(words) != len(pos_tags):
            print(f&quot;Warning: Length mismatch: Words: {len(words)}, NER tags: {len(ner_tags)}, POS tags: {len(pos_tags)}&quot;)
            continue
            
        ents = []
        current_ent = None
        current_ent_start = None
        
        # Process each token
        for idx, (token, tag) in enumerate(zip(doc, ner_tags)):
            # If it's the beginning of an entity
            if tag.startswith(&quot;B-&quot;):
                # If we were tracking an entity, add it to our list
                if current_ent is not None:
                    ents.append((current_ent_start, token.idx + len(token), current_ent))
                
                # Start tracking a new entity
                current_ent = tag[2:]  # Remove &quot;B-&quot; prefix
                current_ent_start = token.idx
            
            # If it's inside an entity
            elif tag.startswith(&quot;I-&quot;):
                # Continue tracking the current entity
                pass
            
            # If it's outside any entity
            elif tag == &quot;O&quot;:
                # If we were tracking an entity, add it to the list
                if current_ent is not None:
                    ents.append((current_ent_start, token.idx, current_ent))
                    current_ent = None
                    current_ent_start = None
        
        # Add the last entity if we were tracking one
        if current_ent is not None:
            ents.append((current_ent_start, len(sentence), current_ent))
        
        # Create spans for each entity
        spans = []
        for start, end, label in ents:
            span = doc.char_span(start, end, label=label)
            if span is not None:
                spans.append(span)
        
        # Filter overlapping spans
        filtered_spans = filter_spans(spans)
        
        # Add entities to the doc
        doc.ents = filtered_spans
        
        # Add the doc to the DocBin
        db.add(doc)
    
    return db

</code></pre>
<p>I tried to build an NER model but didn't got the expected output. I need help in the function</p>
<pre><code>convert_to_spacy_format(data)
</code></pre>
",Named Entity Recognition (NER),converting data spacy format convert spacy format name entity recognition model dataset structurecan somebody help ner model converting data spacy format dataset format shown screenshot though build model giving output test tried build ner model got expected output need help function
How to use Hugging Face model with 512 max tokens on longer text (for Named Entity Recognition),"<p>I have been using the Named Entity Recognition (NER) model <a href=""https://huggingface.co/cahya/bert-base-indonesian-NER"" rel=""nofollow noreferrer"">https://huggingface.co/cahya/bert-base-indonesian-NER</a> on Indonesian text as follows:</p>
<pre class=""lang-py prettyprint-override""><code>text = &quot;...&quot;
model_name = &quot;cahya/bert-base-indonesian-NER&quot;
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForTokenClassification.from_pretrained(model_name)
nlp = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer, aggregation_strategy=&quot;simple&quot;)
entities = nlp(text)
</code></pre>
<p>This works great, but when <code>text</code> contains more than 512 tokens I get the error:</p>
<blockquote>
<p>The size of tensor a (1098) must match the size of tensor b (512) at non-singleton dimension 1</p>
</blockquote>
<p>What is the best way to check if <code>text</code> contains more than 512 tokens, and then split it into manageable chunks that I can use for NER?</p>
<hr />
<p>Counting the number of tokens seems straightforward:</p>
<pre><code>n_tokens = len(tokenizer.encode(text, add_special_tokens=True, truncation=False))
</code></pre>
<p>However, it is unclear what is the best way to split this. Surely there should be a suite of functions for this?</p>
",Named Entity Recognition (NER),use hugging face model max token longer text named entity recognition using named entity recognition ner model indonesian text follows work great contains token get error size tensor must match size tensor b non singleton dimension best way check contains token split manageable chunk use ner counting number token seems straightforward however unclear best way split surely suite function
Named Entity Recognition using Python spaCy,"<p>I want to code a Named Entity Recognition system using Python <code>spaCy</code> package. However, I couldn't install my local language inside <code>spaCy</code> package. Is there anyone who can tell me how to install or otherwise use my local language?</p>
<p>I tried:</p>
<pre><code>python -m spacy download xx_ent_wiki_sm
</code></pre>
",Named Entity Recognition (NER),named entity recognition using python spacy want code named entity recognition system using python package however install local language inside package anyone tell install otherwise use local language tried
Re-Training spaCy&#39;s NER v1.8.2 - Training Volume and Mix of Entity Types,"<p>I'm in the process of (re-) training spaCy's Named Entity Recognizer and have a couple of doubts that I hope a more experienced researcher/practitioner can help me figure out:</p>
<ol>
<li>If a few hundred examples are considered 'a good starting point', then what would be a reasonable number to aim for? Is 100 000 entity/label excessive?</li>
<li>If I introduce a new label, is it best if the number of the entities of that labeled are roughly the same (balanced) during training?</li>
<li>Regarding the mixing in 'examples of other entity types':
<ul>
<li><p>do I just add random known categories/labels to my training set eg: <code>('The Business Standard published in its recent issue on crude oil and natural gas ...', [(4,21, 'ORG')], )</code>?</p>
</li>
<li><p>can I use the same text for various labels? e.g. <code>('The Business Standard published in its recent issue on crude oil and natural gas ...', [(55,64, 'COMMODITY')], )</code>?</p>
</li>
</ul>
</li>
</ol>
<ul>
<li><p>on a similar note let's assume I want spaCyto also recognize a second <code>COMMODITY</code> could I then just use the same sentence and label a different region e.g. <code>('The Business Standard published in its recent issue on crude oil and natural gas ...', [(69,80, 'COMMODITY')], )</code>? Is that how it's supposed to be done?</p>
<ul>
<li>what ratio between new and other (old) labels is considered reasonable</li>
</ul>
</li>
</ul>
<p>I'm working with Python2.7 in Ubuntu 16.04 using spaCy 1.8.2</p>
",Named Entity Recognition (NER),training spacy ner v training volume mix entity type process training spacy named entity recognizer couple doubt hope experienced researcher practitioner help figure hundred example considered good starting point would reasonable number aim entity label excessive introduce new label best number entity labeled roughly balanced training regarding mixing example entity type add random known category label training set eg use text various label e g similar note let assume want spacyto also recognize second could use sentence label different region e g supposed done ratio new old label considered reasonable working python ubuntu using spacy
Training a new entity type with spacy,"<p>Need help to try adding new entity and train my own model with spacy named entity recognition. I wanted first to try the example already done here:</p>
<p><a href=""https://github.com/explosion/spaCy/blob/master/examples/training/train_new_entity_type.py"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/master/examples/training/train_new_entity_type.py</a></p>
<p>but i'am getting this error :</p>
<pre><code>ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-c46f384e-5989-4902-a775-7618ffadd54e.json
An exception has occurred, use %tb to see the full traceback.

SystemExit: 2
/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.
  warn(&quot;To exit: use 'exit', 'quit', or Ctrl-D.&quot;, stacklevel=1)
</code></pre>
<p>How to resolve this?</p>
",Named Entity Recognition (NER),training new entity type spacy need help try adding new entity train model spacy named entity recognition wanted first try example already done getting error resolve
How to transcribe multiple audio files at once using Whisper finetuned model?,"<p><strong>TL;DR: I'm trying to transcribe multiple files together using Hugging face fine-tuned whisper ai model and extract the output as a single text file</strong></p>
<p>I have this code which works and transcribes an audio and shows its output as a string of text. But I want to improve upon this by making this transcribe multiple files together, and exporting its output to a text file with each line representing a single audio file.</p>
<h2>What did I try?</h2>
<p>Im not a coder but I asked bing to generate a code and it came up with this which has errors.</p>
<pre><code>audio_files = [&quot;/content/audio1&quot;, &quot;/content/audio2&quot;, ..., &quot;/content/audioN&quot;]
transcriptions = []

for audio_file in audio_files:
    transcription = pipe(audio_file, chunk_length_s=10, stride_length_s=(4, 2))
    transcriptions.append(transcription)

with open(&quot;transcriptions.txt&quot;, &quot;w&quot;) as f:
    for transcription in transcriptions:
        f.write(transcription + &quot;\n&quot;)
</code></pre>
<h2>What I want?</h2>
<p>I need a code which transcribes <strong>all the audio that I have into a single text file</strong> on which each line represents an audio file(preferably starting with the file name). If I <strong>can specify a folder which has all the files</strong> for transcription instead of entering each file manually, that would be AWESOME.</p>
<h2>Whats my workspace?</h2>
<p>I'm using hugging face open ai whisper(fine-tuned) to transcribe my files on google colab.</p>
<p>Any of your help is deeply appreciated.</p>
",Named Entity Recognition (NER),transcribe multiple audio file using whisper finetuned model tl dr trying transcribe multiple file together using hugging face fine tuned whisper ai model extract output single text file code work transcribes audio show output string text want improve upon making transcribe multiple file together exporting output text file line representing single audio file try im coder asked bing generate code came ha error want need code transcribes audio single text file line represents audio file preferably starting file name specify folder ha file transcription instead entering file manually would awesome whats workspace using hugging face open ai whisper fine tuned transcribe file google colab help deeply appreciated
is there a method to detect person and associate a text?,"<p>I have a text like :</p>
<blockquote>
<p>Take a loot at some of the first confirmed Forum speakers:  <strong>John
Sequiera</strong>  Graduated in Biology at Facultad de Ciencias Exactas y
Naturales,University of Buenos Aires, Argentina. In 2004 obtained a
PhD in Biology (Molecular Neuroscience), at University of Buenos
Aires, mentored by Prof. Marcelo Rubinstein. Between 2005 and 2008
pursued postdoctoral training at Pasteur Institute (Paris) mentored by
Prof Jean-Pierre Changeux, to investigate the role of nicotinic
receptors in executive behaviors. Motivated by a deep interest in
investigating human neurological diseases, in 2009 joined the
Institute of Psychiatry at King’s College London where she performed
basic research with a translational perspective in the field of
neurodegeneration.
Since 2016 has been chief of instructors / Adjunct professor at University of Buenos Aires, Facultad de Ciencias Exactas y Naturales.
<strong>Tom Gonzalez</strong> is a professor of Neuroscience at the Sussex Neuroscience, School of Life Sciences, University of Sussex. Prof.
Baden studies how neurons and networks compute, using the beautiful
collection of circuits that make up the vertebrate retina as a model.</p>
</blockquote>
<p>I want to have in output :
<code>[{&quot;person&quot; : &quot;John Sequiera&quot; , &quot;content&quot;: &quot;Graduated in Biology at Facultad....&quot;},{&quot;person&quot; : &quot;Tom Gonzalez&quot; , &quot;content&quot;: &quot;is a professor of Neuroscience at the Sussex...&quot;}]</code></p>
<p>so we want to get NER : PER for person and in content we put all contents after detecting person until we found a new person in the text ...</p>
<p>it is possible ?</p>
<p>i try to use spacy to extract NER , but i found a difficulty to get content :</p>
<pre><code>import spacy
​
nlp = spacy.load(&quot;en_core_web_lg&quot;)
doc = nlp(text)
​
for ent in doc.ents:
    print(ent.text,ent.label_)
</code></pre>
",Named Entity Recognition (NER),method detect person associate text text like take loot first confirmed forum speaker john sequiera graduated biology facultad de ciencias exacta naturales university buenos aire argentina obtained phd biology molecular neuroscience university buenos aire mentored prof marcelo rubinstein pursued postdoctoral training pasteur institute paris mentored prof jean pierre changeux investigate role nicotinic receptor executive behavior motivated deep interest investigating human neurological disease joined institute psychiatry king college london performed basic research translational perspective field neurodegeneration since ha chief instructor adjunct professor university buenos aire facultad de ciencias exacta naturales tom gonzalez professor neuroscience sussex neuroscience school life science university sussex prof baden study neuron network compute using beautiful collection circuit make vertebrate retina model want output want get ner per person content put content detecting person found new person text possible try use spacy extract ner found difficulty get content
How to correctly identify entity types for tokens using spaCy using python?,"<p>I'm using spaCy to extract and identify entity types (like ORG, GPE, DATE, etc.) from a text description. However, I am noticing some incorrect results, and I'm unsure how to fix this.</p>
<p>Here is the code I am using:</p>
<pre><code>import spacy

nlp = spacy.load(&quot;en_core_web_sm&quot;)

def getPayeeName(description):
    description = description.replace(&quot;-&quot;, &quot; &quot;).replace(&quot;/&quot;, &quot; &quot;).strip()
    doc = nlp(description)

    for token in doc:
        print(f&quot;Token: {token.text}, Entity: {token.ent_type_ if token.ent_type_ else 'None'}&quot;)

# Example input
description = &quot;UPI DR 400874707203 BENGALORE 08 JAN 2024 14:38:56 MEDICAL LTD HDFC 50200&quot;
getPayeeName(description)
</code></pre>
<p>Token: UPI, Entity: ORG</p>
<p>Token: DR, Entity: ORG</p>
<p>Token: 400874707203, Entity: None</p>
<p>Token: BENGALORE, Entity: None</p>
<p>Token: 08, Entity: DATE</p>
<p>Token: JAN, Entity: DATE</p>
<p>Token: 2024, Entity: DATE</p>
<p>Token: 14:38:56, Entity: None</p>
<p>Token: MEDICAL, Entity: ORG</p>
<p>Token: LTD, Entity: ORG</p>
<p>Token: HDFC, Entity: ORG</p>
<p>Token: 50200, Entity: ORG</p>
<ul>
<li><p>50200 is identified as ORG, but it is just a number.</p>
</li>
<li><p>BENGALORE is a city, but it is not recognized as a GPE or location
(returns None).</p>
</li>
<li><p>UPI and DR are acronyms/abbreviations, but they are incorrectly
identified as ORG.</p>
</li>
</ul>
<p>I want the entity recognition to be more accurate and reliable.
How can I fix these issues? Are there additional spaCy configurations, custom rules, or pre-trained models I should use to improve the entity recognition?</p>
<p>Note: I tried ChatGPT as well, but still this issue is not solved.</p>
",Named Entity Recognition (NER),correctly identify entity type token using spacy using python using spacy extract identify entity type like org gpe date etc text description however noticing incorrect result unsure fix code using token upi entity org token dr entity org token entity none token bengalore entity none token entity date token jan entity date token entity date token entity none token medical entity org token ltd entity org token hdfc entity org token entity org identified org number bengalore city recognized gpe location return none upi dr acronym abbreviation incorrectly identified org want entity recognition accurate reliable fix issue additional spacy configuration custom rule pre trained model use improve entity recognition note tried chatgpt well still issue solved
Recommending a pre-train NER model for geospatial entities,"<p>I am trying to find the best pre-trained Hugging Face Transformer model exclusively dedicated to geospatial or location entities to extract location entities in English from a text. Does it work way better than roberta-large?</p>
",Named Entity Recognition (NER),recommending pre train ner model geospatial entity trying find best pre trained hugging face transformer model exclusively dedicated geospatial location entity extract location entity english text doe work way better roberta large
NLP - Specific Text Extraction,"<p>I have to identify the country name from a random text. I have the country list.</p>
<p>I am struggling to find a solution that can train the model on the country list and when I provide a random text to that model as an input, it identifies the country name as an output.</p>
<p>eg:-</p>
<ul>
<li>&quot;I live in India&quot; will give &quot;India&quot;</li>
<li>&quot;London is the capital of United Kingdom&quot; will give &quot;United Kingdom&quot;</li>
</ul>
",Named Entity Recognition (NER),nlp specific text extraction identify country name random text country list struggling find solution train model country list provide random text model input identifies country name output eg live india give india london capital united kingdom give united kingdom
Output probabilities of tokens generated by Llama 2 using Transformers,"<p>Given input tokens, LLMs output the tokens in their vocabulary that have the highest probability of coming after the input tokens.</p>
<p>I would like to print the probability of each token generated by the model in response to a prompt to see how confident the model is in its generated tokens. I would like to do this on Llama-2-7b-chat-hf on a simple prompt like : &quot;Could you give me 3 cities located in Europe ?&quot;.</p>
<p>In order to do this I have the following code :</p>
<pre><code>from transformers import LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM, AutoTokenizer
import torch
import numpy as np

device = &quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;

model=LlamaForCausalLM.from_pretrained(&quot;Llama-2-7b-chat-hf&quot;).to(device)
tokenizer= LlamaTokenizer.from_pretrained(&quot;Llama-2-7b-chat-hf&quot;)

prompt = &quot;Could you give me 3 cities located in Europe ?&quot;

inputs = tokenizer([prompt], return_tensors=&quot;pt&quot;).to(device)

outputs=model.generate(**inputs,return_dict_in_generate=True, output_scores=True,max_new_tokens=75)

transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)

input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]
generated_tokens = outputs.sequences[:,input_length:]

for tok, score in zip(generated_tokens[0], transition_scores[0]):
        # | token | token string | logits | probability
            print(f&quot;| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy(force=True):.4f} | {np.exp(score.numpy(force=True)):.2%}&quot;)
</code></pre>
<p>As a result I receive the following :</p>
<pre><code>| token | token string | logits | probability
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
| 10605 | Here     | -3.0267 | 4.85%
|   526 | are      | 0.0000 | 100.00%
| 29871 |          | -0.3506 | 70.43%
| 29941 | 3        | 0.0000 | 100.00%
| 14368 | cities   | 0.0000 | 100.00%
|  5982 | located  | 0.0000 | 100.00%
|   297 | in       | 0.0000 | 100.00%
|  4092 | Europe   | 0.0000 | 100.00%
| 29901 | :        | 0.0000 | 100.00%
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
| 29896 | 1        | 0.0000 | 100.00%
| 29889 | .        | 0.0000 | 100.00%
|  3681 | Paris    | 0.0000 | 100.00%
| 29892 | ,        | 0.0000 | 100.00%
|  3444 | France   | 0.0000 | 100.00%
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
| 29906 | 2        | 0.0000 | 100.00%
| 29889 | .        | 0.0000 | 100.00%
|  9184 | Rome     | -1.0413 | 35.30%
| 29892 | ,        | 0.0000 | 100.00%
| 12730 | Italy    | 0.0000 | 100.00%
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
| 29941 | 3        | 0.0000 | 100.00%
| 29889 | .        | 0.0000 | 100.00%
|  4517 | London   | 0.0000 | 100.00%
| 29892 | ,        | 0.0000 | 100.00%
|  3303 | United   | 0.0000 | 100.00%
| 12626 | Kingdom  | 0.0000 | 100.00%
|     2 | &lt;/s&gt;     | 0.0000 | 100.00%
</code></pre>
<p>As you can see, most of the words have a probability of 100% of being chosen which seems very odd to me. Llama 2 has a vocabulary of 32000 tokens surely there are other tokens that could be used at the place of those tokens, I would agree with something like 70% but 100% should be impossible. Which makes me believe something is wrong in my code.</p>
<p>Would you agree with me ? And if yes, would you know what is wrong in my code ?</p>
",Named Entity Recognition (NER),output probability token generated llama using transformer given input token llm output token vocabulary highest probability coming input token would like print probability token generated model response prompt see confident model generated token would like llama b chat hf simple prompt like could give city located europe order following code result receive following see word probability chosen seems odd llama ha vocabulary token surely token could used place token would agree something like impossible make believe something wrong code would agree yes would know wrong code
"NER versus LLM to extract name, gender, role and company from text","<p>I need to extract the name, gender, job title and employer/company name from newspaper articles, running the process on local hardware (no Cloud allowed) due to copyright reasons.</p>
<p>I've been playing around with Llama 3.1 but I'm finding I don't get useable results with the models smaller than 70B parameters, and at that size the models run much too slowly on the best hardware I have to throw at them.</p>
<p>Is there another, smaller LLM that might be good at this while using fewer processing resources?</p>
<p>Is there is NER I can use to extract all that data? The NERs I've looked into extract name but not gender. (I don't know if they extract the other data because gender is a showstopper for me.)</p>
<p>Alternatively, is there an approach I can take where I do a first pass with a NER, and then pass the names through an LLM together with the original newspaper article to extract the other data, and get better results, faster than a single LLM pass?</p>
<p>Or if the answer is I should be training some model, what is a good model for me to use as my starting point? I'm very much at the beginning of my machine learning journey and would love to be pointed in the right direction.</p>
<p>Thanks in advance!</p>
",Named Entity Recognition (NER),ner versus llm extract name gender role company text need extract name gender job title employer company name newspaper article running process local hardware cloud allowed due copyright reason playing around llama finding get useable result model smaller b parameter size model run much slowly best hardware throw another smaller llm might good using fewer processing resource ner use extract data ners looked extract name gender know extract data gender showstopper alternatively approach take first pas ner pas name llm together original newspaper article extract data get better result faster single llm pas answer training model good model use starting point much beginning machine learning journey would love pointed right direction thanks advance
Handling Multiple Entity Candidates in Short Texts for Entity Linking with SciSpacy,"<p>I am working on linking short texts to entities in a biomedical knowledge graph (UMLS CUIs) using SciSpacy for a research project. The goal is to analyze the relationship between the linked entity and a separate predefined entity.</p>
<p>My challenge is managing multiple possible entities identified in the texts, which introduces noise into the results. Although I use heuristics such as regex, a manual stop list, and filtering by semantic categories (TUIs) to clean the data, the issue persists due to the text complexity. I typically select the top ~3 entities per text based on the NER score, with a relatively high threshold.</p>
<p>For instance, the text &quot;Standard PRS for Alzheimer's&quot; incorrectly links entities for &quot;Standard&quot; and &quot;PRS,&quot; in addition to &quot;Alzheimer's.&quot; Another example, &quot;Other diseases of respiratory system, NEC,&quot; captures &quot;respiratory&quot; and &quot;diseases&quot; but misses &quot;NEC&quot; (Necrotizing enterocolitis), which should be prioritized.</p>
<p>I've tried filtering results by semantic similarity using a biomedical model, but this approach is still imprecise and heavily dependent on the number of results. The linker often seems to prioritize entities appearing earlier in the text. I also use an abbreviation expander to handle non-standard acronym forms.</p>
<p>I think a smarter linker (not supported by scispacy) might help, or better matching at the sentence/whole text level, but I don't know much about that. (I do some filtering of results using sentence transformers, but that's just cossine sim - I couldn't find a clear cutoff that generalized well).</p>
<p>I do not have the resources/time to learn to fine-tune a new linker model+data (this is just a sub-component in my overall phd).</p>
<p>I'm looking for advice on more effective strategies for entity linking at the sentence or whole-text level without the resources to fine-tune a new model. Compatability with SciSpacy is important, since linkage to the UMLS ontology (for the KG CUI entites) is a must.</p>
",Named Entity Recognition (NER),handling multiple entity candidate short text entity linking scispacy working linking short text entity biomedical knowledge graph umls cuis using scispacy research project goal analyze relationship linked entity separate predefined entity challenge managing multiple possible entity identified text introduces noise result although use heuristic regex manual stop list filtering semantic category tuis clean data issue persists due text complexity typically select top entity per text based ner score relatively high threshold instance text standard pr alzheimer incorrectly link entity standard pr addition alzheimer another example disease respiratory system nec capture respiratory disease miss nec necrotizing enterocolitis prioritized tried filtering result semantic similarity using biomedical model approach still heavily dependent number result linker often seems prioritize entity appearing earlier text also use abbreviation expander handle non standard acronym form think smarter linker supported scispacy might help better matching sentence whole text level know much filtering result using sentence transformer cossine sim find clear cutoff generalized well resource time learn fine tune new linker model data sub component overall phd looking advice effective strategy entity linking sentence whole text level without resource fine tune new model compatability scispacy important since linkage umls ontology kg cui entites must
How to reconstruct text entities with Hugging Face&#39;s transformers pipelines without IOB tags?,"<p>I've been looking to use Hugging Face's Pipelines for NER (named entity recognition). However, it is returning the entity labels in inside-outside-beginning (IOB) format but <a href=""https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)"" rel=""noreferrer"">without the IOB labels</a>. So I'm not able to map the output of the pipeline back to my original text. Moreover, the outputs are masked in BERT tokenization format (the default model is BERT-large).</p>

<p>For example: </p>

<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
nlp_bert_lg = pipeline('ner')
print(nlp_bert_lg('Hugging Face is a French company based in New York.'))
</code></pre>

<p>The output is:</p>

<pre><code>[{'word': 'Hu', 'score': 0.9968873858451843, 'entity': 'I-ORG'},
{'word': '##gging', 'score': 0.9329522848129272, 'entity': 'I-ORG'},
{'word': 'Face', 'score': 0.9781811237335205, 'entity': 'I-ORG'},
{'word': 'French', 'score': 0.9981815814971924, 'entity': 'I-MISC'},
{'word': 'New', 'score': 0.9987512826919556, 'entity': 'I-LOC'},
{'word': 'York', 'score': 0.9976728558540344, 'entity': 'I-LOC'}]
</code></pre>

<p>As you can see, New York is broken up into two tags.</p>

<p>How can I map Hugging Face's NER Pipeline back to my original text?</p>

<p>Transformers version: 2.7</p>
",Named Entity Recognition (NER),reconstruct text entity hugging face transformer pipeline without iob tag looking use hugging face pipeline ner named entity recognition however returning entity label inside outside beginning iob format without iob label able map output pipeline back original text moreover output masked bert tokenization format default model bert large example output see new york broken two tag map hugging face ner pipeline back original text transformer version
Why is Spacy not executing the training pipeline even though the code runs without error?,"<p>I am training a custom NER model with Spacy version 3.5.0 using some dummy data. My entire code and dummy data is given below. <a href=""https://github.com/dmoonat/Named-Entity-Recognition/blob/main/NER_with_spaCy.ipynb"" rel=""nofollow noreferrer"">This is exact same code give in the 2nd half of this link</a>. The code is running fine, but it only executes until the <strong>Initializing pipeline</strong> step of the training and the <strong>Training pipeline</strong> is not executed.</p>
<p>Any idea why the training pipeline is not being executed?</p>
<pre><code>import pandas as pd
import os
from tqdm import tqdm
from spacy.tokens import DocBin

train = [
          (&quot;An average-sized strawberry has about 200 seeds on its outer surface and are quite edible.&quot;,{&quot;entities&quot;:[(17,27,&quot;Fruit&quot;)]}),
          (&quot;The outer skin of Guava is bitter tasting and thick, dark green for raw fruits and as the fruit ripens, the bitterness subsides. &quot;,{&quot;entities&quot;:[(18,23,&quot;Fruit&quot;)]}),
          (&quot;Grapes are one of the most widely grown types of fruits in the world, chiefly for the making of different wines. &quot;,{&quot;entities&quot;:[(0,6,&quot;Fruit&quot;)]}),
          (&quot;Watermelon is composed of 92 percent water and significant amounts of Vitamins and antioxidants. &quot;,{&quot;entities&quot;:[(0,10,&quot;Fruit&quot;)]}),
          (&quot;Papaya fruits are usually cylindrical in shape and the size can go beyond 20 inches. &quot;,{&quot;entities&quot;:[(0,6,&quot;Fruit&quot;)]}),
          (&quot;Mango, the King of the fruits is a drupe fruit that grows in tropical regions. &quot;,{&quot;entities&quot;:[(0,5,&quot;Fruit&quot;)]}),
          (&quot;undefined&quot;,{&quot;entities&quot;:[(0,6,&quot;Fruit&quot;)]}),
          (&quot;Oranges are great source of vitamin C&quot;,{&quot;entities&quot;:[(0,7,&quot;Fruit&quot;)]}),
          (&quot;A apple a day keeps doctor away. &quot;,{&quot;entities&quot;:[(2,7,&quot;Fruit&quot;)]})
        ]

db = DocBin() # create a DocBin object

for text, annot in tqdm(train): # data in previous format
    doc = nlp.make_doc(text) # create doc object from text
    ents = []
    for start, end, label in annot[&quot;entities&quot;]: # add character indexes
        span = doc.char_span(start, end, label=label, alignment_mode=&quot;contract&quot;)
        if span is None:
            print(&quot;Skipping entity&quot;)
        else:
            ents.append(span)
    doc.ents = ents # label the text with the ents
    db.add(doc)

db.to_disk(&quot;./train.spacy&quot;) # save the docbin object

!python -m spacy init fill-config base_config.cfg config.cfg

!python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy
</code></pre>
<p><strong>Expected output</strong></p>
<p><a href=""https://i.sstatic.net/dm2Vn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dm2Vn.png"" alt=""enter image description here"" /></a></p>
<p><strong>Output I got</strong></p>
<p><a href=""https://i.sstatic.net/5Fj8z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5Fj8z.png"" alt=""enter image description here"" /></a></p>
",Named Entity Recognition (NER),spacy executing training pipeline even though code run without error training custom ner model spacy version using dummy data entire code dummy data given exact code give nd half link code running fine executes initializing pipeline step training training pipeline executed idea training pipeline executed expected output output got
"How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)","<p>How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)</p>
<p>I have short product descriptions that I’d like to transform into structured attributes.</p>
<p>Example:</p>
<p>Input:</p>
<pre><code>“La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml”
</code></pre>
<p>Output:</p>
<pre><code>Year = 2017

Color = Red

Weight = 750

Weight Unit = ml
</code></pre>
<p>If everything was in this format it would be trivial to write a regular expression and be done with it, but there are many different formats and nuances. It is increasingly cumbersome to hard-code logic for each format. Trying to create a generic solution I immediately run into issues with a “basic” approach:</p>
<ol>
<li><p>There are several different data providers, and each has its own format. For the example above, another provider might use “(Red) 2017 La Lecciaia Cabernet Sauvignon 750 ML”. Even for a given provider, there may be multiple formats and they may change over time. Formats are not always strictly followed.</p>
</li>
<li><p>There are many ways of expressing particular components. As an example, Weight might be expressed as any one of these: “1.5L”, “1 1/2 Liters”, “1500ml”, etc.</p>
</li>
<li><p>Parts of the description may be confused for target components. There may be a white wine from a brand called “Red Head Vineyard”. A weight of “2000 ml” may be confused for a year, etc. I’m only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues.</p>
</li>
<li><p>I’d consider this more of a “nice to have” but would be useful to be able to parse out even more detail like the algo would be smart enough to know that “La Lecciaia” is the brand and “Cabernet Sauvignon” is the grape variety. Assuming this would take more up front work and harder to get right but if there’s a straightforward method of doing this would be good to know about.</p>
</li>
</ol>
<p>I’d like to develop a general-purpose function that can accept a description from any format. I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage. I have 1,000+ example records that I could potentially use to train a model. Something that can run locally would be preferred but not absolutely necessary.</p>
<p>I’m not looking for a specific implementation but for guidance from anyone who’s worked on a similar problem. Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies.</p>
<p>Appreciate any insight into approaches or suggested learning resources.</p>
<p></p>
<p>I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense.</p>
",Named Entity Recognition (NER),derive attribute label short plain text description ner llm derive attribute label short plain text description ner llm short product description like transform structured attribute example input output everything wa format would trivial write regular expression done many different format nuance increasingly cumbersome hard code logic format trying create generic solution immediately run issue basic approach several different data provider ha format example another provider might use red la lecciaia cabernet sauvignon ml even given provider may multiple format may change time format always strictly followed many way expressing particular component example weight might expressed one l liter ml etc part description may confused target component may white wine brand called red head vineyard weight ml may confused year etc using wine example sake simplicity general audience product domain ha conceptual issue consider nice would useful able parse even detail like algo would smart enough know la lecciaia brand cabernet sauvignon grape variety assuming would take front work harder get right straightforward method would good know like develop general purpose function accept description format little experience nlp artificial intelligence suspect useful tool algos leverage example record could potentially use train model something run locally would preferred absolutely necessary looking specific implementation guidance anyone worked similar problem open hybrid approach additional logic manual oversight could account initial inaccuracy appreciate insight approach suggested learning resource looked online information many approach involve significant amount front work unclear work practical sense
Break after first PER sequence found with Spacy,"<p>I am trying to extract only the first speaker's name from a list of texts using spaCy. Currently, my function returns all &quot;PER&quot; tags, but I want to reduce the overhead and get only the first contiguous sequence of &quot;PER&quot; entities. Here’s the example output I get:</p>
<pre><code>Detected Names in Text: ['garcía', 'lópez']
Detected Names in Text: ['j. jesus orozco alfaro']
Detected Names in Text: ['josé guadarrama márquez', 'josé guadarrama']
Detected Names in Text: ['pedro sánchez', 'josé manuel albares', 'pablo iglesias']
</code></pre>
<p>But I want the result to be:</p>
<pre><code>Detected Names in Text: ['garcía']
Detected Names in Text: ['j. jesus orozco alfaro']
Detected Names in Text: ['josé guadarrama márquez']
Detected Names in Text: ['pedro sánchez']
</code></pre>
<p>Here is the code I am currently using:</p>
<pre><code>import spacy
from spacy.matcher import Matcher

nlp = spacy.load(&quot;es_core_news_lg&quot;)

texts = [
    &quot;El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.&quot;,
    &quot;PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO&quot;,
    &quot;            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez&quot;,
    &quot;El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.&quot;
]
texts = [text.lower() for text in texts]

matcher = Matcher(nlp.vocab)

patterns = [
    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;c&quot;}],
    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sr&quot;}],
    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sra&quot;}]
]

matcher.add(&quot;LEGISLATIVE_TITLES&quot;, patterns)

# Function to find a sequence of PER entities allowing one MISC
def find_per_sequence(doc, start_idx=0):
    per_entities = []
    misc_count = 0
    
    for ent in doc[start_idx:].ents:
        if ent.label_ == &quot;PER&quot;:
            per_entities.append(ent.text)
        elif ent.label_ == &quot;MISC&quot; and misc_count &lt; 1:
            misc_count += 1
            per_entities.append(ent.text)
        else:
            break  # Should stop if any other entity or second MISC is encountered
    
    return per_entities

for text in texts:
    doc = nlp(text)
    
    # Find matches
    matches = matcher(doc)
    
    # Extract the first match and its position
    title_start = None
    title_end = None
    for match_id, start, end in matches:
        title_start = start
        title_end = end
        break

    # If a title was found, start searching for PER entities from that position
    if title_start is not None:
        names = find_per_sequence(doc, start_idx=title_end)
    else:
        names = find_per_sequence(doc)

    # Output the detected names for each text
    print(f&quot;Detected Names in Text: {names}&quot;)
</code></pre>
<p>What I'm looking for:</p>
<p>I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of &quot;PER&quot; entities in the text, ignoring any subsequent &quot;PER&quot; entities after encountering a different type of entity. The provided function returns multiple names or partial names, and I need a way to ensure only the first name or sequence is included. How can I achieve this?</p>
",Named Entity Recognition (NER),break first per sequence found spacy trying extract first speaker name list text using spacy currently function return per tag want reduce overhead get first contiguous sequence per entity example output get want result code currently using looking want modify find per sequence function return first contiguous sequence per entity text ignoring subsequent per entity encountering different type entity provided function return multiple name partial name need way ensure first name sequence included achieve
Replace Prediction Head for XLM-RoBERTa-Base,"<p>I want to replace the XLM-RoBERTa-Base prediction head for a NER task (9 tags) with the weights for very specific words. For example, for the NER tag &quot;B-PER&quot; I would like to use the weights from the XLM-RoBERTa vocabulary for the word &quot;George&quot;. The words for the individual labels look like this:</p>
<pre><code>labels = [&quot;I-PER&quot;, &quot;B-PER&quot;, &quot;I-LOC&quot;, &quot;B-LOC&quot;, &quot;I-ORG&quot;, &quot;B-ORG&quot;, &quot;I-MISC&quot;, &quot;B-MISC&quot;, &quot;O&quot;]
tag_prediction = {&quot;I-PER&quot;: &quot;Miller&quot;,
                  &quot;B-PER&quot;: &quot;George&quot;,
                  &quot;I-LOC&quot;: &quot;Jersey&quot;,
                  &quot;B-LOC&quot;: &quot;Paris&quot;,
                  &quot;I-ORG&quot;: &quot;The&quot;,
                  &quot;B-ORG&quot;: &quot;FC&quot;,
                  &quot;I-MISC&quot;: &quot;Cup&quot;,
                  &quot;B-MISC&quot;: &quot;World&quot;,
                  &quot;O&quot;: &quot;by&quot;}
</code></pre>
<p>However, if I now evaluate my new model, I get very bad results:</p>
<pre><code>('eval_loss': 5.455610752105713, 'eval_precision': 0.017951501990589938, 'eval_recall': 0.043909348441926344, 'eval_f1': 0.02548425217078559, 'eval_accuracy': 0.02312910520081835)
</code></pre>
<p>Even if I pass a perfect example sentence such as &quot;George Miller lives in New Jersey&quot;, all words are classified incorrectly here. All words are classified as &quot;I-PER&quot;.</p>
<p>Here is my code, which extracts the weights from XLM-RoBERTa:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import XLMRobertaForTokenClassification, XLMRobertaTokenizer
import os

# Load model and tokenizer
model = XLMRobertaForTokenClassification.from_pretrained(&quot;xlm-roberta-base&quot;, num_labels=9)
tokenizer = XLMRobertaTokenizer.from_pretrained(&quot;xlm-roberta-base&quot;)
print(model)

# Define labels and their most common prediction
labels = [&quot;I-PER&quot;, &quot;B-PER&quot;, &quot;I-LOC&quot;, &quot;B-LOC&quot;, &quot;I-ORG&quot;, &quot;B-ORG&quot;, &quot;I-MISC&quot;, &quot;B-MISC&quot;, &quot;O&quot;]
tag_prediction = {&quot;I-PER&quot;: &quot;Miller&quot;,
                  &quot;B-PER&quot;: &quot;George&quot;,
                  &quot;I-LOC&quot;: &quot;Jersey&quot;,
                  &quot;B-LOC&quot;: &quot;Paris&quot;,
                  &quot;I-ORG&quot;: &quot;The&quot;,
                  &quot;B-ORG&quot;: &quot;FC&quot;,
                  &quot;I-MISC&quot;: &quot;Cup&quot;,
                  &quot;B-MISC&quot;: &quot;World&quot;,
                  &quot;O&quot;: &quot;by&quot;}

# Convert words to token IDs
token_ids = {}
for label in labels:
    name = tag_prediction[label]
    encoded_ids = tokenizer.encode(name, add_special_tokens=False)
    token_ids[label] = encoded_ids

# Save old weights/biases
old_weights = model.classifier.weight.detach().clone()
old_bias = model.classifier.bias.detach().clone()

# Output the old weights and biases for each output feature
for idx, label in enumerate(labels):
    weight = old_weights[idx].detach().numpy()
    bias = old_bias[idx].item()
    print(f&quot;{label}:&quot;)
    print(f&quot;  Weight: {weight[0:10]}&quot;)
    print(f&quot;  Bias: {bias}&quot;)

# Initialize new weights/biases
new_weights = old_weights.clone()
new_bias = old_bias.clone()

# Replace weights/bias of the tags with those of the specific words (e.g.: Replace weights/bias of 'B-PER' with 'John')
for label in labels:
    ids = token_ids[label]
    idx = labels.index(label)
    for token_id in ids:
        new_weights[idx] = model.roberta.embeddings.word_embeddings.weight[token_id].detach()
        new_bias[idx] = 0

# Insert weights/biases into model
model.classifier.weight = torch.nn.Parameter(new_weights)
model.classifier.bias = torch.nn.Parameter(new_bias)
</code></pre>
<p>Any tips or advice?</p>
",Named Entity Recognition (NER),replace prediction head xlm roberta base want replace xlm roberta base prediction head ner task tag weight specific word example ner tag b per would like use weight xlm roberta vocabulary word george word individual label look like however evaluate new model get bad result even pas perfect example sentence george miller life new jersey word classified incorrectly word classified per code extract weight xlm roberta tip advice
Extracting and Identifying locations with NLP + Spacy,"<p>My goal is to be able to recognize (aka identify) and identify (aka name, retrieve an ID) locations from text using NLP. I'm using Spacy specifically.</p>
<p>There are about 1,000 possible locations, but the difficulty is that they are unlikely to be written in a fully qualified way, not to mention spelling mistakes and aliases. For example, the Mission neighborhood in San Francisco written in a fully-qualified way might be <code>(1) Mission (2) City of San Francisco (3) San Francisco County (4) California (5) US</code>. (The numbers are just to illustrate the separate pieces.) However, many people might write it as <code>(1) Mission (2) City of San Francisco</code>, or <code>(1) Mission</code>, or <code>(1) Mission (2) City of San Francisco (4) California</code>. (Not to mention that #1 might be called &quot;Mission District&quot;, #2 might be called &quot;San Francisco&quot;, #4 might be &quot;CA&quot;, etc.)</p>
<p>So my goal is to be able have an ID for &quot;Mission&quot; and all other neighborhoods, and ID for California and some other states, etc. If the text is <em>like</em> <code>Mission, San Francisco, CA</code> then I get the Mission ID. If the text is <em>like</em> <code>San Francisco, CA</code> then I get the San Francisco ID.</p>
<p>It's also easy to create synthetic training data by creating aliases of the individual location pieces (e.g., (a) &quot;City of San Francisco&quot;, (b) &quot;San Francisco&quot;, (c) &quot;San Francisco City&quot;) and permutations of the &quot;name chain&quot; (e.g, 1 + 2 + 3 + 4 + 5, 1 + 2, 1 + 2 + 5, etc) for each alias. Rough estimate is about 50 combinations of alias and name chain per location, or about O(50,000) total values.</p>
<p>So, extraction seems to be a good job for NER. The surrounding text usually has a bit of context (e.g., &quot;Location: ....&quot; or &quot;Comes from ...&quot;.</p>
<p>However, I'm unsure about the ability to do identification. My understanding is that much of the NER identification (e.g., Spacy's <a href=""https://spacy.io/api/entitylinker"" rel=""nofollow noreferrer"">EntityLinker</a>, which I planned on using) relies on <a href=""https://github.com/explosion/projects/tree/master/nel-emerson/"" rel=""nofollow noreferrer"">surrounding context</a>. I expect that there will be very little surrounding context that would help disambiguate one of the O(1000) locations from others. I also understand that EntityLinker matches on the token is lookup and not statistical (in other words, the value is from disambiguating when you have multiple exact-string matches and not from disambiguating from multiple very-fuzzy matches).</p>
<p>The KnowledgeBase / LookupDB does have a <a href=""https://spacy.io/api/inmemorylookupkb#add_alias"" rel=""nofollow noreferrer"">mechanism for setting aliases</a>, so I could add each permutation as an alias. But at that point I feel like I'm not getting any value out of the EntityLinker's statistical models.</p>
<p>If I have to create a gazetteer for the identification aspect, then maybe it makes sense to put all my effort into the gazetteer and skip the NER?</p>
",Named Entity Recognition (NER),extracting identifying location nlp spacy goal able recognize aka identify identify aka name retrieve id location text using nlp using spacy specifically possible location difficulty unlikely written fully qualified way mention spelling mistake alias example mission neighborhood san francisco written fully qualified way might number illustrate separate piece however many people might write mention might called mission district might called san francisco might ca etc goal able id mission neighborhood id california state etc text like get mission id text like get san francisco id also easy create synthetic training data creating alias individual location piece e g city san francisco b san francisco c san francisco city permutation name chain e g etc alias rough estimate combination alias name chain per location total value extraction seems good job ner surrounding text usually ha bit context e g location come however unsure ability identification understanding much ner identification e g spacy entitylinker planned using relies surrounding context expect little surrounding context would help disambiguate one location others also understand entitylinker match token lookup statistical word value disambiguating multiple exact string match disambiguating multiple fuzzy match knowledgebase lookupdb doe mechanism setting alias could add permutation alias point feel like getting value entitylinker statistical model create gazetteer identification aspect maybe make sense put effort gazetteer skip ner
is it possible to train NER model on en_core_web_lg without static_vectors?,"<p>I am trying to train an NER model with custom tokenization. it works fine with the en_core_web_sm model, but I am trying to increase accuracy so I am now trying with en_core_web_lg. no matter what I seem to do to disable static vectors, it always ends up complaining:
<code>RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False</code></p>
<p>I have decided against using static vectors as I must use custom tokenization, and as I am new I decided that simply not using it would be better. is this wrong?</p>
<p>this is the code I am using for the large model:</p>
<pre><code>def train_model(train_data, output_dir, n_iter=50):
    # Enable GPU
    spacy.require_gpu()
    
    # Load the pre-trained model including tok2vec, but excluding vectors
    nlp = spacy.load(&quot;en_core_web_lg&quot;, exclude=[&quot;vectors&quot;])
    
    # Set custom tokenizer
    nlp.tokenizer = custom_tokenizer(nlp)
    
    # Ensure no component is using static vectors
    for component in nlp.pipe_names:
        if hasattr(nlp.get_pipe(component), &quot;include_static_vectors&quot;):
            nlp.get_pipe(component).include_static_vectors = False
    
    # Check if the NER pipe exists, if not add it
    if &quot;ner&quot; not in nlp.pipe_names:
        ner = nlp.add_pipe(&quot;ner&quot;, last=True)
    else:
        ner = nlp.get_pipe(&quot;ner&quot;)
    
    # Add labels to the NER model
    for example in train_data:
        for ent in example.reference.ents:
            ner.add_label(ent.label_)
    
    # Create an optimizer with the correct components
    optimizer = nlp.create_optimizer()

    for i in range(n_iter):
        random.shuffle(train_data)
        losses = {}
        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            nlp.update(batch, drop=0.5, losses=losses, sgd=optimizer)
        print(f&quot;Iteration {i + 1}/{n_iter}: Losses {losses}&quot;)
    
    # Save the model to the output directory
    if output_dir is not None:
        nlp.to_disk(output_dir)
        print(f&quot;Saved model to {output_dir}&quot;)
</code></pre>
<p>it results in the previously mentioned runtime error</p>
",Named Entity Recognition (NER),possible train ner model en core web lg without static vector trying train ner model custom tokenization work fine en core web sm model trying increase accuracy trying en core web lg matter seem disable static vector always end complaining decided using static vector must use custom tokenization new decided simply using would better wrong code using large model result previously mentioned runtime error
Dep extraction rules using NLPPort,"<p>When I'm trying to extract to deps from chunk and entities (even from depchunks) from sentence extract &quot;A Lua e o Sol estão muito distantes.&quot; I got some entries like this:</p>
<pre><code>(...)
A: Triple [subject=o Sol, predicate=ser, object=A Lua]
[A: o &lt;START:thing&gt; Sol &lt;END&gt; ser A &lt;START:thing&gt; Lua &lt;END&gt;]
A: Triple [subject=A Lua, predicate=ser, object=o Sol]
[A: A &lt;START:thing&gt; Lua &lt;END&gt; ser o &lt;START:thing&gt; Sol &lt;END&gt;]

B: Triple [subject=A Lua e o Sol, predicate=estar, object=muito distantes]
[B: A &lt;START:thing&gt; Lua &lt;END&gt; e o &lt;START:thing&gt; Sol &lt;END&gt; estar muito distantes]



--&gt; [Attention!]  E: Triple [subject=o Sol, predicate=ser, object=A Lua]
[E: o &lt;START:thing&gt; Sol &lt;END&gt; ser A &lt;START:thing&gt; Lua &lt;END&gt;]
E: Triple [subject=estão, predicate=ser, object=muito distantes]
[E: estão ser muito distantes]
--&gt; [Attention!] E: Triple [subject=A Lua, predicate=ser, object=o Sol]
[E: A &lt;START:thing&gt; Lua &lt;END&gt; ser o &lt;START:thing&gt; Sol &lt;END&gt;]

F: Triple [subject=A Lua e o Sol, predicate=estar, object=muito distantes]
[F: A &lt;START:thing&gt; Lua &lt;END&gt; e o &lt;START:thing&gt; Sol &lt;END&gt; estar muito distantes]
(...)
</code></pre>
<p>How can I adjust the pipeline from NLPPORT to skip (or improve) this rule that tell us that Lua is a subject and &quot;o Sol&quot; an object when we have connections between two entities (&quot;e&quot;, cc)  + (&quot;Lua&quot;, conj) ?</p>
<p>Is this right ?</p>
<p>If I use deps to create a KG to KBQA from a question &quot;O sol é a lua ?&quot; I will have entries like this:</p>
<pre><code>E: Triple [subject=é, predicate=ser, object=a lua]
[E: é ser a lua]
F: Triple [subject=O sol, predicate=ser, object=a lua]
[F: O sol ser a lua]
</code></pre>
<p>So, I think will be a problem. I think that correct would be &quot;O Sol e a Lua&quot; are two subjects connected  by ('e' cc) being one only entity (or two?) suffering the action of same verb.</p>
",Named Entity Recognition (NER),dep extraction rule using nlpport trying extract deps chunk entity even depchunks sentence extract lua e sol est muito distantes got entry like adjust pipeline nlpport skip improve rule tell u lua subject sol object connection two entity e cc lua conj right use deps create kg kbqa question sol lua entry like think problem think correct would sol e lua two subject connected e cc one entity two suffering action verb
How to process a very long document in spaCy?,"<p>I'm trying to perform a NLP analysis of a text in Spanish. So, to do lemmatization I'm using Spacy, because NLTK has not a Spanish version for lemma. The problem with Spacy is that I a have restriction in the numbers of words that I can pass through Lemmatizer:</p>
<blockquote>
<p><strong>ValueError:</strong> [E088] Text of length 6095095 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may
cause memory allocation errors. If you're not using the parser or NER,
it's probably safe to increase the <code>nlp.max_length</code> limit. The limit
is in number of characters, so you can check whether your inputs are
too long by checking <code>len(text)</code>.</p>
</blockquote>
<p>I tried with <code>nlp.max_length= 6095095</code> but the session crashed after using all available RAM.</p>
<p>Any suggestion?</p>
",Named Entity Recognition (NER),process long document spacy trying perform nlp analysis text spanish lemmatization using spacy nltk ha spanish version lemma problem spacy restriction number word pas lemmatizer valueerror e text length exceeds maximum parser ner model require roughly gb temporary memory per character input mean long text may cause memory allocation error using parser ner probably safe increase limit limit number character check whether input long checking tried session crashed using available ram suggestion
Spacy Custom Name Entity Recognition (NER) &#39;catastrophic forgetting&#39; issue,"<p>The model is unable to remember the previous labels on which it was trained
i know that its 'catastrophic forgetting', but no example or blog seems to help this issue.
the most common response for this is this blog is this <a href=""https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting"" rel=""nofollow noreferrer"">https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting</a> but this is pretty old now and is not helping</p>
<p>Here is my code:</p>
<pre><code>from __future__ import unicode_literals, print_function
import json
labeled_data = []
with open(r&quot;/content/emails_labeled.jsonl&quot;, &quot;r&quot;) as read_file:
    for line in read_file:
        data = json.loads(line)
        labeled_data.append(data)

TRAIN_DATA = []
for entry in labeled_data:
    entities = []
    for e in entry['labels']:
        entities.append((e[0], e[1],e[2]))
    spacy_entry = (entry['text'], {&quot;entities&quot;: entities})
    TRAIN_DATA.append(spacy_entry)       
import plac
import random
import warnings
from pathlib import Path
import spacy
from spacy.util import minibatch, compounding


# new entity label
LABEL = &quot;OIL&quot;

# training data
# Note: If you're using an existing model, make sure to mix in examples of
# other entity types that spaCy correctly recognized before. Otherwise, your
# model might learn the new type, but &quot;forget&quot; what it previously knew.
# https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting
'''
TRAIN_DATA = [
    (
        &quot;Horses are too tall and they pretend to care about your feelings&quot;,
        {&quot;entities&quot;: [(0, 6, LABEL)]},
    ),
    (&quot;Do they bite?&quot;, {&quot;entities&quot;: []}),
    (
        &quot;horses are too tall and they pretend to care about your feelings&quot;,
        {&quot;entities&quot;: [(0, 6, LABEL)]},
    ),
    (&quot;horses pretend to care about your feelings&quot;, {&quot;entities&quot;: [(0, 6, LABEL)]}),
    (
        &quot;they pretend to care about your feelings, those horses&quot;,
        {&quot;entities&quot;: [(48, 54, LABEL)]},
    ),
    (&quot;horses?&quot;, {&quot;entities&quot;: [(0, 6, LABEL)]}),
]
'''

@plac.annotations(
    model=(&quot;Model name. Defaults to blank 'en' model.&quot;, &quot;option&quot;, &quot;m&quot;, str),
    new_model_name=(&quot;New model name for model meta.&quot;, &quot;option&quot;, &quot;nm&quot;, str),
    output_dir=(&quot;Optional output directory&quot;, &quot;option&quot;, &quot;o&quot;, Path),
    n_iter=(&quot;Number of training iterations&quot;, &quot;option&quot;, &quot;n&quot;, int),
)
def main(model='/content/LinkModelOutput', new_model_name=&quot;Oil21&quot;, output_dir='/content/Last', n_iter=30):
    &quot;&quot;&quot;Set up the pipeline and entity recognizer, and train the new entity.&quot;&quot;&quot;
    random.seed(0)
    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print(&quot;Loaded model '%s'&quot; % model)
    else:
        nlp = spacy.blank(&quot;en&quot;)  # create blank Language class
        print(&quot;Created blank 'en' model&quot;)
    # Add entity recognizer to model if it's not in the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if &quot;ner&quot; not in nlp.pipe_names:
        ner = nlp.create_pipe(&quot;ner&quot;)
        nlp.add_pipe(ner)
    # otherwise, get it, so we can add labels to it
    else:
        ner = nlp.get_pipe(&quot;ner&quot;)

    ner.add_label(LABEL)  # add new entity label to entity recognizer
    # Adding extraneous labels shouldn't mess anything up
    #ner.add_label(&quot;VEGETABLE&quot;)
    if model is None:
        optimizer = nlp.begin_training()
    else:
        optimizer = nlp.resume_training()
    move_names = list(ner.move_names)
    # get names of other pipes to disable them during training
    pipe_exceptions = [&quot;ner&quot;, &quot;trf_wordpiecer&quot;, &quot;trf_tok2vec&quot;]
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]
    # only train NER
    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():
        # show warnings for misaligned entity spans once
        warnings.filterwarnings(&quot;once&quot;, category=UserWarning, module='spacy')

        sizes = compounding(1.0, 4.0, 1.001)
        # batch up the examples using spaCy's minibatch
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            batches = minibatch(TRAIN_DATA, size=sizes)
            losses = {}
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.entity.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)
            print(&quot;Losses&quot;, losses)

    # test the trained model
    test_text = &quot;Here is Hindustan petroleum's oil reserves coup in Australia. Details can be found at https://www.textfixer.com/tools/remove-line-breaks.php?&quot;
    doc = nlp(test_text)
    print(&quot;Entities in '%s'&quot; % test_text)
    for ent in doc.ents:
        print(ent.label_, ent.text)

    # save model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.meta[&quot;name&quot;] = new_model_name  # rename model
        nlp.to_disk(output_dir)
        print(&quot;Saved model to&quot;, output_dir)

        # test the saved model
        print(&quot;Loading from&quot;, output_dir)
        nlp2 = spacy.load(output_dir)
        # Check the classes have loaded back consistently
        assert nlp2.get_pipe(&quot;ner&quot;).move_names == move_names
        doc2 = nlp2(test_text)
        for ent in doc2.ents:
            print(ent.label_, ent.text)


if __name__ == &quot;__main__&quot;:
    plac.call(main)
</code></pre>
<p>and the data annotation was done on 'Daccano'.
Here is a look at the data:</p>
<pre><code>{&quot;id&quot;: 174, &quot;text&quot;: &quot;service\tmarathon petroleum reduces service postings marathon petroleum co said it reduced the contract price it will pay for all grades of service oil one dlr a barrel effective today the decrease brings marathon s posted price for both west texas intermediate and west texas sour to dlrs a bbl the south louisiana sweet grade of service was reduced to dlrs a bbl the company last changed its service postings on jan reuter&quot;, &quot;meta&quot;: {}, &quot;annotation_approver&quot;: null, &quot;labels&quot;: [[61, 70, &quot;OIL&quot;], [147, 150, &quot;OIL&quot;]]}
{&quot;id&quot;: 175, &quot;text&quot;: &quot;mutual funds\tmunsingwear inc mun th qtr jan loss shr loss cts vs loss seven cts net loss vs loss revs mln vs mln year shr profit cts vs profit cts net profit vs profit revs mln vs mln avg shrs vs note per shr adjusted for for stock split july and for split may reuter&quot;, &quot;meta&quot;: {}, &quot;annotation_approver&quot;: null, &quot;labels&quot;: []}
</code></pre>
",Named Entity Recognition (NER),spacy custom name entity recognition ner catastrophic forgetting issue model unable remember previous label wa trained know catastrophic forgetting example blog seems help issue common response blog pretty old helping code data annotation wa done daccano look data
Do I need to use Named Entity Recognition (NER) in tokenization?,"<p>I am working on an NLP project for sentiment analysis. I am using SpaCy to tokenize sentences. As I was reading the <a href=""https://spacy.io/usage/linguistic-features#named-entities"" rel=""nofollow noreferrer"">documentation</a>, I learned about NER. I've read that it can be used to extract entities from text for aiding a user's searching.</p>
<p>The thing I am trying to understand is how to embody it (<em>if I should</em>) in my tokenization process. I am giving an example.</p>
<pre class=""lang-py prettyprint-override""><code>text = &quot;Let's not forget that Apple Pay in 2014 required a brand new iPhone in order to use it.  A significant portion of Apple's user base wasn't able to use it even if they wanted to.  As each successive iPhone incorporated the technology and older iPhones were replaced the number of people who could use the technology increased.&quot;

sentence = sp(text) # sp = spacy.load('en_core_web_sm')

for word in sentence:
    print(word.text)

# Let
# 's
# not
# forget
# that
# Apple
# Pay
# in
# etc...

for word in sentence.ents:
  print(word.text + &quot; _ &quot; + word.label_ + &quot; _ &quot; + str(spacy.explain(word.label_)))

# Apple Pay _ ORG _ Companies, agencies, institutions, etc.
# 2014 _ DATE _ Absolute or relative dates or periods
# iPhone _ ORG _ Companies, agencies, institutions, etc.
# Apple _ ORG _ Companies, agencies, institutions, etc.
# iPhones _ ORG _ Companies, agencies, institutions, etc.
</code></pre>
<p>The first loops shows that 'Apple' and 'Pay' are different tokens. When printing the discovered entities in the second loop, it understands that 'Apply Pay' is an ORG. If yes, how could I achieve that (let's say) &quot;type&quot; of tokenization?</p>
<p>My thinking is, shouldn't 'Apple' and 'Pay' be tokenized as a single word together so that, when I create my classifier it will recognize it as an entity and not recognize a fruit ('Apple') and a verb ('Pay').</p>
",Named Entity Recognition (NER),need use named entity recognition ner tokenization working nlp project sentiment analysis using spacy tokenize sentence wa reading documentation learned ner read used extract entity text aiding user searching thing trying understand embody tokenization process giving example first loop show apple pay different token printing discovered entity second loop understands apply pay org yes could achieve let say type tokenization thinking apple pay tokenized single word together create classifier recognize entity recognize fruit apple verb pay
Named Entity Recognition using TFLite on Android,"<p>I have a named entity recognition TensorFlow Lite model that I trained in Python and that I would like to run on an Android. To make predictions, the model takes in a dictionary of two tensors, namely <code>input_ids</code> and <code>attention_mask</code> like so:</p>
<pre><code>Inputs: {'input_ids': &lt;tf.Tensor: shape=(1, 3), dtype=int32, numpy=
         array([[101, 2054, 102]])&gt;, 
         'attention_mask': &lt;tf.Tensor: shape=(1, 3), dtype=int32, numpy=
         array([[1, 1, 1,]])&gt;}
</code></pre>
<p>For context, the model was trained on texts of varying lengths, so the model's default input shape for both <code>input_ids</code> and <code>attention_mask</code> is <code>(1, 1)</code>.</p>
<p>Here's the code I'm using in Kotlin:</p>
<pre><code>// Load model
val nerHelper = NERHelper(getApplication())
val tflite = Interpreter(nerHelper.loadModelFile())

// Set example inputShape
val inputShape = intArrayOf(1, 3)

// Create input_ids and attention_mask tensors
val inputIdsBuffer = TensorBuffer.createFixedSize(inputShape, org.tensorflow.lite.DataType.FLOAT32)
val attentionMaskBuffer = TensorBuffer.createFixedSize(inputShape, org.tensorflow.lite.DataType.FLOAT32)

// Example input_ids
inputIdsBuffer.loadArray(intArrayOf(101, 2054, 102))
// Example attention_mask
attentionMaskBuffer.loadArray(intArrayOf(1, 1, 1))

// Create output buffer
val outputShape = intArrayOf(1, 3, 4)
val outputBuffer = TensorBuffer.createFixedSize(outputShape, org.tensorflow.lite.DataType.FLOAT32)

// Run the model
try {
    val inputs: Array&lt;Any&gt; = arrayOf(inputIdsBuffer.buffer, attentionMaskBuffer.buffer)
    val outputs: MutableMap&lt;Int, Any&gt; = mutableMapOf(0 to outputBuffer.buffer.rewind())

    // Resize inputs for input_ids and attention_mask
    tflite.resizeInput(0, intArrayOf(1, 3))
    tflite.resizeInput(1, intArrayOf(1, 3))

    try {
        tflite.runForMultipleInputsOutputs(inputs, outputs)
    } catch (e: Exception) {
        Log.e(&quot;ModelInference&quot;, &quot;Error during model run&quot;, e)
    }

    tflite.runForMultipleInputsOutputs(inputs, outputs)
    Log.d(&quot;ModelInference&quot;, &quot;Model run successfully&quot;)

    val output = outputBuffer.floatArray
    output.forEach { value -&gt; Log.d(&quot;ModelInference&quot;, &quot;Output: $value&quot;) }
} catch (e: Exception) {
    Log.e(&quot;ModelInference&quot;, &quot;Error during model inference&quot;, e)
}
</code></pre>
<p>When I run this in Kotlin, an error is thrown at</p>
<pre><code>try {
        tflite.runForMultipleInputsOutputs(inputs, outputs)
    } catch (e: Exception) {
        Log.e(&quot;ModelInference&quot;, &quot;Error during model run&quot;, e)
    }
</code></pre>
<p>saying:</p>
<pre><code>java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (serving_default_attention_mask:0) with 4 bytes from a Java Buffer with 3 bytes.
</code></pre>
<p>I understand that this error originates from there existing a discrepancy in the size between my inputs and what the model is expecting, but I'm struggling to figure out what exactly is causing this issue.</p>
",Named Entity Recognition (NER),named entity recognition using tflite android named entity recognition tensorflow lite model trained python would like run android make prediction model take dictionary two tensor namely like context model wa trained text varying length model default input shape code using kotlin run kotlin error thrown saying understand error originates existing discrepancy size input model expecting struggling figure exactly causing issue
gensim for Political Ad verification?,"<p>I am trying to build a model that uses transcribed audio and on-screen text to classify a video ad as political or non-political, as well as extracts the name of the candidate and sponsor. How can I go about doing this? One possible solution I thought of was using a dictionary of commonly used (or necessitated phrases) like &quot;I approve this message&quot; and &quot;This ad has been sponsored by ..., Vote for&quot;
and using Spacy NER to extract the candidate's name from these phrases.
If anyone has any solutions/suggestions please let me know.</p>
<p>Trying to classify the ad as political or non-political and extract the candidate's name, constituency, and party</p>
",Named Entity Recognition (NER),gensim political ad verification trying build model us transcribed audio screen text classify video ad political non political well extract name candidate sponsor go one possible solution thought wa using dictionary commonly used necessitated phrase like approve message ad ha sponsored vote using spacy ner extract candidate name phrase anyone ha solution suggestion please let know trying classify ad political non political extract candidate name constituency party
Problems with Named Entity Recognition in spaCy using German de_dep_news_trf Pipeline,"<p>I'm currently working on a project using spaCy with the German trained pipeline <code>de_dep_news_trf</code>.</p>
<p>Unfortunately, I'm having issues with named entity recognition (NER).</p>
<p>When I run a simple sentence like <em>&quot;Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin.&quot;</em>, no entities are detected.</p>
<p>I've followed these steps to set up my Python environment (3.12)(Windows) in a PyCharm Community project:</p>
<pre class=""lang-bash prettyprint-override""><code>python.exe -m pip install --upgrade pip
pip install -U pip setuptools wheel
pip install -U spacy
python -m spacy download de_dep_news_trf --timeout 600
pip install spacy[transformers]
</code></pre>
<p>Here is a snippet of my code:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy


def process_text_with_spacy(text_to_process):
    doc = nlp(text_to_process)
    data = {
        &quot;text&quot;: text_to_process,
        &quot;sentences&quot;: []
    }
    for sent in doc.sents:
        process_sentence_data = {
            &quot;sentence&quot;: sent.text,
            &quot;entities&quot;: []
        }
        for ent in sent.ents:
            process_sentence_data[&quot;entities&quot;].append({
                &quot;text&quot;: ent.text,
                &quot;start&quot;: ent.start_char,
                &quot;end&quot;: ent.end_char,
                &quot;label&quot;: ent.label_
            })
        data[&quot;sentences&quot;].append(process_sentence_data)
    return data


nlp = spacy.load('de_dep_news_trf')

sample_text = &quot;Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin.&quot;

processed_data = process_text_with_spacy(sample_text)

print(&quot;Text:&quot;, sample_text)
for sentence_data in processed_data[&quot;sentences&quot;]:
    print(&quot;Sentence:&quot;, sentence_data[&quot;sentence&quot;])
    print(&quot;Entities:&quot;, sentence_data[&quot;entities&quot;])
</code></pre>
<p>Output:</p>
<pre class=""lang-bash prettyprint-override""><code>Text: Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin.
Sentence: Berlin ist die Hauptstadt von Deutschland.
Entities: []
Sentence: Angela Merkel war die Bundeskanzlerin.
Entities: []
</code></pre>
<p>When using <code>de_core_news_lg</code>, the output for each sentence is:</p>
<pre class=""lang-bash prettyprint-override""><code>Text: Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin.
Sentence: Berlin ist die Hauptstadt von Deutschland.
Entities: [{'text': 'Berlin', 'start': 0, 'end': 6, 'label': 'LOC'}, {'text': 'Deutschland', 'start': 30, 'end': 41, 'label': 'LOC'}]
Sentence: Angela Merkel war die Bundeskanzlerin.
Entities: [{'text': 'Angela Merkel', 'start': 43, 'end': 56, 'label': 'PER'}]
</code></pre>
<p>However, when I use <code>de_dep_news_trf</code>, the results are empty.
Model <code>de_dep_news_trf</code> is selected based on &quot;accuracy&quot; from the SpaCy website.</p>
<p>Could someone explain why <code>de_dep_news_trf</code> does not return the same result? Is there a specific reason or setting that could cause this difference?</p>
<p>Thank you for your help!</p>
",Named Entity Recognition (NER),problem named entity recognition spacy using german de dep news trf pipeline currently working project using spacy german trained pipeline unfortunately issue named entity recognition ner run simple sentence like berlin ist die hauptstadt von deutschland angela merkel war die bundeskanzlerin entity detected followed step set python environment window pycharm community project snippet code output using output sentence however use result empty model selected based accuracy spacy website could someone explain doe return result specific reason setting could cause difference thank help
Information extracting from plain text using NLP,"<p>Me and my friends working on a hobby project and trying to extract data from plain text. Not something too complicated, just trying to extract name, birth date or somethings like that.</p>
<p>Let's say that we have a text file like this,</p>
<p>&quot;Hello my name is John and I'm 22 years old. I'm living in USA and I like playing video games&quot;</p>
<p>We want to fill a table like this
Name: John
Age: 22
From: USA</p>
<p>Looking for NLP since like last week and I don't even know where to start. Every kind of help appreciated.</p>
",Named Entity Recognition (NER),information extracting plain text using nlp friend working hobby project trying extract data plain text something complicated trying extract name birth date somethings like let say text file like hello name john year old living usa like playing video game want fill table like name john age usa looking nlp since like last week even know start every kind help appreciated
Downloading transformers models to use offline,"<p>I have a trained transformers NER model that I want to use on a machine not connected to the internet. When loading such a model, currently it downloads cache files to the .cache folder.</p>
<p>To load and run the model offline, you need to copy the files in the .cache folder to the offline machine. However, these files have long, non-descriptive names, which makes it really hard to identify the correct files if you have multiple models you want to use. Any thoughts on this?</p>
<p>Example of model files</p>
<p><a href=""https://i.sstatic.net/0CFZj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0CFZj.png"" alt=""enter image description here"" /></a></p>
",Named Entity Recognition (NER),downloading transformer model use offline trained transformer ner model want use machine connected internet loading model currently downloads cache file cache folder load run model offline need copy file cache folder offline machine however file long non descriptive name make really hard identify correct file multiple model want use thought example model file
Custom Named Entity Recognition (NER) Model with spaCy V3,"<p>This is my first time building a custom model with SPACY NER.</p>
<pre><code># Define a function to create spaCy DocBin objects from the annotated data
def get_spacy_doc(file, data):
  # Create a blank spaCy pipeline
  nlp = spacy.blank('en')
  db = DocBin()

  # Iterate through the data
  for text, annot in tqdm(data):
    doc = nlp.make_doc(text)
    annot = annot['entities']

    ents = []
    entity_indices = []

    # Extract entities from the annotations
    for start, end, label in annot:
      skip_entity = False
      for idx in range(start, end):
        if idx in entity_indices:
          skip_entity = True
          break
      if skip_entity:
        continue

      entity_indices = entity_indices + list(range(start, end))
      try:
        span = doc.char_span(start, end, label=label, alignment_mode='strict')
      except:
        continue

      if span is None:
        # Log errors for annotations that couldn't be processed
        err_data = str([start, end]) + &quot;    &quot; + str(text) + &quot;\n&quot;
        file.write(err_data)
      else:
        ents.append(span)

    try:
      doc.ents = ents
      db.add(doc)
    except:
      pass

  return db
</code></pre>
<pre><code># Split the annotated data into training and testing sets
from sklearn.model_selection import train_test_split
train, test = train_test_split(cv_data, test_size=0.2)

# Display the number of items in the training and testing sets
len(train), len(test)

# Open a file to log errors during annotation processing
file = open('/content/drive/MyDrive/trial_domain_extraction/trained_models/train_file.txt','w')

# Create spaCy DocBin objects for training and testing data
db = get_spacy_doc(file, train)
db.to_disk('/content/drive/MyDrive/trial_domain_extraction/trained_models/train_data.spacy')

db = get_spacy_doc(file, test)
db.to_disk('/content/drive/MyDrive/trial_domain_extraction/trained_models/test_data.spacy')

# Close the error log file
file.close()
</code></pre>
<pre><code># Train a spaCy NER model using the provided configuration and data
!python -m spacy train /content/drive/MyDrive/trial_domain_extraction/config/config.cfg  --output /content/drive/MyDrive/trial_domain_extraction/trained_models/output  --paths.train /content/drive/MyDrive/trial_domain_extraction/trained_models/train_data.spacy  --paths.dev /content/drive/MyDrive/trial_domain_extraction/trained_models/test_data.spacy  --gpu-id 0
</code></pre>
<p>Output pipeline:</p>
<pre><code>============================= Training pipeline =============================
ℹ Pipeline: []
ℹ Initial learn rate: 0.001
E    #       SCORE 
---  ------  ------
  0       0    0.00
100     200    0.00
200     400    0.00
300     600    0.00
400     800    0.00
500    1000    0.00
600    1200    0.00
700    1400    0.00
800    1600    0.00
</code></pre>
<p>I have followed the following article to the T.<a href=""https://medium.com/@mjghadge9007/building-your-own-custom-named-entity-recognition-ner-model-with-spacy-v3-a-step-by-step-guide-15c7dcb1c416"" rel=""nofollow noreferrer"">text</a></p>
<p>My pipeline looks empty (please refer above.) Is it because there was too little training data?
I have used 13 entities. Would you suggest training with more amount of annotated data would resolve this issue? If yes, how much quantitatively?</p>
",Named Entity Recognition (NER),custom named entity recognition ner model spacy v first time building custom model spacy ner output pipeline followed following article text pipeline look empty please refer wa little training data used entity would suggest training amount annotated data would resolve issue yes much quantitatively
Information extraction from borderless table,"<p>How can I extract value from the following reports with different report structures? <a href=""https://i.sstatic.net/wjiUcoXY.png"" rel=""nofollow noreferrer"">enter image description here</a> <a href=""https://i.sstatic.net/82ywSyHT.png"" rel=""nofollow noreferrer"">enter image description here</a><a href=""https://i.sstatic.net/kZRqyjyb.png"" rel=""nofollow noreferrer"">enter image description here</a> <a href=""https://i.sstatic.net/Ep0tsZPa.png"" rel=""nofollow noreferrer"">enter image description here</a><a href=""https://i.sstatic.net/Z4uiBpmS.png"" rel=""nofollow noreferrer"">enter image description here</a><a href=""https://i.sstatic.net/Ex8RUuZP.png"" rel=""nofollow noreferrer"">enter image description here</a> <a href=""https://i.sstatic.net/XWRIPWSc.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I have tried converting it to txt first, after that the entities from the report such as assets, current assets, etc. will be predicted using XLM-RoBERTa with NER</p>
",Named Entity Recognition (NER),information extraction borderless table extract value following report different report structure enter image description enter image description hereenter image description enter image description hereenter image description hereenter image description enter image description tried converting txt first entity report asset current asset etc predicted using xlm roberta ner
Filtering stop words out of a multiple text files (using a list of stop words),"<p>I have a folder named <strong>cleaned_texts</strong>. The folder contains text files(a.txt, b.txt, c.txt etc) and each text file contains tokenized words in this format:<strong>['Rise', 'of', 'e-health', 'and', 'its', 'Germany', 'dollar']</strong>.</p>
<p>Example:</p>
<p>a.txt contains <strong>['Rise', 'of', 'e-health', 'and', 'its', 'Thailand', 'YEN', 'India']</strong> and</p>
<p>b.txt contains <strong>['PESO', 'Man', 'development', 'never', 'Japan', 'year', 'date', 'Canada']</strong>.</p>
<p>I also have another folder named <strong>StopWords</strong> which also contains text files and each text file contains a stop word. The text files are named in this format (currency.txt, names.txt, geographic.txt etc).</p>
<p>Example:</p>
<p>currency.txt contains names of currencies <strong>(Eg: BAHT | Thailand, PESO  | Mexico, YEN | Japan etc)</strong>.</p>
<p>geographic.txt contains names of countries <strong>(Eg: Canada, China, India, Germany etc)</strong>.</p>
<p>I want to filter all the stop words contained in the text files inside the StopWords folder, from all the text files in the cleaned_texts folder.</p>
<p>I looped through the stop words folder, Combined all the stop words and converted it to a list. My challenge is how to filter the stop words from my cleaned_texts files. I have been on it for days now but i couldn't figure out how to do it.</p>
<p>Here is my script:</p>
<pre><code>import glob
import codecs
import os

#Cleaned texts
os.getcwd()
clean_texts_folder =  os.path.join(os.getcwd(), 'cleaned_texts')

clean_text_data = []
for root, folders, files in os.walk(clean_texts_folder):
    for file in files:
        path = os.path.join(root, file)
        with codecs.open(path, encoding='utf-8', errors='ignore') as info:
            clean_text_data.append(info.read())


#Stop Words
stopwords_folder_path = &quot;StopWords&quot;
stopwords_files = glob.glob(os.path.join(stopwords_folder_path, '*.txt'))

for file in stopwords_files:
    with open(file, 'r') as w:
        stop_words = w.read()
        
        map_dict = {'|': ''}
        res = ''.join(
            idx if idx not in map_dict else map_dict[idx] for idx in stop_words)
        new_list = res.split()

#new_list Output= ['SMITH', 'Surnames', 'from', '1990', 'Thailand', 'YEN', 'India', 'PESO', 'Japan', 'Canada']


#Trying to save the filtered texts
folder_name = &quot;new_texts&quot;
Path(folder).mkdir(parents=True, exist_ok=True)
filtered_sentence = []
for index, word in enumerate(clean_text_data):
    if word not in new_list:
        #print(filtered_sentence.append(word))
        file_path = Path(folder_name, f&quot;{index}.txt&quot;)
        with pathlib.Path.open(file_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
           f.write(f&quot;{filtered_sentence }&quot;)

</code></pre>
<p><strong>Actual/Resulting Output:</strong>
&quot;None&quot; is printing in all the text files.</p>
<p>a.txt = None</p>
<p>b.txt = None</p>
<p>c.txt = None</p>
<p><strong>Expected Output:</strong></p>
<p>a.txt = ['Rise', 'of', 'e-health', 'and', 'its']</p>
<p>b.txt = ['Man', 'development', 'never','year', 'date']</p>
",Named Entity Recognition (NER),filtering stop word multiple text file using list stop word folder named cleaned text folder contains text file txt b txt c txt etc text file contains tokenized word format rise e health germany dollar example txt contains rise e health thailand yen india b txt contains peso man development never japan year date canada also another folder named stopwords also contains text file text file contains stop word text file named format currency txt name txt geographic txt etc example currency txt contains name currency eg baht thailand peso mexico yen japan etc geographic txt contains name country eg canada china india germany etc want filter stop word contained text file inside stopwords folder text file cleaned text folder looped stop word folder combined stop word converted list challenge filter stop word cleaned text file day figure script actual resulting output none printing text file txt none b txt none c txt none expected output txt rise e health b txt man development never year date
"How to extract specific content, like name or DOB, from a document using NLP and python?","<p>I want to extract very specific content like name, address and dob from a document (say for example, a resume). Assuming I have 1000 of such documents, I want to automate it using machine learning and natural language processing. And preferably python.<br>
How can I do that? or Where do I start?</p>

<p>Update: I am aware of NER but I am looking to extract very specific information from a document which can be loaded into an excel or something.</p>

<p>Example: From a project report, I would like to extract the topic, team member names and tenure of the project.</p>
",Named Entity Recognition (NER),extract specific content like name dob document using nlp python want extract specific content like name address dob document say example resume assuming document want automate using machine learning natural language processing preferably python start update aware ner looking extract specific information document loaded excel something example project report would like extract topic team member name tenure project
How can I process raw text format job posts from LinkedIn or similar sites into a key-value format?,"<p>Ive collected some job posts from linked for research purpose. I would like to get specific datas from these job posts and save inside my sql database. So how can I process the job posts txt files and get specific fields such as title, description, skills, requirements list, notes (if there is any), role, location, benefits etc. And also every job posts have their own formats. I guess I would need to use some NLP techniques like NER but im clueless on how to actually approach this. Im not an ML guy so some suggestions, references would be great.</p>
",Named Entity Recognition (NER),process raw text format job post linkedin similar site key value format ive collected job post linked research purpose would like get specific data job post save inside sql database process job post txt file get specific field title description skill requirement list note role location benefit etc also every job post format guess would need use nlp technique like ner im clueless actually approach im ml guy suggestion reference would great
How do I use a model that can be loaded with AutoModelForSequenceClassification with AutoModelForTokenClassification for Ner fine tuning?,"<p>While I can fine tune the BERT model with the custom data in the specific domain by loading it with AutoModelForTokenClassification, I can load another BERT model trained in that domain only with AutoModelForSequenceClassification and as a result I get a tensor shape error. If this is the case, how can I perform fine tuning using AutoModelForSequenceClassification?</p>
<p>The size of tensor a (716) must match the size of tensor b (512) at non-singleton dimension 1</p>
",Named Entity Recognition (NER),use model loaded automodelforsequenceclassification automodelfortokenclassification ner fine tuning fine tune bert model custom data specific domain loading automodelfortokenclassification load another bert model trained domain automodelforsequenceclassification result get tensor shape error case perform fine tuning using automodelforsequenceclassification size tensor must match size tensor b non singleton dimension
spaCy Custom NER model only works on its own,"<p>I have created a custom NER model for spaCy and can load and run it and it works as expected.</p>
<pre><code>
nlp = spacy.load('/content/drive/MyDrive/Custom_NER/trained_models/output/model-best')

text = &quot;I want to book a ticket for 2 adults and 2 children travelling to Norwich from London Liverpool Street departing at 09:00 and arriving by 14:00 leaving Tomorrow and returning on 14/05/24&quot;

doc = nlp(text)

for ent in doc.ents:
  print(ent.text, &quot;  -&gt;&gt;&gt;&gt;  &quot;, ent.label_)
</code></pre>
<p>Outputs:</p>
<pre><code>```2   -&gt;&gt;&gt;&gt;   STATION
to   -&gt;&gt;&gt;&gt;   TO
Norwich   -&gt;&gt;&gt;&gt;   STATION
from   -&gt;&gt;&gt;&gt;   FROM
London   -&gt;&gt;&gt;&gt;   STATION
Liverpool   -&gt;&gt;&gt;&gt;   STATION
Street   -&gt;&gt;&gt;&gt;   STATION
departing at   -&gt;&gt;&gt;&gt;   TIME_KEY
09:00   -&gt;&gt;&gt;&gt;   TICKET_TIME
arriving by   -&gt;&gt;&gt;&gt;   TIME_KEY
14:00   -&gt;&gt;&gt;&gt;   TICKET_TIME
leaving Tomorrow   -&gt;&gt;&gt;&gt;   TIME_KEY
returning   -&gt;&gt;&gt;&gt;   TO
14/05/24   -&gt;&gt;&gt;&gt;   DATE```
</code></pre>
<p>However, if I try to combine my NER model with a built-in pipeline so I can use other features using</p>
<pre><code>
    nlp = spacy.load('en_core_web_sm', exclude=[&quot;ner&quot;])
    
    ner_nlp = spacy.load('/content/drive/MyDrive/Custom_NER/trained_models/output/model-best')
    nlp.add_pipe(&quot;ner&quot;, source=ner_nlp)

</code></pre>
<p>My output is suddenly:</p>
<pre><code>I want   -&gt;&gt;&gt;&gt;   STATION
to book   -&gt;&gt;&gt;&gt;   STATION
a ticket   -&gt;&gt;&gt;&gt;   STATION
for 2   -&gt;&gt;&gt;&gt;   STATION
adults and   -&gt;&gt;&gt;&gt;   STATION
2 children   -&gt;&gt;&gt;&gt;   STATION
travelling to   -&gt;&gt;&gt;&gt;   STATION
Norwich from   -&gt;&gt;&gt;&gt;   STATION
London Liverpool   -&gt;&gt;&gt;&gt;   STATION
Street departing   -&gt;&gt;&gt;&gt;   STATION
at 09:00   -&gt;&gt;&gt;&gt;   STATION
and arriving   -&gt;&gt;&gt;&gt;   STATION
by 14:00   -&gt;&gt;&gt;&gt;   STATION
leaving Tomorrow   -&gt;&gt;&gt;&gt;   STATION
and returning   -&gt;&gt;&gt;&gt;   STATION
on 14/05/24   -&gt;&gt;&gt;&gt;   STATION
</code></pre>
<p>So I am unsure why this would be happening.
I believe it maybe something to do with the new model using a transformer and the built in model using words2vec</p>
<p>Any help would be gladly appreciated</p>
",Named Entity Recognition (NER),spacy custom ner model work created custom ner model spacy load run work expected output however try combine ner model built pipeline use feature using output suddenly unsure would happening believe maybe something new model using transformer built model using word vec help would gladly appreciated
Feedback on spaCy NER Model Annotations for Recipe Ingredient Extraction,"<p>I'm training a spaCy NER model to specifically identify and categorize lines of ingredients in recipes. The goal is to accurately extract ingredients along with their quantities, units, and preparation instructions from various recipes. Below is an outline of how I'm annotating the data:</p>
<h3>Spanning Labels</h3>
<ul>
<li><strong>Quantity:</strong> Digits related to a unit (e.g., &quot;2&quot;, &quot;3/4&quot;)</li>
<li><strong>Ingredient:</strong> Actual ingredient name (e.g., &quot;sugar&quot;, &quot;milk&quot;)</li>
<li><strong>Measurement Unit:</strong> The unit of measurement (e.g., &quot;cup&quot;, &quot;tbsp&quot;)</li>
<li><strong>Instruction:</strong> Preparation instructions (e.g., &quot;finely diced&quot;, &quot;divided&quot;)</li>
</ul>
<h3>Relationship Labels</h3>
<ul>
<li><strong>quantity_of:</strong> Relates the span label quantity to the ingredient</li>
<li><strong>action_to:</strong> Relates the span label instruction to the ingredient</li>
<li><strong>unit_of:</strong> Relates the span label measurement unit to the quantity</li>
</ul>
<p>I've provided two examples: one simple ingredient line and one complex one.</p>
<p><a href=""https://i.sstatic.net/08yFkwCY.png"" rel=""nofollow noreferrer"">Simple Line</a></p>
<p><a href=""https://i.sstatic.net/fj2kSO6t.png"" rel=""nofollow noreferrer"">Complex Line</a></p>
<p>I'm seeking feedback on a couple of points:</p>
<ol>
<li><strong>Annotation Overkill:</strong> Are there too many categories or overly detailed annotations for effective learning and practical application? Should some categories be merged or omitted?</li>
<li><strong>Training Data Volume:</strong> How much annotated data is typically recommended to achieve a robust model in this type of NER task? I want to ensure the model is reliable across various recipe formats.</li>
</ol>
<p>What I've tried:
I trained a model using a similar labeling set-up, minus the relationship labels and the &quot;Measurement Unit&quot; spanning label. I trained the model on about 700 samples from random recipes. The results were subpar; the model had a hard time identifying certain aspects, specifically in situations where the quantities and their units were not next to each other (e.g. 3 garlic cloves)</p>
",Named Entity Recognition (NER),feedback spacy ner model annotation recipe ingredient extraction training spacy ner model specifically identify categorize line ingredient recipe goal accurately extract ingredient along quantity unit preparation instruction various recipe outline annotating data spanning label quantity digit related unit e g ingredient actual ingredient name e g sugar milk measurement unit unit measurement e g cup tbsp instruction preparation instruction e g finely diced divided relationship label quantity relates span label quantity ingredient action relates span label instruction ingredient unit relates span label measurement unit quantity provided two example one simple ingredient line one complex one simple line complex line seeking feedback couple point annotation overkill many category overly detailed annotation effective learning practical application category merged omitted training data volume much annotated data typically recommended achieve robust model type ner task want ensure model reliable across various recipe format tried trained model using similar labeling set minus relationship label measurement unit spanning label trained model sample random recipe result subpar model hard time identifying certain aspect specifically situation quantity unit next e g garlic clove
Custom spaCy NLP model inside Rasa SpacyNLP pipeline,"<p>I am trying to integrate a custom NER model for my Rasa chatbot but I am having a hard time understanding how the SpacyTokenizer and SpacyFeaturizer in the SpacyNLP pipeline in Rasa are related to my custom NER component (en_CustomNer). I am asking this because my trained model only has &quot;tok2vec&quot; and &quot;ner&quot; components in its pipeline and I am not sure how this affects the initialization of SpacyTokenizer and SpacyFeaturizer in my chatbot pipeline. Here is my config.yml in Rasa (I am using Rasa version 3.6.18 and spaCy version 3.7.4):</p>
<pre><code>language: en

pipeline:
    - name: SpacyNLP
      model: en_CustomNer  ## my custom NER model
    - name: SpacyTokenizer
    - name: SpacyFeaturizer
      pooling: mean
    - name: LexicalSyntacticFeaturizer
    - name: CountVectorsFeaturizer
    - name: CountVectorsFeaturizer
      analyzer: char_wb
      min_ngram: 2
      max_ngram: 4
    - name: DIETClassifier
      epochs: 150
      constrain_similarities: true
    - name: SpacyEntityExtractor
    - name: FallbackClassifier
      threshold: 0.1
      ambiguity_threshold: 0.1
</code></pre>
<p>I have trained my NER model using spaCy by creating a base-config.cfg file using the basic tutorial at <a href=""https://spacy.io/usage/training"" rel=""nofollow noreferrer"">here</a>. I then followed a this  tutorial on Rasa Blog <a href=""https://stackoverflow.com"">rasa-spacy-integration</a> and managed to make it work inside my Rasa pipeline. However, the Rasa blog post I followed takes a different approach and uses a pre-trained pipeline, only replacing the &quot;ner&quot; component for the custom spaCy model. This result in a larger model that has component like &quot;tagger&quot;, &quot;parser&quot;, &quot;lemmatizer&quot; on top of the 2 components (&quot;tok2vec&quot; and &quot;ner&quot;) my model has. I am wondering if there is a reason for this (maybe the Rasa pipeline makes use of the other pre-trained components?) and if there is a problem with my custom NER model only having these 2 components (tok2vec and ner)? Here is the full config.cfg I used for training my model:</p>
<pre><code>[paths]
train = &quot;./ner/train.spacy&quot;
dev = &quot;./ner/test.spacy&quot;
vectors = &quot;en_core_web_lg&quot;
init_tok2vec = null

[system]
gpu_allocator = null
seed = 0

[nlp]
lang = &quot;en&quot;
pipeline = [&quot;tok2vec&quot;,&quot;ner&quot;]
batch_size = 1000
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;}
vectors = {&quot;@vectors&quot;:&quot;spacy.Vectors.v1&quot;}

[components]

[components.ner]
factory = &quot;ner&quot;
incorrect_spans_key = null
moves = null
scorer = {&quot;@scorers&quot;:&quot;spacy.ner_scorer.v1&quot;}
update_with_oracle_cut_size = 100

[components.ner.model]
@architectures = &quot;spacy.TransitionBasedParser.v2&quot;
state_type = &quot;ner&quot;
extra_state_tokens = false
hidden_width = 64
maxout_pieces = 2
use_upper = true
nO = null

[components.ner.model.tok2vec]
@architectures = &quot;spacy.Tok2VecListener.v1&quot;
width = ${components.tok2vec.model.encode.width}
upstream = &quot;*&quot;

[components.tok2vec]
factory = &quot;tok2vec&quot;

[components.tok2vec.model]
@architectures = &quot;spacy.Tok2Vec.v2&quot;

[components.tok2vec.model.embed]
@architectures = &quot;spacy.MultiHashEmbed.v2&quot;
width = ${components.tok2vec.model.encode.width}
attrs = [&quot;NORM&quot;,&quot;PREFIX&quot;,&quot;SUFFIX&quot;,&quot;SHAPE&quot;]
rows = [5000,1000,2500,2500]
include_static_vectors = true

[components.tok2vec.model.encode]
@architectures = &quot;spacy.MaxoutWindowEncoder.v2&quot;
width = 256
depth = 8
window_size = 1
maxout_pieces = 3

[corpora]

[corpora.dev]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.dev}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[corpora.train]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.train}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[training]
dev_corpus = &quot;corpora.dev&quot;
train_corpus = &quot;corpora.train&quot;
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_epochs = 0
max_steps = 20000
eval_frequency = 200
frozen_components = []
annotating_components = []
before_to_disk = null
before_update = null

[training.batcher]
@batchers = &quot;spacy.batch_by_words.v1&quot;
discard_oversize = false
tolerance = 0.2
get_length = null

[training.batcher.size]
@schedules = &quot;compounding.v1&quot;
start = 100
stop = 1000
compound = 1.001
t = 0.0

[training.logger]
@loggers = &quot;spacy.WandbLogger.v3&quot;
project_name = &quot;new_custom_ner&quot;
remove_config_values = []
log_dataset_dir = &quot;./output&quot;
model_log_interval = 1000
entity = null
run_name = null

[training.optimizer]
@optimizers = &quot;Adam.v1&quot;
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001
learn_rate = 0.001

[training.score_weights]
ents_f = 1.0
ents_p = 0.0
ents_r = 0.0
ents_per_type = null

[pretraining]

[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = null
after_init = null

[initialize.components]

[initialize.tokenizer]
</code></pre>
<p>Should I replace my model or is it ok the way it is right now? I am worried that my custom model affects the way the SpacyTokenizer and SpacyFeaturizer work.</p>
",Named Entity Recognition (NER),custom spacy nlp model inside rasa spacynlp pipeline trying integrate custom ner model rasa chatbot hard time understanding spacytokenizer spacyfeaturizer spacynlp pipeline rasa related custom ner component en customner asking trained model ha tok vec ner component pipeline sure affect initialization spacytokenizer spacyfeaturizer chatbot pipeline config yml rasa using rasa version spacy version trained ner model using spacy creating base config cfg file using basic tutorial followed tutorial rasa blog href managed make work inside rasa pipeline however rasa blog post followed take different approach us pre trained pipeline replacing quot ner quot component custom spacy model result larger model ha component like quot tagger quot quot parser quot quot lemmatizer quot top component quot tok vec quot quot ner quot model ha wondering reason maybe rasa pipeline make use pre trained component problem custom ner model component tok vec ner full config cfg used training model p replace model ok way right worried custom model affect way spacytokenizer spacyfeaturizer work
"Named entity recognition (NER) task on a large dataset from a data frame column using chunking, and append to results to the original data frame","<p>I want to perform a NER task on a column of a dataframe. The shape of the dataframe is:</p>
<pre><code>import pandas
    df.shape()
    (1312, 12)
</code></pre>
<p>Now the column I wanted to use is called the <code>TEXT</code> column for the NER task. So I have attempted to first convert the column and all the row values to a single string. The code snippet:</p>
<pre><code># Convert to string type 
df['TEXT']=df['TEXT'].astype('string')
non_empty_texts = [text.strip() for text in df['TEXT'] if text.strip()]
# Join the rows into a single string
joined_text =  '\n'.join(non_empty_texts)
</code></pre>
<p>After that, define the chunk size, split the <code>joined_text</code> into chunks, and process each chunk, and predict the entities using the given model.</p>
<pre><code># Define the chunk size 
chunk_size = 5000  

# Split the joined_text into chunks
chunks = [joined_text[i:i+chunk_size] for i in range(0, len(joined_text), chunk_size)]

# Initialize an empty list to store entities
all_entities = []

# Process each chunk and extract entities
for chunk in chunks:
    entities = model.predict_entities(chunk, labels, threshold=0.5)
    all_entities.extend(entities)
</code></pre>
<p>Finally convert the list of dictionary into a dataframe:</p>
<pre><code># Convert the list of dictionaries to a DataFrame
output= pd.DataFrame(all_entities)
</code></pre>
<p>The issue is the shape of the <code>output</code> is <code>(700,4)</code> and the shape of the original data frame <code>df</code> is <code>13212</code>. Therefore, merging makes no sense. The aim is, I need to have ID column in the original dataframe <code>df</code>, merged to the <code>output</code> dataframe</p>
<p>Any suggestions are much appreciated! Please let me know if you need more information!</p>
",Named Entity Recognition (NER),named entity recognition ner task large dataset data frame column using chunking append result original data frame want perform ner task column dataframe shape dataframe column wanted use called column ner task attempted first convert column row value single string code snippet define chunk size split chunk process chunk predict entity using given model finally convert list dictionary dataframe issue shape shape original data frame therefore merging make sense aim need id column original dataframe merged dataframe suggestion much appreciated please let know need information
Understanding examples of Custom NER using spaCy,"<p>I am wanting to create my own custom Named Entity Recognition using spaCy. Whilst I understand the principles of the process and that I need 50-100 examples to train the model.</p>
<p>What I am struggling to understand is if the number of examples needs to be just be done for each new entity or exponentially for each value the new entity could be?</p>
<p>For example say I had the phrase
&quot;The LA Rams play football at SoFi stadium&quot;
Do I need 50-100 versions of this statement to give named entities to the football team and their stadium?</p>
<p>Or do I need to do 50-100 sentences for all NFL teams and their stadiums individually? i.e.
&quot;The Arizonal Cardinals play ball at State Farm stadium&quot;, &quot;The 49ers play their home games at Levi's stadium&quot;</p>
<p>In American football this is made harder due to sponsorship and these name actually being organizations, but in UK football this is not so common with Wrexham Football Club playing at the Racecourse stadium.</p>
<p>Thank you</p>
",Named Entity Recognition (NER),understanding example custom ner using spacy wanting create custom named entity recognition using spacy whilst understand principle process need example train model struggling understand number example need done new entity exponentially value new entity could example say phrase la ram play football sofi stadium need version statement give named entity football team stadium need sentence nfl team stadium individually e arizonal cardinal play ball state farm stadium er play home game levi stadium american football made harder due sponsorship name actually organization uk football common wrexham football club playing racecourse stadium thank
server hardware dataset for training SpaCy text annotator,"<p>I was looking for a server hardware dataset to train my NER model specifically my SpaCy annotator to detect different components from unstructured data and give me properties associated with the component</p>
<ol>
<li>First option was to manually do it from an open source NER annotator( <a href=""https://tecoholic.github.io/ner-annotator/"" rel=""nofollow noreferrer"">https://tecoholic.github.io/ner-annotator/</a>)</li>
<li>Train it on existing data</li>
<li>create my own dataset</li>
</ol>
<p>if i were to choose any of these routes an advice or suggestions ?or is there any other way i am not looking into</p>
<p><a href=""https://tecoholic.github.io/ner-annotator/"" rel=""nofollow noreferrer"">https://tecoholic.github.io/ner-annotator/</a></p>
",Named Entity Recognition (NER),server hardware dataset training spacy text annotator wa looking server hardware dataset train ner model specifically spacy annotator detect different component unstructured data give property associated component first option wa manually open source ner annotator train existing data create dataset choose route advice suggestion way looking
Training process with the default Spacy configuration file does not produce any log output,"<p>My config.cfg is generated according to the instructions provided at <a href=""https://spacy.io/usage/training/"" rel=""nofollow noreferrer"">Spacy Quickstart</a>
I run the train command:<code>python -m spacy train config.cfg --output ./output --paths.train .\train.spacy --paths.dev .\dev.spacy </code></p>
<pre><code>[paths]
train = null
dev = null
vectors = null
init_tok2vec = null

[system]
gpu_allocator = &quot;pytorch&quot;
seed = 0

[nlp]
lang = &quot;en&quot;
pipeline = [&quot;transformer&quot;,&quot;ner&quot;]
batch_size = 128
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;}
vectors = {&quot;@vectors&quot;:&quot;spacy.Vectors.v1&quot;}

[components]

[components.ner]
factory = &quot;ner&quot;
incorrect_spans_key = null
moves = null
scorer = {&quot;@scorers&quot;:&quot;spacy.ner_scorer.v1&quot;}
update_with_oracle_cut_size = 100

[components.ner.model]
@architectures = &quot;spacy.TransitionBasedParser.v2&quot;
state_type = &quot;ner&quot;
extra_state_tokens = false
hidden_width = 64
maxout_pieces = 2
use_upper = false
nO = null

[components.ner.model.tok2vec]
@architectures = &quot;spacy-transformers.TransformerListener.v1&quot;
grad_factor = 1.0
pooling = {&quot;@layers&quot;:&quot;reduce_mean.v1&quot;}
upstream = &quot;*&quot;

[components.transformer]
factory = &quot;transformer&quot;
max_batch_items = 4096
set_extra_annotations = {&quot;@annotation_setters&quot;:&quot;spacy-transformers.null_annotation_setter.v1&quot;}

[components.transformer.model]
@architectures = &quot;spacy-transformers.TransformerModel.v3&quot;
name = &quot;roberta-base&quot;
mixed_precision = false

[components.transformer.model.get_spans]
@span_getters = &quot;spacy-transformers.strided_spans.v1&quot;
window = 128
stride = 96

[components.transformer.model.grad_scaler_config]

[components.transformer.model.tokenizer_config]
use_fast = true

[components.transformer.model.transformer_config]

[corpora]

[corpora.dev]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.dev}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[corpora.train]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.train}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[training]
accumulate_gradient = 3
dev_corpus = &quot;corpora.dev&quot;
train_corpus = &quot;corpora.train&quot;
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
patience = 1600
max_epochs = 0
max_steps = 20000
eval_frequency = 200
frozen_components = []
annotating_components = []
before_to_disk = null
before_update = null

[training.batcher]
@batchers = &quot;spacy.batch_by_padded.v1&quot;
discard_oversize = true
size = 2000
buffer = 256
get_length = null

[training.logger]
@loggers = &quot;spacy.ConsoleLogger.v1&quot;
progress_bar = false

[training.optimizer]
@optimizers = &quot;Adam.v1&quot;
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001

[training.optimizer.learn_rate]
@schedules = &quot;warmup_linear.v1&quot;
warmup_steps = 250
total_steps = 20000
initial_rate = 0.00005

[training.score_weights]
ents_f = 1.0
ents_p = 0.0
ents_r = 0.0
ents_per_type = null

[pretraining]

[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = null
after_init = null

[initialize.components]

[initialize.tokenizer]
</code></pre>
<p>There are no logs generated during the training process:</p>
<p>=========================== Initializing pipeline ===========================
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaMode
l: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_
head.layer_norm.weight', 'lm_head.layer_norm.bias']</p>
<ul>
<li>This IS expected if you are initializing RobertaModel from the checkpoint of a model trained o
n another task or with another architecture (e.g. initializing a BertForSequenceClassification m
odel from a BertForPreTraining model).</li>
<li>This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that
you expect to be exactly identical (initializing a BertForSequenceClassification model from a Be
rtForSequenceClassification model).
✔ Initialized pipeline</li>
</ul>
<p>============================= Training pipeline =============================
ℹ Pipeline: ['transformer', 'ner']
ℹ Initial learn rate: 0.0
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE</p>
<hr />
<p>The initial_rate specified in the config file is 0.00005, however, during training, it displays 'Initial learn rate: 0.0'.  Is this inconsistency the cause of the problem?
I want to know how to make training process normal.</p>
",Named Entity Recognition (NER),training process default spacy configuration file doe produce log output config cfg generated according instruction provided spacy quickstart run train command log generated training process initializing pipeline weight model checkpoint roberta base used initializing robertamode l lm head bias lm head decoder weight lm head dense bias lm head dense weight lm head layer norm weight lm head layer norm bias expected initializing robertamodel checkpoint model trained n another task another architecture e g initializing bertforsequenceclassification odel bertforpretraining model expected initializing robertamodel checkpoint model expect exactly identical initializing bertforsequenceclassification model rtforsequenceclassification model initialized pipeline training pipeline pipeline transformer ner initial learn rate e loss trans loss ner ents f ents p ents r score initial rate specified config file however training display initial learn rate inconsistency cause problem want know make training process normal
Text conversion processing and extracting information,"<p>I have a call center software which consist of industry specific calls, in which all calls are recorded and then all calls transform into text. After transforming into text, every calls is passed to other agent who is human, who extract all information from text. I have to extract information from AI, or some sort of services and analyze conversation between two if them, then save it in DB.</p>
<p>Let say a call center person name is Bob and the person whom need assisstance from bob is foo. the conversation between the two person is below.</p>
<pre><code>Bob : Hey Foo, I am Bob, How may I help you today?
Foo: Hey, I am good, I have a issue with product which I have purchased from you.

Bob : We never like to hear customers are unhappy. Why don’t you start by giving me your full name and order number so I can try to address this issue for you?
Foo : Yeah, it is a electric bike, and Number is BBM-3344.

and so on...
</code></pre>
<p>Now I have to extract all information from conversation like</p>
<ol>
<li>sentiments (light or harsh mood, happy or sad etc.)</li>
<li>filler words (um, ha, etc words)</li>
<li>confident level</li>
<li>answer is appropriate, answer related to topic</li>
<li>engagements (engagement between the two)</li>
<li>talk time</li>
<li>conversation topic</li>
<li>Number of questions asked.</li>
</ol>
<p>and all other information.</p>
<p>Now my questions are:</p>
<ol>
<li>What information we extract from conversation other than that?</li>
<li>How to extract all information from python and their libraries/packages?</li>
</ol>
",Named Entity Recognition (NER),text conversion processing extracting information call center software consist industry specific call call recorded call transform text transforming text every call passed agent human extract information text extract information ai sort service analyze conversation two save db let say call center person name bob person need assisstance bob foo conversation two person extract information conversation like sentiment light harsh mood happy sad etc filler word um ha etc word confident level answer appropriate answer related topic engagement engagement two talk time conversation topic number question asked information question information extract conversation extract information python library package
Spacy matching countries in messy data,"<p>I have some natural language text that I am trying to parse using <code>spacy</code> but having a hard time getting this to cover all possible variations.</p>
<p>Data is basically a set of recommendations for a set of countries, embedded together in a single text block. I want to extract the list of countries and their associated recommendations.</p>
<p>For example:</p>
<pre><code>Consider implementing the recommendations of the Special Rapporteur on violence against women and CEDAW (India) (Thailand), and France recommends to strengthen measures to increase the participation by ethnic minority women in line with CEDAW recommendations, and consider intensifying human rights education (Ghana).   
</code></pre>
<p>Should be processed into 2 lists:</p>
<pre><code>states = [[&quot;india&quot;, &quot;thailand&quot;], [&quot;ghana&quot;]]
recommendations = [&quot;Consider implementing the recommendations of the Special Rapporteur on violence against women and CEDAW&quot;, &quot;France recommends to strengthen measures to increase the participation by ethnic minority women in line with CEDAW recommendations, and consider intensifying human rights education&quot;]
</code></pre>
<p>So far I have been getting away with a custom sentence segmentation in <code>spacy</code> but this is growing out of hand to handle lots of edge cases such as countries appearing in the form <code>(Country1, Country2 and Country3)</code> or sometimes missing parenthesis or typos in countries. Some countries can also have very different spellings, like <code>iran</code>, <code>islamic republic of iran</code>, <code>iran (islamic republic of)</code>, <code>iran, islamic republic of</code>.</p>
<p>Looking for some guidance on what a proper way would be to handle this. I Was thinking using <code>spacy</code>'s <code>Matcher</code> but it wasn't clear how to apply it to such a use case.</p>
",Named Entity Recognition (NER),spacy matching country messy data natural language text trying parse using hard time getting cover possible variation data basically set recommendation set country embedded together single text block want extract list country associated recommendation example processed list far getting away custom sentence segmentation growing hand handle lot edge case country appearing form sometimes missing parenthesis typo country country also different spelling like looking guidance proper way would handle wa thinking using clear apply use case
Pytesseract to return text inside bounding box,"<p>I am currently trying to do named entity extraction on a set of documents. My plan is:</p>
<ol>
<li>Do OCR using pytesseract</li>
<li>Extract the text</li>
<li>Apply an LLM to get the entities like patient name, age etc.</li>
</ol>
<p>One of the example scans look like this: <a href=""https://i.sstatic.net/4XDAH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4XDAH.png"" alt=""enter image description here"" /></a></p>
<p>The output of pytesseract using: <code>text = pytesseract.image_to_string(image, lang='eng', config='--psm 12')</code> is this:</p>
<pre><code>HR

Community General Hospital

Patient Account #

Medical Record #

12345

INPATIENT REGISTRATION AND SUMMARY FORM

215043

Patient Name (Last) (Fir) (Middle)

‘Attending Physician Number and Name

Patieat Type

Tae Date

“Adin time

Brown, John
</code></pre>
<p>Now, as you can clearly see, <strong>the &quot;Patient Name (Last)(First)(Middle)&quot; comes first</strong> and the <strong>actual name of the patient (Brown, John) comes at the end.</strong> How can I make sure the pytesseract returns the text according to the boxes below, i.e. &quot;Patient Name (Last)(First)(Middle)&quot; followed by &quot;Brown, John&quot; and then proceed to &quot;Attending Physician Number and Name&quot;? If tesseract cannot do it, is there a way to get this for any document?</p>
<p>The reason for this requirement is that the LLM can more accurately tell the patient name if it falls right after &quot;Patient Name&quot; rather than later in the output text.</p>
",Named Entity Recognition (NER),pytesseract return text inside bounding box currently trying named entity extraction set document plan ocr using pytesseract extract text apply llm get entity like patient name age etc one example scan look like output pytesseract using clearly see patient name last first middle come first actual name patient brown john come end make sure pytesseract return text according box e patient name last first middle followed brown john proceed attending physician number name tesseract way get document reason requirement llm accurately tell patient name fall right patient name rather later output text
How to convert Doccano exported JSONL format to spaCy format？,"<p>I want to use my own data set to train a named entity recognition model. The data set is exported by the annotation tool Doccano. The format is JSONL (not JSON), but spaCy does not support such a data format input model. How do I convert it? ?
Here's what my dataset looks like:</p>
<pre><code>{&quot;id&quot;:17,&quot;text&quot;:&quot;In this work, the effect of CFs with zeolite on the mechanical, tribological prop-erties, and structure of PTFE was investigated. \nThe developed materials with a CF content of 1–5 wt.% retained their deformation and strength properties at the level of the initial polymer. \nThe compressive stress of PCM increased by 7–53%, and the yield point by 30% relative to the initial polymer. \nIt was found that with an increase in the content of fillers, the degree of crystallinity increased, and the density decreased in comparison with unfilled PTFE. \nCombining fillers (CF\/Zt) into PTFE reduced the wear rate by 810 times relative to the initial polymer. Tribochemical reactions were shown by IR spectroscopy.\nSEM established the formation of secondary structures in the form of tribofilms on the friction surface, which, together with CFs, protect the surface layer of the material from destruction during friction. \nThe wear resistance of the composite material PTFE\/CF\/Zt was effectively improved, and the coefficient of friction was low compared to PTFE\/CF\/Kl and PTFE\/CF\/Vl.&quot;,&quot;entities&quot;:[{&quot;id&quot;:298,&quot;label&quot;:&quot;composite&quot;,&quot;start_offset&quot;:1049,&quot;end_offset&quot;:1059},{&quot;id&quot;:545,&quot;label&quot;:&quot;composite&quot;,&quot;start_offset&quot;:960,&quot;end_offset&quot;:971},{&quot;id&quot;:299,&quot;label&quot;:&quot;composite&quot;,&quot;start_offset&quot;:1064,&quot;end_offset&quot;:1074},{&quot;id&quot;:607,&quot;label&quot;:&quot;value&quot;,&quot;start_offset&quot;:176,&quot;end_offset&quot;:184}],&quot;relations&quot;:[],&quot;Comments&quot;:[]}
</code></pre>
<p>I have also tried many online methods, but none of them seem to work.</p>
",Named Entity Recognition (NER),convert doccano exported jsonl format spacy format want use data set train named entity recognition model data set exported annotation tool doccano format jsonl json spacy doe support data format input model convert dataset look like also tried many online method none seem work
Need Guidance on Implementing Stratified K-Fold for Sequence Labeling (NER) Dataset,"<p>I'm currently working on a Named Entity Recognition (NER) task and I'm looking to implement Stratified K-Fold cross-validation for my dataset. However, I'm struggling to find specific references or guidelines on how to perform this technique for sequence labeling tasks like NER.</p>
<p>In fact, I already trying to perform this but the process always resulting an error.</p>
<p>Here are the key points I'm interested in:</p>
<ol>
<li><p>Stratified K-Fold for Sequence Labeling: I want to understand how to adapt the standard Stratified K-Fold cross-validation technique for sequence labeling tasks. In NER, each token in a sequence is labeled with a specific entity type (e.g., person, organization, location). How can I ensure that each fold in the cross-validation maintains the same distribution of entity types as the original dataset?</p>
</li>
<li><p>Handling Sequential Dependence: Since sequence labeling tasks have a sequential dependency between tokens, how should I ensure that this dependency is preserved during the cross-validation process? Should I shuffle the entire sequences or just the samples?</p>
</li>
<li><p>Existing References or Implementations: I've searched for resources or existing implementations that demonstrate how to apply Stratified K-Fold specifically to sequence labeling datasets, but I haven't found much relevant information. Are there any papers, articles, or code examples that address this topic directly?</p>
</li>
</ol>
<p>Any guidance, references, or insights on how to implement Stratified K-Fold cross-validation for sequence labeling datasets like NER would be greatly appreciated. Thank you!</p>
",Named Entity Recognition (NER),need guidance implementing stratified k fold sequence labeling ner dataset currently working named entity recognition ner task looking implement stratified k fold cross validation dataset however struggling find specific reference guideline perform technique sequence labeling task like ner fact already trying perform process always resulting error key point interested stratified k fold sequence labeling want understand adapt standard stratified k fold cross validation technique sequence labeling task ner token sequence labeled specific entity type e g person organization location ensure fold cross validation maintains distribution entity type original dataset handling sequential dependence since sequence labeling task sequential dependency token ensure dependency preserved cross validation process shuffle entire sequence sample existing reference implementation searched resource existing implementation demonstrate apply stratified k fold specifically sequence labeling datasets found much relevant information paper article code example address topic directly guidance reference insight implement stratified k fold cross validation sequence labeling datasets like ner would greatly appreciated thank
Dynamic Filtering of Non-Descriptive Adjectives and Nouns in Text Data?,"<p>How can I dynamically filter out non-descriptive adjectives and nouns from text data without relying on predefined lists?</p>
<p>I'm using spaCy for part-of-speech tagging and Named Entity Recognition, but I want a more dynamic approach. Any suggestions? Or should I try a whole different method?</p>
<pre><code>import spacy
from collections import Counter

nlp = spacy.load(&quot;en_core_web_sm&quot;)

text = &quot;&quot;
doc = nlp(text)

def is_descriptive(adjective, noun):
    common_nouns = [list of common nouns that are not usefull]
    if noun.lower() in common_nouns:
        return False
    generic_adjectives = [list of generic adjectives that are not useful]
    if adjective.lower() in generic_adjectives:
        return False
    return True

phrases = []
for i in range(len(doc) - 1):
    if doc[i].pos_ == &quot;ADJ&quot; and doc[i + 1].pos_ == &quot;NOUN&quot; and is_descriptive(doc[i].text, doc[i + 1].text):
        phrases.append(f&quot;{doc[i].text} {doc[i + 1].text}&quot;)

word_freq = Counter(phrases)

most_used_phrases = word_freq.most_common(10)
print(&quot;Adjective and Phrase Frequency&quot;)
for phrase, freq in most_used_phrases:
    print(f&quot;{phrase} {freq}&quot;)

</code></pre>
",Named Entity Recognition (NER),dynamic filtering non descriptive adjective noun text data dynamically filter non descriptive adjective noun text data without relying predefined list using spacy part speech tagging named entity recognition want dynamic approach suggestion try whole different method
Parsing city of origin / destination city from a string,"<p>I have a pandas dataframe where one column is a bunch of strings with certain travel details. My goal is to parse each string to extract the city of origin and destination city (I would like to ultimately have two new columns titled 'origin' and 'destination').</p>

<p>The data:</p>

<pre><code>df_col = [
    'new york to venice, italy for usd271',
    'return flights from brussels to bangkok with etihad from â‚¬407',
    'from los angeles to guadalajara, mexico for usd191',
    'fly to australia new zealand from paris from â‚¬422 return including 2 checked bags'
]
</code></pre>

<p>This should result in:</p>

<pre><code>Origin: New York, USA; Destination: Venice, Italy
Origin: Brussels, BEL; Destination: Bangkok, Thailand
Origin: Los Angeles, USA; Destination: Guadalajara, Mexico
Origin: Paris, France; Destination: Australia / New Zealand (this is a complicated case given two countries)
</code></pre>

<p>Thus far I have tried:
A variety of NLTK methods, but what has gotten me closest is using the <code>nltk.pos_tag</code> method to tag each word in the string. The result is a list of tuples with each word and associated tag. Here's an example...</p>

<pre><code>[('Fly', 'NNP'), ('to', 'TO'), ('Australia', 'NNP'), ('&amp;', 'CC'), ('New', 'NNP'), ('Zealand', 'NNP'), ('from', 'IN'), ('Paris', 'NNP'), ('from', 'IN'), ('â‚¬422', 'NNP'), ('return', 'NN'), ('including', 'VBG'), ('2', 'CD'), ('checked', 'VBD'), ('bags', 'NNS'), ('!', '.')]
</code></pre>

<p>I am stuck at this stage and am unsure how to best implement this. Can anyone point me in the right direction, please? Thanks.</p>
",Named Entity Recognition (NER),parsing city origin destination city string panda dataframe one column bunch string certain travel detail goal parse string extract city origin destination city would like ultimately two new column titled origin destination data result thus far tried variety nltk method ha gotten closest using method tag word string result list tuples word associated tag example stuck stage unsure best implement anyone point right direction please thanks
Replace personal pronoun with previous person mentioned (noisy coref),"<p>I want to do a noisy resolution such that given a personal prounoun, that pronoun is replace by the previous(nearest) person.</p>
<p>For example:</p>
<p><code>Alex is looking at buying a U.K. startup for $1 billion. He is very confident that this is going to happen. Sussan is also in the same situation. However, she has lost hope.</code></p>
<p>the output is:</p>
<p><code>Alex is looking at buying a U.K. startup for $1 billion. Alex is very confident that this is going to happen. Sussan is also in the same situation. However, Susan has lost hope.</code></p>
<p>Another example,</p>
<p><code>Peter is a friend of Gates. But Gates does not like him. </code></p>
<p>In this case, the output would be :</p>
<p><code>Peter is a friend of Gates. But Gates does not like Gates.</code></p>
<p>Yes! This is super noisy.</p>
<p>Using spacy:
I have extracted the <code>Person</code> using NER, but how can I replace pronouns appropriately?</p>
<p>Code:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_sm&quot;)
for ent in doc.ents:
  if ent.label_ == 'PERSON':
    print(ent.text, ent.label_)
</code></pre>
",Named Entity Recognition (NER),replace personal pronoun previous person mentioned noisy coref want noisy resolution given personal prounoun pronoun replace previous nearest person example output another example case output would yes super noisy using spacy extracted using ner replace pronoun appropriately code
How to use SpaCy NER?,"<p>I am working on a mini-project to cluster similar sentences together. Before I can achieve that, I have to perform pre-processing to the extremely dirty data (these data are all user inputs, free text).</p>
<p>One of the pre-processing step that I thought of is to identify each sentence and classify it with a category. Even though it is free text, there are some key token per sentence such as &quot;LOOK&quot;, &quot;REPLACE&quot;, &quot;CHECK&quot;.</p>
<p>To classify each sentences, I looked into NLP and discovered SpaCy. One of the component in SpaCy, NER, seems like the perfect task for this. I believe my use case is that I do not need to use the full SpaCy pipeline for this classification. I also understand that I need to add in my custom labels into the NER.</p>
<p>My question is - can I just add my custom label into SpaCy NER using <code>add_label()</code> and run the NER onto my dataset without the having to re-train the SpaCy model? My end goal is to simply classify the sentences into a category, based on the keywords.</p>
<p>This is quite unclear despite several days of researching.</p>
<p>Greatly appreciate any clarification on this. TIA.</p>
",Named Entity Recognition (NER),use spacy ner working mini project cluster similar sentence together achieve perform pre processing extremely dirty data data user input free text one pre processing step thought identify sentence classify category even though free text key token per sentence look replace check classify sentence looked nlp discovered spacy one component spacy ner seems like perfect task believe use case need use full spacy pipeline classification also understand need add custom label ner question add custom label spacy ner using run ner onto dataset without train spacy model end goal simply classify sentence category based keywords quite unclear despite several day researching greatly appreciate clarification tia
NER grouping into objects,"<p>Given a text, I am able to recognise all entities by using a discriminative NER model. Let us imagine that we have a page of text in which the different entities that make up a group of people are described and for each of them we have data such as 'first name', 'surname' and 'date of birth'. The NER model is able to find all these entities but is unable to group them into the 'person' object. How could I solve this NLP task?</p>
<p>I have tried to group these entities based on the span in the original text using heuristics. but this requires a lot of effort for more complex cases.
Also, if these data are extracted from different pages?</p>
",Named Entity Recognition (NER),ner grouping object given text able recognise entity using discriminative ner model let u imagine page text different entity make group people described data first name surname date birth ner model able find entity unable group person object could solve nlp task tried group entity based span original text using heuristic requires lot effort complex case also data extracted different page
How to get probability of prediction per entity from Spacy NER model?,"<p>I used this <a href=""https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py"" rel=""noreferrer"">official example code</a> to train a NER model from scratch using my own training samples. </p>

<p>When I predict using this model on new text, I want to get the probability of prediction of each entity.</p>

<blockquote>
<pre><code>    # test the saved model
    print(""Loading from"", output_dir)
    nlp2 = spacy.load(output_dir)
    for text, _ in TRAIN_DATA:
        doc = nlp2(text)
        print(""Entities"", [(ent.text, ent.label_) for ent in doc.ents])
        print(""Tokens"", [(t.text, t.ent_type_, t.ent_iob) for t in doc])
</code></pre>
</blockquote>

<p>I am unable to find a method in Spacy to get the probability of prediction of each entity.</p>

<p>How do I get this probability from Spacy? I need it to apply a cutoff on it.</p>
",Named Entity Recognition (NER),get probability prediction per entity spacy ner model used official example code train ner model scratch using training sample predict using model new text want get probability prediction entity unable find method spacy get probability prediction entity get probability spacy need apply cutoff
SpaCy v3 custom NER model training,"<p>I am trying to create a NLP project using spacy and python that extracts entities from text.</p>
<p>I need some custom entities so I created a JSON file with annotated articles that I am using to train my model with.</p>
<p>The issue I am facing is that instead of creating a new custom model, I want to add few entities to a pre existing model like &quot;en_core_web_sm&quot; but when I change my source for [components.ner] in my config file to &quot;en_core_web_sm' I am getting only the new entities output. Its like the pre-trained model lost all its entities and is only using the new custom entities.</p>
<p>Note: This custom trained model is much more accurate than when I use the basic &quot;eng&quot; model but it just does not output the existing entities, just the new custom ones.</p>
<p><a href=""https://i.sstatic.net/CK5LY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CK5LY.png"" alt=""custom trained model output"" /></a></p>
<p><a href=""https://i.sstatic.net/hRnAu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hRnAu.png"" alt=""en_core_web_sm model output"" /></a></p>
<p>Please let me know how I could keep those pre-made entites in my new model.</p>
<p>here is the tutorial I used to create this:
<a href=""https://www.youtube.com/watch?v=p_7hJvl7P2A&amp;t=685s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=p_7hJvl7P2A&amp;t=685s</a></p>
",Named Entity Recognition (NER),spacy v custom ner model training trying create nlp project using spacy python extract entity text need custom entity created json file annotated article using train model issue facing instead creating new custom model want add entity pre existing model like en core web sm change source component ner config file en core web sm getting new entity output like pre trained model lost entity using new custom entity note custom trained model much accurate use basic eng model doe output existing entity new custom one please let know could keep pre made entites new model tutorial used create
Named Entity Recognition on Search Engine Queries with Python,"<p>I'm trying to do Named Entity Recognition on search engine queries with Python.</p>
<p>The big thing about search engine queries are that they are usually incomplete or all lowercase.</p>
<p>For this task, I've been recommended Spacy, NLTK, Stanford NLP, Flair, Transformers by Hugging Face as some approaches to this problem.</p>
<p>I was wondering if anybody in the SO community knew the best approach to dealing with NER for search engine queries, because so far I've ran into problems.</p>
<p>For example, with Spacy:</p>
<pre><code>import spacy

# Load the pre-trained model
nlp = spacy.load(&quot;en_core_web_sm&quot;)

# Process a text
text = &quot;google and apple are looking at buying u.k. startup for $1 billion&quot;
text = &quot;who is barack obama&quot;
doc = nlp(text)

# Extract entities
for ent in doc.ents:
    print(ent.text, ent.label_)
</code></pre>
<p>For the first query I got:</p>
<pre><code>google ORG
u.k. GPE
$1 billion MONEY
</code></pre>
<p>This is a great answer. However, for the search query &quot;who is barack obama&quot;, in lower case, it returned no entities.</p>
<p>I'm sure I'm not the first person to do NER on search engine queries in Python, so I'm hoping to find someone who can point me in the right direction.</p>
",Named Entity Recognition (NER),named entity recognition search engine query python trying named entity recognition search engine query python big thing search engine query usually incomplete lowercase task recommended spacy nltk stanford nlp flair transformer hugging face approach problem wa wondering anybody community knew best approach dealing ner search engine query far ran problem example spacy first query got great answer however search query barack obama lower case returned entity sure first person ner search engine query python hoping find someone point right direction
Implement BiLSTM-CRF for NER task Using Deprecated Frameworks,"<p>I've been trying to build a BiLSTM-CRF model for a Named Entity Recognition task, for which, obviously, I've been using TensorFlow 2.16 and Keras 3.0. However, while trying to implement a CRF layer using deprecated keras_contrib or the tensorflow_addons GitHub resources. I've been running into a myriad of issues as these frameworks might not be compatible with the latest TF and Keras versions. However, I'm unwilling to downgrade to a lower version of TF just for this task. Are there any suitable alternatives present that I can use for accomplishing my task?</p>
<p>Added the code below.</p>
<pre><code>from keras.layers import Embedding, SimpleRNN, Dense, LSTM, GRU, Bidirectional, TimeDistributed, Input
from keras_contrib.layers import CRF
from keras_contrib.losses import crf_loss
from keras_contrib.metrics import crf_accuracy 
import tensorflow as tf
from keras.optimizers import RMSprop
import keras


optimizer = Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999, amsgrad=False)
input = Input(shape=(max_length,))
model= Embedding(vocab_size, embedding_dimension, embeddings_initializer=&quot;uniform&quot;, trainable=False)(input)
model= LSTM(360, return_sequences=True, dropout=0.2, recurrent_dropout=0.2, kernel_initializer=keras.initializers.he_normal())(model)
model= Bidirectional(LSTM(180, return_sequences=True, dropout=0.2, recurrent_dropout=0.2, kernel_initializer=keras.initializers.he_normal()))(model)
model= TimeDistributed(Dense(27, activation='relu'))(model)
crf = CRF(units=27, sparse_target=True)
out = crf(model)
model = keras.Model(input, out)
model.compile(optimizer=optimizer, loss=crf_loss, metrics=[crf_accuracy, 'accuracy'])
model.summary()
</code></pre>
<p>Output:</p>
<pre><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer (InputLayer)        │ (None, 78)             │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ embedding (Embedding)           │ (None, 78, 300)        │     5,727,000 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm (LSTM)                     │ (None, 78, 360)        │       951,840 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ bidirectional (Bidirectional)   │ (None, 78, 360)        │       779,040 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ time_distributed                │ (None, 78, 27)         │         9,747 │
│ (TimeDistributed)               │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ crf (CRF)                       │ (None, 78, 27)         │         1,539 │
└─────────────────────────────────┴────────────────────────┴───────────────┘


 Total params: 7,469,166 (28.49 MB)
 Trainable params: 7,469,166 (28.49 MB)
 Non-trainable params: 0 (0.00 B)
**
</code></pre>
<pre><code>history = model.fit(
    train_text,train_labels,
    validation_data=(val_text, val_labels),
    epochs=10,
    batch_size=32,
    verbose=1,
    callbacks=[MacroF1Callback((val_text, val_labels), (train_text, train_labels))]
)
</code></pre>
<p>Output:</p>
<pre><code>----&gt; 7 history = model.fit(
      8     train_text,train_labels,
      9     validation_data=(val_text, val_labels),

c:\keras\src\utils\traceback_utils.py in error_handler(*args, **kwargs)
    121             # To get the full stack trace, call:
    122             # `keras.config.disable_traceback_filtering()`
--&gt; 123             raise e.with_traceback(filtered_tb) from None
    124         finally:
    125             del filtered_tb

c:\Python310\lib\site-packages\keras_contrib\layers\crf.py in call(self, X, mask)
    290 
    291         if self.test_mode == 'viterbi':
--&gt; 292             test_output = self.viterbi_decoding(X, mask)
    293         else:
    294             test_output = self.get_marginal_prob(X, mask)

c:\Python310\lib\site-packages\keras_contrib\layers\crf.py in viterbi_decoding(self, X, mask)
    557 
...
**module 'keras.backend' has no attribute 'dot'**

Arguments received by CRF.call():
  • X=tf.Tensor(shape=(None, 78, 27), dtype=float32)
  • mask=None
</code></pre>
",Named Entity Recognition (NER),implement bilstm crf ner task using deprecated framework trying build bilstm crf model named entity recognition task obviously using tensorflow kera however trying implement crf layer using deprecated kera contrib tensorflow addons github resource running myriad issue framework might compatible latest tf kera version however unwilling downgrade lower version tf task suitable alternative present use accomplishing task added code output output
"Version incompatibility between Spacy, Cuda, Pytorch and Python","<p>I want run spacy in GPu. The configuration that I installed for the Spacy is defined below</p>
<p>Name: spacy</p>
<p>Version: 3.7.4</p>
<p>The Cuda configuration that have in my Ubuntu 20.04.1 LTS based machine is</p>
<p>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243</p>
<p>I have a restriction to upgrade the Cuda. The PyTorch version that I have installed on the machine is &quot;2.2.1+cu121&quot; which have no support for the Cuda version I have.</p>
<p>I tried to downgrade the Pytorch and found that I need to downgrade the Python version as well.</p>
<p>My current python version is Python 3.12.2. If I go for a compatible Python version for pytorch that should be &gt;=3.6,&lt;3.7.0 for pytorch 1.4.0.</p>
<p>If I downgrade both Pytorch and Python that is not compatible with Spacy version.</p>
<p>I need to do a transformer-based NER task in Spacy. Therefore I am not sure what is the minimum requirement of spacy.</p>
<p>How can I handle this version incompatibility?</p>
",Named Entity Recognition (NER),version incompatibility spacy cuda pytorch python want run spacy gpu configuration installed spacy defined name spacy version cuda configuration ubuntu lts based machine nvcc nvidia r cuda compiler driver copyright c nvidia corporation built sun jul pdt cuda compilation tool release v restriction upgrade cuda pytorch version installed machine cu support cuda version tried downgrade pytorch found need downgrade python version well current python version python go compatible python version pytorch pytorch downgrade pytorch python compatible spacy version need transformer based ner task spacy therefore sure minimum requirement spacy handle version incompatibility
Issue with &#39;ValueError&#39; when computing metrics in NER using transformers library (Tuple is empty),"<p>Description:
I am encountering issues while trying to compute metrics for Named Entity Recognition (NER) using the Hugging Face transformers library. The specific errors are 'ValueError' and I've been struggling to resolve them for quite some time.</p>
<p>Code:
I have a function compute_metrics that takes an EvalPrediction object as input. The function aims to process the predicted logits and true labels, but I'm facing issues with the structure of the EvalPrediction object.</p>
<p>Here is the relevant part of the code:</p>
<pre><code>from transformers.trainer_utils import EvalPrediction


def compute_metrics(eval_preds):
    &quot;&quot;&quot;
    Compute evaluation metrics for Named Entity Recognition (NER) tasks.

    Parameters:
    eval_preds (EvalPrediction): An object containing the predicted logits and the true labels.

    Returns:
    A dictionary containing precision, recall, F1 score, and accuracy.
    &quot;&quot;&quot;
    if not isinstance(eval_preds, EvalPrediction):
        raise ValueError(&quot;Invalid eval_preds structure. Expected an EvalPrediction object.&quot;)

    predictions = eval_preds.predictions

    if isinstance(predictions, tuple):
        if len(predictions) &gt; 0:
            pred_logits = predictions[0]
        else:
            raise ValueError(&quot;Tuple predictions is empty.&quot;)
    else:
        pred_logits = predictions

    # Ensure pred_logits has at least two dimensions
    if len(pred_logits.shape) == 1:
        pred_logits = np.expand_dims(pred_logits, axis=0)

    # Rest of your code...
    # Get predicted labels by argmax along the token dimension
    pred_labels = np.argmax(pred_logits, axis=2)
    # ... (rest of your code)
      # Filter out padding tokens where label is -100
    predictions = [
        [label_list[pred] for (pred, label) in zip(pred_label, true_label) if label != -100]
        for pred_label, true_label in zip(pred_labels, labels)
    ]

    # Filter out padding tokens in true labels
    true_labels = [
        [label_list[label] for label in true_label if label != -100]
        for true_label in labels
    ]

    # Compute metrics
    results = metric.compute(predictions=predictions, references=true_labels)

    return {
        &quot;precision&quot;: results[&quot;overall_precision&quot;],
        &quot;recall&quot;: results[&quot;overall_recall&quot;],
        &quot;f1&quot;: results[&quot;overall_f1&quot;],
        &quot;accuracy&quot;: results[&quot;overall_accuracy&quot;],
    }

trainer = Trainer(
    model,
    args,
   train_dataset=tokenized_datasets[&quot;train&quot;],
   eval_dataset=tokenized_datasets[&quot;validation&quot;],
   data_collator=data_collator,
   tokenizer=tokenizer,
   compute_metrics=compute_metrics
)

trainer.train()

</code></pre>
<p>Error Messages:
The errors I'm encountering are as follows:</p>
<pre><code>
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-103-3435b262f1ae&gt; in &lt;cell line: 1&gt;()
----&gt; 1 trainer.train()

5 frames
&lt;ipython-input-101-8c3cb1696dcb&gt; in compute_metrics(eval_preds)
     21             pred_logits = predictions[0]
     22         else:
---&gt; 23             raise ValueError(&quot;Tuple predictions is empty.&quot;)
     24     else:
     25         pred_logits = predictions

ValueError: Tuple predictions is empty.

</code></pre>
<p>Objective:
I'm seeking assistance to understand and resolve these errors. I suspect the issue might be related to the structure of the EvalPrediction object and how I'm accessing the predicted logits.</p>
<p>Additional Information:</p>
<p>I'm using the Hugging Face transformers library for NER.
I have verified that the input data and labels are correctly formatted.
My full code is <a href=""https://colab.research.google.com/drive/1yUGdIiPuB0-JnojCxbLnrRPAsv4HjZ2O?usp=sharing"" rel=""nofollow noreferrer"">here</a></p>
",Named Entity Recognition (NER),issue valueerror computing metric ner using transformer library tuple empty description encountering issue trying compute metric named entity recognition ner using hugging face transformer library specific error valueerror struggling resolve quite time code function compute metric take evalprediction object input function aim process predicted logits true label facing issue structure evalprediction object relevant part code error message error encountering follows objective seeking assistance understand resolve error suspect issue might related structure evalprediction object accessing predicted logits additional information using hugging face transformer library ner verified input data label correctly formatted full code
Is there a faster method to process pandas list of string values,"<p>There are 13000 values approximately for a given column. The below function works in a way that the input is a list of strings and does the NER tagging for each word in the list. On an average there could be 300 words in a list across 13000 values. It takes around more than 1 hour for the function to process the current column. Hence, I would like to have a solution which processed it faster. I am running on azure ml notebook with a standard CPU compute.</p>
<p>Function :</p>
<pre><code>def perform_ner_batch(texts):
    if not texts:  # Check if texts is empty
        return []
    # Perform NER on the provided texts
    list_entity = []
    for i in texts:
      ner_result = ner_pipeline(i)
      if ner_result == []:
        list_entity.append('O')
      for results in ner_result:
        list_entity.append(results['entity_group'])
    return list_entity

</code></pre>
<p>Calling the function:</p>
<pre><code>df['entities'] = df['Tokenized_Abstract_list'].apply(lambda x: perform_ner_batch(x))

</code></pre>
",Named Entity Recognition (NER),faster method process panda list string value value approximately given column function work way input list string doe ner tagging word list average could word list across value take around hour function process current column hence would like solution processed faster running azure ml notebook standard cpu compute function calling function
"NLP, NER --&gt; python extraction of personal informations (like names, surnames, fiscal codes)","<p>i am working on a project to extract personal information from custom documents. In particular, i have a txt file with a lot of names, surnames and information but i would like to extract names and italian fiscal codes.
My actual approach is based on regex but i am not very satisfied because the regex pattern does match always all I need. I was thinking about an NLP approach but i do not know how. I think that actually there are no libraries trained on italian vocabulary. Please, could you kindly help me or give me a few advices?
Thank you very much in advance!!</p>
<p>I have tried an approach based on regex which works well on standard documents, on strongly custom documents it often fails.</p>
",Named Entity Recognition (NER),nlp ner python extraction personal information like name surname fiscal code working project extract personal information custom document particular txt file lot name surname information would like extract name italian fiscal code actual approach based regex satisfied regex pattern doe match always need wa thinking nlp approach know think actually library trained italian vocabulary please could kindly help give advice thank much advance tried approach based regex work well standard document strongly custom document often fails
How to use Azure ML-created labels to train custom models in Cognitive Services for Language?,"<p>I would like to use the labeling environment from Azure Machine Learning to label named entities and classifications. I want to use these labels to train a custom NER and custom text classification model using Azure Cognitive Service for Language. The reason why I want to use the labeling environment in Azure ML, rather than the labeling tool of Azure Cognitive Services for Language itself is because especially the text classification labeling is a lot more convenient in Azure ML.</p>
<p>From what I read in the <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-text-labeling-projects"" rel=""nofollow noreferrer"">documentation</a> Azure ML exports these labels to an &quot;Azure Machine Learning dataset (v1) with labels&quot;, CSV file (in the case of text classification labels) or a CoNLL file (in the case of NER labels). However, to train a custom NER or text classification model in Azure Cognitive Services for Language I can only import labeled data as a json file in <a href=""https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/custom-text-classification/concepts/data-formats?tabs=multi-classification"" rel=""nofollow noreferrer"">this format for text classification</a> and <a href=""https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/custom-named-entity-recognition/concepts/data-formats"" rel=""nofollow noreferrer"">this format for NER</a>. Is there any way to export the labels from Azure Machine Learning as a json file? If not, does anyone know how to convert the Azure Machine Learning dataset to a json file?</p>
",Named Entity Recognition (NER),use azure ml created label train custom model cognitive service language would like use labeling environment azure machine learning label named entity classification want use label train custom ner custom text classification model using azure cognitive service language reason want use labeling environment azure ml rather labeling tool azure cognitive service language especially text classification labeling lot convenient azure ml read documentation azure ml export label azure machine learning dataset v label csv file case text classification label conll file case ner label however train custom ner text classification model azure cognitive service language import labeled data json file format text classification format ner way export label azure machine learning json file doe anyone know convert azure machine learning dataset json file
Adding Linear layers to Thinc Model Example - Understanding Data Dimensions Through Model Architecture,"<p>Trying to learn the inner workings of models trained with Spacy, and Thinc models are it. Looking at <a href=""https://colab.research.google.com/github/explosion/thinc/blob/master/examples/02_transformers_tagger_bert.ipynb#scrollTo=EYYyAeLqRc6S"" rel=""nofollow noreferrer"">this tutorial</a> and I'm modifying the model to see what breaks and what works. Instead of tagging, I'm modifying it to fit a NER dataset I have with 16 classes. I want to add several layers after the TransformerTokenizer + Transformer layers already outlined in this tutorial, but I'm getting tons of dimension ValueErrors. Also, it's important to me that the TransformersTagger layer outputs the last hidden layer of the given transformer model, which I'm not confident this code is doing. Here's the error I'm getting:</p>
<pre><code>ValueError: Attempt to change dimension 'nI' for model 'linear' from 512 to 16
</code></pre>
<p>And here is my full code adaptation to date. To be fair, I don't like that there's a softmax(num_ner_classes) prior to the Linear() layer, but I can't get anything else to work with with_array() after the Transformer layer:</p>
<pre><code>@dataclass
class TokensPlus:
    batch_size: int
    tok2wp: List[Ints1d]
    input_ids: torch.Tensor
    token_type_ids: torch.Tensor
    attention_mask: torch.Tensor

    def __init__(self, inputs: List[List[str]], wordpieces: BatchEncoding):
        self.input_ids = wordpieces[&quot;input_ids&quot;]
        self.attention_mask = wordpieces[&quot;attention_mask&quot;]
        self.token_type_ids = wordpieces[&quot;token_type_ids&quot;]
        self.batch_size = self.input_ids.shape[0]
        self.tok2wp = []
        for i in range(self.batch_size):
            print(i, inputs[i])
            spans = [wordpieces.word_to_tokens(i, j) for j in range(len(inputs[i]))]
            print(spans)
            self.tok2wp.append(self.get_wp_starts(spans))

    def get_wp_starts(self, spans: List[Optional[TokenSpan]]) -&gt; Ints1d:
        &quot;&quot;&quot;Calculate an alignment mapping each token index to its first wordpiece.&quot;&quot;&quot;
        alignment = numpy.zeros((len(spans)), dtype=&quot;i&quot;)
        for i, span in enumerate(spans):
            if span is None:
                raise ValueError(
                    &quot;Token did not align to any wordpieces. Was the tokenizer &quot;
                    &quot;run with is_split_into_words=True?&quot;
                )
            else:
                alignment[i] = span.start
        return alignment

@thinc.registry.layers(&quot;transformers_tokenizer.v1&quot;)
def TransformersTokenizer(name: str) -&gt; Model[List[List[str]], TokensPlus]:
    def forward(model, inputs: List[List[str]], is_train: bool):
        tokenizer = model.attrs[&quot;tokenizer&quot;]
        wordpieces = tokenizer(
            inputs,
            is_split_into_words=True,
            add_special_tokens=True,
            return_token_type_ids=True,
            return_attention_mask=True,
            return_length=True,
            return_tensors=&quot;pt&quot;,
            padding=&quot;longest&quot;
        )
        return TokensPlus(inputs, wordpieces), lambda d_tokens: []

    return Model(&quot;tokenizer&quot;, forward, attrs={&quot;tokenizer&quot;: AutoTokenizer.from_pretrained(name)})

def convert_transformer_inputs(model, tokens: TokensPlus, is_train):
    kwargs = {
        &quot;input_ids&quot;: tokens.input_ids,
        &quot;attention_mask&quot;: tokens.attention_mask,
        &quot;token_type_ids&quot;: tokens.token_type_ids,
    }
    return ArgsKwargs(args=(), kwargs=kwargs), lambda dX: []

def convert_transformer_outputs(model: Model, inputs_outputs: Tuple[TokensPlus, Tuple[torch.Tensor]], is_train: bool) -&gt; Tuple[List[Floats2d], Callable]:
    tplus, trf_outputs = inputs_outputs
    wp_vectors = torch2xp(trf_outputs[0])
    tokvecs = [wp_vectors[i, idx] for i, idx in enumerate(tplus.tok2wp)]

    def backprop(d_tokvecs: List[Floats2d]) -&gt; ArgsKwargs:
        # Restore entries for BOS and EOS markers
        d_wp_vectors = model.ops.alloc3f(*trf_outputs[0].shape, dtype=&quot;f&quot;)
        for i, idx in enumerate(tplus.tok2wp):
            d_wp_vectors[i, idx] += d_tokvecs[i]
        return ArgsKwargs(
            args=(trf_outputs[0],),
            kwargs={&quot;grad_tensors&quot;: xp2torch(d_wp_vectors)},
        )

    return tokvecs, backprop

@thinc.registry.layers(&quot;transformers_encoder.v1&quot;)
def Transformer(name: str = &quot;bert-large-cased&quot;) -&gt; Model[TokensPlus, List[Floats2d]]:
    return PyTorchWrapper(
        AutoModel.from_pretrained(name),
        convert_inputs=convert_transformer_inputs,
        convert_outputs=convert_transformer_outputs,
    )

@thinc.registry.layers(&quot;TransformersNer.v1&quot;)
def TransformersNer(name: str, num_ner_classes: int = 16) -&gt; Model[List[List[str]], List[Floats2d]]:
    return chain(
        TransformersTokenizer(name),
        Transformer(name),
        with_array(Softmax(num_ner_classes)),
        Linear(512, 1024)
    )
</code></pre>
<p>How do I best determine how to pipe the output of the PyTorchWrapped TransformersTagger layer into a Linear() + more layers down the chain? I've been using this model visualization but even when I run model.initialize() on the first examples of my data, there are still a lot of (?, ?).</p>
<pre><code>import pydot

def visualize_model(model):
    def get_label(layer):
        layer_name = layer.name
        nO = layer.get_dim(&quot;nO&quot;) if layer.has_dim(&quot;nO&quot;) else &quot;?&quot;
        nI = layer.get_dim(&quot;nI&quot;) if layer.has_dim(&quot;nI&quot;) else &quot;?&quot;
        return f&quot;{layer.name}|({nO}, {nI})&quot;.replace(&quot;&gt;&quot;, &quot;&amp;gt;&quot;)
    dot = pydot.Dot()
    dot.set(&quot;rankdir&quot;, &quot;LR&quot;)
    dot.set_node_defaults(shape=&quot;record&quot;, fontname=&quot;arial&quot;, fontsize=&quot;10&quot;)
    dot.set_edge_defaults(arrowsize=&quot;0.7&quot;)
    nodes = {}
    for i, layer in enumerate(model.layers):
        label = get_label(layer)
        node = pydot.Node(layer.id, label=label)
        dot.add_node(node)
        nodes[layer.id] = node
        if i == 0:
            continue
        from_node = nodes[model.layers[i - 1].id]
        to_node = nodes[layer.id]
        if not dot.get_edge(from_node, to_node):
            dot.add_edge(pydot.Edge(from_node, to_node))
    print(dot)

</code></pre>
<p>Produces:</p>
<pre><code>digraph G {
rankdir=LR;
node [fontname=arial, fontsize=10, shape=record];
edge [arrowsize=&quot;0.7&quot;];
176 [label=&quot;tokenizer|(?, ?)&quot;];
177 [label=&quot;pytorch|(?, ?)&quot;];
176 -&gt; 177;
179 [label=&quot;with_array(softmax)|(16, 1024)&quot;];
177 -&gt; 179;
180 [label=&quot;linear|(512, 1024)&quot;];
179 -&gt; 180;
}
</code></pre>
",Named Entity Recognition (NER),adding linear layer thinc model example understanding data dimension model architecture trying learn inner working model trained spacy thinc model looking tutorial modifying model see break work instead tagging modifying fit ner dataset class want add several layer transformertokenizer transformer layer already outlined tutorial getting ton dimension valueerrors also important transformerstagger layer output last hidden layer given transformer model confident code error getting full code adaptation date fair like softmax num ner class prior linear layer get anything else work array transformer layer best determine pipe output pytorchwrapped transformerstagger layer linear layer chain using model visualization even run model initialize first example data still lot produce
Scispacy for biomedical named entitiy recognition(NER),"<p><strong>How to label entities using scispacy?</strong></p>

<p>When I tried to perform NER using <code>scispacy</code>, it identified the biomedical entities by labeling them as <code>Entity</code> but failed to label them as gene/protein, etc.. So how do I do that using <code>scispacy</code>? Or is <code>scispacy</code> not capable of labeling data? The image is attached for reference:
<a href=""https://i.sstatic.net/QySa5.png"" rel=""nofollow noreferrer"">jupyter notebook snippet</a></p>
",Named Entity Recognition (NER),scispacy biomedical named entitiy recognition ner label entity using scispacy tried perform ner using identified biomedical entity labeling failed label gene protein etc using capable labeling data image attached reference jupyter notebook snippet
Fine-tuning BERT for custom NER,"<p>I have domain specific data in a pandas dataframe and I am attempting to fine-tune BERT-NER for classification task.</p>
<p>Here's my data:</p>
<pre><code>from transformers import pipeline
import pandas as pd

# Load data
df = pd.DataFrame({'text': [&quot;The income statement shows&quot;, &quot;Total expenses for the quarter are&quot;, &quot;Net income is positive.&quot;]})

# Use a pre-trained NER model to predict entity labels
ner_pipeline = pipeline(&quot;ner&quot;, model=&quot;dslim/bert-base-NER&quot;)
df['entities'] = df['text'].apply(lambda text: ner_pipeline(text))
</code></pre>
<p>This returns an empty list of entities as it is trained to recognize <code>[PER], [LOC]</code> entities and not custom tags like <code>[INC]</code> or <code>[EXP]</code>.</p>
<p>How do I prepare the data/annotate and fine-tune the model for custom tags / class labels following the <code>B-I-O</code> labeling scheme?</p>
",Named Entity Recognition (NER),fine tuning bert custom ner domain specific data panda dataframe attempting fine tune bert ner classification task data return empty list entity trained recognize entity custom tag like prepare data annotate fine tune model custom tag class label following labeling scheme
Named entity recognition (NER) features,"<p>I'm new to Named Entity Recognition and I'm having some trouble understanding what/how features are used for this task. </p>

<p>Some papers I've read so far mention features used, but don't really explain them, for example in 
<a href=""https://arxiv.org/pdf/cs/0306050.pdf"" rel=""noreferrer"">Introduction to the CoNLL-2003 Shared Task:Language-Independent Named Entity Recognition</a>, the following features are mentioned: </p>

<blockquote>
  <p>Main features used by the the sixteen systems that participated in the
  CoNLL-2003 shared task sorted by performance on the English test data.
  Aff: affix information (n-grams); bag: bag of words; cas: global case
  information; chu: chunk tags; doc: global document information; gaz:
  gazetteers; lex: lexical features; ort: orthographic information; pat:
  orthographic patterns (like Aa0); pos: part-of-speech tags; pre:
  previously predicted NE tags; quo: flag signing that the word is
  between quotes; tri: trigger words.</p>
</blockquote>

<p>I'm a bit confused by some of these, however. For example:</p>

<ul>
<li>isn't bag of words supposed to be a method to generate features (one for each word)? How can BOW itself be <em>a feature</em>? Or does this simply mean we have a feature for each word as in BOW, besides all the other features mentioned?</li>
<li>how can a gazetteer be a feature?</li>
<li>how can POS tags exactly be used as features ? Don't we have a POS tag for each word? Isn't each object/instance a ""text""?</li>
<li>what is global document information?</li>
<li>what is the feature trigger words?</li>
</ul>

<p>I think all I need here is to just to look at an example table with each of these features as columns and see their values to understand how they really work, but so far I've failed to find an easy to read dataset. </p>

<p>Could someone please clarify or point me to some explanation or example of these features being used?</p>
",Named Entity Recognition (NER),named entity recognition ner feature new named entity recognition trouble understanding feature used task paper read far mention feature used really explain example introduction conll shared task language independent named entity recognition following feature mentioned main feature used sixteen system participated conll shared task sorted performance english test data aff affix information n gram bag bag word ca global case information chu chunk tag doc global document information gaz gazetteer lex lexical feature ort orthographic information pat orthographic pattern like aa po part speech tag pre previously predicted ne tag quo flag signing word quote tri trigger word bit confused however example bag word supposed method generate feature one word bow feature doe simply mean feature word bow besides feature mentioned gazetteer feature po tag exactly used feature po tag word object instance text global document information feature trigger word think need look example table feature column see value understand really work far failed find easy read dataset could someone please clarify point explanation example feature used
How to modify or retrain existing OpenNLP models?,"<p>Is there any way to retrain existing OpenNLP models?? i.e to append new items to the existing models from OpenNLP ?</p>

<p>Suppose I want to add few new entries to existing <strong>en-ner-date.bin</strong> because some of the words are not getting detected as date.</p>

<p><em>Note: I don't want to make new model. I just want to modify the existing one...</em></p>

<p>I have seen something like model builder-add on but there is no concrete example about how to use it.</p>

<p>Any help will be appreciated.</p>
",Named Entity Recognition (NER),modify retrain existing opennlp model way retrain existing opennlp model e append new item existing model opennlp suppose want add new entry existing en ner date bin word getting detected date note want make new model want modify existing one seen something like model builder add concrete example use help appreciated
"chat bot with Named Entity Recognition, NER","<p>Advice is needed in creating a Telegram bot assistant that will be able to recognize natural language (Russian), analyze it, and, depending on the context, extract entities (currently only dates). The user's text can be very diverse, including handwritten dates and dates in numerical format. I plan to write it in Node.JS. I am aware of the existence of GPT, but for some reasons, I cannot use it. Please share your experience on how to best implement this task, what pitfalls to watch out for, and the complexity of implementation. I have no experience in working with machine learning, neural networks, and NLP.</p>
<p>At the moment, I'm doing a lot of googling and have identified 2 libraries that seem suitable to me. These are TensorFlow and Natural. How well could they fit for solving this task?</p>
",Named Entity Recognition (NER),chat bot named entity recognition ner advice needed creating telegram bot assistant able recognize natural language russian analyze depending context extract entity currently date user text diverse including handwritten date date numerical format plan write node j aware existence gpt reason use please share experience best implement task pitfall watch complexity implementation experience working machine learning neural network nlp moment lot googling identified library seem suitable tensorflow natural well could fit solving task
AttributeError: &#39;spacy.pipeline.ner.EntityRecognizer&#39; object has no attribute &#39;add_pipe&#39;,"<p>Why does the following code throw the error that the <code>add_pipe</code> attribute is no defined?</p>
<pre class=""lang-py prettyprint-override""><code>if 'ner' not in nlp.pipe_names:
    ner = nlp.create_pipe('ner')
    ner.add_pipe(ner , last = True)
    for _, annotation in train_data:
      for ent in annotation['entities']:
        ner.add_label(ent[2])
</code></pre>
",Named Entity Recognition (NER),attributeerror spacy pipeline ner entityrecognizer object ha attribute add pipe doe following code throw error attribute defined
Extract table of content of a book with ocr,"<p>I'm trying to extract the information of the table of content of a book. Given one or multiple images of a table of content I need to understand the chapters name, the pages, etc. Basically the information of the table of the content of the book.</p>
<p>This is an example of a possible input:</p>
<p><a href=""https://i.sstatic.net/OzhdJ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OzhdJ.jpg"" alt=""enter image description here"" /></a></p>
<p>The output should be a dictionary (stupid but structured example):</p>
<pre><code>chapter1: {
  name: &quot;The Basics of Blender&quot;
  page: 1
  content: [
    name: &quot;What ...&quot;
  ]
}
</code></pre>
<p>I only found a couple of papers but I did not understand them. Do you know if there are some pertained models for this task?</p>
<p>A simple ocr is not a possible solutions since I need to understand the context maybe with a document layout analysis. There aren't many documentations about this specific topic.</p>
<p>Papers:</p>
<p><a href=""https://www.researchgate.net/publication/290730251"" rel=""nofollow noreferrer"">TOC structure extraction from OCR-ed books</a></p>
<p><a href=""https://www.researchgate.net/publication/220933073"" rel=""nofollow noreferrer"">On the Reading of Tables of Contents</a></p>
",Named Entity Recognition (NER),extract table content book ocr trying extract information table content book given one multiple image table content need understand chapter name page etc basically information table content book example possible input output dictionary stupid structured example found couple paper understand know pertained model task simple ocr possible solution since need understand context maybe document layout analysis many documentation specific topic paper toc structure extraction ocr ed book reading table content
extracting names and associated labels from text with language model,"<p>I am trying to extract information from scientific literature on microalgae and i need to be able to scan a text for various names and find their corresponding category.</p>
<p>As an simple example, say I have 3 names (Peter, John, Linda) and I want to find their job title from this list of titles (Carpenter, Fisherman, Ninja) and this is the text:
&quot;Peter lives in a house he build himself, he is a great carpenter. Linda likes to run marathons when she is not busy running her own business. John is 40 and lives from fishing&quot;</p>
<p>I would like a response like so (Peter = carpenter, John = Fisherman, Linda = NA).
Currently I am trying bert but can only find a way to extract a single label from a text and I cannot find a way to contextualise it to the person in question.</p>
<p>Does anyone have suggestion on how to go about this?</p>
<p>(UPDATE) A more specific example would be using this text:
&quot;Nannochloropsis gaditana is in the genus of Nannochloropsis comprising six known species all of which are unicellur. Unlike these species all species in the genus Arthrospira are filamentous such as Arthrospira platensis.&quot;</p>
<p>Here i need to extract:  Nannochlorpsis gaditana = unicellur, Arthrospira platensis = filamentous.</p>
",Named Entity Recognition (NER),extracting name associated label text language model trying extract information scientific literature microalgae need able scan text various name find corresponding category simple example say name peter john linda want find job title list title carpenter fisherman ninja text peter life house build great carpenter linda like run marathon busy running business john life fishing would like response like peter carpenter john fisherman linda na currently trying bert find way extract single label text find way contextualise person question doe anyone suggestion go update specific example would using text nannochloropsis gaditana genus nannochloropsis comprising six known specie unicellur unlike specie specie genus arthrospira filamentous arthrospira platensis need extract nannochlorpsis gaditana unicellur arthrospira platensis filamentous
Pytorch Model Gives Zero Accuracy After Saving,"<p>I built a Pytorch model to perform some Named Entity Recognition tasks. It gives a validation accuracy of ~90% after 40 epochs and ~87% on test data. However, when I save the model and reload it, the accuracy crashes to almost zero.</p>
<p>The model definition is as follows:</p>
<pre><code>class NERModel(nn.Module):
  def __init__(self):
    super(NERModel,self).__init__()
    self.base_count = 128
    self.relu = nn.ReLU()
    self.sigmoid = nn.Sigmoid()
    self.BatchNorm = nn.BatchNorm1d(num_features=100,)#100 is just a placeholder which will be updated
    self.SpanEmbed = nn.Embedding(num_embeddings=len(char2idx),embedding_dim=EMBEDDING_DIM,_weight = torch.from_numpy(embed_matrix)).type(torch.float)#,_freeze = True
    self.SentEmbed = nn.Embedding(num_embeddings=len(char2idx),embedding_dim=EMBEDDING_DIM,_weight = torch.from_numpy(embed_matrix)).type(torch.float)#,_freeze = True

    self.Span_Layer_Stack_PreS = nn.Sequential(
        #nn.Embedding(num_embeddings=len(char2idx),embedding_dim=EMBEDDING_DIM,_weight = torch.from_numpy(embed_matrix),_freeze = True).type(torch.float),
        self.SpanEmbed,
        nn.LSTM(input_size=EMBEDDING_DIM,hidden_size=1800,bidirectional=True,batch_first=True).type(torch.float)
        )

    self.Span_Layer_Stack_PostS = nn.Sequential(
        # nn.BatchNorm1d(num_features=MAX_SPAN_LEN),#100 is just a placeholder which will be updated
        nn.Dropout(0.5),
        nn.Linear(in_features = 3600,out_features = 1920),
        nn.ReLU(),
        nn.Linear(in_features = 1920,out_features = 640),
        nn.ReLU(),
        nn.Linear(in_features = 640,out_features = 1280)

    )

    self.Sent_Layer_Stack_PreS = nn.Sequential(
        # nn.Embedding(num_embeddings=len(char2idx),embedding_dim=EMBEDDING_DIM,_weight = torch.from_numpy(embed_matrix),_freeze = True).type(torch.float),
        self.SentEmbed,
        nn.LSTM(input_size=EMBEDDING_DIM,hidden_size=1800,bidirectional=True,batch_first = True).type(torch.float)
        )

    self.Sent_Layer_Stack_PostS = nn.Sequential(
        nn.BatchNorm1d(num_features=3600),#100 is just a placeholder which will be updated
        nn.Dropout(0.5),
        nn.Linear(in_features = 3600,out_features = 1920),
        nn.ReLU(),
        nn.Linear(in_features = 1920,out_features = 640),
        nn.ReLU(),
        nn.Linear(in_features = 640,out_features = 1280)

    )

    self.Comb_Layer_Stack = nn.Sequential(
        nn.Linear(in_features = 2560,out_features = 2048),
        nn.ReLU(),
        nn.Linear(in_features = 2048,out_features = 1024),
        nn.ReLU(),
        nn.Linear(in_features = 1024,out_features = 512),
        nn.ReLU(),
        nn.Linear(in_features = 512,out_features = 512),
        nn.ReLU(),
        nn.Linear(in_features = 512,out_features = 256),
        nn.ReLU(),
        nn.Linear(in_features = 256,out_features = 128),
        nn.ReLU(),
        nn.Linear(in_features = 128,out_features = len(label_idx)),
        nn.Sigmoid()
    )


  def forward(self,x:torch.Tensor) -&gt; torch.Tensor:

    span_input,sent_input = torch.split(x,[int(x.shape[1]/2),int(x.shape[1]/2)],dim = 1)
    span_out_tot,(span_out,span_c_state_out) = self.Span_Layer_Stack_PreS(span_input.type(torch.long))
    sent_out_tot,(sent_out,sent_c_state_out) = self.Sent_Layer_Stack_PreS(sent_input.type(torch.long))

    #stack the layers
    span_out = torch.hstack([span_out[0],span_out[1]])
    sent_out = torch.hstack([sent_out[0],sent_out[1]])

    #run the stacked layers through the post stack layers
    span_out = self.Span_Layer_Stack_PostS(span_out.type(torch.float))
    sent_out = self.Sent_Layer_Stack_PostS(sent_out.type(torch.float))

    #combine the layers
    comb_layers = torch.hstack([span_out,sent_out])

    span_label = self.Comb_Layer_Stack(comb_layers)

    return(span_label)
</code></pre>
<p>My training code (which also contains the code to save the model at checkpoints) is as follows:</p>
<pre><code>def train_model(model:NERModel,num_epochs = 10,lrate = 0.005, save_freq = 2):

  torch.manual_seed(42)

  # m_accuracy = Accuracy(task=&quot;multiclass&quot;, num_classes=len(label_idx)).to(device)

  loss_fn = nn.BCELoss()
  opt = torch.optim.Adam(params=[param for param in model.parameters() if param.requires_grad == True],lr=lrate)

  epochs = num_epochs

  scheduler = lr_scheduler.ReduceLROnPlateau(opt,mode='min',factor=0.8,patience=2,verbose=True)

  mb = master_bar(range(epochs))

  for epoch in mb:#range(epochs):#tqdm(range(epochs),total = epochs):

    train_loss = 0
    train_acc = 0

    model.train()
    print(f'Epoch {epoch + 1}')
    total_data_processed = 0
    start_time = timer.perf_counter()

    for batch, (X,y) in progress_bar(enumerate(train_dataloader),total = len(train_dataloader),parent=mb):#enumerate(train_dataloader):#tqdm(enumerate(train_dataloader),total = len(train_dataloader)):

      y_pred = model(X).to(device)

      loss = loss_fn(y_pred.type(torch.float),y.type(torch.float))#torch.argmax(y_pred,dim=1).type(torch.float),y.type(torch.float))

      train_loss+=loss

      opt.zero_grad()

      loss.backward()

      opt.step()

      train_acc+= get_accuracy(torch.where(y_pred&gt;0.5,1.0,0.0),y)#torch.argmax(y_pred,dim=1),y)

      total_data_processed += len(X)

    model.eval()

    with torch.inference_mode():
      total_val_loss, total_val_acc = 0, 0
      for X_val,y_val in val_dataloader:
        y_val_pred = model(X_val)
        total_val_loss += loss_fn(y_val_pred.type(torch.float),y_val.type(torch.float))
        total_val_acc += get_accuracy(torch.where(y_val_pred&gt;0.5,1.0,0.0),y_val)
      val_loss = total_val_loss/len(val_dataloader)
      val_acc = total_val_acc/len(val_dataloader)
      scheduler.step(val_loss)
      print('Avg. Training Loss:{:.4f} | Val Loss:{:.4f} | Avg. Training Accuracy:{:.4f} | Val Accuracy:{:.4f}'\
      .format(train_loss/len(train_dataloader),val_loss,train_acc/len(train_dataloader),val_acc))

    end_time = timer.perf_counter()
    print('Epoch {:2d} | Elapsed Time:{:.3f}s'.format(epoch + 1, end_time - start_time))
    mb.write('Epoch {:2d} completed | Elapsed Time:{:.3f}s'.format(epoch + 1, end_time - start_time))

    if (epoch + 1) % save_freq == 0 and epoch + 1 &gt; 0:
      filepath = config.environ_path[config.environ]['save'] + &quot;Char_Tok_Models/Models/cp{:02d}.pth&quot;.format(epoch + 1)
      print('Saving model to:',filepath)
      torch.save({
              'epoch': epoch,
              'model_state_dict': model.state_dict(),
              'loss': val_loss,
              'accuracy': val_acc,
              }, filepath)

      print('Save completed')

def test_model(model_file:str,d_loader:DataLoader):

  torch.manual_seed(42)

  # m_accuracy = Accuracy(task=&quot;multiclass&quot;, num_classes=len(label_idx)).to(device)

  test_model = NERModel().to(device)

  loss_fn = nn.BCELoss()

  checkpoint = torch.load(config.environ_path[config.environ]['save'] + &quot;Char_Tok_Models/Models/&quot; + model_file)

  test_model.load_state_dict(checkpoint['model_state_dict'])

  test_model.eval()

  with torch.inference_mode():
    total_test_loss, total_test_acc = 0, 0
    for batch,(X_test,y_test) in progress_bar(enumerate(d_loader),total=len(d_loader)):
      y_test_pred = test_model(X_test)
      total_test_loss += loss_fn(y_test_pred.type(torch.float),y_test.type(torch.float))
      total_test_acc += get_accuracy(torch.where(y_test_pred&gt;0.5,1.0,0.0),y_test)
    test_loss = total_test_loss/len(test_dataloader)
    test_acc = total_test_acc/len(test_dataloader)
    # scheduler.step(test_loss)
    print('Test Loss:{:.4f} | Test Accuracy:{:.4f}'.format(test_loss,test_acc))
</code></pre>
<p>The code to make predictions is as follows:</p>
<pre><code>def init_model(model_file:str):

  torch.manual_seed(42)

  new_model = NERModel().to(device)

  checkpoint = torch.load(config.environ_path[config.environ]['save'] + &quot;Char_Tok_Models/Models/&quot; + model_file,map_location=torch.device(device))

  new_model.load_state_dict(checkpoint['model_state_dict'])

  return new_model


def predict(input_sent:str,model:NERModel):

  torch.manual_seed(42)

  pred_can_spans = get_can_spans(input_sent)
  can_spans = []
  for span in pred_can_spans:
    can_spans.append(span[2])
    # print(span[2])
  enc_spans = pad_encode_text(can_spans)
  # for span in enc_spans:
  #   print(span)
  enc_input_sent = pad_encode_text([input_sent])[0]
  enc_input_sent_arr = np.repeat([np.array(enc_input_sent)],len(enc_spans),axis=0)
  # print(enc_input_sent_arr.shape)
  enc_spans_arr = np.array(enc_spans)
  # print(enc_spans_arr.shape)
  pred_input = torch.from_numpy(np.concatenate([enc_spans_arr,enc_input_sent_arr],axis=1)).to(device)
  # print(pred_input.shape,type(pred_input))

  model.eval()

  with torch.no_grad():

    pred_out = model(pred_input)

    pred_out = torch.where(pred_out&gt;0.5,1.0,0.0)

  outcome = zip(pred_input,pred_out)

  print('initial outcome length',len(pred_input))

  filt_outcome = [item for item in outcome if item[1][label_idx['']] != 1]

  print('final outcome length',len(filt_outcome))

  return pred_out
</code></pre>
<p>The function I use to calculate the accuracy metric is as follows:</p>
<pre><code>def get_accuracy(y_pred,y_true):
  y_pred_arr = y_pred.cpu().numpy();y_true_arr = y_true.cpu().numpy()
  comp_out = np.where(np.sum(np.equal(y_pred_arr,y_true_arr),axis=1)&lt;y_true_arr.shape[1],0,1)
  return np.sum(comp_out)/comp_out.shape[0]
</code></pre>
<p>I have seeded the torch random seed as well but no effect. What could the problem be? I'd appreciate any help. Thanks.</p>
<p>I was expecting the model to give me close to the accuracy achieved during test but it didn't.</p>
",Named Entity Recognition (NER),pytorch model give zero accuracy saving built pytorch model perform named entity recognition task give validation accuracy epoch test data however save model reload accuracy crash almost zero model definition follows training code also contains code save model checkpoint follows code make prediction follows function use calculate accuracy metric follows seeded torch random seed well effect could problem appreciate help thanks wa expecting model give close accuracy achieved test
Entity Extraction in Rasa,"<p>I want to create a chatbot that extract words from the user message as entity and send it to dictionary and in return get the meaning of that word.</p>
<p>But the problem is entity values are not getting extracted, and I am getting empty brackets [ ]. I am trying to solve this issue for weeks now. Now, I am exhausted and desperate. Please help me to figure this out.</p>
<p>Here is all the files:
<a href=""https://github.com/Attiqakaleem0/Rasa-word-meaning-bot"" rel=""nofollow noreferrer"">https://github.com/Attiqakaleem0/Rasa-word-meaning-bot</a></p>
<p>Installation versions on my system are:
Rasa Version      :         3.6.13
Minimum Compatible Version: 3.5.0
Rasa SDK Version  :         3.6.2
Python Version    :         3.10.0
Operating System  :         Windows-10-10.0.19045-SP0</p>
",Named Entity Recognition (NER),entity extraction rasa want create chatbot extract word user message entity send dictionary return get meaning word problem entity value getting extracted getting empty bracket trying solve issue week exhausted desperate please help figure file installation version system rasa version minimum compatible version rasa sdk version python version operating system window sp
Labelling 100k dataset for BERT-NER,"<p>How i label dataset untill 100k++ effectively?I will use it for BERT-NER?and if there is method can you give me like code/tutorial/source for implementing?thank you!BTW, dataset i will use for my BERT-NER is N2C2 NLP dataset</p>
",Named Entity Recognition (NER),labelling k dataset bert ner label dataset untill k effectively use bert ner method give like code tutorial source implementing thank btw dataset use bert ner n c nlp dataset
Extract information from recognized text business card scanner in android app,"<p>I have developed android app with Business Card OCR scanning feature. Using MlKit text recognization, got the text from the image taken. I was extract information of website, email, phone number. But now I am facing difficulties to detect human name, address and Company Name. Because business card structure/format is not specific, even in some case I can identify the address, but it's difficult to identify the human name and company name.</p>
<p>For example</p>
<p><a href=""https://i.sstatic.net/YBS2k.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YBS2k.png"" alt=""enter image description here"" /></a></p>
<p>In above business card regex detect RKB Solution as person name, but R. Krishnamoorthy was the person name</p>
<p>another case was, In card scaned text it detect &quot;James Smith&quot; and Some &quot;Pixel Fusion&quot; is company name, so issue is as per regex patter James Smith and Pixel Fusion both are person name, or both are company name.
So please guide me the right direction.</p>
",Named Entity Recognition (NER),extract information recognized text business card scanner android app developed android app business card ocr scanning feature using mlkit text recognization got text image taken wa extract information website email phone number facing difficulty detect human name address company name business card structure format specific even case identify address difficult identify human name company name example business card regex detect rkb solution person name r krishnamoorthy wa person name another case wa card scaned text detect james smith pixel fusion company name issue per regex patter james smith pixel fusion person name company name please guide right direction
How to make named entity recognition provide better categorization of data,"<p>Following is a default categorization of data from a news article.</p>
<pre><code>Christiane Amanpour 268 287 PERSON
Hamas 155 160 ORG
Rania 6 11 PERSON
Warner 0 6 ORG
</code></pre>
<p>But I would like to change the behavior as follows</p>
<pre><code>I would want to categorize `Christiane Amanpour` as a journalist
I would want to categorize `Rania` as a queen
I would want to categorize `Warner` as a cricket player
</code></pre>
<p>How exactly I train the data to do this</p>
",Named Entity Recognition (NER),make named entity recognition provide better categorization data following default categorization data news article would like change behavior follows exactly train data
"How to extract named entity recognition tags of the type &quot;b-per&quot;, &quot;b-misc&quot; etc using spaCy and Python?","<p>The documentation I can find suggests using the &quot;labels_&quot; attribute of the spans in a spaCy document. However for me this returns POS tags not NER tags. I need to have types indicating if it begins the NER span (&quot;B&quot; or otherwise &quot;I&quot;) and the type of NER tag (e.g. &quot;per&quot; for a person), and otherwise &quot;o&quot; if it is not an NER span.</p>
<pre><code>for idx, doc in enumerate(train_docs):
    for ent in doc.ents:
        ner_structured_docs.append([idx, ent.text,ent.label_])
</code></pre>
<p>this gives the index (i.e. sentence id),because each doc is a separate sentence. It then gives the span (ent). ent.text gives the text of the span correctly. However ent.label_ gives the POS tag (e.g. &quot;NOUN&quot;, &quot;DET&quot;) not NER tag.</p>
",Named Entity Recognition (NER),extract named entity recognition tag type b per b misc etc using spacy python documentation find suggests using label attribute span spacy document however return po tag ner tag need type indicating begin ner span b otherwise type ner tag e g per person otherwise ner span give index e sentence id doc separate sentence give span ent ent text give text span correctly however ent label give po tag e g noun det ner tag
What components of spaCy Pipeline can be disabled so that the sentence tokenization can still work and the pipeline be faster?,"<p>I want to use the spaCy pipeline only for sentence tokenization as it's the best for my language but I want it to be as minimal as possible.</p>
<p>So far I figured I could get rid of tagger and ner components:</p>
<p><code>nlp = spacy.load(&quot;pl_core_news_sm&quot;, disable=['tagger', 'ner'])</code></p>
<p>I noticed that without <code>tok2vec</code> it doesn't work (which seems very odd).
I don't want to try all combinations because I'd surely miss something.</p>
<p>So does anyone know what components can be disabled so that the tokenization can still work and the pipeline be faster?</p>
",Named Entity Recognition (NER),component spacy pipeline disabled sentence tokenization still work pipeline faster want use spacy pipeline sentence tokenization best language want minimal possible far figured could get rid tagger ner component noticed without work seems odd want try combination surely miss something doe anyone know component disabled tokenization still work pipeline faster
extract information from an image from forms,"<p>I have some form filled with</p>
<blockquote>
<p>name:something, family name: something and etc.</p>
</blockquote>
<p>all the pages are similar format.
I have the images of this forms. first I extracted the text from image by <code>tesseract(python)</code> then I am trying to convert each page to a dictionary:</p>
<pre><code>{ &quot;page1&quot;:{&quot;name&quot;:&quot;something&quot;,&quot;family_name&quot;:&quot;something&quot;,etc}
</code></pre>
<p>which method is the best? I used the regex and it does not give me good result because for example name may be return as name in tesseract output? what is the best open source solution?
should I change tesseract? is there any machine learning model I can replace with regex? should I use image processing? How can I train a model to learn my form patterns to extract the data.</p>
<p>I tried tesseract and regex but the results are not good because the image does not have a good quality.</p>
",Named Entity Recognition (NER),extract information image form form filled name something family name something etc page similar format image form first extracted text image trying convert page dictionary method best used regex doe give good result example name may return name tesseract output best open source solution change tesseract machine learning model replace regex use image processing train model learn form pattern extract data tried tesseract regex result good image doe good quality
Efficient way to find an approximate string match and replacing with predefined string,"<p>I need to build a <code>NER</code> system (<a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Named Entity Recognition</a>). For simplicity, I am doing it by using approximate string matching as input can contain typos and other minor modifications. I have come across some great libraries like: <a href=""https://pypi.org/project/fuzzywuzzy/"" rel=""nofollow noreferrer"">fuzzywuzzy</a> or even faster <a href=""https://github.com/maxbachmann/RapidFuzz"" rel=""nofollow noreferrer"">RapidFuzz</a>. But unfortunately I didn't find a way to return the position where the match occurs. As, for my purpose I not only need to find the match, but also I need to know where the match happened. As for <code>NER</code>, I need to replace those matches with some predefined string.</p>
<p>For example, If any one of the line is found in input string I want to replace them with the string <code>COMPANY_NAME</code>:</p>
<pre><code>google
microsoft
facebook
International Business Machine
</code></pre>
<p>Like, input: <code>S/he works at Google</code> will be transformed to <code>S/he works at COMPANY_NAME</code>.
You can safely assume that, all the input and the pattern to match are already preprocessed and most importantly they are in lower-case now. So, there is no problem with case-sensitivity.</p>
<p>Currently, I have approached with a sliding window technique. And a sliding window is passed over the input string from left to right and this window has exactly the size of the pattern we want to match. For example, when I want to match with <code>International Business Machine</code>, I run a sliding window of size <code>3</code> from left to right and try to find the best match by observing each <code>3</code> consecutive tokens at the same time with a stride of <code>1</code>. I do believe, it is not the best way to do it, also it cannot find the <em>best</em> match.</p>
<p>So, what is the efficient way to find the <strong>best</strong> possible match along with the quantification on the found match (how much they are similar) and the position of the match(es), such that we can replace them with a given fixed string (if the calculated similarity is not less than a threshold)? Obviously, a single input may contain multiple portions to be replaced, each of them will be replaced separately, like: <code>Google and Microsoft are big companies</code> will become <code>COMPANY_NAME and COMPANY_NAME are big companies</code> etc.</p>
<p>EDIT: fixed link to RapidFuzz</p>
",Named Entity Recognition (NER),efficient way find approximate string match replacing predefined string need build system named entity recognition simplicity using approximate string matching input contain typo minor modification come across great library like fuzzywuzzy even faster rapidfuzz unfortunately find way return position match occurs purpose need find match also need know match happened need replace match predefined string example one line found input string want replace string like input transformed assume input pattern match already preprocessed importantly lower case problem case sensitivity currently approached sliding window technique sliding window passed input string left right window ha exactly size pattern want match example want match run sliding window size left right try find best match observing consecutive token time stride believe best way also find best match efficient way find best possible match along quantification found match much similar position match e replace given fixed string calculated similarity le threshold obviously single input may contain multiple portion replaced replaced separately like become etc edit fixed link rapidfuzz
How to extract paragraphs from pdf?,"<p>I have a pdf downloaded from SSRN here: <a href=""https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4337484"" rel=""nofollow noreferrer"">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4337484</a>. The default file name is &quot;SSRN-id4337484.pdf&quot;.</p>
<p>I then extract the text from the downloaded file using the <code>pdftotext</code> package:</p>
<pre><code>from pdftotext import PDF

with open(&quot;SSRN-id4337484.pdf&quot;, &quot;rb&quot;) as f:
    pdf = PDF(f)
</code></pre>
<p>Each page is lumped into one, long string with &quot;\r\n&quot; tags indicating line breaks. For example if I print page 2:</p>
<pre><code>[In ]: pdf[2]
[Out]:
'Journal of AI\r\narticles which have been published in English Language.\r\n\r\n3. WHAT `IS CHATGPT?\r\nTo begin, we asked ChatGPT to explain itself. The prompt we started with was “What is ChatGPT?”. ChatGPT\r\ngenerated this response.\r\n“ChatGPT is a large language model trained by OpenAI. It is designed to generate human-like text based on a\r\ngiven prompt or context. It can be used for a variety of natural language processing tasks, such as text\r\ncompletion, conversation generation, and language translation”. (By ChatGPT January 23, 2023).\r\nAs a follow up, we used another prompt “How does it work?” To know how conversational it is, we\r\nintentionally wrote “How does it work?” instead of &quot;How does ChatGPT work?”. It returned with the response\r\nbelow.\r\n“ChatGPT is based on a transformer architecture, a type of neural network that has been shown to be very\r\neffective for natural language processing tasks. It is trained on a large dataset of text, such as books and\r\narticles, and learns to generate text that is similar to the text it was trained on. When given a prompt or\r\ncontext, the model processes the input and generates a response. The response is generated one word at a\r\ntime, with the model predicting the next word based on the input and the words it has generated so far. The\r\nmodel uses attention mechanisms to focus on the most relevant parts of the input and generate a response\r\nthat is coherent and appropriate for the given context. After the training, the model can be fine-tuned on a\r\nspecific task such as question answering or dialogue generation by providing it with task-specific examples\r\nand fine-tuning the model on this data. It can also generate text in multiple languages by fine-tuning the\r\nmodel on the multilingual dataset or by providing the model with the language code” (By ChatGPT January\r\n23, 2023).\r\nThen the topic of conversation was changed to discuss COVID-19 and impacts on educational deliveries using\r\nthe prompt “Now discuss how COVID-19 has changed educational pedagogies.” Within seconds it wrote the\r\npassage below.\r\nThe COVID-19 pandemic has had a significant impact on the way education is delivered, with many schools\r\nand universities shifting to online or remote learning in order to comply with social distancing guidelines. This\r\nhas resulted in a rapid shift to digital and online pedagogy, as educators have had to quickly adapt to this\r\nnew mode of instruction.\r\nOne of the major changes that has occurred is the increased use of technology for teaching and learning. This\r\nhas included the use of online learning platforms, such as Zoom, Google Classroom and other video\r\nconferencing tools, as well as the use of digital materials, such as e-books, videos, and interactive activities.\r\nThis has allowed for increased flexibility and accessibility in the delivery of education, as students can now\r\naccess materials and participate in classes from anywhere with internet access.\r\nAdditionally, there has been a shift towards more self-directed and asynchronous learning, where students\r\nare given more autonomy in their learning and are able to complete coursework at their own pace. This\r\napproach allows students to work at a time that is convenient for them, and it can be beneficial for students\r\nwho may have other responsibilities or who may have difficulty attending live classes. However, the shift to\r\nonline learning has also highlighted the digital divide, where students in low-income or rural areas may not\r\nhave access to the technology or internet connectivity needed to fully participate in remote learning. And it\r\nalso brought challenges such as lack of interaction, lack of motivation and lack of accountability.\r\nOverall, COVID-19 has accelerated the adoption of technology and digital pedagogy in education, but it has\r\nalso highlighted the need for equity and accessibility in the delivery of education. The education system will\r\nhave to continue to adapt and evolve to meet the needs of students, teachers and parents during these\r\n\r\n54\r\n\r\n\x0c'`
​
</code></pre>
<p>I can of course print it nicely using the line breaks:</p>
<pre><code>[In ]: pdf[2].split('\r\n')
[Out]: 
['Journal of AI',
 'articles which have been published in English Language.',
 '',
 '3. WHAT IS CHATGPT?',
 'To begin, we asked ChatGPT to explain itself. The prompt we started with was “What is ChatGPT?”. ChatGPT',
 'generated this response.',
 '“ChatGPT is a large language model trained by OpenAI. It is designed to generate human-like text based on a',
 'given prompt or context. It can be used for a variety of natural language processing tasks, such as text',
 'completion, conversation generation, and language translation”. (By ChatGPT January 23, 2023).',
 'As a follow up, we used another prompt “How does it work?” To know how conversational it is, we',
 'intentionally wrote “How does it work?” instead of &quot;How does ChatGPT work?”. It returned with the response',
 'below.',
 '“ChatGPT is based on a transformer architecture, a type of neural network that has been shown to be very',
 'effective for natural language processing tasks. It is trained on a large dataset of text, such as books and',
 'articles, and learns to generate text that is similar to the text it was trained on. When given a prompt or',
 'context, the model processes the input and generates a response. The response is generated one word at a',
 'time, with the model predicting the next word based on the input and the words it has generated so far. The',
 'model uses attention mechanisms to focus on the most relevant parts of the input and generate a response',
 'that is coherent and appropriate for the given context. After the training, the model can be fine-tuned on a',
 'specific task such as question answering or dialogue generation by providing it with task-specific examples',
 'and fine-tuning the model on this data. It can also generate text in multiple languages by fine-tuning the',
 'model on the multilingual dataset or by providing the model with the language code” (By ChatGPT January',
 '23, 2023).',
 'Then the topic of conversation was changed to discuss COVID-19 and impacts on educational deliveries using',
 'the prompt “Now discuss how COVID-19 has changed educational pedagogies.” Within seconds it wrote the',
 'passage below.',
 'The COVID-19 pandemic has had a significant impact on the way education is delivered, with many schools',
 'and universities shifting to online or remote learning in order to comply with social distancing guidelines. This',
 'has resulted in a rapid shift to digital and online pedagogy, as educators have had to quickly adapt to this',
 'new mode of instruction.',
 'One of the major changes that has occurred is the increased use of technology for teaching and learning. This',
 'has included the use of online learning platforms, such as Zoom, Google Classroom and other video',
 'conferencing tools, as well as the use of digital materials, such as e-books, videos, and interactive activities.',
 'This has allowed for increased flexibility and accessibility in the delivery of education, as students can now',
 'access materials and participate in classes from anywhere with internet access.',
 'Additionally, there has been a shift towards more self-directed and asynchronous learning, where students',
 'are given more autonomy in their learning and are able to complete coursework at their own pace. This',
 'approach allows students to work at a time that is convenient for them, and it can be beneficial for students',
 'who may have other responsibilities or who may have difficulty attending live classes. However, the shift to',
 'online learning has also highlighted the digital divide, where students in low-income or rural areas may not',
 'have access to the technology or internet connectivity needed to fully participate in remote learning. And it',
 'also brought challenges such as lack of interaction, lack of motivation and lack of accountability.',
 'Overall, COVID-19 has accelerated the adoption of technology and digital pedagogy in education, but it has',
 'also highlighted the need for equity and accessibility in the delivery of education. The education system will',
 'have to continue to adapt and evolve to meet the needs of students, teachers and parents during these',
 '',
 '54',
 '',
 '\x0c']
</code></pre>
<p>My question is how to extract paragraphs from those long strings into a list? So each paragraph is an element in that list. Note that sometimes one paragraph can span across two pages.</p>
",Named Entity Recognition (NER),extract paragraph pdf pdf downloaded ssrn default file name ssrn id pdf extract text downloaded file using package page lumped one long string r n tag indicating line break example print page course print nicely using line break question extract paragraph long string list paragraph element list note sometimes one paragraph span across two page
"Pre-trained models(Spacy, NLTK etc. ) for Name, Entity, Product, Place recognition for short descriptions","<p>I have very short descriptions of not exceeding 40 characters. I'm using Spacy's NER model to identify Name, Entity, Products and foods.
The problem with the text descriptions I have are very short and are not proper English sentences. Spacy fails to identify Name, Entity, Products and foods and returns nulls.</p>
<p>For example, the first description in my table is &quot;Monster Ultra Strawberry&quot;. But when I try to get the entity type tags for individual tokens the spacy model returns a null.</p>
<p>My code is as below.</p>
<pre><code>nlp = spacy.load('en_core_web_lg')

    docs = nlp(data['desc'][0])
    for token in docs:
        print(token.ent_type_)
</code></pre>
<p>Please let me know what other models I can use in this situation? Are there model classifiers that  classify a word into these buckets - Name, Org, Food, Product etc. without additional context?</p>
",Named Entity Recognition (NER),pre trained model spacy nltk etc name entity product place recognition short description short description exceeding character using spacy ner model identify name entity product food problem text description short proper english sentence spacy fails identify name entity product food return null example first description table monster ultra strawberry try get entity type tag individual token spacy model return null code please let know model use situation model classifier classify word bucket name org food product etc without additional context
"Python API usage for coreference, semantic graph and NERC","<h1>Intro</h1>
<p>Hi, I have been using freeling for a few months now to extract triplets. So far I have succeded in doing so by using the dependency tree and the full parse tree, but I am trying to add NERC.</p>
<h1>My work so far</h1>
<p>I checked the tutorial for python, but I couldn't find anything beyond depdency parsing. So I went through the class list (since the same classes should be available for python and c++) but it is not very clear how to retrieve the named entities and after checking the output of the analyzer sampler I have a few questions about the performance of the NER module.</p>
<h1>Problems</h1>
<p>So what I'm asking if anyone can help me with is the following:</p>
<ol>
<li>Doubt about entities: Using the example &quot;Sobre la mesa María ve y coge una manzana, un sombrero, una llave y dos paraguas rojo.&quot; I realized that working with capitalized words and lowercase produce different results, but by making it all lowercase the entity recognition stops recognizing &quot;maría&quot; as a person. Is there are workaround for this or am I going in the wrong direction? The main problem is that &quot;maría&quot; not recognized as a named entity (which i need it to be by the way) results in &quot;maría&quot; not being the subject of the sentence anymore. Im using:</li>
</ol>
<p>neclass = pyfreeling.ner(lpath + &quot;/nerc/ner/ner-ab-rich.dat&quot;)</p>
<ol start=""2"">
<li>How to retrieve named entities: Kind of a follow up of the previous question, how do I get the named entities? I couldn't find any code related to this and the semantic graph i obtain holds 0 entities.</li>
</ol>
<p>Any comments and suggestions are welcomed, thanks in advance.</p>
",Named Entity Recognition (NER),python api usage coreference semantic graph nerc intro hi using freeling month extract triplet far succeded using dependency tree full parse tree trying add nerc work far checked tutorial python find anything beyond depdency parsing went class list since class available python c clear retrieve named entity checking output analyzer sampler question performance ner module problem asking anyone help following doubt entity using example sobre la mesa mar coge una manzana un sombrero una llave paraguas rojo realized working capitalized word lowercase produce different result making lowercase entity recognition stop recognizing mar person workaround going wrong direction main problem mar recognized named entity need way result mar subject sentence anymore im using neclass pyfreeling ner lpath nerc ner ner ab rich dat retrieve named entity kind follow previous question get named entity find code related semantic graph obtain hold entity comment suggestion welcomed thanks advance
read complex financial tables using R or Python,"<p>I have about 100 reports like below where there is a table with financial information I need to extract. The job is too big to do manually.</p>
<p>I need to extract the 'key persons' name and total renumeration, and save this in some sort of table.
The <strong>challenge</strong> which I'm sure many of you are aware of is that extracting data from PDFs in in Python or R or extremely difficult. What makes it harder is that each company presents a slightly different table compared to the one below.</p>
<p>What I need is some systematic method of grabbing the required information. I am not asking by any means for a solution, as I know it is a big task, instead I am looking for advice in how to achieve my goal.
I have used all the regular Python and R packages, and so far I have not come up with any great systematic way of completing my task. Possibly a machine learning approach is the way to go? I'm not sure. But I have tried many methods of completing this task, to no avail.
Any advice would be appreciated.
Thanks</p>
<p><a href=""https://i.sstatic.net/4xjWJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4xjWJ.png"" alt=""table"" /></a></p>
",Named Entity Recognition (NER),read complex financial table using r python report like table financial information need extract job big manually need extract key person name total renumeration save sort table challenge sure many aware extracting data pdfs python r extremely difficult make harder company present slightly different table compared one need systematic method grabbing required information asking mean solution know big task instead looking advice achieve goal used regular python r package far come great systematic way completing task possibly machine learning approach way go sure tried many method completing task avail advice would appreciated thanks
Title Extraction/Identification from PDFs,"<p>I have a large number of pdfs in different formats. Among other things, I need to extract their titles (not the document name, but a title in the text). Due to the range of formats, the titles are not in the same locations in the pdfs. Further, some of the pdfs are actually scanned images (I need to use OCR/Optical Character Recognition on them). The titles are sometimes one line, sometimes 2. They do not tend to have the same set of words. In the range of physical locations the titles usually show up, there are often other words (ie if doc 1 has title 1 at x1, y1, doc 2 might have title 2 at x2, y2 but have other non-title text at x1 y1). Further, there are some very rare cases where the pdfs don't have a title.</p>

<p>So far I can use pdftotext to extract text within a given bounding box, and convert it to a text file. If there's a title, this lets me capture the title, but often with other extraneous words included. This also only works on non-image pdfs. I'm wondering if a) There's a good way to identify the title from among all the words I extract for a document (because there are often extraneous words), ideally with a good way to identify that no title exists, and b) if there are any tools that are equivalent to pdftotext that will also work on scanned images (I do have an ocr script working, but it does ocr over an entire image rather than a section of one). </p>

<p>One method that somewhat answers the title dilemma is to extract the words in the bounding box, use the rest of the document to identify which of the bounding box words are keywords for the document, and construct the title from the keywords. This wouldn't extract the actual title, but may give words that could construct a reasonable alternative. I'm already extracting keywords for other parts of the project, but I would definitely prefer to extract the actual title as people may be using the verbatim title for lookup purposes. </p>

<p>Further note if it wasn't clear - I'm trying to do this programatically with open source/free tools, ideally in Python, and I will have a large number of documents (10,000+).  </p>
",Named Entity Recognition (NER),title extraction identification pdfs large number pdfs different format among thing need extract title document name title text due range format title location pdfs pdfs actually scanned image need use ocr optical character recognition title sometimes one line sometimes tend set word range physical location title usually show often word ie doc ha title x doc might title x non title text x rare case pdfs title far use pdftotext extract text within given bounding box convert text file title let capture title often extraneous word included also work non image pdfs wondering good way identify title among word extract document often extraneous word ideally good way identify title exists b tool equivalent pdftotext also work scanned image ocr script working doe ocr entire image rather section one one method somewhat answer title dilemma extract word bounding box use rest document identify bounding box word keywords document construct title keywords extract actual title may give word could construct reasonable alternative already extracting keywords part project would definitely prefer extract actual title people may using verbatim title lookup purpose note clear trying programatically open source free tool ideally python large number document
How to conduct nested named entity recognition in OpenNLP?,"<p>I am currently working on a java web server project, that requires the use of Natural Language processing, specifically <strong>Named Entity Recognition</strong> (<strong>NER</strong>). </p>

<p>I was using OpenNLP for java, since it was easy to add custom training data. It works perfectly. </p>

<p>However, I need to also be able to extract entites inside of entities (<strong>Nested named entity recognition</strong>). I tried doing this in OpenNLP, but I got parsing errors. So my guess is that OpenNLP sadly does not support nested entities.</p>

<p>Here is an example of what I need to parse:</p>

<p>Remind me to <strong>[START:reminder]</strong> give some presents to <strong>[START:contact]</strong> John <strong>[END]</strong> and <strong>[START:contact]</strong> Charlie <strong>[END][END]</strong>.</p>

<p>If this cannot be achieved with OpenNLP, is there any other Java NLP Library that could do this. If there are no Java libraries at all, are there any NLP libraries in any other language that can do this?</p>

<p>Please help. Thanks!</p>
",Named Entity Recognition (NER),conduct nested named entity recognition opennlp currently working java web server project requires use natural language processing specifically named entity recognition ner wa using opennlp java since wa easy add custom training data work perfectly however need also able extract entites inside entity nested named entity recognition tried opennlp got parsing error guess opennlp sadly doe support nested entity example need parse remind start reminder give present start contact john end start contact charlie end end achieved opennlp java nlp library could java library nlp library language please help thanks
How to classifiy NER using sequence label/ IOB tag?,"<p>I am trying to recognize and classify the entity types based on the IOB/Sequence labeling. </p>

<p>For example:</p>

<pre><code>Alex  B-PER
Tan   I-PER
is O
a O
president O
. O
</code></pre>

<p>I only can classify it by token which mean it classify Alex  B-PER and Tan   I-PER as a different class/entity type. </p>

<p>So my question how to classify Alex B-PER and Tan  I-PER as one/same class which is PERSON?</p>

<p>Thank you.</p>
",Named Entity Recognition (NER),classifiy ner using sequence label iob tag trying recognize classify entity type based iob sequence labeling example classify token mean classify alex b per tan per different class entity type question classify alex b per tan per one class person thank
How to train OpenNLP model to extract multi set words,"<p>I am newbie to Open NLP - Entity extraction with NER, I had train and evaluated models for Entity extraction in Open NLP NER, which works fine when I give input text with an entity of one word Eg: ""I want to buy Cadbury""</p>

<p>But It does not works works for the Multi-word scenarios Eg: ""I want to but an Apple MacBook""</p>

<p>How train the models to pick the multi word</p>

<p>PS: I have understood that I need to do something related with BiGrams provided in NLP, but how do i do it with OpenNLP?</p>
",Named Entity Recognition (NER),train opennlp model extract multi set word newbie open nlp entity extraction ner train evaluated model entity extraction open nlp ner work fine give input text entity one word eg want buy cadbury doe work work multi word scenario eg want apple macbook train model pick multi word p understood need something related bigram provided nlp opennlp
Stanford NLP - NER - Train NER with names that have multiple tokens,"<p>I have recently started taking a look at Stanford NLP (using the C# port). 
I have planned on using NER to identify store location names and product names - to do this i will need to train the tagger, which i am in the process of doing. </p>

<p>However, some locations for example ""Kings Cross"" should only really be considered a location when both tokens are together. i.e i wouldn't want ""Kings"" getting tagged as a location when it is used by itself in a sentence.</p>

<p>So my question really is: Is there a defined way that it is recommended that I deal with locations/names that have a space in them (both in my training files, and in code)?</p>

<p>Thank you for any help. </p>
",Named Entity Recognition (NER),stanford nlp ner train ner name multiple token recently started taking look stanford nlp using c port planned using ner identify store location name product name need train tagger process however location example king cross really considered location token together e want king getting tagged location used sentence question really defined way recommended deal location name space training file code thank help
What do the BILOU tags mean in Named Entity Recognition?,"<p>Title pretty much sums up the question.  I've noticed that in some papers people have referred to a BILOU encoding scheme for NER as opposed to the typical BIO tagging scheme (Such as this paper by Ratinov and Roth in 2009 <a href=""http://cogcomp.cs.illinois.edu/page/publication_view/199"" rel=""noreferrer"">http://cogcomp.cs.illinois.edu/page/publication_view/199</a>)</p>

<p>From working with the 2003 CoNLL data I know that</p>

<pre><code>B stands for 'beginning' (signifies beginning of an NE)
I stands for 'inside' (signifies that the word is inside an NE)
O stands for 'outside' (signifies that the word is just a regular word outside of an NE)
</code></pre>

<p>While I've been told that the words in BILOU stand for</p>

<pre><code>B - 'beginning'
I - 'inside'
L - 'last'
O - 'outside'
U - 'unit'
</code></pre>

<p>I've also seen people reference another tag </p>

<pre><code>E - 'end', use it concurrently with the 'last' tag
S - 'singleton', use it concurrently with the 'unit' tag
</code></pre>

<p>I'm pretty new to the NER literature, but I've been unable to find something clearly explaining these tags.  My questions in particular relates to what the difference between 'last' and 'end' tags are, and what 'unit' tag stands for.</p>
",Named Entity Recognition (NER),bilou tag mean named entity recognition title pretty much sum question noticed paper people referred bilou encoding scheme ner opposed typical bio tagging scheme paper ratinov roth working conll data know told word bilou stand also seen people reference another tag pretty new ner literature unable find something clearly explaining tag question particular relates difference last end tag unit tag stand
Extracting the relationship between entities in Stanford CoreNLP,"<p>I want to extract the complete relationship between two entities using Stanford CoreNLP (or maybe other tools).</p>

<p>For example:</p>

<blockquote>
  <p>Windows is <em>more popular than</em> Linux.</p>
  
  <p>This tool <em>requires</em> Java.</p>
  
  <p>Football is <em>the most popular game in</em> the World.</p>
</blockquote>

<p>What is the fastest way? And what is the best practice for that?</p>

<p>Thanks in advance</p>
",Named Entity Recognition (NER),extracting relationship entity stanford corenlp want extract complete relationship two entity using stanford corenlp maybe tool example window popular linux tool requires java football popular game world fastest way best practice thanks advance
SpaCy for detecting merchant name,"<p>I'm quite new in the NLP world and the spaCy is the first tool I'm facing with.</p>
<p>I need to autodetect a merchant name from the bank SMS messages.
For instance:
&quot;Payment of 4,10 EUR in <strong>FARMACIA LITORAL</strong> with your card ending in 1234 accepted.&quot;
or
&quot;Paid €3.77 at <strong>AliExpress</strong> Spent today: €3.77&quot;</p>
<p>My first thought was to use Named Entity and detect &quot;ORG&quot; or &quot;GPE&quot; tags but it doesn't works for abbreviations and cuts like &quot;AMZN MKTP&quot; or  &quot;ASICS FO&quot;.</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_trf&quot;)
doc = nlp(&quot;Paid €3.77 at AMZN MKTP Spent today: €3.77&quot;)

for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)
</code></pre>
<p>3.77 6 10 MONEY
today 30 35 DATE
3.77 38 42 MONEY</p>
<p>Advise please in what direction to move?</p>
",Named Entity Recognition (NER),spacy detecting merchant name quite new nlp world spacy first tool facing need autodetect merchant name bank sm message instance payment eur farmacia litoral card ending accepted paid aliexpress spent today first thought wa use named entity detect org gpe tag work abbreviation cut like amzn mktp asics fo money today date money advise please direction move
How can I develop a model that detects organization names in an unstructured text,"<p>I am creating a resume parser algorithm in Python to extract experiences in a resume in pdf format. I am trying to detect organization names for each experience.</p>
<p>I have tried default spaCy NER model and most popular NER models on the huggingface. Problem here I think is since the text is in a unstructured format like:</p>
<pre><code>Silver Technologies Ltd. Singapore Software Developer May 2008 – May 2009
</code></pre>
<p>This model can't parse and tag the text. I am trying to extract <code>Silver Technologies Ltd.</code> from this text. Other parts of the parser are using a rule based algorithm so I can use a rule based algorithm for organization detection. But I can't think of a rule that works on most of the examples. I would appreciate any suggestions that uses a model or a rule based algorithm.</p>
",Named Entity Recognition (NER),develop model detects organization name unstructured text creating resume parser algorithm python extract experience resume pdf format trying detect organization name experience tried default spacy ner model popular ner model huggingface problem think since text unstructured format like model parse tag text trying extract text part parser using rule based algorithm use rule based algorithm organization detection think rule work example would appreciate suggestion us model rule based algorithm
Find all locations / cities / places in a text,"<p>If I have a text containing for example an article of a newspaper in Catalan language, how could I find all cities from that text?</p>

<p>I have been looking at the package nltk for python and I have downloaded the corpus for catalan language (nltk.corpus.cess_cat).</p>

<p>What I have at this moment:
I have installed all necessary from nltk.download().  An example of what I have at this moment:</p>

<pre><code>te = nltk.word_tokenize('Tots els gats son de Sant Cugat del Valles.')

nltk.pos_tag(te)
</code></pre>

<p>The city is 'Sant Cugat del Valles'. What I get from the output is:</p>

<pre><code>[('Tots', 'NNS'),
 ('els', 'NNS'),
 ('gats', 'NNS'),
 ('son', 'VBP'),
 ('de', 'IN'),
 ('Sant', 'NNP'),
 ('Cugat', 'NNP'),
 ('del', 'NN'),
 ('Valles', 'NNP')]
</code></pre>

<p>NNP seems to indicate nouns whose first letter is uppercase. Is there a way of getting places or cities and not all names? 
 Thank you</p>
",Named Entity Recognition (NER),find location city place text text containing example article newspaper catalan language could find city text looking package nltk python downloaded corpus catalan language nltk corpus ce cat moment installed necessary nltk download example moment city sant cugat del valles get output nnp seems indicate noun whose first letter uppercase way getting place city name thank
Is there a way to tell spaCy that certain words are related to a certain number? e.g. Feed rate and aspirator rate were 3l/hr and 100% respectively,"<p>I'm very new to Python, spaCy, and even stack overflow in general. So forgive me if my question is too vague. I would like to ask if there's a way to tell spaCy that certain words in a sentence are related to certain number?</p>
<pre><code>sentence = &quot;The feed rate, aspirator rate, inlet and outlet temperature and air flow rate were approximately 3l/hr, 100%, 120C, 90C, and 357l/hr, respectively.&quot;
</code></pre>
<p>From above, we know that feed rate is 3l/hr, aspirator rate is 100%, inlet temperature is 120C, outlet temperature is 90C, and finally, air flow rate is 357l/hr</p>
<p>I would like to do the same for parameters. Where I can extract the parameter name, and value that comes with it. Is it possible?</p>
<p>After looking at dependency parser (thank you for that), I realised that I can use the patterns entity ruler</p>
<pre><code>import spacy

#Build upon the spaCy Small Model
nlp = spacy.blank(&quot;en&quot;)
ruler = nlp.add_pipe(&quot;entity_ruler&quot;)
patterns = [
    {&quot;label&quot;: &quot;PARAMETER&quot;, &quot;pattern&quot;: [{&quot;TEXT&quot;:{&quot;REGEX&quot;:r&quot;(inlet temperature|outlet temperature|inlet|outlet)&quot;}}]},    
    {&quot;label&quot;: &quot;TEMP&quot;, &quot;pattern&quot;: [{&quot;TEXT&quot;:{&quot;REGEX&quot;:&quot;\d+(C|K|F)&quot;}}]},
    {&quot;label&quot;: &quot;aspirator rate&quot;, &quot;pattern&quot;: [{&quot;TEXT&quot;:{&quot;REGEX&quot;:r&quot;\d{1,5}&quot;}},{&quot;LOWER&quot;:r&quot;%&quot;}]},
    {&quot;label&quot;: &quot;feed rate&quot;, &quot;pattern&quot;: [{&quot;TEXT&quot;:{&quot;REGEX&quot;:r&quot;\d{1,5}[ml/min]&quot;}},{&quot;LOWER&quot;:r&quot;ml/min&quot;}]}
    
]

ruler.add_patterns(patterns)

text = &quot;The feed rate, aspirator rate, inlet and outlet temperature and air flow rate were approximately 3 ml/min, 100%, 120C and 90C and 357 l/hr, respectively.&quot;

##Process the text with spaCy
doc = nlp(text)

# Iterate over the entities in the document
for ent in doc.ents:
    print (ent.label_,ent.text)
</code></pre>
<p>This gives a very awkward output of</p>
<pre><code>PARAMETER inlet
PARAMETER outlet
aspirator rate 100%
TEMP 120C
TEMP 90C

</code></pre>
<p>It seems that I was unable to get the feed rate or the parameter &quot;outlet temperature&quot;</p>
<p>May I ask for help on the following issues?</p>
<ol>
<li>I'm only trying things out to see if I can separate them like this.
May I ask for your opinions on how to use REGEX to extract words
like  &quot;ml/min&quot;,&quot;L/hr&quot; or anything with special characters.</li>
<li>Sometimes scientific articles have different ways of writing. Some
will write &quot;inlet air temperature&quot; while some will write &quot;inlet
temperature&quot;. May I ask if I can use REGEX to encompass all these
varieties? So I can assign PARAMETER --&gt; outlet air temperature</li>
</ol>
<p>Thank you so much!</p>
",Named Entity Recognition (NER),way tell spacy certain word related certain number e g feed rate aspirator rate l hr respectively new python spacy even stack overflow general forgive question vague would like ask way tell spacy certain word sentence related certain number know feed rate l hr aspirator rate inlet temperature c outlet temperature c finally air flow rate l hr would like parameter extract parameter name value come possible looking dependency parser thank realised use pattern entity ruler give awkward output seems wa unable get feed rate parameter outlet temperature may ask help following issue trying thing see separate like may ask opinion use regex extract word like ml min l hr anything special character sometimes scientific article different way writing write inlet air temperature write inlet temperature may ask use regex encompass variety assign parameter outlet air temperature thank much
Problems with reproducing the training of the spaCy pipeline,"<p>I'm trying to reproduce the training of one of the spaCy pipeline for Italian language: <strong>it_core_news_sm</strong>.
This pipeline is trained on 2 datasets:</p>
<ol>
<li><em>UD_Italian-ISDT</em> for the conllu tasks</li>
<li><em>WikiNer</em> for NET tagging</li>
</ol>
<p>Where can I find more info about the data used to trained? They used both training and dev sets to train the pipeline? Did They group sentences together as suggested by the spaCy command <em>convert</em>?</p>
<p>And also, how is it possible to train the pipeline on 2 datasets? Should I train first the pipeline on the first dataset and then the NER component on the second dataset, or is it possible to do it simultaneously?</p>
<p>Anyway, at the moment I trained the dataset on just the UD_Italian-ISDT dataset for POS tagging (coarse-grained and fine-grained), parsing, lemmatization and morphological analysis using the config file for the training available <a href=""https://github.com/explosion/spacy-models/releases/tag/it_core_news_sm-3.6.0"" rel=""nofollow noreferrer"">here</a>. I used the train set to train and validation set to test the pipeline and I obtain results far lower respect to those claimed by spaCy <a href=""https://github.com/explosion/spacy-models/releases/tag/it_core_news_sm-3.6.0"" rel=""nofollow noreferrer"">here</a>. Here's my results:</p>
<p>pos_acc: 0.9020224719</p>
<p>morph_acc: 0.9004449638</p>
<p>tag_acc: 0.9001348315</p>
<p>dep_uas: 0.7801636499</p>
<p>dep_las: 0.7451524919</p>
<p>sents_p: 0.9754816112</p>
<p>sents_r: 0.9875886525</p>
<p>sents_f: 0.9814977974</p>
<p>lemma_acc: 0.9028083577</p>
<p>Could someone help me with this? Where I an find more info about the setting of the training and what reasons could cause these scores?</p>
",Named Entity Recognition (NER),problem reproducing training spacy pipeline trying reproduce training one spacy pipeline italian language core news sm pipeline trained datasets ud italian isdt conllu task wikiner net tagging find info data used trained used training dev set train pipeline group sentence together suggested spacy command convert also possible train pipeline datasets train first pipeline first dataset ner component second dataset possible simultaneously anyway moment trained dataset ud italian isdt dataset po tagging coarse grained fine grained parsing lemmatization morphological analysis using config file training available used train set train validation set test pipeline obtain result far lower respect claimed spacy result po acc morph acc tag acc dep uas dep la sent p sent r sent f lemma acc could someone help find info setting training reason could cause score
Displacy not recognising custom entities,"<p>I am trying to write a custom ner program. I am able to extract those entities properly. it's just the displacy is not able to display them.</p>
<pre><code>import spacy
from spacy import displacy
import re
from IPython.display import HTML

# Load the spaCy English model
nlp = spacy.load(&quot;en_core_web_sm&quot;)

# Process the cleaned text
doc = nlp(cleaned_text)

# Define the custom NER patterns
ner_patterns = [
    {&quot;label&quot;: &quot;BUN&quot;, &quot;pattern&quot;: [{&quot;SHAPE&quot;: &quot;Xxxxx&quot;}, {&quot;LOWER&quot;: &quot;bun&quot;}]},
    {&quot;label&quot;: &quot;SerumUricAcid&quot;, &quot;pattern&quot;: [{&quot;SHAPE&quot;: &quot;Xxxxx&quot;}, {&quot;LOWER&quot;: &quot;serum&quot;}, {&quot;LOWER&quot;: &quot;uric&quot;}, {&quot;LOWER&quot;: &quot;acid&quot;}]},
    {&quot;label&quot;: &quot;SerumPotassium&quot;, &quot;pattern&quot;: [{&quot;SHAPE&quot;: &quot;Xxxxx&quot;}, {&quot;LOWER&quot;: &quot;serum&quot;}, {&quot;LOWER&quot;: &quot;potassium&quot;}]}
]

# Add the custom NER patterns to the entity ruler
ruler = nlp.add_pipe(&quot;entity_ruler&quot;, after='ner')
ruler.add_patterns(ner_patterns)

# Extracted values
extracted_values = {}

# Process the tokens to extract values
for token in doc:
    if token.text in [&quot;BUN&quot;, &quot;Serum&quot;, &quot;Uric&quot;, &quot;Acid&quot;, &quot;Potassium&quot;]:
        label = token.text
        next_token = token.nbor()
        if next_token.like_num:
            value = next_token.text
            extracted_values[label] = value

# Print the extracted values
print(&quot;Extracted values:&quot;)
for label, value in extracted_values.items():
    print(label + &quot;:&quot;, value)

# Prepare entity highlighting with linear gradients
colors = {
    &quot;BUN&quot;: &quot;linear-gradient(90deg, #aa9cfc, #fc9ce7)&quot;,
    &quot;SerumUricAcid&quot;: &quot;linear-gradient(90deg, #ffabab, #ffd8a8)&quot;,
    &quot;SerumPotassium&quot;: &quot;linear-gradient(90deg, #a8e6cf, #dcedc1)&quot;
}
options = {&quot;ents&quot;: [&quot;BUN, &quot;SerumUricAcid&quot;, &quot;SerumPotassium&quot;], &quot;colors&quot;: colors}

# Render the displaCy visualization with highlighting
html = displacy.render(doc, style=&quot;ent&quot;, options=options)

# Display the HTML
HTML(html)
</code></pre>
<p>In the output I am getting the extracted values just fine. The displacy is working just fine with normal entities. its just not able to recognise custom labels. It prints the text again with no highlight whatsoever.
I will also add the cleaned text that I am using in the code. <br/><br/>
BIOCHEMISTRY KIDNEY FUNCTION TEST (KFT) TEST VALUE UNIT REFERENCE BUN 10.27 mg/dl 7.9 - 20 Serum Urea 22 mg/dl 13 - 40 Serum Creatinine H 0.9 mg/dl 0.5 - 0.8 Serum Calcium 9.0 mg/dl 8.8 - 10.6 Serum Potassium 3.9 mmol/L 3.5 - 5.1 Serum Sodium L 132 mmol/L 136 - 146 Serum Uric Acid 5 mg/dl 2.6 - 6 Urea / Creatinine Ratio 24.44 BUN / Creatinine Ratio 11.41 ~~~ End of report ~~~ LABSMART SAMPLE REPORT Patient Name: Mrs. Dummy Registered on: 09/08/2022 11:35 AM 1001 Age / Sex: 34 YRS / F Collected on: 09/08/2022 Referred By: Dr. Self Received on: 12/08/2022 Reg. no. / UHID: 1001 / Reported on: 09/08/2022 11:35 AM Investigations: Kidney Function Test (KFT) Page 1 of 1 Mr. Sachin Sharma DMLT, Lab Incharge Dr. A. K. Asthana MBBS, MD Pathologist</p>
",Named Entity Recognition (NER),displacy recognising custom entity trying write custom ner program able extract entity properly displacy able display output getting extracted value fine displacy working fine normal entity able recognise custom label print text highlight whatsoever also add cleaned text using code biochemistry kidney function test kft test value unit reference bun mg dl serum urea mg dl serum creatinine h mg dl serum calcium mg dl serum potassium mmol l serum sodium l mmol l serum uric acid mg dl urea creatinine ratio bun creatinine ratio end report labsmart sample report patient name mr dummy registered age sex yr f collected referred dr self received reg uhid reported investigation kidney function test kft page mr sachin sharma dmlt lab incharge dr k asthana mbbs md pathologist
Regex code to identify Keyword followed by complex pattern (from variable human inputs),"<p>I am working on an NLP project with  data that requires some cleaning of PII. I have dates and names mostly taken care using spaCy NER, but I need to find instances of (case insensitive) Room followed by alphanumeric. I have something that works, but it is wildly ugly. I inherited a pattern from a coworker, but theirs missed many patterns, and will grab things like <code>The room was...</code> and miss <code>room  16b/16a/16c</code>. I am working python 3.X environment.</p>
<p>I am trying to learn and the best I can using regex101. While this pattern mostly works at finding the alpha-numeric pattern after room, desired, but will grab any word after room, this is undesired. <code>The room was cleaned</code> becomes <code>The room number cleaned</code>. I think the <code>\w?</code> is the cause of the undesired. I added the pattern after the <code>?\w? ?</code> and don't think it is efficient, nor easy to understand what is going on.</p>
<p>My pattern</p>
<pre><code>(room|rm) ?(#|number|no.?)? ?\w? ?(\d+|[a-z0-9\-?\/\(\) ]{2,5})[a-z]?| (room?) ([0-9]+[a-z/]+)+| room? [a-z]?[0-9/]+
</code></pre>
<p>and examples</p>
<pre><code>room (b7)
rm 2
rm no 4
room a12
Room 12
Roomd25
room D25
ROOM D25
ROOM C-11
room 17
room A4B
room 101
rm #17 
room 37/39
ROOM B-1
room C 29 from 
room C23/25/27
room 16b/18a/18b
Clean the room now _AVOID_
</code></pre>
",Named Entity Recognition (NER),regex code identify keyword followed complex pattern variable human input working nlp project data requires cleaning pii date name mostly taken care using spacy ner need find instance case insensitive room followed alphanumeric something work wildly ugly inherited pattern coworker missed many pattern grab thing like miss working python x environment trying learn best using regex pattern mostly work finding alpha numeric pattern room desired grab word room undesired becomes think cause undesired added pattern think efficient easy understand going pattern example
How to extract location from each hover element?,"<p>I am trying to extract the location of each commenters when hovered to the commenter's name from <a href=""https://interesting.quora.com/Don-t-stay-in-a-relationship-when-you-aren-t-ready-to-commit-fully-https-www-quora-com-Is-it-bad-that-I-want-to-end"" rel=""nofollow noreferrer"">this website</a>. However, it kept outputting the location of poster.</p>
<p>This is my draft code:</p>
<pre><code>cmts = []

all_comments = driver.find_elements(By.CSS_SELECTOR, 'div[class=&quot;q-box qu-ml--small qu-flex--auto&quot;]')

for comments in all_comments:
    cmter = comments.find_element(By.CSS_SELECTOR, 'a[class=&quot;q-box Link___StyledBox-t2xg9c-0 dFkjrQ puppeteer_test_link qu-color--gray_dark qu-cursor--pointer qu-hover--textDecoration--underline&quot;]')

    webdriver.ActionChains(driver).move_to_element(cmter).perform()
    time.sleep(1.5)
    try:
        cmterloc = driver.find_element(By.XPATH, '//div[@class=&quot;q-text qu-truncateLines--2&quot;][contains(., &quot;Lives in&quot;) or contains(., &quot;Lived in&quot;)]/span/span'))
    except:
        cmterloc = 'None'
    cmt = comments.find_element(By.CSS_SELECTOR, 'div[class=&quot;q-text&quot;]').text

    if cmt != '' and [cmter.text, cmterloc, cmt] not in cmts:
        cmts.append([cmter.text, cmterloc, cmt])
</code></pre>
<p>The expected output will be something like this:
[author_name1, author_location1, comment1]</p>
<p>[author_name2, author_location2, comment2]</p>
<p>[author_name3, author_location3, comment3]</p>
",Named Entity Recognition (NER),extract location hover element trying extract location commenters hovered commenter name website however kept outputting location poster draft code expected output something like author name author location comment author name author location comment author name author location comment
Text extraction from .docx file,"<p>I'm aware of libraries that are capable of this, but they aren't useful to me given constraints in their licensing structure or the cost associated.  I'd like to extract the text contents from a Word file (docx) and am close, but encountering a small issue.</p>
<p>First, I extract to a temporary directory:</p>
<pre><code>using (ZipArchive archive = ZipFile.OpenRead(_Filename))
{
    archive.ExtractToDirectory(_TempDirectory);
}
</code></pre>
<p>Then load the <code>XmlDocument</code> and find the body:</p>
<pre><code>XmlDocument doc = new XmlDocument();
doc.Load(_TempDirectory + &quot;word/document.xml&quot;);
XmlNodeList nodes = doc.GetElementsByTagName(&quot;w:body&quot;);
</code></pre>
<p>And then iterate over each of the nodes and joining the extracted text using a <code>StringBuilder</code>:</p>
<pre><code>StringBuilder sb = new StringBuilder();
foreach (XmlNode node in nodes)
{
    sb.Append(ExtractAllText(node));
}

return sb.ToString();
</code></pre>
<p>The contents of <code>ExtractAllText</code> is:</p>
<pre><code>private string ExtractAllText(XmlNode node)
{
    StringBuilder sb = new StringBuilder();

    if (node.NodeType == XmlNodeType.Text)
    {
        Console.WriteLine(&quot;Text: node name &quot; + node.Name + &quot; &quot; + node.Value);

        sb.Append(node.InnerText);

        if (node.Attributes != null)
        {
            foreach (var attr in node.Attributes) Console.WriteLine(attr.ToString());
            // displays nothing for any node!
        }
    }

    if (node.HasChildNodes)
    {
        foreach (XmlNode curr in node.ChildNodes)
        {
            sb.Append(ExtractAllText(curr));
        }
    }

    return sb.ToString();
}
</code></pre>
<p>The problem I'm facing is this: sometimes strings are split into two separate nodes and sometimes they aren't.  Further, the XML gives indication that whitespace should be preserved (e.g. <code>&lt;w:t xml:space=&quot;preserve&quot;&gt;, &lt;/w:t&gt;</code> but I cannot seem to access this via <code>node.Attributes</code>.   I'm assuming that accessing this attribute (<code>xml:space</code> and <code>preserve</code>) would allow me to understand whether or not to append whitespace to the <code>StringBuilder</code> before or after the text of that particular node.</p>
<p>This results in a Word doc that looks like this:
<a href=""https://i.sstatic.net/S96m3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/S96m3.png"" alt=""word doc"" /></a></p>
<p>Resulting in a string like this:
<code>My Name123 Vine Street, San Jose CA 95128 USA |(408) 555-1212my@email.com | www.mysite.com | linkedinobjectiveThis is my resume objective section!</code></p>
<p>Note how &quot;My Name&quot; and &quot;123 Vine Street&quot; are joined, and other similar examples exist.</p>
<p>Any input or advice would be much appreciated!</p>
<p>Edit:
Also tried with <code>DocumentFormat.OpenXml</code> and encountered similar results:</p>
<pre><code>using (WordprocessingDocument doc = WordprocessingDocument.Open(_Filename, false))
{
    body = doc.MainDocumentPart.Document.Body;
    return body.InnerText;
}
</code></pre>
<p>Shows:</p>
<pre><code>My Name123 Vine Street, San Jose CA 95128 USA  |  (408) 555-1212my@email.com | www.mysite.com | linkedinobjectiveThis is my resume objective section!
</code></pre>
<p>Edit 2: another attempt with <code>DocumentFormat.OpenXml</code> produced again the same problem:</p>
<pre><code>using (WordprocessingDocument doc = WordprocessingDocument.Open(_Filename, false))
{
    foreach (OpenXmlElement element in doc.MainDocumentPart.Document.Body.Elements())
    {
        if (element is Table)
        {
            Console.WriteLine(&quot;Table: &quot; + element.InnerText);
        }
        else if (element is Paragraph)
        {
            Console.WriteLine(&quot;Paragraph: &quot; + element.InnerText);
        }
        else if (element is Run)
        {
            Console.WriteLine(&quot;Run: &quot; + element.InnerText);
        }
    }
}
</code></pre>
<p>Results in:</p>
<pre><code>Table: My Name123 Vine Street, San Jose CA 95128 USA  |  (408) 555-1212my@email.com | www.mysite.com | linkedin
Paragraph: objective
Paragraph: This is my resume objective section!
</code></pre>
",Named Entity Recognition (NER),text extraction docx file aware library capable useful given constraint licensing structure cost associated like extract text content word file docx close encountering small issue first extract temporary directory load find body iterate node joining extracted text using content problem facing sometimes string split two separate node sometimes xml give indication whitespace preserved e g seem access via assuming accessing attribute would allow understand whether append whitespace text particular node result word doc look like resulting string like note name vine street joined similar example exist input advice would much appreciated edit also tried encountered similar result show edit another attempt produced problem result
Custom NER model is not returning any entitiy and giving blank iterations,"<p>I am trying to learn how to train a custom NER model, earlier it was training all fine, but not sure what happened, now it is not returning any entity, and giving blank iterations:</p>
<pre><code>Training_data = [
    ('what is the price of McVeggie?', {'entities': [(21, 29, 'FoodProduct')]}),
    ('what is the price of McEgg?', {'entities': [(21, 26, 'FoodProduct')]}),
    ('what is the price of McChicken?', {'entities': [(21, 30, 'FoodProduct')]}),
    ('what is the price of McSpicy Paneer?', {'entities': [(21, 35, 'FoodProduct')]}),
    ('what is the price of McSpicy Chicken?', {'entities': [(21, 36, 'FoodProduct')]}),
] 
# Testing sample data       
testing_sample='what is the price of McAloo?'

import random
from pathlib import Path
import spacy
from tqdm import tqdm
from spacy.training.example import Example

model = None
output_dir=Path(&quot;C:\\Users\\Desktop\\Folder1&quot;)
iterations = 20

# loading a blank model

if model is not None:
    nlp = spacy.load(model)
    print(&quot;Loaded model '%s'&quot; % model)
else:
    nlp = spacy.blank('en')
    print(&quot;Created blank 'en' model&quot;)

# Setting up the pipeline

if 'ner' not in nlp.pipe_names:
    ner_pipe = nlp.add_pipe('ner',name='ner_pipe',last=True)
else:
    ner_pipe = nlp.get_pipe('ner')

# Adding entities labels to the ner pipeline

for text, annotations in Training_data:
    for entity in annotations.get('entities'):
        ner_pipe.add_label(entity[2])

# Getting names of other pipes to disable them during training

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
other_pipes

# Training NER model

with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.begin_training()
    for itn in range(iterations):
        print(&quot;Iteration Number:&quot; + str(itn))
        random.shuffle(Training_data)
        losses = {}
        for text, annotations in Training_data:
            doc = nlp.make_doc(text)
            example = Example.from_dict(doc, annotations)
            nlp.update([example], drop=0.2, sgd=optimizer, losses=losses)
        print(losses)
</code></pre>
<p>This code was able to train the entities earlier may be 3-4 days back, but today while checking it is not training, and giving blank iterations:</p>
<pre><code>Iteration Number:0
{}
Iteration Number:1
{}
Iteration Number:2
{}
</code></pre>
<p>also while testing I am getting the following error:</p>
<blockquote>
<p>ValueError: [E109] Component 'ner_pipe' could not be run. Did you
forget to call <code>initialize()</code>?</p>
</blockquote>
<p>Earlier it was giving the following data when tested and that is what I am looking for as well:</p>
<pre><code>Entities [('McVeggie', 'FoodProduct')]
Entities [('McEgg', 'FoodProduct')]
Entities [('McChicken', 'FoodProduct')]
Entities [('McSpicy Paneer', 'FoodProduct')]
Entities [('McSpicy Chicken', 'FoodProduct')]
</code></pre>
<p>I am kind of lost, can I please get some help with this?</p>
",Named Entity Recognition (NER),custom ner model returning entitiy giving blank iteration trying learn train custom ner model earlier wa training fine sure happened returning entity giving blank iteration code wa able train entity earlier may day back today checking training giving blank iteration also testing getting following error valueerror e component ner pipe could run forget call earlier wa giving following data tested looking well kind lost please get help
Name Entity Recognition in of own language,"<p>I have a dataset of my native language, can I generate a Named Entity Recognition(NER) model for my language?
How should I proceed with this, Is any tutorial based on this that develops a NER model of my own language which helps to learn from scratch?</p>
",Named Entity Recognition (NER),name entity recognition language dataset native language generate named entity recognition ner model language proceed tutorial based develops ner model language help learn scratch
Convert BERT model for Named Entity Recognition to ONNX. Input sequence length,"<p>I have exported my BERT model based on the following:
<a href=""https://github.com/microsoft/onnxruntime-inference-examples/blob/main/python/azureml/export.py"" rel=""nofollow noreferrer"">Microsoft ONNX export</a></p>
<p>Assuming that it is saved as <code>model.onnx</code></p>
<p>Shall I expect that my input sequence should always be truncated to the <code>max_len</code> dimension(512)? Or is there a way to handle dynamic inputs?</p>
<p>Further, is there a way to pack model and tokenizer to run inference in a pipeline? I saw that current huggingface libraries do not support loading ONNX models for NER task.</p>
",Named Entity Recognition (NER),convert bert model named entity recognition onnx input sequence length exported bert model based following microsoft onnx export assuming saved shall expect input sequence always truncated dimension way handle dynamic input way pack model tokenizer run inference pipeline saw current huggingface library support loading onnx model ner task
TextRank with Scattertext Visualisation,"<p>I recently tried to visualize TextRank using code, but I realized that the terms in the graph are not lemmatized. Is there a way to fix the following code so that all words in textrank_df['parse'] are lemmatized? I checked the pipeline components and all required components are in place ('tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'), so I'm really not sure where went wrong.</p>
<pre><code>import pytextrank
import spacy
import scattertext as st
nlp = spacy.load('en_core_web_sm')
nlp.add_pipe(&quot;textrank&quot;, last=True)
   
convention_df = textrank_df.assign(
    parse=lambda textrank_df: textrank_df['Combined'].apply(nlp),
)

corpus = st.CorpusFromParsedDocuments(
    convention_df,
    category_col='Response Variable',
    parsed_col='parse',
    feats_from_spacy_doc=st.PyTextRankPhrases()).build()
</code></pre>
<p>I tried the following code1, but it shows:
AttributeError: module 'pytextrank' has no attribute 'TextRank'.
I think it might be something to do with the format after this alteration.</p>
<ul>
<li><p>code 1</p>
<p>convention_df = textrank_df.assign(
parse=lambda textrank_df: textrank_df['Combined'].apply(lambda x: [token.lemma_ for token in nlp(x)]))</p>
</li>
</ul>
<p>I also tried code 2 which adds use_lemmas=True in PyTextRankPhrases() but did not work as well. The word is still presented in its original form.</p>
<ul>
<li><p>code 2</p>
<p>corpus = st.CorpusFromParsedDocuments(
convention_df,
category_col='Response Variable',
parsed_col='parse',
feats_from_spacy_doc=st.PyTextRankPhrases(use_lemmas=True)).build()</p>
</li>
</ul>
",Named Entity Recognition (NER),textrank scattertext visualisation recently tried visualize textrank using code realized term graph lemmatized way fix following code word textrank df parse lemmatized checked pipeline component required component place tok vec tagger parser attribute ruler lemmatizer ner really sure went wrong tried following code show attributeerror module pytextrank ha attribute textrank think might something format alteration code convention df textrank df assign parse lambda textrank df textrank df combined apply lambda x token lemma token nlp x also tried code add use lemma true pytextrankphrases work well word still presented original form code corpus st corpusfromparseddocuments convention df category col response variable parsed col parse feat spacy doc st pytextrankphrases use lemma true build
Entity extraction using custom rules with LLMs,"<p>I would like to perform a query on a database using natural language. However, running direct queries is not possible, and I have to do it via an API. For that, given a sentence, I'd like to extract some custom entities from it.</p>
<p>For example, if the sentence is: &quot;How many more than 20 years old male users viewed a page or logged in in the last 30 days?&quot;
The entities are:</p>
<pre><code>&lt;gender, equals, male&gt;,
&lt;age, greater than, 20&gt;,
&lt;event name, equals, view page&gt;,
&lt;event name, equals, login&gt;,
&lt;event timestamp, more than, 30 days&gt;
</code></pre>
<p>The first element of each entity (triplet) comes from the list of columns
The second element is inferred from context (nature of the operator if it's a single value or array to compare with)
The third element is also inferred from the context and must belong to the chosen column (first element)</p>
<p>I'm not able to restrict either of these elements for the entity. I'd like an agent first to check all the columns that are available, choose one and view their unique values. Once it gets that, either choose that column (first element) and value (third element) or look again and repeat these steps.</p>
<p>Any help on this would be great! I'm using langchain for this but using any other approach is fine too.</p>
",Named Entity Recognition (NER),entity extraction using custom rule llm would like perform query database using natural language however running direct query possible via api given sentence like extract custom entity example sentence many year old male user viewed page logged last day entity first element entity triplet come list column second element inferred context nature operator single value array compare third element also inferred context must belong chosen column first element able restrict either element entity like agent first check column available choose one view unique value get either choose column first element value third element look repeat step help would great using langchain using approach fine
How to compute a confusion matrix using spaCy&#39;s Scorer/Example classes?,"<p>I am trying to calculate the Accuracy and Specificity of a NER model using spaCy's API. The scorer.scores(example) method <a href=""https://spacy.io/api/scorer#score"" rel=""nofollow noreferrer"">found here</a> computes the Recall, Precision and F1_Score for the spans predicted by the model, but does not allow for the extrapolation of TP, FP, TN, or FN.</p>
<p>Below is the code I have currently written, with an example of the data structure I am using when passing my expected found entites into the model.</p>
<p>Code Being Used to Score the Model:</p>
<pre><code>import spacy
from spacy.scorer import Scorer
from spacy.training.example import Example

scorer = Scorer()
example = []
for obs in example_list:
    print('Input for a prediction:', obs['full_text'])
    pred = custom_nlp(obs['full_text'])  ## custom_nlp is the custome model I am using to generate docs
    print('Predicted based off of input:', pred, '// Entities being reviewed:', obs['entities'])
    temp = Example.from_dict(pred, {'entities': obs['entities']})
    example.append(temp)
scores = scorer.score_spans(example, &quot;ents&quot;)
</code></pre>
<p>The data structure I am currently using to load the Example class (list of dictionaries):
example_list[0]
{'full_text': 'I would like to remove my kid Florence from the will. How do I do that?',
'entities': [(30, 38, 'PERSON')]}</p>
<p>The result that I am returning from running print(scores) is as expected; a dictionary of tokenization's precision, recall, f1_score, as well as the entity recognition's precision, recall and f1_score.</p>
<pre><code>{'ents_p': 0.8731019522776573,
 'ents_r': 0.9179019384264538,
 'ents_f': 0.8949416342412452,
 'ents_per_type': {'PERSON': {'p': 0.9039145907473309,
   'r': 0.9694656488549618,
   'f': 0.9355432780847145},
  'GPE': {'p': 0.7973856209150327,
   'r': 0.9384615384615385,
   'f': 0.8621908127208481},
  'STREET_ADDRESS': {'p': 0.8308457711442786,
   'r': 0.893048128342246,
   'f': 0.8608247422680412},
  'ORGANIZATION': {'p': 0.9565217391304348,
   'r': 0.7415730337078652,
   'f': 0.8354430379746837},
  'CREDIT_CARD': {'p': 0.9411764705882353, 'r': 1.0, 'f': 0.9696969696969697},
  'AGE': {'p': 1.0, 'r': 1.0, 'f': 1.0},
  'US_SSN': {'p': 1.0, 'r': 1.0, 'f': 1.0},
  'DOMAIN_NAME': {'p': 0.4, 'r': 1.0, 'f': 0.5714285714285715},
  'TITLE': {'p': 0.8709677419354839, 'r': 0.84375, 'f': 0.8571428571428571},
  'PHONE_NUMBER': {'p': 0.8275862068965517,
   'r': 0.8275862068965517,
   'f': 0.8275862068965517},
  'EMAIL_ADDRESS': {'p': 1.0, 'r': 1.0, 'f': 1.0},
  'DATE_TIME': {'p': 1.0, 'r': 1.0, 'f': 1.0},
  'NRP': {'p': 1.0, 'r': 1.0, 'f': 1.0},
  'IBAN_CODE': {'p': 1.0, 'r': 1.0, 'f': 1.0},
  'IP_ADDRESS': {'p': 0.75, 'r': 0.75, 'f': 0.75},
  'ZIP_CODE': {'p': 0.8333333333333334,
   'r': 0.7142857142857143,
   'f': 0.7692307692307692},
  'US_DRIVER_LICENSE': {'p': 1.0, 'r': 1.0, 'f': 1.0}}}
</code></pre>
<p>How can I extrapolate the TP, FP, TN and FN from this function using some form of an attribute?</p>
",Named Entity Recognition (NER),compute confusion matrix using spacy scorer example class trying calculate accuracy specificity ner model using spacy api scorer score example method found computes recall precision f score span predicted model doe allow extrapolation tp fp tn fn code currently written example data structure using passing expected found entites model code used score model data structure currently using load example class list dictionary example list full text would like remove kid florence entity person result returning running print score expected dictionary tokenization precision recall f score well entity recognition precision recall f score extrapolate tp fp tn fn function using form attribute
How do I approach this named-entity classification task?,"<p>I am asking a related question <a href=""https://stackoverflow.com/questions/8219772/how-do-i-form-a-feature-vector-for-a-classifier-targeted-at-named-entity-recogni"">here</a> but this question is more general. I have taken a large corpora and annotated some words with their named-entities. In my case, they are domain-specific and I call them: Entity, Action, Incident. I want to use these as a seed for extracting more named-entities. For example, following is one sentence:</p>

<blockquote>
  <p>When the robot had a technical glitch, the object was thrown but was later caught by another robot.</p>
</blockquote>

<p>is tagged as:</p>

<blockquote>
  <p>When the <strong>(robot)/Entity</strong> had a <strong>(technical glitch)/Incident</strong>, the
  <strong>(object)/Entity</strong> was <strong>(thrown)/Action</strong> but was later <strong>(caught)/Action</strong> by
  <strong>(another robot)/Entity</strong>.</p>
</blockquote>

<p>Given examples like this, is there anyway I can train a classifier to recognize new named-entities? For instance, given a sentence like this:</p>

<blockquote>
  <p>The nanobot had a bug and so it crashed into the wall.</p>
</blockquote>

<p>should be tagged somewhat like this:</p>

<blockquote>
  <p>The <strong>(nanobot)/Entity</strong> had a <strong>(bug)/Incident</strong> and so it <strong>(crashed)/Action</strong> into the <strong>(wall)/Entity</strong>.</p>
</blockquote>

<p>Of course, I am aware that 100% accuracy is not possible but I would be interested in knowing any formal approaches to do this. Any suggestions?</p>
",Named Entity Recognition (NER),approach named entity classification task asking related question href question general taken large corpus annotated word named entity case domain specific call entity action incident want use seed extracting named entity example following one sentence p robot technical glitch object wa thrown wa later caught another robot tagged robot entity technical glitch incident object entity wa thrown action wa later caught action another robot entity given example like anyway train classifier recognize new named entity instance given sentence like nanobot bug crashed wall tagged somewhat like nanobot entity bug incident crashed action wall entity course aware accuracy possible would interested knowing formal approach suggestion
Machine learning entity candidate scoring,"<p>I am trying to understand the machine learning part behind Google's <a href=""https://ai.googleblog.com/2018/08/the-machine-learning-behind-android.html"" rel=""noreferrer"">Smart Linkify</a>. The article states the following regarding their <code>generate candidate entities</code> model.</p>

<blockquote>
  <p>A given input text is first split into words (based on space
  separation), then all possible word subsequences of certain maximum
  length (15 words in our case) are generated, and for each candidate
  the scoring neural net assigns a value (between 0 and 1) based on
  whether it represents a valid entity:</p>
</blockquote>

<p><a href=""https://i.sstatic.net/OewSD.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/OewSD.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p>Next, the generated entities that overlap are removed, favoring the
  ones with the higher score over the conflicting ones with a lower
  score.</p>
</blockquote>

<p>If I understand correctly the model tries every word in the sentence and a combination of that word up to 15 words total?</p>

<p>How can you train such model? I assume it's supervised learning but don't understand how such data could be labeled. Is it similar to NER where the entity is specified by character position? And there are only 2 entities in the data <code>entity</code> and <code>non-entity</code>.</p>

<p>And for the output of the model, the so called ""candidate score"", how can a a neural network return a single numerical value? (the score). Or is the output layer just a single node?</p>

<p>A more detailed explanation on:</p>

<ul>
<li><code>Possible word subsequences of certain maximum length</code> means it considers every word with the 7 words before and 7 after the word?</li>
<li>How can the neural net generate a score when its a binary classification <code>entity</code> and <code>non-entity</code>? Or do they mean the probability score for entity?</li>
<li>How to train a binary NER? Like any other NER except replace all entities to type 'entity' and then generate negative samples for <code>non-entity?</code></li>
<li>How can this model be fast, as they claim, when it processes every word in the text plus 7 words before and after said word?</li>
</ul>

<p>is what I'm looking for, to understand. </p>
",Named Entity Recognition (NER),machine learning entity candidate scoring trying understand machine learning part behind google smart linkify article state following regarding model given input text first split word based space separation possible word subsequence certain maximum length word case generated candidate scoring neural net assigns value based whether represents valid entity next generated entity overlap removed favoring one higher score conflicting one lower score understand correctly model try every word sentence combination word word total train model assume supervised learning understand data could labeled similar ner entity specified character position entity data output model called candidate score neural network return single numerical value score output layer single node detailed explanation mean considers every word word word neural net generate score binary classification mean probability score entity train binary ner like ner except replace entity type entity generate negative sample model fast claim process every word text plus word said word looking understand
Computing precision and recall in Named Entity Recognition,"<p>Now I am about to report the results from Named Entity Recognition. One thing that I find a bit confusing is that my understanding of precision and recall was that one simply sums up true positives, true negatives, false positives and false negatives over all classes.</p>

<p>But this seems implausible now that I think of it as each misclassification would give simultaneously rise to one false positive and one false negative (e.g. a token that should have been labelled as ""A"" but was labelled as ""B"" is a false negative for ""A"" and false positive for ""B""). Thus the number of the false positives and the false negatives over all classes would be the same which means that precision is (always!) equal to recall. This simply can't be true so there is an error in my reasoning and I wonder where it is. It is certainly something quite obvious and straight-forward but it escapes me right now.</p>
",Named Entity Recognition (NER),computing precision recall named entity recognition report result named entity recognition one thing find bit confusing understanding precision recall wa one simply sum true positive true negative false positive false negative class seems implausible think misclassification would give simultaneously rise one false positive one false negative e g token labelled wa labelled b false negative false positive b thus number false positive false negative class would mean precision always equal recall simply true error reasoning wonder certainly something quite obvious straight forward escape right
Sugestions on the best way to work with NLP mixed some numerical and categorical features,"<p>I'm working with a <strong>dataset of medicinal products across different countries</strong>, with each country having it's own data source. This results in the data not always being quite 'standardized' (for a lack of a better word), so one of the problems I'm trying to solve is to have the dosage in the same format across all countries. I've been doing it 'manually' for each country using <code>regex</code>, while having into account some criteria that I want to use as features in the model. For example: the <strong>number of active substances</strong> of the product, the <strong>pharmaceutical form</strong> and if some <strong>specific active substance is present in the product</strong>. By doing this 'manually' for like 1/3 of the countries, I've got a reasonable amount of records to train a model.</p>
<pre><code>Name   ActiveSubstances   NumberOfActSubst   PharmaceuticalForm   Dosage        DosageFinal

X      ['Y','Z']          2                  Tablet               '20mg/5mg'    '20 mg + 5 mg'

A      ['B']              1                  Tablet               '(50 microg+10mg)/ml''50 µg/ml + 10mg/ml'
</code></pre>
<p>I want this DosageFinal field to be filled automatically. What would be the best way to approach this task? <strong>I looked into parallel networks and the idea would be to use one NN to get the embeddings of the text variables, and another NN to collect the embeddings of the only numeric feature and later concatenate the embeddings.</strong> Am I overcomplicating it?</p>
",Named Entity Recognition (NER),sugestions best way work nlp mixed numerical categorical feature working dataset medicinal product across different country country data source result data always quite standardized lack better word one problem trying solve dosage format across country manually country using account criterion want use feature model example number active substance product pharmaceutical form specific active substance present product manually like country got reasonable amount record train model want dosagefinal field filled automatically would best way approach task looked parallel network idea would use one nn get embeddings text variable another nn collect embeddings numeric feature later concatenate embeddings overcomplicating
ent.sent.text in spacy returns labels instead of the sentence for NER problem,"<p>I'm trying to solve a Name Entity Recognision(NER) Problem using SpaCy of the PDF files. I want to get the modal verbs(will, shall, should, must, etc..) from the pdf files.</p>
<p>I trained the data in spaCy. When predicting using the trained modal, the <code>ent.sent.text</code> attribute of the modal usualy returns the text or can say the sentence from which the label extracted. But in my case it returns the label itself not the sentence. Anyone help me please.</p>
<p>The codes are giving below:</p>
<h1>Code for data preparation</h1>
<pre><code>def load_training_data_from_csv(file_path):
    nlp = spacy.load('en_core_web_md')
    train_data = []
    with open(file_path, 'r', encoding='cp1252') as f:
        reader = csv.DictReader(f)
        for row in reader:
            sentence = row['text']
            start, end = int(row['start']), int(row['end'])
            label = row['label']
            train_data.append((sentence, {&quot;entities&quot;: [(start, end, label)]}))
            # Check the alignment
            from spacy.training import offsets_to_biluo_tags
            doc = nlp.make_doc(sentence)
            tags = offsets_to_biluo_tags(doc, [(start, end, label)])
            if '-' in tags:
                print(f&quot;Warning: Misaligned entities in '{sentence}' with entities {[(start, end, label)]}&quot;)
    return train_data
</code></pre>
<h1>Training the model</h1>
<pre><code>def train_spacy_ner(train_data, n_iter=10):
    # Load the existing model
    nlp = spacy.load('en_core_web_md')

    # Add the NER pipeline if it doesn't exist
    if &quot;ner&quot; not in nlp.pipe_names:
        ner = nlp.create_pipe(&quot;ner&quot;)
        nlp.add_pipe(ner, last=True)
    else:
        ner = nlp.get_pipe(&quot;ner&quot;)


    # Add the new label &quot;CURRENCY&quot; to the NER model
    ner.add_label(&quot;WILL&quot;)
    ner.add_label(&quot;SHALL&quot;)
    ner.add_label(&quot;MUST&quot;)


    # Train the NER model
    optimizer = nlp.begin_training()
    for i in range(n_iter):
        print(&quot;Epoch - &quot;, i) if i % 2 == 0 or i == n_iter else None
        random.shuffle(train_data)
        losses = {}
        for text, annotations in train_data:
            doc = nlp.make_doc(text)
            example = spacy.training.Example.from_dict(doc, annotations)
            nlp.update([example], sgd=optimizer, losses=losses)
        print(&quot;loss : &quot;, losses) if i % 2 == 0 or i == n_iter else None

    return nlp
</code></pre>
<h1>Calling the functions</h1>
<pre><code># nlp = spacy.load(&quot;en_core_web_md&quot;)
file_path = &quot;/content/trainData.csv&quot;
TRAIN_DATA = load_training_data_from_csv(file_path)

# Train the model
nlp = train_spacy_ner(TRAIN_DATA)
nlp.to_disk('custom_NER')
</code></pre>
<h1>Predicting using model (here is the problem starting)</h1>
<pre><code>import spacy

nlp = spacy.load('custom_NER')
text = &quot;The language will be in english&quot;

doc = nlp(text)
# print(doc.ents)
for ent in doc.ents:
  print(ent.sent.text, ent.start_char, ent.end_char, ent.label_)
</code></pre>
<p><code>ent.sent.text</code> should return the sentence used above. But here the label itself is returing.</p>
<h1>Output getting</h1>
<pre><code>will 13 17 WILL
</code></pre>
<h1>Expecting output</h1>
<pre><code>The language will be in english 13 17 WILL
</code></pre>
<h1>Train Data</h1>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Text</th>
<th>start</th>
<th>end</th>
<th>label</th>
</tr>
</thead>
<tbody>
<tr>
<td>I will do the procedures</td>
<td>2</td>
<td>6</td>
<td>will</td>
</tr>
<tr>
<td>You should send the letters</td>
<td>4</td>
<td>10</td>
<td>should</td>
</tr>
</tbody>
</table>
</div>",Named Entity Recognition (NER),ent sent text spacy return label instead sentence ner problem trying solve name entity recognision ner problem using spacy pdf file want get modal verb shall must etc pdf file trained data spacy predicting using trained modal attribute modal usualy return text say sentence label extracted case return label sentence anyone help please code giving code data preparation training model calling function predicting using model problem starting return sentence used label returing output getting expecting output train data text start end label procedure send letter
"How does Apple find dates, times and addresses in emails?","<p>In the iOS email client, when an email contains a date, time or location, the text becomes a hyperlink and it is possible to create an appointment or look at a map simply by tapping the link. It not only works for emails in English, but in other languages also. I love this feature and would like to understand how they do it. </p>

<p>The naive way to do this would be to have many regular expressions and run them all. However I  this is not going to scale very well and will work for only a specific language or date format, etc. I think that Apple must be using some concept of machine learning to extract entities (8:00PM, 8PM, 8:00, 0800, 20:00, 20h, 20h00, 2000 etc.).</p>

<p>Any idea how Apple is able to extract entities so quickly in its email client? What machine learning algorithm would you to apply accomplish such task? </p>
",Named Entity Recognition (NER),doe apple find date time address email io email client email contains date time location text becomes hyperlink possible create appointment look map simply tapping link work email english language also love feature would like understand naive way would many regular expression run however going scale well work specific language date format etc think apple must using concept machine learning extract entity pm pm h h etc idea apple able extract entity quickly email client machine learning algorithm would apply accomplish task
How to resolve Error in seqeval in NER bert finetuning?,"<p>I'm trying to finetune a NER model, (BERT/BioBERT) and after first epoch of training, in Evaluation part, I got the following error, Any idea what is wrong?</p>
<pre><code>ValueError: Predictions and/or references don't match the expected format.
Expected format: {'predictions': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence'), 
'references': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence')},
Input predictions: [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], ..., [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]],
Input references: [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], ..., [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]
</code></pre>
<p>I am using very standard eval function and if I remove the evaluation from trainer, The model trains without any problem and the results are good, but I have little to no metrics.</p>
<pre class=""lang-py prettyprint-override""><code>def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = seqeval.compute(predictions=true_predictions, references=true_labels)
    return {
        &quot;precision&quot;: results[&quot;overall_precision&quot;],
        &quot;recall&quot;: results[&quot;overall_recall&quot;],
        &quot;f1&quot;: results[&quot;overall_f1&quot;],
        &quot;accuracy&quot;: results[&quot;overall_accuracy&quot;],
    }
</code></pre>
<p>Thanks.</p>
",Named Entity Recognition (NER),resolve error seqeval ner bert finetuning trying finetune ner model bert biobert first epoch training evaluation part got following error idea wrong using standard eval function remove evaluation trainer model train without problem result good little metric thanks
spaCy named entity recognition does not seem to work if the entity was at the beginning of the string,"<p>I'm using spaCy named entity recognition (NER) to parse names out of English sentences and I've noticed that if the named entity was at the very beginning of the sentence, it won't be picked up. let me show you an example:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_sm&quot;)

s = 'It must have made sense to Dumbledore, though, because he put it back in his pocket and said, Hagrid’s late.'

doc = nlp(s)

named_entities = [] 
if doc.ents:
    for ent in doc.ents:
        named_entities.append((ent.text, ent.label_))

for name in named_entities:
    print(name)
</code></pre>
<p>this works as expected and gives the output</p>
<pre><code>('Dumbledore', 'PERSON')
('Hagrid', 'ORG')
</code></pre>
<p>however, if I were to use the following sentence</p>
<pre><code>s = &quot;Dumbledore, however, was choosing another lemon drop and did not answer.&quot;
</code></pre>
<p>or any other sentence that starts with &quot;Dumbledore&quot;, the code above will not pick up Dumbledore as a person.</p>
<p>any suggestions on what could be the problem here</p>
",Named Entity Recognition (NER),spacy named entity recognition doe seem work entity wa beginning string using spacy named entity recognition ner parse name english sentence noticed named entity wa beginning sentence picked let show example work expected give output however use following sentence sentence start dumbledore code pick dumbledore person suggestion could problem
Building your own NLP API,"<p>I'm building a chatbot and I'm new to NLP.</p>

<p>(api.ai &amp; AlchemyAPI are too expensive for my use case. And wit.ai seems to be buggy and constantly changing at the moment.)</p>

<p>For the NLP experts, how easily can I replicate their services locally?</p>

<p>My vision so far (with node, but open to Python):</p>

<ul>
<li>entity extraction via StanfordNER</li>
<li>intent via NodeNatural's LogisticRegressionClassifier</li>
<li>training UI with text and validate/invalidate buttons (any prebuilt tools for this?)</li>
</ul>

<p>Are entities and intents all I'll need for a chatbot? How good will NodeNatural/StanfordNER be compared to NLP-as-a-service? What headaches am I not seeing?</p>
",Named Entity Recognition (NER),building nlp api building chatbot new nlp api ai alchemyapi expensive use case wit ai seems buggy constantly changing moment nlp expert easily replicate service locally vision far node open python entity extraction via stanfordner intent via nodenatural logisticregressionclassifier training ui text validate invalidate button prebuilt tool entity intent need chatbot good nodenatural stanfordner compared nlp service headache seeing
"In a NLP AI model, how to create a persistent context per user?","<p>I’m developing a new <a href=""https://talers.io"" rel=""nofollow noreferrer"">word processor application</a>, and I’m interested in the capacities of AI and NLP to guess the end of a sentence a user is currently typing.</p>
<p>I would like the AI model to know about the context, i.e. the previous text that has been written (introducing characters, places, and the tone of the text).</p>
<p>It would work somehow like GitHub Copilot: the copilot is aware of the rest of the code, and can generate appropriate answers.</p>
<p>I imagine two possibilities:</p>
<ol>
<li>fine-tuning a model with the other inputs of the user</li>
<li>pass a part of the current text as context to the chat API before asking to complete an unfinished sentence.</li>
</ol>
<p>Solution <strong>#1</strong> is scalable (you can train your model with arbitrary amounts of text) and will produce very precise results, but there are two drawbacks:</p>
<ul>
<li>fine-tuning is slow (not the most annoying part),</li>
<li>if there are millions of users, it means millions of different fine-tuned models. Can any NLP engine process that as of today?</li>
</ul>
<p>Solution <strong>#2</strong> will work great for small texts, but it's suboptimal, as you will have to resend the context every time you start a new conversation with the AI. And the bigger the context, the worse is this solution.</p>
<p>How to achieve that with AI? GitHub Copilot is kind of achieving that, so I guess it's possible.</p>
",Named Entity Recognition (NER),nlp ai model create persistent context per user developing new word processor application interested capacity ai nlp guess end sentence user currently typing would like ai model know context e previous text ha written introducing character place tone text would work somehow like github copilot copilot aware rest code generate appropriate answer imagine two possibility fine tuning model input user pas part current text context chat api asking complete unfinished sentence solution scalable train model arbitrary amount text produce precise result two drawback fine tuning slow annoying part million user mean million different fine tuned model nlp engine process today solution work great small text suboptimal resend context every time start new conversation ai bigger context worse solution achieve ai github copilot kind achieving guess possible
Fine-tuned MLM based RoBERTa not improving performance,"<p>We have lots of domain-specific data (200M+ data points, each document having ~100 to ~500 words) and we wanted to have a domain-specific LM.</p>
<p>We took some sample data points (2M+) &amp; fine-tuned RoBERTa-base (using HF-Transformer) using the Mask Language Modelling (MLM) task.</p>
<p>So far,</p>
<ol>
<li>we did 4-5 epochs (512 sequence length, batch-size=48)</li>
<li>used cosine learning rate scheduler (2-3 cycles/epochs)</li>
<li>We used dynamin masking (masked 15% tokens)</li>
</ol>
<p>Since the RoBERTa model is finetuned on domain-specific data, we do expect this model to perform better than the pre-trained-RoBERTa which is trained on general texts (wiki data, books, etc)</p>
<p>We did perform some tasks like Named Entity Recognition (NER), Text Classification, and Embedding generation to perform cosine similarity tasks. We did this on both finetuned domain-specific RoBERTa and pre-trained-RoBERTa.</p>
<p>Surprisingly, the results are the same (very small difference) for both models. We did try Spacy models too, but the results are the same.</p>
<p>Perplexity scores indicate that finetuned MLM-based RoBERTa has a minimal loss.</p>
<ol>
<li>Can anyone please help us understand why MLM based model is NOT performing better?</li>
<li>Should we go for more data OR more epochs OR both, to see some effect?</li>
<li>are we doing anything wrong here? Let me know if any required details are missing. I will update</li>
</ol>
<p>any suggestions OR any valuable links addressing these concerns would be really helpful</p>
<p>Huggingface discussion page: <a href=""https://discuss.huggingface.co/t/fine-tuned-mlm-based-roberta-not-improving-performance/36913"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/fine-tuned-mlm-based-roberta-not-improving-performance/36913</a></p>
",Named Entity Recognition (NER),fine tuned mlm based roberta improving performance lot domain specific data data point document word wanted domain specific lm took sample data point fine tuned roberta base using hf transformer using mask language modelling mlm task far epoch sequence length batch size used cosine learning rate scheduler cycle epoch used dynamin masking masked token since roberta model finetuned domain specific data expect model perform better pre trained roberta trained general text wiki data book etc perform task like named entity recognition ner text classification embedding generation perform cosine similarity task finetuned domain specific roberta pre trained roberta surprisingly result small difference model try spacy model result perplexity score indicate finetuned mlm based roberta ha minimal loss anyone please help u understand mlm based model performing better go data epoch see effect anything wrong let know required detail missing update suggestion valuable link addressing concern would really helpful huggingface discussion page
How to set multiple sequences as features in KERAS,"<p>I want to make Named Entity Recognition model with Keras.
These are the links that I have followed:</p>
<p><a href=""https://valueml.com/named-entity-recognition-using-lstm-in-keras/"" rel=""nofollow noreferrer"">https://valueml.com/named-entity-recognition-using-lstm-in-keras/</a>
<a href=""https://djajafer.medium.com/named-entity-recognition-and-classification-with-keras-4db04e22503d"" rel=""nofollow noreferrer"">https://djajafer.medium.com/named-entity-recognition-and-classification-with-keras-4db04e22503d</a></p>
<p>Data looks like this:</p>
<pre><code>                word label
0          Thousands     O
1                 of     O
2      demonstrators     O
3               have     O
4            marched     O
...              ...   ...
44187          there     O
44188   accidentally     O
44189             or     O
44190   deliberately     O
44191              .     O
</code></pre>
<p>They are using word to vectors, so they are indexing the words and labels, so that <code>X</code> are my features (index sequences of words) and <code>y</code> are my results (index sequences of labels):</p>
<pre><code>max_len = 30
X = [[word2idx[w[0]] for w in s] for s in list_of_sentances]
X = pad_sequences(maxlen=max_len, sequences=X, padding=&quot;post&quot;, value=num_words-1)

y = [[label2idx[w[1]] for w in s] for s in list_of_sentances]
y = pad_sequences(maxlen=max_len, sequences=y, padding=&quot;post&quot;, value=label2idx[&quot;O&quot;])
y = [to_categorical(i, num_classes=num_labels) for i in y]
</code></pre>
<p>But what if I have dataset like this:
<a href=""https://i.sstatic.net/pd9wh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pd9wh.png"" alt=""enter image description here"" /></a></p>
<p>here I have another column and that is <code>POS</code>. How can I add values of <code>POS</code> column to my features?
So basically, I do not want only <code>word</code> values in my X, i also want <code>POS</code> values in my X. *(or any other values)
What If I have multiple columns, such as:</p>
<pre><code>word
POS
is_capital_letter
word_length
</code></pre>
<p>...</p>
<p>How can I add all of these columns to my features?</p>
<p>This is my model:
X = np.array(X)
y = np.array(y)</p>
<pre><code>x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)

print(&quot;x_train shape&quot;, x_train.shape)
print(&quot;x_test shape&quot;, x_test.shape)
#x_train shape (750, 75)
#x_test shape (250, 75)


input_word = Input(shape=(max_len,))

model = Embedding(input_dim = vocab_len+1, output_dim = 75, input_length = max_len)(input_word)
model = SpatialDropout1D(0.25)(model)
model = Bidirectional(LSTM(units = 25, return_sequences=True, recurrent_dropout = 0.2))(model)
out = TimeDistributed(Dense(num_labels, activation = &quot;softmax&quot;))(model)

model = Model(input_word, out)
</code></pre>
",Named Entity Recognition (NER),set multiple sequence feature kera want make named entity recognition model kera link followed data look like using word vector indexing word label feature index sequence word result index sequence label dataset like another column add value column feature basically want value x also want value x value multiple column add column feature model x np array x np array
How can I provide a relation extraction data set including tuple for casual inference using name entity recognition by spacy?,"<p>I have around 7.000 sentences, for which I have done a refined Name-Entity-Recognition (i.e., for specific entities) using SpaCy. Now I want to do relationship extraction (basically causal inference) and I do not know how to use NER to provide training set.</p>

<p>As far as I read there are a different approaches to perform relationship extraction:</p>

<ul>
<li>1) Handwritten patterns </li>
<li>2) Supervised machine learning </li>
<li>3) Semi-supervised machine learning</li>
</ul>

<p>Since I want to use supervised machine learning I need training data.</p>

<p>It would be nice if anyone could give me some direction, many thanks. Here is a screen shoot of my data frame, entities are provided by a customised spaCy model. I have access to the syntactic dependencies and part-of-speech tags of each sentence, as given by spaCy:</p>

<p><a href=""https://i.sstatic.net/AcwNz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AcwNz.png"" alt=""enter image description here""></a></p>
",Named Entity Recognition (NER),provide relation extraction data set including tuple casual inference using name entity recognition spacy around sentence done refined name entity recognition e specific entity using spacy want relationship extraction basically causal inference know use ner provide training set far read different approach perform relationship extraction handwritten pattern supervised machine learning semi supervised machine learning since want use supervised machine learning need training data would nice anyone could give direction many thanks screen shoot data frame entity provided customised spacy model access syntactic dependency part speech tag sentence given spacy
Using GPT-3 to identify relationships in a corpus,"<p>I have a corpus of 15K news articles. I would like to train a GPT model (3 or 4) to ingest these texts and then output how the locations, events, actions, participants, and things described in the texts are related to one another. So if the corpus says John Smith took part in a protest, I'd like to tell me this and what other people took part, how the protest was related to specific locations, etc. Is this possible?</p>
<p>If so can someone please point me in the right direction for learning how to do it? When I do searches all I'm finding is links about using GPT models to give extractive or abstractive summaries of individual texts. I suppose that's related but not quite the same.</p>
",Named Entity Recognition (NER),using gpt identify relationship corpus corpus k news article would like train gpt model ingest text output location event action participant thing described text related one another corpus say john smith took part protest like tell people took part protest wa related specific location etc possible someone please point right direction learning search finding link using gpt model give extractive abstractive summary individual text suppose related quite
Creating a custom dataset based on CoNLL2003,"<p>I'm working on a named entity recognition (NER) project and would like to create my own dataset based on the CoNLL2003 dataset (link: <a href=""https://huggingface.co/datasets/conll2003"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/conll2003</a>). I've been looking at the CoNLL2003 data and I'm having trouble understanding how the chunk column is labeled. I'm not sure if it's based on the part-of-speech (POS) tags or on something else.
Ideally, I'd like to automate the process of creating the chunk labels for my custom dataset, rather than doing it manually. Can someone explain how the chunk column is labeled in CoNLL2003 and provide some guidance on how I can programmatically generate the same labels for my own dataset?</p>
<p>To expalin more let’s take the first row of the dataset and try to working on it and i should have the same results.</p>
<p>The first sentence of the dataset which is : <code>EU rejects German call to boycott British lamb.</code></p>
<ul>
<li>To do this on python this is the code :</li>
</ul>
<pre><code># import libraries and modules needed for the project
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk import pos_tag, ne_chunk
import re
# We take the first sentence from the dataset conll2003
Sentence = &quot;EU rejects German call to boycott British lamb.&quot;
</code></pre>
<p>The tokens of the same sentence is : <code>['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']</code></p>
<ul>
<li>To do this on python this is the code :</li>
</ul>
<pre><code>import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk import pos_tag, ne_chunk
import re
# Tokenize the sentence
tokens = word_tokenize(Sentence)
# Print the tokens
print(tokens)
</code></pre>
<p>The part of speech of the same sentence is : <code>['NNP', 'VBZ', 'JJ', 'NN', 'TO', 'VB','JJ', 'NN', '.']</code> which is <code>[22, 42, 16, 21, 35, 37, 16, 21, 7]</code></p>
<ul>
<li>To do this on python this is the code :</li>
</ul>
<pre><code>pos_tags = {'&quot;': 0, &quot;''&quot;: 1, '#': 2, '$': 3, '(': 4, ')': 5, ',': 6, '.': 7, ':': 8, '``': 9, 'CC': 10, 'CD': 11, 'DT': 12, 'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23, 'NNS': 24, 'NN|SYM': 25, 'PDT': 26, 'POS': 27, 'PRP': 28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33, 'SYM': 34, 'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': 42, 'WDT': 43, 'WP': 44, 'WP$': 45, 'WRB': 46}
# POS Tagging of the tokens
pos_tagged = pos_tag(tokens)
# Print the POS Tagged tokens
print(pos_tagged)
# Keep only the POS tags in a list
pos_tags_only = [pos_tags[tag] for word, tag in pos_tagged]
# Print the POS
print(pos_tags_only)
</code></pre>
<p>The chunk tags of the same sentence should be : <code>['B-NP', 'B-VP', 'B-NP', 'I-NP', 'B-VP', 'I-VP','B-NP', 'I-NP', 'O']</code> which is <code>[11, 21, 11, 12, 21, 22, 11, 12, 0]</code> but how they do it i don’t have any idea i alredy test a code but i don’t get exactly the same result as this <code>['B-NP', 'B-VP', 'B-NP', 'I-NP', 'B-VP', 'I-VP','B-NP', 'I-NP', 'O']</code>
please if any one have an idea of how they do it plese guide me</p>
<pre><code># To start with, this is the chunk tag set used in conll2003 dataset
chunk_tags = {'O': 0, 'B-ADJP': 1, 'I-ADJP': 2, 'B-ADVP': 3, 'I-ADVP': 4, 'B-CONJP': 5, 'I-CONJP': 6, 'B-INTJ': 7, 'I-INTJ': 8, 'B-LST': 9, 'I-LST': 10, 'B-NP': 11, 'I-NP': 12, 'B-PP': 13, 'I-PP': 14, 'B-PRT': 15, 'I-PRT': 16, 'B-SBAR': 17, 'I-SBAR': 18, 'B-UCP': 19, 'I-UCP': 20, 'B-VP': 21, 'I-VP': 22}  
</code></pre>
",Named Entity Recognition (NER),creating custom dataset based conll working named entity recognition ner project would like create dataset based conll dataset link looking conll data trouble understanding chunk column labeled sure based part speech po tag something else ideally like automate process creating chunk label custom dataset rather manually someone explain chunk column labeled conll provide guidance programmatically generate label dataset expalin let take first row dataset try working result first sentence dataset python code token sentence python code part speech sentence python code chunk tag sentence idea alredy test code get exactly result please one idea plese guide
Getting the input text from transformers pipeline,"<p>I am following the tutorial on <a href=""https://huggingface.co/docs/transformers/pipeline_tutorial"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/pipeline_tutorial</a> to use transformers pipeline for inference. For example, the following code snippet works for getting the NER results from ner pipeline.</p>
<pre><code>    # KeyDataset is a util that will just output the item we're interested in.
    from transformers.pipelines.pt_utils import KeyDataset
    from datasets import load_dataset
    model = ...
    tokenizer = ...
    pipe = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)
    dataset = load_dataset(&quot;my_ner_dataset&quot;, split=&quot;test&quot;)
    
    for extracted_entities in pipe(KeyDataset(dataset, &quot;text&quot;)):
        print(extracted_entities)
</code></pre>
<p>In NER, as well as many applications, we would like to also get the input so that I can store the result as (text, extracted_entities) pair for later processing. Basically I am looking for something like:</p>
<pre><code>    # KeyDataset is a util that will just output the item we're interested in.
    from transformers.pipelines.pt_utils import KeyDataset
    from datasets import load_dataset
    model = ...
    tokenizer = ...
    pipe = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)
    dataset = load_dataset(&quot;my_ner_dataset&quot;, split=&quot;test&quot;)
    
    for text, extracted_entities in pipe(KeyDataset(dataset, &quot;text&quot;)):
        print(text, extracted_entities)
</code></pre>
<p>Where <code>text</code> is the raw input text (possibly batched) that get fed into the pipeline.</p>
<p>Is this doable ?</p>
",Named Entity Recognition (NER),getting input text transformer pipeline following tutorial use transformer pipeline inference example following code snippet work getting ner result ner pipeline ner well many application would like also get input store result text extracted entity pair later processing basically looking something like raw input text possibly batched get fed pipeline doable
How to evaluate Named Entity Recognition for an ASR output?,"<p>I am trying to evaluate the output of an Automated Speech Recognition system for the recognition of named entities.</p>
<p>I currently have access to an API that detects named entities. The aim is not to evaluate this API, but to evaluate the ASR output. So we can assume that this system detects properly NE.</p>
<p>So for an ASR output, I also have a reference (Gold standard) text.
I would like to calculate the precision and recall of the ASR system when it comes to NE.</p>
<p>So far my approach is:</p>
<ul>
<li>Send the reference text to the API and get a list of named entities for the whole text.</li>
<li>Send the ASR output text to the API and get a list of named entities in the output.</li>
<li>Compare both lists and find: True Positives (entities in the reference list of named entities that are also in the list of output entities), False Negatives (if there is an entity detected in the reference list that is not in the output list) and False Positives (there is an entity detected in the output list that is not in the reference list)</li>
<li>Calculate precision and recall</li>
</ul>
<p>The API also returns the position of the entities. I thought about using it to check for the &quot;matching&quot; entities in the output text but in case an entity is missed or the spelling is different, it cannot be used.</p>
<p>I do not perform any normalization as we want to take into account capitalization, and we decided it needs a perfect match.</p>
<p>Is this approach correct or are there other aspects to take into account?
Thank you</p>
",Named Entity Recognition (NER),evaluate named entity recognition asr output trying evaluate output automated speech recognition system recognition named entity currently access api detects named entity aim evaluate api evaluate asr output assume system detects properly ne asr output also reference gold standard text would like calculate precision recall asr system come ne far approach send reference text api get list named entity whole text send asr output text api get list named entity output compare list find true positive entity reference list named entity also list output entity false negative entity detected reference list output list false positive entity detected output list reference list calculate precision recall api also return position entity thought using check matching entity output text case entity missed spelling different used perform normalization want take account capitalization decided need perfect match approach correct aspect take account thank
NLP how can i match multiple token words from a larger text,"<p>I am currently having trouble with the following. I receive a job offer and I have to extract certain words from my CSV file. These words that I am trying to extract can be multiple tokens long (up to 4 tokens long)  However, I have to keep in mind that there can be instances of misspellings and use of abbreviations. So a direct matching algorithm wouldn't give me a good result. What can I do to check whether the words in my CSV file are mentioned in the text? Keep in mind, I do not have a large dataset.</p>
<p>My original plan was to do a similarity match between the words in my CSV file and the whole text. To solve misspellings and abbreviations, I added a column with possible variations/abbreviations and also did a similarity match on those. If a similarity score would be above a certain threshold, and is the highest match, then it would be a 'match'. To do multiple-word matching, I added n-grams when doing the similarity match. However, I got a lot of false positives. Even setting a higher threshold did not solve my issue.</p>
<p>I also tried building a custom NER model. This worked decently. I even used my NER model to extract potentially relevant words and then did a similarity match to get good results. However, my solution needs to be easily expandable. Adding new words to the CSV file is easy, but retraining the NER model each time isn't ideal.</p>
",Named Entity Recognition (NER),nlp match multiple token word larger text currently trouble following receive job offer extract certain word csv file word trying extract multiple token long token long however keep mind instance misspelling use abbreviation direct matching algorithm give good result check whether word csv file mentioned text keep mind large dataset original plan wa similarity match word csv file whole text solve misspelling abbreviation added column possible variation abbreviation also similarity match similarity score would certain threshold highest match would match multiple word matching added n gram similarity match however got lot false positive even setting higher threshold solve issue also tried building custom ner model worked decently even used ner model extract potentially relevant word similarity match get good result however solution need easily expandable adding new word csv file easy retraining ner model time ideal
How to fine tune a Huggingface Seq2Seq model with a dataset from the hub?,"<p>I want to train the <code>&quot;flax-community/t5-large-wikisplit&quot;</code> model with the <code>&quot;dxiao/requirements-ner-id&quot;</code> dataset. (Just for some experiments)</p>
<p>I think my general procedure is not correct, but I don't know how to go further.</p>
<p>My Code:</p>
<p>Load tokenizer and model:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel
checkpoint = &quot;flax-community/t5-large-wikisplit&quot;
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).cuda()
</code></pre>
<p>Load dataset that I want to train:</p>
<pre><code>from datasets import load_dataset
raw_dataset = load_dataset(&quot;dxiao/requirements-ner-id&quot;)
</code></pre>
<p>The raw_dataset looks like this ['id', 'tokens', 'tags', 'ner_tags']</p>
<p>I want to get the sentences as sentence and not as tokens.</p>
<pre><code>def tokenToString(tokenarray):
  string = tokenarray[0]
  for x in tokenarray[1:]:
    string += &quot; &quot; + x
  return string

def sentence_function(example):
  return {&quot;sentence&quot; :  tokenToString(example[&quot;tokens&quot;]),
          &quot;simplefiedSentence&quot; : tokenToString(example[&quot;tokens&quot;]).replace(&quot;The&quot;, &quot;XXXXXXXXXXX&quot;)}

wikisplit_req_set = raw_dataset.map(sentence_function)
wikisplit_req_set
</code></pre>
<p>I tried to restructure the dataset such that it looks like the wikisplit dataset:</p>
<pre><code>simple1dataset = wikisplit_req_set.remove_columns(['id', 'tags', 'ner_tags', 'tokens']);
complexdataset = wikisplit_req_set.remove_columns(['id', 'tags', 'ner_tags', 'tokens']);
complexdataset[&quot;train&quot;] = complexdataset[&quot;train&quot;].add_column(&quot;simple_sentence_1&quot;,simple1dataset[&quot;train&quot;][&quot;sentence&quot;]).add_column(&quot;simple_sentence_2&quot;,simple1dataset[&quot;train&quot;][&quot;simplefiedSentence&quot;])
complexdataset[&quot;test&quot;] = complexdataset[&quot;test&quot;].add_column(&quot;simple_sentence_1&quot;,simple1dataset[&quot;test&quot;][&quot;sentence&quot;]).add_column(&quot;simple_sentence_2&quot;,simple1dataset[&quot;test&quot;][&quot;simplefiedSentence&quot;])
complexdataset[&quot;validation&quot;] = complexdataset[&quot;validation&quot;].add_column(&quot;simple_sentence_1&quot;,simple1dataset[&quot;validation&quot;][&quot;sentence&quot;]).add_column(&quot;simple_sentence_2&quot;,simple1dataset[&quot;validation&quot;][&quot;simplefiedSentence&quot;])
trainingDataSet = complexdataset.rename_column(&quot;sentence&quot;, &quot;complex_sentence&quot;)
trainingDataSet
</code></pre>
<p>Tokenize it:</p>
<pre><code>def tokenize_function(example):
    model_inputs = tokenizer(example[&quot;complex_sentence&quot;],truncation=True, padding=True)
    targetS1 = tokenizer(example[&quot;simple_sentence_1&quot;],truncation=True, padding=True)
    targetS2 = tokenizer(example[&quot;simple_sentence_2&quot;],truncation=True, padding=True)
    model_inputs['simple_sentence_1'] = targetS1['input_ids']
    model_inputs['simple_sentence_2'] = targetS2['input_ids']
    model_inputs['decoder_input_ids'] = targetS2['input_ids']
    return model_inputs

tokenized_datasets = trainingDataSet.map(tokenize_function, batched=True)
tokenized_datasets=tokenized_datasets.remove_columns(&quot;complex_sentence&quot;)
tokenized_datasets=tokenized_datasets.remove_columns(&quot;simple_sentence_1&quot;)
tokenized_datasets=tokenized_datasets.remove_columns(&quot;simple_sentence_2&quot;)
tokenized_datasets=tokenized_datasets.remove_columns(&quot;simplefiedSentence&quot;)
tokenized_datasets
</code></pre>
<p>DataLoader:</p>
<pre><code>from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
data_collator
</code></pre>
<p>Training:</p>
<pre><code>from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, TrainingArguments, EvalPrediction, DataCollatorWithPadding, Trainer

bleu = evaluate.load(&quot;bleu&quot;)

training_args = Seq2SeqTrainingArguments(
  output_dir = &quot;/&quot;,
  log_level = &quot;error&quot;,
  num_train_epochs = 0.25,
  learning_rate = 5e-4,
  lr_scheduler_type = &quot;linear&quot;,
  warmup_steps = 50,
  optim = &quot;adafactor&quot;,
  weight_decay = 0.01,
  per_device_train_batch_size = 1,
  per_device_eval_batch_size = 1,
  gradient_accumulation_steps = 16,
  evaluation_strategy = &quot;steps&quot;,
  eval_steps = 50,
  predict_with_generate=True,
  generation_max_length = 128,
  save_steps = 500,
  logging_steps = 10,
  push_to_hub = False,
  auto_find_batch_size=True
)

trainer = Seq2SeqTrainer(
    model,
    training_args,
    train_dataset=tokenized_datasets[&quot;train&quot;],
    eval_dataset=tokenized_datasets[&quot;validation&quot;],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=bleu,

)
trainer.train()
</code></pre>
<p>The Problem is, that I do not understand how the model know the expected value and how it calculate its loss. Can someone give me some ideas what happens where?</p>
<p>I hope some one can help me understand my own code, because the documentation by Hugging Face does not help me enough. Maybe someone have some Codeexamples or something else. I do not completely understand how I fine tune the model and how I get the parameters the model expects to train it. I also do not understand how the training works and what the parameters do.</p>
",Named Entity Recognition (NER),fine tune huggingface seq seq model dataset hub want train model dataset experiment think general procedure correct know go code load tokenizer model load dataset want train raw dataset look like id token tag ner tag want get sentence sentence token tried restructure dataset look like wikisplit dataset tokenize dataloader training problem understand model know expected value calculate loss someone give idea happens hope one help understand code documentation hugging face doe help enough maybe someone codeexamples something else completely understand fine tune model get parameter model expects train also understand training work parameter
"Segment pdf, doc, docx documents into paragraph level texts","<p>I want to parse several long scientific documents for a <strong>NER</strong> task. The documents come as pdf, doc and docx and, unfortunately, do not share the same structure. </p>

<p>I seek to extract the raw text from the documents in Python, while creating sub-documents for tables and text.</p>

<p>I have functions that reliably extract the text from the documents and write them, together with the document labels, in a list. </p>

<pre class=""lang-py prettyprint-override""><code>import re, os
import textract, PyPDF2
​
import pandas as pd
import numpy as np
def get_doc_content(filename):
  text = textract.process(filename).decode(""utf-8"")
  return re.sub(""[\n|]"", "" "", text) 

def get_pdf_content(filename):
  pdf_file = open(filename, mode='rb')
  pdf_doc = PyPDF2.PdfFileReader(pdf_file)
  current_page = 0
  text = ''
  while(current_page &lt; pdf_doc.numPages ):  
    page = pdf_doc.getPage(current_page)
    text += re.sub(""[\n]"","""", page.extractText()) 
    current_page += 1
  return text

input_dir = '/my_path.../'
file_names = []
content = []
for fname in os.listdir(input_dir):
  fpath = os.path.join(input_dir, fname)
  if fname.endswith("".doc"") or fname.endswith("".docx""):
    file_names.append(fname)
    content.append(get_doc_content(fpath))    
  elif fname.endswith("".pdf""):
    file_names.append(fname)
    content.append(get_pdf_content(fpath))    
  else:
    print ('file type not supported: %s' % (fname))
    continue

df= pd.DataFrame({'text':content, 'doc_label':file_names})
</code></pre>

<p>This gives me following output -  looking at the first entry:
<strong>Output1:</strong></p>

<pre class=""lang-py prettyprint-override""><code>df.head(1)
&gt;&gt;&gt; text                doc_label
    extracted raw text...   doc_1


</code></pre>

<p>The parsed file, however, contains several tables and long raw text, so I seek to have an output similar to this (for the first example file). 
<strong>Output2:</strong></p>

<pre class=""lang-py prettyprint-override""><code>df
&gt;&gt;&gt; text              par_label        doc_label
    extracted table 1...  table_1          doc_1
    extracted table 2...  table_2          doc_1
    extracted text 1...   text_1           doc_1       
    extracted table 3...  table_3          doc_1
    extracted text 2...   text_2           doc_1
    .
    .
    .
</code></pre>

<p><strong>My question:</strong> How can I adjust my code (are there Python workarounds) to detect tables/text chunks in those documents and obtain <strong>output2</strong> instead of <strong>output1</strong>.</p>

<p>I do not need to have a perfect automatism for this, but some way to segment those documents would be very helpful.</p>
",Named Entity Recognition (NER),segment pdf doc docx document paragraph level text want parse several long scientific document ner task document come pdf doc docx unfortunately share structure seek extract raw text document python creating sub document table text function reliably extract text document write together document label list give following output looking first entry output parsed file however contains several table long raw text seek output similar first example file output question adjust code python workarounds detect table text chunk document obtain output instead output need perfect automatism way segment document would helpful
Update built-in NER model of Spacy instead of overwrite,"<p>I am using an inbuilt model of Spacy that is <code>en_core_web_lg</code> and want to train it using my custom entities. While doing that, I am facing two issues,</p>
<ol>
<li><p>It overwrite the new trained data with the old one and results in not recognizing the other entities. for example,
Before training, it can recognize the PERSON and ORG but after training it doesn't recognize the PERSON and ORG.</p>
</li>
<li><p>During the training process, it is giving me the following error,</p>
</li>
</ol>
<pre><code>UserWarning: [W030] Some entities could not be aligned in the text &quot;('I work in Google.',)&quot; with entities &quot;[(9, 15, 'ORG')]&quot;. Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.
</code></pre>
<p>Here is my whole code,</p>
<pre><code>import spacy
import random
from spacy.util import minibatch, compounding
from pathlib import Path
from spacy.training.example import Example
sentence = &quot;&quot;
body1 = &quot;James work in Facebook and love to have tuna fishes in the breafast.&quot;
nlp_lg = spacy.load(&quot;en_core_web_lg&quot;)
print(nlp_lg.pipe_names)
doc = nlp_lg(body1)
for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)


train = [
    ('I had tuna fish in breakfast', {'entities': [(6,14,'FOOD')]}),
    ('I love prawns the most', {'entities': [(6,12,'FOOD')]}),
    ('fish is the rich source of protein', {'entities': [(0,4,'FOOD')]}),
    ('I work in Google.', {'entities': [(9,15,'ORG')]})
    ]


ner = nlp_lg.get_pipe(&quot;ner&quot;)

for _, annotations in train:
    for ent in annotations.get(&quot;entities&quot;):
        ner.add_label(ent[2])

disable_pipes = [pipe for pipe in nlp_lg.pipe_names if pipe != 'ner']

with nlp_lg.disable_pipes(*disable_pipes):
    optimizer = nlp_lg.resume_training()
    for interation in range(30):
        random.shuffle(train)
        losses = {}

        batches = minibatch(train, size=compounding(1.0,4.0,1.001))
        for batch in batches:
            text, annotation = zip(*batch)
            doc1 = nlp_lg.make_doc(str(text))
            example = Example.from_dict(doc1, annotations)
            nlp_lg.update(
                [example],
                drop = 0.5,
                losses = losses,
                sgd = optimizer
                )
            print(&quot;Losses&quot;,losses)

doc = nlp_lg(body1)
for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)

</code></pre>
<p>Expected Output :</p>
<pre><code>James 0 5 PERSON
Facebook 14 22 ORG
tuna fishes 40 51 FOOD

</code></pre>
<p>Currently recognizing no entities..</p>
<p>Please let me know where I am doing it wrong. Thanks!</p>
",Named Entity Recognition (NER),update built ner model spacy instead overwrite using inbuilt model spacy want train using custom entity facing two issue overwrite new trained data old one result recognizing entity example training recognize person org training recognize person org training process giving following error whole code expected output currently recognizing entity please let know wrong thanks
How to extract name from string using nltk,"<p>I am trying to extract name(Indian) from unstructured string.</p>

<p>Here come my code:</p>

<pre><code>text = ""Balaji Chandrasekaran Bangalore |  Senior Business Analyst/ Lead Business Analyst An accomplished Senior Business Analyst with a track record of handling complex projects in given period of time, exceeding above the expectation. Successful at developing product road maps and leading cross-functional software teams from prototype to release. Professional Competencies Systems Development Life Cycle (SDLC) Agile methodologies Business process improvement Requirements gathering &amp; Analysis Project Management UML Specification UI &amp; UX (Wireframe Designing) Functional Specification Test Scenario Creation SharePoint Admin Work History Senior Business Analyst (Aug 2012 Current) YouBox Technology pvt ltd, Chennai Translating business goals, feature concepts and customer needs into prioritized product requirements and use cases. Expertized in designing innovative wireframes combining user experience analysis and technology models. Extensive Experience in implementing soft wares for Shipping/Logistics firms to handle CRM, Finance, Logistics, Operations, Intermodal, and documentation. Strong interpersonal skills, highly adept at diplomatically facilitating discussions and negotiations with stakeholders. Education Bachelor of Engineering: Electronics &amp; Communication, 2011 CES Tech Hosur Accomplishment Successful onsite implementation at various locations around the globe for Europe Shipping Company. - (Pre Study, General Design, and Functional Specification) Organized Business Analyst Forum and conducted various activities to develop skill sets of Business Analysts.""
if text != """":
    grammar = """"""PERSON: {&lt;NNP&gt;}""""""
    chunkParser = nltk.RegexpParser(grammar)
    tagged = nltk.pos_tag(nltk.word_tokenize(text))
    tree = chunkParser.parse(tagged)

    for subtree in tree.subtrees():
        if subtree.label() == ""PERSON"": 
            pronouns.append(' '.join([c[0] for c in subtree]))

    print(pronouns)
</code></pre>

<blockquote>
  <p>['Balaji', 'Chandrasekaran', 'Bangalore', '|','Senior', 'Business',
  'Analys', '/', 'Lead', 'Business', 'Analyst', 'Senior', 'Business',
  'Analyst', 'Successful', 'Development', 'Life', 'Cycle', 'SDLC',
  'Agile', 'Business', 'Requirements', 'Analysis', 'Project',
  'Management', 'UML', 'Specification', 'UI', 'UX', 'Wireframe',
  'Designing', 'Functional', 'Specification', 'Test', 'Scenario',
  'Creation', 'SharePoint', 'Admin', 'Work', 'History', 'Senior',
  'Business', 'Analyst', 'Aug', 'Current', 'Technology', 'Chennai',
  'Translating', 'CRM', 'Finance', 'Logistics', 'Operations',
  'Intermodal', 'Education', 'Bachelor', 'Engineering', 'Electronics',
  'Communication', 'Accomplishment', 'Successful', 'Mediterranean',
  'Ship', 'Company', 'MSC', 'Georgia', 'MSC', 'Cambodia', 'MSC',  'MSC',
  'South', 'Successful', 'Stake', 'MSC', 'Geneva', 'Switzerland', 'Pre',
  'Study', 'General', 'Design', 'Functional', 'Specification', 'O',
  'Business', 'Analyst', 'Forum', 'Business']</p>
</blockquote>

<p>But actually i need to get only <strong>Balaji Chandrasekaran</strong> , I even try to use Standford ner lib.Which fails to pick <strong>Balaji Chandrasekaran</strong></p>

<p>Can any one help to extract name from the un strcuture string, or suggest me any good tutorial to do that.</p>

<p>Thank you in advance.</p>
",Named Entity Recognition (NER),extract name string using nltk trying extract name indian unstructured string come code balaji chandrasekaran bangalore senior business analys lead business analyst senior business analyst successful development life cycle sdlc agile business requirement analysis project management uml specification ui wireframe designing functional specification test scenario creation sharepoint admin work history senior business analyst aug current technology chennai translating finance logistics operation intermodal education bachelor engineering electronics communication accomplishment successful mediterranean ship company msc georgia msc cambodia msc msc south successful stake msc geneva switzerland pre study general design functional specification business analyst forum business actually need get balaji chandrasekaran even try use standford ner lib fails pick balaji chandrasekaran one help extract name un strcuture string suggest good tutorial thank advance
Named Entity Recognition by dictionary in text,"<p>I need to extract keywords from text. I have a dictionary of keywords, let's say</p>
<pre><code>apache-spark
java
pathon
amazon-web-services
apache-kafka
</code></pre>
<p>and I have a job post for example:</p>
<pre><code>Design, develop and maintain ETL processing pipelines for data ingestion and sharing
Contribute to system architecture design discussions and improvements
Communicating with different teams regarding data quality, consistency and availability

Our technology stack:

GCP (BigQuery, GCS, PubSub, DataProc etc)
Spark, Kafka, Kudu
Airflow, dbt
Tableau
4+ years experience as a Data Engineer
Proven track record of working with SQL, Python, Airflow and Docker
Experience in large scale data processing (Apache Spark or similar) and Scala/Java is a big plus
Strong expertise in cloud-based data warehouses like Google BigQuery
Fluency in English verbal and written skills.
</code></pre>
<p>In the text we have <code>Apache Spark</code> keyword. My dictionary contains slightly different keyword  - <code>apache-spark</code>. The same is with <code>Kafka</code> - in my dictionary I have <code>apache-kafka</code>.</p>
<p>Will it be possible to extract such keywords from text with Stanford NER? Is it a task for Stanford NER or I'm on the wrong way?</p>
",Named Entity Recognition (NER),named entity recognition dictionary text need extract keywords text dictionary keywords let say job post example text keyword dictionary contains slightly different keyword dictionary possible extract keywords text stanford ner task stanford ner wrong way
How to extract date and temporal expressions from German text in Python?,"<p>I want to extract dates and other temporal expressions from unstructured written texts in German language, including formats such as</p>
<ul>
<li>'Es ist der 1. Januar 2020.'</li>
<li>'Gestern hat es geschneit.'</li>
<li>'Heute Abend ab 18 Uhr eine Stunde'</li>
<li>'2023-02-28'</li>
<li>'4 Tage ab Montag'</li>
<li>'Bis 20.03.2023'</li>
<li>'Von Donnerstag den 18. bis zum 21. Mai'</li>
<li>'Samstag 18.03.'</li>
<li>'2023-03-15.'
Other temporal expressions that I want to extract include</li>
<li>'nächste Woche'</li>
<li>'im letzten Monat'</li>
<li>'in 2 Stunden'</li>
<li>and more.</li>
</ul>
<p>I followed <a href=""https://www.qualicen.de/natural-language-processing-timeline-extraction-with-regexes-and-spacy/"" rel=""nofollow noreferrer"">this guide</a> for named entity recognition and applied the test case on the following python packages:</p>
<ul>
<li>spaCy models (de_core_news_md, de_core_news_lg, de_dep_news_trf, en_core_web_lg)</li>
<li><a href=""https://github.com/explosion/spacy-stanza"" rel=""nofollow noreferrer"">spaCy_stanza</a> (<code>nlp = spacy_stanza.load_pipeline(&quot;de&quot;, package=&quot;hdt&quot;)</code>)</li>
<li>datefinder</li>
<li>htmldate, after reading this <a href=""https://adrien.barbaresi.eu/blog/evaluation-date-extraction-python.html"" rel=""nofollow noreferrer"">persuading article</a></li>
<li>dateutil parser (and other parsers)</li>
</ul>
<p>For comparison I translated <a href=""https://raw.githubusercontent.com/qualicen/timeline/master/history_of_germany.txt"" rel=""nofollow noreferrer"">the English text</a> in the mentioned guide to German, using google translate. Then I executed the different tools on the text, as well on the given text snippets.</p>
<p>The English spaCy model en_core_web_lg is able to find 596 dates and temporal expressions, including both specific dates like 'January 1st, 2020' and more general expressions like 'the next decade' and 'a Christmas holiday'. It can also recognize time periods, such as 'the summer of 1989' and 'the early 1950s', as well as relative expressions like 'four weeks later', 'six months', and 'two days later'.</p>
<p>However, I was not able to get the German spaCy models to recognize a single date or temporal expression in the translated text or the text snippets given above. I saved the translation in a local file, and it is easily readable, containing string snippets like &quot;im Jahr 1907&quot; and &quot;Zwischen 1994 und 1998.&quot; Just to check, I printed an excerpt to the terminal and checked the type, being &lt;class 'str'&gt;. Interestingly, the English spaCy model was able to recognize 333 dates in the German text! The other packages, spacy_stanza, datefinder, and htmldate, were also not able to recognize any dates on the German text.</p>
<p>This is the basic setup I am using:</p>
<pre><code>nlp = spacy.load(&quot;de_core_news_lg&quot;)
doc = nlp(text)
for ent in filter(lambda e: e.label_=='DATE',doc.ents):
    print(ent.text)
</code></pre>
<p>The labels assigned to the requested dates above:</p>
<pre><code>1. ADJA adjective, attributive
Januar NN noun, singular or mass
2020. NE proper noun
Gestern ADV adverb
21. ADJA adjective, attributive
Mai NN noun, singular or mass
heute ADV adverb
</code></pre>
<p>After consulting the <a href=""https://spacy.io/models/de"" rel=""nofollow noreferrer"">spaCy documentation</a>, I noticed that the German word list de_core_news_lg does not include any NER-label for dates or other temporal expressions. To confirm this, I retrieved the list of available labels using the following code:</p>
<pre><code>for label in label_list:
  print(label, spacy.explain(label))
</code></pre>
<p>(Unfortunately, the <code>PDAT</code> label turned out to be an <code>attributive demonstrative pronoun</code>.) In comparison to the English word list, the only available NER-related labels in the German word list are <code>LOC</code>, <code>MISC</code>, <code>ORG</code>, and <code>PER</code>. Therefore, none of the four German spaCy word lists are suitable for recognizing dates or temporal expressions.</p>
<p>In similar threads on stackoverflow it was suggested using regex. Regex can be an effective approach, although it is limited to the covered date formats. Nevertheless I read that NLTK offers a regex-approach, which I could include in the final solution (together with some NER model training). For now I'm looking for a solution in Python, that reaches comparable efficiency to the English model on english text.</p>
<p>To sum up, the applied NER methods cannot recognize dates since they lack a specific NER label for them. However, I found that the en_core_web_lg model for English date recognition is able to detect dates in an English text and about half as many in a German text.</p>
<p>Currently, the best solution I can think of is to translate the text to English and use the English model for date recognition. However, this would be a workaround.</p>
",Named Entity Recognition (NER),extract date temporal expression german text python want extract date temporal expression unstructured written text german language including format e ist der januar gestern hat e geschneit heute abend ab uhr eine stunde tage ab montag bi von donnerstag den bi zum mai samstag temporal expression want extract include n chste woche im letzten monat stunden followed guide named entity recognition applied test case following python package spacy model de core news md de core news lg de dep news trf en core web lg spacy stanza datefinder htmldate reading persuading article dateutil parser parser comparison translated english text mentioned guide german using google translate executed different tool text well given text snippet english spacy model en core web lg able find date temporal expression including specific date like january st general expression like next decade christmas also recognize time period summer early well relative expression like four week later six month two day later however wa able get german spacy model recognize single date temporal expression translated text text snippet given saved translation local file easily readable containing string snippet like im jahr zwischen und check printed excerpt terminal checked type class str interestingly english spacy model wa able recognize date german text package spacy stanza datefinder htmldate also able recognize date german text basic setup using label assigned requested date consulting spacy documentation noticed german word list de core news lg doe include ner label date temporal expression confirm retrieved list available label using following code unfortunately label turned comparison english word list available ner related label german word list therefore none four german spacy word list suitable recognizing date temporal expression similar thread stackoverflow wa suggested using regex regex effective approach although limited covered date format nevertheless read nltk offer regex approach could include final solution together ner model training looking solution python reach comparable efficiency english model english text sum applied ner method recognize date since lack specific ner label however found en core web lg model english date recognition able detect date english text half many german text currently best solution think translate text english use english model date recognition however would workaround
mask entities with Spacy NER?,"<p>I am working on some topic modeling and my data is heavy on locations. I want to mask them so the model doesn't see them as unique words.</p>
<p>I can find them with Spacy using NER, and this is how I'm currently doing the masking:</p>
<pre><code>output_text = []
i = 0

for e in doc.ents: # doc is the spacy doc
    output_text.append(doc[i:e.start].text)
    output_text.append(&quot;MASK&quot;)
    i = e.end

output_text.append(doc[i:].text)

final_text = ' '.join(output_text)
</code></pre>
<p>Is there a better (faster) way to accomplish the same thing?</p>
",Named Entity Recognition (NER),mask entity spacy ner working topic modeling data heavy location want mask model see unique word find spacy using ner currently masking better faster way accomplish thing
spacy can&#39;t find GPU when training model,"<p>I'm trying to train a basic NER model on a Paperspace P4000 server, Spacy 3.4.1 and python 3.9.16. I used the Quickstart template to created a base_config.cfg etc. Everything seems to be formatted correctly because it works fine when I set the config to use the CPU.</p>
<p>When I set it to use the GPU, it uses the CPU regardless.</p>
<p><code>python -m spacy init fill-config ./spacy_model_gpu/base_config_gpu.cfg ./spacy_model_gpu/config.cfg</code></p>
<p><code>python -m spacy train ./spacy_model_gpu/config.cfg --output ./output --paths.train ./spacy_model_gpu/train.spacy --paths.dev ./spacy_model_cpu/train.spacy</code></p>
<p>If I add <code>--gpu-id 0</code> to the spacy train command as suggested <a href=""https://github.com/explosion/spaCy/discussions/7121"" rel=""nofollow noreferrer"">here</a>, it throws this error:</p>
<pre><code>    return _run_code(code, main_globals, None,
  File &quot;/usr/lib/python3.9/runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
  File &quot;/usr/local/lib/python3.9/dist-packages/spacy/__main__.py&quot;, line 4, in &lt;module&gt;
    setup_cli()
  File &quot;/usr/local/lib/python3.9/dist-packages/spacy/cli/_util.py&quot;, line 74, in setup_cli
    command(prog_name=COMMAND)
  File &quot;/usr/local/lib/python3.9/dist-packages/click/core.py&quot;, line 1130, in __call__
    return self.main(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/dist-packages/click/core.py&quot;, line 1055, in main
    rv = self.invoke(ctx)
  File &quot;/usr/local/lib/python3.9/dist-packages/click/core.py&quot;, line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File &quot;/usr/local/lib/python3.9/dist-packages/click/core.py&quot;, line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File &quot;/usr/local/lib/python3.9/dist-packages/click/core.py&quot;, line 760, in invoke
    return __callback(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/dist-packages/typer/main.py&quot;, line 532, in wrapper
    return callback(**use_params)  # type: ignore
  File &quot;/usr/local/lib/python3.9/dist-packages/spacy/cli/train.py&quot;, line 45, in train_cli
    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)
  File &quot;/usr/local/lib/python3.9/dist-packages/spacy/cli/train.py&quot;, line 67, in train
    setup_gpu(use_gpu)
  File &quot;/usr/local/lib/python3.9/dist-packages/spacy/cli/_util.py&quot;, line 581, in setup_gpu
    require_gpu(use_gpu)
  File &quot;/usr/local/lib/python3.9/dist-packages/thinc/util.py&quot;, line 195, in require_gpu
    raise ValueError(&quot;No GPU devices detected&quot;)
ValueError: No GPU devices detected
</code></pre>
<p><code>nvidia-smi</code> gives me this:</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro P4000        Off  | 00000000:00:05.0 Off |                  N/A |
| 46%   37C    P8     6W / 105W |      2MiB /  8192MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<p>and here's some further due diligence for GPU existence:</p>
<pre><code>&gt;&gt;&gt; torch.cuda.is_available()
True
&gt;&gt;&gt; torch.cuda.device(0)
&lt;torch.cuda.device object at 0x7f4a4d97a880&gt;
&gt;&gt;&gt; torch.cuda.current_device()
0
&gt;&gt;&gt; torch.cuda.device_count()
1
&gt;&gt;&gt; torch.cuda.get_device_name(0)
'Quadro P4000'
</code></pre>
",Named Entity Recognition (NER),spacy find gpu training model trying train basic ner model paperspace p server spacy python used quickstart template created base config cfg etc everything seems formatted correctly work fine set config use cpu set use gpu us cpu regardless add spacy train command suggested throw error give due diligence gpu existence
Extracting names from a text file using Spacy,"<p>I have a text file which contains lines as shown below:</p>
<pre><code>Electronically signed : Wes Scott, M.D.; Jun 26 2010 11:10AM CST

The patient was referred by Dr. Jacob Austin.  

Electronically signed by Robert Clowson, M.D.; Janury 15 2015 11:13AM CST

Electronically signed by Dr. John Douglas, M.D.; Jun 16 2017 11:13AM CST

The patient was referred by
Dr. Jayden Green Olivia.  
</code></pre>
<p>I want to extract all names using Spacy. I am using Spacy's part of speech tagging and entity recognition but not able to get success.
May I please know on how it could done? Any help would be appreciable</p>
<p>I am using some code in this way:</p>
<pre><code>import spacy
nlp = spacy.load('en')
document_string= &quot;&quot;&quot; Electronically signed by stupid: Dr. John Douglas, M.D.; 
    Jun 13 2018 11:13AM CST&quot;&quot;&quot;
doc = nlp(document_string)
for sentence in doc.ents:
    print(sentence, sentence.label_) 
</code></pre>
",Named Entity Recognition (NER),extracting name text file using spacy text file contains line shown want extract name using spacy using spacy part speech tagging entity recognition able get success may please know could done help would appreciable using code way
extract multiple values from a free text column in a cvs file,"<p>I have a CVS file with a column that consists of a series of medical tests, in free text, with the date, the test names and results, like it follows.
I need to extract the values of each of those medical tests and turn them into columns.
I would like to know if there is a way of doing it without having to train a model of NLP to extract those information.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Patient</th>
<th>Results</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>&quot;01/03/2022 - HMG - Plaques: 65000 01/03/2022 - HMG - Haemoglobin: 7.8 01/05/2022 - Urea: 50.0 01/05/2022 - HMG - Plaques: 80000&quot;</td>
</tr>
<tr>
<td>2</td>
<td>&quot;06/01/2022 - ALT/TGP: 25.0 06/01/2022 - AST/TGO: 40.0 06/03/2022 - Bilirrubin: 0.8&quot;</td>
</tr>
<tr>
<td>3</td>
<td>&quot;01/03/2022 - HMG - Haematocrit: 40 01/03/2022 - HMG - Haemoglobin: 10.2&quot;</td>
</tr>
</tbody>
</table>
</div>
<p>CSV:</p>
<pre><code>Patient;Results 
1;&quot;01/03/2022 - HMG - Plaques: 65000 01/03/2022 - HMG - Haemoglobin: 7.8 01/05/2022 - Urea: 50.0 01/05/2022 - HMG - Plaques: 80000&quot; 
2;&quot;06/01/2022 - ALT/TGP: 25.0 06/01/2022 - AST/TGO: 40.0 06/03/2022 - Bilirrubin: 0.8&quot; 
3;&quot;01/03/2022 - HMG - Haematocrit: 40 01/03/2022 - HMG - Haemoglobin: 10.2&quot; 
</code></pre>
",Named Entity Recognition (NER),extract multiple value free text column cv file cv file column consists series medical test free text date test name result like follows need extract value medical test turn column would like know way without train model nlp extract information patient result hmg plaque hmg haemoglobin urea hmg plaque alt tgp ast tgo bilirrubin hmg haematocrit hmg haemoglobin csv
How to read tagged PDF in C# using iText 7?,"<p>I am trying to read pieces of information from tables in PDF files in C# using iText 7.</p>
<p>Currently I'm using <code>TextRegionEventFilter</code> and <code>PdfTextExtractor</code> to get the text in the cells using the cell's X, Y locations.</p>
<p>If you have programmed it like this before you know that this sorta works if the X, Y location of your data elements stay constant in each PDF file.</p>
<p>But because the table's row height and column width in my PDF files keep changing, this approach won't work very well.</p>
<p>So, I thought if I can extract the PDF content as a string that includes the tags, then I can precisely pinpoint the pieces of information I need by parsing the raw string much like an HTML file.</p>
<p>To my disappointment, the PDF content I extract using <code>LocationTextExtractionStrategy</code>'s <code>GetResultantText</code> as a string doesn't contain any of the tags.</p>
<p>So, my question is: Is it possible to extract PDF content with tags using iText 7? I searched high and low for how to read tagged PDF in iText, but nothing comes up.</p>
<p><a href=""https://i.sstatic.net/ZdjVk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZdjVk.png"" alt=""enter image description here"" /></a></p>
",Named Entity Recognition (NER),read tagged pdf c using itext trying read piece information table pdf file c using itext currently using get text cell using cell x location programmed like know sorta work x location data element stay constant pdf file table row height column width pdf file keep changing approach work well thought extract pdf content string includes tag precisely pinpoint piece information need parsing raw string much like html file pdf content extract using string contain tag question possible extract pdf content tag using itext searched high low read tagged pdf itext nothing come
How to normalise keywords extracted with Named Entity Recognition,"<p>I'm trying to employ NER to extract keywords (tags) from job postings. This can be anything along with <code>React, AWS, Team Building, Marketing</code>.</p>
<p>After training a custom model in SpaCy I'm presented with a problem - extracted tags are not unified/normalized across all of the data.</p>
<p>For example, if job posting is about <code>frontend development</code>, NER can extract the keyword <code>frontend</code> in many ways (depending on job description), for example: <code>Frontend</code>, <code>Front End</code>, <code>Front-End</code>, <code>front-end</code> and so on.</p>
<p>Is there a reliable way to normalise/unify the extracted keywords? All the keywords go directly into the database and, with all the variants of each keyword, I would end up with too much noise.</p>
<p>One way to tackle the problem would be to create mappings such as:</p>
<pre><code>&quot;Frontend&quot;: [&quot;Front End&quot;, &quot;Front-End&quot;, &quot;front-end&quot;]
</code></pre>
<p>but that approach seems not too bright. Perhaps within SpaCy itself there's an option to normalise tags?</p>
",Named Entity Recognition (NER),normalise keywords extracted named entity recognition trying employ ner extract keywords tag job posting anything along training custom model spacy presented problem extracted tag unified normalized across data example job posting ner extract keyword many way depending job description example reliable way normalise unify extracted keywords keywords go directly database variant keyword would end much noise one way tackle problem would create mapping approach seems bright perhaps within spacy option normalise tag
What does config inside ``super().__init__(config)`` actually do?,"<p>I have the following code to create a custom model for Named-entity-recognition. Using ChatGPT and Copilot, I've commented it to understand its functionality.</p>
<p>However, the point with <code>config</code> inside <code>super().__init__(config)</code> is not clear for me. Which role does it play since we have already used <code>XLMRobertaConfig</code> at the beginning?</p>
<pre class=""lang-py prettyprint-override""><code>import torch.nn as nn
from transformers import XLMRobertaConfig
from transformers.modeling_outputs import TokenClassifierOutput
from transformers.models.roberta.modeling_roberta import RobertaModel
from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel

# Create a class for a custom model, which inherit from RobertaPreTrainedModel since we want to use the weights of a pretained model in the body of a custom model
class XLMRobertaForTokenClassification(RobertaPreTrainedModel):
    # Common practice in 🤗 Transformers 
    # allows the XLMRobertaForTokenClassification class to inherit the configuration functionality and attributes from the XLMRobertaConfig class
    config_class = XLMRobertaConfig

    # initialize the model
    def __init__(self, config):
        # call the initialization function of the parent class (RobertaPreTrainedModel)
        super().__init__(config)              # config is necessary when working with pretrained models to ensure the initialization with the correct configuration of parent class
        self.num_labels = config.num_labels   # number of classes to predict

        # Load model BODY
        self.roberta = RobertaModel(config, add_pooling_layer=False) # returns all hidden states not just [CLS]
        
        # Set up token CLASSIFICATION HEAD
        self.dropout = nn.Dropout(config.hidden_dropout_prob)             
        self.classifier = nn.Linear(config.hidden_size, config.num_labels) # linear transformation layer takes (batch_size, sequence_length, hidden_size) 
                                                                           # to produce output tensor of shape (batch_size, sequence_length, num_labels)
                                                                           # which can be interpreted as probability distribution over the labels for each token in the input sequence.
        
        # Load the pretrained weights for the model body and 
        # ... randomly initialize weights of token classification head
        self.init_weights()

    # define the forward pass
    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, 
                labels=None, **kwargs):
        # Feed the data through model BODY to get encoder representations
        outputs = self.roberta(input_ids, attention_mask=attention_mask,
                               token_type_ids=token_type_ids, **kwargs)
        
        # Apply classifier to encoder representation 
        sequence_output = self.dropout(outputs[0]) # apply dropout to the first element of output tensor, i.e., last_hidden_state
        logits = self.classifier(sequence_output)  # apply the linear transformation to get the logits (i.e., raw output of the model)
        # Calculate losses if labels are provided
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1)) # apply cross entropy function on flattend logits and flattend labels
        # Return model output object
        return TokenClassifierOutput(loss=loss, logits=logits, 
                                     hidden_states=outputs.hidden_states, 
                                     attentions=outputs.attentions)
</code></pre>
<p><strong>EDIT</strong>: I quote directly from the book I'm working on: <em>&quot;<code>config_class</code>  ensures that the standard <code>XLMRobertaConfig</code> settings are used when initilize a new model&quot;</em>. If I understand it correctly, could we change these defualt parameters by overwriting the default settings in the <code>config</code>?</p>
",Named Entity Recognition (NER),doe config inside super init config actually following code create custom model named entity recognition using chatgpt copilot commented understand functionality however point inside clear role doe play since already used beginning edit quote directly book working ensures standard setting used initilize new model understand correctly could change defualt parameter overwriting default setting
How do I get confidence score in spacy 3.5 prediction?,"<p>I am working with spacy 3.5. Everything is great, but did not find a way to get confidence score from my NER prediction.</p>
<p>I trained a custom NER model with spacy 3.5. I am able to make predictions using the model. However there is no way through which I can get confidence scores for my predictions.</p>
",Named Entity Recognition (NER),get confidence score spacy prediction working spacy everything great find way get confidence score ner prediction trained custom ner model spacy able make prediction using model however way get confidence score prediction
Keywords extracted from text using KeyBERT and lambda function appear to be similar,"<p>I am trying to extract keywords from text held in a pandas dataframe column.</p>
<p>The dataframe's name is <code>memo_</code> and the column's name is <code>Text</code>. I am applying the KeyBERT model as shown below. I am not getting the right output. The keywords seem to be similar for all rows despite the text being different. Any guidance on this will be helpful.</p>
<pre class=""lang-py prettyprint-override""><code>kw_model = KeyBERT(model='all-mpnet-base-v2')
memo_['Text'].apply(lambda x: kw_model.extract_keywords(x ,keyphrase_ngram_range=(1, 3), stop_words='english', highlight=False, top_n=10))
</code></pre>
",Named Entity Recognition (NER),keywords extracted text using keybert lambda function appear similar trying extract keywords text held panda dataframe column dataframe name column name applying keybert model shown getting right output keywords seem similar row despite text different guidance helpful
Cleaning Up (or Avoiding) Extra Whitespace with PyPDF2,"<p>I've been extracting text from PDFs using PyPDF2. However it seems to be inputting erroneous white space in between words. Does anyone know of way to avoid this, or clean it after the fact? Here is an example:</p>

<blockquote>
  <p>'IN THE MATTER OF  an application submitted by 1113 York Avenue Realty
  Company,  L.L.C. and 60th Street Devel opment LLC pursuant to Sections
  197-c and 201 of the New York  City Charter for an amendment of th e
  Zoning Map, Section Nos. 8c and 8d:'</p>
</blockquote>

<p>Here ""development"" is spelt ""devel opment"" and ""the"" is the spelt ""th e"". I'd like to correct this.</p>

<p>Here is <a href=""http://www1.nyc.gov/assets/planning/download/pdf/about/cpc/000198.pdf"" rel=""nofollow noreferrer"">PDF</a>. The example text is from list item number 1, on the first page.</p>
",Named Entity Recognition (NER),cleaning avoiding extra whitespace pypdf extracting text pdfs using pypdf however seems inputting erroneous white space word doe anyone know way avoid clean fact example matter application submitted york avenue realty company l l c th street devel opment llc pursuant section c new york city charter amendment th e zoning map section c development spelt devel opment spelt th e like correct pdf example text list item number first page
Trying to find human names in a file using ntlk,"<p>I'd like to extract human names from a text file. I'm getting a blank line as output for some reason. Here is my code:</p>
<pre><code>import nltk
import re
nltk.download('names')
nltk.download('punkt')
from nltk.corpus import names

# Create a list of male and female names from the nltk names corpus
male_names = names.words('male.txt')
female_names = names.words('female.txt')
all_names = set(male_names + female_names)

def flag_people_names(text):
    possible_names = []
    words = nltk.word_tokenize(text)
    for word in words:
        # Split the word by ' ', '.' or '_' and check each part
        parts = re.split('[ _.]', word)
        for part in parts:
            if part.lower() in all_names:
                possible_names.append(word)
                break
    return possible_names

# Read text file
with open('sample.txt', 'r') as file:
    text = file.read()

# Call function to flag possible names
names = flag_people_names(text)
print(names)
</code></pre>
<p>Here is the input file called sample.txt</p>
<pre><code>James is a really nice guy
Gina is a friend of james.
Gina and james like to play with Andy.
</code></pre>
<p>I get this as the output:</p>
<pre><code>[]
</code></pre>
<p>I'd like to get James, Gina and Andy.</p>
<p>I'm on a MAC Catalina with python3.8.5.
Any idea what's not working here?</p>
",Named Entity Recognition (NER),trying find human name file using ntlk like extract human name text file getting blank line output reason code input file called sample txt get output like get james gina andy mac catalina python idea working
Updating an already existing spacy NER model,"<p>I want to update and already existing spacy model 'en_core_web_sm' and train it with additional data. </p>

<p>My data is in the same format as mentioned in spacy's documentation 
<a href=""https://spacy.io/usage/training"" rel=""nofollow noreferrer"">https://spacy.io/usage/training</a></p>

<p>I've followed the same steps as mentioned in the documentation for updating an NER model with my data. </p>

<pre><code>def model_train(output_dir=None, n_iter=100):
    """"""Load the model, set up the pipeline and train the entity recognizer.""""""
    model=('en_core_web_sm')
    nlp = spacy.load(model, entity = False, parser = False)  # load existing spaCy model
    print(""Loaded model '%s'"" % model)
    print (nlp.pipe_names)

#     # create the built-in pipeline components and add them to the pipeline

    ner = nlp.get_pipe(""ner"")

#     # add labels
    for texts, annotations in TRAIN_DATA:        
        for ent in annotations.get(""entities""):
#             print (ent)
            ner.add_label(ent[2])
#             print (ent[2])

    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]
    with nlp.disable_pipes(*other_pipes):  # only train NER

        if model is None:
            nlp.begin_training()
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            losses = {}
#             # batch up the examples using spaCy's minibatch
            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(
                    [texts],  # batch of texts
                    [annotations],  # batch of annotations
                    drop=0.5,  # dropout - make it harder to memorise data
                    losses=losses,
                )
            print(""Losses"", losses)


        nlp.to_disk(output_dir)
        print(""Saved model to"", output_dir)
</code></pre>

<p>The error that I'm getting is </p>

<pre><code>Loaded model 'en_core_web_sm'
['tagger', 'parser', 'ner']
------------------------------------------------------------------------- 
</code></pre>

<p>-
    TypeError                                 Traceback (most recent call last)
     in 
    ----> 1 model_train()</p>

<pre><code>&lt;ipython-input-337-91366511ed4d&gt; in model_train(output_dir, n_iter)
     56                     [annotations],  # batch of annotations
     57                     drop=0.5,  # dropout - make it harder to 
memorise data
---&gt; 58                     losses=losses,
     59                 )
     60             print(""Losses"", losses)

C:\ProgramData\Anaconda3\lib\site-packages\spacy\language.py in 
update(self, docs, golds, drop, sgd, losses, component_cfg)
    432                 doc = self.make_doc(doc)
    433             if not isinstance(gold, GoldParse):
--&gt; 434                 gold = GoldParse(doc, **gold)
    435             doc_objs.append(doc)
    436             gold_objs.append(gold)

TypeError: type object argument after ** must be a mapping, not tuple
</code></pre>
",Named Entity Recognition (NER),updating already existing spacy ner model want update already existing spacy model en core web sm train additional data data format mentioned spacy documentation followed step mentioned documentation updating ner model data error getting typeerror traceback recent call last model train
Spacy Include Rule Matcher In Pipeline,"<p>I have certain phrases in my corpus that I need spacy to disregard (with the hopes of avoiding overfitting). They're fairly simple regex formulas that I can also write in the format of Spacy's rule based matching <a href=""https://spacy.io/usage/linguistic-features#section-rule-based-matching"" rel=""nofollow noreferrer"">like here</a>.</p>

<p>I'd like the matches to be marked as stop words in my model before going on to the NER and TextCat pipes. I see how to write the matcher, but I'm not sure how to incorporate it into my model? Do I just add it as a pipe?</p>

<p>Thanks!</p>
",Named Entity Recognition (NER),spacy include rule matcher pipeline certain phrase corpus need spacy disregard hope avoiding overfitting fairly simple regex formula also write format spacy rule based matching like like match marked stop word model going ner textcat pipe see write matcher sure incorporate model add pipe thanks
Problem to extract NER subject + verb with spacy and Matcher,"<p>I work on an NLP project and i have to use spacy and spacy Matcher to extract all named entities who are nsubj (subjects) and the verb to which it relates : the governor verb of my NE nsubj.
Example :</p>
<pre><code>Georges and his friends live in Mexico City
&quot;Hello !&quot;, says Mary
</code></pre>
<p>I'll need to extract &quot;Georges&quot; and &quot;live&quot; in the first sentence and &quot;Mary&quot; and &quot;says&quot; in the second one but i don't know how many words will be between my named entity and the verb to which it relate. So i decided to explore spacy Matcher more.
So i'm struggling to write a pattern on Matcher to extract my 2 words. When the NE subj is before the verb, i get good results but i don't know how to write a pattern to match a NE subj after words which it correlates to. I could also, according to the guideline, do this task with &quot;regular spacy&quot; but i don't know how to do that. The problem with Matcher concerns the fact that i can't manage the type of dependency between the NE and VERB and grab the good VERB. I'm new with spacy, i've always worked with NLTK or Jieba (for chineese). I don't know even how to tokenize a text in sentence with spacy. But i chose to split the whole text in sentences to avoir bad matching between two sentences.
Here is my code</p>
<pre><code>import spacy
from nltk import sent_tokenize
from spacy.matcher import Matcher

nlp = spacy.load('fr_core_news_md')

matcher = Matcher(nlp.vocab)

def get_entities_verbs():

    try:

        # subjet before verb
        pattern_subj_verb = [{'ENT_TYPE': 'PER', 'DEP': 'nsubj'}, {&quot;POS&quot;: {'NOT_IN':['VERB']}, &quot;DEP&quot;: {'NOT_IN':['nsubj']}, 'OP':'*'}, {'POS':'VERB'}]
        # subjet after verb
        # this pattern is not good

        matcher.add('ent-verb', [pattern_subj_verb])

        for sent in sent_tokenize(open('Le_Ventre_de_Paris-short.txt').read()):
            sent = nlp(sent)
            matches = matcher(sent)
            for match_id, start, end in matches:
                span = sent[start:end]
                print(span)

    except Exception as error:
        print(error)


def main():

    get_entities_verbs()

if __name__ == '__main__':
    main()
</code></pre>
<p>Even if  it's french, i can assert you that i get good results</p>
<pre><code>Florent regardait
Lacaille reparut
Florent baissait
Claude regardait
Florent resta
Florent, soulagé
Claude s’était arrêté
Claude en riait
Saget est matinale, dit
Florent allait
Murillo peignait
Florent accablé
Claude entra
Claude l’appelait
Florent regardait
Florent but son verre de punch ; il le sentit
Alexandre, dit
Florent levait
Claude était ravi
Claude et Florent revinrent
Claude, les mains dans les poches, sifflant
</code></pre>
<p>I have some wrong results but 90% is good. I just need to grab the first ans last word of each line to have my couple NE/verb.
So my question is. How to extract NE when NE is subj with the verb which it correlates to with Matcher or simply how to do that with spacy (not Matcher) ? There are to many factors to be taken into account. Do you have a method to get the best results as possible even if 100% is not possible.
I need a pattern matching VERB governor + NER subj after from this pattern:</p>
<pre><code>pattern = [
        {
            &quot;RIGHT_ID&quot;: &quot;person&quot;,
            &quot;RIGHT_ATTRS&quot;: {&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;DEP&quot;: &quot;nsubj&quot;},
        },
        {
            &quot;LEFT_ID&quot;: &quot;person&quot;,
            &quot;REL_OP&quot;: &quot;&lt;&quot;,
            &quot;RIGHT_ID&quot;: &quot;verb&quot;,
            &quot;RIGHT_ATTRS&quot;: {&quot;POS&quot;: &quot;VERB&quot;},
        }
        ]
</code></pre>
<p>All credit to polm23 for this pattern</p>
",Named Entity Recognition (NER),problem extract ner subject verb spacy matcher work nlp project use spacy spacy matcher extract named entity nsubj subject verb relates governor verb ne nsubj example need extract george live first sentence mary say second one know many word named entity verb relate decided explore spacy matcher struggling write pattern matcher extract word ne subj verb get good result know write pattern match ne subj word correlate could also according guideline task regular spacy know problem matcher concern fact manage type dependency ne verb grab good verb new spacy always worked nltk jieba chineese know even tokenize text sentence spacy chose split whole text sentence avoir bad matching two sentence code even french assert get good result wrong result good need grab first last word line couple ne verb question extract ne ne subj verb correlate matcher simply spacy matcher many factor taken account method get best result possible even possible need pattern matching verb governor ner subj pattern credit polm pattern
Need approach on building Custom NER for extracting below keywords from any format of payslips,"<p>I am trying to build a generic extraction of below parameters from any format of payslip:</p>

<ol>
<li>Name</li>
<li>His PostCode</li>
<li>Pay Date</li>
<li>Net Pay.</li>
</ol>

<p>Challenge I am facing is due to variety of format that may come, I want to apply NER (Spacy) to learn these under the entities</p>

<ol>
<li>Name - PERSON</li>
<li>His PostCode</li>
<li>Pay Date - DATE</li>
<li>Net Pay. - MONEY</li>
</ol>

<p>But I am unsuccess so far, I even tried to build a custom EntityMatcher for Postcode &amp; Date but to no success.</p>

<p>I seek any guideline and approach to make me take the right path in achieving the above ask, as to what is the right and best approach under the ML to achieve this.</p>

<p>A snippet of Custom NER I tried to build</p>

<pre><code>import spacy
import random
import threading
import time
from DateEntityMatcher import  DateEntityMatcher
from PostCodeEntityMatcher import PostCodeEntityMatcher


class IncomeValidatorModel(object):
    """""" Threading example class
    The run() method will be started and it will run in the background
    until the application exits.
    """"""

    def __init__(self, interval=1):
        """""" Constructor
        :type interval: int
        :param interval: Check interval, in seconds
        """"""
        self.interval = interval

        thread = threading.Thread(target=self.run, args=())
        thread.daemon = True                            # Daemonize thread
        thread.start()                                  # Start the execution

    def run(self):
        """""" Method that runs forever """"""
        while True:
            # Do something
            print('Doing something important in the background')
            DATA = [
                (u""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR M HASAN    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
                {'entities': [(203, 218, 'ORG'), (100, 106, 'PERSON'), (1097, 1103, 'MONEY')]}),
                (u""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR K KHANA    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
                {'entities': [(203, 218, 'ORG'), (100, 106, 'PERSON'), (1097, 1103, 'MONEY')]}),
                (u""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR M MENON    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
                {'entities': [(203, 218, 'ORG'), (100, 106, 'PERSON'), (1097, 1103, 'MONEY')]}),
                (u""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR F JAHAN    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
                {'entities': [(203, 218, 'ORG'), (100, 106, 'PERSON'), (1097, 1103, 'MONEY')]}),
                (u""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR A JAHAN    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
                {'entities': [(203, 218, 'ORG'), (100, 106, 'PERSON'), (1097, 1103, 'MONEY')]}),
                (u""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR M HASAN    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
                {'entities': [(203, 218, 'ORG'), (100, 106, 'PERSON'), (1097, 1103, 'MONEY')]}),
                (u""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR M HASAN    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
                {'entities': [(203, 218, 'ORG'), (100, 106, 'PERSON'), (1097, 1103, 'MONEY')]}),
                (u""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR M HASAN    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
                {'entities': [(203, 218, 'ORG'), (100, 106, 'PERSON'), (1097, 1103, 'MONEY')]}),
                (u""Sample Payslip    Matrix House    Basing View    Basingstoke    Hampshire    RG21 4FF    Advantage Resourcing    6th Floor, Matrix House, Basing View, Basingstoke, Hampshire, RG21 4FF    Registered Number 03341461    COMPANY    DIVISION    Advantage Resourcing UK    SWINDON    WORKER NO.    NAME    PERIOD    PAY DATE    IND    123456    Sample Payslip    14/2016    08/07/2016    W1    DEPARTMENT    TAX CODE    N.I. NO./TABLE LETTER    NAT    1100L    JA123456A/A    PAYMENTS    DEDUCTIONS    Wk Ending    Timesheet    Description    Units    Rate    Amount    Deduction    Amount    03/07/2016    GEN000499628 Hourly Rate    40.00    10.00    400.00    Tax    87.60    03/07/2016    GEN000499628 Week Day Overtime    10.00    15.00    150.00    NI    59.40    03/07/2016    GEN000499628 Saturday Overtime    5.00    20.00    100.00    TOTAL PAYMENTS    650.00    TOTAL DEDUCTIONS    147.00    CUMULATIVES    GROSS TO DATE    650.00    Current Holiday Entitlement: 0.00 Unit(s)    TAXABLE PAY TO DATE    650.00    EE PENSION TO DATE    0.00    ER PENSION TO DATE    0.00    TAX TO DATE    87.60     TO DATE    68.17    TO DATE    59.40    c Safe Computing Limited 2002    NET PAY    503.00"",
                {'entities': [(89, 109, 'ORG'), (0, 14, 'PERSON'), (1186, 1191, 'MONEY')]}),
                (u""Mubssar Hasan    Matrix House    Basing View    Basingstoke    Hampshire    RG21 4FF    Advantage Resourcing    6th Floor, Matrix House, Basing View, Basingstoke, Hampshire, RG21 4FF    Registered Number 03341461    COMPANY    DIVISION    Advantage Resourcing UK    SWINDON    WORKER NO.    NAME    PERIOD    PAY DATE    IND    123456    Sample Payslip    14/2016    08/07/2016    W1    DEPARTMENT    TAX CODE    N.I. NO./TABLE LETTER    NAT    1100L    JA123456A/A    PAYMENTS    DEDUCTIONS    Wk Ending    Timesheet    Description    Units    Rate    Amount    Deduction    Amount    03/07/2016    GEN000499628 Hourly Rate    40.00    10.00    400.00    Tax    87.60    03/07/2016    GEN000499628 Week Day Overtime    10.00    15.00    150.00    NI    59.40    03/07/2016    GEN000499628 Saturday Overtime    5.00    20.00    100.00    TOTAL PAYMENTS    650.00    TOTAL DEDUCTIONS    147.00    CUMULATIVES    GROSS TO DATE    650.00    Current Holiday Entitlement: 0.00 Unit(s)    TAXABLE PAY TO DATE    650.00    EE PENSION TO DATE    0.00    ER PENSION TO DATE    0.00    TAX TO DATE    87.60     TO DATE    68.17     TO DATE    59.40    c Safe Computing Limited 2002    NET PAY    503.00"",
                {'entities': [(88, 108, 'ORG'), (0, 13, 'PERSON'), (1186, 1191, 'MONEY')]}),
                (u""Oracle Corp Anil Menon Work Date 01/09/2019 PAYMENTS Tax 100 Net Pay 2000"",
                 {'entities': [(0, 10, 'ORG'), (12, 21, 'PERSON'), (69, 72, 'MONEY')]}),
                (u""Huawei Corp Anil Menon Work Date 01/06/2019 PAYMENTS Tax 100 Net Pay 1900"",
                 {'entities': [(0, 10, 'ORG'), (12, 21, 'PERSON'), (69, 72, 'MONEY')]}),
                (u""Tata Corp Nitin Garg Work Date 20/04/2019 PAYMENTS Tax 100 Net Pay 1900"",
                 {'entities': [(0, 8, 'ORG'), (10, 19, 'PERSON'), (67, 70, 'MONEY')]}),
                (u""Accenture Corp Amol Joshi Work Date 20/04/2019 PAYMENTS Tax 100 Net Pay 900"",
                 {'entities': [(0, 15, 'ORG'), (17, 26, 'PERSON'), (72, 74, 'MONEY')]}),
                (u""Cognizant Corp Anup Nair Work Date 20/04/2019 PAYMENTS Tax 100 Net Pay 900"",
                 {'entities': [(0, 15, 'ORG'), (17, 25, 'PERSON'), (71, 73, 'MONEY')]}),
                (u""Cognizant Corp Sajit Kumar Work Date 20/04/2019 PAYMENTS Tax 100 Net Pay 1900"",
                 {'entities': [(0, 15, 'ORG'), (17, 27, 'PERSON'), (73, 76, 'MONEY')]}),
                (u""Tata Corp Saurabh Dave Work Date 20/04/2019 PAYMENTS Tax 100 Net Pay 1300"",
                 {'entities': [(0, 8, 'ORG'), (10, 21, 'PERSON'), (69, 72, 'MONEY')]}),
                (u""Capgemini PLC Mubashshir Hasan Work Date 20/04/2019 PAYMENTS Tax 100 Net Pay 1700"",
                 {'entities': [(0, 12, 'ORG'), (14, 29, 'PERSON'), (77, 80, 'MONEY')]}),
                (u""Capgemini PLC Sagar Pande Work Date 20/04/2019 PAYMENTS Tax 100 Net Pay 1700"",
                 {'entities': [(0, 12, 'ORG'), (14, 24, 'PERSON'), (72, 75, 'MONEY')]}),
                (u""Capgemini PLC Sreeram Yegappan Work Date 20/04/2019 PAYMENTS Tax 100 Net Pay 2000"",
                 {'entities': [(0, 12, 'ORG'), (14, 29, 'PERSON'), (77, 80, 'MONEY')]})
            ]

            # nlp = spacy.blank('en')  # new, empty model. Let’s say it’s for the English language
            global nlp
            nlp = spacy.load('en_core_web_sm')
            nlp.entity.add_label('ORG')
            nlp.entity.add_label('PERSON')
            nlp.entity.add_label('MONEY')

            # add NER pipeline
            # ner = nlp.create_pipe('ner')  # our pipeline would just do NER
            # nlp.add_pipe(ner, last=True)  # we add the pipeline to the model
            postcde_entity_matcher = PostCodeEntityMatcher(nlp, ['NN1 3LE', 'NN2 8HF', 'IG3 8TH', 'NN4 7YH', 'RG21 5GH'], 'POSTCDE')
            nlp.entity.add_label('POSTCDE')
            nlp.add_pipe(postcde_entity_matcher, before='ner')

            date_entity_matcher = DateEntityMatcher(nlp, ['20/04/2019','20/04/2019', '25/04/2016', '20/04/2019', '20/07/2019', '20/12/2019'], 'DATE')
            nlp.entity.add_label('DATE')
            nlp.add_pipe(date_entity_matcher, before='ner')

            optimizer = nlp.begin_training()

            for i in range(11):
                random.shuffle(DATA)
                for text, annotations in DATA:
                    nlp.update([text], [annotations], sgd=optimizer)

            time.sleep(self.interval)

    def extractPayslipData(self, data):
        doc = nlp(data)
        for entity in doc.ents:
            print(entity.label_, ' | ', entity.text)
        return doc.ents
</code></pre>
",Named Entity Recognition (NER),need approach building custom ner extracting keywords format payslip trying build generic extraction parameter format payslip name postcode pay date net pay challenge facing due variety format may come want apply ner spacy learn entity name person postcode pay date date net pay money unsuccess far even tried build custom entitymatcher postcode date success seek guideline approach make take right path achieving ask right best approach ml achieve snippet custom ner tried build
I have created a custom NER using spacy and i want to train it with additional data but what to change in config.cfg file?,"<p>I have created a spacy NER model for named entity recognition and its having tok2vec and ner as components in the pipeline. Now i want to add some more data to it, so i am using a model-best directory from where I can load my trained model for predictions. If i follow the documentation without changing anything from config.cfg file then the newly created model-best have no information about it's previous trained data.</p>
<p><code>! python -m spacy convert one.json ./ -t spacy</code></p>
<p><code>! python -m spacy init fill-config base_config.cfg config.cfg</code></p>
<p><code>! python -m spacy train config.cfg --output ./ --paths.train ./one.spacy --paths.dev ./one.spacy</code></p>
<p>After running them two folders got created (model-best and model-last)</p>
<p>now to train it with new data i tried like this:</p>
<pre><code>import spacy
from spacy.tokens import DocBin
from tqdm import tqdm
import json

nlp=spacy.load('model-best')
f = open('two.json')
TRAIN_DATA = json.load(f)
</code></pre>
<pre><code>db = DocBin()
for text, annot in tqdm(TRAIN_DATA['annotations']): 
    doc = nlp.make_doc(text) 
    ents = []
    for start, end, label in annot[&quot;entities&quot;]:
        span = doc.char_span(start, end, label=label, alignment_mode=&quot;contract&quot;)
        if span is None:
            print(&quot;Skipping entity&quot;)
        else:
            ents.append(span)
    doc.ents = ents 
    db.add(doc)

db.to_disk(&quot;./training_data.spacy&quot;) 
</code></pre>
<p><code>! python -m spacy init fill-config base_config.cfg config.cfg</code></p>
<p><code>! python -m spacy train config.cfg --output ./ --paths.train ./training_data.spacy --paths.dev ./training_data.spacy</code></p>
<p>After running them, it replaced my model-best folder with new one and it can only recognnise the new data now
what changes should i make in my config.cfg inorder to train it properly so that it can remember both old data and new data?</p>
",Named Entity Recognition (NER),created custom ner using spacy want train additional data change config cfg file created spacy ner model named entity recognition tok vec ner component pipeline want add data using model best directory load trained model prediction follow documentation without changing anything config cfg file newly created model best information previous trained data running two folder got created model best model last train new data tried like running replaced model best folder new one recognnise new data change make config cfg inorder train properly remember old data new data
Address Splitting with NLP,"<p>I am working currently on a project that should identify each part of an address, for example from &quot;str. Jack London 121, Corvallis, ARAD, ap. 1603, 973130 &quot; the output should be like this:</p>
<pre><code>street name: Jack London; 
no: 121; city: Corvallis; 
state: ARAD; 
apartment: 1603; 
zip code: 973130
</code></pre>
<p>The problem is that not all of the input data are in the same format so some of the elements may be missing or in different order, but it is guaranteed to be an address.</p>
<p>I checked some sources on the internet, but a lot of them are adapted for US addresses only - like Google API Places, the thing is that I will use this for another country.</p>
<p>Regex is not an option since the address may variate too much.</p>
<p>I also thought about NLP to use Named Entity Recognition model but I'm not sure that will work.</p>
<p>Do you know what could a be a good way to start, and maybe help me with some tips?</p>
",Named Entity Recognition (NER),address splitting nlp working currently project identify part address example str jack london corvallis arad ap output like problem input data format element may missing different order guaranteed address checked source internet lot adapted u address like google api place thing use another country regex option since address may variate much also thought nlp use named entity recognition model sure work know could good way start maybe help tip
Base word for same word is different using spacy,"<p>I am trying to extract base word for an entire text however it reacts differently for a same word coming at different locations. Below is the code for reference:</p>
<pre><code>from nltk.stem import PorterStemmer
import spacy
import pandas as pd

ps = PorterStemmer()

# spacy.cli.download(&quot;en&quot;)
nlp = spacy.load(&quot;en_core_web_sm&quot;)

words = [  &quot;meeting&quot;,&quot;eating&quot;, &quot;adjustable&quot;,  &quot;meeting&quot;, &quot;eats&quot;, &quot;eating&quot;, &quot;eat&quot;, &quot;ate&quot;, &quot;rafting&quot;, &quot;better&quot;, &quot;good&quot;, &quot;best&quot;, 'coming', &quot;ability&quot;, &quot;steal&quot;, &quot;stolen&quot;, 'children']
word_joined = &quot; &quot;.join(words)
# print(word_joined)
doc = nlp(word_joined)

words_lemma = []
words_stemm = []


words_stemm = [ps.stem(w) for w in words]
words_lemma = [w.lemma_ for w in doc]

pd.DataFrame(list(zip(words, words_stemm, words_lemma))) 
</code></pre>
<p><a href=""https://i.sstatic.net/DNG7s.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DNG7s.png"" alt=""enter image description here"" /></a></p>
<p>Is it something related to positioning of the word in a sentence from linguistical prospective?</p>
",Named Entity Recognition (NER),base word word different using spacy trying extract base word entire text however reacts differently word coming different location code reference something related positioning word sentence linguistical prospective
Adding an entity to a dataset,"<p>It would be helpful if you could help me add a GPA entity to Spacey Model for the named entity since there is no GPA entity in the resume dataset.</p>
",Named Entity Recognition (NER),adding entity dataset would helpful could help add gpa entity spacey model named entity since gpa entity resume dataset
How to load custom spacynlp model on server,"<p>How to load custom spacyNlp ner model on ec2 server. I am facing error that &quot;not able to load model&quot; on server though i download it.</p>
<p>My custom model get load on server</p>
",Named Entity Recognition (NER),load custom spacynlp model server load custom spacynlp ner model ec server facing error able load model server though download custom model get load server
Stanford&#39;s Stanza NLP: find all words ids for a given span,"<p>I am using a Stanza pipeline that extracts both words and named entities.</p>
<p>The sentence.entities gives me a list of recognized named entities with their start and end characters. Here is an example:</p>
<pre><code>{
  &quot;text&quot;: &quot;Dante Alighieri&quot;,
  &quot;type&quot;: &quot;PER&quot;,
  &quot;start_char&quot;: 1,
  &quot;end_char&quot;: 16
}
</code></pre>
<p>The sentence.words gives a list of all tokenized words also with their start and end characters: Here is a fragment of the corresponding example:</p>
<pre><code>{
  &quot;id&quot;: 1,
  &quot;text&quot;: &quot;Dante&quot;,
  &quot;lemma&quot;: &quot;Dante&quot;,
  &quot;upos&quot;: &quot;PROPN&quot;,
  &quot;xpos&quot;: &quot;SP&quot;,
  &quot;head&quot;: 3,
  &quot;deprel&quot;: &quot;nsubj&quot;,
  &quot;start_char&quot;: 1,
  &quot;end_char&quot;: 6
}
{
  &quot;id&quot;: 2,
  &quot;text&quot;: &quot;Alighieri&quot;,
  &quot;lemma&quot;: &quot;Alighieri&quot;,
  &quot;upos&quot;: &quot;PROPN&quot;,
  &quot;xpos&quot;: &quot;SP&quot;,
  &quot;head&quot;: 1,
  &quot;deprel&quot;: &quot;flat:name&quot;,
  &quot;start_char&quot;: 7,
  &quot;end_char&quot;: 16
}
{
  &quot;id&quot;: 3,
  &quot;text&quot;: &quot;scrisse&quot;,
  &quot;lemma&quot;: &quot;scrivere&quot;,
  &quot;upos&quot;: &quot;VERB&quot;,
  &quot;xpos&quot;: &quot;V&quot;,
  &quot;feats&quot;: &quot;Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin&quot;,
  &quot;head&quot;: 0,
  &quot;deprel&quot;: &quot;root&quot;,
  &quot;start_char&quot;: 17,
  &quot;end_char&quot;: 24
}
</code></pre>
<p>I need to generate a list of all words that are included in the named entity span. Using the above example those would be the words with Id 1 and 2 but not 3</p>
",Named Entity Recognition (NER),stanford stanza nlp find word id given span using stanza pipeline extract word named entity sentence entity give list recognized named entity start end character example sentence word give list tokenized word also start end character fragment corresponding example need generate list word included named entity span using example would word id
Annotate Entity inside another Entity,"<p>We have to extract an entity which is inside another entity, any idea on how can we annotate the training data to train a NER model for this task. We are using Flair model for custom entity training and prediction.</p>
<p>Ex: Text:  &quot;&quot; Address: 123, ABC Company, 4th floor, xyz street, state, country.&quot;&quot;
We have a sample like this, where whole text itself is an entity of type &quot;Address&quot; and in the same text we have another entity called &quot;Company Name&quot;.
For train a flair model, we are converting the data into BIEO format, not sure how to annotate the data and train the model.</p>
",Named Entity Recognition (NER),annotate entity inside another entity extract entity inside another entity idea annotate training data train ner model task using flair model custom entity training prediction ex text address abc company th floor xyz street state country sample like whole text entity type address text another entity called company name train flair model converting data bieo format sure annotate data train model
How to count the number of nouns from Spacy from a dataframe column?,"<p>I have a dataframe like that (as an example).</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>text</th>
</tr>
</thead>
<tbody>
<tr>
<td>I left the country.</td>
</tr>
<tr>
<td>Andrew is from America and he loves apples.</td>
</tr>
</tbody>
</table>
</div>
<p>I want to add a new column, number of nouns, where Spacy should count the NOUNS pos tags. How do I convert that in Python?</p>
<pre><code>import pandas as pd
import spacy

# the dataframe

# NLP Spacy with POS tags
nlp = spacy.load(&quot;en_core_web_sm&quot;)
</code></pre>
<p>My question is, how to apply nlp on the &quot;text&quot; column, check if the pos is NOUN and count it and give it as a feature?</p>
<p>Thanks!</p>
",Named Entity Recognition (NER),count number noun spacy dataframe column dataframe like example text left country andrew america love apple want add new column number noun spacy count noun po tag convert python question apply nlp text column check po noun count give feature thanks
Spacy tokenization add extra white space for dates with hyphen separator when I manually build the Doc,"<p>I've been trying to solve a problem with the spacy Tokenizer for a while, without any success. Also, I'm not sure if it's a problem with the tokenizer or some other part of the pipeline.</p>
<p><strong>Description</strong></p>
<p>I have an application that for reasons besides the point, creates a spacy <code>Doc</code> from the spacy vocab and the list of tokens from a string (see code below). Note that while this is not the simplest and most common way to do this, according to <a href=""https://spacy.io/api/doc"" rel=""nofollow noreferrer"">spacy doc</a> this can be done.</p>
<p>However, when I create a <code>Doc</code> for a text that contains compound words or dates with hyphen as a separator, the behavior I am getting is not what I expected.</p>
<pre><code>import spacy
from spacy.language import Doc

# My current way
doc = Doc(nlp.vocab, words=tokens)  # Tokens is a well defined list of tokens for a certein string

# Standard way
doc = nlp(&quot;My text...&quot;)
</code></pre>
<p>For example, with the following text, if I create the <code>Doc</code> using the standard procedure, the spacy <code>Tokenizer</code> recognizes the <code>&quot;-&quot;</code> as tokens but the <code>Doc</code> text is the same as the input text, in addition the spacy NER model correctly recognizes the DATE entity.</p>
<pre><code>import spacy

doc = nlp(&quot;What time will sunset be on 2022-12-24?&quot;)
print(doc.text)

tokens = [str(token) for token in doc]
print(tokens)

# Show entities
print(doc.ents[0].label_)
print(doc.ents[0].text)
</code></pre>
<p>Output:</p>
<pre><code>What time will sunset be on 2022-12-24?
['What', 'time', 'will', 'sunset', 'be', 'on', '2022', '-', '12', '-', '24', '?']

DATE
2022-12-24
</code></pre>
<p>On the other hand, if I create the <code>Doc</code> from the model's <code>vocab</code> and the previously calculated tokens, the result obtained is different. Note that for the sake of simplicity I am using the tokens from <code>doc</code>, so I'm sure there are no differences in tokens. Also note that I am manually running each pipeline model in the correct order with the <code>doc</code>, so at the end of this process I would theoretically get the same results.</p>
<p>However, as you can see in the output below, while the Doc's tokens are the same, the Doc's text is different, there were blank spaces between the digits and the date separators.</p>
<pre><code>doc2 = Doc(nlp.vocab, words=tokens)

# Run each model in pipeline
for model_name in nlp.pipe_names:
    pipe = nlp.get_pipe(model_name)
    doc2 = pipe(doc2)

# Print text and tokens
print(doc2.text)
tokens = [str(token) for token in doc2]
print(tokens)

# Show entities
print(doc.ents[0].label_)
print(doc.ents[0].text)
</code></pre>
<p>Output:</p>
<pre><code>what time will sunset be on 2022 - 12 - 24 ? 
['what', 'time', 'will', 'sunset', 'be', 'on', '2022', '-', '12', '-', '24', '?']

DATE
2022 - 12 - 24
</code></pre>
<p>I know it must be something silly that I'm missing but I don't realize it.</p>
<p>Could someone please explain to me what I'm doing wrong and point me in the right direction?</p>
<p>Thanks a lot in advance!</p>
<p><strong>EDIT</strong></p>
<p>Following the <em>Talha Tayyab</em> suggestion, I have to create an array of booleans with the same length that my list of tokens to indicate for each one, if the token is followed by an empty space. Then pass this array in doc construction as follows: <code>doc = Doc(nlp.vocab, words=words, spaces=spaces)</code>.</p>
<p>To compute this list of boolean values ​​based on my original text string and list of tokens, I implemented the following vanilla function:</p>
<pre><code>def get_spaces(self, text: str, tokens: List[str]) -&gt; List[bool]:
     
    # Spaces
    spaces = []
    # Copy text to easy operate
    t = text.lower()

    # Iterate over tokens
    for token in tokens:

        if t.startswith(token.lower()):

            t = t[len(token):]  # Remove token

            # If after removing token we have an empty space
            if len(t) &gt; 0 and t[0] == &quot; &quot;:
                spaces.append(True)
                t = t[1:]  # Remove space
            else:
                spaces.append(False)

    return spaces

</code></pre>
<p>With these two improvements in my code, the result obtained is as expected. However, now I have the following question:</p>
<p>Is there a more spacy-like way to compute whitespace, instead of using my vanilla implementation?</p>
",Named Entity Recognition (NER),spacy tokenization add extra white space date hyphen separator manually build doc trying solve problem spacy tokenizer without success also sure problem tokenizer part pipeline description application reason besides point creates spacy spacy vocab list token string see code note simplest common way according spacy doc done however create text contains compound word date hyphen separator behavior getting expected example following text create using standard procedure spacy recognizes token text input text addition spacy ner model correctly recognizes date entity output hand create model previously calculated token result obtained different note sake simplicity using token sure difference token also note manually running pipeline model correct order end process would theoretically get result however see output doc token doc text different blank space digit date separator output know must something silly missing realize could someone please explain wrong point right direction thanks lot advance edit following talha tayyab suggestion create array booleans length list token indicate one token followed empty space pas array doc construction follows compute list boolean value based original text string list token implemented following vanilla function two improvement code result obtained expected however following question spacy like way compute whitespace instead using vanilla implementation
LSTM named entity recognition model - shape are incompatible or logits/labels have different dimensions - Tensorflow 2.9,"<p>I am working on NLP LSTM named entity extraction model but running into different errors below are more details about error. I am running this code in jupiter notebook</p>
<p>Tensorflow version 2.9</p>
<p>Both input and output are of length 50</p>
<p><strong>input sentence</strong> : [123 88 170 221 132 52 105 32 211 91 126 211 24 221 134 154 221 162
215 80 144 101 61 136 68 133 40 200 133 40 218 131 139 199 124 74
184 92 213 185 221 221 221 221 221 221 221 221 221 221]</p>
<p><strong>output sentece label</strong>: [ 7 7 7 7 0 7 6 2 7 5 1 7 7 7 7 7 7 7 7 10 7 7 7 7
3 8 7 3 8 7 7 7 7 7 7 7 7 6 2 7 7 7 7 7 7 7 7 7
7 7]</p>
<p>Added upto 5 layers to train the model</p>
<p><strong>Here is the model:</strong></p>
<pre><code>model = tf.keras.Sequential([

tf.keras.layers.Embedding(num_words, 50, input_length=50),

tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),

tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),

tf.keras.layers.Dropout(0.5),

tf.keras.layers.Dense(64, activation=‘relu’),

tf.keras.layers.Dense(num_tags, activation=‘softmax’)
])
</code></pre>
<p><strong>If I use loss function as “categorical_crossentropy” , I get this error:</strong>
ValueError: Shapes (None, 50) and (None, 11) are incompatible</p>
<p><strong>If I use loss function as “sparse_categorical_crossentropy” , I get this error</strong>:
logits and labels must have the same first dimension, got logits shape [13,11] and labels shape [650]
[[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]</p>
<p><strong>I tried adding input shape as first layer but still no luck</strong>
tf.keras.layers.Input(shape=(max_len,))</p>
<p>Can anyone help , how to solve this. Tried  different approaches but no luck</p>
<p><strong>Here is model summary</strong></p>
<pre><code>Layer (type)                Output Shape              Param #   
=================================================================
 embedding_18 (Embedding)    (None, 50, 50)            11100     
                                                                 
 bidirectional_35 (Bidirecti  (None, 50, 128)          58880     
 onal)                                                           
                                                                 
 bidirectional_36 (Bidirecti  (None, 64)               41216     
 onal)                                                           
                                                                 
 dropout_17 (Dropout)        (None, 64)                0         
                                                                 
 dense_35 (Dense)            (None, 64)                4160      
                                                                 
 dense_36 (Dense)            (None, 11)                715       
                                                                 
=================================================================
Total params: 116,071
Trainable params: 116,071
Non-trainable params: 0
_________________________________________________________________
</code></pre>
",Named Entity Recognition (NER),lstm named entity recognition model shape incompatible logits label different dimension tensorflow working nlp lstm named entity extraction model running different error detail error running code jupiter notebook tensorflow version input output length input sentence output sentece label added upto layer train model model use loss function categorical crossentropy get error valueerror shape none none incompatible use loss function sparse categorical crossentropy get error logits label must first dimension got logits shape label shape node sparse categorical crossentropy sparsesoftmaxcrossentropywithlogits sparsesoftmaxcrossentropywithlogits tried adding input shape first layer still luck tf kera layer input shape max len anyone help solve tried different approach luck model summary
"Generate modifiers for a keyword using dependency parsing, NLP","<p>I have a dataframe containing reviews of a particular product with the columns month and review. I want perform a type of text analysis on the review column, whereby you can query the for a particular keyword and it will return a list of modifiers for that keyword.</p>
<pre><code>df = pd.DataFrame({'month': ['Jan', 'Feb', 'Mar', 'Apr', 'Apr'],
                   'review': ['there should be 'share' button on each item. right now when my wife wants me to buy her something, she has to dictate the item id which is horrendous.', 'always nice but high prices', 'this app currently needs more than 3 gigs of space on my phone. that is ridiculous. guess it has to go. /edit cool, trying again, thanks for the answer.', 'impossible to login in the app, is there any way to get the barcode of the card? if i click the link in the email for the card print thingy it just shows a broken image.', 'i cannot change my location and language preference'],
                   'sentiment': [&quot;positive&quot;, &quot;negative&quot;, &quot;positive&quot;, &quot;negative&quot;, &quot;neutral&quot;]})
</code></pre>
<p>For example, say the reviews dataset is from the hospitality industry, and performed sentiment analysis. Upon checking the most frequent words in the positive and negative reviews, you got this.</p>
<p><strong>Positive</strong>: hotel, location, staff, view, room, breakfast</p>
<p><strong>Negative</strong>: hotel, staff, room, breakfast, window, bed, Wi-Fi</p>
<p>You wanted to go deeper into the analysis and uncover exactly what it was about these objects that were – or were not – working as expected by customers. For example, why were windows such a prominent aspect of negative reviews?</p>
<p>So, you  set out to create a syntactic dependency tree, which connects all terms in the input text according to their syntactic relation. Then, you queried this tree to pinpoint precisely what it was about a given keyword (for example, &quot;room&quot; or &quot;location&quot;) that customers did or did not especially like (<strong>this is where I need help, I don't know how to implement this in code</strong>)</p>
<p>I want a resulting list of modifiers so I can create word clouds to visualize the frequency of each modifier for the given keyword, such as the word cloud below, for the keyword &quot;room&quot;:</p>
<p><a href=""https://i.sstatic.net/izB1F.jpg"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Honestly I don't even know where to start, I'm currently working with spaCy's dependency parsing to see how it works and what it returns. So while I do that, I also seek help from here.</p>
",Named Entity Recognition (NER),generate modifier keyword using dependency parsing nlp dataframe containing review particular product column month review want perform type text analysis review column whereby query particular keyword return list modifier keyword example say review dataset hospitality industry performed sentiment analysis upon checking frequent word positive negative review got positive hotel location staff view room breakfast negative hotel staff room breakfast window bed wi fi wanted go deeper analysis uncover exactly wa object working expected customer example window prominent aspect negative review set create syntactic dependency tree connects term input text according syntactic relation queried tree pinpoint precisely wa given keyword example room location customer especially like need help know implement code want resulting list modifier create word cloud visualize frequency modifier given keyword word cloud keyword room enter image description honestly even know start currently working spacy dependency parsing see work return also seek help
What ({!Lookup.minorType == country}) means in JAPE for GATE,"<p>While I am going through JAPE(GATE) learning resources, came across below peace of JAPE rule which is eliminating the(bold formatted) text from becoming the annotation.</p>
<p>JAPE Rule: ({!Lookup.minorType == country})</p>
<p>Text: <strong>University of Sheffield</strong> US</p>
<p>What exactly the meaning of the above statement? My quick interpretation is <strong>minorType</strong> shouldn't be equal to type <strong>country</strong>. But if that is true why the below statements are not working in the same way as above?</p>
<p>({Lookup.minorType != country})</p>
<p>({Lookup.minorType == !country})</p>
<p>Any helpful links to understand LHS and RHS rule syntaxes in detailed manner would be appreciated.</p>
",Named Entity Recognition (NER),lookup minortype country mean jape gate going jape gate learning resource came across peace jape rule eliminating bold formatted text becoming annotation jape rule lookup minortype country text university sheffield u exactly meaning statement quick interpretation minortype equal type country true statement working way lookup minortype country lookup minortype country helpful link understand lh rh rule syntax detailed manner would appreciated
How to do sentiment analysis with topic modeling or NER [ Python]?,"<p>I have the following code for sentiment analysis. I was wondering how can I include topic modeling or NER within it? (the dataset is of customers' reviews of 3 websites, a csv file of 2 columns, one the reviews and one the rating of 0 as negative and 1 for positive)</p>
<pre><code> from nltk.corpus import stopwords
    from nltk.stem.porter import PorterStemmer
    
    dataset = pd.read_csv('full_db.csv') 
    X = dataset.iloc[:,0].values
    y = dataset.iloc[:, 1].values
    
    corpus = []
    
    for i in range(0, len(X)):
        review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])  #replace punctuations with space
        review = review.lower()  #transfering all the letters to lower-case
        review = review.split()  #spliting the review into words
        #apply stemming
        ps = PorterStemmer()
        all_stopwords = stopwords.words('english')
        no_stopwords = [&quot;not&quot;,&quot;don't&quot;,'aren','don','ain',&quot;aren't&quot;, 'couldn', &quot;couldn't&quot;, &quot;wasn't&quot;]
        for Nostopword in no_stopwords:
            all_stopwords.remove(Nostopword)
        review = [ps.stem(word) for word in review if not word in set(all_stopwords)] 
        review = ' '.join(review)
        corpus.append(review)

    #Splitting the dataset into Training set and Test set
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)
    
    #logistic regression
    # Initialize a logistic regression model 
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import confusion_matrix, accuracy_score,classification_report
    logistic = LogisticRegression(random_state=42, solver='lbfgs',
                                multi_class='multinomial')
    # Train the model
    logistic = logistic.fit(X_train, y_train)
    y_pred = logistic.predict(X_test)
</code></pre>
",Named Entity Recognition (NER),sentiment analysis topic modeling ner python following code sentiment analysis wa wondering include topic modeling ner within dataset customer review website csv file column one review one rating negative positive
Spacy Permission Error 13,"<p>I am getting Permission error 13 when trying to save a trained model in spacy. I have tried changing the directory as well. I am trying to reproduce this example given <a href=""https://spacy.io/usage/training#example-new-entity-typetrain_new_entity_type.py"" rel=""nofollow noreferrer"">here</a>, to train custom entities in spacy`s named entity recognizer.</p>

<pre><code>import random 

TRAIN_DATA = [
     (""Uber blew through $1 million a week"", {'entities': [(0, 4, 'ORG')]}),
     (""Google rebrands its business apps"", {'entities': [(0, 6, ""ORG"")]})
]

nlp = spacy.blank('en')
optimizer = nlp.begin_training()
for i in range(20):
    random.shuffle(TRAIN_DATA)
    for text, annotations in TRAIN_DATA:
        nlp.update([text], [annotations], sgd=optimizer)
nlp.to_disk('/model')
</code></pre>

<p>Here is the error I am getting</p>

<pre><code>PermissionError                           Traceback (most recent call last)
&lt;ipython-input-5-115363841730&gt; in &lt;module&gt;()
     14     for text, annotations in TRAIN_DATA:
     15         nlp.update([text], [annotations], sgd=optimizer)
---&gt; 16 nlp.to_disk('/model')

~/anaconda2/envs/py35/lib/python3.5/site-packages/spacy/language.py in to_disk(self, path, disable)
    596             serializers[name] = lambda p, proc=proc: proc.to_disk(p, vocab=False)
    597         serializers['vocab'] = lambda p: self.vocab.to_disk(p)
--&gt; 598         util.to_disk(path, serializers, {p: False for p in disable})
    599 
    600     def from_disk(self, path, disable=tuple()):

~/anaconda2/envs/py35/lib/python3.5/site-packages/spacy/util.py in to_disk(path, writers, exclude)
    508     path = ensure_path(path)
    509     if not path.exists():
--&gt; 510         path.mkdir()
    511     for key, writer in writers.items():
    512         if key not in exclude:

~/anaconda2/envs/py35/lib/python3.5/pathlib.py in mkdir(self, mode, parents, exist_ok)
   1214             self._raise_closed()
   1215         try:
-&gt; 1216             self._accessor.mkdir(self, mode)
   1217         except FileNotFoundError:
   1218             if not parents or self.parent == self:

~/anaconda2/envs/py35/lib/python3.5/pathlib.py in wrapped(pathobj, *args)
    369         @functools.wraps(strfunc)
    370         def wrapped(pathobj, *args):
--&gt; 371             return strfunc(str(pathobj), *args)
    372         return staticmethod(wrapped)
    373 

PermissionError: [Errno 13] Permission denied: '/model'
</code></pre>
",Named Entity Recognition (NER),spacy permission error getting permission error trying save trained model spacy tried changing directory well trying reproduce example given train custom entity spacy named entity recognizer error getting
Do I need to retrain Bert for NER to create new labels?,"<p>I am very new to natural language processing and I was thinking about working on named entity recognition NER. A friend of mine who works with NLP advised me to check out BERT, which I did. When reading the documentation and checking out the CoNLL-2003 data set, I noticed that the only labels are person, organization, location, miscellanious and outside. What if instead of outside, I want the model to recognize date, time, and other labels. I get that I would need a dataset labelled as such so, assuming that I have that, do I need to retrain BERT from stratch or can I somehow fine tune the existing model without needing to restart the whole process?</p>
",Named Entity Recognition (NER),need retrain bert ner create new label new natural language processing wa thinking working named entity recognition ner friend mine work nlp advised check bert reading documentation checking conll data set noticed label person organization location miscellanious outside instead outside want model recognize date time label get would need dataset labelled assuming need retrain bert stratch somehow fine tune existing model without needing restart whole process
spacy tokenizer is not recognizing period as suffix consistently,"<p>I have been working on a custom NER model to extract products that have strange identifiers that I can't control.</p>
<p>You can see from this example that in some cases it isn't picking up the period as a suffix. I added a custom tokenizer to handle products with hyphens (below). What do I need to add to handle this case and not jeopardize the other existing tokenization? Any input would be appreciated.</p>
<pre><code>issue_text = &quot;I really like stereo receivers, I want to buy the new ASX8E11F.&quot; 
print(nlp_custom_ner.tokenizer.explain(issue_text))

issue_text = &quot;I really like stereo receivers, I want to buy the new RK8BX.&quot; 
print(nlp_custom_ner.tokenizer.explain(issue_text))
</code></pre>
<p><strong>Output</strong></p>
<pre><code>[('TOKEN', 'I'), ('TOKEN', 'really'), ('TOKEN', 'like'), ('TOKEN', 'stereo'), ('TOKEN', 'receivers'), ('SUFFIX', ','), ('TOKEN', 'I'), ('TOKEN', 'want'), ('TOKEN', 'to'), ('TOKEN', 'buy'), ('TOKEN', 'the'), ('TOKEN', 'new'), ('TOKEN', 'ASX8E11F.')]

[('TOKEN', 'I'), ('TOKEN', 'really'), ('TOKEN', 'like'), ('TOKEN', 'stereo'), ('TOKEN', 'receivers'), ('SUFFIX', ','), ('TOKEN', 'I'), ('TOKEN', 'want'), ('TOKEN', 'to'), ('TOKEN', 'buy'), ('TOKEN', 'the'), ('TOKEN', 'new'), ('TOKEN', 'RK8BX'), ('SUFFIX', '.')]
</code></pre>
<p>I added a custom infix tokenizer to handle products with hyphens that is working.</p>
<pre><code>import spacy
from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER
from spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS
from spacy.util import compile_infix_regex

# Default tokenizer
nlp = spacy.load(&quot;en_core_web_sm&quot;)
doc = nlp(&quot;AXDR-PXXT-001&quot;)
print([t.text for t in doc])

# Modify tokenizer infix patterns
infixes = (
    LIST_ELLIPSES
    + LIST_ICONS
    + [
        r&quot;(?&lt;=[0-9])[+\-\*^](?=[0-9-])&quot;,
        r&quot;(?&lt;=[{al}{q}])\.(?=[{au}{q}])&quot;.format(
            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES
        ),
        r&quot;(?&lt;=[{a}]),(?=[{a}])&quot;.format(a=ALPHA),
        # ✅ Commented out regex that splits on hyphens between letters:
        # r&quot;(?&lt;=[{a}])(?:{h})(?=[{a}])&quot;.format(a=ALPHA, h=HYPHENS),
        r&quot;(?&lt;=[{a}0-9])[:&lt;&gt;=/](?=[{a}])&quot;.format(a=ALPHA),
    ]
)

infix_re = compile_infix_regex(infixes)
nlp.tokenizer.infix_finditer = infix_re.finditer
doc = nlp(&quot;AXDR-PXXT-001&quot;)
print([t.text for t in doc])
</code></pre>
<p><strong>Output</strong></p>
<pre><code>['AXDR', '-', 'PXXT-001']
['AXDR-PXXT-001']
</code></pre>
",Named Entity Recognition (NER),spacy tokenizer recognizing period suffix consistently working custom ner model extract product strange identifier control see example case picking period suffix added custom tokenizer handle product hyphen need add handle case jeopardize existing tokenization input would appreciated output added custom infix tokenizer handle product hyphen working output
how can i extract title and content in it from a list of urls in excel and save it to .txt file?,"<p>For each of the articles, given in the input.xlsx file, extract the article text and save the extracted article in a text file with URL_ID as its file name.
While extracting text, please make sure your program extracts only the article title and the article text. It should not extract the website header, footer, or anything other than the article text.</p>
<pre><code>i tried this code..

import requests
from bs4 import BeautifulSoup

url = 'https://www.ebay.com/b/Smart-Watches/178893/bn_152365'

response = requests.get(url)

soup = BeautifulSoup(response.content, 'html.parser')

Title = [title.text for name in soup.find_all('a', 
class_='title')]
Description= [Description.text for desc in soup.find_all('p', 
class_='description')]

for i in range(len(Title)):
print(Title[i])
print(Description[i])    
print()
def Save_to_file():
f = open('URL_ID.txt', 'w', encoding=&quot;utf-8&quot;)
for i in range(len(Title)):
    f.write(Title[i] + &quot;\n&quot;)
    f.write(Description[i] + &quot;\n&quot;)
f.close()
</code></pre>
<p>.....</p>
<p>NO errors and no output</p>
<p>....</p>
",Named Entity Recognition (NER),extract title content list url excel save txt file article given input xlsx file extract article text save extracted article text file url id file name extracting text please make sure program extract article title article text extract website header footer anything article text error output
Cast topic modeling outcome to dataframe,"<p>I have used <code>BertTopic</code> with <code>KeyBERT</code> to extract some <code>topics</code> from some <code>docs</code></p>
<pre><code>from bertopic import BERTopic
topic_model = BERTopic(nr_topics=&quot;auto&quot;, verbose=True, n_gram_range=(1, 4), calculate_probabilities=True, embedding_model='paraphrase-MiniLM-L3-v2', min_topic_size= 3)
topics, probs = topic_model.fit_transform(docs)
</code></pre>
<p>Now I can access the <code>topic name</code></p>
<pre><code>freq = topic_model.get_topic_info()
print(&quot;Number of topics: {}&quot;.format( len(freq)))
freq.head(30)

   Topic    Count   Name
0   -1       1     -1_default_greenbone_gmp_manager
1    0      14      0_http_tls_ssl tls_ssl
2    1      8       1_jboss_console_web_application
</code></pre>
<p>and inspect the topics</p>
<pre><code>[('http', 0.0855701486234524),          
 ('tls', 0.061977919455444744),
 ('ssl tls', 0.061977919455444744),
 ('ssl', 0.061977919455444744),
 ('tcp', 0.04551718585531556),
 ('number', 0.04551718585531556)]

[('jboss', 0.14014705432060262),
 ('console', 0.09285308122803233),
 ('web', 0.07323749337563096),
 ('application', 0.0622930523123512),
 ('management', 0.0622930523123512),
 ('apache', 0.05032395169459188)]
</code></pre>
<p>What I want is to have a final data<code>frame</code> that has in one <code>column</code> the <code>topic name</code> and in another <code>column</code> the elements of the <code>topic</code></p>
<pre><code>expected outcome:

  class                         entities
o http_tls_ssl tls_ssl           HTTP...etc
1 jboss_console_web_application  JBoss, console, etc
</code></pre>
<p>and one dataframe with the topic name on different columns</p>
<pre><code>  http_tls_ssl tls_ssl           jboss_console_web_application
o http                           JBoss
1 tls                            console
2 etc                            etc
</code></pre>
<p>I did not find out how to do this. Is there a way?</p>
",Named Entity Recognition (NER),cast topic modeling outcome dataframe used extract access inspect topic want final data ha one another element one dataframe topic name different column find way
Build Spacy NER Loop for Dataframe,"<p>currently I want to perform spacy NER on all text files in my directory and have as output &quot;Number of NER/Total Words in Text&quot;. I dont know how to automate it. Currently I use:</p>
<pre><code>def read_txt_files(PATH:str):
    
    results = defaultdict(list)
    for file in Path(PATH).iterdir():
        with open(file, &quot;rt&quot;,newline='', encoding=&quot;utf8&quot;) as file_open:
            results[&quot;file_num&quot;].append(file.name)
            results[&quot;text&quot;].append(file_open.read().replace('\n',&quot; &quot;))
    df = pd.DataFrame(results)
    
    return df
def Specificity(input_data: pd.Series):
    specificity = [0]*len(input_data)
    
    for i in tqdm(range(len(input_data)), desc = 'Get the Specificity'):
        specificity[i] = len((ner(input_data[i])).ents)/len((input_data[i]))
    
    #[len(ner(data[i]).ents)/len(data[i]) for i in tqdm(range(len(data)))]
    
    return specificity
</code></pre>
<p>But it somehow just shows the wrong values for specificity, much lower than it should be.</p>
<p>When I perform NER on a single text file it looks like this:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_sm&quot;)
text = open(r&quot;mydirectory&quot;, 'r',encoding='utf-8').read()
parsed_text = nlp(text)
named_entities = parsed_text.ents
num_words = len ([ token
    for token in parsed_text
    if not token . is_punct ])
num_entities = len ( named_entities )
specificity_score = num_entities/num_words
</code></pre>
<p>Is there a way to &quot;switch&quot; both specificity measures and let the &quot;second&quot; code perform?</p>
",Named Entity Recognition (NER),build spacy ner loop dataframe currently want perform spacy ner text file directory output number ner total word text dont know automate currently use somehow show wrong value specificity much lower perform ner single text file look like way switch specificity measure let second code perform
BERT NER detect multiple words as one entity,"<p>I' using bert to train custom ner model.i'm using simpletransformer pacakge. I have 2 custom entity - place, other</p>
<p>In dataset as for word column I have multiple words for particular label in row eg</p>
<p>Sentence_id |words         |labels
17.         |united states |place
17.         |south Africa. |place
Eg  have sentence
Hi I'm XYZ from United states</p>
<p>While predicting model is predicting output for each word. I want model to take 2 words for predicting ner. Eg instead of united it should use united states as entity</p>
<p>Is there any way or configuration that where we can pass numerical of words(n-grams) that model should take</p>
",Named Entity Recognition (NER),bert ner detect multiple word one entity using bert train custom ner model using simpletransformer pacakge custom entity place dataset word column multiple word particular label row eg sentence id word label united state place south africa place eg sentence hi xyz united state predicting model predicting output word want model take word predicting ner eg instead united use united state entity way configuration pas numerical word n gram model take
Can I generate SPARQL queries automatically from natural language statement in apache jena API?,"<p>I have built a domain specific ontology. And I need a framework over it where user asks a query in natural language and the system turns this query to the SPARQL query automatically. If somehow I extract named entities and query word from the natural language query  statement...  What are the possibilities to have a simple SPARQL query with it. Which tools and techniques might help me?</p>
<p>Note: I only have simple factoid questions right now... not the complicated ones.</p>
<p>I have domain specific ontology. And I need a framework over it where user asks a query in natural language and the system turns this query to the SPARQL query automatically. Can Apache jena API work for it?</p>
",Named Entity Recognition (NER),generate sparql query automatically natural language statement apache jena api built domain specific ontology need framework user asks query natural language system turn query sparql query automatically somehow extract named entity query word natural language query statement possibility simple sparql query tool technique might help note simple factoid question right complicated one domain specific ontology need framework user asks query natural language system turn query sparql query automatically apache jena api work
spaCy cannot find method add_label for EntityRecognizer class in python,"<p>I want to update the entity recognizer in spaCy with an own example. After getting the instance of the Named Entity Recognizer pipeline component I want to call the method <code>add_label()</code> on it:</p>
<pre><code>import spacy

nlp = spacy.load('en_core_web_sm')
ner = nlp.get_pipe('ner')
ner.add_label()
</code></pre>
<p>I cannot call <code>add_label()</code>, because it <code>Cannot find reference 'add_label' in 'Pipe | Pipe</code>. I first thought that <code>add_label()</code> might be deprecated in spaCy v3, but the <a href=""https://spacy.io/api/entityrecognizer#add_label"" rel=""nofollow noreferrer"">docs</a> implies that it is not. The pipeline also includes the <code>ner</code> component:</p>
<pre><code>print(nlp.pipe_names)
output: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']
</code></pre>
<p>Further informations:</p>
<ul>
<li>spaCy version 3.4.3</li>
<li>Pipelines en_core_web_sm (3.4.1)</li>
<li>Python version 3.10.4</li>
</ul>
<p>Does anyone have an idea why I cannot call <code>add_label()</code> on the <code>EntityRecognizer</code>?</p>
<p>Update: Seems to be a problem with <strong>PyCharm (17.0.5+1-b653.14 x86_64)</strong>. Cannot reproduce the issue using <strong>VS Code</strong>.</p>
",Named Entity Recognition (NER),spacy find method add label entityrecognizer class python want update entity recognizer spacy example getting instance named entity recognizer pipeline component want call method call first thought might deprecated spacy v doc implies pipeline also includes component information spacy version pipeline en core web sm python version doe anyone idea call update seems problem pycharm b x reproduce issue using v code
"E966 `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component","<pre><code>nlp = spacy.blank('en')

def train_model(train_data):
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last = True)
    
    for _, annotation in train_data:
        for ent in annotation['entities']:
            ner.add_label(ent[2])
            
    
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        optimizer = nlp.begin_training()
        for itn in range(10):
            print(&quot;Statring iteration &quot; + str(itn))
            random.shuffle(train_data)
            losses = {}
            index = 0
            for text, annotations in train_data:
                try:
                    nlp.update(
                        [text], 
                        [annotations], 
                        drop=0.2,
                        sgd=optimizer,
                        losses=losses)
                except Exception as e:
                    pass
                
            print(losses)
    
</code></pre>
<pre><code>train_model(train_data)
</code></pre>
<p>Error:</p>
<p>[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got &lt;spacy.pipeline.ner.EntityRecognizer object at 0x7ff8f26c3a50&gt; (name: 'None').</p>
<ul>
<li><p>If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.</p>
</li>
<li><p>If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.</p>
</li>
<li><p>If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.</p>
</li>
</ul>
<p>How do I train the model?</p>
",Named Entity Recognition (NER),e take string name registered component factory callable component error e take string name registered component factory callable component expected string got spacy pipeline ner entityrecognizer object x ff f c name none created component remove nlp create pipe call instead passed component like call string name instead e g using custom component add decorator function component class component factory custom component assign name e g run add pipeline train model
lemmatizing verbs in SVOs,"<p><em>I've looked at the suggested similar questions, and I think that this question has enough specificity that it warrants being asked, but I am completely okay if someone can point to an already answered question that solves my problem.</em></p>
<p>I have a corpus of texts which I have rendered as (textacy) SVOs, and then saved to a data frame, only to discover that it would be better if the verbs in question were lemmatized so that &quot;talk&quot;, &quot;talked&quot;, &quot;was talking&quot;, &quot;is talking&quot;, etc. are rendered as the same verb and not 4 or more different verbs.</p>
<p>The current code grabs the texts out of a data frame and places them in a list:</p>
<pre class=""lang-py prettyprint-override""><code>texts_women = talks_f.text.tolist()
texts_w = [text.lower() for text in texts_women]
</code></pre>
<p>Then it creates a spaCy pipe and runs the texts through it:</p>
<pre class=""lang-py prettyprint-override""><code>nlp = spacy.load('en_core_web_sm')
docs_w = list(nlp.pipe(texts_w))
</code></pre>
<p>I had written the following code before realizing that the verbs could use being normalized:</p>
<pre class=""lang-py prettyprint-override""><code>def createSVOs(doc, svo_list):
    # Create the list of tuples for the document
    svotriples = list(textacy.extract.triples.subject_verb_object_triples(doc))
    # Convert to list of dictionaries
    for item in svotriples:
        svo_list.append(
            {
                'subject': item[0][-1], 
                'verb': item[1][-1], 
                'object': item[2]
            }
        )
</code></pre>
<p>Originally we had converted the dictionary values to strings -- `'subject': str(item[0][-1]) -- and then this list of dictionaries was converted to a pandas dataframe, where we do a number of other things.</p>
<p>Backtracking into the code, I first tried lemmatizing the df['verb'] column, with a variety of errors depending on what I tried, with most of them being:</p>
<pre class=""lang-py prettyprint-override""><code>TypeError: 'spacy.tokens.token.Token' object is not subscriptable
</code></pre>
<p>I eventually decided to try to lemmatize the docs before I fed them into the textacy SVO creator, but then I get:</p>
<pre class=""lang-py prettyprint-override""><code>AttributeError: 'str' object has no attribute 'sents'
</code></pre>
<p>Is it possible to get both SVOs and lemmas? That is, an SVO where the V, verb, is lemmatized? What am I missing here?</p>
",Named Entity Recognition (NER),lemmatizing verb svos looked suggested similar question think question ha enough specificity warrant asked completely okay someone point already answered question solves problem corpus text rendered textacy svos saved data frame discover would better verb question lemmatized talk talked wa talking talking etc rendered verb different verb current code grab text data frame place list creates spacy pipe run text written following code realizing verb could use normalized originally converted dictionary value string subject str item list dictionary wa converted panda dataframe number thing backtracking code first tried lemmatizing df verb column variety error depending tried eventually decided try lemmatize doc fed textacy svo creator get possible get svos lemma svo v verb lemmatized missing
How to extract the output froman NLP model to a dataframe?,"<p>I have trained an NLP Model (NER) and I have results in the below format:</p>
<pre><code>for text, _ in TEST_DATA:
    doc = nlp(text)
    print([(ent.text, ent.label_) for ent in doc.ents])

#Output
[('1131547', 'ID'), ('12/9/2019', 'Date'), ('USA', 'ShippingAddress')]
[('567456', 'ID'), ('Hills', 'ShippingAddress')]

#I need the output in the below format

ID       Date     ShippingAddress 
1131547 12/9/2019 USA     
567456    NA      Hills    
</code></pre>
<p>Thanks for your help in advance</p>
",Named Entity Recognition (NER),extract output froman nlp model dataframe trained nlp model ner result format thanks help advance
"How to replace multiple substrings in a long string, while continuously updating indices","<p>Lets say I have a fixed string such as:</p>
<p><code>&quot;Jacob went to the park to play&quot;</code></p>
<p>And, I put this string through a BERT NER model to identify entities; for the sake of argument, lets assume that the model only returned &quot;Jacob&quot; as a person, but decomposed it into several subwords, so the NER output would be something like:</p>
<p><code> ents = [{'word_piece':'##Ja'','start'0:,'end'2:,'tag':'B-PER'},{'word_piece':'##co'','start'2:,'end'4:,'tag':'B-PER'}, {'word_piece':'##b'','start'4:,'end'5:,'tag':'I-PER'}]</code></p>
<p>(this isn't the actual format, but contains all the information necessary. Lets say that <strong>all</strong> entities have a char length of 5 i.e. I-PER, B-PER, I-ORG etc.</p>
<p>I want to put the tokens back into the string, so it would be something like this:</p>
<p><code>&quot;B-PER B-PER I-PER went to the park to play&quot;</code></p>
<p>I.e. I want to loop through each entity, then update the string, and the string indices.</p>
<p>I have tried looping through each example, adding 7 (as all entities are = 5 + spaces on either side) * number of times an element has been replaced + len(word_piece.replace(##))...</p>
<pre><code>i = 0
for eg in ents:
   start = i*7 + eg['start']
   end = i*7 + eg['end'] + len(eg['start']-eg['end'])
   stem = sent[:start]
   tail = sent[end:]
   sent = stem + ' ' + eg['tag'] + ' ' + tail
   i += 1
   
</code></pre>
<p>But I still cannot seem to get it right. The offsets begin to accumulate and lead to a jumble.</p>
",Named Entity Recognition (NER),replace multiple substring long string continuously updating index let say fixed string put string bert ner model identify entity sake argument let assume model returned jacob person decomposed several subwords ner output would something like actual format contains information necessary let say entity char length e per b per org etc want put token back string would something like e want loop entity update string string index tried looping example adding entity space either side number time element ha replaced len word piece replace still seem get right offset begin accumulate lead jumble
"Extract information about education institute, grades, year and degree from text using NLP in Python","<p>I want to extract information about education institute, degree, year of passing and grades (CGPA/GPA/Percentage) from text using NLP in Python.
For example, if I have the input:</p>
<blockquote>
<p>NBN Sinhgad School Of Engineering,Pune 2016 - 2020 Bachelor of Engineering Computer Science CGPA: 8.78 Vidya Bharati Chinmaya Vidyalaya,Jamshedpur 2014 - 2016 Intermediate-PCM,Economics CBSE Percentage: 88.8 Vidya Bharati Chinmaya Vidyalaya,Jamshedpur 2003 - 2014 Matriculation,CBSE CGPA: 8.6 EXPERIENCE</p>
</blockquote>
<p>I want the ouput:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>[{
  ""Institute"": ""NBN Sinhgad School Of Engineering"",
  ""Degree"": ""Bachelor of Engineering Computer Science"",
  ""Grades"": ""8.78"",
  ""Year of Passing"": ""2020""
}, {
  ""Institute"": ""Vidya Bharati Chinmaya Vidyalaya"",
  ""Degree"": ""Intermediate-PCM,Economics"",
  ""Grades"": ""88.8"",
  ""Year of Passing"": ""2016""
}, {
  ""Institute"": ""Vidya Bharati Chinmaya Vidyalaya"",
  ""Degree"": ""Matriculation,CBSE"",
  ""Grades"": ""8.6"",
  ""Year of Passing"": ""2014""
}]</code></pre>
</div>
</div>
</p>
<p>Can it be done without training any custom NER model? Is there any pre-trained NER available to do this?</p>
",Named Entity Recognition (NER),extract information education institute grade year degree text using nlp python want extract information education institute degree year passing grade cgpa gpa percentage text using nlp python example input nbn sinhgad school engineering pune bachelor engineering computer science cgpa vidya bharati chinmaya vidyalaya jamshedpur intermediate pcm economics cbse percentage vidya bharati chinmaya vidyalaya jamshedpur matriculation cbse cgpa experience want ouput done without training custom ner model pre trained ner available
Is there a way to set up fuzzy string matching in spaCy EntityRuler?,"<p>I want to extract some entities from a text and I want to give spaCy a higher &quot;grade of liberty&quot; so that a string can be recognized even if it doesn't match perfectly.</p>
<pre><code>text_to_parse = 'I really like stack-overflow'

ruler = nlp.add_pipe('entity_ruler', before='ner', config={&quot;phrase_matcher_attr&quot;: &quot;LOWER&quot;})

patterns = [
                {'label': 'THING', 'pattern': 'stackoverflow'},
            ]

ruler.add_patterns(patterns)
</code></pre>
<p>In this case the string is not matched because of the dash (-).</p>
<p>I want spacy to match all the possible variants within a certain limit (for example 80% match) so that strings like &quot;stickoverflow&quot; or &quot;stack-overflow&quot; are recognized.</p>
<p>Usually I rely on libraries like fuzzysearch or fuzzywuzzy but I wonder if there is a way to do so in spaCy EntityRuler.</p>
",Named Entity Recognition (NER),way set fuzzy string matching spacy entityruler want extract entity text want give spacy higher grade liberty string recognized even match perfectly case string matched dash want spacy match possible variant within certain limit example match string like stickoverflow stack overflow recognized usually rely library like fuzzysearch fuzzywuzzy wonder way spacy entityruler
How to create a simple Italian Model for a Named Entity Extraction of Persons using OpenNLP?,"<p>I have to do a project with OpenNLP, strictly in italian language. Since it's almost impossible to find some existing structures in this language, my idea is to create a simple model myself. Reading some <a href=""https://stackoverflow.com/questions/46950036/opennlp-model-builder-addon-doesnt-continue"">posts</a> on this platform, my idea is try to do this using <a href=""https://github.com/apache/opennlp-addons/tree/master/modelbuilder-addon"" rel=""nofollow noreferrer"">model-builder addon</a>.</p>
<p>First of all, it's possible to obtain my goal with this addon?
If so, referring to this other <a href=""https://stackoverflow.com/questions/20509678/opennlp-foreign-names-does-not-get-recognized/21323571#21323571"">post</a>, what kind of file is meant by &quot;modelOutFile&quot;? In my case I don't have an existing model.
N.B.: the addon uses some deprecated functions (such as <code>nameFinderME.train()</code>).</p>
<p>Naively, I tried to pass as a &quot;modelOutFile&quot; a simple empty file &quot;model.bin&quot;, but, of course I bumped into an error:</p>
<blockquote>
<p>Cannot invoke &quot;java.util.Properties.getProperty(String)&quot; because &quot;manifest&quot; is null</p>
</blockquote>
<p>Furthermore, I used a few names and sentences for the test (I only wanted to know if this worked), not the large amount requested (15000 sentences at least).
I'm open to other suggestions instead of the use of modelbuilder addons.
Hope someone can help me.</p>
",Named Entity Recognition (NER),create simple italian model named entity extraction person using opennlp project opennlp strictly italian language since almost impossible find existing structure language idea create simple model reading model builder addon first possible obtain goal addon referring href kind file meant quot modeloutfile quot case existing model n b addon us deprecated function p naively tried pas modeloutfile simple empty file model bin course bumped error invoke java util property getproperty string manifest null furthermore used name sentence test wanted know worked large amount requested sentence least open suggestion instead use modelbuilder addons hope someone help
construct dataset for ner train,"<p>i have in input :</p>
<pre><code>text = &quot;Apple est une entreprise, James Alfred travaille ici&quot;
spans = [
    {
&quot;start&quot;:0,
&quot;end&quot;:5,
&quot;label&quot;:&quot;ORG&quot;
},
{
&quot;start&quot;:26,
&quot;end&quot;:38,
&quot;label&quot;:&quot;PER&quot;
}
]

correspondance_dict = {&quot;PER&quot;:2, &quot;ORG&quot;: 4 , &quot;O&quot; : 0}
</code></pre>
<p>i want to tokenize the text and construct label according to spans list i.e :</p>
<p>i want to have in output :</p>
<pre><code>tokenized_text = [&quot;Apple&quot;, &quot;est&quot;, &quot;une&quot;, &quot;entreprise&quot;, &quot;,&quot; , &quot;James&quot;,&quot;Alfred&quot;, &quot;travaille&quot;, &quot;ici&quot;]
labels = [4,0,0,0,0,2,2,0,0]  #this list constructed with correspondance_dict and spans (4 because Apple is ORG and  the &quot;2,2&quot; because &quot;James,Alfred&quot; is person 
</code></pre>
",Named Entity Recognition (NER),construct dataset ner train input want tokenize text construct label according span list e want output
Spark NLP Evaluation (com.johnsnowlabs.nlp.eval._) for Scala 2.12?,"<p>Spark NLP Evaluation <code>com.johnsnowlabs.nlp.eval._</code> for Scala <code>2.12.x</code> not available. With  spark version <code>3.x</code>, it's not working with libraryDependency <code>&quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-eval&quot; % &quot;2.2.2&quot;</code> in sbt. There is no repo for Scala version <code>2.12.x</code> at
<a href=""https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/"" rel=""nofollow noreferrer"">https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/</a></p>
<p>The procedure given for Evaluating NER DL at <a href=""https://github.com/JohnSnowLabs/spark-nlp/blob/master/docs/en/evaluation.md#evaluating-ner-dl"" rel=""nofollow noreferrer"">https://github.com/JohnSnowLabs/spark-nlp/blob/master/docs/en/evaluation.md#evaluating-ner-dl</a> is not working.</p>
<p>Anyone knows how to perform NER evaluation in Spark NLP using Scala ?</p>
",Named Entity Recognition (NER),spark nlp evaluation com johnsnowlabs nlp eval scala spark nlp evaluation scala available spark version working librarydependency sbt repo scala version procedure given evaluating ner dl working anyone know perform ner evaluation spark nlp using scala
How can I get specific columns form txt file and save them to new file using python,"<p>I have this txt file <strong>sentences.txt</strong> that contains texts below</p>
<pre><code>a01-000u-s00-00 0 ok 154 19 408 746 1661 89 A|MOVE|to|stop|Mr.|Gaitskell|from
</code></pre>
<p><code>a01-000u-s00-01 0 ok 156 19 395 932 1850 105 nominating|any|more|Labour|life|Peers</code></p>
<p>which contains 10 columns
I want to use the panda's data frame to extract only the file name (at column 0) and corresponding text (column 10) without the <strong>(|)</strong> character
I wrote this code</p>
<pre><code>def load() -&gt; pd.DataFrame:

 df = pd.read_csv('sentences.txt',sep=' ', header=None)
 data = []
 with open('sentences.txt') as infile:
    for line in infile:
        file_name, _, _, _, _, _, _, _, _, text = line.strip().split(' ')
        data.append((file_name, cl_txt(text)))

 df = pd.DataFrame(data, columns=['file_name', 'text'])
 df.rename(columns={0: 'file_name', 9: 'text'}, inplace=True)
 df['file_name'] = df['file_name'].apply(lambda x: x + '.jpg')
 df = df[['file_name', 'text']]
 return df

def cl_txt(input_text: str) -&gt; str:
 text = input_text.replace('+', '-')
 text = text.replace('|', ' ')
 return text

load()
</code></pre>
<p>the error I got</p>
<p>ParserError: Error tokenizing data. C error: Expected 10 fields in line 4, saw 11</p>
<p>where my expected process.txt file results should look like below without \n</p>
<pre><code>a01-000u-s00-00  A MOVE to stop Mr. Gaitskell from
a01-000u-s00-01  nominating any more Labour life Peers
</code></pre>
<p><a href=""https://i.sstatic.net/udNIh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/udNIh.png"" alt=""enter image description here"" /></a></p>
",Named Entity Recognition (NER),get specific column form txt file save new file using python txt file sentence txt contains text contains column want use panda data frame extract file name column corresponding text column without character wrote code error got parsererror error tokenizing data c error expected field line saw expected process txt file result look like without n
"Extract two specified words from the dataframe and place them in a new column, then delete the rows","<p>This is the dataframe:</p>
<pre><code>data = {&quot;Company&quot; : [[&quot;ConsenSys&quot;] , [&quot;Cognizant&quot;], [&quot;IBM&quot;], [&quot;IBM&quot;], [&quot;Reddit, Inc&quot;], [&quot;Reddit, Inc&quot;], [&quot;IBM&quot;]],
&quot;skills&quot; : [['services', 'scientist technical expertise', 'databases'], ['datacomputing tools experience', 'deep learning models', 'cloud services'], ['quantitative analytical projects', 'financial services', 'field experience'],
['filesystems server architectures', 'systems', 'statistical analysis', 'data analytics', 'workflows', 'aws cloud services'], ['aws services'], ['data mining statistics', 'statistical analysis', 'aws cloud', 'services', 'data discovery', 'visualization'], ['communication skills experience', 'services', 'manufacturing environment', 'sox compliance']]}

dff = pd.DataFrame(data)
dff
</code></pre>
<ul>
<li>I need to create a new column, and I want to start by taking specific
words out of the skills column.</li>
<li>The row that does not include those specific words should then be
deleted.</li>
<li>Specific words: 'services', 'statistical analysis'</li>
</ul>
<p>Expected Output:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>Company</th>
<th>skills</th>
<th>new_col</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>[ConsenSys]</td>
<td>[services, scientist technical expertise, databases]</td>
<td>[services]</td>
</tr>
<tr>
<td>1</td>
<td>[IBM]</td>
<td>[filesystems server architectures, systems, statistical analysis, data analytics, workflows, aws cloud services]</td>
<td>[services, statistical analysis]</td>
</tr>
<tr>
<td>2</td>
<td>[Reddit, Inc]</td>
<td>[data mining statistics, statistical analysis, aws cloud, services, data discovery, visualization]</td>
<td>[statistical analysis]</td>
</tr>
<tr>
<td>3</td>
<td>[IBM]</td>
<td>['communication skills experience', 'services', 'manufacturing environment', 'sox compliance']</td>
<td>[services]</td>
</tr>
</tbody>
</table>
</div>
<p>I tried quite a lot of code in an effort to extract a specific word from the one that was available on Stack Overflow, but I was unsuccessful.</p>
",Named Entity Recognition (NER),extract two specified word dataframe place new column delete row dataframe need create new column want start taking specific word skill column row doe include specific word deleted specific word service statistical analysis expected output company skill new col consensys service scientist technical expertise database service ibm filesystems server architecture system statistical analysis data analytics workflow aws cloud service service statistical analysis reddit inc data mining statistic statistical analysis aws cloud service data discovery visualization statistical analysis ibm communication skill experience service manufacturing environment sox compliance service tried quite lot code effort extract specific word one wa available stack overflow wa unsuccessful
Convert spaCy `Doc` into CoNLL 2003 sample,"<p>I was planning to <a href=""https://towardsdatascience.com/named-entity-recognition-ner-with-bert-in-spark-nlp-874df20d1d77"" rel=""nofollow noreferrer"">train a Spark NLP custom NER model</a>, which uses the CoNLL 2003 format to do so (this blog even leaves some <a href=""https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/resources/conll2003"" rel=""nofollow noreferrer"">traning sample data</a> to speed-up the follow-up). This &quot;sample data&quot; is NOT useful for me, as I have my own training data to train a model with; this data however, consists of a list of spaCy <a href=""https://spacy.io/api/doc"" rel=""nofollow noreferrer"">Doc</a> objects and quite honestly, I don't know how to carry on with this conversion. I have found three approaches so far, each with some <em>considerable</em> weakness:</p>
<ol>
<li><p>In spaCy's documentation, I have found <a href=""https://spacy.io/universe/project/spacy-conll"" rel=""nofollow noreferrer"">an example code</a> about how to build a SINGLE Doc to CoNLL using <a href=""https://github.com/BramVanroy/spacy_conll"" rel=""nofollow noreferrer""><code>spacy_conll</code> project</a>, but notice it uses a blank spacy model, so it is not clear where &quot;my own labeled data&quot; comes to play; <a href=""https://github.com/BramVanroy/spacy_conll#in-python"" rel=""nofollow noreferrer"">furthermore</a>, it seems <code>conll_formatter</code> component is &quot;added at the end of the pipeline&quot;, so it seems &quot;no direct conversion from Doc to CoNLL is actually done&quot;... Is my grasping correct?</p>
</li>
<li><p>In Prodigy forum (another product of the same designers of spaCy), I found <a href=""https://support.prodi.gy/t/ner-format-to-conll/1153"" rel=""nofollow noreferrer"">this purposal</a>, however that &quot;CoNLL&quot; (2003 I suppose?) format seems to be incomplete: the POS tag seems to be missing (which can be easily obtained via <a href=""https://spacy.io/api/token#attributes"" rel=""nofollow noreferrer""><code>Token.pos_</code></a>, as well as the <em>&quot;Syntactic chunk&quot;</em> (whose spaCy equivalent, does not seem to exist). These four fields are mentioned in <a href=""https://www.clips.uantwerpen.be/conll2003/ner/#:%7E:text=Spanish%20and%20Dutch.-,Software%20and%20Data,-The%20CoNLL%2D2003"" rel=""nofollow noreferrer"">CoNLL 2003 official documentation</a>.</p>
</li>
<li><p>Speaking of a &quot;direct conversion from Doc to CoNLL&quot;, I have also found <a href=""https://github.com/explosion/spaCy/issues/5188#issuecomment-602848635"" rel=""nofollow noreferrer"">this</a> implementation based on <code>textacy</code> library, but it seems this implementation got deprecated by version <a href=""https://github.com/chartbeat-labs/textacy/blob/main/CHANGES.md#0110-2021-04-12"" rel=""nofollow noreferrer"">0.11.0</a>, because <em>&quot;CONLL-U [...] wasn't enforced or guaranteed&quot;</em> , so I am not sure whether to use it or not (BTW, the most up-to-date <code>textacy</code> implementation when writing these lines, is <code>0.12.0</code>)</p>
</li>
</ol>
<p>My current code looks like:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.training import offsets_to_biluo_tags
from spacy.tokens import Span

print(&quot;SPACY HELPER MODEL&quot;)
base_model = &quot;en_core_web_sm&quot;
nlp = spacy.load(base_model)
to_disable= ['parser', 'lemmatizer', 'ner']
_ = [nlp.remove_pipe(item) for item in to_disable]
print(&quot;Base model used: &quot;, base_model)
print(&quot;Removed components: &quot;, to_disable)
print(&quot;Enabled components: &quot;, nlp.pipe_names)

# Assume text is already available as sentences...
# so no need for spaCy `sentencizer` or similar
print(&quot;\nDEMO SPACY DOC LIST BUILDING...&quot;, end=&quot;&quot;)
doc1 = nlp(&quot;iPhone X is coming.&quot;)
doc1.ents = [Span(doc1, 0, 2, label=&quot;GADGET&quot;)]
doc2 = nlp(&quot;Space X is nice.&quot;)
doc2.ents = [Span(doc1, 0, 2, label=&quot;BRAND&quot;)]
docs = [doc1, doc2]
print(&quot;DONE!&quot;)

print(&quot;\nCoNLL 2003 CONVERSION:\n&quot;)
results = []
for doc in docs:
    # Preliminary: whole sentence
    whole_sentence = doc.text
    # 1st item (CoNLL 2003): word
    words = [token.text for token in doc]
    # 2nd item (CoNLL 2003): POS
    pos = [token.tag_ for token in doc]
    # 3rd item (CoNLL 2003): syntactic chunk tag
    sct = [&quot;[UNKNOWN]&quot; for token in doc]
    # 4th item (CoNLL 2003): named entities
    spacy_entities = [
        (ent.start_char, ent.end_char, ent.label_)
        for ent in doc.ents
    ]
    biluo_entities = offsets_to_biluo_tags(doc, spacy_entities)
    results.append((whole_sentence, words, pos, sct, biluo_entities))

for result in results:
    print(
        &quot;\nDOC TEXT (NOT included in CoNLL 2003, just for demo): &quot;,
        result[0], &quot;\n&quot;
    )
    print(&quot;-DOCSTART- -X- -X- O&quot;)
    for w,x,y,z in zip(result[1], result[2], result[3], result[4]):
        print(w,x,y,z)

# Pending: write to a file, but that's easy, and out of topic.
</code></pre>
<p>Which gives as output:</p>
<pre><code>DOC TEXT (NOT included in CoNLL 2003, just for demo):  iPhone X is coming.

-DOCSTART- -X- -X- O
iPhone NNP [UNKNOWN] B-GADGET
X NNP [UNKNOWN] L-GADGET
is VBZ [UNKNOWN] O
coming VBG [UNKNOWN] O
. . [UNKNOWN] O

DOC TEXT (NOT included in CoNLL 2003, just for demo):  Space X is nice.

-DOCSTART- -X- -X- O
Space NNP [UNKNOWN] B-BRAND
X NNP [UNKNOWN] L-BRAND
is VBZ [UNKNOWN] O
nice JJ [UNKNOWN] O
. . [UNKNOWN] O
</code></pre>
<p>Have you done something like this before?</p>
<p>Thanks!</p>
",Named Entity Recognition (NER),convert spacy conll sample wa planning train spark nlp custom ner model us conll format blog even leaf traning sample data speed follow sample data useful training data train model data however consists list spacy doc object quite honestly know carry conversion found three approach far considerable weakness spacy documentation found example code build single doc conll using project notice us blank spacy model clear labeled data come play furthermore seems component added end pipeline seems direct conversion doc conll actually done grasping correct prodigy forum another product designer spacy found purposal however conll suppose format seems incomplete po tag seems missing easily obtained via well syntactic chunk whose spacy equivalent doe seem exist four field mentioned conll official documentation speaking direct conversion doc conll also found implementation based library seems implementation got deprecated version conll u enforced guaranteed sure whether use btw date implementation writing line current code look like give output done something like thanks
Training custom SpaCy NER model gives training error,"<p>I want to train my own custom NER with SpaCy for recogrnizing addresses.</p>
<p>This is my data:</p>
<pre><code>training_data = [('send to: Aargauerstrasse 8005', {'entities': [(9, 28, 'ADDRESS')]}), 
                ('send to: Abeggweg 8057', {'entities': [(9, 21, 'ADDRESS')]}), 
                ('send to: Abendweg 8038', {'entities': [(9, 21, 'ADDRESS')]}), 
                ('send to: Ackermannstrasse 8044', {'entities': [(9, 29, 'ADDRESS')]}), 
                ('send to: Aehrenweg 8050', {'entities': [(9, 22, 'ADDRESS')]}), 
                ('send to: Aemmerliweg 8050', {'entities': [(9, 24, 'ADDRESS')]}), 
                ('send to: Albisgütliweg 8045', {'entities': [(9, 26, 'ADDRESS')]}), 
                ('send to: Albisstrasse 8038', {'entities': [(9, 25, 'ADDRESS')]}), 
                ('send to: Albulastrasse 8048', {'entities': [(9, 26, 'ADDRESS')]}), 
                ('send to: Alderstrasse 8008', {'entities': [(9, 25, 'ADDRESS')]})]
</code></pre>
<p>I have followed this tutorial (official tutorial... 20min 30sec):
<a href=""https://www.youtube.com/watch?v=IqOJU1-_Fi0&amp;t=1328s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=IqOJU1-_Fi0&amp;t=1328s</a></p>
<p>These are my functions:</p>
<pre><code># CREATING BLANK MODEL
def create_blank_nlp(train_data):

    nlp = spacy.blank(&quot;en&quot;) # prazan model
    nlp.add_pipe(&quot;transformer&quot;)
    nlp.add_pipe(&quot;parser&quot;)
    
    ner = nlp.create_pipe(&quot;ner&quot;) # ubaci custom ner
    nlp.add_pipe(&quot;ner&quot;, last = True)
    ner = nlp.get_pipe(&quot;ner&quot;)
        
    for _, data in train_data:
        for ent in data.get(&quot;entities&quot;):
            ner.add_label(ent[2])

    return nlp

nlp = create_blank_nlp(train_data)
optimizer = nlp.begin_training()

# TRAINING 
for i in range(5):
    
    random.shuffle(train_data)
    
    losses = {}

    sizes = compounding(1.0, 5.0, 150.0)
    batches = minibatch(train_data, size = sizes)
    for batch in batches:
        for text, annotations in batch:
            
            doc = nlp.make_doc(text)
            example = Example.from_dict(doc, annotations)
            
            nlp.update([example], drop = 0.2, sgd = optimizer, losses = losses)

        
    print(&quot;Lossess at iteration&quot;, i, losses)
</code></pre>
<p>What should I do?</p>
",Named Entity Recognition (NER),training custom spacy ner model give training error want train custom ner spacy recogrnizing address data followed tutorial official tutorial min sec function
How to only extract only organization names from text using spacy,"<p>I want to extract only organization from a string or column.</p>
<p>I am using this code:</p>
<pre><code>def ent(doc):
    for x in (nlp(doc)).ents:
        if x.label_ != &quot;ORG&quot;: continue
        else:
            return (x.text)


d= (&quot;Brock Group (American Industrial Partners) acquires Aegion's Energy Services Businesses&quot;)
ent(d)


</code></pre>
<p>However, this code only extract only one organization not all; like in this case only gives:</p>
<pre><code>'Brock Group'
</code></pre>
",Named Entity Recognition (NER),extract organization name text using spacy want extract organization string column using code however code extract one organization like case give
REGEX: how to i get the name more the character &quot; : &quot;,"<p>Im using python to extract some info</p>
<p>i wanna get the words/names before the charcter <code>:</code>
but the problem is everythig is tied together</p>
<p>from here</p>
<pre class=""lang-none prettyprint-override""><code>Morgan Stanley.Erik Woodring: 
</code></pre>
<p>i just wanna extract <code>&quot;Erik Woodring:&quot;</code></p>
<p>or from here</p>
<pre class=""lang-none prettyprint-override""><code>market.Operator: 
</code></pre>
<p>i just wanna extract <code>Operator:</code></p>
<p>sometimes there are questiosn like this</p>
<pre><code>to acquire?Tim Cook:
</code></pre>
<p>i just wanna extract <code>&quot;Tim Cook:&quot;</code></p>
<p>this is what i tried</p>
<p><code>\w*(?=.*:)</code></p>
<p>this is not getting what i wanted, its returning a lot of words</p>
",Named Entity Recognition (NER),regex get name character im using python extract info wan na get word name charcter problem everythig tied together wan na extract wan na extract sometimes questiosn like wan na extract tried getting wanted returning lot word
EntityRuler in spacy,"<p>I want to extract text which contain ORG, DATE and Location name. How I can do it ?</p>
<p>exp1 - Hcode Technologies, Karnal Haryana
July 2021 to DEC 2021</p>
<p>exp 2 - I am working at VAsitum from jun 1998.</p>
<p>I am thinking about EntityRuler but not able.</p>
<p>I tried with following pattern
pattern = [{&quot;ENT_TYPE&quot;: &quot;ORG&quot;}, {&quot;TEXT&quot;: {&quot;REGEX&quot;: &quot;\s&quot;}, &quot;OP&quot;: &quot;{,10}&quot;}, {&quot;ENT_TYPE&quot;: &quot;GPE&quot;}]
But not solved.</p>
",Named Entity Recognition (NER),entityruler spacy want extract text contain org date location name exp hcode technology karnal haryana july dec exp working vasitum jun thinking entityruler able tried following pattern pattern ent type org text regex op ent type gpe solved
How to perform same operation on multiple text files and save the output in different files using python?,"<p>I have written a code which extracts stop words from a text file and outputs two new text files. One file contains the stop words from that text file and another file contains the data without stop words. Now I have more than 100 text file in a folder, I would like to perform the same operation on all those file simultaneously.</p>
<p>For example there is a Folder A which contains 100 text file the code should be executed on all those text files simultaneously. The output should be two new text files such as 'Stop_Word_Consist_Filename.txt' and 'Stop_word_not_Filename.txt' which should be stored in a separate folder.That means for every 100 text files there will 200 output text files stored in a new folder. Please note the 'Filename' in both these output file is the actual name of the text file meaning 'Walmart.txt' should have 'Stop_Word_Consist_Walmart.txt' and 'Stop_word_not_Walmart.txt'. I did try few things and I know loop in involved giving the path directory but I didn't get any success.</p>
<p>Apologies for such a long question.</p>
<p>Following is the code for 1 file.</p>
<pre><code>import numpy as np
import pandas as pd

# Pathes of source files and that for after-modifications
files_path = os.getcwd()
# another folder, your should create first to store files after modifications in
files_after_path = os.getcwd() + '/' + 'Stopwords_folder'
os.makedirs(files_after_path, exist_ok=True)
text_files = os.listdir(files_path)
data = pd.DataFrame(text_files)
data.columns = [&quot;Review_text&quot;]

import re
import nltk
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

def clean_text(df):
    all_reviews = list()
    #lines = df[&quot;Review_text&quot;].values.tolist()
    lines = data.values.tolist()

    for text in lines:
        #text = text.lower()
        text = [word.lower() for word in text]

        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        text = pattern.sub('', str(text))
        
        emoji = re.compile(&quot;[&quot;
                           u&quot;\U0001F600-\U0001FFFF&quot;  # emoticons
                           u&quot;\U0001F300-\U0001F5FF&quot;  # symbols &amp; pictographs
                           u&quot;\U0001F680-\U0001F6FF&quot;  # transport &amp; map symbols
                           u&quot;\U0001F1E0-\U0001F1FF&quot;  # flags (iOS)
                           u&quot;\U00002702-\U000027B0&quot;
                           u&quot;\U000024C2-\U0001F251&quot;
                           &quot;]+&quot;, flags=re.UNICODE)
        text = emoji.sub(r'', text)
        
        text = re.sub(r&quot;i'm&quot;, &quot;i am&quot;, text)
        text = re.sub(r&quot;he's&quot;, &quot;he is&quot;, text)
        text = re.sub(r&quot;she's&quot;, &quot;she is&quot;, text)
        text = re.sub(r&quot;that's&quot;, &quot;that is&quot;, text)        
        text = re.sub(r&quot;what's&quot;, &quot;what is&quot;, text)
        text = re.sub(r&quot;where's&quot;, &quot;where is&quot;, text) 
        text = re.sub(r&quot;\'ll&quot;, &quot; will&quot;, text)  
        text = re.sub(r&quot;\'ve&quot;, &quot; have&quot;, text)  
        text = re.sub(r&quot;\'re&quot;, &quot; are&quot;, text)
        text = re.sub(r&quot;\'d&quot;, &quot; would&quot;, text)
        text = re.sub(r&quot;\'ve&quot;, &quot; have&quot;, text)
        text = re.sub(r&quot;won't&quot;, &quot;will not&quot;, text)
        text = re.sub(r&quot;don't&quot;, &quot;do not&quot;, text)
        text = re.sub(r&quot;did't&quot;, &quot;did not&quot;, text)
        text = re.sub(r&quot;can't&quot;, &quot;can not&quot;, text)
        text = re.sub(r&quot;it's&quot;, &quot;it is&quot;, text)
        text = re.sub(r&quot;couldn't&quot;, &quot;could not&quot;, text)
        text = re.sub(r&quot;have't&quot;, &quot;have not&quot;, text)
        
        text = re.sub(r&quot;[,.\&quot;!@#$%^&amp;*(){}?/;`~:&lt;&gt;+=-]&quot;, &quot;&quot;, text)
        tokens = word_tokenize(text)
        table = str.maketrans('', '', string.punctuation)
        stripped = [w.translate(table) for w in tokens]
        words = [word for word in stripped if word.isalpha()]
        stop_words = set(stopwords.words(&quot;english&quot;))
        stop_words.discard(&quot;not&quot;)
        PS = PorterStemmer()
        words = [PS.stem(w) for w in words if not w in stop_words]
        words = ' '.join(words)
        all_reviews.append(words)
    return all_reviews,stop_words

for entry in data:
    #all_reviews , stop_words = clean_text(entry)
    for r in all_reviews: 
        if not r in stop_words: 
            appendFile = open(f'No_Stopwords{entry}.txt','a') 
            appendFile.write(&quot; &quot;+r) 
            appendFile.close() 
    
    for r in stop_words: 
        appendFile = open(f'Stop_Word_Consist{entry}.txt','a') 
        appendFile.write(&quot; &quot;+r) 
        appendFile.close() 
        
    all_reviews , stop_words = clean_text(entry)
</code></pre>
<p>UPDATE :</p>
<p>So I have made changes to the code. I did got two output files Stop_Word_Consist and No_Stop_word. But I am not getting the required Data inside. Meaning Stop_word consist does not have the stop words I am looking for. I am pretty sure I made some mistakes in indentation. I would appreciate the help.</p>
",Named Entity Recognition (NER),perform operation multiple text file save output different file using python written code extract stop word text file output two new text file one file contains stop word text file another file contains data without stop word text file folder would like perform operation file simultaneously example folder contains text file code executed text file simultaneously output two new text file stop word consist filename txt stop word filename txt stored separate folder mean every text file output text file stored new folder please note filename output file actual name text file meaning walmart txt stop word consist walmart txt stop word walmart txt try thing know loop involved giving path directory get success apology long question following code file update made change code got two output file stop word consist stop word getting required data inside meaning stop word consist doe stop word looking pretty sure made mistake indentation would appreciate help
How to extract numbers (along with comparison adjectives or ranges),"<p>I am working on two NLP projects in Python, and both have a similar task to <strong>extract numerical values and comparison operators</strong> from sentences, like the following:</p>

<pre><code>""... greater than $10 ... "",
""... weight not more than 200lbs ..."",
""... height in 5-7 feets ..."",
""... faster than 30 seconds ... ""
</code></pre>

<p>I found two different approaches to solve this problem:</p>

<ul>
<li>using very complex regular expressions.</li>
<li>using <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Named Entity Recognition</a> (and some regexes, too).</li>
</ul>

<p>How can I parse numerical values out of such sentences? I assume this is a common task in NLP.</p>

<hr>

<p>The desired output would be something like:</p>

<p><strong>Input:</strong></p>

<blockquote>
  <p>""greater than $10""</p>
</blockquote>

<p><strong>Output:</strong></p>

<pre><code>{'value': 10, 'unit': 'dollar', 'relation': 'gt', 'position': 3}
</code></pre>
",Named Entity Recognition (NER),extract number along comparison adjective range working two nlp project python similar task extract numerical value comparison operator sentence like following found two different approach solve problem using complex regular expression using named entity recognition regexes parse numerical value sentence assume common task nlp desired output would something like input greater output
NLTK to Identify Names,"<p>I am attepmting to extract names with the nltk python module.</p>
<pre><code>import nltk
#!pip install svgling

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
</code></pre>
<pre><code>import nltk

from nltk import ne_chunk, pos_tag, word_tokenize
from nltk.tree import Tree

text = &quot;Elon Musk 889-888-8888 elonpie@tessa.net Jeff Bezos (345)123-1234 bezzi@zonbi.com Reshma Saujani example.email@email.com 888-888-8888 Barkevious Mingo&quot;

nltk_results = ne_chunk(pos_tag(word_tokenize(text)))
for nltk_result in nltk_results:
    if type(nltk_result) == Tree:
        name = ''
        for nltk_result_leaf in nltk_result.leaves():
            name += nltk_result_leaf[0] + ' '
        print ('Type: ', nltk_result.label(), 'Name: ', name)
</code></pre>
<p>The output I get from the following code above is as follows:</p>
<pre><code>Type:  PERSON Name:  Elon 
Type:  GPE Name:  Musk 
Type:  PERSON Name:  Jeff Bezos 
Type:  ORGANIZATION Name:  Barkevious Mingo 
</code></pre>
<p>This is not correct.  First of all, Some names are broken up.  Farily common ones, too, like Elon Musk.  Next, all names are not identified.  The desired output would be:</p>
<pre><code>Type:  PERSON Name:  Elon Musk
Type:  PERSON Name:  Jeff Bezos
Type:  PERSON Name:  Reshma Saujani 
Type:  PERSON Name:  Barkevious Mingo 
</code></pre>
<p>Is there a better option in python?</p>
",Named Entity Recognition (NER),nltk identify name attepmting extract name nltk python module output get following code follows correct first name broken farily common one like elon musk next name identified desired output would better option python
spaCy nlp pipeline order of operations,"<p>Does anyone have a chronological list of operations performed by</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_sm')
doc = nlp(text)
</code></pre>

<p>I can see the major components with <code>nlp.pipe_names</code></p>

<pre><code>['tagger', 'parser', 'ner']
</code></pre>

<p>and an alphabetical list of factory operations with <code>nlp.factories</code></p>

<pre><code>{'merge_entities': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'merge_noun_chunks': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'ner': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'parser': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'sbd': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'sentencizer': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'similarity': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'tagger': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'tensorizer': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'textcat': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'tokenizer': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;}
</code></pre>

<p>but I can't figure out when the <strong>lemmatizer</strong> is invoked. 
Lemmatization has to happen after <strong>tokenization</strong> and <strong>POS tagging</strong>, and it will run with the <strong>parser</strong> and <strong>ner</strong> disabled. The spaCy <a href=""https://spacy.io/usage/spacy-101#pipelines"" rel=""nofollow noreferrer"">pipeline docs</a> don't mention it at all. Thanks!</p>
",Named Entity Recognition (NER),spacy nlp pipeline order operation doe anyone chronological list operation performed see major component alphabetical list factory operation figure lemmatizer invoked lemmatization ha happen tokenization po tagging run parser ner disabled spacy pipeline doc mention thanks
Convert NER dataset to spaCy formt for Arabic text,"<p>I tried to convert a NER dataset written in English to spaCy format, everything worked fine but when I tried to do the same thing with a dataset written in Arabic:</p>
<pre><code>!python -m spacy convert -c iob -s -n 10 -b nlp /gdrive/MyDrive/LearningSpacy/SimpleNER-Dataset.txt .
</code></pre>
<p>I got this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main
    &quot;__main__&quot;, mod_spec)
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;/usr/local/lib/python3.7/dist-packages/spacy/__main__.py&quot;, line 4, in &lt;module&gt;
    setup_cli()
  File &quot;/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py&quot;, line 71, in setup_cli
    command(prog_name=COMMAND)
  File &quot;/usr/local/lib/python3.7/dist-packages/click/core.py&quot;, line 829, in __call__
    return self.main(*args, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/click/core.py&quot;, line 782, in main
    rv = self.invoke(ctx)
  File &quot;/usr/local/lib/python3.7/dist-packages/click/core.py&quot;, line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File &quot;/usr/local/lib/python3.7/dist-packages/click/core.py&quot;, line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File &quot;/usr/local/lib/python3.7/dist-packages/click/core.py&quot;, line 610, in invoke
    return callback(*args, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/typer/main.py&quot;, line 532, in wrapper
    return callback(**use_params)  # type: ignore
  File &quot;/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py&quot;, line 74, in convert_cli
    converter = _get_converter(msg, converter, input_path)
  File &quot;/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py&quot;, line 257, in _get_converter
    input_data = file_.read()
  File &quot;/usr/lib/python3.7/codecs.py&quot;, line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byt
</code></pre>
<p>This is how the SimpleNER-Dataset.txt looks like:</p>
<p><a href=""https://i.sstatic.net/9rktc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9rktc.png"" alt=""enter image description here"" /></a></p>
",Named Entity Recognition (NER),convert ner dataset spacy formt arabic text tried convert ner dataset written english spacy format everything worked fine tried thing dataset written arabic got error simplener dataset txt look like
NLP Named Entity Recognition,"<p>I want to use Named Entity Recognition algorithm for extract the names and locations from the text without  using any libraries.</p>
<p>Example:</p>
<blockquote>
<p>American Airlines said it would launch a direct flight to Bengaluru.</p>
</blockquote>
<p>Answer:</p>
<pre><code>Entity: American Airlines
Location: Bengaluru
</code></pre>
<p>What is the best practice of Named Entity? Is it like storing all the names and locations on CSV file and comparing with the sentence?</p>
",Named Entity Recognition (NER),nlp named entity recognition want use named entity recognition algorithm extract name location text without using library example american airline said would launch direct flight bengaluru answer best practice named entity like storing name location csv file comparing sentence
pandas: text analysis: Transfer raw data to dataframe,"<p>I need to read lines from a text file and extract the
quoted person name and quoted text from each line.</p>
<p>lines look similar to this:</p>
<blockquote>
<p>&quot;Am I ever!&quot;, Homer Simpson responded.</p>
</blockquote>
<blockquote>
<p>Remarks:</p>
<p>Hint: Use the returned object from the '<code>open</code>' method to get the file
handler. Each line you read is expected to contain a new-line in the
end of the line. Remove the new-line as following: <code>line_cln =line.strip()</code></p>
</blockquote>
<blockquote>
<p>There are the options for each line (assume one of these
three options): The first set of patterns, for which the person name
appears before the quoted text. The second set of patterns, for which
the quoted text appears before the person. Empty lines.</p>
</blockquote>
<blockquote>
<p>Complete the <code>transfer_raw_text_to_dataframe</code> function to return a
dataframe    with the extracted person name and text as explained
above.  The information is expected to be extracted from the lines of
the given <code>'filename'</code> file.</p>
</blockquote>
<blockquote>
<p>The returned dataframe should include two columns:</p>
<ul>
<li><code>person_name</code> - containing the extracted person name for each line.</li>
<li><code>extracted_text</code> - containing the extracted quoted text for each line.</li>
</ul>
<p>The returned values:</p>
<ul>
<li>dataframe - The dataframe with the extracted information as described above.</li>
</ul>
<ul>
<li>Important Note: if a line does not contain any quotation pattern, no information should be saved in the
corresponding row in the dataframe.</li>
</ul>
</blockquote>
<p>what I got so far: [edited]</p>
<pre><code>def transfer_raw_text_to_dataframe(filename):

    data = open(filename)
    
    quote_pattern ='&quot;(.*)&quot;'
    name_pattern = &quot;\w+\s\w+&quot;
    
    df = open(filename, encoding='utf8')
    lines = df.readlines()
    df.close()
    dataframe = pd.DataFrame(columns=('person_name', 'extracted_text'))
    i = 0  

    for line in lines:
        quote = re.search(quote_pattern,line)
        extracted_quotation = quote.group(1)

        name = re.search(name_pattern,line)
        extracted_person_name = name.group(0)
        
        df2 = {'person_name': extracted_person_name, 'extracted_text': extracted_quotation}
        dataframe = dataframe.append(df2, ignore_index = True)

        dataframe.loc[i] = [person_name, extracted_text]
        i =i+1
            
    return dataframe
</code></pre>
<p>the dataframe is created with the correct shape, problem is, the person name in each row is: 'Oh man' and the quote is 'Oh man, that guy's tough to love.' (in all of them)
which is weird because it's not even in the txt file...</p>
<p><strong>can anyone help me fix this?</strong></p>
<p><strong>Edit:</strong> I need to extract from a simple txt file that contains these lines only:</p>
<pre><code>&quot;Am I ever!&quot;, Homer Simpson responded.
&quot;Hmmm. So... is it okay if I go to the women's conference with Chloe?&quot;, Lisa Simpson answered.
&quot;Really? Uh, sure.&quot;, Bart Simpson answered.
&quot;Sounds great.&quot;, Bart Simpson replied.
Homer Simpson responded: &quot;Danica Patrick in my thoughts!&quot;
C. Montgomery Burns: &quot;Trust me, he'll say it, or I'll bust him down to Thursday night vespers.&quot;
&quot;Gimme that torch.&quot; Lisa Simpson said.
&quot;No! No, I've got a lot more mothering left in me!&quot;, Marge Simpson said.
&quot;Oh, Homie, I don't care if you're a billionaire. I love you just because you're...&quot; Marge Simpson said.
&quot;Damn you, e-Bay!&quot; Homer Simpson answered.
</code></pre>
",Named Entity Recognition (NER),panda text analysis transfer raw data dataframe need read line text file extract quoted person name quoted text line line look similar ever homer simpson remark hint use returned object method get file handler line read expected contain new line end line remove new line following option line assume one three option first set pattern person name appears quoted text second set pattern quoted text appears person empty line complete function return dataframe extracted person name text explained information expected extracted line given file returned dataframe include two column containing extracted person name line containing extracted quoted text line returned value dataframe dataframe extracted information described important note line doe contain quotation pattern information saved corresponding row dataframe got far edited dataframe created correct shape problem person name row oh man quote oh man guy tough love weird even txt file anyone help fix edit need extract simple txt file contains line
how to tokenise Apple Holdings and Washington D.C. as one token each?,"<p>Tried using spacy and nltk to tokenise above two named entities as a token each instead of separate ones. that is to have &quot;Apple Holdings&quot; as a token and &quot;Washington D.C.&quot; as another token instead of &quot;Apple&quot; as a token, &quot;Holdings&quot; as a token, &quot;Washington&quot; as a token and &quot;D.C.&quot; as a token.</p>
<p>How do I do that for other similar cases?</p>
<p>Pls kindly advise.</p>
<p>Thank you so much</p>
",Named Entity Recognition (NER),tokenise apple holding washington c one token tried using spacy nltk tokenise two named entity token instead separate one apple holding token washington c another token instead apple token holding token washington token c token similar case pls kindly advise thank much
How to define ner_model in spacy,"<pre class=""lang-py prettyprint-override""><code>def all_ents(v):
        return [(ent.text, ent.label_) for ent in ner_model(v).ents]

df1['Entities'] = df1['text'].apply(lambda v: all_ents(v))

df1.head()
</code></pre>
<p>when executing this shows ner_model not defined can I please know how to define ner model in spacy</p>
",Named Entity Recognition (NER),define ner model spacy executing show ner model defined please know define ner model spacy
Loading saved NER back into HuggingFace pipeline?,"<p>I am doing some research into HuggingFace's functionalities for transfer learning (specifically, for named entity recognition). To preface, I am a bit new to transformer architectures. I briefly walked through their example off of their website:</p>
<pre><code>from transformers import pipeline

nlp = pipeline(&quot;ner&quot;)

sequence = &quot;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&quot; \
       &quot;close to the Manhattan Bridge which is visible from the window.&quot;

print(nlp(sequence))
</code></pre>
<p>What I would like to do is save and run this locally without having to download the &quot;ner&quot; model every time (which is over 1 GB in size). In their documentation, I see that you can save the pipeline using the &quot;pipeline.save_pretrained()&quot; function to a local folder. The results of this are various files which I am storing into a specific folder.</p>
<p>My question would be how can I load this model back up into a script to continue classifying as in the example above after saving? The output of &quot;pipeline.save_pretrained()&quot; is multiple files.</p>
<p>Here is what I have tried so far:</p>
<p>1: Following the documentation about pipeline</p>
<pre><code>pipe = transformers.TokenClassificationPipeline(model=&quot;pytorch_model.bin&quot;, tokenizer='tokenizer_config.json')
</code></pre>
<p>The error I got was: 'str' object has no attribute &quot;config&quot;</p>
<p>2: Following HuggingFace example on ner:</p>
<pre><code>from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch

model = AutoModelForTokenClassification.from_pretrained(&quot;path to folder following .save_pretrained()&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;path to folder following .save_pretrained()&quot;)

label_list = [
&quot;O&quot;,       # Outside of a named entity
&quot;B-MISC&quot;,  # Beginning of a miscellaneous entity right after another miscellaneous entity
&quot;I-MISC&quot;,  # Miscellaneous entity
&quot;B-PER&quot;,   # Beginning of a person's name right after another person's name
&quot;I-PER&quot;,   # Person's name
&quot;B-ORG&quot;,   # Beginning of an organisation right after another organisation
&quot;I-ORG&quot;,   # Organisation
&quot;B-LOC&quot;,   # Beginning of a location right after another location
&quot;I-LOC&quot;    # Location
]

sequence = &quot;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&quot; \
       &quot;close to the Manhattan Bridge.&quot;

# Bit of a hack to get the tokens with the special tokens
tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))
inputs = tokenizer.encode(sequence, return_tensors=&quot;pt&quot;)

outputs = model(inputs)[0]
predictions = torch.argmax(outputs, dim=2)

print([(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].tolist())])
</code></pre>
<p>This yields an error: list index out of range</p>
<p>I also tried printing out just predictions which is not returning the text format of the tokens along with their entities.</p>
<p>Any help would be much appreciated!</p>
",Named Entity Recognition (NER),loading saved ner back huggingface pipeline research huggingface functionality transfer learning specifically named entity recognition preface bit new transformer architecture briefly walked example website would like save run locally without download ner model every time gb size documentation see save pipeline using pipeline save pretrained function local folder result various file storing specific folder question would load model back script continue classifying example saving output pipeline save pretrained multiple file tried far following documentation pipeline error got wa str object ha attribute config following huggingface example ner yield error list index range also tried printing prediction returning text format token along entity help would much appreciated
Should I use tok2vec before ner in spacy?,"<p>I'm currently training a model for named entity recognition and I could not find out how the pipeline in spacy should be structured in order to achieve better results. Does it make sense to use tok2vec before the ner component?</p>
",Named Entity Recognition (NER),use tok vec ner spacy currently training model named entity recognition could find pipeline spacy structured order achieve better result doe make sense use tok vec ner component
How many data is needed to creat useful word vectors using gensim?,"<p>I try to do named entity recognition (nlp) on tweets for identifying if a tweet talks about diseases. For that purpose I'm currently try to create my own word2vec in gensim and import them for training a new ner model in spacy. My question would be, how many data is needed in order to create a useful word vector for my purpose?</p>
",Named Entity Recognition (NER),many data needed creat useful word vector using gensim try named entity recognition nlp tweet identifying tweet talk disease purpose currently try create word vec gensim import training new ner model spacy question would many data needed order create useful word vector purpose
JSON to Conll2003 conversion,"<p>I am using a labeling tool that provides NER labels in the format of</p>
<p>[ &quot;text&quot;: &quot;I am a boy and I come from New York&quot;, labels [ {&quot;start&quot;:20, &quot;end&quot;:28, &quot;label&quot;:&quot;Location&quot;}].</p>
<p>My models needs CONLL2003 format. Wondering if there are tools or scripts to do the conversion. So i can train Hugging face BERT NER model.</p>
<p>Thanks,</p>
",Named Entity Recognition (NER),json conll conversion using labeling tool provides ner label format text boy come new york label start end label location model need conll format wondering tool script conversion train hugging face bert ner model thanks
How to construct the pipeline for ner &amp; archive better results using spacy?,"<p>I'm currently trying to do named entity recognition for tweets using spacy. For that purpose i created with Gensim word vectors which I'm using to train a new blank ner model. At the moment I am a bit confused how the set up the pipeline for my purpose. Regarding this, I have the following question:</p>
<ul>
<li>The pipeline which I'm currently using consists only of one ner component. Have someone recommendations for construction the pipeline (e.g. using tok2vec before ner)?</li>
</ul>
<p>I also wonder if my approach of training a new model with previously created word vectors is the right one and how i could further improve my prediction accuracy.</p>
",Named Entity Recognition (NER),construct pipeline ner archive better result using spacy currently trying named entity recognition tweet using spacy purpose created gensim word vector using train new blank ner model moment bit confused set pipeline purpose regarding following question pipeline currently using consists one ner component someone recommendation construction pipeline e g using tok vec ner also wonder approach training new model previously created word vector right one could improve prediction accuracy
Get the start position and end position of found named entities in xml,"<p>I'm new in xml parsing. I have a xml file which has a content and an identified entities (person and location ). Number of &quot;person&quot; entity in the file is close to 10 and &quot;location&quot; is just 3.</p>
<pre><code>&lt;em&gt;
Mad Max:
&lt;location&gt;Fury Road&lt;/location 
&lt;/em&gt;
</code></pre>
<p>and so on ..</p>
<p>I wanted to extract the content and start position and end position of each of the entities present in the xml file (using Python - for loop). But not sure how to start writing code to get the positions of it from the xml file.</p>
<p>Can someone please help me?</p>
",Named Entity Recognition (NER),get start position end position found named entity xml new xml parsing xml file ha content identified entity person location number person entity file close location wanted extract content start position end position entity present xml file using python loop sure start writing code get position xml file someone please help
TypeError: __init__() got an unexpected keyword argument &#39;has_model_config&#39; while training Token classification model,"<h3>System Info</h3>
<ul>
<li>transformers version: 4.18.1</li>
<li>Platform: Linux Jupyter Notebook, TF2.3 Python 3.6, 2 GPU</li>
<li>Python version: '1.7.1+cu101'</li>
<li>Using GPU in script?: yes</li>
<li>Using distributed or parallel set-up in script?: no</li>
</ul>
<h3>Information</h3>
<ul>
<li>[ ] The official example scripts</li>
<li>[X] My own modified scripts</li>
</ul>
<h3>Tasks</h3>
<ul>
<li>[ ] An officially supported task in the <code>examples</code> folder (such as GLUE/SQuAD, ...)</li>
<li>[X] My own task or dataset (give details below)</li>
</ul>
<h3>Reproduction</h3>
<pre><code>model_checkpoint = &quot;xlm-roberta-large-finetuned-conll03-english&quot;
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,add_prefix_space=True)
</code></pre>
<p><code>train_examples ={'texts':[x[0] for x in train_set],'tag_names':[x[1] for x in train_set]}</code></p>
<pre><code>def isin(a, b):
    return a[1] &gt; b[0] and a[0] &lt; b[1]
</code></pre>
<pre><code>def tokenize_and_align_labels(examples, label2id, max_length=256):
    tokenized_inputs = tokenizer(examples[&quot;texts&quot;], truncation=True, padding='max_length', max_length=max_length,
                                 return_offsets_mapping=True)
   

    labels = []
    for i, label_idx_for_single_input in enumerate(tqdm.tqdm(examples[&quot;tag_names&quot;])):

     

        labels_for_single_input = ['O' for _ in range(max_length)]
       
        text_offsets = tokenized_inputs['offset_mapping'][i]
       
        for entity in label_idx_for_single_input:

            tag = entity['tag']
          
            tag_offset = [entity['start'], entity['end']]
          

           
            affected_token_ids = [j for j in range(max_length) if isin(tag_offset, text_offsets[j])]


            if len(affected_token_ids) &lt; 1:
              
                continue
            if any(labels_for_single_input[j] != 'O' for j in affected_token_ids):
          
                continue

            for j in affected_token_ids:
                labels_for_single_input[j] = 'I_' + tag
            labels_for_single_input[affected_token_ids[-1]] = 'L_' + tag
            labels_for_single_input[affected_token_ids[0]] = 'B_' + tag

        label_ids = [label2id[x] for x in labels_for_single_input]
        labels.append(label_ids)

    tokenized_inputs[&quot;labels&quot;] = labels
    print(tokenized_inputs.keys())
    return tokenized_inputs


</code></pre>
<pre><code>class MyDataset(torch.utils.data.Dataset):
    def __init__(self, examples):
        self.encodings = examples
        self.labels = examples['labels']

    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item[&quot;labels&quot;] = torch.tensor([self.labels[idx]])
        return item

    def __len__(self):

        return len(self.labels)

train_data=MyDataset(train_data)
</code></pre>
<pre><code>model = AutoModelForTokenClassification.from_pretrained(model_checkpoint,id2label=id2label,label2id=label2id,ignore_mismatched_sizes=True)
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model.to(device)
</code></pre>
<pre><code>args = TrainingArguments(
    &quot;xlmroberta-finetuned-ner&quot;,
#     evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    num_train_epochs=2,
    weight_decay=0.01,
    per_device_train_batch_size=4,
    # per_device_eval_batch_size=32
    fp16=True
    # bf16=True #Ampere GPU
)

</code></pre>
<pre><code>trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_data,
    # eval_dataset=train_data,
    # data_collator=data_collator,
    # compute_metrics=compute_metrics,
    tokenizer=tokenizer)
trainer.train()
</code></pre>
<pre><code>Using amp half precision backend
 FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
***** Running training *****
  Num examples = 141648
  Num Epochs = 2
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed &amp; accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 35412
 MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.
TypeError: __init__() got an unexpected keyword argument 'has_model_config'
</code></pre>
<h3>Expected behavior</h3>
<p>To train NER model</p>
",Named Entity Recognition (NER),typeerror init got unexpected keyword argument ha model config training token classification model system info transformer version platform linux jupyter notebook tf python gpu python version cu using gpu script yes using distributed parallel set script information official example script x modified script task officially supported task folder glue squad x task dataset give detail reproduction expected behavior train ner model
How can I iterate on a column with spacy to get named entities?,"<p>I got a dataframe with a column named &quot;categories&quot;.  Some data of this column looks like this <code>{[], [], [amazon], [clothes], [telecommunication],[],...}</code>. Every row has only one of this values. My task is now to give this values their entities. I tried a lot but it didn't go well. This was my first attempt</p>
<pre><code>import spacy
nlp = spacy.load(&quot;de_core_news_sm&quot;)
doc=list(nlp.pipe(df.categories))
print([(X.text, X.label_) for X in doc.ents])
AttributeError 'list' object has no attribute 'ents'
---------------------------------------------------------------------------
AttributeError Traceback (most recent call last)
in ----&gt; 1 print([(X.text, X.label_) for X in doc.ents])
AttributeError: 'list' object has no attribute 'ents'
</code></pre>
<p>My second attempt:</p>
<pre><code>for token in doc:
print(token.doc, token.pos_, token.dep_)
AttributeError 'spacy.tokens.doc.Doc' object has no attribute 'pos_'
---------------------------------------------------------------------------
AttributeError Traceback (most recent call last)
in 1 for token in doc: ----&gt; 2 print(token.doc, token.pos_, token.dep_)
AttributeError 'spacy.tokens.doc.Doc' object has no attribute 'pos_'
</code></pre>
<p>Third attempt:</p>
<pre><code>docs = df[&quot;categories&quot;].apply(nlp)
for token in docs:
    print(token.text, token.pos_, token.dep_)
AttributeError 'spacy.tokens.doc.Doc' object has no attribute 'docs'
---------------------------------------------------------------------------
AttributeError Traceback (most recent call last)
in 1 docs = df[&quot;categories&quot;].apply(nlp) 2 for token in docs: ----&gt; 3              print(token.docs, token.pos_, token.dep_) 
AttributeError: 'spacy.tokens.doc.Doc' object has no attribute 'docs'
</code></pre>
<p>I just want to iterate spacy on this column to give me for the values an entity. For the empty values it should give me no entity. The column is a string. Thanks for help.</p>
",Named Entity Recognition (NER),iterate column spacy get named entity got dataframe column named category data column look like every row ha one value task give value entity tried lot go well wa first attempt second attempt third attempt want iterate spacy column give value entity empty value give entity column string thanks help
Plotting an ROC curve with spacy,"<p>I have managed to train a custom NER model using Spacy and I would like to demonstrate its ROC curve. I am able to use Scorer (<a href=""https://spacy.io/api/scorer"" rel=""nofollow noreferrer"">https://spacy.io/api/scorer</a>) to get final Recall, Precision and F-score but I would like to visualise the ROC curve in matplotlib.</p>
<p>How would I go about plotting the ROC using this platform?</p>
",Named Entity Recognition (NER),plotting roc curve spacy managed train custom ner model using spacy would like demonstrate roc curve able use scorer get final recall precision f score would like visualise roc curve matplotlib would go plotting roc using platform
"Python and NLP, extract key words and person name from multiple pdf files using NLP &amp; Python","<p>I hope to extract some information from multiple pdf files (e.g. xxx1.pdf, xxx2.pdf, xxx3.pdf...), the output dataframe including 4 fields: 1.file name, 2.the context of each pdf, 3.specific keywords, and 4.the Person's name related to the keyword.</p>
<p>I partly wrote some code to generate the first three fields, apart from the person's name, I assume it is more about NLP skills, e.g. Spacy, NLTK etc, which I am very new here. What I wish to generate is if the keyword ('Chair Man' in my case) be found in each pdf file, then search the closet Person's name to the key words (before or after), then put the name in the dataframe. I may sense the logic is to calculate the shortest distance between a Person name and the key word, but I could not make it, here is my part of code:</p>
<pre><code>import pandas as pd
import numpy as np
from tkinter import filedialog
from pathlib import Path
import os

import PyPDF2


file_path = filedialog.askdirectory()
file_list = Path(file_path + '/').rglob('*.pdf')

file_names = os.listdir(file_path)
file_names = [file for file in file_names if file.endswith('.pdf')]
print(len(file_names))

file_df = []
context = []

for file in file_list:
    print(file.stem)
    file_name = file.stem

    pdf = PyPDF2.PdfFileReader(file)
    page_num = pdf.getNumPages()
    text = ''
    for i in range(0, page_num):
        PgOb = pdf.getPage(i)
        text += PgOb.extractText()
    # print(text)
    file_df.append(file_name)
    context.append(text)

pdf_df = pd.DataFrame({'File': file_df, 'Context': context})


pdf_df['Chair_Man_Check'] = np.nan
pdf_df.loc[pdf_df['Context'].str.contains('Chair Man'), 'Chair_Man_Check'] = 'Chair Man  - Yes'
print(pdf_df)
</code></pre>
<p>the ideal output is like this below:</p>
<pre><code>  File_Name                    Context  Chair_Man_Check          Name
  xxx1.pdf  file context blah blah...   Chair Man -Yes   James Volks
  xxx2.pdf                  blah.....              NaN           NaN
  xxx3.pdf                   blahhhhh  Chair Man - Yes  Tim Aloepman
</code></pre>
<p>Could you provide some help please? many thanks!</p>
",Named Entity Recognition (NER),python nlp extract key word person name multiple pdf file using nlp python hope extract information multiple pdf file e g xxx pdf xxx pdf xxx pdf output dataframe including field file name context pdf specific keywords person name related keyword partly wrote code generate first three field apart person name assume nlp skill e g spacy nltk etc new wish generate keyword chair man case found pdf file search closet person name key word put name dataframe may sense logic calculate shortest distance person name key word could make part code ideal output like could provide help please many thanks
Why does Stanford NER tagger give different tags for similar lists?,"<p>I would like to understand better why the Stanford NER (Name Entity Recognition) tagger yields different results for the same words, depending on the list of words you submit to it.</p>
<p>Here is an example:</p>
<pre><code>from nltk.tag import StanfordNERTagger

user =&quot;MYUSERPATH&quot;

stpath = user + 'PATHTOSTANFORDTAGGER'
St = StanfordNERTagger(stpath + 'classifiers/english.all.3class.distsim.crf.ser.gz', stpath+'stanford-ner.jar', encoding ='utf-8')

words1 = [&quot;I&quot;,&quot;am&quot;, &quot;amazed&quot;, &quot;by&quot;, &quot;Dylan&quot;,&quot;van&quot;, &quot;Baarle&quot;, &quot;and&quot;, &quot;Remco&quot;, &quot;Evenepoel&quot;]
words2 = [&quot;I&quot;,&quot;am&quot;, &quot;amazed&quot;, &quot;by&quot;,&quot;Dylan&quot;,&quot;van&quot;, &quot;Baarle&quot;, &quot;and&quot;,&quot;Remco&quot;, &quot;Evenepoel&quot;, &quot;Paris&quot;,&quot;Roubaix&quot;,&quot;is&quot;,&quot;a&quot;,&quot;great&quot;,&quot;race&quot;,&quot;I&quot;,&quot;watch&quot;,&quot;on&quot;,&quot;Eurosport&quot;]
text_pars = St.tag(words1)
text_pars2 = St.tag(words2)
print(words1)
print(text_pars)
print(text_pars2)
</code></pre>
<p>Here the list <code>words2</code> is the concatenation of <code>words1</code> and a second piece of a sentence. When I compare the tags of these two sentences, I can see that the output for the same words is not the same.</p>
<p>Here is the output of <code>print(text_pars)</code>, which tagged <code>words1</code>. It accurately tagged &quot;Remco&quot; and &quot;Evenepoel&quot; and a PERSON.</p>
<pre><code>[('I', 'O'), ('am', 'O'), ('amazed', 'O'), ('by', 'O'), ('Dylan', 'PERSON'), ('van', 'PERSON'), ('Baarle', 'PERSON'), ('and', 'O'), ('Remco', 'PERSON'), ('Evenepoel', 'PERSON')]
</code></pre>
<p>The output of the second instance yields different results. It now tags &quot;Remco&quot; and &quot;Evenepoel&quot; as 'ORGANIZATION':</p>
<pre><code>[('I', 'O'), ('am', 'O'), ('amazed', 'O'), ('by', 'O'), ('Dylan', 'PERSON'), ('van', 'PERSON'), ('Baarle', 'PERSON'), ('and', 'O'), ('Remco', 'ORGANIZATION'), ('Evenepoel', 'ORGANIZATION'), ('Paris', 'ORGANIZATION'), ('Roubaix', 'ORGANIZATION'), ('is', 'O'), ('a', 'O'), ('great', 'O'), ('race', 'O'), ('I', 'O'), ('watch', 'O'), ('on', 'O'), ('Eurosport', 'LOCATION')]
</code></pre>
<p>Why are they different? Does it have to do with the surroundings of the words (Many words tagged as organization after it)?</p>
",Named Entity Recognition (NER),doe stanford ner tagger give different tag similar list would like understand better stanford ner name entity recognition tagger yield different result word depending list word submit example list concatenation second piece sentence compare tag two sentence see output word output tagged accurately tagged remco evenepoel person output second instance yield different result tag remco evenepoel organization different doe surroundings word many word tagged organization
How to highlight or color code the text from the NER output having offsets using python?,"<p>I have fine tuned a <code>Hugging face</code> <code>token classification</code> model for <code>NER task</code>. I use <code>pipeline</code> from Hugging face to do prediction on test text data.</p>
<p>I tag the data as <code>BIOL</code> format. <code>B stands of Beginning, I stand for Including, O means no entity, L means Last</code></p>
<p><strong>Example:</strong></p>
<p><code>Joh J Mathew</code> will be tagged as <code>B_PERSON</code> <code>I_PERSON</code> <code>L_PERSON</code></p>
<p><strong>Here is how the output looks like:</strong></p>
<pre><code>model = AutoModelForTokenClassification.from_pretrained(&quot;model_x&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;model_x&quot;)
    
token_classifier = pipeline(&quot;token-classification&quot;, model=model, aggregation_strategy=&quot;max&quot;,tokenizer=tokenizer)

text=(&quot;&quot;&quot;'IOWA DRIVER LICENSE 1 SAMPLE 2 MARK LIMITED-TERM 8 123 NORTH STREET APT 201 DES MOINES, IA 50301-1234 
Onom d DL No. 123XX6789 4a iss 1107/2016 4b exp 01/12/2021 15 Sex M 16 Hgt 5\'-08&quot; 18 Eyes BRO 9a End NONE 9 
Class C 12 Rest NONE Mark Sample DONOR MED ALERT: Y HEARING IMP: Y MED ADV DIR: Y 3 OOB 01/12/1967 5 
DD 12345678901234567890123 NIVIA AL NA LANG ---- QUE EROL DE USA 01/12/67&quot;&quot;&quot;)

for ent in token_classifier(text):
    print(ent)

{'entity_group': 'B_LAST_NAME', 'score': 0.9999994, 'word': 'SAMPLE', 'start': 23, 'end': 29}
{'entity_group': 'B_FIRST_NAME', 'score': 0.99999905, 'word': '', 'start': 32, 'end': 33}
{'entity_group': 'L_FIRST_NAME', 'score': 0.9999949, 'word': 'MARK', 'start': 32, 'end': 36}
{'entity_group': 'B_ADDRESS', 'score': 0.9999989, 'word': '123', 'start': 52, 'end': 55}
{'entity_group': 'I_ADDRESS', 'score': 0.99999917, 'word': 'NORTHSTREETAPT201DESMOINES,IA', 'start': 56, 'end': 91}
{'entity_group': 'I_DRIVER_LICENSE_NUMBER', 'score': 0.9999995, 'word': '123XX6789', 'start': 118, 'end': 127}
{'entity_group': 'L_ISSUE_DATE', 'score': 0.99999964, 'word': '1107/2016', 'start': 135, 'end': 144}
{'entity_group': 'I_EXPIRY_DATE', 'score': 0.99999964, 'word': '01/12/2021', 'start': 152, 'end': 162}
{'entity_group': 'B_PERSON_NAME', 'score': 0.99999905, 'word': 'Mark', 'start': 234, 'end': 238}
{'entity_group': 'I_PERSON_NAME', 'score': 0.9999993, 'word': 'Sample', 'start': 239, 'end': 245}
{'entity_group': 'L_DATE_OF_BIRTH', 'score': 0.99999976, 'word': '01/12/1967', 'start': 301, 'end': 311}
</code></pre>
<p>So, given the offset values <code>entity_group</code>, <code>word</code>, <code>start</code>, <code>end</code> how can I highlight the original text with the <code>entity_group</code> so that it is easy to visulize.</p>
<p><strong>Final Output</strong></p>
<p><a href=""https://i.sstatic.net/Ef8WB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ef8WB.png"" alt=""enter image description here"" /></a></p>
<p><code>Is there any python-library that I can use to do it.?</code></p>
",Named Entity Recognition (NER),highlight color code text ner output offset using python fine tuned model use hugging face prediction test text data tag data format example tagged output look like given offset value highlight original text easy visulize final output
"TypeError: (&#39;Keyword argument not understood:&#39;, &#39;b_regularizer&#39;)","<p>Below is the code for my Python Script named NER_model.py
The problem is with the line <strong>self.model.add</strong>, in the function named <strong>make_and_compile</strong>, where one of the input arguments is <strong>b_regularizer</strong>. Did the recent versions of Keras have a different name for 'b_regularizer' ?</p>
<p>Can someone plz tell me how to resolve this error?</p>
<pre><code># Keras imports
from tensorflow import keras
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers.wrappers import TimeDistributed
from keras.layers.wrappers import Bidirectional
from keras.layers.core import Dropout
from keras.regularizers import l2
from keras import metrics

import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix, classification_report


class NER():
    def __init__(self, data_reader):
        self.data_reader = data_reader
        self.x, self.y = data_reader.get_data();
        self.model = None
        self.x_train = None
        self.y_train = None
        self.x_test = None
        self.y_test = None

    def make_and_compile(self, units = 150, dropout = 0.2, regul_alpha = 0.0):
        self.model = Sequential()
        # Bidirectional LSTM with 100 outputs/memory units
        self.model.add(Bidirectional(LSTM(units, 
                                          return_sequences=True,
                                          kernel_regularizer=l2(regul_alpha),
                                          b_regularizer=l2(regul_alpha)),
                                    input_shape = [self.data_reader.max_len, 
                                                   self.data_reader.LEN_WORD_VECTORS]))
        self.model.add(TimeDistributed(Dense(self.data_reader.LEN_NAMED_CLASSES, 
                                             activation='softmax',
                                             kernel_regularizer=l2(regul_alpha),
                                             b_regularizer=l2(regul_alpha))))
        self.model.add(Dropout(dropout))
        self.model.compile(loss='categorical_crossentropy',
                           optimizer='adam',
                           metrics=['accuracy'])
        print(self.model.summary())
</code></pre>
<p><strong>And the error is:</strong></p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~\6th SEM - NLP Project\NER_model.py in make_and_compile(self, units, dropout, regul_alpha)
     29                 self.model = Sequential()
     30                 # Bidirectional LSTM with 100 outputs/memory units
---&gt; 31         self.model.add(Bidirectional(LSTM(units, 
     32                                                                                   return_sequences=True,
     33                                                                                   kernel_regularizer=l2(regul_alpha),

~\AppData\Roaming\Python\Python39\site-packages\keras\layers\recurrent_v2.py in __init__(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, return_sequences, return_state, go_backwards, stateful, time_major, unroll, **kwargs)
   1087     self.return_runtime = kwargs.pop('return_runtime', False)
   1088 
-&gt; 1089     super(LSTM, self).__init__(
   1090         units,
   1091         activation=activation,

~\AppData\Roaming\Python\Python39\site-packages\keras\layers\recurrent.py in __init__(self, *args, **kwargs)
   1135   def __init__(self, *args, **kwargs):
   1136     self._create_non_trackable_mask_cache()
-&gt; 1137     super(DropoutRNNCellMixin, self).__init__(*args, **kwargs)
   1138 
   1139   @tf.__internal__.tracking.no_automatic_dependency_tracking

~\AppData\Roaming\Python\Python39\site-packages\keras\layers\recurrent.py in __init__(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, return_sequences, return_state, go_backwards, stateful, unroll, **kwargs)
   2816         trainable=kwargs.get('trainable', True),
   2817         **cell_kwargs)
-&gt; 2818     super(LSTM, self).__init__(
   2819         cell,
   2820         return_sequences=return_sequences,

~\AppData\Roaming\Python\Python39\site-packages\keras\layers\recurrent.py in __init__(self, cell, return_sequences, return_state, go_backwards, stateful, unroll, time_major, **kwargs)
    417       kwargs['input_shape'] = input_shape
    418 
--&gt; 419     super(RNN, self).__init__(**kwargs)
    420     self.cell = cell
    421     self.return_sequences = return_sequences

~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\training\tracking\base.py in _method_wrapper(self, *args, **kwargs)
    627     self._self_setattr_tracking = False  # pylint: disable=protected-access
    628     try:
--&gt; 629       result = method(self, *args, **kwargs)
    630     finally:
    631       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~\AppData\Roaming\Python\Python39\site-packages\keras\engine\base_layer.py in __init__(self, seed, force_generator, **kwargs)
   3436       **kwargs: other keyword arguments that will be passed to the parent class
   3437     &quot;&quot;&quot;
-&gt; 3438     super().__init__(**kwargs)
   3439     self._random_generator = backend.RandomGenerator(
   3440         seed, force_generator=force_generator)

~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\training\tracking\base.py in _method_wrapper(self, *args, **kwargs)
    627     self._self_setattr_tracking = False  # pylint: disable=protected-access
    628     try:
--&gt; 629       result = method(self, *args, **kwargs)
    630     finally:
    631       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~\AppData\Roaming\Python\Python39\site-packages\keras\engine\base_layer.py in __init__(self, trainable, name, dtype, dynamic, **kwargs)
    339     }
    340     # Validate optional keyword arguments.
--&gt; 341     generic_utils.validate_kwargs(kwargs, allowed_kwargs)
    342 
    343     # Mutable properties

~\AppData\Roaming\Python\Python39\site-packages\keras\utils\generic_utils.py in validate_kwargs(kwargs, allowed_kwargs, error_message)
   1172   for kwarg in kwargs:
   1173     if kwarg not in allowed_kwargs:
-&gt; 1174       raise TypeError(error_message, kwarg)
   1175 
   1176 

TypeError: ('Keyword argument not understood:', 'b_regularizer')
</code></pre>
",Named Entity Recognition (NER),typeerror keyword argument understood b regularizer code python script named ner model py problem line self model add function named make compile one input argument b regularizer recent version kera different name b regularizer someone plz tell resolve error error
How should \n be dealt in text while training for NER ? Should they be removed or replaced by space?,"<p>I have documents where often the ORG name will come as follows(separated by newline):</p>
<pre><code>Name_Part1 Name_Part2
Name_Part3
</code></pre>
<p>Here, I would like to training a NER model using spacy CNN. Should I be replacing <code>'\n'</code> by <code>' '</code> (space) or leaving as it is. Will there be issues in sentence boundary detection?</p>
<p>Is there any industry standard method or protocol for this process?</p>
",Named Entity Recognition (NER),n dealt text training ner removed replaced space document often org name come follows separated newline would like training ner model using spacy cnn replacing space leaving issue sentence boundary detection industry standard method protocol process
Looking for a model on how to match a keyword / part(s) of word into a full name,"<p>Our data set is a lot of ship movements. A captain usually enters destination in form of a short code like: &quot;<em>CHN</em>&quot;, &quot;<em>Singapore</em>&quot;, &quot;<em>XIAO HU</em>&quot; etc.
These keywords are pretty short, can be somewhat misspelled and can be a code for a port, name of the city or just random gibberish depending on user's mood :)</p>
<p>We have a large dictionary of destination words and want to try and match the code into a real destination (Like Singapore Port X).</p>
<p>So, basically it's a word =&gt; words predictor, depending on earlier history (ie. same vessels usually goes on same route).</p>
<p>Is there a good NLP model to use to implement these kind of predictions? Right now we're using regular text/pattern matching and it WORKS, but thinking this could be solved better by ML.</p>
<p>Since we also have the actual trip data, we <strong>do</strong> know the actual destination afterwards, so it's possible to have a decent training data set. Unfortunately the captain might be different for same vessel on different trips, so the &quot;style&quot; of codes might not be consistent.</p>
",Named Entity Recognition (NER),looking model match keyword part word full name data set lot ship movement captain usually enters destination form short code like chn singapore xiao hu etc keywords pretty short somewhat misspelled code port name city random gibberish depending user mood large dictionary destination word want try match code real destination like singapore port x basically word word predictor depending earlier history ie vessel usually go route good nlp model use implement kind prediction right using regular text pattern matching work thinking could solved better ml since also actual trip data know actual destination afterwards possible decent training data set unfortunately captain might different vessel different trip style code might consistent
"spaCy, NER, documentation about the different label types of a particular LM","<p>I am using spaCy for named entity recognition (NER). According to <a href=""https://spacy.io/models/en"" rel=""nofollow noreferrer"">the spaCy docs</a>, the language model <code>en_core_web_sm</code> is able to recognize 18 different entity types, i. e., it provides 18 labels such as <code>DATE</code>, <code>PERSON</code> or <code>ORG</code>.</p>
<p>I am particularly interested in the labels <code>LOC</code> (location), <code>FAC</code> (facilities) and <code>GPE</code> (gepolitical entities). Is there documentation about which objects are typically labelled with those labels? Have the guidelines used for labelling the entities been published?</p>
<p>I am asking, because sometimes it is not clear to me why a particular object is labelled as say <code>FAC</code> and not as <code>GPE</code>, or why an object is not labelled at all. Let's have a look at an example:</p>
<pre><code>#spacy.cli.download('en_core_web_sm')
nlp = spacy.load('en_core_web_sm')
text = 'Alice Miller went to the Empire State Building. Next she went to Times Square. Finally she went to the train station.'
doc = nlp_en(text)
displacy.render(doc, style='ent')
</code></pre>
<p>The output is:</p>
<p><a href=""https://i.sstatic.net/zxft7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zxft7.png"" alt=""enter image description here"" /></a></p>
<p>To my mind, the Empire State Building is correctly labelled as <code>GPE</code>. The Times Square, however, is labelled as <code>FAC</code>; I expected <code>GPE</code>. And &quot;train station&quot; is not recognized at all; I expected <code>FAC</code>.</p>
",Named Entity Recognition (NER),spacy ner documentation different label type particular lm using spacy named entity recognition ner according spacy doc language model able recognize different entity type e provides label particularly interested label location facility gepolitical entity documentation object typically labelled label guideline used labelling entity published asking sometimes clear particular object labelled say object labelled let look example output mind empire state building correctly labelled time square however labelled expected train station recognized expected
Can we train multiple NER spacy model at the same time?,"<p>I am working on the SPacy 3.1 NER pipeline.
I have created an application using FastAPI and running from the VM server by exposing the IP using the Unicorn server command [uvicorn main: app --host 0.0.0.0 --port 00].
Now, when I call my training API and start the training non of the other API are working, seems like they all are in queue.
I have used async and await.</p>
<p>Can anyone guide me on how to run multiple APIs when training is in progress.</p>
",Named Entity Recognition (NER),train multiple ner spacy model time working spacy ner pipeline created application using fastapi running vm server exposing ip using unicorn server command uvicorn main app host port call training api start training non api working seems like queue used async await anyone guide run multiple apis training progress
Remove B and I tagging at NER,"<p>I have news articles, I want to do NER using deepavlov to that articles. The entity uses the BIO tagging scheme. Here “B” denotes beginning of an entity, “I” stands for “inside” and is used for all words comprising the entity except the first one, and “O” means the absence of entity. The NER codes are like this:</p>

<pre><code>def listOfTuples(list1, list2): 
    return list(map(lambda x, y:(x,y), list1, list2)) 

ner_result = []
for x in split:
    for y in split[0]:
        news_ner = ner_model([str(y)])
        teks =  news_ner[0][0]
        tag = news_ner[1][0]
        ner_result.extend(listOfTuples(teks, tag))

print([i for i in ner_result if i[1] != 'O'])
</code></pre>

<p>Well, the NER results are like this.</p>

<pre><code>[('KOMPAScom', 'B-ORG'), ('Kompascom', 'I-ORG'), ('IFCN', 'B-ORG'), ('-', 'I-ORG'), ('International', 'I-ORG'), ('Fact', 'I-ORG'), ('-', 'I-ORG'), ('Checking', 'I-ORG'), ('Network', 'I-ORG'), ('Kompascom', 'B-ORG'), ('49', 'B-CARDINAL'), ('IFCN', 'B-ORG'), ('Kompascom', 'B-ORG'), ('Redaksi', 'B-ORG'), ('Kompascom', 'I-ORG'), ('Wisnu', 'B-PERSON'), ('Nugroho', 'I-PERSON'), ('Jakarta', 'B-GPE'), ('Rabu', 'B-DATE'), ('17', 'I-DATE'), ('/', 'I-DATE'), ('10', 'I-DATE'), ('/', 'I-DATE'), ('2018', 'I-DATE'), ('KOMPAScom', 'B-ORG'), ('Redaksi', 'B-ORG'), ('Kompascom', 'I-ORG'), ('Wisnu', 'B-PERSON'), ('Nugroho', 'I-PERSON'), ('Kompascom', 'B-ORG'), ('Bentara', 'I-ORG'), ('Budaya', 'I-ORG'), ('Jakarta', 'I-ORG'), ('Palmerah', 'I-ORG')]
</code></pre>

<p>I want to remove the tags of B and I, then merge the text in tags B and I, so the output goes like this.</p>

<pre><code>[('KOMPAScom Kompascom', 'ORG'), ('IFCN - International Fact - Checking Network', 'ORG'), ('Kompascom', 'ORG'), ('49', 'CARDINAL'), ('IFCN', 'ORG'), ('Kompascom', 'ORG'), ('Redaksi Kompascom', 'ORG'), ('Wisnu Nugroho', 'PERSON'), ('Jakarta', 'GPE'), ('Rabu 17/10/2018', 'DATE'), ('KOMPAScom', 'ORG'), ('Redaksi Kompascom', 'ORG'), ('Wisnu Nugroho', 'PERSON'), ('Kompascom Bentara Budaya Jakarta Palmerah', 'ORG')]
</code></pre>

<p>Do you have any ideas?</p>
",Named Entity Recognition (NER),remove b tagging ner news article want ner using deepavlov article entity us bio tagging scheme b denotes beginning entity stand inside used word comprising entity except first one mean absence entity ner code like well ner result like want remove tag b merge text tag b output go like idea
Unable to load the spacy model &#39;en_core_web_lg&#39; on Google colab,"<p>I am using spacy in google colab to build an NER model for which I have downloaded the spaCy 'en_core_web_lg' model using</p>
<pre><code>import spacy.cli
spacy.cli.download(&quot;en_core_web_lg&quot;)
</code></pre>
<p>and I get a message saying</p>
<pre><code>✔ Download and installation successful
You can now load the model via spacy.load('en_core_web_lg')
</code></pre>
<p>However then when i try to load the model</p>
<pre><code>nlp = spacy.load('en_core_web_lg')
</code></pre>
<p>the following error is printed:</p>
<pre><code>OSError: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
</code></pre>
<p>Could anyone help me with this problem?</p>
",Named Entity Recognition (NER),unable load spacy model en core web lg google colab using spacy google colab build ner model downloaded spacy en core web lg model using get message saying however try load model following error printed could anyone help problem
Looking for dataset with US and other addresses,"<p>I'm looking for dataset which contain address within some sentences to train NER, preferable US addresses. Can't find such dataset. Do you know any?</p>
",Named Entity Recognition (NER),looking dataset u address looking dataset contain address within sentence train ner preferable u address find dataset know
How to properly extract entities like facilities and establishment from text using NLP and Entity recognition?,"<p>I need to identify all the <code>establishments</code> and <code>facilities</code> from a given text using natural language processing and NER.</p>
<p><strong>Example text:</strong></p>
<blockquote>
<p>The government panned to build new parks, swimming pool and commercial complex for out town and improve existing housing complex, schools and townhouse.</p>
</blockquote>
<p><strong>Expected entities to be identified:</strong></p>
<blockquote>
<p>parks, swimming pool, commercial complex, housing complex, school and townhouse</p>
</blockquote>
<p>I did explore some python libraries like  Spacy and NLTK but results were not great only 2 entities were identified. I reckon the data needs to be pre-processed properly.</p>
<p>What should I do to improve the results ? Is there any other libraries/framework that is better for this use case ? Is there any way to train our model using the existing db ?</p>
",Named Entity Recognition (NER),properly extract entity like facility establishment text using nlp entity recognition need identify given text using natural language processing ner example text government panned build new park swimming pool commercial complex town improve existing housing complex school townhouse expected entity identified park swimming pool commercial complex housing complex school townhouse explore python library like spacy nltk result great entity identified reckon data need pre processed properly improve result library framework better use case way train model using existing db
Improving accuracy of NER on Spacy for a tag that is not following one format,"<p>I am using Spacy model for NER on my datasets. It is showing poor tagging on B-Address and I-ADDRESS. The reason is because I have different type of address in my document. Some start with number, some start with name of building, some start with po box. Any idea how I can increase my accuracy on address tag?</p>
",Named Entity Recognition (NER),improving accuracy ner spacy tag following one format using spacy model ner datasets showing poor tagging b address address reason different type address document start number start name building start po box idea increase accuracy address tag
How to keep structure of text after feeding it to a pipeline for NER,"<p>I've build an NER (named entity recognition) model, based on a HuggingFace existing model and that I fine-tuned to recognize my custom entities. The text I want to run my model on is in a <code>txt</code> file.</p>
<p>The code of how I use the model:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline

# loading the fine-tuned model
ner_pipeline =  pipeline('token-classification', model=&quot;./my-model.model/&quot;, tokenizer=&quot;./my-model.model/&quot;, ignore_labels=[])

with open(my_file, 'r', encoding=&quot;utf8&quot;) as f:
  lines = f.readlines()
  joined_lines = ' '.join(lines)

  result = ner_pipeline(joined_lines, aggregation_strategy='first')
  text = &quot;&quot;
      
  for group in result:
     if group[&quot;entity_group&quot;] != 'O':
        # substitute the entity with its tag
        text += group[&quot;entity_group&quot;]+ &quot; &quot;
     else:
        text += group[&quot;word&quot;] + &quot; &quot;
</code></pre>
<p>Basically what I do is substituting the entities recognized with the entity tag, and leave the rest of the text as is.</p>
<p>With my code, the final <code>text</code> is filled with the content exactly as I want it, but the structure is lost. While doing <code>' '.join(lines)</code> I'm basically throwing away the <code>\n</code>s inside the text, that however I would like to keep in my reconstructed text.</p>
<p>I've tried feeding the pipeline with single sentences (each of the <code>f.readlines()</code>) end not the full joined text, but the results are far worse. The model works a lot better predicting on the whole text.</p>
<p>Does anyone knows a way how I could keep or retrieve the structure of the original text? Thanks.</p>
",Named Entity Recognition (NER),keep structure text feeding pipeline ner build ner named entity recognition model based huggingface existing model fine tuned recognize custom entity text want run model file code use model basically substituting entity recognized entity tag leave rest text code final filled content exactly want structure lost basically throwing away inside text however would like keep reconstructed text tried feeding pipeline single sentence end full joined text result far worse model work lot better predicting whole text doe anyone know way could keep retrieve structure original text thanks
Is it possible to re-train a finetuned NER model on a dataset with a different tagset (respect from the first training dataset)?,"<p>first time here and very new to NLP, be gentle plz.</p>
<p>I used PyTorch to finetune XLM-RoBERTa with a german dataset for NER that has 7 tags. Let's call this model <em>xlmr-finetuned</em>.</p>
<p>Now I have another german NER dataset that comes from historical newspapers (so it contains many errors), and I would like to finetune or re-train the last model (<em>xlmr-finetuned</em>) on this dataset. The problem is that this historical dataset has 11 tags (the same 7 tags as the first + 4 new tags). So, when trying to load the model with <em>.from_pretrained</em> I get a size mismatch error about the sizes of the model (<em>xlmr-finetuned</em>) and the configuration of the new dataset I want to finetuneit on:</p>
<pre><code>RuntimeError: Error(s) in loading state_dict for XLMRobertaForTokenClassification:
size mismatch for classifier.weight: copying a param with shape torch.Size([7, 768]) from checkpoint, the shape in current model is torch.Size([11, 768]).
size mismatch for classifier.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([11]).
</code></pre>
<p>So, my question is, is it possible to finetune, with a different tagset, an already finetuned model? Or, on the contrary, transfer learning can only be done between very similar datasets?</p>
<p>I'm already considering solutions that involve modifying the datasets, like removing the 4 new tags in the new dataset, or telling the model in the first finetuning that there are 11 tags instead of 7 (even if there are not).</p>
<p>Thanks a lot in advance!</p>
",Named Entity Recognition (NER),possible train finetuned ner model dataset different tagset respect first training dataset first time new nlp gentle plz used pytorch finetune xlm roberta german dataset ner ha tag let call model xlmr finetuned another german ner dataset come historical newspaper contains many error would like finetune train last model xlmr finetuned dataset problem historical dataset ha tag tag first new tag trying load model pretrained get size mismatch error size model xlmr finetuned configuration new dataset want finetuneit question possible finetune different tagset already finetuned model contrary transfer learning done similar datasets already considering solution involve modifying datasets like removing new tag new dataset telling model first finetuning tag instead even thanks lot advance
How to identify body part names in a text with python,"<p>I am trying to specify whether an entity is a body part. For example in &quot;Other specified disorders of the right ear,&quot; I want to be able to identify the right ear as an entity. I tried some named entity recognition methods but they identify all entities, not just the body parts. I tried using scispacy to do so but I have not managed so far. I tried concise_concepts from spacy to create a separate entity for body parts but that didn't work either. Please guide me through how I can do that and a snippet code would be appreciated.</p>
",Named Entity Recognition (NER),identify body part name text python trying specify whether entity body part example specified disorder right ear want able identify right ear entity tried named entity recognition method identify entity body part tried using scispacy managed far tried concise concept spacy create separate entity body part work either please guide snippet code would appreciated
Creating tables from unstructured texts about stock market,"<p>I am trying to extract information such as profits, revenues and others along with their corresponding dates and quarters from an unstructured text about stock market and convert it into a report in the table form but as there is not format of the input text, it is hard to know which entity belong to what date and quarters and which value belong to which entity. Chunking works on few documents but not enough. Is there any unsupervised way to linking entities with their corresponding dates, values and quarters?</p>
",Named Entity Recognition (NER),creating table unstructured text stock market trying extract information profit revenue others along corresponding date quarter unstructured text stock market convert report table form format input text hard know entity belong date quarter value belong entity chunking work document enough unsupervised way linking entity corresponding date value quarter
How to detect a news article belong to which location?,"<p>Suppose that I have a news article that in the text there are some locations.
I can extract all the locations with NER like Spacy.</p>
<p>But, in addition to NER, the output
should return a list of found locations in the news article along a percentage that the model thinks the news article belongs to the corresponding location. For example, the model outputs New York, Los Angeles, and London for the found locations and returns 0.3, 0.6, and 0.1 for the percentages, respectively. This in turn means that the model has found three locations in the news body and would assign a 60 percent probability that the news revolves around Los Angeles.</p>
<p>Do anyone have a solution about this?
One can use a third party library too.</p>
",Named Entity Recognition (NER),detect news article belong location suppose news article text location extract location ner like spacy addition ner output return list found location news article along percentage model think news article belongs corresponding location example model output new york los angeles london found location return percentage respectively turn mean model ha found three location news body would assign percent probability news revolves around los angeles anyone solution one use third party library
Training epochs interpretation during spaCy NER training,"<p>I Was training my NER model with transformers, and am not really sure why the training stopped at some point, or why did it even go with so many batches. This is how my configuration file looks like (relevant part):</p>
<pre><code>[training]
train_corpus = &quot;corpora.train&quot;
dev_corpus = &quot;corpora.dev&quot;
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_epochs = 2
max_steps = 0
eval_frequency = 200
frozen_components = []
before_to_disk = null

[training.batcher]
@batchers = &quot;spacy.batch_by_words.v1&quot;
discard_oversize = false
tolerance = 0.2
get_length = null

[training.batcher.size]
@schedules = &quot;compounding.v1&quot;
start = 100
stop = 1000
compound = 1.001
t = 0.0

[training.optimizer]
@optimizers = &quot;Adam.v1&quot;
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001
learn_rate = 0.00005
</code></pre>
<p>And this is the training log:</p>
<pre><code>============================= Training pipeline =============================
ℹ Pipeline: ['transformer', 'ner']
ℹ Initial learn rate: 5e-05
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0         398.75     40.97    2.84    3.36    2.46    0.03
  0     200         906.30   1861.38   94.51   94.00   95.03    0.95
  0     400         230.06   1028.51   98.10   97.32   98.89    0.98
  0     600          90.22   1013.38   98.99   98.40   99.58    0.99
  0     800          80.64   1131.73   99.02   98.25   99.81    0.99
  0    1000          98.50   1260.47   99.50   99.16   99.85    1.00
  0    1200          73.32   1414.91   99.49   99.25   99.73    0.99
  0    1400          84.94   1529.75   99.70   99.56   99.85    1.00
  0    1600          55.61   1697.55   99.75   99.63   99.87    1.00
  0    1800          80.41   1936.64   99.75   99.63   99.87    1.00
  0    2000         115.39   2125.54   99.78   99.69   99.87    1.00
  0    2200          63.06   2395.48   99.80   99.75   99.85    1.00
  0    2400         104.14   2574.36   99.87   99.79   99.96    1.00
  0    2600          86.07   2308.35   99.88   99.79   99.97    1.00
  0    2800          81.05   1853.15   99.90   99.87   99.93    1.00
  0    3000          52.67   1462.61   99.96   99.93   99.99    1.00
  0    3200          57.99   1154.62   99.94   99.91   99.97    1.00
  0    3400         110.74    847.50   99.90   99.85   99.96    1.00
  0    3600          90.49    621.99   99.90   99.91   99.90    1.00
  0    3800          51.03    378.93   99.87   99.78   99.97    1.00
  0    4000          93.40    274.80   99.95   99.93   99.97    1.00
  0    4200         138.98    203.28   99.91   99.87   99.96    1.00
  0    4400         106.16    127.60   99.70   99.75   99.64    1.00
  0    4600          70.28     87.25   99.95   99.94   99.96    1.00
✔ Saved pipeline to output directory
training/model-last

</code></pre>
<p>I was trying to train my model for 2 epochs (<code>max_epochs=2</code>), and my train file has around 123591 Examples, and dev file has 2522 Examples.</p>
<p>My question is:</p>
<ul>
<li><p>Since my minimum batch size is 100, I expect my training to end before the 2400th eval batch, right? Because 2400th batch evaluated implies I have a <em>minimum</em> of 2400*100 = 240000, and it would actually be even more than that, since my batch size is increasing. So why did it go all the way to # 4600?</p>
</li>
<li><p>The training ended automatically, but the E still reads the 0th epoch. Why is that?</p>
</li>
</ul>
<p>Edit: In continuation to my 2nd bullet point, I'm curious to know why did the training went all the way upto 4600 batches, because 4600 batches at minimum means 4600*100 = 460000 examples, and I gave 123591  examples for train, so I'm clearly well above and over the 1st epoch, but E still reads as 0.</p>
",Named Entity Recognition (NER),training epoch interpretation spacy ner training wa training ner model transformer really sure training stopped point even go many batch configuration file look like relevant part training log wa trying train model epoch train file ha around example dev file ha example question since minimum batch size expect training end th eval batch right th batch evaluated implies minimum would actually even since batch size increasing go way training ended automatically e still read th epoch edit continuation nd bullet point curious know training went way upto batch batch minimum mean example gave example train clearly well st epoch e still read
Parsing/identifying sections in job descriptions,"<p>I'm trying to solve quite a difficult problem - building a generic parser for job descriptions. The idea is, given a job description, the parser should be able to identify and extract different sections such as job title, location, job description, responsibilities, qualifications etc. The job description will basically be scraped from a web page.</p>

<p>A rule based approach (such as regular expressions) doesn't work since the scenario is too generic. My next approach was to train a custom NER classifier using SpaCy; I've done this numerous times before. However, I'm running into several problems.</p>

<ol>
<li><p>The entities can be very small in size (location, job title etc.) or very large (responsibilities, qualifications etc.). I'm not sure how well NER works if the entities are several lines or a paragraph long? Most of the use cases I've seen are those in which the entities aren't longer than a few words max. Does Spacy's NER work well if the text of the entities I want to identify is quite long in size? (I can give examples if required to make it clearer).</p></li>
<li><p>Is there any other strategy besides NER that I can use to parse these job descriptions as I've mentioned?</p></li>
</ol>

<p>Any help here would be greatly appreciated. I've been banging my head along different walls for a few months, and I have made some progress, but I'm not sure if I'm on the right track, or if a better approach exists.</p>
",Named Entity Recognition (NER),parsing identifying section job description trying solve quite difficult problem building generic parser job description idea given job description parser able identify extract different section job title location job description responsibility qualification etc job description basically scraped web page rule based approach regular expression work since scenario generic next approach wa train custom ner classifier using spacy done numerous time however running several problem entity small size location job title etc large responsibility qualification etc sure well ner work entity several line paragraph long use case seen entity longer word max doe spacy ner work well text entity want identify quite long size give example required make clearer strategy besides ner use parse job description mentioned help would greatly appreciated banging head along different wall month made progress sure right track better approach exists
Custom NERs training with spaCy 3 throws ValueError,"<p>I am trying to add custom NER labels using spacy 3. I found tutorials for older versions and made adjustments for spacy 3. Here is the whole code I am using:</p>
<pre><code>import random
import spacy
from spacy.training import Example

LABEL = 'ANIMAL'
TRAIN_DATA = [
    (&quot;Horses are too tall and they pretend to care about your feelings&quot;, {'entities': [(0, 6, LABEL)]}),
    (&quot;Do they bite?&quot;, {'entities': []}),
    (&quot;horses are too tall and they pretend to care about your feelings&quot;, {'entities': [(0, 6, LABEL)]}),
    (&quot;horses pretend to care about your feelings&quot;, {'entities': [(0, 6, LABEL)]}),
    (&quot;they pretend to care about your feelings, those horses&quot;, {'entities': [(48, 54, LABEL)]}),
    (&quot;horses?&quot;, {'entities': [(0, 6, LABEL)]})
]
nlp = spacy.load('en_core_web_sm')  # load existing spaCy model
ner = nlp.get_pipe('ner')
ner.add_label(LABEL)
print(ner.move_names) # Here I see, that the new label was added
optimizer = nlp.create_optimizer()
# get names of other pipes to disable them during training
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != &quot;ner&quot;]
with nlp.disable_pipes(*other_pipes):  # only train NER
    for itn in range(20):
        random.shuffle(TRAIN_DATA)
        losses = {}
        for text, annotations in TRAIN_DATA:
            doc = nlp(text)
            example = Example.from_dict(doc, annotations)
            nlp.update([example], drop=0.35, sgd=optimizer, losses=losses)
        print(losses)
# test the trained model # add some dummy sentences with many NERs

test_text = 'Do you like horses?'
doc = nlp(test_text)
print(&quot;Entities in '%s'&quot; % test_text)
for ent in doc.ents:
    print(ent.label_, &quot; -- &quot;, ent.text)
</code></pre>
<p>This code outputs the ValueError exception, but only after 2 iterations - notice the first 2 lines:</p>
<pre><code>{'ner': 9.862242701536594}
{'ner': 8.169456698315201}
Traceback (most recent call last):
  File &quot;.\custom_ner_training.py&quot;, line 46, in &lt;module&gt;
    nlp.update([example], drop=0.35, sgd=optimizer, losses=losses)
  File &quot;C:\ogr\moje\python\spacy_pg\myvenv\lib\site-packages\spacy\language.py&quot;, line 1106, in update
    proc.update(examples, sgd=None, losses=losses, **component_cfg[name])
  File &quot;spacy\pipeline\transition_parser.pyx&quot;, line 366, in spacy.pipeline.transition_parser.Parser.update
  File &quot;spacy\pipeline\transition_parser.pyx&quot;, line 478, in spacy.pipeline.transition_parser.Parser.get_batch_loss
  File &quot;spacy\pipeline\_parser_internals\ner.pyx&quot;, line 310, in spacy.pipeline._parser_internals.ner.BiluoPushDown.set_costs
ValueError
</code></pre>
<p>I see the <code>ANIMAL</code> label was added by calling <code>ner.move_names</code>.</p>
<p>When I change my the value <code>LABEL = 'PERSON</code>, the code runs successfully and recognizes horses as <code>PERSON</code> on the new data. This is why I am assuming, there is no error in the code itself.</p>
<p>Is there something I am missing? What am I doing wrong? Could someone reproduce, please?</p>
<p>NOTE: This is my first question ever here. I hope I provided all information. If not, let me know in the comments.</p>
",Named Entity Recognition (NER),custom ners training spacy throw valueerror trying add custom ner label using spacy found tutorial older version made adjustment spacy whole code using code output valueerror exception iteration notice first line see label wa added calling change value code run successfully recognizes horse new data assuming error code something missing wrong could someone reproduce please note first question ever hope provided information let know comment
How to get a graph with the best performing runs via Sweeps (Weights &amp; Biases)?,"<p>For my NER model I use Weights &amp; Biases sweeps for hyperparameter search. I do a grid search with about 100 runs and there are some really meaningful graphs. However, I can't figure out how to create a graph that shows about the best 10 runs in terms of f-score. Does anyone know how to do this?</p>
",Named Entity Recognition (NER),get graph best performing run via sweep weight bias ner model use weight bias sweep hyperparameter search grid search run really meaningful graph however figure create graph show best run term f score doe anyone know
How to extract Person Names from a data frame in Python using Spacy,"<p>I have a table which has people names in text. I would like to de identify that text by removing the people's names from every instance, while maintaining the rest of the sentence.</p>
<pre><code>Row Num            Current Sent                            Ideal Sent
1                 Garry bought a cracker.                 bought a cracker.
2                 He named the parrot Eric.               He named the parrot.
3                 The ship was maned by Captain Jones.    The ship was maned by Captain.
</code></pre>
<p>How can I do that with Spacy? I know you have to identify the label as a 'PERSON' and then apply it to each row, but I can't seem to get the intended result. This is what I have so far:</p>
<pre><code>def pro_nn_finder(text):
    doc = nlp(text)
    return[ent.text for ent in doc.ents if ent.label_ == 'PERSON']

df.apply(pro_nn_finder)
</code></pre>
",Named Entity Recognition (NER),extract person name data frame python using spacy table ha people name text would like de identify text removing people name every instance maintaining rest sentence spacy know identify label person apply row seem get intended result far
How do I extract full entity names from a hugging face model without IO tags,"<p>I am using a model from hugging face, specifically <code>Davlan/distilbert-base-multilingual-cased-ner-hrl</code>. However, I am not able to extract full entity names from the result.</p>
<p>If I run the following code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained(&quot;Davlan/distilbert-base-multilingual-cased-ner-hrl&quot;)
model = AutoModelForTokenClassification.from_pretrained(&quot;Davlan/distilbert-base-multilingual-cased-ner-hrl&quot;)
nlp = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)

example = &quot;My name is Johnathan Smith and I work at Apple&quot;
ner_results = nlp(example, aggregation_strategy=&quot;max&quot;)
print(ner_results)
</code></pre>
<p>Then I get output:</p>
<pre><code>[{'entity': 'B-PER', 'score': 0.9998949, 'index': 4, 'word': 'Johna', 'start': 11, 'end': 16}, {'entity': 'I-PER', 'score': 0.999726, 'index': 5, 'word': '##tha', 'start': 16, 'end': 19}, {'entity': 'I-PER', 'score': 0.9997751, 'index': 6, 'word': '##n', 'start': 19, 'end': 20}, {'entity': 'I-PER', 'score': 0.99974835, 'index': 7, 'word': 'Smith', 'start': 21, 'end': 26}, {'entity': 'B-ORG', 'score': 0.99870986, 'index': 12, 'word': 'Apple', 'start': 41, 'end': 46}]
</code></pre>
<p>It looks like I might be able to post process this so <code>Jonathan Smith</code> is all one word. But ideally I would like this to be done for me and have no partial words identified.</p>
",Named Entity Recognition (NER),extract full entity name hugging face model without io tag using model hugging face specifically however able extract full entity name result run following code get output look like might able post process one word ideally would like done partial word identified
How to extract the address data from a String in Python,"<p>I am trying to extract relevant address info form an string and discard the garbage.
So this:</p>
<pre><code>al domicilio social de la Compañía, Avenida de Burgos, 109 - 28050 Madrid (Indicar Asesoría Jurídica – Protección de Datos) 
</code></pre>
<p>Should be:</p>
<pre><code>Avenida de Burgos, 109 - 28050 Madrid
</code></pre>
<p>What i've done:</p>
<p>I am using stanza NER to find locations from text.</p>
<p>After that, I am using the indexes of the found entities to get the full address.
For eg: If A Madrid (Spanish city) is found in  text[120:128] i will extract the string text[60:101] to get the full address.</p>
<p>My current code is:</p>
<pre><code>##
##STANZA NER FOR LOCATIONS
##
!pip install stanza
#Download the spanish model
import stanza
stanza.download('es') 
#create and run the ner tagger
nlp = stanza.Pipeline(lang='es', processors='tokenize,ner')
text = 'al domicilio social de la Compañía, Avenida de Burgos, 109 - 28050 Madrid (Indicar Asesoría Jurídica – Protección de Datos) '
doc = nlp(text)

#print results of NER tagger
print([ent for ent in doc.ents if ent.type==&quot;LOC&quot;], sep='\n')
print(*[text[int(ent.start_char)-60:int(ent.end_char)+15] for ent in doc.ents if ent.type==&quot;LOC&quot;], sep='\n')
</code></pre>
<p>After this, in this particular case, which should be reproducible. I get the next address.</p>
<pre><code>cilio social de la Compañía, Avenida de Burgos, 109 - 28050 Madrid (Indicar Aseso
</code></pre>
<p>Which contains extra &quot;garbage&quot; info --&gt; &quot; cilio social de la Compañía,&quot; and &quot;(Indicar Aseso&quot;</p>
<p>In the next part of the process,I am using the <a href=""https://github.com/openvenues/libpostal"" rel=""nofollow noreferrer"">libpostal</a> library to parse the address as it follows:</p>
<pre><code>!pip install postal
from postal.parser import parse_address
parse_address('Avenida de Burgos, 109 - 28050 Madrid')
</code></pre>
<p>Which works reliably in most cases, but only with clean addresses.</p>
<pre><code>  [('avenida de burgos', 'road'),
 ('109', 'house_number'),
 ('28050', 'postcode'),
 ('madrid', 'city')]
</code></pre>
<p>So, to sum up, I am searching from another tecnique apart from regex to help me discard garbage info from addresses apart from regex. (Libraries which do this if they exist or a new NLP approach ... )
Thanks</p>
",Named Entity Recognition (NER),extract address data string python trying extract relevant address info form string discard garbage done using stanza ner find location text using index found entity get full address eg madrid spanish city found text extract string text get full address current code particular case reproducible get next address contains extra garbage info cilio social de la compa indicar aseso next part process using libpostal library parse address follows work reliably case clean address sum searching another tecnique apart regex help discard garbage info address apart regex library exist new nlp approach thanks
Named Entity Recognition Systems for German Texts,"<p>I am working on a <strong>Named Entity Recognition (NER)</strong> project in which I got a large amount of text in the sense that it is too much to read or skim read. Therefore, I want to <strong>create an overview of what is mentioned</strong> by extracting <em>named entities</em> (places, names, times, maybe topics) and create an index of kind (entity, list of pages/lines where it is mentioned). I have worked through <a href=""https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/"" rel=""nofollow noreferrer"">Standford's NLP lecture</a>, (parts of) Eisenstein's <a href=""https://mitpress.mit.edu/books/introduction-natural-language-processing"" rel=""nofollow noreferrer""><em>Introduction to NLP</em></a> book found some literature and systems for English texts. As <strong>my corpus is in German</strong>, I would like to ask how I can approach this problem. Also, this is my first NLP project, so I would not know if I could solve this challenge even if texts were in English.</p>
<p>As a first step</p>
<ul>
<li>are there German NER systems out there which I could use?</li>
</ul>
<p>The further roadmap of my project is:</p>
<ul>
<li>How can I avoid mapping misspellings or rare names to a <code>NUL</code>/<code>UNK</code> token? This is relevant because there are also some historic passages that use words no longer in use or that follow old orthography. I think the relevant terms are <strong>tokenisation</strong> or <strong>stemming</strong>.</li>
<li>I thought about fine-tuning or transfer learning the base NER model to a corpus of historic texts to improve NER.</li>
</ul>
<p>A major challenge is that there is <strong>no annotated dataset</strong> for my corpus available and I could only manually annotate a tiny fraction of it. So I would be happy for hints on German annotated datasets which I could incorporate into my project.</p>
<p>Thank you in advance for your inputs and fruitful discussions.</p>
",Named Entity Recognition (NER),named entity recognition system german text working named entity recognition ner project got large amount text sense much read skim read therefore want create overview mentioned extracting named entity place name time maybe topic create index kind entity list page line mentioned worked standford nlp lecture part eisenstein introduction nlp book found literature system english text corpus german would like ask approach problem also first nlp project would know could solve challenge even text english first step german ner system could use roadmap project avoid mapping misspelling rare name token relevant also historic passage use word longer use follow old orthography think relevant term tokenisation stemming thought fine tuning transfer learning base ner model corpus historic text improve ner major challenge annotated dataset corpus available could manually annotate tiny fraction would happy hint german annotated datasets could incorporate project thank advance input fruitful discussion
Document Layout Analysis for text extraction,"<p>I need to analyze the layout structure of different documents type like: <strong>pdf</strong>, <strong>doc</strong>, <strong>docx</strong>, <strong>odt</strong> etc.</p>
<p>My task is:
Giving a document, group the text in blocks finding the correct boundaries of each.</p>
<p>I did some tests using Apache Tika, which is a good extractor, it is a very good tool but it often mess up the order of the block, let me explain a bit what i mean with ORDER.</p>
<p>Apache Tika just extracts the text, so if my document has two columns, Tika extracts the entire text of the first column and then the text of the second column, which is ok...but sometimes the text on the first column is related to the text  on the second, like a table that has row relation.</p>
<p>So i must take care of the positions of each block, so the problems are:</p>
<ol>
<li><p>Define the box boundaries, which is hard... i should understand if a sentence is starting a new block or not.</p>
</li>
<li><p>Define the <em>orientation</em>, for example, giving a table the &quot;sentence&quot; should be the row, NOT the column.</p>
</li>
</ol>
<p>So basically here i have to deal with the <strong>layout structure</strong> to correcly understand the block boundaries.</p>
<p>I give you a visual example:</p>
<p><a href=""https://i.sstatic.net/i6vHT.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/i6vHT.png"" alt=""enter image description here"" /></a></p>
<p>A classical extractor returns:</p>
<pre><code>2019
2018
2017
2016
2015
2014
Oregon Arts Commission Individual Artist Fellowship...
</code></pre>
<p>Which is <em><strong>wrong</strong></em> (in my case) because the dates are related to the texts on the right.</p>
<p>This task is preparatory for other NLP analysis, so it is very important, because, for example doing, when i need to recognize the entities(NER) inside the text, and then identify their relations, <strong>working with the correct context is very important</strong>.</p>
<p>How to extract the text from the document and assembly related pieces of text (understanding the layout structure of the document) under the same block?</p>
",Named Entity Recognition (NER),document layout analysis text extraction need analyze layout structure different document type like pdf doc docx odt etc task giving document group text block finding correct boundary test using apache tika good extractor good tool often mess order block let explain bit mean order apache tika extract text document ha two column tika extract entire text first column text second column ok sometimes text first column related text second like table ha row relation must take care position block problem define box boundary hard understand sentence starting new block define orientation example giving table sentence row column basically deal layout structure correcly understand block boundary give visual example classical extractor return wrong case date related text right task preparatory nlp analysis important example need recognize entity ner inside text identify relation working correct context important extract text document assembly related piece text understanding layout structure document block
how to make colab use GPU for spacy training NER model,"<p>I have <strong>40,000</strong> records and I the training process is very <em>slow</em>, this is the line I use in colab for training</p>
<pre><code>! python -m spacy train config.cfg --output /content/ --paths.train /content/training_data.spacy --paths.dev /content/training_data.spacy
</code></pre>
<p>when I run this cell, this show up</p>
<blockquote>
<p>ℹ Saving to output directory: /content
<br/>ℹ Using CPU
<br/>ℹ To switch to GPU 0, use the option: --gpu-id 0</p>
</blockquote>
<p>I want to use the <code>GPU</code> but when I use the <code>--gpu-id 0</code> in end of cell command, I got <code>ERROR</code></p>
<p>and I changed runtime of colab to GPU</p>
",Named Entity Recognition (NER),make colab use gpu spacy training ner model record training process slow line use colab training run cell show saving output directory content using cpu switch gpu use option gpu id want use use end cell command got changed runtime colab gpu
How to extract a table without all borders into text with Python?,"<p>I am trying to extract a table like this into a Dataframe. How to do that (and extract even the names splitted on several lines) with Python?</p>
<p>Also, I want this to be general and to be applied on each table (even if it doesn't this structure), so giving the coordinates for each separate and different table won't work that well.</p>
<p><a href=""https://i.sstatic.net/xtY4t.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xtY4t.png"" alt=""enter image description here"" /></a></p>
",Named Entity Recognition (NER),extract table without border text python trying extract table like dataframe extract even name splitted several line python also want general applied table even structure giving coordinate separate different table work well
How to make spacy train faster on NER for Persian language,"<p>I have a blank model from <code>spacy</code>, in the <code>config file</code> I use the widget <a href=""https://spacy.io/usage/training"" rel=""nofollow noreferrer"">Training Pipelines &amp; Models</a> with this config:</p>
<blockquote>
<p>Language = Arabic
<br/>Components = ner
<br/>Hardware = CPU
<br/>Optimize for = accuracy</p>
</blockquote>
<p>then in <code>config-file</code> I changed the:</p>
<pre><code>[nlp]
lang = &quot;ar&quot;
</code></pre>
<p>to</p>
<pre><code>[nlp]
lang = &quot;fa&quot;
</code></pre>
<p>because there is no pretrained <code>GPU (transformer)</code> for <code>persian-language</code>.</p>
<p>and as you know the <code>accuracy</code> type is very slow and I have <strong>400,000</strong> records.</p>
<p>this is my <code>config-file</code></p>
<pre><code>[paths]
train = null
dev = null
vectors = null
[system]
gpu_allocator = null

[nlp]
lang = &quot;fa&quot;
pipeline = [&quot;tok2vec&quot;,&quot;ner&quot;]
batch_size = 1000

[components]

[components.tok2vec]
factory = &quot;tok2vec&quot;

[components.tok2vec.model]
@architectures = &quot;spacy.Tok2Vec.v2&quot;

[components.tok2vec.model.embed]
@architectures = &quot;spacy.MultiHashEmbed.v2&quot;
width = ${components.tok2vec.model.encode.width}
attrs = [&quot;ORTH&quot;, &quot;SHAPE&quot;]
rows = [5000, 2500]
include_static_vectors = true

[components.tok2vec.model.encode]
@architectures = &quot;spacy.MaxoutWindowEncoder.v2&quot;
width = 256
depth = 8
window_size = 1
maxout_pieces = 3

[components.ner]
factory = &quot;ner&quot;

[components.ner.model]
@architectures = &quot;spacy.TransitionBasedParser.v2&quot;
state_type = &quot;ner&quot;
extra_state_tokens = false
hidden_width = 64
maxout_pieces = 2
use_upper = true
nO = null

[components.ner.model.tok2vec]
@architectures = &quot;spacy.Tok2VecListener.v1&quot;
width = ${components.tok2vec.model.encode.width}

[corpora]

[corpora.train]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.train}
max_length = 0

[corpora.dev]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.dev}
max_length = 0

[training]
dev_corpus = &quot;corpora.dev&quot;
train_corpus = &quot;corpora.train&quot;

[training.optimizer]
@optimizers = &quot;Adam.v1&quot;

[training.batcher]
@batchers = &quot;spacy.batch_by_words.v1&quot;
discard_oversize = false
tolerance = 0.2

[training.batcher.size]
@schedules = &quot;compounding.v1&quot;
start = 100
stop = 1000
compound = 1.001

[initialize]
vectors = ${paths.vectors}
</code></pre>
<p>How can I make the training process faster?</p>
",Named Entity Recognition (NER),make spacy train faster ner persian language blank model use widget training pipeline model config language arabic component ner hardware cpu optimize accuracy changed pretrained know type slow record make training process faster
NER ORG search for a company returns the word &quot;company&quot; instead of its name,"<p>I'm working on an NLP/NER script using transformers/BERT and I'm having an issue extracting the name of a company from a set of texts.</p>
<p>In all the texts the script will be used on, the company's name will be presented like this:</p>
<p><em>&quot;COMPANY NAME: the company's name is XXX&quot;</em>
<strong>or</strong>
<em>&quot;NAME: the company's name is XXX&quot;</em></p>
<p>this is my code:</p>
<pre><code>def get_company_info(text, tokenizer_1, model_1, tokenizer_2, model_2):
    company_info = {&quot;name&quot;: None}
    try:
        start_company_index = re.search('name', text, re.I).span()[0]
        info = NLP_2(
            text[start_company_index:start_company_index+100], tokenizer_2, model_2)
        for data in info:
            if data['entity_group'] == 'ORG':
                company_info['name'] = data['word']
                break
    except:
        pass
</code></pre>
<p>However, the BERT script returns the word &quot;company&quot; since it finds it in the text and assumes correctly that it is the subject I'm looking for but I want to extract the name of the company instead.</p>
<p>Is there a simple way to avoid this or do I have to fine-tune the model?</p>
<p>I'm using regex to delimit the field of the search but I cannot simply use <code>re.search(&quot;company&quot;)</code> to start the search after the word company, because sometimes there will be 2 consecutive mentions of the word.</p>
",Named Entity Recognition (NER),ner org search company return word company instead name working nlp ner script using transformer bert issue extracting name company set text text script used company name presented like company name company name xxx name company name xxx code however bert script return word company since find text assumes correctly subject looking want extract name company instead simple way avoid fine tune model using regex delimit field search simply use start search word company sometimes consecutive mention word
How to improve Named Entity Recognition in edu.stanford.nlp.pipeline.StanfordCoreNLP,"<p>I am investigating NLP processing of large texts with <code>edu.stanford.nlp.pipeline.StanfordCoreNLP</code>.</p>
<p>with the following properties my results contain all required PERSON names, however there are a large amount of &quot;<em>mistakes</em>&quot;.</p>
<pre><code>        val props = Properties()
        props.setProperty(&quot;annotators&quot;, &quot;tokenize,ssplit,pos,lemma,ner,entitymentions,regexner,parse,depparse,coref,kbp,quote&quot;)
        props.setProperty(&quot;coref.algorithm&quot;, &quot;neural&quot;)

        consumeBook(&quot;book.txt&quot;, StanfordCoreNLP(props))
</code></pre>
<p>Where consume book resembles:-</p>
<pre><code>private fun consumeBook(fileName: String, pipeline: StanfordCoreNLP) = File(fileName).forEachLine { currentLine -&gt;

    val data = currentLine.trimIndent()

    if (data.isNotEmpty()) {
        val document = CoreDocument(data)

        pipeline.annotate(document)

        document.sentences().forEach { sentence -&gt;
            sentence.entityMentions().filter { it.entityType() == &quot;PERSON&quot; }.forEach { entityMention -&gt;
                characters.add(entityMention.text())
            }
        }
    }
}
</code></pre>
<p>where <code>characters</code> is declared as follows:-</p>
<pre><code>private val characters: MutableSet&lt;String&gt; = TreeSet()
</code></pre>
<p>My set of <code>characters</code> does contain all the full names of the books characters, however it also contains <code>LOCATION</code>s &amp; <code>CITY</code>s from the book along with values such as these</p>
<pre><code>he
her
hers
him
his
she
</code></pre>
<p>is there additional configuration i can add/refactor to improve the quality of the PERSON Named Entity Recognition?</p>
<p>from my investigation so far IMHO Standford NLP library is much better than Apache OpenNLP.</p>
<p>The version of Stanford NLP I am using is:-</p>
<pre><code>implementation 'edu.stanford.nlp:stanford-corenlp:4.4.0'
implementation 'edu.stanford.nlp:stanford-corenlp:4.4.0:models'
</code></pre>
",Named Entity Recognition (NER),improve named entity recognition edu stanford nlp pipeline stanfordcorenlp investigating nlp processing large text following property result contain required person name however large amount mistake consume book resembles declared follows set doe contain full name book character however also contains book along value additional configuration add refactor improve quality person named entity recognition investigation far imho standford nlp library much better apache opennlp version stanford nlp using
How to deploy a ML model for entity extraction (with SVM or Perceptron),"<p>I am trying to extract soft skills from vacancies and I am currently struggling with applying a ML model (SVM or Perceptron) to my dataset. First I made a custom spaCy NER that eventually resulted in a dataset for the model to train upon. This dataset looks like this:</p>
<pre><code>        words   POS        labels
37         We  PRON             O
38        are   AUX             O
39     always   ADV             O
40         on   ADP             O
41        the   DET             O
42    lookout  NOUN             O
43        for   ADP             O
44  motivated   ADJ  I-SOFT_SKILL
45       life  NOUN             O
46    science  NOUN             O
</code></pre>
<p>I want to extract the skills in different vacancies. And I created a perceptron that seems to be working.</p>
<pre><code>print('the shape is: ',X_train.shape, y_train.shape)
per = Perceptron(verbose=10, n_jobs=-1, max_iter=10)
per.partial_fit(X_train, y_train, classes)
</code></pre>
<p><strong>Output</strong></p>
<pre><code>the shape is:  (51759, 6155) (51759,)

[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.
-- Epoch 1-- Epoch 1

Norm: 17.15, NNZs: 95, Bias: -3.000000, T: 51759, Avg. loss: 0.009795
Total training time: 0.55 seconds.
-- Epoch 1
Norm: 37.97, NNZs: 378, Bias: -3.000000, T: 51759, Avg. loss: 0.018296
Total training time: 0.57 seconds.
Norm: 42.68, NNZs: 457, Bias: 3.000000, T: 51759, Avg. loss: 0.025696
Total training time: 0.50 seconds.
[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    1.1s remaining:    0.0s
[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    1.1s finished
Perceptron(max_iter=10, n_jobs=-1, verbose=10)
</code></pre>
<p>Now, when I try to apply the model to completely unseen data, I get a ValueError:</p>
<pre><code>ValueError: X has 6747 features, but Perceptron is expecting 6155 features as input.
</code></pre>
<p>so does the amount of features in the unseen vacancies need to be exactly the same as in the training model? Or is there another, better way to do this?</p>
<p>If additional information is needed I can provide that! Would be thankful if somebody knew the problem!</p>
",Named Entity Recognition (NER),deploy ml model entity extraction svm perceptron trying extract soft skill vacancy currently struggling applying ml model svm perceptron dataset first made custom spacy ner eventually resulted dataset model train upon dataset look like want extract skill different vacancy created perceptron seems working output try apply model completely unseen data get valueerror doe amount feature unseen vacancy need exactly training model another better way additional information needed provide would thankful somebody knew problem
"How to replace a word in a string, based on a condition?","<p>I have a column in a dataframe like this.</p>
<pre><code>Text
&quot;Lorum Ipsum Rotterdam dolor sit.&quot; 
&quot;ed ut perspiciatis Boekarest, New York, consectetur adipiscing elit, sed &quot; 
&quot;Excepteur sint occaecat Glasgow cupidatat non proident, sunt in culpa&quot;
</code></pre>
<p>I want every geographical location to be replaced by &quot;<em>GPE</em>&quot;.</p>
<p>I am using spacy to detect the entities. This works fine, as shown below.</p>
<pre><code>nlp = spacy.load('en_core_web_lg')

for value in df['text']:
    doc = nlp(value)
    for ent in doc.ents:
        print(ent.text, ent.label_)
</code></pre>
<pre><code>Output: 
Rotterdam GPE
Boekarest GPE
New York GPE
Glasgow GPE 
</code></pre>
<p>I tried the code below in order to replace the city names within the columns, but it doesn't work.</p>
<pre><code>for value in df['text']:
    doc = nlp(value)
    for ent in doc.ents:
        for word in value.split():
            if ent.label_ == &quot;GPE&quot;:
                word.replace(ent.label, &quot;_GPE_&quot;)
</code></pre>
<p>Does anyone see what I am doing wrong?</p>
",Named Entity Recognition (NER),replace word string based condition column dataframe like want every geographical location replaced gpe using spacy detect entity work fine shown tried code order replace city name within column work doe anyone see wrong
How to tokenize double dots as separate tokens in spaCy?,"<p>I have code like that:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
nlp = spacy.load('de_core_news_md')
doc = nlp('92637 Weiden i.d.OPf..')
tokens = list(doc)
</code></pre>
<p>So, tokens looks like that:</p>
<pre><code>92637
Weiden
i.d
.
OPf
..
</code></pre>
<p>How can I split last (<strong>double-dot</strong>) token into two tokens with <strong>single-dot</strong>?<br />
It's necessary for fit into NER-labeling.</p>
<p>So, I expecting that:</p>
<pre><code>92637
Weiden
i.d
.
OPf
.
.
</code></pre>
",Named Entity Recognition (NER),tokenize double dot separate token spacy code like token look like split last double dot token two token single dot necessary fit ner labeling expecting
Using NLP or machine learning to extract keywords off a sentence,"<p>I'm new to the ML/NLP field so my question is what technology would be most appropriate to achieve the following goal:</p>

<p>We have a short sentence - ""Where to go for dinner?"" or ""What's your favorite bar?"" or ""What's your favorite cheap bar?""</p>

<p>Is there a technology that would enable me to train it providing the following data sets:</p>

<ul>
<li>""Where to go for dinner?"" -> Dinner</li>
<li>""What's your favorite bar?"" -> Bar</li>
<li>""What's your favorite cheap restaurant?"" -> Cheap, Restaurant</li>
</ul>

<p>so that next time we have a similar question about an unknown activity, say, ""What is your favorite expensive [whatever]"" it would be able to extract ""expensive"" and [whatever]?</p>

<p>The goal is if we can train it with hundreds of variations(or thousands) of the question asked and relevant output data expected, so that it can work with everyday language.</p>

<p>I know how to make it even without NLP/ML if we have a dictionary of expected terms like Bar, Restaurant, Pool, etc., but we also want it to work with unknown terms.</p>

<p>I've seen examples with Rake and Scikit-learn for classification of ""things"", but I'm not sure how would I feed text into those and all those examples had predefined outputs for training.</p>

<p>I've also tried Google's NLP API, Amazon Lex and Wit to see how good they are at extracting entities, but the results are disappointing to say the least.</p>

<p>Reading about summarization techniques, I'm left with the impression it won't work with small, single-sentence texts, so I haven't delved into it.</p>
",Named Entity Recognition (NER),using nlp machine learning extract keywords sentence new ml nlp field question technology would appropriate achieve following goal short sentence go dinner favorite bar favorite cheap bar technology would enable train providing following data set go dinner dinner favorite bar bar favorite cheap restaurant cheap restaurant next time similar question unknown activity say favorite expensive whatever would able extract expensive whatever goal train hundred variation thousand question asked relevant output data expected work everyday language know make even without nlp ml dictionary expected term like bar restaurant pool etc also want work unknown term seen example rake scikit learn classification thing sure would feed text example predefined output training also tried google nlp api amazon lex wit see good extracting entity result disappointing say least reading summarization technique left impression work small single sentence text delved
Extracting country name from an address,"<p>I've a large dataset with an address column. I would like to extract the countries from the address. In many cases, the address column contains states, cities, and zip code, but the country names.  You can see samples of my data</p>
<p><img src=""https://www.linkpicture.com/q/Screen-Shot-2022-06-26-at-12.56.41-PM.png"" alt=""Text"" /></p>
<p>I'm using python, How I can extract the country name in all these cases.</p>
",Named Entity Recognition (NER),extracting country name address large dataset address column would like extract country address many case address column contains state city zip code country name see sample data using python extract country name case
NER - how to check if a common noun indicates a place (subcategorization),"<p>I am looking for a way to find, in a sentence, if a common noun refers to places. This is easy for proper nouns, but I didn't find any straightforward solution for common nouns.</p>
<p>For example, given the sentence <em>&quot;After a violent and brutal attack, a group of college students travel into the countryside to find refuge from the town they fled, but soon discover that the small village is also home to a coven of serial killers&quot;</em> I would like to mark the following nouns as referred to places: <em>countryside</em>, <em>town</em>, <em>small village</em>, <em>home</em>.</p>
<p>Here is the code I'm using:</p>
<pre><code>import spacy
nlp = spacy.load('en_core_web_lg')

# Process whole documents
text = (&quot;After a violent and brutal attack, a group of college students travel into the countryside to find refuge from the town they fled, but soon discover that the small village is also home to a coven of satanic serial killers&quot;)
doc = nlp(text)

# Analyze syntax
print(&quot;Noun phrases:&quot;, [chunk.text for chunk in doc.noun_chunks])
print(&quot;Verbs:&quot;, [token.lemma_ for token in doc if token.pos_ == &quot;VERB&quot;])

# Find named entities, phrases and concepts
for entity in doc.ents:
    print(entity.text, entity.label_)
</code></pre>
<p>Which gives as output the following:</p>
<pre><code>Noun phrases: ['a violent and brutal attack', 'a group', 'college students', 'the countryside', 'refuge', 'the town', 'they', 'the small village', 'a coven', 'serial killers']
Verbs: ['travel', 'find', 'flee', 'discover']
</code></pre>
",Named Entity Recognition (NER),ner check common noun indicates place subcategorization looking way find sentence common noun refers place easy proper noun find straightforward solution common noun example given sentence violent brutal attack group college student travel countryside find refuge town fled soon discover small village also home coven serial killer would like mark following noun referred place countryside town small village home code using give output following
How to create a custom spaPcy pipeline component using the Thinc model,"<p>I'd like to create a custom pipeline component in spaCy which uses a pre-trained Thinc model. I'd like to modify the output prediction from Thinc and then pass the modified value back into the pipeline i.e. effectively modifying the ner pipeline component.</p>
<p><a href=""https://i.sstatic.net/JMMBJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JMMBJ.png"" alt=""SpaCy pipeline"" /></a></p>
<p>I was thinking of doing this via a custom pipeline component, something like:</p>
<pre><code>from spacy.language import Language

@Language.component(&quot;my_ner&quot;)
def my_ner(doc):

    class_probabilities = thinc_do_something(data, model, num_samples)
    class_value = np.argmax(class_probabilities, axis=1)
    
    return doc

nlp = spacy.load(&quot;en_core_web_sm&quot;, exclude=[&quot;ner&quot;])
nlp.add_pipe(&quot;my_ner&quot;, after=&quot;parser&quot;)  # Insert after the parser
print(nlp.pipe_names)  # ['tagger', 'parser', 'my_ner']
doc = nlp(&quot;This is a sentence.&quot;)
</code></pre>
<p>My aim is for the pipe to run as per the original ner component, but with my custom ner component modifying the class probabilities. Unfortunately I don't understand from the spaCy documentation:</p>
<ul>
<li>How to access the pre trained model from inside the pipeline?</li>
<li>How to access the data used for the model prediction within the pipeline?</li>
<li>Where I need to write the model predicted value back to as part of my modified ner pipline?</li>
<li>Is there a better way of doing this?</li>
</ul>
",Named Entity Recognition (NER),create custom spapcy pipeline component using thinc model like create custom pipeline component spacy us pre trained thinc model like modify output prediction thinc pas modified value back pipeline e effectively modifying ner pipeline component wa thinking via custom pipeline component something like aim pipe run per original ner component custom ner component modifying class probability unfortunately understand spacy documentation access pre trained model inside pipeline access data used model prediction within pipeline need write model predicted value back part modified ner pipline better way
BERT embeddings + LSTM for NER,"<p>I am working with the Conll-2003 dataset for Named Entity Recognition. What I want to do is to use the BERT embeddings as an input to a simple LSTM. Here's the code:</p>
<pre><code>class Model(nn.Module):

    def __init__(self, params):
        super(Model, self).__init__()

        self.params = params

        self.embedding = BertModel.from_pretrained(&quot;bert-large-cased&quot;)
        self.tokenizer = BertTokenizer.from_pretrained(&quot;bert-large-cased&quot;)

        for param in self.embedding.parameters():
            param.requires_grad = False                 

        params.max_sen_len += 2     # &quot;[CLS]&quot;, &quot;[SEP]&quot;
        params.embedding_dim = params.bert_dim

        self.lstm = nn.LSTM(params.embedding_dim, params.hidden_dim, batch_first=True)
        self.fc = nn.Linear(params.hidden_dim, params.num_of_tags)


    def forward(self, sentences, labels):
        tokenized_sentences = []
        tokenized_sen = []
        tokenized_word = []
        
        tokenized_labels = []
        tokenized_sen_labels = []
        idx = -1
        is_first = True

        for sen, lab in zip(sentences, labels):
            if sen[0] != &quot;[CLS]&quot;:
                sen.insert(0, &quot;[CLS]&quot;)
                sen.append(&quot;[SEP]&quot;)
                lab.insert(0, self.params.pad_tag_num)
                lab.append(self.params.pad_tag_num)
            idx = -1
            for word in sen:
                idx += 1
                tokenized_word = self.tokenizer.tokenize(word)

                is_first = True
                for token in tokenized_word:
                    tokenized_sen.append(token)
                    if is_first:
                        tokenized_sen_labels.append(lab[idx])
                        is_first = False
                    else:
                        tokenized_sen_labels.append(-1)

            tokenized_sentences.append(tokenized_sen)
            tokenized_labels.append(tokenized_sen_labels)
            tokenized_sen = []
            tokenized_sen_labels = []

        labels = tokenized_labels
        labels = pad_sequences([[l for l in lab] for lab in labels],
            maxlen=self.params.max_sen_len, value=self.params.pad_tag_num, padding=&quot;post&quot;,  
            dtype=&quot;long&quot;, truncating=&quot;post&quot;)

        mask = (labels &gt;= 0)
        mask = torch.FloatTensor(mask)
        if self.params.cuda:
            mask = mask.cuda()

        inputs = pad_sequences([self.tokenizer.convert_tokens_to_ids(sen) for sen in tokenized_sentences],
                        maxlen=self.params.max_sen_len, dtype=&quot;long&quot;, truncating=&quot;post&quot;, padding=&quot;post&quot;)

        inputs = torch.LongTensor(inputs)
        if self.params.cuda:
            inputs = inputs.cuda()

        x = self.embedding(inputs, attention_mask = mask)[0]

        x, _ = self.lstm(x)
        x = x.contiguous()
        x = x.view(-1, x.shape[2])
        x = self.fc(x)

        return F.log_softmax(x, dim=1), labels


self.params.pad_tag_num = -1
</code></pre>
<p>I use only the first wordpiece of each tokenized word, the rest is set to -1.</p>
<p>My first question is: am I doing it the right way? I did the same with ELMo and GloVe embeddings and the worst accuracy I obtain for BERT (haven't checked the F1 score).</p>
<p>The second question is: Should I somehow care about the entity labels? I mean there are labels like B-PER and I-PER. This is the same type of an enity. Should I somehow treat it as one entity and classify the whole? Or should I classify each word?</p>
",Named Entity Recognition (NER),bert embeddings lstm ner working conll dataset named entity recognition want use bert embeddings input simple lstm code use first wordpiece tokenized word rest set first question right way elmo glove embeddings worst accuracy obtain bert checked f score second question somehow care entity label mean label like b per per type enity somehow treat one entity classify whole classify word
Translate dataset for NER spacy model,"<p>I am trying to train a model for entity recognition on resumes. More specifically, I am trying to train a model to recognize education, professional experience, skills, etc.. on resumes. I am using a dataset of resumes I found online that is already formatted in a way that a spacy 'ner' model would recognize. But the dataset is in English, and I need French data.
At some point, I will probably build the dataset manually, but for now I am going to settle for translating the dataset I already have.
For example, let's manufacture a datapoint:</p>
<p>[['I went to New York ', {entity : [11,19, Location], [3, 7, verb]}]]. The numbers represent the position of the first and last character. So 'New York' is a location.</p>
<p>So the issue here is that the translation will shift, change, the position of the entities that are important for us. So then my question is  : Is there a better way to do this ?</p>
",Named Entity Recognition (NER),translate dataset ner spacy model trying train model entity recognition resume specifically trying train model recognize education professional experience skill etc resume using dataset resume found online already formatted way spacy ner model would recognize dataset english need french data point probably build dataset manually going settle translating dataset already example let manufacture datapoint went new york entity location verb number represent position first last character new york location issue translation shift change position entity important u question better way
How can I extract some contents in the cells of web-scraped csv file?,"<p>I am struggling with dealing with a csv file that scraped one crowdfunding website.</p>
<p>My goal is successfully load all information as separate columns, but I found some information are mixed in a single column when I load it using 1) R, 2) Stata, and 3) Python.</p>
<p>Since the real data is really dirty, let me suggest abbreviate version of current dataset.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Pledge</th>
<th>creator</th>
</tr>
</thead>
<tbody>
<tr>
<td>000001</td>
<td>13.7</td>
<td>{&quot;urls&quot;:{&quot;web&quot;:{&quot;user&quot;:&quot;www.kickstarter.com/profile/731&quot;}}, &quot;name&quot;:John&quot;,&quot;id&quot;:709510333}</td>
</tr>
<tr>
<td>000002</td>
<td>26.4</td>
<td>{&quot;urls&quot;:{&quot;web&quot;:{&quot;user&quot;:&quot;www.kickstarter.com/profile/759&quot;}}, &quot;name&quot;:Kellen&quot;,&quot;id&quot;:703514812}</td>
</tr>
<tr>
<td>000003</td>
<td>7.6</td>
<td>{&quot;urls&quot;:{&quot;web&quot;:{&quot;user&quot;:&quot;www.kickstarter.com/profile/7522&quot;}}, &quot;name&quot;:Jach&quot;,&quot;id&quot;:609542647}</td>
</tr>
</tbody>
</table>
</div>
<p>My goal was extracting the &quot;name&quot; and &quot;id&quot; as separate columns, though they are all mixed with URLs in the creator column.</p>
<p>Is there any way that I can extract names (John, Kellen, Jach) and ids as separate columns?
I prefer R, but Stata and Python would also be helpful!</p>
<p>Thank you so much for considering this.</p>
",Named Entity Recognition (NER),extract content cell web scraped csv file struggling dealing csv file scraped one crowdfunding website goal successfully load information separate column found information mixed single column load using r stata python since real data really dirty let suggest abbreviate version current dataset id pledge creator url web user name john id url web user name kellen id url web user name jach id goal wa extracting name id separate column though mixed url creator column way extract name john kellen jach id separate column prefer r stata python would also helpful thank much considering
How can I extract some contents in the web-scraped csv file?,"<p>I am struggling with dealing with a csv file that scraped one crowdfunding website.</p>
<p>My goal is loading the csv file on Stata, and save it in .dta format. (Stata dataset).</p>
<p>Stata successfully loaded the dataset, like the screenshot I attached.
v20, v21 are variables I wanted.</p>
<p>In v22, I wanted to know the name of &quot;creator&quot;. However, it is not successfully excluded. As the selected cell in the screenshot shows.
<a href=""https://i.sstatic.net/xq0OX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xq0OX.jpg"" alt=""enter image description here"" /></a>
Is there any way that I can extract &quot;name&quot; and &quot;id&quot; of the creator as separate columns?</p>
<p>Also, in v23, I want to extract the location of each row.
<a href=""https://i.sstatic.net/36lMX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/36lMX.jpg"" alt=""enter image description here"" /></a>
However, I found the &quot;name&quot;, &quot;state&quot;, &quot;short_name&quot; are in the cell, without being extracted separately.
<a href=""https://i.sstatic.net/iOlcD.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iOlcD.jpg"" alt=""enter image description here"" /></a></p>
<p>Would there be any way that I can load them as separate columns?</p>
<p>Thank you for reading my post! Any help will be appreciated:)</p>
",Named Entity Recognition (NER),extract content web scraped csv file struggling dealing csv file scraped one crowdfunding website goal loading csv file stata save dta format stata dataset stata successfully loaded dataset like screenshot attached v v variable wanted v wanted know name creator however successfully excluded selected cell screenshot show way extract name id creator separate column also v want extract location row however found name state short name cell without extracted separately would way load separate column thank reading post help appreciated
How to change a pandas column with lists in every row to strings,"<p>I have used a Named Entity Recognizer to extract skills from job vacancies. This worked, but now the entities are in a pandas column as lists. I want to create a new column with just the skills and using the replace function or astype(str) both do not work.</p>
<p>Here you can see my dataframe:</p>
<pre><code>                                                Skills
0                  [(Responsibility), (communication)]
1    [(responsibility), (responsibility), (Leadersh...
2                                      [(Flexibility)]
3                                                   []
4                                      [(communicate)]
Name: skills, dtype: object
</code></pre>
<p>This is how I eventually want my column to look like:</p>
<pre><code>                                          Skills_clean
0                        Responsibility, communication
1           responsibility, responsibility, Leadership
2                                          Flexibility
3                                                  NaN
4                                          communicate
Name: skills, dtype: object
</code></pre>
<p>So I want to lose all the square and round brackets, or does any body have a better idea on how to do this?</p>
",Named Entity Recognition (NER),change panda column list every row string used named entity recognizer extract skill job vacancy worked entity panda column list want create new column skill using replace function astype str work see dataframe eventually want column look like want lose square round bracket doe body better idea
"How do I loop through a column containing geo-coordinate text and extract latitude and longitude, and append as separate columns?","<p>I have some data from CDC in csv format which includes geo-coordinates in a single text column as follows.  The name of the (Pandas) DataFrame is 'geotest':</p>
<pre><code># My original data from CDC:
geotest.head(2)

RiskFactor  GeoLocation
Smoking     (44.9033791, -122.9017427)
Alcohol     (38.84054, -97.6097)
</code></pre>
<p>What I would like to do is extract the latitude and longitudinal coordinates and append them to separate columns to my original dataset as follows:</p>
<pre><code># How I would like my data to look like:  Original + two new columns appended:
RiskFactor  GeoLocation                  Latitude    Longitude
Smoking     (44.9033791, -122.9017427)   44.9033791  -122.9017427
Alcohol     (38.84054,   -97.6097)       38.84054    -97.6097
</code></pre>
<p>Here is the Python code I have tried but which fails to accomplish what I would like to do. My guess is that the &quot;(&quot; and &quot;,&quot; characters are key regex items to do this. The code inside the loop works on individual string examples but, like most tutorials on the web, they seldom discuss how to loop through multiple rows of data and are therefore unhelpful at scaling up to real-world applications:</p>
<pre><code>import re
# finds latitude:
for x in geotest:
    start = s.find('(') + 1
    end = s.find(',', start)
    s[start:end]
# finds longitude
for x in geotest:
    start = f.find(',') + 1
    end = f.find(')', start)
    f[start:end]
</code></pre>
<p>Thank you all for any guidance!</p>
",Named Entity Recognition (NER),loop column containing geo coordinate text extract latitude longitude append separate column data cdc csv format includes geo coordinate single text column follows name panda dataframe geotest would like extract latitude longitudinal coordinate append separate column original dataset follows python code tried fails accomplish would like guess character key regex item code inside loop work individual string example like tutorial web seldom discus loop multiple row data therefore unhelpful scaling real world application thank guidance
R - how to count all words in a df row and add output to a new column? Ideally with tidyverse or tidytext,"<p>I'm trying to find the location of words in a text, and also the total wordcount of the same text.</p>
<pre><code># library(tidyverse)
# library(tidytext)
txt&lt;-tibble(text=c(&quot;we're meeting here today to talk about our earnings. we will also discuss global_warming.&quot;, &quot;hi all, global_warming and the on-going strike is at the top of our agenda, because unionizing threatens our revenue goals.&quot;, &quot;we will discuss global_warming tomorrow, today the focus is our Q3 earnings&quot;))
dict &lt;- tibble(words=c(&quot;global_warming&quot;))
x&lt;-txt %&gt;% unnest_tokens(output = &quot;words&quot;,
                          input = &quot;text&quot;,
                          drop = FALSE) %&gt;%
  group_by(text) %&gt;%
  mutate(word_loc = row_number()) %&gt;%
  ungroup() %&gt;%
  inner_join(dict)
</code></pre>
<p>This gives me the following output:</p>
<pre><code># A tibble: 3 x 3
  text                                                                                        words        word_loc
  &lt;chr&gt;                                                                                       &lt;chr&gt;           &lt;int&gt;
1 we're meeting here today to talk about our earnings. we will also discuss global_warming.   global_warm…       14
2 hi all, global_warming and the on-going strike is at the top of our agenda, because unioni… global_warm…        3
3 we will discuss global_warming tomorrow, today the focus is our Q3 earnings                 global_warm…        4
</code></pre>
<p>How can I add one column, that gives me the total word count for each row?</p>
",Named Entity Recognition (NER),r count word df row add output new column ideally tidyverse tidytext trying find location word text also total wordcount text give following output add one column give total word count row
Removing named entities from a document using spacy,"<p>I have tried to remove words from a document that are considered to be named entities by spacy, so basically removing ""Sweden"" and ""Nokia"" from the string example. I could not find a way to work around the problem that entities are stored as a span. So when comparing them with single tokens from a spacy doc, it prompts an error.</p>

<p>In a later step, this process is supposed to be a function applied to several text documents stored in a pandas data frame.</p>

<p>I would appreciate any kind of help and advice on how to maybe better post questions as this is my first one here.</p>

<pre><code>
nlp = spacy.load('en')

text_data = u'This is a text document that speaks about entities like Sweden and Nokia'

document = nlp(text_data)

text_no_namedentities = []

for word in document:
    if word not in document.ents:
        text_no_namedentities.append(word)

return "" "".join(text_no_namedentities)

</code></pre>

<p>It creates the following error:</p>

<blockquote>
  <p>TypeError: Argument 'other' has incorrect type (expected spacy.tokens.token.Token, got spacy.tokens.span.Span)</p>
</blockquote>
",Named Entity Recognition (NER),removing named entity document using spacy tried remove word document considered named entity spacy basically removing sweden nokia string example could find way work around problem entity stored span comparing single token spacy doc prompt error later step process supposed function applied several text document stored panda data frame would appreciate kind help advice maybe better post question first one creates following error typeerror argument ha incorrect type expected spacy token token token got spacy token span span
"In R, how to find the locations of all dictionary words, in a dataframe?","<p>I'm analyzing corporate meetings, and I want to measure at what time people in the meetings bring up certain topics. Time meaning the location of the words.</p>
<p>For example, in three meetings, when do people bring up &quot;unionizing&quot; and other words in my dictionary?</p>
<pre><code>df &lt;- data.frame(text = c(&quot;we're meeting here today to talk about our earnings. we will also discuss unionizing efforts.&quot;, &quot;hi all, unionizing and the on-going strike is at the top of our agenda, because unionizing threatens our revenue goals.&quot;, &quot;we will discuss unionizing tomorrow, today the focus is our Q3 earnings&quot;))

dict &lt;- c(&quot;unions&quot;, &quot;strike&quot;, &quot;unionizing&quot;)
</code></pre>
<p>Desired output:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>text</th>
<th>count</th>
<th>word</th>
</tr>
</thead>
<tbody>
<tr>
<td>we're meeting here today...</td>
<td>(location of word)</td>
<td>unionizing</td>
</tr>
<tr>
<td>hi all, unionizing an...</td>
<td>(location of word)</td>
<td>unionizing</td>
</tr>
<tr>
<td>hi all, unionizing an...</td>
<td>(location of word)</td>
<td>strike</td>
</tr>
<tr>
<td>hi all, unionizing an...</td>
<td>(location of word)</td>
<td>unionizing</td>
</tr>
<tr>
<td>we will discuss unionizing tomorrow...</td>
<td>(location of word)</td>
<td>unionizing</td>
</tr>
</tbody>
</table>
</div>
<p>I asked a question about finding the first time a word is used, <a href=""https://stackoverflow.com/questions/72392438/in-r-how-to-find-the-location-of-a-word-in-a-string"">here</a>, and I tried to modify the code, but was unsuccessful.</p>
",Named Entity Recognition (NER),r find location dictionary word dataframe analyzing corporate meeting want measure time people meeting bring certain topic time meaning location word example three meeting people bring unionizing word dictionary desired output text count word meeting today location word unionizing hi unionizing location word unionizing hi unionizing location word strike hi unionizing location word unionizing discus unionizing tomorrow location word unionizing asked question finding first time word used href tried modify code wa unsuccessful p
Adding more custom entities into pretrained custom NER Spacy3,"<p>I've a huge amount of textual data and wanted to add around 50 different entities. Initially when I started working with it, I was getting memory error. As we know spacy can handle 1,00,000 tokens per GB and maximum up to 10,00,000. So I chunked my dataset into 5 sets and using annotator created multiple JSON file for the same. Now I started with one JSON and successfully completed creating the model and now I want to add more data into it so that I don't miss out any tags and there's a good variety of data is used while training in the model. Please guide me how to proceed next.</p>
",Named Entity Recognition (NER),adding custom entity pretrained custom ner spacy huge amount textual data wanted add around different entity initially started working wa getting memory error know spacy handle token per gb maximum chunked dataset set using annotator created multiple json file started one json successfully completed creating model want add data miss tag good variety data used training model please guide proceed next
Rasa: Conflict between regex entiity extractor and dietClassifier,"<p>I have two entity named entity1 and entity2 <br>
entity1 consists of examples like &quot;abc&quot; &quot;def&quot; &quot;xyz&quot; <br>
entity2 consists of examples like &quot;abc/123&quot; &quot;def/325&quot; <br></p>
<p>But regex entity extractor is extracting abc from abc/123 and storing it in entity1 slot instead of storing entire abc/123 in entity2 slot and diet classifier is being overriden by regex entity extractor.<br></p>
<p>I have added lot of training examples, but still result is same.</p>
",Named Entity Recognition (NER),rasa conflict regex entiity extractor dietclassifier two entity named entity entity entity consists example like abc def xyz entity consists example like abc def regex entity extractor extracting abc abc storing entity slot instead storing entire abc entity slot diet classifier overriden regex entity extractor added lot training example still result
Are there any pre-trained NER entity-linking models available?,"<p>I want to run entity-linking for a project of mine. I used Spacy for the NER on a corpus of documents. Is there an existing linking model I can simply use to link the entities found?</p>
<p>The documentation I have found seems to be how to train a custom one.</p>
<p>Examples:</p>
<p><a href=""https://spacy.io/api/kb"" rel=""nofollow noreferrer"">https://spacy.io/api/kb</a></p>
<p><a href=""https://github.com/explosion/spaCy/issues/4511"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/issues/4511</a></p>
<p>Thanks!</p>
",Named Entity Recognition (NER),pre trained ner entity linking model available want run entity linking project mine used spacy ner corpus document existing linking model simply use link entity found documentation found seems train custom one example thanks
NLP Text Pre Processing,"<p>I'm trying to pre-process some case judgement files available on the internet. These files are in JSON format but some of the files have strange text in them. I initially thought this could be utf-8 encoding issue but it doesn't solve the problem.</p>
<p>Below is a sample json text:
I would like to clean the text and perform text analytics to identify Named Entities.</p>
<pre><code>Karnataka High Court
Shinde, on 4 January, 2010
Author: Mohan Shantanagoudar
EN 'THE 1-ma COURT OF KARNATAKA CIRCUIT Bmca
AT GUI.-BARGA T '

DA'1'ED THIS THE ow: DAY OF' JANUAR§f&quot;'2C}.'1~{3._j:    

BEFORE

HONBLE MRJUSTICE MOHAN SE3-®;si*;';axt$:;é; GOU:&gt;%a$gi=:;;vT 3
\,v.P_Ng.40334;2QQ8 (éMl&lt;:pcT;« &quot;&quot;  
Bmmfaam:     

SURESE-I CHANDRA ,   - 
S/O LATE,' SHANKECRRAO .SHI'E$II);E§ &quot;
AGE ABOUT 52 YEARS, ' L 9'   
occ:AGR1CUL*mR1=:. &amp;A9voc';acY ' 
R:E:s1DENT Oms'-Q'NME§,KuN_19A..4  _ .«
TALUK: B}iALKI, §*éOW_R.E.S;IDING M' &quot; 
H.No.19--1C'--,-'4, NfEW*&quot;A'DARf3}i&quot;*~COLCNY_ '
BIDAR. D1533: BEAR.   

 1111    ._ ..PETITEONERS
{SR1 K;..M.&lt;j;HAi:r;..;x);3voa::é:1?:x;3-- I

SU'N'I'i'A xa§;&quot;0% SU3§iE_§SA1~~§ bciégxmorarx SE-{KNEE
AG  93395'? 4-:2. YE2}\RSs
ace: 1~§Q&quot;TU_SI*3WII'%&quot;i9:., ..... .4 «

 &quot; R.E;s:E&gt;::é§'r%'a3:§«f H.N9.§9/'Q 1, I\z'IUKEEI\EiE) NAGAR,

 ' Eﬁ}£;'§E{Ekf§iE'E3E§TEEi§._S0L§s.£3&quot;ﬁE?;{

., REZSELQNDE,-'1'éf&quot;§'



'EE'1§¢:~:.i£'€;:&quot;§i. ?€'€.§%i&lt;3:;. 32$ féieé :m.{§%3:' :%r%':££:E€$ 226 5: 22?

 :3? %;E:{7:-§:{m::s€é:§':,z':'E.i:';:: Q? §E.1€:§§.f&amp;-'£1; grag,-iiiggg if: q':,:.::2;-33%: {her {ﬁfﬁﬁf
</code></pre>
<p>Can anyone help me with some direction to solve this problem? Are these files corrupted?</p>
",Named Entity Recognition (NER),nlp text pre processing trying pre process case judgement file available internet file json format file strange text initially thought could utf encoding issue solve problem sample json text would like clean text perform text analytics identify named entity anyone help direction solve problem file corrupted
Does the IOB tagging method for Named Entity Recognition (NER) has any advantage in terms of model accuracy or computational time?,"<p>Can we do NER without the IOB tags and with only the entities as labels? I am specifically working on token classification for visual documents like receipts. For example, <a href=""https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/CORD/Fine_tuning_LayoutLMv2ForTokenClassification_on_CORD.ipynb"" rel=""nofollow noreferrer"">This HuggingFace tutorial</a> for LayoutLM on the CORD dataset for receipt information extraction does not use the IOB scheme.</p>
<p>I have trained the LayoutLMv2 model without IOB tagging and it trains well. But will doing it with IOB tags make any difference?</p>
",Named Entity Recognition (NER),doe iob tagging method named entity recognition ner ha advantage term model accuracy computational time ner without iob tag entity label specifically working token classification visual document like receipt example huggingface tutorial layoutlm cord dataset receipt information extraction doe use iob scheme trained layoutlmv model without iob tagging train well iob tag make difference
"Using Topic Modelling or another NLP approach, is it possible to define words that go into topics/categories for better defined topic model?","<p>I have a problem where I am using topic modelling and taking into consideration LDA &amp; LSA approaches however have found that some of the topics are not being defined as accurately as I like. Is it possible to define words into topics to help the allow the machine to learn better and easier? If not, what techniques could I alternatively use to counter this problem?</p>
<p>As previously explained, I have tried LDA and LSA techniques for topic modelling and found LDA to be most accurate giving a coherence score of 0.46, and have redefined the topic names. However, the words in the topics do not reflect the topic names, and this requires tuning of the model.</p>
<p>I have researched into other NLP solutions such as keyword extractions and named entity relationship (NER) but do not think they are suitable for my problem.</p>
<p>I am wanting to have 2 levels of categorization if possible, where level 1 is an overview and level 2 is in more detail. The example below is a loosely summarized customer feedback example:</p>
<p><strong>Level 1</strong></p>
<ul>
<li><p>Training</p>
</li>
<li><p>Communication</p>
</li>
<li><p>Technology</p>
</li>
<li><p>Products &amp; Services</p>
</li>
<li><p>Other</p>
</li>
</ul>
<p><strong>Level 2</strong></p>
<ul>
<li><p>Internal</p>
</li>
<li><p>External</p>
</li>
<li><p>Resolution Good</p>
</li>
<li><p>Resolution Bad</p>
</li>
<li><p>Unclear feedback</p>
</li>
</ul>
<p>Ideally this is the format I would like the topic modelling output to produce but unsure if this is viable?</p>
<p>Realistically, working on the weighting of the text would work. Example:</p>
<p>'<em>Great training from the company</em>' - Would be categorized as Training (Level 1) and Resolution Good (level 2). The words being picked up here are great and training as they outweigh the other words in terms of categorization.</p>
<p>Happy to provide further information if required.</p>
",Named Entity Recognition (NER),using topic modelling another nlp approach possible define word go topic category better defined topic model problem using topic modelling taking consideration lda lsa approach however found topic defined accurately like possible define word topic help allow machine learn better easier technique could alternatively use counter problem previously explained tried lda lsa technique topic modelling found lda accurate giving coherence score redefined topic name however word topic reflect topic name requires tuning model researched nlp solution keyword extraction named entity relationship ner think suitable problem wanting level categorization possible level overview level detail example loosely summarized customer feedback example level training communication technology product service level internal external resolution good resolution bad unclear feedback ideally format would like topic modelling output produce unsure viable realistically working weighting text would work example great training company would categorized training level resolution good level word picked great training outweigh word term categorization happy provide information required
How to add a custom tag to spacy NER,"<p>I would like to add some tags to <code>&quot;en_core_web_sm&quot;</code>
For example: I want the Spacy NER model to tag &quot;16GB&quot; as a RAM and &quot;Intel core i7&quot; as a CPU
How can I do this?</p>
<p>I tried the following:</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_sm&quot;) 
ruler = EntityRuler(nlp)
patterns = [{&quot;label&quot;: &quot;RAM&quot;, &quot;pattern&quot;: &quot;16gb ram&quot;}]


ruler = nlp.add_pipe(&quot;entity_ruler&quot;)
ruler.add_patterns(patterns)
doc = nlp(&quot;16gb ram is my order&quot;)
for ent in doc.ents:
    print(ent.text,ent.label_)
</code></pre>
<p>and the output was: 16 CARDINAL</p>
",Named Entity Recognition (NER),add custom tag spacy ner would like add tag example want spacy ner model tag gb ram intel core cpu tried following output wa cardinal
"Encoding IOB format, entity nested inside other entity","<p>I have a dataset and I have to do named entity recognition with it. I would convert the dataset which is a json to IOB format but i have an issue:
The dataset contains entity nested in other entity, for example</p>
<p>Sentence:</p>
<pre><code>Traitement des manifestations neurologiques progressives des patients adultes et des enfants atteints de maladie de Niemann-Pick type C
</code></pre>
<p>Entity:</p>
<pre><code>&quot;manifestations neurologiques progressives des patients  atteints de maladie de Niemann-Pick type C&quot;    and     &quot;adultes&quot;    and    &quot;enfants&quot; 
</code></pre>
<p>How should I encode the bigger one with nested entity inside?</p>
<p>I thought about:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">word</th>
<th style=""text-align: left;"">tag</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">Traitement</td>
<td style=""text-align: left;"">O</td>
</tr>
<tr>
<td style=""text-align: left;"">des</td>
<td style=""text-align: left;"">O</td>
</tr>
<tr>
<td style=""text-align: left;"">manifestations</td>
<td style=""text-align: left;"">B-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">neurologiques</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">progressives</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">des</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">patients</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">adultes</td>
<td style=""text-align: left;"">B-Caracteristique_du_sujet</td>
</tr>
<tr>
<td style=""text-align: left;"">et</td>
<td style=""text-align: left;"">O</td>
</tr>
<tr>
<td style=""text-align: left;"">des</td>
<td style=""text-align: left;"">O</td>
</tr>
<tr>
<td style=""text-align: left;"">enfants</td>
<td style=""text-align: left;"">B-Caracteristique_du_sujet</td>
</tr>
<tr>
<td style=""text-align: left;"">atteints</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">de</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">maladie</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">de</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">Niemann-Pick</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">type</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">C</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
</tbody>
</table>
</div>
<p>But i'm not sure it's correct and comprehensible for an algorithm then.</p>
",Named Entity Recognition (NER),encoding iob format entity nested inside entity dataset named entity recognition would convert dataset json iob format issue dataset contains entity nested entity example sentence entity encode bigger one nested entity inside thought word tag traitement de manifestation b cible neurologiques cible progressive cible de cible patient cible adultes b caracteristique du sujet et de enfants b caracteristique du sujet atteints cible de cible maladie cible de cible niemann pick cible type cible c cible sure correct comprehensible algorithm
Spacy understanding how to use POS and NER,"<p>I understand the concept of Parts of Speech and Named Entity recognition.</p>
<p>But, I don't understand how we feed that into the model? How do I create a features table for this? Would it be like</p>
<p>'My dog is brown'</p>
<p>Then the features table is like the below where we have a column for WORD-POS-NER?:</p>
<p><a href=""https://i.sstatic.net/TkkvJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TkkvJ.png"" alt=""enter image description here"" /></a></p>
<p>Thanks alot for the help</p>
",Named Entity Recognition (NER),spacy understanding use po ner understand concept part speech named entity recognition understand feed model create feature table would like dog brown feature table like column word po ner thanks alot help
How to extract relation between entities for stock prediction,"<p>I am trying to extract relation between two entities (entity1- relation- entity2) from news articles for stock prediction. I have used NER for entity extraction. It would be great if anyone could help me with relationship extraction.</p>
",Named Entity Recognition (NER),extract relation entity stock prediction trying extract relation two entity entity relation entity news article stock prediction used ner entity extraction would great anyone could help relationship extraction
Data Extraction in Python,"<p>I've been given a data set consisting of three columns. One column has transaction information, one has a store number, and one has sections. My goal is to extract the store number from the transaction information column for 300 different stores using entity extraction. My thought process behind this was to make something similar to how companies search resumes for key words using a word bank, since I have the store numbers in a separate column already. I have the .csv file read into my program, and I have the store numbers stored into their own array. I'm trying to figure out how to search the transaction information column for those store numbers.</p>
<p>Code so far:</p>
<pre><code>import pandas as pd
import numpy as np

file = pd.read_csv(r'C:\Users\cspea\Desktop\assignment.csv')
print(file)

store_number_array = file['store_number'].to_numpy()
print(store_number_array)
</code></pre>
<p>Sample data set (in .csv format):</p>
<pre><code>transaction_descriptor,store_number,dataset
DOLRTREE 2257 00022574 ROSWELL,2257,train
AUTOZONE #3547,3547,train
TGI FRIDAYS 1485 0000,1485,train
BUFFALO WILD WINGS 003,3,train
J. CREW #568 0,568,train
</code></pre>
<p>Any tips would be greatly appreciated. Thanks for your time and assistance in advance :)</p>
",Named Entity Recognition (NER),data extraction python given data set consisting three column one column ha transaction information one ha store number one ha section goal extract store number transaction information column different store using entity extraction thought process behind wa make something similar company search resume key word using word bank since store number separate column already csv file read program store number stored array trying figure search transaction information column store number code far sample data set csv format tip would greatly appreciated thanks time assistance advance
Error with using Example in Example.from_dict() (SpaCy3),"<p>I'm trying to train an NER model with SpaCy- v3, and there's this error I'm facing in the Example.from_dict() method. In fact, I had referred answers from <a href=""https://stackoverflow.com/questions/66675261/how-can-i-work-with-example-for-nlp-update-problem-with-spacy3-0"">this earlier question</a> on how to use the Example class.</p>
<p>Here is the code snippet:</p>
<pre><code>nlp = spacy.blank('en')
if 'ner' not in nlp.pipe_names:
    ner = nlp.create_pipe('ner')
    nlp.add_pipe('ner', last=True)
else:
    ner = nlp.get_pipe('ner')

for _, annotations in TRAIN_DATA:
    for label in annotations['entities']:
        ner.add_label(label[2])

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.begin_training()
    for epoch in range(EPOCHS):
        random.shuffle(TRAIN_DATA)
        losses = {}
        print(f'Epoch {epoch+1} of {EPOCHS}:')
        for text, annotations in TRAIN_DATA:
            doc = nlp.make_doc(text)
            example = Example.from_dict(doc, annotations)
            nlp.update([example], drop=0.2, sgd=optimizer, losses=losses) #SGD
        print(losses) #Print losses after each epoch
</code></pre>
<p>Above, <code>TRAIN_DATA</code> is a list with tuples like this:</p>
<pre><code>('El Salvador achieved independence from Spain in 1821 and from the Central American Federation in 1839 .',
 {'entities': [(97, 101, 'tim'),
   (39, 44, 'org'),
   (48, 52, 'tim'),
   (66, 93, 'org'),
   (0, 11, 'geo')]})
</code></pre>
<p>And finally, this is the error traceback:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
/tmp/ipykernel_34/168282795.py in &lt;module&gt;
      8         for text, annotations in TRAIN_DATA:
      9             doc = nlp.make_doc(text)
---&gt; 10             example = Example.from_dict(doc, annotations)
     11             nlp.update([example], drop=0.2, sgd=optimizer, losses=losses) #SGD
     12         print(losses) #Print losses after each epoch

/opt/conda/lib/python3.7/site-packages/spacy/training/example.pyx in spacy.training.example.Example.from_dict()

/opt/conda/lib/python3.7/site-packages/spacy/training/example.pyx in spacy.training.example.annotations_to_doc()

/opt/conda/lib/python3.7/site-packages/spacy/training/example.pyx in spacy.training.example._add_entities_to_doc()

/opt/conda/lib/python3.7/site-packages/spacy/training/iob_utils.py in offsets_to_biluo_tags(doc, entities, missing)
    102                     biluo[starts[s]] = &quot;O&quot;
    103         else:
--&gt; 104             for token_index in range(start_char, end_char):
    105                 if token_index in tokens_in_ents.keys():
    106                     raise ValueError(

TypeError: 'numpy.float64' object cannot be interpreted as an integer
</code></pre>
<p>My first question on Stack Overflow. Hope I've provided all necessary info to be eligible for help. Thanks in advance!</p>
",Named Entity Recognition (NER),error using example example dict spacy trying train ner model spacy v error facing example dict method fact referred answer href earlier question use example class code snippet list tuples like finally error traceback first question stack overflow hope provided necessary info help thanks advance
Can we deduce the relationship b/w a dimension of a word vector with the linguistic characteristic it represents?,"<p>Let's imagine we generated a 200 dimension word vector using any pre-trained model of the word ('hello') as shown in the below image.</p>
<p><a href=""https://i.sstatic.net/5TfJp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5TfJp.png"" alt=""Word_Vector"" /></a></p>
<p>So, by any means can we tell which linguistic feature is represented by each d_i of this vector?</p>
<p>For example, d1 might be looking at whether the word is a noun; d2 might tell whether the word is a named entity or not and so on.</p>
",Named Entity Recognition (NER),deduce relationship b w dimension word vector linguistic characteristic represents let imagine generated dimension word vector using pre trained model word hello shown image mean tell linguistic feature represented vector example might looking whether word noun might tell whether word named entity
Will NER improve Text Categorization?,"<p>I was wondering - if I'm doing text categorization (with SpaCy, using their textcat-multi
component for example), will those results improve if an NER component was before it in the pipeline? My thinking about that is: if a sentence like &quot;Senior Javascript Developer&quot; would be categorized as, say, &quot;A&quot; (or any other category), and if then <code>Javascript</code> would be tagged as a &quot;Programming Language&quot; entity or similar, would the textcat pick that up, and use that to say, for example, a sentence like &quot;Python Engineer&quot;, is similar because of that entity (and will also be categorized in &quot;A&quot;)? Assuming <code>Python</code> is also a &quot;Programming Language&quot; entity of course.</p>
<p>My understanding of it is that the textcat component will take the tok2vec vectors and look for similarity there, but will the vectors be similar in one or more dimensions if the found entity using NER is similar? Am I thinking about this the right way? If it's at all possible, how would that work with SpaCy?</p>
",Named Entity Recognition (NER),ner improve text categorization wa wondering text categorization spacy using textcat multi component example result improve ner component wa pipeline thinking sentence like senior javascript developer would categorized say category would tagged programming language entity similar would textcat pick use say example sentence like python engineer similar entity also categorized assuming also programming language entity course understanding textcat component take tok vec vector look similarity vector similar one dimension found entity using ner similar thinking right way possible would work spacy
Why FLAIR does&#39;t recognize the entire location name of simple sentence?,"<p>I'm tying to to detect simple location with NER algorithm, and I'm getting semi-correct results:</p>
<pre><code>from flair.data   import Sentence
from flair.models import SequenceTagger

tagger   = SequenceTagger.load('ner')
text     = 'Jackson leaves at north Carolina'
sentence = Sentence(text)

tagger.predict(sentence)
for entity in sentence.get_spans('ner'):
    print(entity)
</code></pre>
<p>Output:</p>
<pre><code>Span [1]: &quot;Jackson&quot;   [− Labels: PER (0.9996)]
Span [5]: &quot;Carolina&quot;   [− Labels: LOC (0.7363)]
</code></pre>
<p>I was expecting to receive <code>&quot;north Carolina&quot;</code>.</p>
<ol>
<li>Can FLAIR detect full location description? What do we need for it?</li>
<li>Is there any NER algorithm that cat detect full location description?</li>
</ol>
",Named Entity Recognition (NER),flair doe recognize entire location name simple sentence tying detect simple location ner algorithm getting semi correct result output wa expecting receive flair detect full location description need ner algorithm cat detect full location description
Spacy train ner using multiprocessing,"<p>I am trying to train a custom ner model using spacy. Currently, I have more than 2k records for training and each text consists of more than 100 words, at least more than 2 entities for each record. I running it for 50 iterations.
It is taking more than 2 hours to train completely. </p>

<p>Is there any way to train using multiprocessing? Will it improve the training time?</p>
",Named Entity Recognition (NER),spacy train ner using multiprocessing trying train custom ner model using spacy currently k record training text consists word least entity record running iteration taking hour train completely way train using multiprocessing improve training time
Optimally tag words within a substring with a label when starting &amp; ending indices are provided [python],"<p>I'm trying to format data in the CoNLL format for a NER task (this info is largely irrelevant). What I want to <em>optimally</em> accomplish is this -</p>
<p><strong>Input</strong>:</p>
<ul>
<li>Text: <code>The quick brown fox jumps over the lazy dog.</code></li>
<li>Indices: <code>10 - 18</code> (brown fox), <code>35 - 42</code> (lazy dog)</li>
</ul>
<p><strong>Desired Output</strong>:</p>
<pre><code>The        O
quick      O
brown      X
fox        X
jumps      O
over       O
the        O
lazy       Y
dog        Y
.          O
</code></pre>
<p>Is there a single-pass way to do this (because I have a lot of examples -- over 100k)?</p>
",Named Entity Recognition (NER),optimally tag word within substring label starting ending index provided python trying format data conll format ner task info largely irrelevant want optimally accomplish input text index brown fox lazy dog desired output single pas way lot example k
Which way is better to store data by column from file containing BIO/IOB tagged text?,"<p>I have a file containing BIO / IOB Tagged Text like this:</p>
<p><a href=""https://i.sstatic.net/3t9cc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3t9cc.png"" alt=""BIO/IOB Tagged Text"" /></a></p>
<p>I got this from <a href=""https://github.com/Babelscape/wikineural/tree/master/data/wikineural/en"" rel=""nofollow noreferrer"">Wikineural</a> and despite having the extension .conllu I think that it probably is not because these files have a different <a href=""https://pyconll.readthedocs.io/en/stable/pyconll/unit/token.html"" rel=""nofollow noreferrer"">content</a>. I need to store each column somewhere (idk what could be better) in order to have IDs, Words and Tags separated.
With those data I have to do NER using HMM (Viterbi as decoding).</p>
<p><strong>I'm new on python and NLP so don't go hard on me!!</strong> - I tried to store those data that way (Note that between the various sentences there are empty lines that I want to skip):</p>
<pre><code>ids = []
words = []
tags = []
with open('en/train.conllu', encoding=&quot;utf8&quot;) as fileObj:
    lines = [line.strip() for line in fileObj if line.strip()]
    for line in lines:
        row = line.split()
        ids.append(row[0])
        words.append(row[1])
        tags.append(row[2])
</code></pre>
<p>but I feel like it's a bad way, any suggestion? Is there any particular data structure that I should use considering the final goal (NER)?</p>
<p>I have also another question NER related: since I have to use HMM - BIO/IOB tag already led me in a situation where I can start from learning (by counting TAG -&gt; TAG and TAG -&gt; WORD probability), right? I mean, those data are ready to be analyzed for NER purpose.</p>
",Named Entity Recognition (NER),way better store data column file containing bio iob tagged text file containing bio iob tagged text like got wikineural despite extension conllu think probably file different content need store column somewhere idk could better order id word tag separated data ner using hmm viterbi decoding new python nlp go hard tried store data way note various sentence empty line want skip feel like bad way suggestion particular data structure use considering final goal ner also another question ner related since use hmm bio iob tag already led situation start learning counting tag tag tag word probability right mean data ready analyzed ner purpose
"Extracting mentions, hashtags and, urls and placing them in a new column in a Twitter Dataset with R","<p>I have a Twitter dataset of 30000 tweets and I'm trying to prepare the data for text analysis. I downloaded the dataset with academictwitteR package in R. Inside the dataset, some columns (such as; &quot;user.metrics&quot;, &quot;public.metrics&quot;, &quot;entities&quot; are seperate data frames. I managed to extract  the columns from &quot;user.metrics&quot; and &quot;public.metrics&quot; and merge the extracted columns with my original dataset as following, without a problem;</p>
<pre><code>#extract
extract_publicmetrics &lt;- as.data.frame(mytwitterdata$public_metrics)
colnames(extract_publicmetrics)
[1] &quot;retweet_count&quot; &quot;reply_count&quot;   &quot;like_count&quot;    &quot;quote_count&quot;

#add observation column to bind with the original data (mytwitterdata)
addconsecutivenumbers1 &lt;- cbind(extract_publicmetrics, &quot;observation&quot;=1:nrow(deneme2_publicmetrics)) 
addconsecutivenumbers2 &lt;- cbind(mytwitterdata, &quot;observation&quot;=1:nrow(joined_deneme2))
#merge two data
merged.data &lt;- merge(addconsecutivenumbers1, addconsecutivenumbers2, by=&quot;observation&quot;)
</code></pre>
<p>But, I could not manage to extract &quot;mentions&quot;, &quot;urls&quot;, &quot;hastags&quot; columns from &quot;Entities&quot; dataframe in my dataset.I think it's because &quot;mentions&quot;, &quot;urls&quot;, &quot;hashtags&quot; are nested lists in that data frame (e.g.):</p>
<pre><code>class(mytwitterdata$entities$hashtags)
[1] &quot;list&quot;
</code></pre>
<p>For example, a Tweet may contain no hashtag, one hashtag, or more than one hashtag. I want to create a new column from that list in which the value of the row is NA when there is no hashtag, or the row includes the hashtag as text in the row ( or hashtags separated with commas when it includes more than one hashtag).</p>
<p>Attached is s sample data of 10 rows extracted from the &quot;Entities&quot; dataframe from my dataset:</p>
<p><a href=""https://drive.google.com/file/d/1vfyFIObRS9tCxGNJCG9AMyKgxwgwBDMZ/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1vfyFIObRS9tCxGNJCG9AMyKgxwgwBDMZ/view?usp=sharing</a></p>
",Named Entity Recognition (NER),extracting mention hashtags url placing new column twitter dataset r twitter dataset tweet trying prepare data text analysis downloaded dataset academictwitter package r inside dataset column user metric public metric entity seperate data frame managed extract column user metric public metric merge extracted column original dataset following without problem could manage extract mention url hastags column entity dataframe dataset think mention url hashtags nested list data frame e g example tweet may contain hashtag one hashtag one hashtag want create new column list value row na hashtag row includes hashtag text row hashtags separated comma includes one hashtag attached sample data row extracted entity dataframe dataset
Convert from Prodigy&#39;s JSONL format for labeled NER to spaCy&#39;s training format?,"<p>I am new to Prodigy and spaCy as well as CLI coding. I'd like to use Prodigy to label my data for an NER model, and then use spaCy in python to create models. </p>

<p>Prodigy outputs in SQLite format. SpaCy takes in this other kind of format, not sure what to call it: </p>

<pre><code>TRAIN_DATA = [
    (
        ""Horses are too tall and they pretend to care about your feelings"",
        {""entities"": [(0, 6, LABEL)]},
    ),
    (""Do they bite?"", {""entities"": []}),
    (
        ""horses are too tall and they pretend to care about your feelings"",
        {""entities"": [(0, 6, LABEL)]},
    ),
    (""horses pretend to care about your feelings"", {""entities"": [(0, 6, LABEL)]}),
    (
        ""they pretend to care about your feelings, those horses"",
        {""entities"": [(48, 54, LABEL)]},
    ),
    (""horses?"", {""entities"": [(0, 6, LABEL)]}),
]
</code></pre>

<p>How can I convert from one to the other? It seems like this should be easy, but I cannot find it anywhere. </p>

<p>I have no problem loading in the dataset, just converting. </p>
",Named Entity Recognition (NER),convert prodigy jsonl format labeled ner spacy training format new prodigy spacy well cli coding like use prodigy label data ner model use spacy python create model prodigy output sqlite format spacy take kind format sure call convert one seems like easy find anywhere problem loading dataset converting
How can I parse the action that belongs to a Person using Spacy in Python?,"<p>I have a set of acknowledgements extracted from academic papers that contain sentences like the following:</p>
<blockquote>
<p>I would like to thank PERSON1 for helping me with this paper.</p>
</blockquote>
<blockquote>
<p>We gratefully acknowledge PERSON2 for operating the equipment.</p>
</blockquote>
<blockquote>
<p>PERSON3 and PERSON4 are thanked for their guidance.</p>
</blockquote>
<blockquote>
<p>Thank you to PERSON5, who set up the experiment.</p>
</blockquote>
<blockquote>
<p>PERSON6 analysed the data, and for this we are thankful.</p>
</blockquote>
<p>I used Named Entity Recognition to parse out the person names, and now am trying to find some way to capture what they did. Ideally I'd like to end up with a dataset like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Person</th>
<th>Contribution</th>
</tr>
</thead>
<tbody>
<tr>
<td>PERSON1</td>
<td>helping me with this paper</td>
</tr>
<tr>
<td>PERSON2</td>
<td>operating the equipment</td>
</tr>
<tr>
<td>PERSON3</td>
<td>their guidance</td>
</tr>
<tr>
<td>PERSON4</td>
<td>their guidance</td>
</tr>
<tr>
<td>PERSON5</td>
<td>set up the experiment</td>
</tr>
<tr>
<td>PERSON6</td>
<td>analysed the data</td>
</tr>
</tbody>
</table>
</div>
<p>Is there any way to capture this information using Spacy (or another Python tool)? The result doesn't have to be perfect: I don't mind if I sometimes capture extra information or miss information, as long as I catch most cases.</p>
<p>A couple of notes:</p>
<ul>
<li><p>In real life, the sentences can be much more complicated, eg. &quot;Thanks to PERSON1 for X and PERSON2 for Y and...&quot;. The contributions can also be longer like &quot;thank you to PERSON3 for kindly providing the manuscript which is described in detail below, and for being good friend, and for always having my back.&quot;</p>
</li>
<li><p>I don't need to specifically check for words like &quot;thank&quot;, &quot;acknowledge&quot; - I just want to catch the action that belongs to each person (understanding that I might also catch cases that aren't contributions).</p>
</li>
</ul>
",Named Entity Recognition (NER),parse action belongs person using spacy python set acknowledgement extracted academic paper contain sentence like following would like thank person helping paper gratefully acknowledge person operating equipment person person thanked guidance thank person set experiment person analysed data thankful used named entity recognition parse person name trying find way capture ideally like end dataset like person contribution person helping paper person operating equipment person guidance person guidance person set experiment person analysed data way capture information using spacy another python tool result perfect mind sometimes capture extra information miss information long catch case couple note real life sentence much complicated eg thanks person x person contribution also longer like thank person kindly providing manuscript described detail good friend always back need specifically check word like thank acknowledge want catch action belongs person understanding might also catch case contribution
Convert NER SpaCy format to IOB format,"<p>I have data which is already labelled in SpaCy format. For example:</p>

<pre><code>(""Who is Shaka Khan?"", {""entities"": [(7, 17, ""PERSON"")]}),
(""I like London and Berlin."", {""entities"": [(7, 13, ""LOC""), (18, 24, ""LOC"")]})
</code></pre>

<p>But I want to try training it with any other NER model, such as BERT-NER, which requires IOB tagging instead. Is there any conversion code from SpaCy data format to IOB?</p>

<p>Thanks!</p>
",Named Entity Recognition (NER),convert ner spacy format iob format data already labelled spacy format example want try training ner model bert ner requires iob tagging instead conversion code spacy data format iob thanks
How can I get the confusion matrix used to calculate the metrics for NER models?,"<p>Similar to this previous question <a href=""https://stackoverflow.com/questions/56252016/how-to-calculate-the-overall-accuracy-of-custom-trained-spacy-ner-model-with-con"">How to calculate the overall accuracy of custom trained spacy ner model with confusion matrix?</a></p>

<p>spaCy provides Precision, Recall, F1 scores in the <code>meta.json</code> file when it writes out the trained NER model.  Also these values are available when running the evaluation command <code>python -m spacy evaluate</code>.  However is it possible to get the counts for TP, FP, FN used to calculate these values? </p>

<p>Furthermore is it possible to output the actual text / tokens which resulted in a False Positive or False Negative?</p>
",Named Entity Recognition (NER),get confusion matrix used calculate metric ner model similar previous question href calculate overall accuracy custom trained spacy ner model confusion matrix spacy provides precision recall f score file writes trained ner model also value available running evaluation command however possible get count tp fp fn used calculate value furthermore possible output actual text token resulted false positive false negative
NLP for Text Mining or Chatbot,"<p>I am planning to build a chatbot which can get the user input and analyze and call different web service in Java. For example,</p>
<pre><code>Get customers who bought books between 01/MAR/2019 and 10/MAR/2019. 
Get books published by ABC Publications. 
Create customer with name ABC and address 12, Hill View Street, London.
</code></pre>
<p>For the first one, it has to identify it is a retrieve request as it is &quot;Get&quot; call and it is about the book and also date. In second text need to extract keywords &quot;books&quot; and publication name &quot;ABC&quot;. The third one is different, it's a create customer request call with name and address.</p>
<p>As I am more comfortable with Java, I am looking for NLP which can achieve above. On the internet, I find more on OpenNLP and CoreNLP. Examples and samples are available widely for OpenNLP.</p>
<p>So I want to check whether am I in the right direction? I see a lot of other things like Apache Ruta UIMA but not sure it is applicable for my use case, as I don't find much information in net.</p>
",Named Entity Recognition (NER),nlp text mining chatbot planning build chatbot get user input analyze call different web service java example first one ha identify retrieve request get call book also date second text need extract keywords book publication name abc third one different create customer request call name address comfortable java looking nlp achieve internet find opennlp corenlp example sample available widely opennlp want check whether right direction see lot thing like apache ruta uima sure applicable use case find much information net
NER using Spacy library not giving correct result on resume parser,"<p>I am using SpaCY's named entity recognition to extract the Name, Organization etc from a resume.
Here is my python code.</p>

<pre><code>import spacy
import PyPDF2
mypdf = open('C:\\Users\\akjain\\Downloads\\Resume\\Al Mal Capital_Nader El Boustany_BD Manager.pdf', mode='rb')
pdf_document = PyPDF2.PdfFileReader(mypdf)
first_page = pdf_document.getPage(0)
nlp = spacy.load('en_core_web_sm') 
text = first_page.extractText()
doc = nlp(text)   
for ent in doc.ents: 
    print(ent.text, ent.label_) 
</code></pre>

<p>If I see the output, it does not look great. 
Name is not correctly identified. Last name is considered as Org name, Dubai is treated as Person and so on.</p>

<p><a href=""https://i.sstatic.net/fNCr0.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fNCr0.jpg"" alt=""enter image description here""></a></p>

<p>Here is the snapshot of resume which I got it from a public dataset.</p>

<p>I want to extract candidate name, Organization, location etc from set of resumes. When I read the documentation it says accuracy is more than 95% using spaCy. However in my case it is not.
Is there any way to improve the accuracy of feature extraction?</p>

<p><a href=""https://i.sstatic.net/5naeV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5naeV.jpg"" alt=""enter image description here""></a></p>
",Named Entity Recognition (NER),ner using spacy library giving correct result resume parser using spacy named entity recognition extract name organization etc resume python code see output doe look great name correctly identified last name considered org name dubai treated person snapshot resume got public dataset want extract candidate name organization location etc set resume read documentation say accuracy using spacy however case way improve accuracy feature extraction
Improving the extraction of human names with nltk,"<p>I am trying to extract human names from text. </p>

<p>Does anyone have a method that they would recommend?</p>

<p>This is what I tried (code is below):
I am using <code>nltk</code> to find everything marked as a person and then generating a list of all the NNP parts of that person. I am skipping persons where there is only one NNP which avoids grabbing a lone surname.</p>

<p>I am getting decent results but was wondering if there are better ways to go about solving this problem.</p>

<p>Code:</p>

<pre><code>import nltk
from nameparser.parser import HumanName

def get_human_names(text):
    tokens = nltk.tokenize.word_tokenize(text)
    pos = nltk.pos_tag(tokens)
    sentt = nltk.ne_chunk(pos, binary = False)
    person_list = []
    person = []
    name = """"
    for subtree in sentt.subtrees(filter=lambda t: t.node == 'PERSON'):
        for leaf in subtree.leaves():
            person.append(leaf[0])
        if len(person) &gt; 1: #avoid grabbing lone surnames
            for part in person:
                name += part + ' '
            if name[:-1] not in person_list:
                person_list.append(name[:-1])
            name = ''
        person = []

    return (person_list)

text = """"""
Some economists have responded positively to Bitcoin, including 
Francois R. Velde, senior economist of the Federal Reserve in Chicago 
who described it as ""an elegant solution to the problem of creating a 
digital currency."" In November 2013 Richard Branson announced that 
Virgin Galactic would accept Bitcoin as payment, saying that he had invested 
in Bitcoin and found it ""fascinating how a whole new global currency 
has been created"", encouraging others to also invest in Bitcoin.
Other economists commenting on Bitcoin have been critical. 
Economist Paul Krugman has suggested that the structure of the currency 
incentivizes hoarding and that its value derives from the expectation that 
others will accept it as payment. Economist Larry Summers has expressed 
a ""wait and see"" attitude when it comes to Bitcoin. Nick Colas, a market 
strategist for ConvergEx Group, has remarked on the effect of increasing 
use of Bitcoin and its restricted supply, noting, ""When incremental 
adoption meets relatively fixed supply, it should be no surprise that 
prices go up. And that’s exactly what is happening to BTC prices.""
""""""

names = get_human_names(text)
print ""LAST, FIRST""
for name in names: 
    last_first = HumanName(name).last + ', ' + HumanName(name).first
        print last_first
</code></pre>

<p>Output:</p>

<pre><code>LAST, FIRST
Velde, Francois
Branson, Richard
Galactic, Virgin
Krugman, Paul
Summers, Larry
Colas, Nick
</code></pre>

<p>Apart from Virgin Galactic, this is all valid output. Of course, knowing that Virgin Galactic isn't a human name in the context of this article is the hard (maybe impossible) part.</p>
",Named Entity Recognition (NER),improving extraction human name nltk trying extract human name text doe anyone method would recommend tried code using find everything marked person generating list nnp part person skipping person one nnp avoids grabbing lone surname getting decent result wa wondering better way go solving problem code output apart virgin galactic valid output course knowing virgin galactic human name context article hard maybe impossible part
Is possible to get dependency/pos information for entities in Spacy?,"<p>I am working on extracting entities from scientific text (I am using <strong>scispacy</strong>) and later I will want to extract relations using hand-written rules. I have extracted entities and their character span successfully, and I can also get the pos and dependency tags for tokens and noun chunks. So I am comfortable with the two tasks separately, but I want to bring the two together and I have been stuck for a while.</p>
<p>The idea is that I want to be able to write rules such as: (just an example) if in a sentence/clause there are two entities where the first one is a 'DRUG/CHEMICAL' + is the <em>subject</em>, and the second one is a 'DISEASE' + is an <em>object</em> --&gt; (then) infer 'treatment' relation between the two.</p>
<p>If anyone has any hints on how to approach this task, I would really appreciate it. Thank you!</p>
<p>S.</p>
<p>What I am doing to <strong>extract entities</strong>:</p>
<p><code>doc = nlp(text-with-more-than-one-sent)</code></p>
<p><code>for ent in doc.ents:</code></p>
<pre><code>`... (get information about the ent e.g. its character span)`
</code></pre>
<p><strong>Getting dependency information (for noun chunks and for tokens):</strong></p>
<p><code>for chunk in doc.noun_chunks:</code></p>
<p><code>    print(f&quot;Text: {chunk.text}, Root text: {chunk.root.text}, Root dep: {chunk.root.dep_}, Root head text: {chunk.root.head.text}, POS: {chunk.root.head.pos_}&quot;)</code></p>
<p>_</p>
<p><code>for token in doc:</code></p>
<p><code>    print(f&quot;Text: {token.text}, DEP label: {token.dep_}, Head text: {token.head.text}, Head POS: {token.head.pos_}, Children: {[child for child in token.children]}&quot;)</code></p>
",Named Entity Recognition (NER),possible get dependency po information entity spacy working extracting entity scientific text using scispacy later want extract relation using hand written rule extracted entity character span successfully also get po dependency tag token noun chunk comfortable two task separately want bring two together stuck idea want able write rule example sentence clause two entity first one drug chemical subject second one disease object infer treatment relation two anyone ha hint approach task would really appreciate thank extract entity getting dependency information noun chunk token
Combine Camembert &amp; CRF for token classification,"<p>I want to combine Camembert and CRF in order to perform named entity recognition on French medical data.
I am following this <a href=""https://github.com/shushanxingzhe/transformers_ner/blob/main/models.py"" rel=""nofollow noreferrer"">code</a> combining Bert and CRF, but I can't reproduce the same thing with Camembert as I didn't find a <code>PreTrainedCamembert</code> class to pass and use instead of the <code>BertPreTrainedModel</code> used in the shared code.
I have tried to use the <code>CamembertModel</code> but it gave me a model in which the camembert layers are duplicated as shown below.</p>
<pre><code>BertCRF(
  (embeddings): RobertaEmbeddings(
    (word_embeddings): Embedding(32005, 768, padding_idx=1)
    (position_embeddings): Embedding(514, 768, padding_idx=1)
    (token_type_embeddings): Embedding(1, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): RobertaEncoder(
    (layer): ModuleList(
      (0): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (1): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (2): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (3): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (4): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (5): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (6): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (7): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (8): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (9): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (10): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (11): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): RobertaPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
  (cmbert): CamembertModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(32005, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=21, bias=True)
  (crf): CRF(num_tags=21)
)
</code></pre>
<p>Any clues on how to fix this issue? I want to get a model similar to the BERT &amp; CRF one.</p>
<pre><code>BertCRF(
  (bert): BertPreTrainedModel()
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=21, bias=True)
  (crf): CRF(num_tags=21)
)
</code></pre>
",Named Entity Recognition (NER),combine camembert crf token classification want combine camembert crf order perform named entity recognition french medical data following code combining bert crf reproduce thing camembert find class pas use instead used shared code tried use gave model camembert layer duplicated shown clue fix issue want get model similar bert crf one
Is it possible to get a confidence score on Spacy Named-entity recognition,"<p>I need to get a confidence score on the predictions done by Spacy NER.<br><br>
<strong>CSV file</strong></p>

<pre><code>Text,Amount &amp; Nature,Percent of Class
""T. Rowe Price Associates, Inc."",""28,223,360 (1)"",8.7% (1)
100 E. Pratt Street,Not Listed,Not Listed
""Baltimore, MD 21202"",Not Listed,Not Listed
""BlackRock, Inc."",""21,871,854 (2)"",6.8% (2)
55 East 52nd Street,Not Listed,Not Listed
""New York, NY 10022"",Not Listed,Not Listed
The Vanguard Group,""21,380,085 (3)"",6.64% (3)
100 Vanguard Blvd.,Not Listed,Not Listed
""Malvern, PA 19355"",Not Listed,Not Listed
FMR LLC,""20,784,414 (4)"",6.459% (4)
245 Summer Street,Not Listed,Not Listed
""Boston, MA 02210"",Not Listed,Not Listed
</code></pre>

<p><strong>Code</strong></p>

<pre><code>import pandas as pd
import spacy
with open('/path/table.csv') as csvfile:
    reader1 = csv.DictReader(csvfile)
    data1 =[[""Text"",""Amount &amp; Nature"",""Prediction""]]
    for row in reader1:
        AmountNature = row[""Amount &amp; Nature""]
        nlp = spacy.load('en_core_web_sm') 
        doc1 = nlp(row[""Text""])

        for ent in doc1.ents:
            #output = [ent.text, ent.start_char, ent.end_char, ent.label_]
            label1 = ent.label_
            text1 = ent.text
        data1.append([str(doc1),AmountNature,label1])
my_df1 = pd.DataFrame(data1)
my_df1.columns = my_df1.iloc[0]
my_df1 = my_df1.drop(my_df1.index[[0]])
my_df1.to_csv('/path/output.csv', index=False, header=[""Text"",""Amount &amp; Nature"",""Prediction""])
</code></pre>

<p><strong>Output CSV</strong></p>

<pre><code>Text,Amount &amp; Nature,Prediction
""T. Rowe Price Associates, Inc."",""28,223,360 (1)"",ORG
100 E. Pratt Street,Not Listed,FAC
""Baltimore, MD 21202"",Not Listed,CARDINAL
""BlackRock, Inc."",""21,871,854 (2)"",ORG
55 East 52nd Street,Not Listed,LOC
""New York, NY 10022"",Not Listed,DATE
The Vanguard Group,""21,380,085 (3)"",ORG
100 Vanguard Blvd.,Not Listed,FAC
""Malvern, PA 19355"",Not Listed,DATE
FMR LLC,""20,784,414 (4)"",ORG
245 Summer Street,Not Listed,CARDINAL
""Boston, MA 02210"",Not Listed,GPE
</code></pre>

<p><strong>Here on the above output, is it possible to get a Confident Score on the Spacy NER prectiction. If yes, how do I achieve that?</strong> 
<p>
Can someone please help me on this?</p>
",Named Entity Recognition (NER),possible get confidence score spacy named entity recognition need get confidence score prediction done spacy ner csv file code output csv output possible get confident score spacy ner prectiction yes achieve someone please help
Move spacy NER entity labels to top or bottom,"<p>I am using a spacy visualizer to show labels of each span. I am trying to put the NER labels to the top or bottom of the text for better visualization and comparison. At the moment, I can visualize them with:</p>
<pre><code>nlp=spacy.load(&quot;en_core_web_sm&quot;)
doc = nlp(&quot;This is from Texas Written by Ryan on sunday&quot;)
displacy.render(doc,style=&quot;ent&quot;,jupyter=True)
</code></pre>
<p>as:
<a href=""https://i.sstatic.net/mKLCn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mKLCn.png"" alt=""enter image description here"" /></a></p>
<p><strong>How can I move these entity labels to the top or bottom or left of the span?</strong></p>
<p>The documentation on setting options is <a href=""https://spacy.io/usage/visualizers#ent"" rel=""nofollow noreferrer"">here</a>; there is no explanation on how to do this.</p>
",Named Entity Recognition (NER),move spacy ner entity label top bottom using spacy visualizer show label span trying put ner label top bottom text better visualization comparison moment visualize move entity label top bottom left span documentation setting option explanation
Why are default entities sourced from another language model (Spacy 3.0)?,"<p>So Im using Spacy 3.0 to train basically all pipelines with custom data.
Weirdly enough Spacy seems to source the entities of the NER pipeline from another model, possibly en_core_web_md. So if I type the sentence &quot;Who is Barrack Obama&quot; into spacy it detects the name as a PERSON entity, which is not a behavior I want since I only want to work with my own entitites.</p>
<p>I cant seem to find why it would source the entities though, here is my config:</p>
<pre><code>[paths]
train = &quot;trainer/data/sent_train.spacy&quot;
dev = &quot;trainer/data/sent_train.spacy&quot;
vectors = &quot;en_core_web_md&quot;
init_tok2vec = &quot;en_core_web_md&quot;

[system]
gpu_allocator = null
seed = 0

[nlp]
lang = &quot;en&quot;
pipeline = [&quot;tok2vec&quot;,&quot;tagger&quot;,&quot;parser&quot;,&quot;ner&quot;,&quot;textcat_multilabel&quot;]
batch_size = 1000
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;}

[components]

[components.ner]
factory = &quot;ner&quot;

[components.ner.model]
@architectures = &quot;spacy.TransitionBasedParser.v2&quot;
state_type = &quot;ner&quot;
extra_state_tokens = false
hidden_width = 64
maxout_pieces = 2
use_upper = true
nO = null

[components.parser]
source = &quot;en_core_web_md&quot;

[components.tagger]
source = &quot;en_core_web_md&quot;

[components.textcat_multilabel]
factory = &quot;textcat_multilabel&quot;
scorer = {&quot;@scorers&quot;:&quot;spacy.textcat_multilabel_scorer.v1&quot;}
threshold = 0.5

[components.textcat_multilabel.model]
@architectures = &quot;spacy.TextCatBOW.v2&quot;
exclusive_classes = false
ngram_size = 1
no_output_layer = false
nO = null

[components.tok2vec]
source = &quot;en_core_web_md&quot;

[corpora]

[corpora.dev]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.dev}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[corpora.train]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.train}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[training]
dev_corpus = &quot;corpora.dev&quot;
train_corpus = &quot;corpora.train&quot;
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_epochs = 0
max_steps = 20000
eval_frequency = 200
frozen_components = []
annotating_components = []
before_to_disk = null

[training.batcher]
@batchers = &quot;spacy.batch_by_words.v1&quot;
discard_oversize = false
tolerance = 0.2
get_length = null

[training.batcher.size]
@schedules = &quot;compounding.v1&quot;
start = 100
stop = 1000
compound = 1.001
t = 0.0

[training.logger]
@loggers = &quot;spacy.ConsoleLogger.v1&quot;
progress_bar = false

[training.optimizer]
@optimizers = &quot;Adam.v1&quot;
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001
learn_rate = 0.001

[training.score_weights]
tag_acc = 0.25
dep_uas = 0.12
dep_las = 0.12
dep_las_per_type = null
sents_p = null
sents_r = null
sents_f = 0.0
ents_f = 0.25
ents_p = 0.0
ents_r = 0.0
ents_per_type = null
cats_score = 0.25
cats_score_desc = null
cats_micro_p = null
cats_micro_r = null
cats_micro_f = null
cats_macro_p = null
cats_macro_r = null
cats_macro_f = null
cats_macro_auc = null
cats_f_per_type = null
cats_macro_auc_per_type = null

[pretraining]

[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = null
after_init = null

[initialize.components]

[initialize.tokenizer]
</code></pre>
<p>Since the [components.ner] section says factory = &quot;ner&quot;. It should create a blank new one right? So why are there already entity types in my model? I do have to mention here that I want other components like the parser or tagger to be sourced from the already trained model, so there is no mistake here</p>
",Named Entity Recognition (NER),default entity sourced another language model spacy im using spacy train basically pipeline custom data weirdly enough spacy seems source entity ner pipeline another model possibly en core web md type sentence barrack obama spacy detects name person entity behavior want since want work entitites cant seem find would source entity though config since component ner section say factory ner create blank new one right already entity type model mention want component like parser tagger sourced already trained model mistake
Measuring F1-score for NER,"<p>I am trying to evaluate a model of artificial intelligence for NER (Named Entity Recognition).<br />
In order to compare with other benchmarks, I need to calculate the model's F1-score. However, I am unsure how to code this.</p>
<p>My idea was:<br />
<strong>True-positives</strong>: equal tokens and equal tags, true-positive for the tag<br />
<strong>False-negative</strong>: equal tokens and unequal tags or token did not appear in the prediction, false-negative for the tag<br />
<strong>False-positive</strong>: token does not exist but has been assigned to a tag, example:</p>
<blockquote>
<p>Phrase: &quot;This is a test&quot;<br />
Predicted: {token: This is, tag: WHO}<br />
True pairs: {token: This, tag: WHO} {token: a test, tag: what}<br />
In this case, {token: This is, tag: WHO} is considered as a false positive of WHO.</p>
</blockquote>
<p>The code:</p>
<pre><code>       for val predicted tokens (pseudo-code) {   
       // val = struct { tokens, tags } from a phrase
           for (auto const &amp;j : val.tags) {
                if (j.first == current_tokens) {
                    if (j.second == tag) {
                        true_positives[tag_id]++;
                    } else {
                        false_negatives[tag_id]++;
                    }
                    current_token_exists = true;
                }
                
            }
            if (!current_token_exists) {
                false_positives[tag_id]++;
            }
        }

        for (auto const &amp;i : val.tags) {
            bool find = 0;
            for (auto const &amp;j : listed_tokens) {
                if (i.first == j) {find = 1; break;}
            }
            if (!find) {
                false_negatives[str2tag_id[i.second]]++;
            }
        }
</code></pre>
<p>After this, calculate the F-1:</p>
<pre><code>    float precision_total, recall_total, f_1_total;
    precision_total = total_true_positives / (total_true_positives + total_false_positives);
    recall_total = total_true_positives / (total_true_positives + total_false_negatives);
    f_1_total = (2 * precision_total * recall_total) / (precision_total + recall_total);
</code></pre>
<p>However, I believe that I am wrong in some concept. Does anyone have an opinion?</p>
",Named Entity Recognition (NER),measuring f score ner trying evaluate model artificial intelligence ner named entity recognition order compare benchmark need calculate model f score however unsure code idea wa true positive equal token equal tag true positive tag false negative equal token unequal tag token appear prediction false negative tag false positive token doe exist ha assigned tag example phrase test predicted token tag true pair token tag token test tag case token tag considered false positive code calculate f however believe wrong concept doe anyone opinion
The example of NLP in my homework documentation is crashing,"<p>So I'm new to NLP and I was trying out the example code my documentation but its giving me errors.</p>

<p>Such as </p>

<p>""ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token. similarity method will be based on the tagger, parser, and NER, which may not give useful similarity judgments. This may happen if you're using one of the small models, e.g. <code>en_core_web_sm</code>, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available""</p>

<p>and the second sentence that I typed exactly over from the documentation is giving this error</p>

<p>""can only concatenate str (not ""numpy.float64"") to str""</p>

<p>I could be just doing something silly but I'd appreciate some insights why is this happening</p>

<pre><code>import spacy
nlp = spacy.load('en')

tokens = nlp('cat apple monkey banana')

for token1 in tokens:
    for token2 in tokens:
        print(token1.text, token2.text, token1.similarity(token2))


print(""\nWorking With Sentences\n"")

sentence_to_compare = 'Why is my cat on the car'

sentences = [""Where did my dog go"",
             'hello, where is my car',
             'I\'ve lost my car in my car',
             'i\'d like my boat back',
             'I will name my dog Diana'
             ]

model_sentences = nlp(sentence_to_compare)

for sentence in sentences:
    similarity = nlp(sentence).similarity(model_sentences)
    print(sentence + ""-"" + similarity)
</code></pre>
",Named Entity Recognition (NER),example nlp homework documentation crashing new nlp wa trying example code documentation giving error modelswarning w model using ha word vector loaded result token similarity method based tagger parser ner may give useful similarity judgment may happen using one small model e g ship word vector use context sensitive tensor always add word vector use one larger model instead available second sentence typed exactly documentation giving error concatenate str numpy float str could something silly appreciate insight happening
Extract Only Certain Named Entities From Tokens,"<p>Quick question (hopefully). Is it possible for me to get the named entities of the tokens except for the ones with CARDINAL label (The label is 397). Here is my code below:</p>
<pre><code>spacy_model = spacy.load('en-core-web-lg')
f = open('temp.txt')
tokens = spacy_model(f.read())
named_entities = tokens.ents #Except where named_entities.label = 397
</code></pre>
<p>Is this possible? Any help would be greatly appreciated.</p>
",Named Entity Recognition (NER),extract certain named entity token quick question hopefully possible get named entity token except one cardinal label label code possible help would greatly appreciated
"How to view Precision, Recall, F1 in custom SpaCy tag training?","<p>I try to train a custom POS tagger with Spacy, but when I run training:</p>
<pre><code>python -m spacy train config.cfg --output ./output
</code></pre>
<p>I get the following training metrics:</p>
<pre><code>E    #       LOSS TOK2VEC  LOSS TAGGER  TAG_ACC  SCORE                                                                                       
---  ------  ------------  -----------  -------  ------                                                                                      
  0       0          0.00        56.47    34.57    0.35                                                                                      
  0     200        220.60      9159.67    75.71    0.76                                                                                      
  0     400        416.10      8008.25    80.36    0.80                                                                                      
  0     600        574.27      8974.13    82.15    0.82                                                                                      
  1     800        683.66      8606.33    82.96    0.83 
</code></pre>
<p>However, in different tutorials and examples, I can see additional metrics that would be nice to see as well:</p>
<pre><code>E # LOSS TOK2VEC LOSS NER ENTS_F ENTS_P ENTS_R SCORE
</code></pre>
<p>(True, these are mostly indicated in case of NER training, not POS training examples, but I cannot see any reason why not to indicate them in case of POS as well).
I suspect that it could be customized by the following line in <code>config.cfg</code>:</p>
<pre><code>scorer = {&quot;@scorers&quot;:&quot;spacy.tagger_scorer.v1&quot;}
</code></pre>
<p>but I'm not sure how to actually edit this to get the desired output.</p>
<p>By the way, what is <code>score</code> metric in this context, and why does <code>loss tok2vec</code> keep increasing during the training (while <code>accuracy</code> improves)?</p>
",Named Entity Recognition (NER),view precision recall f custom spacy tag training try train custom po tagger spacy run training get following training metric however different tutorial example see additional metric would nice see well true mostly indicated case ner training po training example see reason indicate case po well suspect could customized following line sure actually edit get desired output way metric context doe keep increasing training improves
Extracting important entities from unstructured data,"<p>I am working on a NLP problem where I am completely stuck at certain point. I am new to these so pardon if the question is dumb.
I have got a completely unstructured text let's say: &quot;<code>a person named x y is travelling to country ab, he spent xyz (alpha/currency/beta/gamma), ate a b c d e f food items and many more.</code>&quot;
now I have to extract</p>
<pre><code>|name of person| country's name | amount spent and the currency | food items he ate | place of              
stay|
</code></pre>
<p><strong>Constraint on this is, the text contains some false information, for example: the food b and c cannot be found in a particular country, and thus it should not be extracted.</strong>
I have a nested dictionary which looks like this:</p>
<pre><code>{country_name: {place 1: {name of hotels:[hotel1, hotel2, hotel3....],
                          eatables: [food1, food2, food3, food4.....],
                          currency_accepted: [c1, c2, c3, c4.......],
                          }
                }
} 
</code></pre>
<p><strong>I want to use this dictionary in the unstructured text so that I can parse the data and extract entities which are relevant in separate columns of dataframe.</strong>
I have seen NER based approaches, but I guess it requires tagging of words, and I have got huge data.</p>
<p><strong>I have tried regex based approach for pattern matching, but that doesn't give all the results, further to that I have tried to match all the entities stored in a list, but this creates the problem of many false entities being extracted and accuracy is quite important here.</strong></p>
<p>I am looking for more improve parsing based approaches, also if there is any way a certain model is trained on this dictionary such that it looks for values of nested dictionary only if a key is found in the unstructured text.</p>
",Named Entity Recognition (NER),extracting important entity unstructured data working nlp problem completely stuck certain point new pardon question dumb got completely unstructured text let say extract constraint text contains false information example food b c found particular country thus extracted nested dictionary look like want use dictionary unstructured text parse data extract entity relevant separate column dataframe seen ner based approach guess requires tagging word got huge data tried regex based approach pattern matching give result tried match entity stored list creates problem many false entity extracted accuracy quite important looking improve parsing based approach also way certain model trained dictionary look value nested dictionary key found unstructured text
"How can I use NER Model from Simple Transformers with phrases instead of words, and startchar_endchar (mapping to text) instead of sentence_id?","<p>My data is in BRAT annotation format and I would like to use NER_Model from SimpleTransformers to test performance on this data with a variety of models. Is it possible to reshape my data in a way that can utilize NER_Model? Any suggestions or logic is greatly appreciated. There are four possible Entities (Microorganism, Habitat, Geographical, Phenotype)</p>
<p>BRAT Annotation .a2 file example data:<br/>
T5  Habitat 256 267 respiratory<br/>
T6  Habitat 281 290 pediatric<br/>
T7  Habitat 339 372 children less than 2 years of age<br/>
T8  Habitat 418 488 children greater than 9 years of age who had lower  respiratory illness</p>
",Named Entity Recognition (NER),use ner model simple transformer phrase instead word startchar endchar mapping text instead sentence id data brat annotation format would like use ner model simpletransformers test performance data variety model possible reshape data way utilize ner model suggestion logic greatly appreciated four possible entity microorganism habitat geographical phenotype brat annotation file example data habitat respiratory habitat pediatric habitat child le year age habitat child greater year age lower respiratory illness
How to load data for only certain label of Spacy&#39;s NER entities?,"<p>I just started to explore spaCy and need it only for GPE (Global political entities) of the name entity recognition (NER) component.</p>
<p>So, to save time on loading I keep only 'ner':</p>
<pre><code>    nlp = spacy.load('en_core_web_sm', disable=['tok2vec','tagger','parser', 'senter', 'attribute_ruler', 'lemmatizer'])
</code></pre>
<p>Then I create a set of cities / states / countries that exist in the text by running:</p>
<pre><code>doc = nlp(txt) 
geo_ents = {str(word) for word in doc.ents if word.label_=='GPE'}
</code></pre>
<p>That means I only need a small subset of the entities with the label_=='GPE'.
I didn't find a way yet to iterate only within that component of the whole model to reduce runtime on big loads of texts.</p>
<p>Would you please guide me to how to load only certain label of Spacy's NER entities? That might be helpful for others in order to get only selected types of entities.</p>
<p>Thank you very much!</p>
",Named Entity Recognition (NER),load data certain label spacy ner entity started explore spacy need gpe global political entity name entity recognition ner component save time loading keep ner create set city state country exist text running mean need small subset entity label gpe find way yet iterate within component whole model reduce runtime big load text would please guide load certain label spacy ner entity might helpful others order get selected type entity thank much
Python| Unable to extract the list of names from the text,"<p>Executing the below to extract the list of names from the text1. The text1 variable is the merge of the pdf's.
But executing the below code gives just one name out of complete input.
Tried to change patterns but didn't work.</p>
<p>Code:</p>
<pre><code>import spacy
from spacy.matcher import Matcher

# load pre-trained model
nlp = spacy.load('en_core_web_sm')

# initialize matcher with a vocab
matcher = Matcher(nlp.vocab)

def extract_name(resume_text):
    nlp_text = nlp(resume_text)
    #print(nlp_text)
    
    # First name and Last name are always Proper Nouns
    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]
    
    #matcher.add('NAME', None, [pattern])
    matcher.add('NAME', [pattern], on_match=None)
    
    matches = matcher(nlp_text)
    
    for match_id, start, end in matches:
        span = nlp_text[start:end]
        #print(span)
        return span.text
</code></pre>
<p>Execution: extract_name(text1)
O/P:  'VIKRAM RATHOD'</p>
<p>Expected O/P: List of all names in the text1</p>
",Named Entity Recognition (NER),python unable extract list name text executing extract list name text text variable merge pdf executing code give one name complete input tried change pattern work code execution extract name text p vikram rathod expected p list name text
How to convert annotated text in XML to CONLL?,"<p>I need to preprocess XML files for a NER task and I am struggling with the conversion of the XML files. I guess there is a nice and easy way to solve the following problem.</p>
<p>Given an annotated text in XML with the following structure as input:</p>
<pre><code>&lt;doc&gt;
   Some &lt;tag1&gt;annotated text&lt;/tag1&gt; in &lt;tag2&gt;XML&lt;/tag2&gt;.
&lt;/doc&gt;
</code></pre>
<p>I want a CoNLL file in IOB2 tagging format as follows as output:</p>
<pre><code>Some          O
annotated     B-TAG1
text          I-TAG1
in            O
XML           B-TAG2
.             O
</code></pre>
",Named Entity Recognition (NER),convert annotated text xml conll need preprocess xml file ner task struggling conversion xml file guess nice easy way solve following problem given annotated text xml following structure input want conll file iob tagging format follows output
TFCamemBERT model trains but no results in test,"<p>Currently I am working on Named Entity Recognition in the medical domain using Camembert, precisely using the model: <a href=""https://huggingface.co/jplu/tf-camembert-base"" rel=""nofollow noreferrer"">TFCamembert</a>.</p>
<p>However I have some problems with the fine-tuning of the model for my task as I am using a private dataset not available on Hugging Face.</p>
<p>The data is divided into text files and annotation files. The text file contains for example:</p>
<pre><code>Le cas présenté concerne un homme âgé de 61 ans (71 kg, 172 cm, soit un indice de masse corporelle de 23,9 kg/m²) admissible à une transplantation pulmonaire en raison d’une insuffisance respiratoire chronique terminale sur emphysème post-tabagique, sous oxygénothérapie continue (1 L/min) et ventilation non invasive nocturne. Il présente, comme principaux antécédents, une dyslipidémie, une hypertension artérielle et un tabagisme sevré estimé à 21 paquets-années (facteurs de risque cardiovasculaires). Le bilan préopératoire a révélé une hypertension artérielle pulmonaire essentiellement postcapillaire conduisant à l’ajout du périndopril (2 mg par jour) et du furosémide (40 mg par jour). La mise en évidence d’un Elispot (enzyme-linked immunospot) positif pour la tuberculose a motivé l’introduction d’un traitement prophylactique par l’association rifampicine-isoniazide (600-300 mg par jour) pour une durée de trois mois.
Deux mois après le bilan préopératoire, le patient a bénéficié d’une transplantation mono-pulmonaire gauche sans dysfonction primaire du greffon5,6. Le donneur et le receveur présentaient tous deux un statut sérologique positif pour cytomegalovirus (CMV) et Epstein Barr Virus (EBV). Une sérologie positive de la toxoplasmose a été mise en évidence uniquement chez le receveur. Le traitement immunosuppresseur d’induction associait la méthylprednisolone (500 mg à jour 0 et 375 mg à jour +1 post-transplantation) et le basiliximab, anticorps monoclonal dirigé contre l’interleukine-2 (20 mg à jour 0 et jour +4 posttransplantation). À partir de jour +2 post-transplantation, l’immunosuppression a été maintenue par une trithérapie par voie orale comprenant le tacrolimus à une posologie initiale de 5 mg par jour, le mofétil mycophénolate (MMF) 2000 mg par jour et la prednisone 20 mg par jour. Les traitements associés sont présentés dans le tableau I.
L’évolution est marquée par la survenue, au jour +5 posttransplantation, d’une dégradation respiratoire sur œdème pulmonaire gauche de reperfusion, avec possible participation cardiogénique. Le rejet aigu de grade III, évoqué par la présence d’infiltrats lymphocytaires aux biopsies transbronchiques, a été confirmé par l’anatomopathologie.
</code></pre>
<p>While the annotation file looks like:</p>
<pre><code>T1 genre 28 33 homme
T2 age 41 47 61 ans
A1 genre T1 masculin
T3 origine 127 326 une transplantation pulmonaire en raison d’une insuffisance respiratoire chronique terminale sur emphysème post-tabagique, sous oxygénothérapie continue (1 L/min) et ventilation non invasive nocturne
T4 issue 1962 2104 une dégradation respiratoire sur œdème pulmonaire gauche de reperfusion, avec possible participation cardiogénique. Le rejet aigu de grade III
A2 issue T4 détérioration
</code></pre>
<p>More details about the prepossessing of <a href=""https://drive.google.com/file/d/1Odq6eTexLg9ZXCjbWbnZiWMGAqg9Ut45/view?usp=sharing"" rel=""nofollow noreferrer"">the data</a> can be found in this <a href=""https://colab.research.google.com/drive/1oqCRFFvzSjDBpfCk5nS2KjBvzCr34Rqz?usp=sharing"" rel=""nofollow noreferrer"">notebook</a>.</p>
<p>The thing is that once I finish the training, I try to run the model on test data it doesn't work at all. I can't figure out where is the problem as the train and test data have the same format.</p>
<pre><code>from datasets import load_metric
import numpy as np

metric = load_metric(&quot;seqeval&quot;)

def evaluate(model, dataset, ner_labels):
  all_predictions = []
  all_labels = []
  for batch in dataset:
    logits = model.predict(batch)[&quot;logits&quot;]
    labels = batch[&quot;labels&quot;]
    predictions = np.argmax(logits, axis = -1)
    for prediction, label in zip(predictions, labels):
      for predicted_idx, label_idx in zip(prediction, label):
        if label_idx == -100:
          continue
        all_predictions.append(ner_labels[predicted_idx])
        all_labels.append(ner_labels[label_idx])
  return metric.compute(predictions=[all_predictions], references=[all_labels])

results = evaluate(model, test_dataset, ner_labels=list(model.config.id2label.values()))
results
</code></pre>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-40-d7e10156347b&gt; in &lt;module&gt;()
     31   return metric.compute(predictions=[all_predictions], references=[all_labels])
     32 
---&gt; 33 results = evaluate(model, test_dataset, ner_labels=list(model.config.id2label.values()))

2 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)
   1127           except Exception as e:  # pylint:disable=broad-except
   1128             if hasattr(e, &quot;ag_error_metadata&quot;):
-&gt; 1129               raise e.ag_error_metadata.to_exception(e)
   1130             else:
   1131               raise

ValueError: in user code:

    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1621, in predict_function  *
        return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1611, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1604, in run_step  **
        outputs = model.predict_step(data)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1572, in predict_step
        return self(x, training=False)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None

    ValueError: Exception encountered when calling layer &quot;tf_camembert_for_token_classification&quot; (type TFCamembertForTokenClassification).
    
    in user code:
    
        File &quot;/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_tf_roberta.py&quot;, line 1681, in call  *
            outputs = self.roberta(
        File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 67, in error_handler  **
            raise e.with_traceback(filtered_tb) from None
    
        ValueError: Exception encountered when calling layer &quot;roberta&quot; (type TFRobertaMainLayer).
        
        in user code:
        
            File &quot;/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_tf_roberta.py&quot;, line 660, in call  *
                batch_size, seq_length = input_shape
        
            ValueError: not enough values to unpack (expected 2, got 1)
        
        
        Call arguments received:
          • input_ids=tf.Tensor(shape=(32,), dtype=int32)
          • attention_mask=tf.Tensor(shape=(32,), dtype=int32)
          • token_type_ids=None
          • position_ids=None
          • head_mask=None
          • inputs_embeds=None
          • encoder_hidden_states=None
          • encoder_attention_mask=None
          • past_key_values=None
          • use_cache=None
          • output_attentions=False
          • output_hidden_states=False
          • return_dict=True
          • training=False
          • kwargs=&lt;class 'inspect._empty'&gt;
    
    
    Call arguments received:
      • input_ids={'input_ids': 'tf.Tensor(shape=(32,), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(32,), dtype=int32)', 'labels': 'tf.Tensor(shape=(32,), dtype=int32)'}
      • attention_mask=None
      • token_type_ids=None
      • position_ids=None
      • head_mask=None
      • inputs_embeds=None
      • output_attentions=None
      • output_hidden_states=None
      • return_dict=None
      • labels=None
      • training=False
      • kwargs=&lt;class 'inspect._empty'&gt;
</code></pre>
<p>When the test used to work, <code>model.predict()</code> accepted data batches normally, but the results were all 0 even though the internal loss in the training phase was deacreasing.</p>
<pre><code>{'age': {'f1': 0.0, 'number': 145, 'precision': 0.0, 'recall': 0.0},
 'anatomie': {'f1': 0.0, 'number': 952, 'precision': 0.0, 'recall': 0.0},
 'date': {'f1': 0.0, 'number': 15, 'precision': 0.0, 'recall': 0.0},
 'dose': {'f1': 0.0, 'number': 27, 'precision': 0.0, 'recall': 0.0},
 'duree': {'f1': 0.0, 'number': 2, 'precision': 0.0, 'recall': 0.0},
 'examen': {'f1': 0.0, 'number': 553, 'precision': 0.0, 'recall': 0.0},
 'frequence': {'f1': 0.0, 'number': 8, 'precision': 0.0, 'recall': 0.0},
 'genre': {'f1': 0.0, 'number': 146, 'precision': 0.0, 'recall': 0.0},
 'mode': {'f1': 0.0, 'number': 79, 'precision': 0.0, 'recall': 0.0},
 'moment': {'f1': 0.0, 'number': 23, 'precision': 0.0, 'recall': 0.0},
 'origine': {'f1': 0.0, 'number': 11, 'precision': 0.0, 'recall': 0.0},
 'overall_accuracy': 0.9089205003328545,
 'overall_f1': 0.0,
 'overall_precision': 0.0,
 'overall_recall': 0.0,
 'pathologie': {'f1': 0.0, 'number': 162, 'precision': 0.0, 'recall': 0.0},
 'sosy': {'f1': 0.0, 'number': 439, 'precision': 0.0, 'recall': 0.0},
 'substance': {'f1': 0.0, 'number': 633, 'precision': 0.0, 'recall': 0.0},
 'traitement': {'f1': 0.0, 'number': 205, 'precision': 0.0, 'recall': 0.0},
 'valeur': {'f1': 0.0, 'number': 192, 'precision': 0.0, 'recall': 0.0}}
</code></pre>
<p>Any clues to solve this gradient issue? Thanks in advance!</p>
",Named Entity Recognition (NER),tfcamembert model train result test currently working named entity recognition medical domain using camembert precisely using model tfcamembert however problem fine tuning model task using private dataset available hugging face data divided text file annotation file text file contains example annotation file look like detail prepossessing data found notebook thing finish training try run model test data work figure problem train test data format test used work accepted data batch normally result even though internal loss training phase wa deacreasing clue solve gradient issue thanks advance
Split the string into multiple sentences with R and pos tagging,"<p>I don't know if this is the right place, but if possible, could you help me split a text into several sentences using R.
I have a database that contains the description of activities that employees perform. I would like to split this text into several sentences and then extract the verb-noun pair from each sentence.
I can do this line by line, but as there are many lines it would take forever, so I would like to know if you guys know how to do this for the entire column.
You guys can see the database in: <a href=""https://docs.google.com/spreadsheets/d/1NiMj37q8_hJhuNFCiQcjO6UBvI9_-OM4/edit?usp=sharing&amp;ouid=115543599430411372875&amp;rtpof=true&amp;sd=true"" rel=""nofollow noreferrer"">https://docs.google.com/spreadsheets/d/1NiMj37q8_hJhuNFCiQcjO6UBvI9_-OM4/edit?usp=sharing&amp;ouid=115543599430411372875&amp;rtpof=true&amp;sd=true</a></p>
<p>I can do it one by one as the following code, but I would like to do it for the entire description</p>
<pre><code>library(udpipe)
&gt; docs &lt;- &quot;Determine and formulate policies and provide overall direction of companies or private and public sector organizations within guidelines set up by a board of directors or similar governing body. Plan, direct, or coordinate operational activities at the highest level of management with the help of subordinate executives and staff managers.&quot;
docs &lt;- setNames(docs, &quot;doc1&quot;)
anno &lt;- udpipe(docs, object = &quot;english&quot;, udpipe_model_repo = &quot;bnosac/udpipe.models.ud&quot;)
anno &lt;- cbind_dependencies(anno, type = &quot;parent&quot;)
subset(anno, upos_parent %in% c(&quot;NOUN&quot;, &quot;VERB&quot;) &amp; upos %in% c(&quot;NOUN&quot;, &quot;VERB&quot;), 
+select = c(&quot;doc_id&quot;, &quot;paragraph_id&quot;, &quot;sentence_id&quot;, &quot;token&quot;, &quot;token_parent&quot;, &quot;dep_rel&quot;,&quot;upos&quot;, &quot;upos_parent&quot;))
</code></pre>
",Named Entity Recognition (NER),split string multiple sentence r po tagging know right place possible could help split text several sentence using r database contains description activity employee perform would like split text several sentence extract verb noun pair sentence line line many line would take forever would like know guy know entire column guy see database one one following code would like entire description
ValueError: No gradients provided for any variable (TFCamemBERT),"<p>Currently I am working on Named Entity Recognition in the medical domain using Camembert, precisely using the model: <a href=""https://huggingface.co/jplu/tf-camembert-base"" rel=""nofollow noreferrer"">TFCamembert</a>.</p>
<p>However I have some problems with the fine-tuning of the model for my task as I am using a private dataset not available on Hugging Face.</p>
<p>The data is divided into text files and annotation files. The text file contains for example:</p>
<pre><code>Le cas présenté concerne un homme âgé de 61 ans (71 kg, 172 cm, soit un indice de masse corporelle de 23,9 kg/m²) admissible à une transplantation pulmonaire en raison d’une insuffisance respiratoire chronique terminale sur emphysème post-tabagique, sous oxygénothérapie continue (1 L/min) et ventilation non invasive nocturne. Il présente, comme principaux antécédents, une dyslipidémie, une hypertension artérielle et un tabagisme sevré estimé à 21 paquets-années (facteurs de risque cardiovasculaires). Le bilan préopératoire a révélé une hypertension artérielle pulmonaire essentiellement postcapillaire conduisant à l’ajout du périndopril (2 mg par jour) et du furosémide (40 mg par jour). La mise en évidence d’un Elispot (enzyme-linked immunospot) positif pour la tuberculose a motivé l’introduction d’un traitement prophylactique par l’association rifampicine-isoniazide (600-300 mg par jour) pour une durée de trois mois.
Deux mois après le bilan préopératoire, le patient a bénéficié d’une transplantation mono-pulmonaire gauche sans dysfonction primaire du greffon5,6. Le donneur et le receveur présentaient tous deux un statut sérologique positif pour cytomegalovirus (CMV) et Epstein Barr Virus (EBV). Une sérologie positive de la toxoplasmose a été mise en évidence uniquement chez le receveur. Le traitement immunosuppresseur d’induction associait la méthylprednisolone (500 mg à jour 0 et 375 mg à jour +1 post-transplantation) et le basiliximab, anticorps monoclonal dirigé contre l’interleukine-2 (20 mg à jour 0 et jour +4 posttransplantation). À partir de jour +2 post-transplantation, l’immunosuppression a été maintenue par une trithérapie par voie orale comprenant le tacrolimus à une posologie initiale de 5 mg par jour, le mofétil mycophénolate (MMF) 2000 mg par jour et la prednisone 20 mg par jour. Les traitements associés sont présentés dans le tableau I.
L’évolution est marquée par la survenue, au jour +5 posttransplantation, d’une dégradation respiratoire sur œdème pulmonaire gauche de reperfusion, avec possible participation cardiogénique. Le rejet aigu de grade III, évoqué par la présence d’infiltrats lymphocytaires aux biopsies transbronchiques, a été confirmé par l’anatomopathologie.
</code></pre>
<p>While the annotation file looks like:</p>
<pre><code>T1 genre 28 33 homme
T2 age 41 47 61 ans
A1 genre T1 masculin
T3 origine 127 326 une transplantation pulmonaire en raison d’une insuffisance respiratoire chronique terminale sur emphysème post-tabagique, sous oxygénothérapie continue (1 L/min) et ventilation non invasive nocturne
T4 issue 1962 2104 une dégradation respiratoire sur œdème pulmonaire gauche de reperfusion, avec possible participation cardiogénique. Le rejet aigu de grade III
A2 issue T4 détérioration
</code></pre>
<p>More details about the prepossessing of <a href=""https://drive.google.com/file/d/1Odq6eTexLg9ZXCjbWbnZiWMGAqg9Ut45/view?usp=sharing"" rel=""nofollow noreferrer"">the data</a> can be found in this <a href=""https://colab.research.google.com/drive/1oqCRFFvzSjDBpfCk5nS2KjBvzCr34Rqz?usp=sharing"" rel=""nofollow noreferrer"">notebook</a>.</p>
<p>The things is that the internal loss of my model does not work, if I run the training of the model without declaring a loss, it does not work, I have to define a loss to be able to run the training!</p>
<p>Here is my train_data converted to Tensor slice dataset.</p>
<pre><code>train_label_encodings = tf.convert_to_tensor(train_label_encodings, dtype=tf.int32)
train_label_encodings.data

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_text_encodings.data),
    train_label_encodings.data
))
train_dataset
</code></pre>
<pre><code>&lt;TensorSliceDataset shapes: ({input_ids: (512,), offset_mapping: (512, 2)}, (512,)), types: ({input_ids: tf.int32, offset_mapping: tf.int32}, tf.int32)&gt;
</code></pre>
<p>I define the model:</p>
<pre><code># Import the model and define an optimizer
from transformers import TFAutoModelForTokenClassification, TFCamembertModel, create_optimizer
import tensorflow as tf

num_train_steps = len(train_dataset) * 5
optimizer, lr_schedule = create_optimizer(
    init_lr = 5e-6,
    num_train_steps = num_train_steps,
    weight_decay_rate = 0.01,
    num_warmup_steps = 0
)

metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

model = TFAutoModelForTokenClassification.from_pretrained(model_id, num_labels=len(unique_labels), label2id=label2id, id2label=id2label)

model.compile(optimizer=optimizer, metrics=['accuracy'])
</code></pre>
<p>Here is the summary of the model:</p>
<pre><code>Model: &quot;tf_camembert_for_token_classification_2&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 roberta (TFRobertaMainLayer  multiple                 110031360 
 )                                                               
                                                                 
 dropout_113 (Dropout)       multiple                  0         
                                                                 
 classifier (Dense)          multiple                  25377     
                                                                 
=================================================================
Total params: 110,056,737
Trainable params: 110,056,737
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p>When I try to launch the training, I get the following error:</p>
<pre><code>import os
from tensorflow.keras.callbacks import TensorBoard

callbacks = []
callbacks.append(TensorBoard(log_dir=os.path.join(output_dir,&quot;logs&quot;)))

model.fit(
    train_dataset,
    callbacks = callbacks,
    epochs = 3,
)
</code></pre>
<pre><code>Epoch 1/3
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-71-54e2d25b9415&gt; in &lt;module&gt;()
      8     train_dataset,
      9     callbacks = callbacks,
---&gt; 10     epochs = 3,
     11 )

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)
   1127           except Exception as e:  # pylint:disable=broad-except
   1128             if hasattr(e, &quot;ag_error_metadata&quot;):
-&gt; 1129               raise e.ag_error_metadata.to_exception(e)
   1130             else:
   1131               raise

ValueError: in user code:

    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 878, in train_function  *
        return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 867, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 860, in run_step  **
        outputs = model.train_step(data)
    File &quot;/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py&quot;, line 911, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py&quot;, line 532, in minimize
        return self.apply_gradients(grads_and_vars, name=name)
    File &quot;/usr/local/lib/python3.7/dist-packages/transformers/optimization_tf.py&quot;, line 232, in apply_gradients
        return super(AdamWeightDecay, self).apply_gradients(zip(grads, tvars), name=name, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py&quot;, line 633, in apply_gradients
        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/utils.py&quot;, line 73, in filter_empty_gradients
        raise ValueError(f&quot;No gradients provided for any variable: {variable}. &quot;

    ValueError: No gradients provided for any variable: (['tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/query/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/query/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/key/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/key/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/value/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/value/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/output/dense/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/output/dense/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/output/La...
</code></pre>
<p>Any clues to solve this gradient issue? Thanks in advance!</p>
",Named Entity Recognition (NER),valueerror gradient provided variable tfcamembert currently working named entity recognition medical domain using camembert precisely using model tfcamembert however problem fine tuning model task using private dataset available hugging face data divided text file annotation file text file contains example annotation file look like detail prepossessing data found notebook thing internal loss model doe work run training model without declaring loss doe work define loss able run training train data converted tensor slice dataset define model summary model try launch training get following error clue solve gradient issue thanks advance
How can I feed multiple of 100 annotated files in training Custom NER model using spacy3,"<p>Currently I have prepared a Custom NER model where I can extract 15 entities. I have trained my model on single annotated file using config.cfg file by spacy3. Now I want to train my model again on 100 annotated files. How can I pass these annotated files?</p>
",Named Entity Recognition (NER),feed multiple annotated file training custom ner model using spacy currently prepared custom ner model extract entity trained model single annotated file using config cfg file spacy want train model annotated file pas annotated file
How do you build training dataset from scratch for a custom multi-class standfordNLP/Stanza NER tagging model in BIOES/BILOU format?,"<p>I am utilizing NLP for a custom application and I want to train my own NER tagger model in StanfordNLP currently known as Stanza.</p>
<p>The default model is limited to very general tags such as LOC, PER, MISC, COUNTRY, TIME etc.</p>
<p>My custom tags are more specific ex. Food, Sport, Software, Brand. How would I got about formatting data I <strong>scraped</strong> off of the web or from PDF files in BIOES/BILOU format?
<a href=""https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)</a></p>
<p>Do I necessarily have to tag them manually? or write a script to generate the data in the format shown below:</p>
<pre><code>Alex S-PER
is O
playing O
basketball I-SPORT
with O
Marty B-PER
. O
Rick E-PER
likes O
to O
eat O
Pizza I-FOOD
in O
Los B-LOC
Angeles E-LOC
</code></pre>
<p>If so then what tools and libraries can I use in Python?</p>
<p>Thank you in advance.</p>
",Named Entity Recognition (NER),build training dataset scratch custom multi class standfordnlp stanza ner tagging model bioes bilou format utilizing nlp custom application want train ner tagger model stanfordnlp currently known stanza default model limited general tag loc per misc country time etc custom tag specific ex food sport software brand would got formatting data scraped web pdf file bioes bilou format necessarily tag manually write script generate data format shown tool library use python thank advance
What is the best way in Python to check if a string is the name of a city or the name of a country (also historical),"<p>I have a long list of strings containing names of cities or countries. The list also contain some names of countries that does not exist anymore like Prussia or the Ottman Empire.</p>

<pre><code>l= [
'Russia',
'Smolensk',
'Moscow',
'Moscow',
'France',
'France',
'Russia',
'Prussia',
'Austria',
'Sweden',
'Constantinople',
'Russia',
'Great Britain',
'Spain',
'Portugal',
'Germany',
'Frankfurt',
'France',
'Leningrad',
'Paris',
'DDR',
'Paris',
'France',
'Paris',
'the United States',
'St. Helena',
]
</code></pre>

<p>I want to split that list into two <code>l_countries</code> and <code>l_cities</code>. One containing the cities names, and another containing country names.</p>

<p>I think the difficulty is to detect old-city and old-country names?</p>

<p>Like:</p>

<ul>
<li>Prussia (does not exist anymore)</li>
<li>The DDR (does not exist anymore)</li>
<li>Yugoslavia (does not exist anymore)</li>
<li>Constantinople (now Istanbul)</li>
<li>Leningrad (now Saint Petersburg)</li>
</ul>
",Named Entity Recognition (NER),best way python check string name city name country also historical long list string containing name city country list also contain name country doe exist anymore like prussia ottman empire want split list two one containing city name another containing country name think difficulty detect old city old country name like prussia doe exist anymore ddr doe exist anymore yugoslavia doe exist anymore constantinople istanbul leningrad saint petersburg
Extracting the entities from Spacy and putting in new dataframe,"<p>So I followed the answer in this question (<a href=""https://stackoverflow.com/questions/60116419/extract-entity-from-dataframe-using-spacy"">Extract entity from dataframe using spacy</a>) and that solved me being able to iterate on a DF. </p>

<p>Issues I am facing is trying to take these results, add a column from original df then put all this into a new df. I want the DOI from original df, the entity text and entity labels from the NER.</p>

<p>Code to grab and put into list:</p>

<pre><code>entities=[]
nlp = spacy.load(""en_ner_bionlp13cg_md"")
for i in df['Abstract'].tolist():
    doc = nlp(i)
    for entity in doc.ents:
        entities.append((df.DOI, entity.text , entity.label_))
</code></pre>

<p>Then I take the entities list and feed into a new df:</p>

<pre><code>df_ner = pd.DataFrame.from_records(entities, columns =['DOI', 'ent_name', 'ent_type'])
</code></pre>

<p>Unfortunately, only the first records gets loaded into the df. What am i missing?</p>

<pre><code>DOI ent_name    ent_type
0   3 10.7501/j.issn.0253-2670.2020.... COVID-19    GENE_OR_GENE_PRODUCT
1   3 10.7501/j.issn.0253-2670.2020.... ACE2    GENE_OR_GENE_PRODUCT
2   3 10.7501/j.issn.0253-2670.2020.... angiotensin converting enzyme II    GENE_OR_GENE_PRODUCT
3   3 10.7501/j.issn.0253-2670.2020.... ACE2    GENE_OR_GENE_PRODUCT
4   3 10.7501/j.issn.0253-2670.2020.... UniProt GENE_OR_GENE_PRODUCT
</code></pre>
",Named Entity Recognition (NER),extracting entity spacy putting new dataframe followed answer question href entity dataframe using spacy solved able iterate df issue facing trying take result add column original df put new df want doi original df entity text entity label ner code grab put list take entity list feed new df unfortunately first record get loaded df missing
Can we train Spacy for text summarization,"<p>Spacy can train for NER, text classification. And we can use it for summarization with its functionalities, so that
Can we train spacy in order to improve its accuracy in summarization?</p>
",Named Entity Recognition (NER),train spacy text summarization spacy train ner text classification use summarization functionality train spacy order improve accuracy summarization
How to extract sentences from one text with only 1 named entity using spaCy?,"<p>I have a list of sentences and I want to be able to append only the sentences with <strong>1 &quot;PERSON&quot; named entity</strong> using spaCy. The code I used was as follows:</p>
<pre><code>test_list = []
for item in sentences: #for each sentence in 'sentences' list
  for ent in item.ents: #for each entity in the sentence's entities 
    if len(ent in item.ents) == 1: #if there is only one entity
      if ent.label_ == &quot;PERSON&quot;: #and if the entity is a &quot;PERSON&quot;
        test_list.append(item) #put the sentence into 'test_list'
</code></pre>
<p>But then I get:</p>
<pre><code>TypeError: object of type 'bool' has no len()
</code></pre>
<p>Am I doing this wrong? How exactly would I complete this task?</p>
",Named Entity Recognition (NER),extract sentence one text named entity using spacy list sentence want able append sentence person named entity using spacy code used wa follows get wrong exactly would complete task
How do I resolve proper nouns when they&#39;re being addressed by only part of their name,"<p>Say I have piece of text like:</p>
<blockquote>
<p>Apple was founded in 1976 by <strong>Steve Jobs</strong>, <strong>Steve Wozniak</strong> and <strong>Ronald Wayne</strong> to develop and sell <strong>Wozniak</strong>'s Apple I personal computer. It was incorporated by <strong>Jobs</strong> and <strong>Wozniak</strong> as Apple Computer, Inc. in 1977, and sales of its computers, among them the Apple II, grew quickly. Apple Computer, Inc. was incorporated on January 3, 1977, without <strong>Wayne</strong>, who had left and sold his share of the company back to <strong>Jobs</strong> and <strong>Wozniak</strong> for $800 only twelve days after having co-founded Apple.</p>
</blockquote>
<p>Here &quot;Jobs&quot;, &quot;Wozniak&quot;, &quot;Wayne&quot; refer to &quot;Steve Jobs&quot;, &quot;Steve Wozniak&quot; and &quot;Ronald Wayne&quot; respectively.</p>
<p>How do I resolve the text to something like</p>
<blockquote>
<p>Apple was founded in 1976 by <strong>Steve Jobs</strong>, <strong>Steve Wozniak</strong> and <strong>Ronald Wayne</strong> to develop and sell Steve Wozniak's Apple I personal computer. It was incorporated by <strong>Steve Jobs</strong> and <strong>Steve Wozniak</strong> as Apple Computer, Inc. in 1977, and sales of its computers, among them the Apple II, grew quickly. Apple Computer, Inc. was incorporated on January 3, 1977, without <strong>Robert Wayne</strong>, who had left and sold his share of the company back to <strong>Steve Jobs</strong> and <strong>Steve Wozniak</strong> for $800 only twelve days after having co-founded Apple.</p>
</blockquote>
<p>Replacing &quot;Jobs&quot; with &quot;Steve Jobs&quot; is obviously what need to be done but how do I detect that there is &quot;Jobs&quot; in the text that corresponds to &quot;Steve Jobs&quot;.</p>
<p>(Steve Jobs and Jobs are detected as separate named entities)</p>
",Named Entity Recognition (NER),resolve proper noun addressed part name say piece text like apple wa founded steve job steve wozniak ronald wayne develop sell wozniak apple personal computer wa incorporated job wozniak apple computer inc sale computer among apple ii quickly apple computer inc wa incorporated january without wayne left sold share company back job wozniak twelve day co founded apple job wozniak wayne refer steve job steve wozniak ronald wayne respectively resolve text something like apple wa founded steve job steve wozniak ronald wayne develop sell steve wozniak apple personal computer wa incorporated steve job steve wozniak apple computer inc sale computer among apple ii quickly apple computer inc wa incorporated january without robert wayne left sold share company back steve job steve wozniak twelve day co founded apple replacing job steve job obviously need done detect job text corresponds steve job steve job job detected separate named entity
R Function for Return New Column to Dataset,"<p>I'm currently working with a dataset with different speakers and am trying to extract the amount of words in a utterance. I am also trying to count the number of backchannels (utterances with three or less words). These metrics would be used for further analysis of the dataset. Please see a slice of the data below.</p>
<pre><code>speaker &lt;- c(&quot;P6&quot;, &quot;P4&quot;, &quot;P5&quot;, &quot;P6&quot;, &quot;P6&quot;)
utterance &lt;- c(&quot;Alright&quot;, &quot;So this is a social talk right? So we’re only supposed to talk about work only&quot;, &quot;yeah&quot;, &quot;And that’s the thing, so it’s not clear to me we need to work or just to be social.&quot;, &quot;But a bit and a bit&quot;)
df &lt;- data.frame(speaker, utterance)
</code></pre>
<p>These are the different functions I've tried out. The issue is that I would like to store the results in a new column in the same dataframe, but this is something I haven't been able to do yet (I'm a beginner with R). I can see with the code below that the first function works as intended, but I am having some issues with the second one. Ideally I would like both functions to just accept the generic column name rather than the dataframe.</p>
<pre><code>#function for utterance length
utterance_length &lt;- function(df){
  df &lt;- df %&gt;%
  mutate(utterance_length = str_count(df$utterance,&quot;\\S+&quot;))
  return(df)}


#function for backchannelling 
backchannelling &lt;- function(df){
  df$backchannelling &lt;- ifelse(df$utterance_length &gt; 3, 0, 1)
  return(df)
}
</code></pre>
<p>How can I: 1) save the new utterance_length column to the data frame (same goes to the backchannelling function); 2) only input column names in the function rather than the dataframe.</p>
",Named Entity Recognition (NER),r function return new column dataset currently working dataset different speaker trying extract amount word utterance also trying count number backchannels utterance three le word metric would used analysis dataset please see slice data different function tried issue would like store result new column dataframe something able yet beginner r see code first function work intended issue second one ideally would like function accept generic column name rather dataframe save new utterance length column data frame go backchannelling function input column name function rather dataframe
How to initialize tok2vec Transformer with a custom spacy ner model,"<p>I have some trouble with the initialization of a tok2vec Transformer with a custom spacy ner model. How do I use tok2vec properly before the ner step starts in the pipeline?</p>
<p>Init:</p>
<pre><code>    nlp = spacy.load(&quot;./output_training_11.11&quot;)
    ner = nlp.get_pipe(&quot;ner&quot;)
    config = {&quot;model&quot;: DEFAULT_TOK2VEC_MODEL}
    nlp.add_pipe(&quot;tok2vec&quot;,  before='ner')
</code></pre>
<p>Training:</p>
<pre><code>with nlp.disable_pipes(*other_pipes):

  optimizer = nlp.begin_training()

  for i in range(100):
    random.shuffle(TRAIN_DATA)
    for text, annotation in TRAIN_DATA:
        doc = nlp.make_doc(text)
        #print(annotation)
        try:
            example = Example.from_dict(doc, annotation)
            #print(example)
        except ValueError as e:
            print(e)
            print(&quot;Text: &quot; , doc[0:50])
            print(&quot;------&quot;)
        nlp.update([example], 
                   sgd=optimizer, drop=dropout)```


nlp(&quot;random text&quot;)


ValueError: [E109] Component 'tok2vec' could not be run. Did you forget to call `initialize()`?
</code></pre>
",Named Entity Recognition (NER),initialize tok vec transformer custom spacy ner model trouble initialization tok vec transformer custom spacy ner model use tok vec properly ner step start pipeline init training
Pinyin Named Entity Recognition,"<p>I'm trying to conduct named entity recognition or pull out the persons, places, etc... from Pinyin, or the romanization of Chinese characters.</p>
<p>For example (from Wikipedia):</p>
<pre><code> &quot;Jiang Zemin, Li Peng and Zhu Rongji led the nation in the 1990s. Under their administration, China's economic performance pulled an estimated 150 million peasants out of poverty and sustained an average annual gross domestic product growth rate of 11.2%.[125][better source needed][126][better source needed] The country joined the World Trade Organization in 2001, and maintained its high rate of economic growth under Hu Jintao and Wen Jiabao's leadership in the 2000s. However, the growth also severely impacted the country's resources and environment,[127][128] and caused major social displacement.[129][130]
Chinese Communist Party general secretary Xi Jinping has ruled since 2012 and has pursued large-scale efforts to reform China's economy [131][132] (which has suffered from structural instabilities and slowing growth),[133][134][135] and has also reformed the one-child policy and prison system,[136] as well as instituting a vast anti corruption crackdown.[137] In 2013, China initiated the Belt and Road Initiative, a global infrastructure investment project.[138] The COVID-19 pandemic broke out in Wuhan, Hubei in 2019.[139][140]&quot;
</code></pre>
<p>I am hoping to extract entities from the above like:</p>
<pre><code>Jiang Zemin
Li Peng
Zhu Rongji
Hu Jintao
Wuhan
Hubei
etc...
</code></pre>
<p>Chinese character NER is pretty sophisticated, but I don't know of a way to extract Pinyin.</p>
<p>My current plan was to try all permutations of the 1300+ chinese syllables as follows:</p>
<pre><code>import pandas as pd
import numpy as np

#import data
data = pd.read_csv('chinese_tones.txt', sep=&quot; &quot;, header=None)
data.columns = [&quot;pinyin&quot;, &quot;character&quot;]

#convert
data['pinyin'] = data['pinyin'].str.replace('\d+', '') #data doesn't have tones, which makes this harder
s = data['pinyin'].drop_duplicates().to_numpy()
combos = pd.Series(np.add.outer(s, s).ravel())

#combine to giant list
all_pinyin = pd.Series(s.tolist() + np.add.outer(s, s).ravel().tolist())
</code></pre>
<p>I was then going to do something along the lines of
<code>.isin()</code> to compare the text data to the list of pinyin.</p>
<p>Does anyone know of a better way to extract entities Pinyin?</p>
",Named Entity Recognition (NER),pinyin named entity recognition trying conduct named entity recognition pull person place etc pinyin romanization chinese character example wikipedia hoping extract entity like chinese character ner pretty sophisticated know way extract pinyin current plan wa try permutation chinese syllable follows wa going something along line compare text data list pinyin doe anyone know better way extract entity pinyin
How to convert spaCy NER dataset format to Flair format?,"<p>I have already labled a dataset using dataturks to train a <code>spaCy</code> NER and everything works fine, however, I just realized that <code>Flair</code> has a different format and I am just wondering if there is a way to convert my &quot;spaCy's NER&quot; json dataset format into the <code>Flair</code> format:</p>
<blockquote>
<p>George N B-PER<br />
Washington N I-PER<br />
went V O<br />
to P O<br />
Washington N B-LOC</p>
</blockquote>
<p>However the spaCy's format will be as follow:</p>
<blockquote>
<p>[(&quot;George Washington went to Washington&quot;,<br />
{'entities': [(0, 6,'PER'),(7, 17,'PER'),(26, 36,'LOC')]})]</p>
</blockquote>
",Named Entity Recognition (NER),convert spacy ner dataset format flair format already labled dataset using dataturks train ner everything work fine however realized ha different format wondering way convert spacy ner json dataset format format george n b per washington n per went v p washington n b loc however spacy format follow george washington went washington entity per per loc
"Issue related with &#39;scorers&#39;, when trying to load a spacy NER model","<p>I'm having issues with spacy when trying to load the NER model:</p>
<pre><code>nlp = spacy.load(&quot;./output_model/model-best&quot;)
</code></pre>
<p>It prompts the following error:</p>
<pre><code>RegistryError: [E892] Unknown function registry: 'scorers'.

Available names: architectures, augmenters, batchers, callbacks, cli, datasets, displacy_colors, factories, initializers, languages, layers, lemmatizers, loggers, lookups, losses, misc, models, ops, optimizers, readers, schedules, tokenizers
</code></pre>
<p>I had a similar error before, and I was able to fix it by doing:</p>
<pre><code>pip install -U spacy
</code></pre>
<p>The same code was working properly yesterday, I wonder if there is a conflit of versions.</p>
",Named Entity Recognition (NER),issue related scorer trying load spacy ner model issue spacy trying load ner model prompt following error similar error wa able fix code wa working properly yesterday wonder conflit version
Spacy doesn&#39;t recognize any entities on self trained NER,"<p>I'm trying to find entities on websites text. So I've copied 20 sites into a text file (all in one line) and tagged entities manually. (according to this tutorial: <a href=""https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/"" rel=""nofollow noreferrer"">https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/</a>)</p>
<p>Usually, the text files contain 5000+ characters and I've tagged two entities per each file.
I've got <code>Spacy 3.2.1</code> and I'm using <code>nlp = spacy.load(&quot;en_core_web_sm&quot;)</code>.</p>
<p>The print of the losses:</p>
<pre><code>Losses {'ner': 438.16809472077654}
Losses {'ner': 448.97231569240785}
Losses {'ner': 470.66727516808453}
Losses {'ner': 477.0379003697807}
Losses {'ner': 8.354419636216686}
Losses {'ner': 86.56611033267922}
...
Losses {'ner': 0.026654468748418734}
Losses {'ner': 0.026672022747312552}
Losses {'ner': 0.026672030436685996}
Losses {'ner': 0.027033395785247216}
Losses {'ner': 0.027033751640550253}
Losses {'ner': 0.02703389312135557}
Losses {'ner': 0.029515604829788607}
</code></pre>
<p>When I use this model to find an entity on another text, it finds nothing. (even though I can see the entities in the text).</p>
<pre><code>doc = nlp(text)
print(&quot;Entities&quot;, [(ent.text, ent.label_) for ent in doc.ents])

displacy.serve(doc, style=&quot;ent&quot;)
</code></pre>
<p>This code just displays:</p>
<pre><code>Entities []
/home/python3.8/site-packages/spacy/displacy/__init__.py:200: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the     `doc.ents` property manually if necessary.
warnings.warn(Warnings.W006)

Using the 'ent' visualizer
Serving on http://0.0.0.0:5000
</code></pre>
<p>All the examples that I found online use a different text to entity ratio:
1 short sentence with 1 - 2 entities inside.
I'm using 1 long text with 1 - 2 entities inside.</p>
<p>Could that be the issue?
And if so, what do I need to do? Prepare more training data?</p>
",Named Entity Recognition (NER),spacy recognize entity self trained ner trying find entity website text copied site text file one line tagged entity manually according tutorial usually text file contain character tagged two entity per file got using print loss use model find entity another text find nothing even though see entity text code display example found online use different text entity ratio short sentence entity inside using long text entity inside could issue need prepare training data
Text Analysis: extracting person name and quotation: how to create a pattern,"<p>I need to extract a name and a quote for a given text such as:</p>
<blockquote>
<p>Homer Simpson said: &quot;Okay, here we go...&quot;</p>
</blockquote>
<p>The returned values:</p>
<blockquote>
<ul>
<li>extracted_person_name - The extracted person name, as appearing in the patterns explained above</li>
<li>extracted_quotation   - The extracted quoted text (withot the surrounding quotation marks).</li>
</ul>
<ul>
<li>Important Note: if the pattern is not found, return None values for both the <br />
extracted person name and the extracted text.</li>
</ul>
</blockquote>
<p>You could expect the input text to look similar to the following pattern:</p>
<blockquote>
<p>Person name said: &quot;quoted text&quot;</p>
</blockquote>
<p>Variations of the above pattern:</p>
<blockquote>
<p>The colon punctuation mark (:) is optional, and and might not appear
in the input sentence. Instead of the word said you could also expect
the words:</p>
<p>answered, responded, replied</p>
</blockquote>
<p><strong>this is what I got so far:</strong></p>
<pre><code>def person_quotation_pattern_extraction(raw_text):
    
    name_pattern = &quot;\w+\s\w+&quot;
    quote_pattern = &quot;[&quot;]\w+[&quot;]&quot;
    
    quote = re.search(quote_pattern, raw_text)
    name = re.search(name_pattern, raw_text)

    
    if re.search(quote_pattern,raw_text):
        extracted_quotation = quote.group(0) 
    else:
        extracted_quotation=None

    if re.search(name_pattern,raw_text):
        extracted_person_name = name.group(0)
    else:
        extracted_person_name=None

    return extracted_person_name, extracted_quotation 
</code></pre>
<p><strong>problem is it returns <code>Null</code>. I'm assuming the patterns are incorrect can you tell me what's wrong with them?</strong></p>
",Named Entity Recognition (NER),text analysis extracting person name quotation create pattern need extract name quote given text homer simpson said okay go returned value extracted person name extracted person name appearing pattern explained extracted quotation extracted quoted text withot surrounding quotation mark important note pattern found return none value extracted person name extracted text could expect input text look similar following pattern person name said quoted text variation pattern colon punctuation mark optional might appear input sentence instead word said could also expect word answered replied got far problem return assuming pattern incorrect tell wrong
How to prevent NER model to overfit entity position with Spacy,"<p>I'm building a custom NER model to detect brands in product titles using a well sized dataset of 98k products with their corresponding title, the train contains around 84k records, validation split 10k and test split 3k.
The only problem with the dataset is that 89% of all product titles have the brand as their first words.</p>
<p>When training the NER model from scratch, it gives a good F1 score of 85% after just few epochs (batch size =32), however when testing the model I noticed the follwoign :</p>
<ul>
<li>The model is strongly biased toward predicting the first word of the title as a brand</li>
<li>The model is very good at detecting brands when they occur as first words, but is quite weak for titles having brands in their middle or end.</li>
</ul>
<p>I had the idea to solve this by resampling the dataset and removing some brands from some titles as their first word and put it at the end or the middle.</p>
<p>However, I would like to know if there is a technique in NLP that allow the model to not give a large importance to the entity position in the text ? I used dropout of 0.6 but with no success</p>
",Named Entity Recognition (NER),prevent ner model overfit entity position spacy building custom ner model detect brand product title using well sized dataset k product corresponding title train contains around k record validation split k test split k problem dataset product title brand first word training ner model scratch give good f score epoch batch size however testing model noticed follwoign model strongly biased toward predicting first word title brand model good detecting brand occur first word quite weak title brand middle end idea solve resampling dataset removing brand title first word put end middle however would like know technique nlp allow model give large importance entity position text used dropout success
Creating training data into a TRAIN.spacy file from manually tagged data with custom entity labels,"<p>I am using spacy to build a custom NER model. I would like to create the train.spacy file using my training data. I was manually creating a list of spacy.training.Example objects but I would like to use spacy v3 training using the config files.
So I have the training data in the form of words and their corresponding labels,for multiple texts. For eg:</p>
<pre><code>words= ['Foreign' 'broking' 'houses' 'raised' 'the' 'target' 'price' 'of'
     'Sun' 'Pharmaceutical' 'Industries,' 'Housing' 'Development' 'Finance'
     'Corporation,' 'SAIL' 'India,' 'Cholamandalam' 'Investment,' 'Sun' 'TV'
     'Network,' 'GAIL' 'India,' 'State' 'Bank' 'of' 'India' 'and' 'Eicher'
     'Motors,' 'post' 'September' 'quarter' 'earnings' 'announcement:']
entity_labels= ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ENTITY', 'I-ENTITY', 'L-ENTITY', 'B-ENTITY', 'I-ENTITY', 'I-ENTITY', 'L-ENTITY', 'B-ENTITY', 'L-ENTITY', 'B-ENTITY', 'L-ENTITY', 'B-ENTITY', 'I-ENTITY', 'L-ENTITY', 'B-ENTITY', 'L-ENTITY', 'B-ENTITY', 'I-ENTITY', 'I-ENTITY', 'L-ENTITY', 'O', 'B-ENTITY', 'L-ENTITY', 'O', 'O', 'O', 'O', 'O']
</code></pre>
<p>Since spacy requires training labels to be in this format</p>
<pre><code>TRAIN_DATA=[(text,{&quot;entities&quot;:[(start,end,label)]})]
</code></pre>
<p>Is there a quicker way to convert my data into this?The approach I am currently planning to implement is to manually go through the words and entities and get the start and end index. I would really appreciate a better way to do this though. Thanks!</p>
",Named Entity Recognition (NER),creating training data train spacy file manually tagged data custom entity label using spacy build custom ner model would like create train spacy file using training data wa manually creating list spacy training example object would like use spacy v training using config file training data form word corresponding label multiple text eg since spacy requires training label format quicker way convert data approach currently planning implement manually go word entity get start end index would really appreciate better way though thanks
Extraction from data from pandas json file,"<p>I have the following json data file which I have converted to pandas dataframe. The columns are as follows</p>
<pre><code>Index(['id', 'title', 'abstract', 'content', 'metadata'], dtype='object')
</code></pre>
<p>I am particularly interested in the column 'metadata' an element of the column looks like</p>
<pre><code>df_json.loc[78, 'metadata']
&quot;{'classification': {'name': 'Manufacturing, Transport &amp; Logistics'}, 'subClassification': {'name': 'Warehousing, Storage &amp; Distribution'}, 'area': {'name': 'Southern Suburbs &amp; Logan'}, 'location': {'name': 'Brisbane'}, 'suburb': {'name': 'Milton'}, 'workType': {'name': 'Casual/Vacation'}}&quot;
</code></pre>
<p>So I want to make  columns extracting the information out of 'metadata' columns for example location. I am not sure how to extract it and put it beside the same json file with added columns such as location etc.</p>
<pre><code>    id  title   abstract    content metadata    clean_content
0   38915469    Recruitment Consultant  We are looking for someone to focus purely on ...   &lt;HTML&gt;&lt;p&gt;Are you looking to join a thriving bu...   {'standout': {'bullet1': 'Join a Sector that i...   Are you looking to join a thriving business th...
1   38934839    Computers Salesperson - Coburg  Passionate about exceptional customer service?...   &lt;HTML&gt;&lt;p&gt;&amp;middot;&amp;nbsp;&amp;nbsp;Casual hours as r...   {'additionalSalaryText': 'Attractive Commissio...   middotnbspnbspCasual hours as required transit...
2   38946054    Senior Developer | SA   Readifarians are known for discovering the lat...   &lt;HTML&gt;&lt;p&gt;Readify helps organizations 

</code></pre>
",Named Entity Recognition (NER),extraction data panda json file following json data file converted panda dataframe column follows particularly interested column metadata element column look like want make column extracting information metadata column example location sure extract put beside json file added column location etc
BILOU Tagging scheme for multi-word entities in Spacy&#39;s NER,"<p>I am working on building a custom NER using spacy for recognizing new entities apart from spacy's NER. Now I have my training data to be tagged and added using spacy.Example. I am using the BILOU scheme. My doubt is that I have entities which have more than 3 words. For example:</p>
<pre><code>Housing Development Finance Corporation reported heavy losses in the past quarter.
</code></pre>
<p>I want to tag Housing Development Finance Corporation as a single Entity using the BILOU scheme. Something like</p>
<pre><code>'Housing'     B-Entity
'Development' I-Entity
'Finance'     I-Entity
'Corporation' L-Entity
</code></pre>
<p>Is this tagging correct?How will the model interpret the order within each entity?Any guidance would be much appreciated.</p>
",Named Entity Recognition (NER),bilou tagging scheme multi word entity spacy ner working building custom ner using spacy recognizing new entity apart spacy ner training data tagged added using spacy example using bilou scheme doubt entity word example want tag housing development finance corporation single entity using bilou scheme something like tagging correct model interpret order within entity guidance would much appreciated
spacy extract entity relationships parse dep tree,"<p>I am trying to extract entities and their relationships from the text. I am attempting to parse the dependency tree with entity extraction to perform that action. Something must be wrong with the recursive function logic that is preventing me from being able to parse that information, but I am not seeing what it is. I wanted to use the dependency tree + entities to form a (person,action,location) extraction.</p>
<p><strong>Desired output</strong>: Person: Lou Pinella, action:exited, Loc:Stadium</p>
<p><strong>Code Example:</strong></p>
<pre><code>import spacy
from spacy import displacy

nlp = spacy.load('en_core_web_lg')
doc = nlp(&quot;Lou Pinella exited from the far left side of the Stadium.&quot;)

def get_children_ent(head):
    if head.children:
        for child in head.children:
            if child.ent_type_ == &quot;LOC&quot;:
                print(f'Loc found: {child}') # it is hitting this branch
                return child
            else:
                return get_children_ent(child)
    else:
        return &quot;No Children&quot;

for ent in doc.ents:
    print(ent.text, ent.label_)
    if ent.label_ == &quot;PERSON&quot;:
        person = ent.text
        head = ent.root.head
        loc = get_children_ent(head)
        print(f'Person: {person}')
        print(f'Head: {head}')
        print(f'Person: {person}, action:{head}, Loc:{loc}')
    
   

displacy.render(doc, options={&quot;fine_grained&quot;: True})
</code></pre>
<p><strong>Print Statements - You can see it is hitting the location logic and printing that, but the return is still None in the recursive function.</strong></p>
<pre><code>Lou Pinella PERSON
Loc found: Stadium
Person: Lou Pinella
Head: exited
Person: Lou Pinella, action:exited, Loc:None
Stadium LOC
</code></pre>
<p><strong>EDITED</strong>: added return get_child_ent(child) to the else.</p>
",Named Entity Recognition (NER),spacy extract entity relationship parse dep tree trying extract entity relationship text attempting parse dependency tree entity extraction perform action something must wrong recursive function logic preventing able parse information seeing wanted use dependency tree entity form person action location extraction desired output person lou pinella action exited loc stadium code example print statement see hitting location logic printing return still none recursive function edited added return get child ent child else
spacy Entity Ruler pattern isn&#39;t working for ent_type,"<p>I am trying to get the entity ruler patterns to use a combination of lemma &amp; ent_type to generate a tag for the phrase &quot;landed (or land) in Baltimore(location)&quot;. It seems to be working with the Matcher, but not the entity ruler I created. I set the override ents to True, so not really sure why this isn't working. It is most likely a user error, I am just not sure what it is. Below is the code example. From the output, you can see that the pattern rule was added after NER and I have set the override ents to true. Any input or suggestions would be appreciated!</p>
<p>The matcher tags the entire phrase (landed in Baltimore), but the entity rule does not.</p>
<p><strong>Code Sample</strong></p>
<pre><code>import spacy
from spacy.matcher import Matcher

nlp = spacy.load('en_core_web_lg')

matcher = Matcher(nlp.vocab)

pattern = [{&quot;LEMMA&quot;:&quot;land&quot;},{}, {&quot;ENT_TYPE&quot;:&quot;GPE&quot;}]
patterns = [{&quot;label&quot;:&quot;FLYING&quot;,&quot;pattern&quot;:[{&quot;LEMMA&quot;:&quot;land&quot;},{}, {&quot;ENT_TYPE&quot;:&quot;GPE&quot;}]}]

matcher.add(&quot;Flying&quot;, [pattern])

rulerActions= EntityRuler(nlp, overwrite_ents=True)
rulerActions = nlp.add_pipe(&quot;entity_ruler&quot;,&quot;ruleActions&quot;).add_patterns(patterns)
# rulerActions.add_patterns(patterns)

print(f'spaCy Pipelines: {nlp.pipe_names}')

doc = nlp(&quot;The student landed in Baltimore for the holidays.&quot;)

matches = matcher(doc)
for match_id, start, end in matches:
    string_id = nlp.vocab.strings[match_id]  # Get string representation
    span = doc[start:end]  # The matched span
    print(f'{string_id}  -&gt;  {span.text}')
    
for ent in doc.ents:
    print(ent.text, ent.label_)
</code></pre>
<p><strong>Print Statements</strong></p>
<pre><code>spaCy Pipelines: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'ruleActions']
Flying  -&gt;  landed in Baltimore
Baltimore GPE
the holidays DATE
</code></pre>
",Named Entity Recognition (NER),spacy entity ruler pattern working ent type trying get entity ruler pattern use combination lemma ent type generate tag phrase landed land baltimore location seems working matcher entity ruler created set override ents true really sure working likely user error sure code example output see pattern rule wa added ner set override ents true input suggestion would appreciated matcher tag entire phrase landed baltimore entity rule doe code sample print statement
Spacy alignment differences when training with DocBin vs custom data reading and batching,"<p>I am just getting started with training Spacy named entity recognition models, and following the basic example described <a href=""https://spacy.io/usage/training#training-data"" rel=""nofollow noreferrer"">here</a>, where you create training examples by instantiating <code>Doc</code> objects and serializing those with <code>DocBin</code>.</p>
<p>My custom <code>preprocess.py</code> file looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>
if __name__ == '__main__':
    nlp = spacy.blank(&quot;en&quot;)
    counter = 0

    db = DocBin()

    with open(sys.argv[1], 'r') as fp:
        line = fp.readline()
        while line:

            record = MyRecord.build(json.loads(line))

            doc = record.to_spacy_doc(nlp=nlp)
            # internally, something like:
            # # char-level indices
            # ent = doc.char_span(0, 5, label='SOMETHING') 
            # doc.set_ents([ent])

            db.add(doc)

            counter += 1
            # hacky way to save 1000 docs in each DocBin
            if counter == 1000:
                db.to_disk(&quot;./train.spacy&quot;)
                db = DocBin()

            if counter == 2000:
                db.to_disk(&quot;./dev.spacy&quot;)
                break
            line = fp.readline()
</code></pre>
<p>Then run the training script with a command like this:</p>
<pre class=""lang-sh prettyprint-override""><code>python -m spacy train config.cfg --output ./output --paths.train train.spacy --paths.dev dev.spacy
</code></pre>
<p>This seems to work well enough. However, I then read that you can write custom data loader functions by writing and registering a generator that yields instances of <code>Example</code> in a process described <a href=""https://spacy.io/usage/training#custom-code-readers-batchers"" rel=""nofollow noreferrer"">here</a>. This interests me, as in theory you could read from larger-than-RAM files during the training loop.</p>
<p>I wrote such a generator myself in <code>functions.py</code>, that yields <code>Example</code> instances from an external data source (which happens to be custom JSONL from disk) where I have some known entity labels (with char-level indices):</p>
<pre class=""lang-py prettyprint-override""><code>@spacy.registry.readers(&quot;corpus_variants.v1&quot;)
def stream_data(source: str) -&gt; Callable[[Language], Iterator[Example]]:
    def generate_stream(nlp: Language):
        counter = 0
        with open(source, 'r') as fp:
            line = fp.readline()
            while line:

                record = MyRecord.build(json.loads(line))

                #doc = nlp(record.text)
                doc = nlp.make_doc(record.doc_with_annotations.text)

                entities = [
                    (start, end, label)     # char-level offets (not token-level)
                    for start, end, label, _
                    in record.get_entity_tuples()
                ]

                gold_dict = dict(
                    entities=entities
                )

                example = Example.from_dict(doc, gold_dict)
                yield example

                counter += 1
                # arbitrarily stop at 20 for debugging purposes, but ideally stream the very large file
                if counter &gt; 20:
                    break
                line = fp.readline()
    return generate_stream
</code></pre>
<p>I also modified <code>config.cfg</code> to contain the following:</p>
<pre><code>[corpora.dev]
@readers = &quot;corpus_variants.v1&quot;
source = &quot;dev.jsonl&quot;

[corpora.train]
@readers = &quot;corpus_variants.v1&quot;
source = &quot;train.jsonl&quot;
</code></pre>
<p>When I run the training command:</p>
<pre class=""lang-sh prettyprint-override""><code>python -m spacy train config.cfg --output ./output --code functions.py
</code></pre>
<p>I get many <code>UserWarning: [W030] Some entities could not be aligned in the text</code> warnings. I've read a few posts on the <a href=""https://github.com/explosion/spaCy/issues/5727"" rel=""nofollow noreferrer"">theory behind these warnings</a>, but I am curious as to why this behavior does not occur when I am saving <code>DocBin</code>s? Is the alignment actually different or do the warnings occur only when explicitly creating <code>Example</code> instances?</p>
<p>I am interested in getting the custom data loader working with this data, but also interested in alternative approaches that essentially allow me to stream arbitrary lines from a (larger-than-RAM) file as training examples.</p>
<p>Finally, what might also be relevant to answering this question is understanding the differences between training a fresh (NER) model from scratch vs. updating an existing one. If I understand Spacy pipelines correctly, there might be some alignment advantages to updating an existing model since the same tokenizer can be used when assembling training examples and during inference.</p>
",Named Entity Recognition (NER),spacy alignment difference training docbin v custom data reading batching getting started training spacy named entity recognition model following basic example described create training example instantiating object serializing custom file look like run training script command like seems work well enough however read write custom data loader function writing registering generator yield instance process described interest theory could read larger ram file training loop wrote generator yield instance external data source happens custom jsonl disk known entity label char level index also modified contain following run training command get many warning read post theory behind warning curious behavior doe occur saving alignment actually different warning occur explicitly creating instance interested getting custom data loader working data also interested alternative approach essentially allow stream arbitrary line larger ram file training example finally might also relevant answering question understanding difference training fresh ner model scratch v updating existing one understand spacy pipeline correctly might alignment advantage updating existing model since tokenizer used assembling training example inference
Gensim doc2vec&#39;s d2v.wv.most_similar() gives not relevant words with high similarity scores,"<p>I've got a dataset of job listings with about 150 000 records. I extracted skills from descriptions using NER using a dictionary of 30 000 skills. Every skill is represented as an unique identificator.</p>
<p>My data example:</p>
<pre><code>          job_title    job_id                                         skills
1  business manager         4               12 13 873 4811 482 2384 48 293 48
2    java developer        55    48 2838 291 37 484 192 92 485 17 23 299 23...
3    data scientist        21    383 48 587 475 2394 5716 293 585 1923 494 3
</code></pre>
<p>Then, I train a doc2vec model using these data where job titles (their ids to be precise) are used as tags and skills vectors as word vectors.</p>
<pre><code>def tagged_document(df):
    for index, row in df.iterrows():
        yield gensim.models.doc2vec.TaggedDocument(row['skills'].split(), [str(row['job_id'])])
        
        
data_for_training = list(tagged_document(data[['job_id', 'skills']]))

model_d2v = gensim.models.doc2vec.Doc2Vec(dm=0, dbow_words=1, vector_size=80, min_count=3, epochs=100, window=100000)

model_d2v.build_vocab(data_for_training)

model_d2v.train(data_for_training, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)
</code></pre>
<p>It works mostly okay, but I have issues with some job titles. I tried to collect more data from them, but I still have an unpredictable behavior with them.</p>
<p>For example, I have a job title &quot;Director Of Commercial Operations&quot; which is represented as 41 data records having from 11 to 96 skills (mean 32). When I get most similar words for it (skills in my case) I get the following:</p>
<pre><code>docvec = model_d2v.docvecs[id_]
model_d2v.wv.most_similar(positive=[docvec], topn=5)
</code></pre>
<pre><code>capacity utilization 0.5729076266288757
process optimization 0.5405482649803162
goal setting 0.5288119316101074
aeration 0.5124399662017822
supplier relationship management 0.5117508172988892
</code></pre>
<p>These are top 5 skills and 3 of them look relevant. However the top one doesn't look too valid together with &quot;aeration&quot;. The problem is that none of the job title records have these skills at all. It seems like a noise in the output, but why it gets one of the highest similarity scores (although generally not high)?
Does it mean that the model can't outline very specific skills for this kind of job titles?
Can the number of &quot;noisy&quot; skills be reduced? Sometimes I see much more relevant skills with lower similarity score, but it's often lower than 0.5.</p>
<p>One more example of correct behavior with similar amount of data:
BI Analyst, 29 records, number of skills from 4 to 48 (mean 21). The top skills look alright.</p>
<pre><code>business intelligence 0.6986587047576904
business intelligence development 0.6861011981964111
power bi 0.6589289903640747
tableau 0.6500121355056763
qlikview (data analytics software) 0.6307920217514038
business intelligence tools 0.6143202781677246
dimensional modeling 0.6032138466835022
exploratory data analysis 0.6005223989486694
marketing analytics 0.5737696886062622
data mining 0.5734485387802124
data quality 0.5729933977127075
data visualization 0.5691111087799072
microstrategy 0.5566076636314392
business analytics 0.5535123348236084
etl 0.5516749620437622
data modeling 0.5512707233428955
data profiling 0.5495884418487549
</code></pre>
",Named Entity Recognition (NER),gensim doc vec v wv similar give relevant word high similarity score got dataset job listing record extracted skill description using ner using dictionary skill every skill represented unique identificator data example train doc vec model using data job title id precise used tag skill vector word vector work mostly okay issue job title tried collect data still unpredictable behavior example job title director commercial operation represented data record skill mean get similar word skill case get following top skill look relevant however top one look valid together aeration problem none job title record skill seems like noise output get one highest similarity score although generally high doe mean model outline specific skill kind job title number noisy skill reduced sometimes see much relevant skill lower similarity score often lower one example correct behavior similar amount data bi analyst record number skill mean top skill look alright
Questions generation in question answering +NLP,"<p>I have a dataset (around 3K to 4K) excel files, each of them has around more or less 12K records which are combinations of FAQs, Email Conversations, comments from the blogs, chats etc.  </p>

<p>Best part is, it has 2 columns one for <em>Questions</em> and another for <em>Answers</em>.</p>

<p>One Sample record from an excel- (Note - <em>can't expose client data so creating only one record at my own to explain the scenario</em>).</p>

<p>eg.
Sample Question - <code>What are IIT colleges in India?</code></p>

<p>Sample Answer - <code>The Indian Institutes of Technology (IITs) are autonomous public institutes of higher education, located in India. They are governed by the Institutes of Technology Act, 1961 which has declared them as institutions of national importance and lays down their powers, duties, and framework for governance. The Institutes of Technology Act, 1961 lists twenty-three institutes.Each IIT is autonomous, linked to the others through a common council (IIT Council), which oversees their administration. The Minister of Human Resource Development is the ex officio Chairperson of the IIT Council. As of 2018, the total number of seats for undergraduate programs in all IITs is 11,279.</code></p>

<p>Client's requirement is- </p>

<blockquote>
  <p>Generate as many as simple questions from (above sample answer) paragraph along with their answers and append it in the same excel.</p>
</blockquote>

<p>(he will then process each excel further by feeding it to his some tool which generates chat-bot stories). </p>

<p>eg. </p>

<ul>
<li>Are IITs autonomous? (Answer: <code>Yes</code>)</li>
<li>What governs the IITs? (Answer: <code>The Institutes of Technology Act, 1961</code>)</li>
<li>In which country IITs are located? (Answer: <code>India</code>)</li>
<li>How many institutes does The Institutes of Technology Act, 1961 lists? (Answer:<code>twenty-three</code>)
etc.</li>
</ul>

<p>Answers generation I can do it using AllenAI, but not sure how to generate questions?
I tried a <a href=""https://github.com/KristiyanVachev/Question-Generation"" rel=""nofollow noreferrer"">repo</a> but it looks incomplete and need more efforts as I'm newbie to NLP or ML, so not getting how to do those changes.</p>

<p>Any help on generating questions in question answering?</p>

<p>Can I create any model on top of existing linguistic model such as spacy's models to generate entities and then generate the questions?    </p>
",Named Entity Recognition (NER),question generation question answering nlp dataset around k k excel file ha around le k record combination faq email conversation comment blog chat etc best part ha column one question another answer one sample record excel note expose client data creating one record explain scenario eg sample question sample answer client requirement generate many simple question sample answer paragraph along answer append excel process excel feeding tool generates chat bot story eg iits autonomous answer governs iits answer country iits located answer many institute doe institute technology act list answer etc answer generation using allenai sure generate question tried repo look incomplete need effort newbie nlp ml getting change help generating question question answering create model top existing linguistic model spacy model generate entity generate question
Merge name forms for same person found via NER with Spacy,"<p>I have a document of text and I want to find out which person the text is &quot;most about&quot;, my approximation of &quot;most about&quot; will be defined as the person mentioned most.</p>
<p>I use <a href=""https://spacy.io/"" rel=""nofollow noreferrer"">Spacy</a> Named Entity Recognition (NER) to get a list of all NER, then filter for type == 'PERSON'. I then get a frequency distribution for each person. This works ok, but Spacy treats each literal named entity token as different, i.e. &quot;John Smith&quot; is not &quot;John&quot; or &quot;Smith&quot;.</p>
<ol>
<li>Is there a better approach to find the person the text is &quot;most
about&quot;?</li>
<li>How can I get a merged/combined count where different forms
of the same names are combined?</li>
<li>I can come up with ways to do this myself but is there a specific name for this concept, so I can research further?</li>
<li>Does some NLP library (Spacy or NLTK for example) have the capability to do this?</li>
</ol>
<p>Here is my current code and output:</p>
<pre><code>import spacy
import nltk


# Load English tokenizer, tagger, parser and NER
nlp = spacy.load(&quot;en_core_web_sm&quot;)


def get_frequency(word_list):
    &quot;&quot;&quot;
    Get the frequency of each word
    &quot;&quot;&quot;
    freqs = nltk.FreqDist(word_list)
    freq_list = [(k, v) for k, v in freqs.items()]
    print(freq_list)
    print()


def get_ner_frequency(text):
    &quot;&quot;&quot;
    Extract Named Entity References (NER) of type &quot;PERSON&quot; from text

    &quot;&quot;&quot;
    # Parse with spacy
    sp_doc = nlp(text)

    # Get named people entities
    people = [ent.text for ent in sp_doc.ents if ent.label_ == &quot;PERSON&quot;]

    return get_frequency(people)


text = &quot;&quot;&quot;
Mr John Smith is a person.
Smith likes to ride horses.
Mike told John to be careful. J. Smith was very careful.
A man called Smithy told me to get a horse. John Smith fell off one day. John got hurt badly.
&quot;&quot;&quot;

get_ner_frequency(text)


</code></pre>
<p>Current Output:</p>
<pre><code>[('John Smith', 2), ('Smith', 1), ('Mike', 1), ('John', 2), ('J. Smith', 1), ('Smithy', 1)]

</code></pre>
<p>Ideally, I would like output that looks like this:</p>
<pre><code>[('John Smith', 7), ('Mike', 1)]
</code></pre>
<p>As with most other NLP problems, I realise that this can not be done perfectly in all cases, I am just after a decent approximation.</p>
",Named Entity Recognition (NER),merge name form person found via ner spacy document text want find person text approximation defined person mentioned use spacy named entity recognition ner get list ner filter type person get frequency distribution person work ok spacy treat literal named entity token different e john smith john smith better approach find person text get merged combined count different form name combined come way specific name concept research doe nlp library spacy nltk example capability current code output current output ideally would like output look like nlp problem realise done perfectly case decent approximation
Named Entity Recognition for NLTK in Python. Identifying the NE,"<p>I need to classify words into their parts of speech. Like a verb, a noun, an adverb etc..
I used the </p>

<pre><code>nltk.word_tokenize() #to identify word in a sentence 
nltk.pos_tag()       #to identify the parts of speech
nltk.ne_chunk()      #to identify Named entities. 
</code></pre>

<p>The out put of  this is a tree.
 Eg </p>

<pre><code>&gt;&gt;&gt; sentence = ""I am Jhon from America""
&gt;&gt;&gt; sent1 = nltk.word_tokenize(sentence )
&gt;&gt;&gt; sent2 = nltk.pos_tag(sent1)
&gt;&gt;&gt; sent3 =  nltk.ne_chunk(sent2, binary=True)
&gt;&gt;&gt; sent3
Tree('S', [('I', 'PRP'), ('am', 'VBP'), Tree('NE', [('Jhon', 'NNP')]), ('from', 'IN'), Tree('NE', [('America', 'NNP')])])
</code></pre>

<p>When accessing the element in this tree, i did it as follows:</p>

<pre><code>&gt;&gt;&gt; sent3[0]
('I', 'PRP')
&gt;&gt;&gt; sent3[0][0]
'I'
&gt;&gt;&gt; sent3[0][1]
'PRP'
</code></pre>

<p>But when accessing a Named Entity:</p>

<pre><code>&gt;&gt;&gt; sent3[2]
Tree('NE', [('Jhon', 'NNP')])
&gt;&gt;&gt; sent3[2][0]
('Jhon', 'NNP')
&gt;&gt;&gt; sent3[2][1]    
Traceback (most recent call last):
  File ""&lt;pyshell#121&gt;"", line 1, in &lt;module&gt;
    sent3[2][1]
  File ""C:\Python26\lib\site-packages\nltk\tree.py"", line 139, in __getitem__
    return list.__getitem__(self, index)
IndexError: list index out of range
</code></pre>

<p>I got the above error.</p>

<p>What i want is to get the output  as 'NE' similar to the previous 'PRP' so i cant identify which word is a Named Entity.
Is there any way of doing this with NLTK in python?? If so please post the command. Or is there a function in the tree library to do this? I need the node value 'NE'</p>
",Named Entity Recognition (NER),named entity recognition nltk python identifying ne need classify word part speech like verb noun adverb etc used put tree eg accessing element tree follows accessing named entity got error want get output ne similar previous prp cant identify word named entity way nltk python please post command function tree library need node value ne
How to use a pickle file for Spacy NER model training?,"<p>I have data for fine-tuning in this format:</p>
<pre><code>[[(('Kaweah', 'NNP'), 'O'),
  (('Delta', 'NNP'), 'O'),
  (('Mental', 'NNP'), 'O'),
  (('Health', 'NNP'), 'O'),
  (('Hospital', 'NNP'), 'O'),
  (('D/p', 'NNP'), 'O'),
  (('Aph', 'NNP'), 'O'),
  (('is', 'VBZ'), 'O'),
  (('located', 'VBN'), 'O'),
  (('at', 'IN'), 'O'),
  (('1100', 'CD'), 'B-GPE'),
  (('SO', 'NNP'), 'I-GPE'),
  (('.', '.'), 'I-GPE'),
  (('AKERS', 'NNP'), 'I-GPE'),
  (('STREET', 'NNP'), 'I-GPE')],
 [(('CHARLTON', 'NNP'), 'O'),
  (('MEMORIAL', 'NNP'), 'O'),
  (('HOSPITAL', 'NNP'), 'O'),
  (('is', 'VBZ'), 'O'),
  (('located', 'VBN'), 'O'),
  (('at', 'IN'), 'O'),
  (('2449', 'CD'), 'B-GPE'),
  (('THIRD', 'NNP'), 'I-GPE'),
  (('STREET', 'NNP'), 'I-GPE'),
  ((',', ','), 'I-GPE'),
  (('GA', 'NNP'), 'I-GPE')]]
</code></pre>
<p>But spacy training format is looking like this:</p>
<pre><code>TRAIN_DATA =[ (&quot;Pizza is a common fast food.&quot;, {&quot;entities&quot;: [(0, 5, &quot;FOOD&quot;)]}),
              (&quot;Pasta is an italian recipe&quot;, {&quot;entities&quot;: [(0, 5, &quot;FOOD&quot;)]}) ]
</code></pre>
<p>What should I do to convert my pickle file to the spacy format?</p>
",Named Entity Recognition (NER),use pickle file spacy ner model training data fine tuning format spacy training format looking like convert pickle file spacy format
Return all possible entity types from spaCy model?,"<p>Is there a method to extract all possible named entity types from a model in spaCy? You can manually figure it out by running on sample text, but I imagine there is a more programmatic way to do this?
For example:</p>
<pre><code>import spacy
model=spacy.load(&quot;en_core_web_sm&quot;)
model.*returns_entity_types*
</code></pre>
",Named Entity Recognition (NER),return possible entity type spacy model method extract possible named entity type model spacy manually figure running sample text imagine programmatic way example
how to extract two entities with conjunction in between those word using spacy python,"<p>My Rule based matching code is like,</p>

<pre><code>nlp = English()

matcher = Matcher(nlp.vocab)

pattern = [{""label"": ""ORG"", ""pattern"": ""XXX""}, {""ORTH"": ""(""}, {""label"": ""ORG"", ""pattern"": ""XXXXX""}, {""ORTH"": "")""}, 
           {""TEXT"": ""and""},
           {""label"": ""ORG"", ""pattern"": ""YY YY YY""}, {""ORTH"": ""(""}, {""label"": ""ORG"", ""pattern"": ""YYY YYY YYY""},{""ORTH"": "")""}]

matcher.add('organisation', None, pattern)

nlp.add_pipe(matcher)
</code></pre>

<p>My input will be like, This is between the XXXX(XXXX) and YY YY YY(YY YY YY).</p>

<p>I want output to be like XXXX(XXXX) and YY YY YY(YY YY YY).</p>

<p>I tried the above code but is not working for me. Is it possible to combine both Phrasematcher and Entity matcher in spacy python.</p>
",Named Entity Recognition (NER),extract two entity conjunction word using spacy python rule based matching code like input like xxxx xxxx yy yy yy yy yy yy want output like xxxx xxxx yy yy yy yy yy yy tried code working possible combine phrasematcher entity matcher spacy python
how I can find almost the same texts?,"<p>I have a column of job postings. I want to remove almost the same texts. They are very similar to each other but they have a small differences so drop_duplications function doesn't work. I tried the following code but I like to find a better way since this one is not very accurate.
In this method I am taking 150 letter of the text and search in al of other text and find similar ones and at the end I keep one and delete all other ones.</p>
<pre><code>bad=[]
ind=[]
ids_sub=[]
datatext=data.copy()
for i in range(datatext.shape[0]):
    datatext['originalText'].values[i]=str(datatext['originalText'].values[i]).replace('(', '').replace(')', '').replace('+', ' ').replace('?', ' ').replace('*', ' ').replace('|',' ').replace('-',' ').replace(':',' ').replace('@',' ')
    datatext['originalText'].values[i]=str(datatext['originalText'].values[i]).replace(&quot;\'&quot;, ' ')
    doc=datatext['originalText'].values[i][350:550]
    if len(doc)==200 and doc!=' ':
                u=datatext[datatext['originalText'].str.contains('|'.join([str(doc)]),na=False)]
                datatext=datatext[~datatext.index.isin(u[1:].index)]
                print(datatext.shape, u.shape,datatext['id'][i+1:i+2])
</code></pre>
<p>do u have a better method? How I can get almost similar text? any NLP method?</p>
<p>For example two following texts are almost the same:</p>
<p><code>&quot;Full job description\nCompany Description\n\n\nARE YOU FORGED FOR AIM RECYCLING?\n\n\nRecycles globally. Join us in our mission to recycle more than 3,000,000 tonnes of metals worldwide each year. At AIM Recycling, we recycle metals to their maximum capacity. For over 80 years, we've been working together to make a positive difference.\n\n\nBe part of our team to contribute to the growth of our company and to support our recycling activities in North America. It's simple: we do it right. We strive for excellence.\n\n\nJob Description\n\n\nUnder the supervision of Manager Asset Data Analytics, the incumbent will be responsible for supporting the deployment of data management and analytical strategies related to the asset fleet of our AIM sites (90 sites) in order to meet asset management excellence objectives.\n\n\nPosition Summary\n\n\nAnalyze data related to AIM's asset fleet to facilitate decision making related to the purchase, allocation, sale, and disposal of mobile and fixed asset in all our Recycling, Kenny and Feeder yard locations.\n\n\nMake recommendations to maximize the value of the equipment fleet and increase its operational availability.\n\n\nParticipate in the drafting of procedures or process mapping related to the mobile asset purchase, asset transfer, and asset disposal processes.\n\n\n\nAsset Maintenance Strategy:\n\nConduct useful life analysis assessment activities for the asset fleet considering asset life cycle data and failure mode patterns.\n\n\nAnalyze financial (acquisition cost, rebuild cost, NBV, depreciation) and maintenance data (cost, estimated life) related to the assets to proceed with data modeling that will include financial indicators such as PV, NPV scenarios.\n\n\nPerform a company wide asset reconciliation for high-level value assets so that the IT maintenance system (Maximo) and financial accounting system (365) reflect the field data (the physical asset inventory)\n\n\nConduct and report on internal and external maintenance cost analyses. Participate in the capital asset replacement plan.\n\n\n\nAsset monitoring_ Business Intelligence and Telematics\n\nLead and facilitate the implementation of specific programs related to the asset management for mobile fleet of heavy vehicles such as the development of daily inspection sheets via electronic tablet.\n\n\nCarry out various business intelligence and telematics projects related to assets in collaboration with the IT department. Promote the tools and ensure users support in the use of these reports and tools.\n\n\nAnalyze data and programs related to our assets in collaboration with the maintenance and finance departments.\n\n\nCollaborate in the validation and update of all site’s global assets/high-level assets register for the Recycling, Kenny and Feeder yard sites. This includes working with the Site Managers to verify/audit the assert inventory at our sites.\n\n\n\nAsset Life Cycle and Performance\n\nDevelop and maintain a set of key performance indicators and targets via business intelligence reports measuring the effectiveness of acquisition programs, asset utilization and maintenance costs for each asset category/site.\n\n\nSupport sites as needed in their asset disposal activities. Conduct asset disposition/sale activities at the appropriate time in the asset lifecycle, with the assistance of a web/auction tool.\n\n\nQualifications\n\n\n\nUniversity degree in engineering or computer science / or equivalent experience\nMaster's degree (an asset)\nSix sigma certification, process mapping (an asset)\nMinimum of 10 years of experience in data analysis/business intelligence\nKnowledge of data manipulation/programming in Advanced Excel, Macro, Pivot table, VLOOKUP\nPrevious experience in financial analysis (desirable)\nSkill in updating SharePoint content (asset) &amp; Cognos (strong asset);\nFamiliarity with Maximo, Visio, Outlook, Excel advanced and 365.\nKnowledge of heavy equipment vehicles, fleet management (an asset)\nHave a valid driver's license.\n\nThe position requires occasional travel\n\n\nAdditional Information\n\n\n\nWorking hours:\n 40 hours/week from Monday to Friday\n\n\nPermanent position Full time Stimulating, dynamic and pleasant work environment\n\n\n\nWhat we offer:\n\nGroup insurance; Group RRSP; Free coffee; Free parking; Subsidized dinner; Gym on site; Bonus plan; Social events (BBQ, Taffy on snow, raffles, etc.).\n\n\nThe American Iron &amp; Metal Company and its subsidiaries offer equal employment opportunities to all. The masculine is only used to lighten the text. Only those selected for an interview will be contacted.&quot;</code></p>
<p>'\n\n\n\n\n\n\n\n\n\nCompany Description\nARE YOU FORGED FOR AIM RECYCLING \nRecycles globally.\nJoin us in our mission to recycle more than 3,000,000 tons of metals worldwide each year. At AIM Recyclage, we recycle metals to the maximum of their capacity. For more than 80 years, we have been working together to make a positive difference. Be part of our team to help grow our company and support our recycling activities in North America.\nIt s simple  we do it well. We strive for excellence.\nJob Description\nUnder the supervision of Manager Asset Data Analytics, the incumbent will be responsible for supporting the deployment of data management and analytical strategies related to the asset fleet of our AIM sites 90 sites in order to meet asset management excellence objectives.\nAnalyze data related to AIM s asset fleet to facilitate decision making related to the purchase, allocation, sale, and disposal of mobile and fixed asset in all our Recycling, Kenny and Feeder yard locations;\nMake recommendations to maximize the value of the equipment fleet and increase its operational availability;\nParticipate in the drafting of procedures or process mapping related to the mobile asset purchase, asset transfer, and asset disposal processes.\nAsset Maintenance Strategy\nConduct useful life analysis assessment activities for the asset fleet considering asset life cycle data and failure mode patterns;\nAnalyze financial acquisition cost, rebuild cost, NBV, depreciation and maintenance data cost, estimated life related to the assets to proceed with data modeling that will include financial indicators such as PV, NPV scenarios;\nPerform a company wide asset reconciliation for high level value assets so that the IT maintenance system Maximo and financial accounting system 365 reflect the field data the physical asset inventory;\nConduct and report on internal and external maintenance cost analyses. Participate in the capital asset replacement plan\nAsset monitoring_ Business Intelligence and Telematics \nLead and facilitate the implementation of specific programs related to the asset management for mobile fleet of heavy vehicles such as the development of daily inspection sheets via electronic tablet;\nCarry out various business intelligence and telematics projects related to assets in collaboration with the IT department. Promote the tools and ensure users support in the use of these reports and tools;\nAnalyze data and programs related to our assets in collaboration with the maintenance and finance departments;\nCollaborate in the validation and update of all site’s global assets/high level assets register for the Recycling, Kenny and Feeder yard sites. This includes working with the Site Managers to verify/audit the assert inventory at our sites.\nAsset Life Cycle And Performance\nDevelop and maintain a set of key performance indicators and targets via business intelligence reports measuring the effectiveness of acquisition programs, asset utilization and maintenance costs for each asset category/site;\nSupport sites as needed in their asset disposal activities. Conduct asset disposition/sale activities at the appropriate time in the asset lifecycle, with the assistance of a web/auction tool;\nCollaborate in value asset evaluation activity related to our new site acquisitions when required and issue asset market value for sale. Coordinate the sale process between the various sites, finance, site managers and buyers;\nParticipate in management of new requests/emails for the Asset Management team as required;\nParticipate in the planning and organization related to asset fleet for all areas of AIM and various one time projects.\nQualifications\nUniversity degree in engineering or computer science / or equivalent experience;\nMaster s degree an asset;\nSix sigma certification, process mapping an asset;\nMinimum of 10 years of experience in data analysis/business intelligence;\nKnowledge of data manipulation/programming in Advanced Excel, Macro, Pivot table, VLOOKUP;\nPrevious experience in financial analysis desirable;\nSkill in updating SharePoint content asset &amp; Cognos strong asset;\nFamiliarity with Maximo, Visio, Outlook, Excel advanced and 365;\nKnowledge of heavy equipment vehicles, fleet management an asset;\nHave a valid driver s license.\nThe position requires occasional travel\nAdditional Information\nWorking hours40 hours/week From Monday to Friday;\nType of employment  permanent, full time.\nWhat We Offer\nGroup insurance after 3 months;\nGroup RRSP after 6 months;\nFree coffee and parking;\nSubsidized dinner;\nGym on site;\nSocial events BBQ, Snow Shoot, Draws, etc..\nThe American Iron &amp; Metals Company and its subsidiaries offer equal employment opportunities to all. The masculine is only used to lighten the text. Only those selected for interview will be contacted.\n      \n\n\n\n        Show more\n\n        \n\n\n\n\n\n        Show less\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n            Seniority level\n          \n\n\n\n            Associate\n          \n\n\n\n\n\n\n\n          Employment type\n        \n\n\n\n          Full time\n        \n\n\n\n\n\n\n\n            Job function\n          \n\n\n\n            Information Technology\n          \n\n\n\n\n\n\n\n            Industries\n          \n\n\n\n          Logistics and Supply Chain, Financial Services, and Accounting\n          \n\n\n\n\n\n'</p>
",Named Entity Recognition (NER),find almost text column job posting want remove almost text similar small difference drop duplication function work tried following code like find better way since one accurate method taking letter text search al text find similar one end keep one delete one u better method get almost similar text nlp method example two following text almost n n n n n n n n n ncompany description nare forged aim recycling nrecycles globally njoin u mission recycle ton metal worldwide year aim recyclage recycle metal maximum capacity year working together make positive difference part team help grow company support recycling activity north america nit simple well strive excellence njob description nunder supervision manager asset data analytics incumbent responsible supporting deployment data management analytical strategy related asset fleet aim site site order meet asset management excellence objective nanalyze data related aim asset fleet facilitate decision making related purchase allocation sale disposal mobile fixed asset recycling kenny feeder yard location nmake recommendation maximize value equipment fleet increase operational availability nparticipate drafting procedure process mapping related mobile asset purchase asset transfer asset disposal process nasset maintenance strategy nconduct useful life analysis assessment activity asset fleet considering asset life cycle data failure mode pattern nanalyze financial cost rebuild cost nbv depreciation maintenance data cost estimated life related asset proceed data modeling include financial indicator pv npv scenario nperform company wide asset reconciliation high level value asset maintenance system maximo financial accounting system reflect field data physical asset inventory nconduct report internal external maintenance cost analysis capital asset replacement plan nasset monitoring business intelligence telematics nlead facilitate implementation specific program related asset management mobile fleet heavy vehicle development daily inspection sheet via electronic tablet ncarry various business intelligence telematics project related asset department promote tool ensure user support use report tool nanalyze data program related asset maintenance finance department ncollaborate validation update site global asset high level asset register recycling kenny feeder yard site includes working site manager verify audit assert inventory site nasset life cycle performance ndevelop maintain set key performance indicator target via business intelligence report measuring effectiveness program asset utilization maintenance cost asset category site nsupport site needed asset disposal activity conduct asset disposition sale activity appropriate time asset lifecycle assistance web auction tool ncollaborate value asset evaluation activity related new site required issue asset market value sale coordinate sale process various site finance site manager buyer nparticipate management new request email asset management team required nparticipate planning organization related asset fleet area aim various one time project nqualifications nuniversity degree engineering computer science equivalent experience nmaster degree asset nsix sigma certification process mapping asset nminimum year experience data analysis business intelligence nknowledge data manipulation programming advanced excel macro pivot table vlookup nprevious experience financial analysis desirable nskill updating sharepoint content asset cognos strong asset nfamiliarity maximo visio outlook excel advanced nknowledge heavy equipment vehicle fleet management asset nhave valid driver license position requires occasional travel nadditional information nworking hour hour week monday friday ntype employment permanent full time nwhat offer ngroup insurance month ngroup rrsp month nfree coffee parking nsubsidized dinner ngym site nsocial event bbq snow shoot draw etc american iron metal company subsidiary offer equal employment opportunity masculine used lighten text selected interview contacted n n n n n show n n n n n n n n show le n n n n n n n n n n n n n n n n seniority level n n n n n associate n n n n n n n n n employment type n n n n n full time n n n n n n n n n job function n n n n n information technology n n n n n n n n n industry n n n n n logistics supply chain financial service accounting n n n n n n n
how to train spacy model which treats &amp; and &#39;and&#39; similar for accurate prediction,"<p>i've trained a spacy NER model which has text mapped to Company entity during training as -</p>
<pre><code>John &amp; Doe &amp; One pvt ltd -&gt; Company
</code></pre>
<p>Now in some cases I find, if giving sentence as below during prediction is being categorized as Others-</p>
<pre><code>John and Doe and One pvt ltd -&gt; Other
</code></pre>
<p>What should be done to overcome this problem where we have cases of &quot;&amp; == and&quot; and &quot;v == vs == versus&quot; etc cases to be understood by the model has same meaning ?</p>
",Named Entity Recognition (NER),train spacy model treat similar accurate prediction trained spacy ner model ha text mapped company entity training case find giving sentence prediction categorized others done overcome problem case v v versus etc case understood model ha meaning
SparkNLP PipelineModel which includes AnnotatorApproach in stages,"<p>In a SparkNLP's <code>PipelineModel</code> all the stages have to be of type <code>AnnotatorModel</code>. But what if one of those annotatormodels requires a certain column in the dataset as input and this input column is the output of an <code>AnnotatorApproach</code>?</p>
<p>For instance, I have a trained model for NER (as the last stage of the pipeline) which requires tokens and POS tags as two of the inputs. The tokens are also required by the POS tagger. But the Tokenizer is an <code>AnnotatorApproach</code> and I am not able to add this to the pipeline.</p>
<p>This is how the Tokenizer is instantiated (in Java):</p>
<pre><code>AnnotatorApproach&lt;TokenizerModel&gt; tokenizer = new Tokenizer();
</code></pre>
<p>This works:</p>
<pre><code>Pipeline pipeline = new Pipeline().setStages( new PipelineStage[]{tokenizer} );
</code></pre>
<p>But this doesn't work, because Tokenizer is not a Transformer:</p>
<pre><code>List&lt;Transformer&gt; list;
list.add(tokenizer);
PipelineModel pipelineModel = new PipelineModel(&quot;ID42&quot;, list);
</code></pre>
",Named Entity Recognition (NER),sparknlp pipelinemodel includes annotatorapproach stage sparknlp stage type one annotatormodels requires certain column dataset input input column output instance trained model ner last stage pipeline requires token po tag two input token also required po tagger tokenizer able add pipeline tokenizer instantiated java work work tokenizer transformer
Information extraction with Spacy with context awareness,"<p>I'm trying to extract project relevant information via web <strong>scraping</strong> using Python+ Spacy and then building a table of projects with few attributes , example phrases that are of interest for me are:</p>
<ul>
<li>The last is the <strong>300-MW Hardin Solar III Energy Center</strong> in <strong>Roundhead, Marion</strong>, and <strong>McDonald</strong> townships in <strong>Hardin County</strong>.</li>
<li>In <strong>July</strong>, OPSB approved the <strong>577-MW Fox Squirrel Solar Farm</strong> in <strong>Madison County</strong>.</li>
<li><strong>San Diego agency</strong> seeking developers for <strong>pumped storage energy project</strong>.</li>
<li>The <strong>$52.5m</strong> royalty revenue-based royalty investment includes the <strong>151MW Old Settler wind farm</strong></li>
</ul>
<p>Here I have highlighted different types of information that I'm interested in , I need to end up with a table with following columns :
{project name} , {Location} ,{company}, {Capacity} , {start date} , {end Date} , {$investment} , {fuelType}</p>
<p>I'm using Spacy , but looking at the dependency tree I couldn't find any common rule , so if I use matchers I will end up with 10's of them , and they will not capture every possible information in text, is there a systematic approach that can help me achieve even a part of this task (EX: Extract capacity and assign it to the proper project name)</p>
",Named Entity Recognition (NER),information extraction spacy context awareness trying extract project relevant information via web scraping using python spacy building table project attribute example phrase interest last mw hardin solar iii energy center roundhead marion mcdonald township hardin county july opsb approved mw fox squirrel solar farm madison county san diego agency seeking developer pumped storage energy project royalty revenue based royalty investment includes mw old settler wind farm highlighted different type information interested need end table following column project name location company capacity start date end date investment fueltype using spacy looking dependency tree find common rule use matcher end capture every possible information text systematic approach help achieve even part task ex extract capacity assign proper project name
Deleting and updating a string and entity index in a text document for NER training data,"<p>I am trying to create a training dataset for NER recognition. For that, I have huge amounts of data that need to be tagged and remove the unnecessary sentences. On removing the unnecessary sentence the index potion must be updated. Last day I saw some incredible code segments from some users about this which I cannot find now. Adapting their code segment I can brief my issue</p>
<p>Let's take a training sample data :</p>
<pre><code>data = [{&quot;content&quot;:'''Hello we are hans and john. I enjoy playing Football.
I love eating grapes. Hanaan is great.''',&quot;annotations&quot;:[{&quot;id&quot;:1,&quot;start&quot;:13,&quot;end&quot;:17,&quot;tag&quot;:&quot;name&quot;},
                                {&quot;id&quot;:2,&quot;start&quot;:22,&quot;end&quot;:26,&quot;tag&quot;:&quot;name&quot;},
                                {&quot;id&quot;:3,&quot;start&quot;:68,&quot;end&quot;:74,&quot;tag&quot;:&quot;fruit&quot;},
                                {&quot;id&quot;:4,&quot;start&quot;:76,&quot;end&quot;:82,&quot;tag&quot;:&quot;name&quot;}]}]
</code></pre>
<p>This can be visualized using the following spacy display code</p>
<pre><code>import json
import spacy
from spacy import displacy

data = [{&quot;content&quot;:'''Hello we are hans and john. I enjoy playing Football.
I love eating grapes. Hanaan is great.''',&quot;annotations&quot;:[{&quot;id&quot;:1,&quot;start&quot;:13,&quot;end&quot;:17,&quot;tag&quot;:&quot;name&quot;},
                                {&quot;id&quot;:2,&quot;start&quot;:22,&quot;end&quot;:26,&quot;tag&quot;:&quot;name&quot;},
                                {&quot;id&quot;:3,&quot;start&quot;:68,&quot;end&quot;:74,&quot;tag&quot;:&quot;fruit&quot;},
                                {&quot;id&quot;:4,&quot;start&quot;:76,&quot;end&quot;:82,&quot;tag&quot;:&quot;name&quot;}]}]

annot_tags = data[data_index][&quot;annotations&quot;]
entities = []
for j in annot_tags:
    start = j[&quot;start&quot;]
    end = j[&quot;end&quot;]
    tag = j[&quot;tag&quot;]
    entitie = (start,end,tag)
    entities.append(entitie)
data_gen = (data[data_index][&quot;content&quot;],{&quot;entities&quot;:entities})
data_one = []
data_one.append(data_gen)

nlp = spacy.blank('en')
raw_text = data_one[0][0]
doc = nlp.make_doc(raw_text)
spans = data_one[0][1][&quot;entities&quot;]
ents = []
for span_start, span_end, label in spans:
    ent = doc.char_span(span_start, span_end, label=label)
    if ent is None:
        continue

    ents.append(ent)

doc.ents = ents
displacy.render(doc, style=&quot;ent&quot;, jupyter=True)
</code></pre>
<p>The output will be</p>
<p><a href=""https://i.sstatic.net/t5701.png"" rel=""nofollow noreferrer"">Output 1</a></p>
<p>Now I want to remove the sentence which is not tagged and update the index values. So the required output is like</p>
<p><a href=""https://i.sstatic.net/syBsw.png"" rel=""nofollow noreferrer"">Required Output</a></p>
<p>Also data must be in the following format. Untagged sentence is removed and index values must be updated so that I can get the output like above.</p>
<p><strong>Required output data</strong></p>
<pre><code>[{&quot;content&quot;:'''Hello we are hans and john.
I love eating grapes. Hanaan is great.''',&quot;annotations&quot;:[{&quot;id&quot;:1,&quot;start&quot;:13,&quot;end&quot;:17,&quot;tag&quot;:&quot;name&quot;},
                                {&quot;id&quot;:2,&quot;start&quot;:22,&quot;end&quot;:26,&quot;tag&quot;:&quot;name&quot;},
                                {&quot;id&quot;:3,&quot;start&quot;:42,&quot;end&quot;:48,&quot;tag&quot;:&quot;fruit&quot;},
                                {&quot;id&quot;:4,&quot;start&quot;:50,&quot;end&quot;:56,&quot;tag&quot;:&quot;name&quot;}]}]
</code></pre>
<p>I was following a post last day and got a near working code.</p>
<p><strong>Code</strong></p>
<pre><code>import re

data = [{&quot;content&quot;:'''Hello we are hans and john. I enjoy playing Football.
I love eating grapes. Hanaan is great.''',&quot;annotations&quot;:[{&quot;id&quot;:1,&quot;start&quot;:13,&quot;end&quot;:17,&quot;tag&quot;:&quot;name&quot;},
                                {&quot;id&quot;:2,&quot;start&quot;:22,&quot;end&quot;:26,&quot;tag&quot;:&quot;name&quot;},
                                {&quot;id&quot;:3,&quot;start&quot;:68,&quot;end&quot;:74,&quot;tag&quot;:&quot;fruit&quot;},
                                {&quot;id&quot;:4,&quot;start&quot;:76,&quot;end&quot;:82,&quot;tag&quot;:&quot;name&quot;}]}]
         
         
         
for idx, each in enumerate(data[0]['annotations']):
    start = each['start']
    end = each['end']
    word = data[0]['content'][start:end]
    data[0]['annotations'][idx]['word'] = word
    
sentences = [ {'sentence':x.strip() + '.','checked':False} for x in data[0]['content'].split('.')]

new_data = [{'content':'', 'annotations':[]}]
for idx, each in enumerate(data[0]['annotations']):
    for idx_alpha, sentence in enumerate(sentences):
        if sentence['checked'] == True:
            continue
        temp = each.copy()
        check_word = temp['word']
        if check_word in sentence['sentence']:
            start_idx = re.search(r'\b({})\b'.format(check_word), sentence['sentence']).start()
            end_idx = start_idx + len(check_word)
            
            current_len = len(new_data[0]['content'])
            
            new_data[0]['content'] += sentence['sentence'] + ' '
            temp.update({'start':start_idx + current_len, 'end':end_idx + current_len})
            new_data[0]['annotations'].append(temp)
            
            sentences[idx_alpha]['checked'] = True
            break
print(new_data)
</code></pre>
<p><strong>Output</strong></p>
<pre><code>[{'content': 'Hello we are hans and john. I love eating grapes. Hanaan is great. ',
  'annotations': [{'id': 1,
    'start': 13,
    'end': 17,
    'tag': 'name',
    'word': 'hans'},
   {'id': 3, 'start': 42, 'end': 48, 'tag': 'fruit', 'word': 'grapes'},
   {'id': 4, 'start': 50, 'end': 56, 'tag': 'name', 'word': 'Hanaan'}]}]
</code></pre>
<p>Here the name john is lost. If more than one tag is present, I can't lose that too</p>
",Named Entity Recognition (NER),deleting updating string entity index text document ner training data trying create training dataset ner recognition huge amount data need tagged remove unnecessary sentence removing unnecessary sentence index potion must updated last day saw incredible code segment user find adapting code segment brief issue let take training sample data visualized using following spacy display code output output want remove sentence tagged update index value required output like required output also data must following format untagged sentence removed index value must updated get output like required output data wa following post last day got near working code code output name john lost one tag present lose
processing text with nlp.pipe taking hours,"<p>I have 300.000 news articles in my dataset and I'm using en_core_web_sm to do POS tagging, parsing, ner extraction. However, this is taking hours and hours and seems to never be done.</p>
<p>The code is working, but very slow. When I take a sample of my dataset 650 articles it takes 37seconds and 6500 articles takes 6min. However I really need my full dataset of 300.000 articles to be completed in a reasonable amount of time...</p>
<pre><code>texts = df[&quot;content&quot;]
for doc in nlp.pipe(texts, n_process=2, batch_size=100, disable=[&quot;senter&quot;,&quot;attribute_ruler&quot; &quot;lemmatizer&quot;]):
df[&quot;spacy_sm&quot;] = texts
</code></pre>
<p>Is there a way to speed this up significantly, or am I missing something here?</p>
",Named Entity Recognition (NER),processing text nlp pipe taking hour news article dataset using en core web sm po tagging parsing ner extraction however taking hour hour seems never done code working slow take sample dataset article take second article take min however really need full dataset article completed reasonable amount time way speed significantly missing something
Label custom NER in pandas dataframe,"<p>I have a dataframe with 3 columns: <code>'text', 'in', 'tar'</code> of <code>type(str, list, list)</code> respectively.</p>
<pre><code>                   text                                       in       tar
0  This is an example text that I use in order to  ...       [2]       [6]
1  Discussion: We are examining the possibility of ...       [3]     [6, 7]
</code></pre>
<p><code>in</code> and <code>tar</code> represent specific entities that I want to tag into the text, and they return the position of each found entity term in the text.</p>
<p>For example, at the 2nd row of the dataframe where <code>in = [3]</code>, I want to take the 3rd word from <code>text</code> column (i.e.: <em>&quot;are&quot;</em>) and label it as <code>&lt;IN&gt;are&lt;/IN&gt;</code>.</p>
<p>Similarly, for the same row, since <code>tar = [6,7]</code>, I also want to take the 6th and 7th word from <code>text</code> column (i.e. <em>&quot;possibility&quot;</em>, <em>&quot;of&quot;</em>) and label them as  <code>&lt;TAR&gt;possibility&lt;/TAR&gt;</code>, <code>&lt;TAR&gt;of&lt;/TAR&gt;</code>.</p>
<p>Can someone help me how to do this?</p>
",Named Entity Recognition (NER),label custom ner panda dataframe dataframe column respectively represent specific entity want tag text return position found entity term text example nd row dataframe want take rd word column e label similarly row since also want take th th word column e possibility label someone help
(Re-)tokenize entities in a dataframe without losing the associated label,"<p><strong>Context</strong></p>
<p>I'm looking for a way to tokenize entities (in a CONLL) from a dataframe by following the rule: [&quot;d'Angers&quot;] =&gt; [&quot;d '&quot;, &quot;Angers&quot;] / [&quot;l'impératrice&quot; ] =&gt; [&quot;l'&quot;, &quot;impératrice&quot;] (split entities on apostrophes).</p>
<p><strong>Code</strong></p>
<p>My initial dataframe looks like this (correponding to CONLL file) :</p>
<pre><code> Sentence  Mention  Tag
9   3   Vincennes   B-LOCATION
10  3   .   O
12  4   Confirmation    O
13  4   des O
14  4   privilèges  O
15  4   de  O
16  4   la  O
17  4   ville   O
18  4   d'Aire  O
19  4   1   O
20  4   ,   O
21  4   au  O
22  4   bailliage   B-ORGANISATION
23  4   d'Amiens    I-ORGANISATION
</code></pre>
<p>First create a Retokenization class :</p>
<pre class=""lang-py prettyprint-override""><code>class Retokenization:
    def __init__(self, mention) -&gt; None:
        self.mention = mention
        self.tokens = self.split_off_apostrophes()
    
    def split_off_apostrophes(self):
        if &quot;'&quot; in self.mention and len(self.mention) &gt; 1:
            if not re.search(r&quot;[\-]&quot;, str(self.mention)) and not re.search(r&quot;[\w]{2,}['][\w]+&quot;, str(self.mention)):
                inter = re.split(r&quot;(\w')&quot;, self.mention)
                tokens = [tok for tok in inter if tok != '']
                return tokens
            else:
                return self.mention.split()
        else:
            return self.mention.split()

</code></pre>
<p>then apply this retokenize class to dataframe :</p>
<pre class=""lang-py prettyprint-override""><code>mentions = df['Mention'].apply(lambda mention : Retokenization(mention).tokens).fillna(value='_')
</code></pre>
<p>OUT :</p>
<pre><code>9           [Vincennes]
10                  [.]
12       [Confirmation]
13                [des]
14         [privilèges]
15                 [de]
16                 [la]
17              [ville]
18           [d', Aire]
19                  [1]
20                  [,]
21                 [au]
22          [bailliage]
23         [d', Amiens]
</code></pre>
<p>Then I recreate the dataframe with my tokenized mentions and the associated labels :</p>
<pre class=""lang-py prettyprint-override""><code>from itertools import chain

df_retokenized = pd.DataFrame({
    
    'Sentence' : df['Sentence'].values.repeat(mentions.str.len()),
    'Mention' : list(chain.from_iterable(mentions.tolist())),
    'Tag' : df['Tag'].values.repeat(mentions.str.len())

    })

</code></pre>
<p>and I add a boolean mask for some missing labels during recreate dataframe :</p>
<pre class=""lang-py prettyprint-override""><code>m1 = df['Tag'].eq('O')
m2 = m1 &amp; df['Tag'].shift(-1).str.startswith('I-')
add_tag = df['Tag'].shift(-1).str.replace(r&quot;\w[-](\w+)&quot;, r&quot;\1&quot;, regex=True)

df['Tag'] = np.select([m2], ['B-' + add_tag], df['Tag'])

</code></pre>
<p>OUT :</p>
<pre><code>Mention Tag
JJ O
/ O
/ O
226 O
/ O
A O

465 O

Vincennes B-LOCATION
. O

Confirmation O
des O
privilèges O
de O
la O
ville O
d' O
Aire O
1 O
, O
au O
bailliage B-ORGANISATION
d' I-ORGANISATION
Amiens I-ORGANISATION
</code></pre>
<p><strong>Problem</strong></p>
<p>The code works but not fully operational. In fact, when I look further in the output file I notice that some retokenized entities have lost their Tag (IOB) for example:</p>
<p>Example # 1</p>
<p>Before :</p>
<pre><code>Projet O
de O
&quot; O
tour O
de O
l'impératrice B-TITLE
Eugénie B-PERSON
</code></pre>
<p>After :</p>
<pre><code>Projet O
de O
&quot; O
tour O
de O
l' O
impératrice 
Eugénie B-PERSON
</code></pre>
<p>Expected output :</p>
<pre><code>Projet O
de O
&quot; O
tour O
de O
l' O
impératrice B-TITLE
Eugénie B-PERSON
</code></pre>
<p>Example #2</p>
<p>Before :</p>
<pre><code>à O
l'ONU B-ORGANISATION
, O
durée O
</code></pre>
<p>After :</p>
<pre><code>à O
l' O
ONU O
, O
durée O
</code></pre>
<p>Expected output :</p>
<pre><code>à O
l' O
ONU B-ORGANISATION
, O
durée O
</code></pre>
<p>I have other examples (but I don't want to overload the question)</p>
<p><strong>Question</strong></p>
<p><em>Is there a way to apply my (re-)tokenization to the dataframe without losing NER tags on re-tokenized entities ?</em></p>
<p>Excuse me for the length (I have not found other way to summarize correctly), if anyone has any leads, thank you in advance.</p>
",Named Entity Recognition (NER),tokenize entity dataframe without losing associated label context looking way tokenize entity conll dataframe following rule anger anger l imp ratrice l imp ratrice split entity apostrophe code initial dataframe look like correponding conll file first create retokenization class apply retokenize class dataframe recreate dataframe tokenized mention associated label add boolean mask missing label recreate dataframe problem code work fully operational fact look output file notice retokenized entity lost tag iob example example expected output example expected output example want overload question question way apply tokenization dataframe without losing ner tag tokenized entity excuse length found way summarize correctly anyone ha lead thank advance
"NLP , Specific Text Extraction","<p>I have to train a model which can identify the country from a random text. I have the country list.</p>
<p>Now I am struggling to find a solution that can train the model on the country list and when I provide a random text to that model as an input, it identifies the country name as an output.</p>
<p>eg:-</p>
<p>&quot;I live in India&quot; will give &quot;India&quot;
&quot;London is the capital of United Kingdom&quot; will give &quot;United Kingdom&quot;</p>
<p>Thanks in advance.</p>
",Named Entity Recognition (NER),nlp specific text extraction train model identify country random text country list struggling find solution train model country list provide random text model input identifies country name output eg live india give india london capital united kingdom give united kingdom thanks advance
Confidence Score of Predicted NER entities using Spacy,"<p>I am trying to predict entities using a custom trained NER model using spacy. I read <a href=""https://github.com/explosion/spaCy/pull/8855"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/pull/8855</a> that confidence scores of each entity can be obtained using spancat. But I have a little confusion regarding to make that work. According to my understanding, we have to train a pipeline using spancat component. So while training, within the config file there is a segment,</p>
<pre><code>[nlp]
lang = &quot;en&quot;
pipeline = [&quot;tok2vec&quot;,&quot;ner&quot;]
batch_size = 1000
</code></pre>
<p>Should we have to change this to</p>
<pre><code>[nlp]
lang = &quot;en&quot;
pipeline = [&quot;tok2vec&quot;,&quot;ner&quot;,&quot;spancat&quot;]
batch_size = 1000
</code></pre>
<p>for the spancat to work.</p>
<p>Then after training, while predicting the entities from unknown text, should we have to use</p>
<pre><code>doc = nlp(data_to_be_predicted)
spans = doc.spans[&quot;spancat&quot;] # SpanGroup
print(spans.attrs[&quot;scores&quot;]) # list of numbers, span length as SpanGroup
</code></pre>
<p>to get the confidence scores.</p>
<p>I am using spacy 3.1.3. I believe according to the documentation, this feature is rolled out by now.</p>
",Named Entity Recognition (NER),confidence score predicted ner entity using spacy trying predict entity using custom trained ner model using spacy read confidence score entity obtained using spancat little confusion regarding make work according understanding train pipeline using spancat component training within config file segment change spancat work training predicting entity unknown text use get confidence score using spacy believe according documentation feature rolled
Deleting the sentence and updating the index,"<p>I am working on a data format like this.</p>
<pre><code>data = [{&quot;content&quot;:'''Hello I am Aniyya. I enjoy playing Football.
I love eating grapes''',&quot;annotations&quot;:[{&quot;id&quot;:1,&quot;start&quot;:11,&quot;end&quot;:17,&quot;tag&quot;:&quot;name&quot;},
                                {&quot;id&quot;:2,&quot;start&quot;:59,&quot;end&quot;:65,&quot;tag&quot;:&quot;fruit&quot;}]}]
</code></pre>
<p><a href=""https://i.sstatic.net/4axJi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4axJi.png"" alt=""enter image description here"" /></a></p>
<p>and i did want a data format like this. The sentences which do not have any entities has to be removed. And update the start and end of other entities according to the removed sentence.</p>
<pre><code>result_data = data = [{&quot;content&quot;:'''Hello I am Aniyya. I love eating grapes''',&quot;annotations&quot;:[{&quot;id&quot;:1,&quot;start&quot;:11,&quot;end&quot;:17,&quot;tag&quot;:&quot;name&quot;},
                                {&quot;id&quot;:2,&quot;start&quot;:33,&quot;end&quot;:39,&quot;tag&quot;:&quot;fruit&quot;}]}]
</code></pre>
<p><a href=""https://i.sstatic.net/RkyDD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RkyDD.png"" alt=""enter image description here"" /></a></p>
<p>I am not getting any particular logic for this. I know this is like asking to code for me, but if any of have time to help me with this i appreciate a lot. i kind of stuck at this. There is a similar type question from me asked previously but it also didnt worked out at me. So thought of describe more details. Solution for this will be helpful for all those who are preparing the dataset related to NLP tasks. Thanks in advance.</p>
<p>Visualization is done with spacy displacy, Code is in <a href=""https://stackoverflow.com/questions/69645737/visualizing-ner-training-data-and-entity-using-displacy"">visualizing NER training data and entity using displacy</a></p>
",Named Entity Recognition (NER),deleting sentence updating index working data format like want data format like sentence entity ha removed update start end entity according removed sentence getting particular logic know like asking code time help appreciate lot kind stuck similar type question asked previously also didnt worked thought describe detail solution helpful preparing dataset related nlp task thanks advance visualization done spacy displacy code href ner training data entity using displacy
Is there a way to manually add tags using dictionaries for named entity recognision?,"<p>I am a beginner and I want to know if there is a way to/How can I manually add tags using dictionaries for named entity recognition. I am using spacy for Named Entity Recognition and when I used the below code  :</p>
<pre><code>import spacy 
from spacy import display
raw_text='''To determine the adulticidal and repellent activities of different solvent leafextracts of Rhinacanthus nasutus against Aedes aegypti and Culex quinquefasciatus.'''  
NER = spacy.load(&quot;en_core_web_sm&quot;) 
text1= NER(raw_text) 
for word in text1.ents:
    print(word.text,word.label_)
</code></pre>
<p>it gave this as output:
Rhinacanthus NORP
Aedes ORG</p>
<p>but i want the word Aedes to be tagged as a person not ORG. How can i achieve this?</p>
",Named Entity Recognition (NER),way manually add tag using dictionary named entity recognision beginner want know way manually add tag using dictionary named entity recognition using spacy named entity recognition used code gave output rhinacanthus norp aedes org want word aedes tagged person org achieve
How to use text processing or data standardisation methods to bucket fields that mean the same thing and map it to the key of below dictionary,"<p>I am trying to find a quick and effective way of mapping the values of the below dictionary to the key of dictionary in a given textbody.</p>
<p>Maybe something along the lines of String matching, fuzzy string matching, NER/Topic modelling maybe, other NLP or general clever text processing techniques that can be used to find related words in the text and then map it to the key of the dictionary. Thanks a ton!</p>
<p>Example: If I find some other form of &quot;Fuel consumption&quot; like &quot;fuel_consumption&quot; or &quot;fuel-consumption&quot; or maybe the order of words could be different but I should be able to find related keyword from the dictionary.</p>
<p>So far I have tried a combination of Regex and Fuzzy String Matching to get work done. But I am looking for a better solution. That will also be able to tell keywords that mean the same thing. Maybe pre trained word embeddings for cars, sports cars, F1/related races and drivers and related text data would help to encode the meaning of these words somehow and will bucket and map two/more related words to the same key of the above dictionary.</p>
<p>The data can come from different websites so different websites display the same info of car specifications/engine using different names/formats. This program should be able to bucket and map these related fields that mean the same thing coming from different websites. Any Data Standardisation techniques for words/text data could also work probabaly.</p>
<p>I am looking for you folks help to point me in the right direction to achieve better results at this task! Thanks a ton!</p>
<p><strong>P.S. Please tell me if anyone knows any resources for pre-trained word embeddings for cars, sports cars, F1/related races and drivers and related text data.</strong></p>
<pre><code>&quot;fuel_consumption_and_emissions&quot; : {
    &quot;Fuel consumption&quot;: [&quot;Fuel consumption***&quot;],
    &quot;CO2 Emissions&quot;: [&quot;CO2 Emissions ***&quot;, &quot;CO2 emissions***&quot;],
    &quot;Emissions&quot;: [&quot;Emissions***&quot;]
},

# Co2 emissions
&quot;co2_emissions&quot; : {
    &quot;Combined&quot;: [&quot;Combined&quot;, &quot;Combined* (ECE+EUDC) (g/km)&quot;, &quot;Combined ECE*&quot;, &quot;Combined (ECE+EUDC)*&quot;,'Combined Consumption:'],
    &quot;Low&quot;: [&quot;Low&quot;,'Low Consumption:'],
    &quot;Mid&quot;: [&quot;Mid&quot;,'Medium Consumption:'],
    &quot;High&quot;: [&quot;High&quot;,'High Consumption:'],
    &quot;Extra High&quot;: [&quot;Extra High&quot;, &quot;Extra high&quot;, &quot;Extra  High&quot;,'Very High Consumption:'],
    &quot;Note&quot;: [&quot;Note&quot;],
    'Fuel Efficiency: Weighted Combined (WLTP)': ['Fuel Efficiency: Weighted Combined (WLTP)'],
    'CO2 Emissions: Weighted Combined (WLTP)': ['CO2 Emissions: Weighted Combined (WLTP)'],
    'CO2 Emissions EU WLTP': ['CO2 Emissions EU WLTP', 'CO2 Emmissions EU WLTP'],
    'CO2 Emissions EU NEDC': ['CO2 Emissions EU NEDC'],
    'Co2 Emissions:' : ['Co2 Emissions:'],
    'Power Consumption:' : ['Power Consumption:'],
    'Battery Range:' : ['Battery Range:']
    
}
</code></pre>
",Named Entity Recognition (NER),use text processing data standardisation method bucket field mean thing map key dictionary trying find quick effective way mapping value dictionary key dictionary given textbody maybe something along line string matching fuzzy string matching ner topic modelling maybe nlp general clever text processing technique used find related word text map key dictionary thanks ton example find form fuel consumption like fuel consumption fuel consumption maybe order word could different able find related keyword dictionary far tried combination regex fuzzy string matching get work done looking better solution also able tell keywords mean thing maybe pre trained word embeddings car sport car f related race driver related text data would help encode meaning word somehow bucket map two related word key dictionary data come different website different website display info car specification engine using different name format program able bucket map related field mean thing coming different website data standardisation technique word text data could also work probabaly looking folk help point right direction achieve better result task thanks ton p please tell anyone know resource pre trained word embeddings car sport car f related race driver related text data
Extracting user interests from social profiles,"<p>This is my first time dabbling in NLP so please excuse my ignorance.  I'm looking for a method to extract interests/likes/hobbies from users' social profiles. Here is an example where all the interests/likes/hobbies are in bold:</p>

<blockquote>
  <p>""I consider myself a pretty diverse character... I'm a professional
  <strong>wrestler</strong>, but I'd take a bullet for <strong>Wall•E</strong>. I train like a one-man genocide machine in the <strong>gym</strong>, but I cried at
  ""<strong>Armageddon</strong>."" I'll head bang to <strong>AC/DC</strong>, and I'm seriously
  considering getting a <strong>Legend of Zelda</strong> tattoo. I'm 420-friendly. I
  like to <strong>party</strong> it up with the frat crowd one night, hang out with
  my <strong>Burning Man</strong> friends the next, play <strong>Halo</strong> and <strong>World of
  Warcraft</strong> the next, and jam with friends that aren't any younger than
  40 the next. My youngest friend is 16, my oldest friend is 66. I'll
  sing <strong>karaoke</strong> at the bars, and I'm my friends' collective
  psychiatrist/shoulder.""</p>
</blockquote>

<p>The profiles are plain text. There are no meta tags or ids associated with any of it, it's just a paragraph of text.</p>

<p>My naiive idea was to take each noun and match it against <a href=""http://www.freebase.com/"" rel=""nofollow"">Freebase</a> to see if it's an activity/artist/movie/book etc.  The problem is that although most entities mentioned will be things the user likes, she will also mention things she <em>doesn't</em> like and I have no means of distinguishing the 2.  </p>

<p>I have 2 questions:</p>

<ol>
<li>What sub field of NLP should I be looking at?  Some googleable algorithms/techniques/authors would be greatly appreciated.</li>
<li>How hard is this problem?</li>
</ol>

<p>Thanks!</p>
",Named Entity Recognition (NER),extracting user interest social profile first time dabbling nlp please excuse ignorance looking method extract interest like hobby user social profile example interest like hobby bold consider pretty diverse character professional wrestler take bullet wall e train like one man genocide machine gym cried armageddon head bang ac dc seriously considering getting legend zelda tattoo friendly like party frat crowd one night hang burning man friend next play halo world warcraft next jam friend younger next youngest friend oldest friend sing karaoke bar friend collective psychiatrist shoulder profile plain text meta tag id associated paragraph text naiive idea wa take noun match freebase see activity artist movie book etc problem although entity mentioned thing user like also mention thing like mean distinguishing question sub field nlp looking googleable algorithm technique author would greatly appreciated hard problem thanks
Can I use NER to detect specific information in text?,"<p>I'm working on a application and I need to create a NLP model to detect specific information in a piece of text. For example, let's take the following two pieces of text:</p>
<ol>
<li><p>John teaches Operational Systems.</p>
</li>
<li><p>The product price will be adjusted 70% by the parameter A and 30% by the parameter B.</p>
</li>
</ol>
<p>In example 1 I need a model to be able to detect Operational System as a topic. I guess a simple NER is able to do the job. In the example 2 I need to detect the combination of parameter A and 70% (parameter A, 70%) and parameter B and 30% (parameter B, 30%). Is NER able to be trained to accomplish this job? Can I train NER to return combination of different terms? There are better techniques to do it?</p>
",Named Entity Recognition (NER),use ner detect specific information text working application need create nlp model detect specific information piece text example let take following two piece text john teach operational system product price adjusted parameter parameter b example need model able detect operational system topic guess simple ner able job example need detect combination parameter parameter parameter b parameter b ner able trained accomplish job train ner return combination different term better technique
spacy remove only org and person names,"<p>I have written the below function which removes all named entities from text. How could I modify it to remove only org and person names? I don't want to remove <code>6</code> from <code>$6</code> from below. Thanks</p>
<pre><code>import spacy
sp = spacy.load('en_core_web_sm')
def NER_removal(text):
    document = sp(text)
    
    text_no_namedentities = []
    
    ents = [e.text for e in document.ents]
    for item in document:
        if item.text in ents:
            pass
        else:
            text_no_namedentities.append(item.text)
    return (&quot; &quot;.join(text_no_namedentities))


NER_removal(&quot;John loves to play at Sofi stadium at 6.00 PM and he earns $6&quot;)
'loves to play at stadium at 6.00 PM and he earns $'
</code></pre>
",Named Entity Recognition (NER),spacy remove org person name written function remove named entity text could modify remove org person name want remove thanks
Adding a condition in the standard SpaCy tokenizer,"<p>I would like to keep the tokenizer that SpaCy normally uses, but adding a condition.</p>
<p>SpaCy usually separates a dot (&quot;.&quot;) from a word and places it as a token. I want to keep that, except in cases where I have the abbreviation: &quot;et al.&quot;, in this case I would like to return as tokens: ['et' , 'al.'], without considering the dot as another token, just in this case.</p>
<p>I have been reviewing the information and it seems to me that the solution would be related to the script below, however, I do not know where I could place this condition.</p>
<pre><code>import spacy
from spacy.lang.char_classes import ALPHA_LOWER, ALPHA_UPPER, PUNCT
from spacy.lang.char_classes import LIST_PUNCT, LIST_ELLIPSES, LIST_QUOTES, LIST_ICONS
from spacy.lang.char_classes import CURRENCY, UNITS, ALPHA_LOWER, CONCAT_QUOTES, PUNCT, ALPHA_UPPER
from spacy.util import compile_suffix_regex

# Default tokenizer
nlp = spacy.load(&quot;pt_core_news_sm&quot;)
doc = nlp(&quot;Esse é um exemplo. Ramon et al., kcal.&quot;)
print([t.text for t in doc]) # ['Esse', 'é', 'um', 'exemplo', '.', 'Ramon', 'et', 'al', '.', ',', 'kcal', '.']

# Modify tokenizer suffix patterns

suffixes = (
LIST_PUNCT
+ LIST_ELLIPSES
+ LIST_QUOTES
+ LIST_ICONS
+ [&quot;'s&quot;, &quot;'S&quot;, &quot;’s&quot;, &quot;’S&quot;, &quot;—&quot;, &quot;–&quot;]
+ [
    r&quot;(?&lt;=[0-9])\+&quot;,
    r&quot;(?&lt;=°[FfCcKk])\.&quot;,
    r&quot;(?&lt;=[0-9])(?:{c})&quot;.format(c=CURRENCY),
    r&quot;(?&lt;=[0-9])(?:{u})&quot;.format(u=UNITS),
    r&quot;(?&lt;=[0-9{al}{e}{p}(?:{q})])\.&quot;.format(
        al=ALPHA_LOWER, e=r&quot;%²\-\+&quot;, q=CONCAT_QUOTES, p=PUNCT
    ),
    r&quot;(?&lt;=[{au}][{au}])\.&quot;.format(au=ALPHA_UPPER),
]
)

suffix_regex = compile_suffix_regex(suffixes)
nlp.tokenizer.suffix_search = suffix_regex.search
doc = nlp(&quot;Esse é um exemplo. Ramon et al., kcal.&quot;)
print([t.text for t in doc]) # Expected -&gt; ['Esse', 'é', 'um', 'exemplo', '.', 'Ramon', 'et', 'al.', ',', 'kcal', '.']
</code></pre>
",Named Entity Recognition (NER),adding condition standard spacy tokenizer would like keep tokenizer spacy normally us adding condition spacy usually separate dot word place token want keep except case abbreviation et al case would like return token et al without considering dot another token case reviewing information seems solution would related script however know could place condition
Obtain return type of a function using its name and NLP using python,"<p>Let's say i have a string which is a function name:</p>
<pre><code>nameString = &quot;return_list_of_apples&quot;
#or
nameString = &quot;return_apples_list&quot;
</code></pre>
<p>I already can extract all the words in the names using nltk and a function that i have like:</p>
<pre><code>nameString = &quot;return_list_of_apples&quot;
wordsList = words_to_list(nameString)
print(wordsList)
&gt;&gt; ['return', 'list', 'of', 'apples']
</code></pre>
<p>How can i obtain return type of function using its name? in this example list.
is there any python module for it?
I think it's in NLP field.</p>
",Named Entity Recognition (NER),obtain return type function using name nlp using python let say string function name already extract word name using nltk function like obtain return type function using name example list python module think nlp field
Failed to convert iob to spaCy binary format,"<p>I try to convert my IOB (token-per-line NER) files (train/test) to Spacy 3 binary format.</p>
<p>Example of input format (with separator &quot;\t&quot;, no blanklines and encoding utf-8) :</p>
<pre><code>Département B-LOCATION
des I-LOCATION
Bouches-du-Rhône    I-LOCATION
.   O
Port    B-INSTALLATION
de  I-INSTALLATION
la  I-INSTALLATION
Ciotat  I-INSTALLATION
.   O
Avant-projet    O
du  O
môle    B-INSTALLATION
Bérouard    I-INSTALLATION
au  O
port    B-INSTALLATION
de  I-INSTALLATION
La  I-INSTALLATION
Ciotat  I-INSTALLATION
.   O
</code></pre>
<p>when I run :</p>
<pre class=""lang-sh prettyprint-override""><code>!python -m spacy convert -c iob -s -n 10 -b fr_core_news_sm /content/ner4archives_v0_train.iob .

!python -m spacy convert -c iob -s -n 10 -b fr_core_news_sm /content/ner4archives_v0_test.iob .
</code></pre>
<p>I have this error :</p>
<pre><code>ValueError: [E903] The token-per-line NER file is not formatted correctly. Try checking whitespace and delimiters. See https://spacy.io/api/cli#convert
</code></pre>
<p>I saw the git directory with example data : <a href=""https://github.com/explosion/spaCy/tree/master/extra/example_data/ner_example_data"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/tree/master/extra/example_data/ner_example_data</a> ; but I cannot find the difference between my data and examples.</p>
<p>I try to reformat my file with different kind of separators (&quot;\t&quot;, &quot; &quot;, &quot;|&quot;), I have always the same error. In addition, i tested if I have empty tokens or labels, it is not.</p>
<p>anyone with leads ? thanks in advance.</p>
",Named Entity Recognition (NER),failed convert iob spacy binary format try convert iob token per line ner file train test spacy binary format example input format separator blanklines encoding utf run error saw git directory example data find difference data example try reformat file different kind separator always error addition tested empty token label anyone lead thanks advance
Adding functionality to a Spacy NER visualizer,"<p>I am using Spacy to visualize NERs in a notebook as follows:</p>
<pre><code>import spacy
from spacy import displacy

text = &quot;When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.&quot;

nlp = spacy.load(&quot;en_core_web_sm&quot;)
doc = nlp(text)
displacy.render(doc, style=&quot;ent&quot;)
</code></pre>
<p>(This is the example in the spacy documentation)</p>
<p>result:
<a href=""https://i.sstatic.net/Rhpyl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Rhpyl.png"" alt=""enter image description here"" /></a></p>
<p>So far so good.</p>
<p>The question is if it is possible to access the HTML object itself, not just rendering it, in order to add functionality via HTML to it.
More precisely what I am looking for is a way to fire a function when clicking everyone of  those &quot;buttons&quot;.
this work around I am looking for is meant to work in a jupyter hub, i.e. Jupyter lab, so solutions using any webframe like streamlit or flask or whatever that can be run in a local server are out of scope.</p>
<p>thanks.</p>
<p>EDIT 1:
If you use the following code in a Jupyter lab you can get the plain HTML:</p>
<pre><code>displacy.render(doc, style=&quot;ent&quot;, jupyter'False'))
</code></pre>
<p>you would get:</p>
<pre><code>'&lt;div class=&quot;entities&quot; style=&quot;line-height: 2.5; direction: ltr&quot;&gt;When \n&lt;mark class=&quot;entity&quot; style=&quot;background: ....... etc....&lt;/div&gt;'
</code></pre>
<p>The idea is transforming the buttons:</p>
<p><a href=""https://i.sstatic.net/mEqRN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mEqRN.png"" alt=""enter image description here"" /></a></p>
<p>...into clickable buttons.</p>
<p>I guess the way to go is parse the HTML add clickable button and do &quot;something&quot; with ipywidgets jslink or so.</p>
",Named Entity Recognition (NER),adding functionality spacy ner visualizer using spacy visualize ners notebook follows example spacy documentation result far good question possible access html object rendering order add functionality via html precisely looking way fire function clicking everyone button work around looking meant work jupyter hub e jupyter lab solution using webframe like streamlit flask whatever run local server scope thanks edit use following code jupyter lab get plain html would get idea transforming button clickable button guess way go parse html add clickable button something ipywidgets jslink
Spacy inference goes OOM when processing several documents,"<p>I'm using spacy to process documents that come through rest api. To be more specific, I'm using transformer based model <code>en_core_web_trf</code> for NER, running on GPU. Here is a code snippet of the spacy related class (It is packed inside some basic flask server and but I don't suppose that matters here)</p>
<pre><code>class SpacyExtractor():
    def __init__(self):
        spacy.require_gpu()
        self.model = spacy.load('en_core_web_trf',
                                 disable=[&quot;tagger&quot;, &quot;parser&quot;, &quot;attribute_ruler&quot;, &quot;lemmatizer&quot;])


    def get_named_entities(self, text: str):
        doc = self.model(text)
        entities = []
        for ent in doc.ents:
            entities.append((ent.text, ent.label_))
        return entities
</code></pre>
<p>The problem is, with each call of <code>get_named_entities</code>, the amount of GPU memory allocated goes up. And it is like 2-3 GB every time (I checked this by repeatedly calling nvidia-smi while the app was processing the docs). So after a few calls, I get OOM error
<code>RuntimeError: CUDA out of memory. Tried to allocate 2.35 GiB (GPU 0; 10.76 GiB total capacity; 5.02 GiB already allocated; 1.18 GiB free; 8.41 GiB reserved in total by PyTorch)</code>
Documents are not huge at all, 1-100 pages of text for each one.
I think I make some mistake, but I just don't see it.
Environment: Ubuntu 18.04, Python 3.8, spacy 3.1.3, cuda 9.1, RTX 2080Ti 11GB RAM</p>
<p>EDIT: Also, I found out the OOM error when processing a single really long document, presented as a single long string.</p>
",Named Entity Recognition (NER),spacy inference go oom processing several document using spacy process document come rest api specific using transformer based model ner running gpu code snippet spacy related class packed inside basic flask server suppose matter problem call amount gpu memory allocated go like gb every time checked repeatedly calling nvidia smi app wa processing doc call get oom error document huge page text one think make mistake see environment ubuntu python spacy cuda rtx ti gb ram edit also found oom error processing single really long document presented single long string
Entity Extraction from Natural Language question,"<p>I am trying to build a <strong>Database Q/A Chatbot</strong> (or specifically a <em><a href=""https://jonaschapuis.com/2017/12/natural-language-interfaces-to-databases-nlidb/#Definition"" rel=""nofollow noreferrer"">Natural Language Interface to Database</a></em> if you will!). And I am having trouble extracting the <strong>entities/slots</strong> from the <em>Natural Language Query</em>.</p>
<p>Take this example, I have a table</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Interns</th>
<th>Branch</th>
<th>Birthday</th>
<th>Salary($/H)</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>Mechanical Engineering</td>
<td>2000-01-20</td>
<td>25</td>
</tr>
<tr>
<td>B</td>
<td>IT Engineering</td>
<td>1999-05-09</td>
<td>45</td>
</tr>
<tr>
<td>A</td>
<td>Electrical Engineering</td>
<td>2000-01-20</td>
<td>35</td>
</tr>
<tr>
<td>C</td>
<td>Mechanical Engineering</td>
<td>2002-09-13</td>
<td>35</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Example questions</strong> that user may ask from this table,</p>
<ul>
<li><code>What is the total salary for intern A?</code>  - <em>Desired Entities:</em> {<strong>Interns</strong>: A}</li>
<li><code>Tell me the aggregate salary for A, B and C.</code>  - <em>Desired Entities:</em> {<strong>Interns</strong>: [A,B,C]} #Notice how column name isn't mentioned</li>
<li><code>Which Interns are persuing Mechanical Engineering Branch?</code> - <em>Desired Entities:</em> {<strong>Branch</strong>: Mechanical Engineering}</li>
</ul>
<h2>Question:</h2>
<p><strong>How to identify these Entities/Slots?</strong></p>
<ul>
<li><a href=""https://stackoverflow.com/a/39622258/13190386"">This answer</a> to a similar question suggests using <em><strong>Rule-based recognizers</strong></em>. But I couldn't find how to build them.</li>
</ul>
<hr />
<h3>Things I have tried:</h3>
<ol>
<li>Creating a custom <strong>Named Entity Recognition</strong> model using <strong>Spacy</strong> to Identify the <em>Interns</em> and <em>Branch names</em>. This model was successfully able to identify <strong>values</strong> that were given in the <strong>Training Data</strong> but was <strong>failing</strong> to identify <strong>new values</strong>.</li>
<li><strong>Rules based on Part Of Speech Tagging:</strong> This approach was kinda successful but wasn't <strong>generic</strong>. This means it may not work if the same sentence is spoken in another way.</li>
</ol>
",Named Entity Recognition (NER),entity extraction natural language question trying build database q chatbot specifically natural language interface database trouble extracting entity slot natural language query take example table intern branch birthday salary h mechanical engineering b engineering electrical engineering c mechanical engineering example question user may ask table desired entity intern desired entity intern b c notice column name mentioned desired entity branch mechanical engineering question identify entity slot href answer similar question suggests using rule based recognizers find build thing tried creating custom named entity recognition model using spacy identify intern branch name model wa successfully able identify value given training data wa failing identify new value rule based part speech tagging approach wa kinda successful generic mean may work sentence spoken another way
Rule based named entity recognizer without parts of speech label or any other information,"<p>I'm working on a project where I am trying to build a named entity recognizer from texts. So basically I want to build and experiment the NER in 3 different ways.</p>

<p>First, I want to build it using only segmented sentences-> tokenized words. To clarify, I want to input only split/tokenized words into the system. Once again, the NER system is rule-based. Hence, it can only use rules to conclude which is a named entity. In the first NER, it will not have any chunk information or part of speech label. Just the tokenized words. Here, the efficiency is not the concern. Rather the concern lies in comparing the 3 different NERs, how they perform. (The one I am asking about is the 1st one).</p>

<p>I thought of it for a while and could not figure out any rules or any idea of coming up with a solution to this problem. One naive approach would be to conclude all words beginning with an uppercase and that does not follow a period to be a named entity. </p>

<p>Am I missing anything? Any heads up or guidelines would help.</p>
",Named Entity Recognition (NER),rule based named entity recognizer without part speech label information working project trying build named entity recognizer text basically want build experiment ner different way first want build using segmented sentence tokenized word clarify want input split tokenized word system ner system rule based hence use rule conclude named entity first ner chunk information part speech label tokenized word efficiency concern rather concern lie comparing different ners perform one asking st one thought could figure rule idea coming solution problem one naive approach would conclude word beginning uppercase doe follow period named entity missing anything head guideline would help
Identify Location Within the Sentence where the Missing Word Belongs,"<p>I have the code below:</p>

<pre><code>import nltk
exampleArray = ['The dog barking']

def processLanguage():
    for item in exampleArray:
        tokenized = nltk.word_tokenize(item)
        tagged = nltk.pos_tag(tokenized)
        print(tagged)

processLanguage()
</code></pre>

<p>The output of the code above are the tokenized words with their corresponding parts of speech. Example :</p>

<pre><code>[('The', 'DT'), ('dog', 'NN'), ('barking', 'NN'), ('.', '.')]

DT = determiner
NN = noun
</code></pre>

<p>The text is supposed to be </p>

<pre><code>The dog is barking
</code></pre>

<p>and supposed to have the POS sequence of</p>

<pre><code>DT -&gt; NN -&gt; VBZ -&gt; VBG

VBZ = verb, present tense, 3rd person singular
VBG = verb, present participle or gerund
</code></pre>

<p>How will I make the program locate within the sentence the position of the missing word?</p>
",Named Entity Recognition (NER),identify location within sentence missing word belongs code output code tokenized word corresponding part speech example text supposed supposed po sequence make program locate within sentence position missing word
Extract text information from PDF files with different layouts - machine learning,"<p>I need assistance with a ML project I am currently trying to create.</p>

<p>I receive a lot of invoices from a lot of different suppliers - all in their own unique layout. I need to extract <strong>3</strong> key elements from the invoices. These <strong>3</strong> elements are all located in a table/line items for all the invoices.</p>

<p>The <strong>3</strong> elements are: </p>

<ul>
<li><strong>1</strong>: Tariff number (digit)</li>
<li><strong>2</strong>: Quantity (always a digit)</li>
<li><strong>3</strong>: Total line amount (monetary value)</li>
</ul>

<p>Please refer to below screenshot, where I have marked these field on a sample invoice.</p>

<p><a href=""https://i.sstatic.net/HYmGZ.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/HYmGZ.png"" alt=""enter image description here""></a></p>

<p>I started this project with a template approach, based on <em>regular expressions</em>. This, however, was not scaleable at all and I ended up with tons of different rules.</p>

<p>I am hoping that machine learning can help me here - or maybe a hybrid solution?</p>

<h1>The common denominator</h1>

<p>In <strong>all</strong> of my invoices, despite of the different layouts, each line item will <strong>always</strong> consist of one <strong>tariff number</strong>. This tariff number is always 8 digits, and is always formatted in one the ways like below:</p>

<ul>
<li>xxxxxxxx</li>
<li>xxxx.xxxx</li>
<li>xx.xx.xx.xx</li>
</ul>

<p>(Where ""x"" is a digit from 0 - 9).</p>

<p><strong>Further</strong>, as you can see on the invoice there is both a Unit Price and a Total Amount per line. The amount I will need is <strong>always</strong> the highest for each line.</p>

<h1>The output</h1>

<p>For each invoice like the one above, I need the output for each line. This could for example be something like this:</p>

<pre><code>{
    ""line"":""0"",
    ""tariff"":""85444290"",
    ""quantity"":""3"",
    ""amount"":""258.93""
},
{
    ""line"":""1"",
    ""tariff"":""85444290"",
    ""quantity"":""4"",
    ""amount"":""548.32""
},
{
    ""line"":""2"",
    ""tariff"":""76109090"",
    ""quantity"":""5"",
    ""amount"":""412.30""
}
</code></pre>

<h1>Where to go from here?</h1>

<p>I am not sure of what I am looking to do falls under machine learning and if so, under which category. Is it computer vision? NLP? Named Entity Recognition?</p>

<p>My initial thought was to:</p>

<ol>
<li>Convert the invoice to text. (The invoices are all in textable PDFs, so I can use something like <code>pdftotext</code> to get the exact textual values)</li>
<li>Create custom <strong>named entities</strong> for <code>quantity</code>, <code>tariff</code> and <code>amount</code></li>
<li>Export the found entities.</li>
</ol>

<p>However, I feel like I might be missing something. </p>

<p><strong>Can anyone assist me in the right direction?</strong></p>

<h1>Edit:</h1>

<p>Please see below for a few more examples of how an invoice table section can look like:</p>

<p>Sample invoice #2
<a href=""https://i.sstatic.net/0dvIC.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/0dvIC.png"" alt=""enter image description here""></a></p>

<p>Sample invoice #3
<a href=""https://i.sstatic.net/8Wh6f.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/8Wh6f.png"" alt=""enter image description here""></a></p>

<h1>Edit 2:</h1>

<p>Please see below for the three sample images, <strong>without</strong> the borders/bounding boxes:</p>

<p>Image 1:
<a href=""https://i.sstatic.net/5Iln5.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/5Iln5.png"" alt=""Sample 1 without bbox""></a></p>

<p>Image 2:
<a href=""https://i.sstatic.net/cEabY.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/cEabY.png"" alt=""Sample 2 without bbox""></a></p>

<p>Image 3:
<a href=""https://i.sstatic.net/cSHzh.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/cSHzh.png"" alt=""Sample 3 without bbox""></a></p>
",Named Entity Recognition (NER),extract text information pdf file different layout machine learning need assistance ml project currently trying create receive lot invoice lot different supplier unique layout need extract key element invoice element located table line item invoice element tariff number digit quantity always digit total line amount monetary value please refer screenshot marked field sample invoice started project template approach based regular expression however wa scaleable ended ton different rule hoping machine learning help maybe hybrid solution common denominator invoice despite different layout line item always consist one tariff number tariff number always digit always formatted one way like xxxxxxxx xxxx xxxx xx xx xx xx x digit see invoice unit price total amount per line amount need always highest line output invoice like one need output line could example something like go sure looking fall machine learning category computer vision nlp named entity recognition initial thought wa convert invoice text invoice textable pdfs use something like get exact textual value create custom named entity export found entity however feel like might missing something anyone assist right direction edit please see example invoice table section look like sample invoice sample invoice edit please see three sample image without border bounding box image image image
Run dependency parser on pre-initialized doc object of spacy,"<p>I am trying to incorporate spacy's dependency parser into a legacy code in java through web API.</p>
<p>All other components tokenizer, tagger, merged_words, NER are done from the legacy NLP code. I am only interested to apply the dependency parser along with the dependency rule matcher of spacy 3.</p>
<p>I have tried the following approach</p>
<ol>
<li>creating a new doc object using <a href=""https://spacy.io/api/doc#init"" rel=""nofollow noreferrer"">https://spacy.io/api/doc#init</a>.</li>
</ol>
<pre class=""lang-py3 prettyprint-override""><code>from spacy.tokens import Doc
sent=[&quot;The heating_temperature was found to be 500 C&quot;]
words=[&quot;The&quot;,&quot;heating_temperature&quot;, &quot;was&quot;, &quot;found&quot;, &quot;to&quot;, &quot;be&quot;, &quot;500&quot;, &quot;C&quot;]
spaces=[True,True,True,True,True,True,True,False]
tags=[&quot;DT&quot;,&quot;NN&quot;,&quot;VBD&quot;,&quot;VBN&quot;,&quot;TO&quot;,&quot;VB&quot;,&quot;CD&quot;,&quot;NN&quot;]
ents=[&quot;O&quot;,&quot;I-PARAMETER&quot;,&quot;O&quot;,&quot;O&quot;,&quot;O&quot;,&quot;O&quot;,&quot;I-VALUE&quot;,&quot;O&quot;]
doc = Doc(nlp.vocab, words=words,spaces=spaces, tags=tags, ents=ents)
</code></pre>
<ol start=""2"">
<li>Create an NLP pipeline with only parser</li>
</ol>
<pre class=""lang-py3 prettyprint-override""><code>#can use nlp.blank too
nlp2 = spacy.load(&quot;en_core_web_sm&quot;, exclude=['attribute_ruler', 'lemmatizer', 'ner', &quot;parser&quot;,&quot;tagger&quot;])
pipeWithParser = nlp2.add_pipe(&quot;parser&quot;, source=spacy.load(&quot;en_core_web_sm&quot;))
processed_dep = pipeWithParser(doc) #refer similar example in https://spacy.io/api/tagger#call
</code></pre>
<p>However, I am getting the following dependency tree</p>
<p><a href=""https://i.sstatic.net/vNRjz.jpg"" rel=""nofollow noreferrer"">dependency tree</a></p>
<p>where every word is an nmod relation to the first word.</p>
<p>What am I missing?
I could use the tagger of spacy too if req. I tried including tagger using above similar method but all tags were labeled 'NN'</p>
",Named Entity Recognition (NER),run dependency parser pre initialized doc object spacy trying incorporate spacy dependency parser legacy code java web api component tokenizer tagger merged word ner done legacy nlp code interested apply dependency parser along dependency rule matcher spacy tried following approach creating new doc object using create nlp pipeline parser however getting following dependency tree dependency tree every word nmod relation first word missing could use tagger spacy req tried including tagger using similar method tag labeled nn
SpaCy: how do you add custom NER labels to a pre-trained model?,"<p>I am new to SpaCy and NLP. I am using SpaCy v 3.1 and Python 3.9.7 64-bit.</p>
<p><strong>My objective</strong>: to use a pre-trained SpaCy model (<code>en_core_web_sm</code>) and add a set of custom labels to the existing NER labels (<code>GPE</code>, <code>PERSON</code>, <code>MONEY</code>, etc.) so that the model can recognize both the default AND the custom entities.</p>
<p>I've looked at the SpaCy documentation and what I need seems to be an <a href=""https://spacy.io/api/entityrecognizer"" rel=""noreferrer"">EntityRecogniser</a>, specifically a new pipe.</p>
<p>However, it is not really clear to me at what point in my workflow I should add this new pipe, since in SpaCy 3 the training happens in CLI, and from the docs it's not even clear to me where the pre-trained model is called.</p>
<p>Any tutorials or pointers you might have are highly appreciated.</p>
<p>This is what I think should be done, but I am not sure how:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy import displacy
from spacy_langdetect import LanguageDetector
from spacy.language import Language
from spacy.pipeline import EntityRecognizer

# Load model
nlp = spacy.load(&quot;en_core_web_sm&quot;)

# Register custom component and turn a simple function into a pipeline component
@Language.factory('new-ner')
def create_bespoke_ner(nlp, name):
    
    # Train the new pipeline with custom labels here??
    
    return LanguageDetector()

# Add custom pipe
custom = nlp.add_pipe(&quot;new-ner&quot;)
</code></pre>
<p>This is what my config file looks like so far. I suspect my new pipe needs to go next to &quot;tok2vec&quot; and &quot;ner&quot;.</p>
<pre><code>[paths]
train = null
dev = null
vectors = null
init_tok2vec = null

[system]
gpu_allocator = null
seed = 0

[nlp]
lang = &quot;en&quot;
pipeline = [&quot;tok2vec&quot;,&quot;ner&quot;]
batch_size = 1000
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;}

[components]

[components.ner]
factory = &quot;ner&quot;
incorrect_spans_key = null
moves = null
update_with_oracle_cut_size = 100
</code></pre>
",Named Entity Recognition (NER),spacy add custom ner label pre trained model new spacy nlp using spacy v python bit objective use pre trained spacy model add set custom label existing ner label etc model recognize default custom entity looked spacy documentation need seems entityrecogniser specifically new pipe however really clear point workflow add new pipe since spacy training happens cli doc even clear pre trained model called tutorial pointer might highly appreciated think done sure config file look like far suspect new pipe need go next tok vec ner
SpaCy NER differentiating numbers or entities,"<p>I am currently playing with SpaCy NER and wondering if SpaCy NER can do these 2 things:</p>

<p><strong>Case 1</strong></p>

<p>Let's say we have 2 sentences that we want to do NER with:</p>

<ol>
<li>Sugar level in his body is increasing.</li>
<li>His overall health quality is increasing.</li>
</ol>

<p>Can we tag ""increasing"" in the first sentence as ""symptoms"" entity, and tag ""increasing"" in the second one as ""good outcome"" entity? Will NER see the difference in those 2 ""increasing"" words?</p>

<p><strong>Case 2</strong></p>

<p>We also have 2 different sentences:</p>

<ol>
<li>My salary is USD 8000 per month</li>
<li>My spending is USD 5000 per month</li>
</ol>

<p>Can NER see the number in the first sentence as ""income"" entity and the number in the second sentence as ""spending""?</p>

<p>Thank you</p>
",Named Entity Recognition (NER),spacy ner differentiating number entity currently playing spacy ner wondering spacy ner thing case let say sentence want ner sugar level body increasing overall health quality increasing tag increasing first sentence symptom entity tag increasing second one good outcome entity ner see difference increasing word case also different sentence salary usd per month spending usd per month ner see number first sentence income entity number second sentence spending thank
How to convert NL to SQL,"<p>I successfully tokenized, removed stop words, completed POS and NER but now I am facing a problem in generating SQL queries. I am not able to convert NER to SQL.</p>
<p>I used have used the NLTK package.</p>
<p>My parser code:</p>
<pre><code>from nltk import load_parser
cp = load_parser('grammars/book_grammars/sql0.fcfg')
query = 'What cities are located in India'
trees = list(cp.parse(query.split()))
print(trees)

answer = trees[0].label()['SEM']
answer = [s for s in answer if s]
print(answer)

q = ' '.join(answer)
print(q)
</code></pre>
<p>My code till NER:</p>
<pre><code>contentArray = ['show sales from Delhi']
def processLanguage():
    try:
        for item in contentArray:
            tokenized = nltk.word_tokenize(item)
            
            stop_words = set(stopwords.words('english'))
            word_tokens = word_tokenize(item)
            filtered_sentence = [w for w in word_tokens if not w in stop_words]
            filtered_sentence = []
            for w in word_tokens:
                if w not in stop_words:
                    filtered_sentence.append(w)

            #print(word_tokens)
            print(filtered_sentence)
            
            tagged = nltk.pos_tag(filtered_sentence)
            print(tagged)
            
            ne_tree = ne_chunk(tagged)
            print(ne_tree)

            iob_tagged = tree2conlltags(ne_tree)
            print (iob_tagged)
            ne_tree = conlltags2tree(iob_tagged)
            print (ne_tree)

            namedEnt = nltk.ne_chunk(tagged)
            namedEnt.draw()

 



    except Exception as e:
        print (str(e))


processLanguage()
</code></pre>
",Named Entity Recognition (NER),convert nl sql successfully tokenized removed stop word completed po ner facing problem generating sql query able convert ner sql used used nltk package parser code code till ner
Adding additional classes in stanford NLP NER,"<p>For stanford NER 3 class model, Location, Person, Organization recognizers are available. Is it possible to add additional classes to this model. For example : Sports as one class to tag sports names.</p>
<p>or if not, is there any model where i can add additional classes.</p>
<p>Note: I didnt exactly mean to add &quot;sports&quot; as a class. I was wondering is there a possibility to add a custom class in that model. If not possible in stanford, is it possible with spacy..</p>
",Named Entity Recognition (NER),adding additional class stanford nlp ner stanford ner class model location person organization recognizers available possible add additional class model example sport one class tag sport name model add additional class note didnt exactly mean add sport class wa wondering possibility add custom class model possible stanford possible spacy
SpaCy 3: how to get the raw data used to train en_core_web_sm?,"<p>I am new to SpaCy. I noticed that there are a number of NER categories listed in the documentation of all <code>en_core_web</code> models:</p>
<pre><code>'CARDINAL', 
'DATE', 
'EVENT', 
'FAC', 
'GPE', 
'LANGUAGE', 
'LAW', 
'LOC', 
'MONEY', 
'NORP', 
'ORDINAL', 
'ORG', 
'PERCENT', 
'PERSON', 
'PRODUCT', 
'QUANTITY', 
'TIME', 
'WORK_OF_ART'
</code></pre>
<p>I need to access the raw data used to assign each word the correct category. In other words, what's the list of words labelled as <code>'WORK_OF_ART'</code>, and is this list available?</p>
<p>The reason I ask this question is that I want to build a custom model that uses some of the default NER categories, as well as my own.</p>
",Named Entity Recognition (NER),spacy get raw data used train en core web sm new spacy noticed number ner category listed documentation model need access raw data used assign word correct category word list word labelled list available reason ask question want build custom model us default ner category well
Is their a way to add the new NER tag found in a new column?,"<p>I want to be able to compare the NER tag found compared to a known location of the original tweet. I am using twitter data and adding it to a pandas dataframe columns ; id, tweet, location. I then use spacy and NER to find the location using the below code (ideally just finding the NER entities; GPE and LOC), I need it to go into a new column.
So it would read: ID, Tweet, Known location, NER location. The main issue I have had is getting the tweet index the same as the new NER tag, as they dont always match up for example is two NER tags are found in one tweet.
Any help would be much appreciated. After I am going to analyse so any suggestions on good methods to use would be great so I can research more on them! Thanks</p>
<pre><code>ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]
ent = ['GPE', 'LOC']

for ent in doc.ents:
  print(ent.text+' -- '+ent.label_+'-- '+spacy.explain(ent.label_))
table = []
for ent in doc.ents:
  table.append( [ent.text,ent.label_,spacy.explain(ent.label_)])
  #ent.start, ent.end - can also use these to see position of text
df3 = pd.DataFrame(table, columns=['Entity', 'Label','Label_Description']) #to use above ones add in here - 'start','end',
#.sort_values(by=['Label']
print(df3)
DF1= df3.loc[df3['Label'].isin(['LOC','GPE'])]
gk2 = DF1.groupby('Entity').sum()
print(gk2) ```

</code></pre>
",Named Entity Recognition (NER),way add new ner tag found new column want able compare ner tag found compared known location original tweet using twitter data adding panda dataframe column id tweet location use spacy ner find location using code ideally finding ner entity gpe loc need go new column would read id tweet known location ner location main issue getting tweet index new ner tag dont always match example two ner tag found one tweet help would much appreciated going analyse suggestion good method use would great research thanks
Can I do any analysis on spacy display using NER?,"<p>When accessing this display in spacy NER, can you add the found entities - in this case any tweets with GPE or LOC - to a new dataframe or do any further analysis on this topic? I thought once I got them into a list I could use geopy to visualive it possibly, any thoughts?</p>
<pre><code>colors = {'LOC': 'linear-gradient(90deg, ~aa9cde, #dc9ce7)', 'GPE' : 'radial-gradient(white, blue)'}
options = {'ents' : ['LOC', 'GPE'],'colors':colors} 
spacy.displacy.render(doc, style='ent',jupyter=True, options=options, )

</code></pre>
",Named Entity Recognition (NER),analysis spacy display using ner accessing display spacy ner add found entity case tweet gpe loc new dataframe analysis topic thought got list could use geopy visualive possibly thought
Regular Expression and Rule Based Matcher to extract legal citations title and volume,"<p>I am trying to extract case title, volume and pages from inconsistence legal documents. I am using two algorithms, regex to and spaCy rule based matching with Entity and POS tags (still learning this...). I am getting over half of the citations with regex (thanks to answer code below) but zero with spaCy. My code is</p>
<pre><code>import re
import en_core_web_sm
nlp = en_core_web_sm.load()

nlp = spacy.load('en_core_web_sm')

from spacy.matcher import Matcher
m_tool = Matcher(nlp.vocab)

doc = open(file='text1.txt', mode='r', encoding='utf-8').read()
#print(text)

doc = nlp(doc)
#print([(ent.text, ent.label_) for ent in doc.ents])


p1 = [{'IS_TITLE': 'NN'}, {'LOWER': 'v'}, {'IS_PUNCT': True}, {'IS_TITLE': 'NN'}]
p2 = [{'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'LOWER': 'v'}, {'IS_PUNCT': True}, {'IS_TITLE': 'NN'}]
p3 = [{'IS_TITLE': 'NN'}, {'LOWER': 'v'}, {'IS_PUNCT': True}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'},]
p4 = [{'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'LOWER': 'v'}, {'IS_PUNCT': True}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}]
p5 = [{'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'LOWER': 'v'}, {'IS_PUNCT': True}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}]
p6 = [{'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'LOWER': 'v'}, {'IS_PUNCT': True}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}]
p7 = [{'IS_TITLE': 'NN'}, {'LOWER': 'v'}, {'IS_PUNCT': True}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}]
p8 = [{'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'LOWER': 'v'}, {'IS_PUNCT': True}, {'IS_TITLE': 'NN'}]
p9 = [{'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}, {'LOWER': 'v'}, {'IS_PUNCT': True}, {'IS_TITLE': 'NN'}, {'IS_TITLE': 'NN'}]
p10 = [{'label': 'PERSON'}]
P11 = [{'label': 'ORG'}, {'label': 'PERSON'}]
p12 = [{'label': 'PERSON'}, {'label': 'ORG'}]
p13 = [{'label': 'ORG'}, {'label': 'ORG'}, {'label': 'ORG'}, {'label': 'ORG'}]

m_tool.add('QBF', None, p1, p2, p3, p4, p5, p6, p6, p7, p8, p9, p10, p11, p12, p13)

phrase_matches = m_tool(doc)
print(phrase_matches)

matches = re.findall(r'(?:[A-Z]\w*\.? )+v\. .*?\d{4}\)', contents)
for match in matches:
    print(match)
</code></pre>
<p>My text1 looks like</p>
<pre><code>text1 = &quot;material fact challenged. Brill v. Guardian Life Ins. Co. of America, 142 N.J. 520, 529 (1995)
(emphasis original).
When a movant establishes certain facts, those who would oppose the motion are under See Della v. Guard Lifal Ins. Co. of SA, 142 N.J. 420, 549 (2011)
an obligation to come forward with controverting facts. Heljon Mgmt. Corp. v. DiLeo, 55 N.J.
Super. 306, 312-13 (No Citations. This was extracted from NJ Sup..). Mere assertions and allegations in the pleadings are
insufficient to defeat motions for summary judgment. Ocean Cape Hotel Corp. v. Masefield
Corp., 63 N.J. Super. 369, 383 (App. Div. 1960). Where the party opposing summary
 &quot;
</code></pre>
<p>I am expecting all matches with both algorithmns,</p>
<pre><code>&quot;Brill v. Guardian Life Ins. Co. of America, 142 N.J. 520, 529 (1995)&quot;
&quot;Della v. Guard Lifal Ins. Co. of SA, 142 N.J. 420, 549 (2011)&quot;
&quot;Heljon Mgmt. Corp. v. DiLeo, 55 N.J. Super. 306, 312-13 (No Citations. This was extracted from NJ Sup..)&quot;
&quot;Ocean Cape Hotel Corp. v. Masefield Corp., 63 N.J. Super. 369, 383 (App. Div. 1960)&quot;
</code></pre>
",Named Entity Recognition (NER),regular expression rule based matcher extract legal citation title volume trying extract case title volume page inconsistence legal document using two algorithm regex spacy rule based matching entity po tag still learning getting half citation regex thanks answer code zero spacy code text look like expecting match algorithmns
Lightweight Rasa NLU server?,"<p>I need to train a large model on a desktop, then move access the model to the web to access it in real time on a Linux (Debian) AWS server. All I want to do is extract entities and intents from the model. Is there a a lightweight version that just queries the trained model available, or do I need to install the full Rasa distribution?</p>
",Named Entity Recognition (NER),lightweight rasa nlu server need train large model desktop move access model web access real time linux debian aws server want extract entity intent model lightweight version query trained model available need install full rasa distribution
How to extract the word from sentence,"<p>I'm new in <code>NLP</code> and I want to create model to accomplish this task</p>
<p>My data is in Persian language</p>
<p>I want to extract this entity from sentence like this</p>
<pre><code>sentence = 'I bought 3 red apples from Bingo store'
sentence = 'من سه تا سیب قرمز از فروشگاه بینگو خریدم' //Persian 
</code></pre>
<p>I want this entity's</p>
<pre><code>product = 'red appel'
unit count = '3'
location = 'Bingo store'
-------------------------
product = 'سیب قرمز'
unit count = '3'
location = 'فروشگاه بینگو'
</code></pre>
",Named Entity Recognition (NER),extract word sentence new want create model accomplish task data persian language want extract entity sentence like want entity
"Trying convert JSON to spacy training format for NER, but not getting any input or error","<p>I'm trying to convert data turks NER Json file to spacy training format using the following code but upon executing the code not giving any input. </p>

<pre><code>import plac
import logging
import argparse
import sys
import os
import json
import pickle
import spacy

dataturks_JSON_FilePath = Path(""C:\\Users\\Desktop\\Project\\Spacy"")

def convert_dataturks_to_spacy(dataturks_JSON_FilePath):
    try:
        training_data = []
        lines=[]
        with open(dataturks_JSON_FilePath, 'r') as f:
            lines = f.readlines()

        for line in lines:
            data = json.loads(line)
            text = data['content']
            entities = []

            for annotation in data['annotation']:

                point = annotation['points'][0]
                labels = annotation['label']

                if not isinstance(labels, list):
                    labels = [labels]

                for label in labels:
                    entities.append((point['start'], point['end'] + 1 ,label))


            training_data.append((text, {""entities"" : entities}))
        return training_data
    except Exception as e:
        logging.exception(""Unable to process ""+dataturks_JSON_FilePath +""\n""+""error = ""+str(e))
        return None
</code></pre>

<p>No input upon executing the script.</p>
",Named Entity Recognition (NER),trying convert json spacy training format ner getting input error trying convert data turk ner json file spacy training format using following code upon executing code giving input input upon executing script
Varaiable entity extraction - No pattern in the entities of a sentence - NLP,"<p>I'm working on a customer support bot which helps business users understand the meaning of certain technical terms or status of some of their requests. A typical sentence looks like below</p>

<ol>
<li>Explain me about Air Compressor/Heating and cooling systems/ Law Of
Thermodynamics </li>
<li>Get me the status of Ticket123/HEATER12</li>
</ol>

<p><strong><em>What have I done so far</em></strong></p>

<p>I currently use Microsoft LUIS to identify the entities where I upload all the possible entities and LUIS does a string match and return them. The problem with this approach is </p>

<ul>
<li>The entity list keeps getting bigger and needs to be updated everyday</li>
<li>User may type in spelling mistakes - In some cases, the word user types may not be a dictionary word for the spelling to be corrected.</li>
</ul>

<p><strong><em>What's my solution (which doesn't seem to work well)</em></strong></p>

<p>I'm currently thinking of an approach to tag the POS and group the noun phrases/nouns but I dont think this will be an effective method.</p>

<p>Also one thing to be noted is that the entities don't follow any pattern. What should be my approach here. Any pointers would be appreciated.</p>
",Named Entity Recognition (NER),varaiable entity extraction pattern entity sentence nlp working customer support bot help business user understand meaning certain technical term status request typical sentence look like explain air compressor heating cooling system law thermodynamics get status ticket heater done far currently use microsoft luis identify entity upload possible entity luis doe string match return problem approach entity list keep getting bigger need updated everyday user may type spelling mistake case word user type may dictionary word spelling corrected solution seem work well currently thinking approach tag po group noun phrase noun dont think effective method also one thing noted entity follow pattern approach pointer would appreciated
How to build &#39;ner_ontonotes_bert_mult&#39; model from scratch,"<p>How can I custom train the model &quot;ner_ontonotes_bert_mult&quot;. I want to train the model with my own dataset which have some different tags. How to train the model and use it?</p>
<p>When I am training my model it accepting only 40 data and after that training stops automatically. How to resolve this.</p>
",Named Entity Recognition (NER),build ner ontonotes bert mult model scratch custom train model ner ontonotes bert mult want train model dataset different tag train model use training model accepting data training stop automatically resolve
SPACY - Confusion about word vectors and tok2vec,"<p>it would be really helpful for me if you would help me understand some underlying concepts about Spacy.</p>
<p>I understand some spacy models have some predefined static vectors, for example, for the Spanish models these are the vectors generated by FastText.
I also understand that there is a tok2vec layer that generates vectors from tokens, and this is used for example as the input of the NER components of the model.</p>
<p>If the above is correct, then I have some questions:</p>
<ul>
<li>Does the NER component also use the static vectors?
<ul>
<li>If yes, then where does the tok2vec layer comes into play?</li>
<li>If no, then is there any advantage on using the lg or md models if you only intend to use the model for e.g. the NER component?</li>
</ul>
</li>
<li>Is the tok2vec layer already trained for pretrained downloaded models, e.g. Spanish?</li>
<li>If I replace the NER component of a pretrained model, does it keep the tok2vec layer untouched i.e. with the learned weights?</li>
<li>Is the tok2vec layer also trained when I train a NER model?</li>
<li>Would the pretrain command help the tok2vec layer learn some domain-specific words that may be OOV?</li>
</ul>
<p>Thanks a lot!</p>
",Named Entity Recognition (NER),spacy confusion word vector tok vec would really helpful would help understand underlying concept spacy understand spacy model predefined static vector example spanish model vector generated fasttext also understand tok vec layer generates vector token used example input ner component model correct question doe ner component also use static vector yes doe tok vec layer come play advantage using lg md model intend use model e g ner component tok vec layer already trained pretrained downloaded model e g spanish replace ner component pretrained model doe keep tok vec layer untouched e learned weight tok vec layer also trained train ner model would pretrain command help tok vec layer learn domain specific word may oov thanks lot
&#39;ner_ontonotes_bert_mult&#39; model Custom train,"<p>When I want to train this 'ner_ontonotes_bert_mult' model with my custom dataset it is showing the error below. (I have saved my datset in the <code>~\.deeppavlov\downloads\ontonotes </code> folder that was mentioned in [deeppavlov documentation][1]. )</p>
<pre><code>
PS C:\Users\sghanta\Desktop\NER&gt; &amp; c:/Users/sghanta/Desktop/NER/env/Scripts/Activate.ps1
(env) PS C:\Users\sghanta\Desktop\NER&gt; &amp; c:/Users/sghanta/Desktop/NER/env/Scripts/python.exe c:/Users/sghanta/Desktop/NER/train_model.py
C:\Users\sghanta\Desktop\NER\env\lib\site-packages\numpy\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:
C:\Users\sghanta\Desktop\NER\env\lib\site-packages\numpy\.libs\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll
C:\Users\sghanta\Desktop\NER\env\lib\site-packages\numpy\.libs\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll
  stacklevel=1)
Traceback (most recent call last):
  File &quot;c:/Users/sghanta/Desktop/NER/train_model.py&quot;, line 12, in &lt;module&gt;
    ner_model = train_model(configs.ner.ner_ontonotes_bert_mult)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\__init__.py&quot;, line 29, in train_model
    train_evaluate_model_from_config(config, download=download, recursive=recursive)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\core\commands\train.py&quot;, line 92, in train_evaluate_model_from_config
    data = read_data_by_config(config)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\core\commands\train.py&quot;, line 58, in read_data_by_config
    return reader.read(data_path, **reader_config)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\dataset_readers\conll2003_reader.py&quot;, line 56, in read
    dataset[name] = self.parse_ner_file(file_name)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\dataset_readers\conll2003_reader.py&quot;, line 106, in parse_ner_file
    raise Exception(f&quot;Input is not valid {line}&quot;)
Exception: Input is not valid 
 O

(env) PS C:\Users\sghanta\Desktop\NER&gt; 
</code></pre>
<p>After cleaning the dataset the above error has gone but this is the new error.
New Error</p>
<pre><code>2021-08-12 02:43:35.335 ERROR in 'deeppavlov.core.common.params'['params'] at line 112: Exception in &lt;class 'deeppavlov.models.bert.bert_sequence_tagger.BertSequenceTagger'&gt;
Traceback (most recent call last):
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\client\session.py&quot;, line 1365, in _do_call
    return fn(*args)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\client\session.py&quot;, line 1350, in _run_fn
    target_list, run_metadata)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\client\session.py&quot;, line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [13,13] rhs shape= [37,37]
         [[{{node save/Assign_76}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\training\saver.py&quot;, line 1290, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\client\session.py&quot;, line 956, in run
    run_metadata_ptr)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\client\session.py&quot;, line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\client\session.py&quot;, line 1359, in _do_run
    run_metadata)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\client\session.py&quot;, line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [13,13] rhs shape= [37,37]
         [[node save/Assign_76 (defined at C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\framework\ops.py:1748) ]]

Original stack trace for 'save/Assign_76':
  File &quot;c:/Users/sghanta/Desktop/NER/train_model.py&quot;, line 12, in &lt;module&gt;
    ner_model = train_model(configs.ner.ner_ontonotes_bert_mult)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\__init__.py&quot;, line 29, in train_model
    train_evaluate_model_from_config(config, download=download, recursive=recursive)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\core\commands\train.py&quot;, line 121, in train_evaluate_model_from_config
    trainer.train(iterator)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\core\trainers\nn_trainer.py&quot;, line 334, in train
    self.fit_chainer(iterator)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\core\trainers\fit_trainer.py&quot;, line 104, in fit_chainer
    component = from_params(component_config, mode='train')
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\core\common\params.py&quot;, line 106, in from_params
    component = obj(**dict(config_params, **kwargs))
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\core\models\tf_backend.py&quot;, line 76, in __call__
    obj.__init__(*args, **kwargs)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\core\models\tf_backend.py&quot;, line 28, in _wrapped
    return func(*args, **kwargs)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\models\bert\bert_sequence_tagger.py&quot;, line 529, in __init__
    **kwargs)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\models\bert\bert_sequence_tagger.py&quot;, line 259, in __init__
    self.load()
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\core\models\tf_backend.py&quot;, line 28, in _wrapped
    return func(*args, **kwargs)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\models\bert\bert_sequence_tagger.py&quot;, line 457, in load
    return super().load(exclude_scopes=exclude_scopes, **kwargs)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\core\models\tf_model.py&quot;, line 251, in load
    return super().load(exclude_scopes=exclude_scopes, **kwargs)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\deeppavlov\core\models\tf_model.py&quot;, line 54, in load
    saver = tf.train.Saver(var_list)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\training\saver.py&quot;, line 828, in __init__
    self.build()
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\training\saver.py&quot;, line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\training\saver.py&quot;, line 878, in _build
    build_restore=build_restore)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\training\saver.py&quot;, line 508, in _build_internal
    restore_sequentially, reshape)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\training\saver.py&quot;, line 350, in _AddRestoreOps
    assign_ops.append(saveable.restore(saveable_tensors, shapes))
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\training\saving\saveable_object_util.py&quot;, line 73, in restore
    self.op.get_shape().is_fully_defined())
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\ops\state_ops.py&quot;, line 227, in assign
    validate_shape=validate_shape)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\ops\gen_state_ops.py&quot;, line 66, in assign
    use_locking=use_locking, name=name)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\framework\op_def_library.py&quot;, line 794, in _apply_op_helper
    op_def=op_def)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\util\deprecation.py&quot;, line 507, in new_func
    return func(*args, **kwargs)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\framework\ops.py&quot;, line 3357, in create_op
    attrs, op_def, compute_device)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\framework\ops.py&quot;, line 3426, in _create_op_internal
    op_def=op_def)
  File &quot;C:\Users\sghanta\Desktop\NER\env\lib\site-packages\tensorflow_core\python\framework\ops.py&quot;, line 1748, in __init__
    self._traceback = tf_stack.extract_stack()
</code></pre>
<p>Can anyone explain how to solve it.
[1]: <a href=""http://docs.deeppavlov.ai/en/master/features/models/ner.html"" rel=""nofollow noreferrer"">http://docs.deeppavlov.ai/en/master/features/models/ner.html</a></p>
",Named Entity Recognition (NER),ner ontonotes bert mult model custom train want train ner ontonotes bert mult model custom dataset showing error saved datset folder wa mentioned deeppavlov documentation cleaning dataset error ha gone new error new error anyone explain solve
How to write the code to avoid the error of &#39;tensorflow&#39; has no attribute &#39;Session&#39; and &#39;global_variables_initializer&#39;,"<h2></h2>
<p>Trying to run a sample code for a Named Entity Recognition model as apractice.</p>
<p>The reference article is: <a href=""https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede"" rel=""nofollow noreferrer"">Named Entity Recognition (NER) with keras and tensorflow</a></p>
<p>github: <a href=""https://github.com/nxs5899/Named-Entity-Recognition_DeepLearning-keras"" rel=""nofollow noreferrer"">https://github.com/nxs5899/Named-Entity-Recognition_DeepLearning-keras</a></p>
<p>However, I have stacked with version difference of tensorflow version.</p>
<p>Since I'm not familiar with Tensorflow, I cannot modify the sample code following the description of the change.</p>
<p>I'd also appreciate it if you could share helpful articles or GitHub to build a Named Entity Recognition model with original data.</p>
<h2>Error Message</h2>
<pre><code>---&gt; 11 sess = tf.Session()
     12 K.set_session(sess)

AttributeError: module 'tensorflow' has no attribute 'Session'
</code></pre>
<h2>Working Code</h2>
<pre class=""lang-py prettyprint-override""><code>from sklearn.model_selection import train_test_split
import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras.backend import eval

X_tr, X_te, y_tr, y_te = train_test_split(new_X, y, test_size=0.1, random_state=2018)
batch_size = 32
import tensorflow as tf
import tensorflow_hub as hub
from keras import backend as K
sess = tf.Session()
K.set_session(sess)

elmo_model = hub.Module(&quot;https://tfhub.dev/google/elmo/2&quot;, trainable=True)
sess.run(tf.global_variables_initializer())
sess.run(tf.tables_initializer())
</code></pre>
<h2>What I tried to do</h2>
<p>Following the related question about <a href=""https://stackoverflow.com/questions/55142951/tensorflow-2-0-attributeerror-module-tensorflow-has-no-attribute-session"">Tensorflow 2.0 - AttributeError: module 'tensorflow' has no attribute 'Session'</a>, I tried to fix my code, but another error was shown.</p>
<p>If it is because of my trial fixed code, I would like to how should I write for the new version of tensorflow.</p>
<h3>Another Error</h3>
<pre><code>module 'tensorflow' has no attribute 'global_variables_initializer'
</code></pre>
<h3>fixed version</h3>
<pre><code>from sklearn.model_selection import train_test_split
import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras.backend import eval

tf.compat.v1.disable_eager_execution()

X_tr, X_te, y_tr, y_te = train_test_split(new_X, y, test_size=0.1, random_state=2018)
batch_size = 32
import tensorflow as tf
import tensorflow_hub as hub
from keras import backend as K
sess = tf.compat.v1.Session()
K.set_session(sess)

elmo_model = hub.Module(&quot;https://tfhub.dev/google/elmo/2&quot;, trainable=True)
sess.run(tf.global_variables_initializer())
sess.run(tf.tables_initializer())
</code></pre>
",Named Entity Recognition (NER),write code avoid error tensorflow ha attribute session global variable trying run sample code named entity recognition model apractice reference article named entity recognition ner kera tensorflow github however stacked version difference tensorflow version since familiar tensorflow modify sample code following description change also appreciate could share helpful article github build named entity recognition model original data error message working code tried following related question href attributeerror module tensorflow ha attribute session tried fix code another error wa shown trial fixed code would like write new version tensorflow another error fixed version
Python Regex: How to select lines between two patterns,"<p>Consider a typical live chat data as follows:</p>
<pre><code>Peter (08:16): 
Hi 
What's up? 
;-D

Anji Juo (09:13): 
Hey, I'm using WhatsApp!

Peter (11:17):
Could you please tell me where is the feedback?

Anji Juo (19:13): 
I don't know where it is. 

Anji Juo (19:14): 
Do you by any chance know where I can catch a taxi ?
🙏🙏🙏
</code></pre>
<p>To convert this raw text file to a DataFrame,  I need to write some regex to identify column names and then extract corresponding values.</p>
<p>Please see <a href=""https://regex101.com/r/X3ubqF/1"" rel=""nofollow noreferrer"">https://regex101.com/r/X3ubqF/1</a></p>
<pre><code>Index(time)     Name        Message
08:16           Peter       Hi 
                            What's up? 
                            ;-D
09:13           Anji Juo    Hey, I'm using WhatsApp!
11:17           Peter       Could you please tell me where is the feedback?
19:13           Anji Juo    I don't know where it is. 
19:14           Anji Juo    Do you by any chance know where I can catch a taxi ?
                            🙏🙏🙏
</code></pre>
<p>The regex <code>r&quot;(?P&lt;Name&gt;.*?)\s*\((?P&lt;Index&gt;(?:\d|[01]\d|2[0-3]):[0-5]\d)\)&quot;</code> can extract the values of the time and name columns perfectly, but I have no idea how to highlight and extract messages from a specific sender for each time index.</p>
",Named Entity Recognition (NER),python regex select line two pattern consider typical live chat data follows convert raw text file dataframe need write regex identify column name extract corresponding value please see regex extract value time name column perfectly idea highlight extract message specific sender time index
Best NLP approach to classify the role of chemicals in patents in python,"<p>I'm new to nlp and I'm working on an ambitious project which consists in analyzing the &quot;role&quot; of a set of chemicals in patents.</p>
<p>Task: Classify a set of chemical compounds as &quot;inputs&quot;, &quot;outputs&quot; or &quot;none&quot; of patents</p>
<p>Data: Patent data with full natural language description</p>
<p>Tools: python</p>
<p>Idea of approach: Compute a set of features of chemicals and then classify chemicals as inputs or outputs with a Decision Tree Classifier</p>
<p>Problem: Computing the features to predict the role of the chemicals. I thought of using spaCy and create two new named entities but the same chemical can have different functions in different patents (maybe a problem?). I also thought of retrieving the sentences of the patents that contain chemicals and create a naive &quot;profile&quot; of the chemical measuring its position in the sentence, text and relation to recurrent words in the special sentences.</p>
",Named Entity Recognition (NER),best nlp approach classify role chemical patent python new nlp working ambitious project consists analyzing role set chemical patent task classify set chemical compound input output none patent data patent data full natural language description tool python idea approach compute set feature chemical classify chemical input output decision tree classifier problem computing feature predict role chemical thought using spacy create two new named entity chemical different function different patent maybe problem also thought retrieving sentence patent contain chemical create naive profile chemical measuring position sentence text relation recurrent word special sentence
Spacy Extract named entity relations from trained model,"<p>How to use Spacy to create a new name entity ""cases"" - in the context of the number of cases of an infectious disease and then extract the dependencies between this and cardinal number of cases.</p>

<p>For example in the following text 'Of these, 879 cases with 4 deaths were reported for the period 9 October to 5 November 1995.' We would want to extract ""879"" and ""cases""</p>

<p>As per the code for ""Training an additional entity type"", on Spacy's example documentation page:</p>

<p><a href=""https://spacy.io/usage/examples#information-extraction"" rel=""nofollow noreferrer"">https://spacy.io/usage/examples#information-extraction</a></p>

<p>I used their existing pretrained ""en_core_web_sm"" english model, to train an addition entity called ""CASES"" successfully:</p>

<pre><code>from __future__ import unicode_literals, print_function

import plac
import random
from pathlib import Path
import spacy
from spacy.util import minibatch, compounding

LABEL = ""CASES""

TRAIN_DATA = results_ent2[0:400]

def main(model=""en_core_web_sm"", new_model_name=""cases"", output_dir='data3', n_iter=30):
    random.seed(0)
    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank(""en"")  # create blank Language class
        print(""Created blank 'en' model"")
    # Add entity recognizer to model if it's not in the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if ""ner"" not in nlp.pipe_names:
        ner = nlp.create_pipe(""ner"")
        nlp.add_pipe(ner)
    # otherwise, get it, so we can add labels to it
    else:
        ner = nlp.get_pipe(""ner"")

    ner.add_label(LABEL)  # add new entity label to entity recognizer
    # Adding extraneous labels shouldn't mess anything up
    if model is None:
        optimizer = nlp.begin_training()
    else:
        optimizer = nlp.resume_training()
    move_names = list(ner.move_names)
    # get names of other pipes to disable them during training
    pipe_exceptions = [""ner"", ""trf_wordpiecer"", ""trf_tok2vec""]
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]
    with nlp.disable_pipes(*other_pipes):  # only train NER
        sizes = compounding(1.0, 4.0, 1.001)
        # batch up the examples using spaCy's minibatch
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            batches = minibatch(TRAIN_DATA, size=sizes)
            losses = {}
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)
            print(""Losses"", losses)

    # test the trained model   

    test_text = ""There were 100 confirmed cases?""
    doc = nlp(test_text)
    print(""Entities in '%s'"" % test_text)F
    for ent in doc.ents:
        print(ent.label_, ent.text)

    # save model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.meta[""name""] = new_model_name  # rename model
        nlp.to_disk(output_dir)
        print(""Saved model to"", output_dir)

        # test the saved model
        print(""Loading from"", output_dir)
        nlp2 = spacy.load(output_dir)
        # Check the classes have loaded back consistently
        assert nlp2.get_pipe(""ner"").move_names == move_names
        doc2 = nlp2(test_text)
        for ent in doc2.ents:
            print(ent.label_, ent.text)

main()
</code></pre>

<p>Testing the output:</p>

<pre><code>test_text = 'Of these, 879 cases with 4 deaths were reported for the period 9 October to 5 November 1995. John was infected. It cost $500'
doc = nlp(test_text)
print(""Entities in '%s'"" % test_text)
for ent in doc.ents:
    print(ent.label_, ent.text)
</code></pre>

<p>we get a results of </p>

<pre><code>Entities in 'Of these, 879 cases with 4 deaths were reported for the period 9 October to 5 November 1995. John was infected. It cost $500'
CARDINAL 879
CASES cases
CARDINAL 4
CARDINAL 9
CARDINAL 5
CARDINAL $500
</code></pre>

<p>The model has been saved and can correctly identify CASES from the above text.</p>

<p>My goal is to extract the number of cases of a given disease/virus from a news article, and then later also the number of deaths.</p>

<p>I now use this newly created model trying to find the dependencies between CASES and CARDINAL:</p>

<p>Again using Spacy's example </p>

<p><a href=""https://spacy.io/usage/examples#new-entity-type"" rel=""nofollow noreferrer"">https://spacy.io/usage/examples#new-entity-type</a></p>

<p>'Training spaCy’s Dependency Parser'</p>

<pre><code>import plac
import spacy


TEXTS = [
    ""Net income was $9.4 million compared to the prior year of $2.7 million. I have 100,000 cases"",
    ""Revenue exceeded twelve billion dollars, with a loss of $1b."",
    ""Of these, 879 cases with 4 deaths were reported for the period 9 October to 5 November 1995. John was infected. It cost $500""
]


def main(model=""data3""):
    nlp = spacy.load(model)
    print(""Loaded model '%s'"" % model)
    print(""Processing %d texts"" % len(TEXTS))

    for text in TEXTS:
        doc = nlp(text)
        relations = extract_currency_relations(doc)
        for r1, r2 in relations:
            print(""{:&lt;10}\t{}\t{}"".format(r1.text, r2.ent_type_, r2.text))


def filter_spans(spans):
    # Filter a sequence of spans so they don't contain overlaps
    # For spaCy 2.1.4+: this function is available as spacy.util.filter_spans()
    get_sort_key = lambda span: (span.end - span.start, -span.start)
    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)
    result = []
    seen_tokens = set()
    for span in sorted_spans:
        # Check for end - 1 here because boundaries are inclusive
        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:
            result.append(span)
        seen_tokens.update(range(span.start, span.end))
    result = sorted(result, key=lambda span: span.start)
    return result


def extract_currency_relations(doc):
    # Merge entities and noun chunks into one token
    spans = list(doc.ents) + list(doc.noun_chunks)
    spans = filter_spans(spans)
    with doc.retokenize() as retokenizer:
        for span in spans:
            retokenizer.merge(span)

    relations = []
    for money in filter(lambda w: w.ent_type_ == ""MONEY"", doc):
        if money.dep_ in (""attr"", ""dobj""):
            subject = [w for w in money.head.lefts if w.dep_ == ""nsubj""]
            if subject:
                subject = subject[0]
                relations.append((subject, money))
        elif money.dep_ == ""pobj"" and money.head.dep_ == ""prep"":
            relations.append((money.head.head, money))
    return relations


main()
</code></pre>

<p>The output is as follows with no dependency detection. Its as if the model has lost this ability, whilst retained the ability to detect the named entities. Or maybe some kind of setting has been switched off?</p>

<pre><code>Loaded model 'data3'
Processing 3 texts
</code></pre>

<p>If i used the original pretrained model 'en_core_web_sm', the results is:</p>

<pre><code>Processing 3 texts
Net income  MONEY   $9.4 million
the prior year  MONEY   $2.7 million
Revenue     MONEY   twelve billion dollars
a loss      MONEY   1b
</code></pre>

<p>Which is the same as the output for the model on Spacy's example page.</p>

<p>Does anybody know what has happened and why my new model, which used transfer learning on the original Spacy 'en_core_web_sm', is now unable to find the dependencies in this example?</p>

<p>EDIT:</p>

<p>If I use updated trained model, it can detect the new entity ""cases"" and cardinal ""100,000"" however it loses the ability to detect money and date.</p>

<p>When i trained the model, I trained it for thousands of sentences, using the base model en_core_web_sm itself to detect all entities and label them so as to avoid the model ""forgetting"" the old entities.</p>

<p><a href=""https://i.sstatic.net/Cmfp2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Cmfp2.png"" alt=""enter image description here""></a></p>
",Named Entity Recognition (NER),spacy extract named entity relation trained model use spacy create new name entity case context number case infectious disease extract dependency cardinal number case example following text case death reported period october november would want extract case per code training additional entity type spacy example documentation page used existing pretrained en core web sm english model train addition entity called case successfully testing output get result model ha saved correctly identify case text goal extract number case given disease virus news article later also number death use newly created model trying find dependency case cardinal using spacy example training spacy dependency parser output follows dependency detection model ha lost ability whilst retained ability detect named entity maybe kind setting ha switched used original pretrained model en core web sm result output model spacy example page doe anybody know ha happened new model used transfer learning original spacy en core web sm unable find dependency example edit use updated trained model detect new entity case cardinal however loses ability detect money date trained model trained thousand sentence using base model en core web sm detect entity label avoid model forgetting old entity
How to convert from CoNLL format to spacy format,"<p>I'm currently working on a NER model. I have a bunch of data stored in CoNLL format that need to be converted to Spacy format. In CoNLL, each word of a sentence has a tag next to it. In Spacy, the tag is only shown to the words that have an actually tag.
<strong>How can I convert from this format below (CoNLL)</strong></p>
<pre><code>From    O
2001    B-DateTime
to  I-DateTime
2004    I-DateTime
,   O
I   O
was O
a   O
stagehand   O
for O
Hartford    B-Company
Stage   I-Company
Company O
.   O
</code></pre>
<p><strong>to this format below (Spacy)</strong></p>
<pre><code>TRAIN_DATA = [('what is the price of polo?', {'entities': [(21, 25, 'PrdName')]}), 
              ('what is the price of ball?', {'entities': [(21, 25, 'PrdName')]}), 
              ('what is the price of jegging?', {'entities': [(21, 28, 'PrdName')]}), 
              ('what is the price of t-shirt?', {'entities': [(21, 28, 'PrdName')]}), 
              ('what is the price of jeans?', {'entities': [(21, 26, 'PrdName')]}), 
              ('what is the price of bat?', {'entities': [(21, 24, 'PrdName')]}), 
              ('what is the price of shirt?', {'entities': [(21, 26, 'PrdName')]}), 
              ('what is the price of bag?', {'entities': [(21, 24, 'PrdName')]}), 
              ('what is the price of cup?', {'entities': [(21, 24, 'PrdName')]}), 
              ('what is the price of jug?', {'entities': [(21, 24, 'PrdName')]}), 
              ('what is the price of plate?', {'entities': [(21, 26, 'PrdName')]}), 
              ('what is the price of glass?', {'entities': [(21, 26, 'PrdName')]}),
              ('what is the price of watch?', {'entities': [(21, 26, 'PrdName')]})]
</code></pre>
",Named Entity Recognition (NER),convert conll format spacy format currently working ner model bunch data stored conll format need converted spacy format conll word sentence ha tag next spacy tag shown word actually tag convert format conll format spacy
"can aws technology query smart enough to understand context, not just literal?","<p>My goal is to let User type in &quot;show me top suppliers for 'gopro'&quot; in the search field, he will be presented with 5 top list.</p>
<p>At the end of day, I want the above query to execute the following sql.</p>
<pre><code>SELECT  &quot;customername&quot; AS &quot;CustomerName&quot;,   &quot;last1yearspend&quot; AS &quot;Last1YearSpend&quot; 
FROM &quot;xx&quot;.&quot;spend&quot; AS &quot;SupplierModel&quot;
WHERE &quot;SupplierModel&quot;.&quot;supplierid&quot; = 333 
ORDER BY &quot;SupplierModel&quot;.&quot;rank&quot; ASC 
LIMIT 5;
</code></pre>
<p>I'm not just talking about 'text' search here.</p>
<p>Not sure even Kendra can do this. Kendra could undertand NLP so 'show me ~' can be handled correctly maybe. But, please note that I'm not looking for 'top suppliers' string here. Rather,it needs to understand how to get 'top suppliers' . To do that, it need to run sql query like above. Not just searching 'literal' value from the indexed data.</p>
<p>Not Comprehend as it just can extract entity and sentiment.</p>
<p>Not ES as it's just simply string match without/with NLP.</p>
<p>Can this even be feasible? Any other technology?</p>
",Named Entity Recognition (NER),aws technology query smart enough understand context literal goal let user type show top supplier gopro search field presented top list end day want query execute following sql talking text search sure even kendra kendra could undertand nlp show handled correctly maybe please note looking top supplier string rather need understand get top supplier need run sql query like searching literal value indexed data comprehend extract entity sentiment e simply string match without nlp even feasible technology
Hugging Face model Bio_ClinicalBERT producing 404 error,"<p>I'm building a Named Entity Recognition (NER) model using the Hugging Face implementation of emilyalsentzer/Bio_ClinicalBERT.  Up to today, I've had no issues with the model.  I'm hopeful that someone can help me understand why it's currently not working as expected.</p>
<p>Question 1 - today, trying to train using:</p>
<pre><code>MODEL_NAME = 'emilyalsentzer/Bio_ClinicalBERT'
model = text.sequence_tagger('bilstm-bert', preproc, bert_model=MODEL_NAME)
</code></pre>
<p>results in this error:
404 Client Error: Not Found for url: <a href=""https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/tf_model.h5"" rel=""nofollow noreferrer"">https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/tf_model.h5</a></p>
<p>Does Hugging Face offer any kind of health check to ascertain the status of their models?</p>
<p>Question 2 - working with files (model.h5, model.json, and preproc.sav) I'd saved from earlier training iterations, I'm getting the same 404 error shown above.  I don't understand wherein these files the call to Hugging Face is occurring.  It doesn't seem to be in the .json, and the .h5 and .sav file formats are hard to inspect.  Read more about what these files are:
<a href=""https://medium.com/analytics-vidhya/how-to-deploy-your-neural-network-model-using-ktrain-ae255b134c77"" rel=""nofollow noreferrer"">https://medium.com/analytics-vidhya/how-to-deploy-your-neural-network-model-using-ktrain-ae255b134c77</a></p>
<p>Back in February, I'd used these exact model.h5, model.json, and preproc.sav files to run the NER app using Streamlit, no problem.  Not sure if this is temporary issue with Bio_ClinicalBERT or if I need to retool my original approach due to potentially permanent problems with this transformer model.</p>
",Named Entity Recognition (NER),hugging face model bio clinicalbert producing error building named entity recognition ner model using hugging face implementation emilyalsentzer bio clinicalbert today issue model hopeful someone help understand currently working expected question today trying train using result error client error found url doe hugging face offer kind health check ascertain status model question working file model h model json preproc sav saved earlier training iteration getting error shown understand wherein file call hugging face occurring seem json h sav file format hard inspect read file back february used exact model h model json preproc sav file run ner app using streamlit problem sure temporary issue bio clinicalbert need retool original approach due potentially permanent problem transformer model
spaCy Named Entity Recognition Not Recognizing Product Entities Such as Foods,"<p>I'm using <a href=""https://spacy.io/usage/linguistic-features#named-entities"" rel=""nofollow noreferrer"">spaCy's Named Entity Recognition</a> to figure out the food word in a sentence. This is the code that I have:</p>
<pre><code>import spacy 
  
nlp = spacy.load('en_core_web_sm') 
  
sentence = &quot;I like to eat pizza.&quot;
  
doc = nlp(sentence) 
  
for ent in doc.ents: 
    print(ent.text, ent.label_)
</code></pre>
<p>Why is it not printing &quot;pizza&quot;? According to <a href=""https://spacy.io/api/annotation#named-entities"" rel=""nofollow noreferrer"">spaCy's entity types</a>, foods belongs to the <code>PRODUCT</code> entity type so shouldn't &quot;pizza&quot; be printed for the <code>ent.text</code> and <code>PRODUCT</code> be printed for the <code>ent.label</code>?</p>
",Named Entity Recognition (NER),spacy named entity recognition recognizing product entity food using spacy named entity recognition figure food word sentence code printing pizza according spacy entity type food belongs entity type pizza printed printed
Does the Hugginface NER pipeline internally deal with long documents?,"<p>My set up looks as follows:</p>
<pre><code>MODEL_CHECKPOINT = &quot;distilroberta-base&quot;
tokenizer = AutoTokenizer.from_pretrained(PATH_TO_MY_MODEL, max_len=512, add_prefix_space=True)
model = AutoModelForTokenClassification.from_pretrained(MODEL_CHEKPOINT, num_labels=32)
ner_pipeline = pipeline(task=&quot;ner&quot;, tokenizer=tokenizer, model=model)
</code></pre>
<p>However I can obtain the NER predictions for an arbitrary length documents. I wonder how is it implemented internally (maybe sliding window approach?)</p>
",Named Entity Recognition (NER),doe hugginface ner pipeline internally deal long document set look follows however obtain ner prediction arbitrary length document wonder implemented internally maybe sliding window approach
Meaning of output/training status of 256 in Stanford NLP NER?,"<p>I have a Python program where I am using os.sys to train the Stanford NER from the command line. This returns an output/training status which I save in the variable &quot;status&quot;, and it is usually 0. However, I just ran it and got an output of 256, as well as not creating a file for the trained model. This error is only occurring for larger sets of training data. I searched through the documentation on the Stanford NLP website and there doesn't seem to be info on the meanings of the outputs or why increasing training data might affect the training. Thanks in advance for any help and problem code is below.</p>
<pre><code>cmdToSys = &quot;java -mx20g -cp stanford-corenlp-4.2.2.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop &quot; + self.trainPropFileName + &quot; -ner.useSUTime false test -ner.applyNumericClassifiers false test &quot;

status = os.system(cmdToSys)
</code></pre>
<p>note: self.trainPropFileName is just the property file</p>
",Named Entity Recognition (NER),meaning output training status stanford nlp ner python program using sys train stanford ner command line return output training status save variable status usually however ran got output well creating file trained model error occurring larger set training data searched documentation stanford nlp website seem info meaning output increasing training data might affect training thanks advance help problem code note self trainpropfilename property file
NER using Spacy Library(Probabilistic Analysis),"<p>Objective: To increase the accuracy of Spacy NER Model with data input as newspaper article links particularly concerning Money Laundering and related keywords.</p>
<p>Is there any way through which I can come up with the probabilities with which a particular entity is classified as Name, Location, Organisation?</p>
",Named Entity Recognition (NER),ner using spacy library probabilistic analysis objective increase accuracy spacy ner model data input newspaper article link particularly concerning money laundering related keywords way come probability particular entity classified name location organisation
Tokenizing Named Entities in Spacy,"<p>can anyone assist please.</p>

<p>I'm attempting to tokenize a document using Spacy whereby named entities are tokenised. For example:</p>

<p>'New York is a city in the United States of America'</p>

<p>would be tokenized as:</p>

<p>['New York', 'is', 'a', 'city', 'in', 'the', 'United States of America']</p>

<p>Any tips on how to do this are very welcome. Have looked at using span.merge(), but with no success, but I am new to coding so am likely to have missed something.</p>

<p>Thank you in advance</p>
",Named Entity Recognition (NER),tokenizing named entity spacy anyone assist please attempting tokenize document using spacy whereby named entity tokenised example new york city united state america would tokenized new york city united state america tip welcome looked using span merge success new coding likely missed something thank advance
Does Spacy support multiple GPUs?,"<p>I was wondering if Spacy supports multi-GPU via <a href=""https://mpi4py.readthedocs.io/en/stable/tutorial.html#running-python-scripts-with-mpi"" rel=""nofollow noreferrer"">mpi4py</a>?</p>
<p>I am currently using Spacy's nlp.pipe for Named Entity Recognition on a high-performance-computing cluster that supports the MPI protocol and has many GPUs. It says <a href=""https://github.com/explosion/spaCy/issues/3394"" rel=""nofollow noreferrer"">here</a> that I would need to specify the GPU to use with cupy, but with PyMPI, I am not sure if the following will work (should I import spacy after calling cupy device?):</p>
<pre><code>
from mpi4py import MPI
import cupy

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
    data = [&quot;His friend Nicolas J. Smith is here with Bart Simpon and Fred.&quot;*100]
else:
    data = None

unit = comm.scatter(data, root=0)

with cupy.cuda.Device(rank):
    import spacy
    from thinc.api import set_gpu_allocator, require_gpu
    set_gpu_allocator(&quot;pytorch&quot;)
    require_gpu(rank)
    nlp = spacy.load('en_core_web_lg')
    nlp.add_pipe(&quot;merge_entities&quot;)
    tmp_list = []
    for doc in nlp.pipe(unit):
        res = &quot; &quot;.join([t.text if not t.ent_type_ else t.ent_type_ for t in doc])
        tmp_list.append(res)

result = comm.gather(tmp_list, root=0)

if comm.rank == 0:
    print (result)
else:
    result = None

</code></pre>
<p>Or if i have 4 GPUs on the same machine and I do not want to use MPI, can I do the following:</p>
<pre><code>from joblib import Parallel, delayed
import cupy

rank = 0

def chunker(iterable, total_length, chunksize):
    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))

def flatten(list_of_lists):
    &quot;Flatten a list of lists to a combined list&quot;
    return [item for sublist in list_of_lists for item in sublist]

def process_chunk(texts):
    with cupy.cuda.Device(rank):
        import spacy
        from thinc.api import set_gpu_allocator, require_gpu
        set_gpu_allocator(&quot;pytorch&quot;)
        require_gpu(rank)
        preproc_pipe = []
        for doc in nlp.pipe(texts, batch_size=20):
            preproc_pipe.append(lemmatize_pipe(doc))
        rank+=1
        return preproc_pipe

def preprocess_parallel(texts, chunksize=100):
    executor = Parallel(n_jobs=4, backend='multiprocessing', prefer=&quot;processes&quot;)
    do = delayed(process_chunk)
    tasks = (do(chunk) for chunk in chunker(texts, len(texts), chunksize=chunksize))
    result = executor(tasks)
    return flatten(result)

preprocess_parallel(texts = [&quot;His friend Nicolas J. Smith is here with Bart Simpon and Fred.&quot;*100], chunksize=1000)
</code></pre>
",Named Entity Recognition (NER),doe spacy support multiple gpus wa wondering spacy support multi gpu via mpi py currently using spacy nlp pipe named entity recognition high performance computing cluster support mpi protocol ha many gpus say would need specify gpu use cupy pympi sure following work import spacy calling cupy device gpus machine want use mpi following
In spacy custom trianed model : Config Validation error ner -&gt; incorrect_spans_key extra fields not permitted,"<p>I am running into the problem whenever I try to load custom trained NER model of spacy inside <strong>docker container</strong>.</p>
<p><strong>Note:</strong>
<em>I am using latest spacy version 3.0 and trained that NER model using CLI commands of spacy, first by converting Train data format into .spacy format</em></p>
<p>The error throws as following(You can check error in image as hyperlinked):</p>
<p><a href=""https://i.sstatic.net/3CjDl.png"" rel=""nofollow noreferrer"">config validation error</a></p>
<p>My trained model file structure looks like this:
<a href=""https://i.sstatic.net/1uSVx.png"" rel=""nofollow noreferrer"">custom ner model structure</a></p>
<p><strong>But while run that model without docker it works perfectly</strong>. What wrong I have done in this process. Plz help me to resolve the error.</p>
<p>Thank you in advance.</p>
",Named Entity Recognition (NER),spacy custom trianed model config validation error ner incorrect span key extra field permitted running problem whenever try load custom trained ner model spacy inside docker container note using latest spacy version trained ner model using cli command spacy first converting train data format spacy format error throw following check error image hyperlinked config validation error trained model file structure look like custom ner model structure run model without docker work perfectly wrong done process plz help resolve error thank advance
How to use spacy train to add entities to an existing custom NER model? (Spacy v3.0),"<p>I am currently implementing a custom NER model interface where a user can interact with a frontend application to add custom entities to train a spacy model.</p>
<p>I want to use spacy train (CLI) to take an existing model (custom NER model) and add the keyword and entity specified by the user, to that model. (Instead of training the whole model again). I can't find this anywhere in the documentation.</p>
<p>For example, let's say I have a model that is already trained for a custom entity of FOOD. (Pizza, Pasta, Bread, etc…). Now I want to take this existing model, and train it for a new entity called DRINKS with keywords like Coca-Cola, Pepsi, Juice, etc… Using spacy train command for spacy v3.0.</p>
<p>The spacy train command that I am using currently is as follows:</p>
<pre><code>&gt; python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy
</code></pre>
<p>I load the model for prediction using:</p>
<pre><code>&gt; nlp1 = spacy.load(R&quot;.\output\model-best&quot;)
</code></pre>
<p>As of now, I was training the model for new entities manually. Below is the code to find keywords in my training data and output a JSON format for training data (old format).</p>
<pre><code>import re

keyword = [&quot;outages&quot;,&quot;updates&quot;,&quot;negative star&quot;,&quot;worst&quot;]
entity = [&quot;PROBLEM&quot;,&quot;PROBLEM&quot;,&quot;COMPLAINT&quot;,&quot;COMPLAINT&quot;]

train = []

for text in df.text:

    for n in range(0,len(keyword)):
    
        start_index = []
        end_index = []

        start_index = [m.start() for m in re.finditer(keyword[n], str(text))]

        if(start_index):

            end_index = [m+len(keyword[n]) for m in start_index]

            for i in range(0,len(start_index)):

                train.append((text,{&quot;entities&quot;: [(start_index[i],end_index[i],entity[n])]}))

train
</code></pre>
<p>After this, I converted my json format into .spacy format with below code.</p>
<pre><code>from tqdm import tqdm
from spacy.tokens import DocBin

db = DocBin() # create a DocBin object

for text, annot in tqdm(train): # data in previous format
    doc = nlp.make_doc(text) # create doc object from text
    ents = []
    for start, end, label in annot[&quot;entities&quot;]: # add character indexes
        span = doc.char_span(start, end, label=label, alignment_mode=&quot;contract&quot;)
        if span is None:
            print(&quot;Skipping entity&quot;)
        else:
            ents.append(span)
    doc.ents = ents # label the text with the ents
    db.add(doc)

db.to_disk(&quot;./train.spacy&quot;)
</code></pre>
",Named Entity Recognition (NER),use spacy train add entity existing custom ner model spacy v currently implementing custom ner model interface user interact frontend application add custom entity train spacy model want use spacy train cli take existing model custom ner model add keyword entity specified user model instead training whole model find anywhere documentation example let say model already trained custom entity food pizza pasta bread etc want take existing model train new entity called drink keywords like coca cola pepsi juice etc using spacy train command spacy v spacy train command using currently follows load model prediction using wa training model new entity manually code find keywords training data output json format training data old format converted json format spacy format code
dictionary-base text analysis with approximate matching,"<p>I have a database/dictionary of over 2 million names, phases , locations and so on. A sample data is the following:</p>

<pre><code>“A person who never made a mistake never tried anything new”

“Nelson Mandela”

“United States of America”

“I didn't attend the funeral, but I sent a nice letter saying I approved of it.”

“Joseph Campbell”

“Insanity: doing the same thing over and over again and expecting different results”

“United Kingdom of Great Britain and Northern Ireland”
</code></pre>

<p>For each entry, there is an ID. </p>

<p>Now, when a user submit a text to my website, the text will be analyzed and will find all the appearances of names, phases and locations in the text with some forgiveness (approximate matching). For examples: </p>

<ol>
<li>The phase “Insanity: doing the same thing over and over again” will be matched with the complete phase “Insanity: doing the same thing over and over again <strong>and expecting different results</strong>”</li>
<li>The phase “Insanity: doing the same thing over again and expecting different results” will be matched with the complete phase “Insanity: doing the same thing over <strong>and over</strong> again and expecting different results”</li>
</ol>

<p>I don’t know if this is a Named Entity problem as my database has phases that are more than 2 lines long. I also don’t want to find the exact phase, I want to approximate matching.</p>

<p>I am looking for open source solution. Thanks in advance</p>
",Named Entity Recognition (NER),dictionary base text analysis approximate matching database dictionary million name phase location sample data following entry id user submit text website text analyzed find appearance name phase location text forgiveness approximate matching example phase insanity thing matched complete phase insanity thing expecting different result phase insanity thing expecting different result matched complete phase insanity thing expecting different result know named entity problem database ha phase line long also want find exact phase want approximate matching looking open source solution thanks advance
Replace entity with its label in SpaCy,"<p>Is there anyway by SpaCy to replace entity detected by SpaCy NER with its label?
For example:
<strong>I am eating an apple while playing with my Apple Macbook.</strong></p>

<p>I have trained NER model with SpaCy to detect ""FRUITS"" entity and the model successfully detects the first ""apple"" as ""FRUITS"", but not the second ""Apple"".</p>

<p>I want to do post-processing of my data by replacing each entity with its label, so I want to replace the first ""apple"" with ""FRUITS"". The sentence will be ""<strong>I am eating an FRUITS while playing with my Apple Macbook.</strong>""</p>

<p>If I simply use regex, it will replace the second ""Apple"" with ""FRUITS"" as well, which is incorrect. Is there any smart way to do this?</p>

<p>Thanks!</p>
",Named Entity Recognition (NER),replace entity label spacy anyway spacy replace entity detected spacy ner label example eating apple playing apple macbook trained ner model spacy detect fruit entity model successfully detects first apple fruit second apple want post processing data replacing entity label want replace first apple fruit sentence eating fruit playing apple macbook simply use regex replace second apple fruit well incorrect smart way thanks
Named entity recognition which identifies context,"<p>I am using a NER model to extract the treatment (ice, heat, or OTC) present in text, but the treatment has multiple contexts.</p>
<ol>
<li>Patient was advised to use ice packs for their knee at home [Treatment - Homecare]</li>
<li>Patient was given ice packs at the clinic [Treatment - In clinic]</li>
</ol>
<p>The NER model extracts ice pack as Treatment entity in all the above sentences, but how do I make it learn the context and further identify if the treatment is homecare suggestion or in-clinic. The context should be learned based on the surrounding words (proceeding and following both).</p>
<p>What techniques should I use for this use case? I don't want to use rule based techniques on top of NER, I have already used those but I was hoping to do this in a more sophisticated manner.</p>
<p>For the entity recognition, I am using a pre-trained biomedical NER (from Scispacy libaray) and added my custom entities to it using Entity Ruler.</p>
",Named Entity Recognition (NER),named entity recognition identifies context using ner model extract treatment ice heat otc present text treatment ha multiple context patient wa advised use ice pack knee home treatment homecare patient wa given ice pack clinic treatment clinic ner model extract ice pack treatment entity sentence make learn context identify treatment homecare suggestion clinic context learned based surrounding word proceeding following technique use use case want use rule based technique top ner already used wa hoping sophisticated manner entity recognition using pre trained biomedical ner scispacy libaray added custom entity using entity ruler
Negspacy for different entity types in the same spacy pipeline?,"<p>I have a spacy pipeline with NER component, and for the NER component, I have also added a negspacy component so I can detect negations of the detected entities. The NER detects two entity types, DISEASE and CHEMICAL. I want to create separate negation detection rules for each entity type. Not sure how to do that.</p>
<pre><code>## Load the medical NER model
## this model identifies two types of entities -- disease (such as headache) and chemical (drug name such as iburprofen)
nlp=spacy.load('en_ner_bc5cdr_md')  ##this model is only compatible with latest versions of spacy

### the following negation rules only apply to DISEASE entity

nlp.add_pipe(&quot;negex&quot;, 
    config={
            &quot;neg_termset&quot;:{
            &quot;pseudo_negations&quot;: [&quot;might not&quot;, 'not report', 'not recall'],
            &quot;preceding_negations&quot;: [&quot;not&quot;,  &quot;denies&quot;, 'denied', &quot;no&quot;, 'refused', 'never',  'refuses', 'deny', 'negative', 'neg',  'declined'],
            &quot;following_negations&quot;:[&quot;declined&quot;, &quot;0/10&quot;, '0 out of 10'],
            &quot;termination&quot;: [&quot;but&quot;, &quot;however&quot;]
        },
        
        &quot;ent_types&quot;:[&quot;DISEASE&quot;]
        
    }
             
    )

</code></pre>
<p>Is it possible to create separate negation rules for CHEMICAL entity?</p>
",Named Entity Recognition (NER),negspacy different entity type spacy pipeline spacy pipeline ner component ner component also added negspacy component detect negation detected entity ner detects two entity type disease chemical want create separate negation detection rule entity type sure possible create separate negation rule chemical entity
How to save model in spacy for custom NER which is again trainable?,"<p>Let's say I have trained my model with label called &quot;FRUIT&quot; and saved it using <code>nlp.to_disk(&quot;F:/Saved Models/FRUIT_pipeline&quot;)</code></p>
<p>Now if I load this model and train again for label called &quot;SEASON&quot;, all my before training is lost. How do I save it for multiple labels in multiple training?</p>
",Named Entity Recognition (NER),save model spacy custom ner trainable let say trained model label called fruit saved using load model train label called season training lost save multiple label multiple training
spaCy nlp - tag entities in string,"<p>Lets say I have a string and want to mark some entities such as Persons, and Locations.</p>
<pre><code>string = 'My name is John Doe, and I live in USA'
string_tagged = 'My name is [John Doe], and I live in {USA}'
</code></pre>
<p>I want to mark persons with [ ] and locations with { }.</p>
<p>My code:</p>
<pre><code>import spacy    
nlp = spacy.load('en')
doc = nlp(string)
sentence = doc.text
for ent in doc.ents:
    if ent.label_ == 'PERSON':
        sentence = sentence[:ent.start_char] + sentence[ent.start_char:].replace(ent.text, '[' + ent.text + ']', 1)
    elif ent.label_ == 'GPE':
        sentence = sentence[:ent.start_char] + sentence[ent.start_char:].replace(ent.text, '{' + ent.text + '}', 1)

    print(sentence[:ent.start_char] + sentence[ent.start_char:])
</code></pre>
<p>...so with the example string this works fine. But with more complicated sentences I get double quotes around some entities. For the sentence:</p>
<pre><code>string_bug = 'Canada, Canada, Canada, Canada, Canada, Canada'
</code></pre>
<p>returns <code>&gt;&gt; {Canada}, {Canada}, {Canada}, {Canada},  {{Canada}}, Canada</code></p>
<p>The reason why I splitted the sentence string into two was to only replace new words (with higher character positions). I think the bug might be in that I am in looping over <code>doc.ents</code>, so I get the old positions of my string, and the string grows for each loop with new [ ] and {}. But feels like there must be some easier way of dealing with this in spaCy.</p>
",Named Entity Recognition (NER),spacy nlp tag entity string let say string want mark entity person location want mark person location code example string work fine complicated sentence get double quote around entity sentence return reason splitted sentence string two wa replace new word higher character position think bug might looping get old position string string grows loop new feel like must easier way dealing spacy
Explanation/interpretation of the parameters in the spaCy config file,"<p>I have a couple of questions regarding the parameters that we define in the <code>config.cfg</code> file. Although spaCy's docs do try to explain them, I feel that the explanation isn't really descriptive enough and that lots of things are scattered around the docs, making it difficult to find exactly what you need, especially with spaCy v3, (unless I'm looking at wrong parts of the website) which is recent and hence has really less question/answers in the forums.
I'm basically building a Named Entity Recognition (NER) model along with a transformer component. My questions are as follows:</p>
<ol>
<li><p>In the following part (same question for <code>corpora.train</code> also), what is the difference between <code>max_length</code> and <code>limit</code>?</p>
<p>For <code>max_length</code> the docs say &quot;Limitations on training document length&quot;<br />
For <code>limit</code>, the docs say &quot;Limitation on number of training examples&quot;</p>
<p>Aren't they both more or less the same thing? I mean I can limit the number of training examples by limiting the document's length itself, right?</p>
</li>
</ol>
<pre><code>[corpora.dev]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.dev}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null
</code></pre>
<ol start=""2"">
<li>In the below snippet, what is the meaning of one 'step'? I understand <code>max_steps=0</code> means infinite steps. But how do I know how many such 'steps' make one epoch? Also how many example sentences are covered in 1 such step?</li>
</ol>
<pre><code>[training]
train_corpus = &quot;corpora.train&quot;
dev_corpus = &quot;corpora.dev&quot;
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_epochs = 10
max_steps = 0
eval_frequency = 200
frozen_components = []
before_to_disk = null
</code></pre>
<ol start=""3"">
<li>How exactly is the <code>learn_rate</code> being modified in the below snippet of code, during the training process? More specifically, what do <code>total_steps</code> and <code>warmup_steps</code> mean?</li>
</ol>
<pre><code>[training.optimizer.learn_rate]
@schedules = &quot;warmup_linear.v1&quot;
warmup_steps = 250
total_steps = 200
initial_rate = 0.00005
</code></pre>
<ol start=""4"">
<li>Finally, in the CLI output of the training process, What exactly is this '#'? It was mentioned in one of <a href=""https://github.com/explosion/spaCy/discussions/7450#discussioncomment-487000"" rel=""nofollow noreferrer"">GitHub discussions</a> that <em>&quot;The # column is the number of optimization steps (= batches processed)&quot;</em> , but what exactly is this 1 batch or 'optimization step'? If the training process shows me the scores for after 200 such 'batches' how do I interpret it (as in how many example sentences have been processed till that point)?</li>
</ol>
",Named Entity Recognition (NER),explanation interpretation parameter spacy config file couple question regarding parameter define file although spacy doc try explain feel explanation really descriptive enough lot thing scattered around doc making difficult find exactly need especially spacy v unless looking wrong part website recent hence ha really le question answer forum basically building named entity recognition ner model along transformer component question follows following part question also difference doc say limitation training document length doc say limitation number training example le thing mean limit number training example limiting document length right snippet meaning one step understand mean infinite step know many step make one epoch also many example sentence covered step exactly modified snippet code training process specifically mean finally cli output training process exactly wa mentioned one github discussion column number optimization step batch processed exactly batch optimization step training process show score batch interpret many example sentence processed till point
How can I get only text part out of recognised object in Microsoft Speech Service,"<p>Following is my output of speech recognition from file from Microsoft Azure Speech SDK. I want to know how can I extract just the 'text' part from this output rather than complete.</p>
<blockquote>
<p>SpeechRecognitionEventArgs(session_id=e28f6907838640e191f214035d69f5e0, result=SpeechRecognitionResult(result_id=c27fa5b36bcd466f8162ca3c6ce5f935, <strong>text=&quot;Hello good morning, my name is Arihant. How may I help you?&quot;</strong>, reason=ResultReason.RecognizedSpeech))</p>
</blockquote>
<blockquote>
<p>CANCELED SpeechRecognitionCanceledEventArgs(session_id=e28f6907838640e191f214035d69f5e0, result=SpeechRecognitionResult(result_id=5681af6a81994a76a11b7e94307c7c2e, text=&quot;&quot;, reason=ResultReason.Canceled))</p>
</blockquote>
<blockquote>
<p>CLOSING on SessionEventArgs(session_id=e28f6907838640e191f214035d69f5e0)</p>
</blockquote>
",Named Entity Recognition (NER),get text part recognised object microsoft speech service following output speech recognition file microsoft azure speech sdk want know extract text part output rather complete speechrecognitioneventargs session id e f e f f e result speechrecognitionresult result id c fa b bcd f ca c ce f text hello good morning name arihant may help reason resultreason recognizedspeech canceled speechrecognitioncanceledeventargs session id e f e f f e result speechrecognitionresult result id af b e c c e text reason resultreason canceled closing sessioneventargs session id e f e f f e
Named entity recognition with deep Learning model,"<p>How to use named entity recognition using Deep Learning? I want to build a model using DL for named entity recognition.</p>
",Named Entity Recognition (NER),named entity recognition deep learning model use named entity recognition using deep learning want build model using dl named entity recognition
Spacy error in loading pretrained custom model with entity rulers and ner pipeline,"<p>I used a spacy blank model with Gensim custom word vectors. Then I trained the model to get the pipeline in the respective order-</p>
<p>entityruler1, ner1, entity ruler2, ner2</p>
<p>After training it, I saved it in a folder through</p>
<p><code>nlp.to_disk('path to folder')</code></p>
<p>However, if I try to load the same model using
<code>nlp1 = spacy.load('path to folder')</code>
It gives me this error-</p>
<p><code>ValueError: [E109] Model for component 'ner' not initialized. Did you forget to load a model, or forget to call begin_training()?</code></p>
<p>I cannot find any solution online. What might be the reason I am getting this? How do I successfully load and use my pretrained model?</p>
",Named Entity Recognition (NER),spacy error loading pretrained custom model entity ruler ner pipeline used spacy blank model gensim custom word vector trained model get pipeline respective order entityruler ner entity ruler ner training saved folder however try load model using give error find solution online might reason getting successfully load use pretrained model
What are the ways of Key-Value extraction from unstructured text?,"<p>I'm trying to figure out what are the ways (and which of them the best one) of extraction of Values for predefined Keys in the unstructured text?</p>

<p>Input:</p>

<ol>
<li>The doctor prescribed me a drug called favipiravir.</li>
<li>His name is Yury.</li>
<li>Ilya has already told me about that.</li>
<li>The weather is cold today.</li>
<li>I am taking a medicine called nazivin.</li>
</ol>

<p>Key list: ['drug', 'name', 'weather']</p>

<p>Output:</p>

<p>['drug=favipiravir', 'drug=nazivin', 'name=Yury', 'weather=cold']</p>

<p>So, as you can see, in the 3d sentence there is no explicit key 'name' and therefore no value extracted (I think there is the difference with NER). At the same time, 'drug' and 'medicine' are synonyms and we should treat 'medicine' as 'drug' key and extract the value also.</p>

<p>And the next question, what if the key set will be mutable?
Should I use as a base regexp approach because of predefined Keys or there is a way to implement it with supervised learning/NN? (but in this case how to deal with mutable keys?)</p>
",Named Entity Recognition (NER),way key value extraction unstructured text trying figure way best one extraction value predefined key unstructured text input doctor prescribed drug called favipiravir name yury ilya ha already told weather cold today taking medicine called nazivin key list drug name weather output drug favipiravir drug nazivin name yury weather cold see sentence explicit key name therefore value extracted think difference ner time drug medicine synonym treat medicine drug key extract value also next question key set mutable use base regexp approach predefined key way implement supervised learning nn case deal mutable key
Extract product type from a free text product description,"<p>I'm looking to extract <strong>product type</strong> from a <strong>free text product description</strong> for grouping purposes.</p>
<p>The context is a learning exercise from an old Kaggel competition (mercari-price-suggestion-challenge).</p>
<p>Google / Stack Overflow offers multiple directions, but no specific out-of-the-box code (I could find).</p>
<p>The best I was able to come up with (I'm an NLP newbie) is via <strong>spacy</strong> package, parse the product name description, and take the ROOT word as the product type itself, <strong>please see code below</strong>.</p>
<p>The solution seems to provide fair results sometimes, please see results sample below, but I think there should be a better practice for an off the shelf code.</p>
<pre><code>import spacy
nlp = spacy.load('en_core_web_sm')

z = train_data
z = z.assign(product_enitity=z['name'].apply(nlp))
z = z.assign(product_enitity_parse=[([(X, X.dep_) for X in Y]) for Y in z['product_enitity']])

def get_root_word(sent):
    return [tok[0] for tok in sent if (tok[1] == &quot;ROOT&quot;) ]

z = z.assign(root_product_name=[get_root_word(sent) for sent in z['product_enitity_parse']])
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>product_enitity_parse</td>
<td>root_product_name</td>
</tr>
<tr>
<td>Air Jordan Retro 6 Sport Blue</td>
<td>[(Air, compound), (Jordan, compound), (Retro, ROOT), (6, nummod), (Sport, compound), (Blue, appos)]</td>
<td>[Retro]</td>
</tr>
<tr>
<td>Salvatore Ferragamo Belt FREE SHIPPING</td>
<td>[(Salvatore, amod), (Ferragamo, compound), (Belt, npadvmod), (FREE, amod), (SHIPPING, ROOT)]</td>
<td>[SHIPPING]</td>
</tr>
<tr>
<td>Raspberry Cherokee Scrubs top and pants.</td>
<td>[(Raspberry, compound), (Cherokee, compound), (Scrubs, nsubj), (top, ROOT), (and, cc), (pants, c...</td>
<td>[top]</td>
</tr>
<tr>
<td>Uggs size 7</td>
<td>[(Uggs, compound), (size, ROOT), (7, nummod)]</td>
<td>[size]</td>
</tr>
<tr>
<td>Essential Oil Diffuser</td>
<td>[(Essential, amod), (Oil, compound), (Diffuser, ROOT)]</td>
<td>[Diffuser]</td>
</tr>
<tr>
<td>Skylanders</td>
<td>[(Skylanders, ROOT)]</td>
<td>[Skylanders]</td>
</tr>
<tr>
<td>Sono human hair extensions 20 inches</td>
<td>[(Sono, nmod), (human, amod), (hair, nsubj), (extensions, ROOT), (20, nummod), (inches, npadvmod)]</td>
<td>[extensions]</td>
</tr>
<tr>
<td>NWT LulaRoe Girls Mae Dress Size 4</td>
<td>[(NWT, compound), (LulaRoe, compound), (Girls, compound), (Mae, compound), (Dress, compound), (S...</td>
<td>[Size]</td>
</tr>
<tr>
<td>ES Bra and Panty set of 4 (36B)</td>
<td>[(ES, compound), (Bra, nmod), (and, cc), (Panty, conj), (set, ROOT), (of, prep), (4, pobj), ((, ...</td>
<td>[set]</td>
</tr>
<tr>
<td>Free Shipping Skeleton Kids Top</td>
<td>[(Free, amod), (Shipping, compound), (Skeleton, compound), (Kids, compound), (Top, ROOT)]</td>
<td>[Top]</td>
</tr>
</tbody>
</table>
</div>",Named Entity Recognition (NER),extract product type free text product description looking extract product type free text product description grouping purpose context learning exercise old kaggel competition mercari price suggestion challenge google stack overflow offer multiple direction specific box code could find best wa able come nlp newbie via spacy package parse product name description take root word product type please see code solution seems provide fair result sometimes please see result sample think better practice shelf code name product enitity parse root product name air jordan retro sport blue air compound jordan compound retro root nummod sport compound blue appos retro salvatore ferragamo belt free shipping salvatore amod ferragamo compound belt npadvmod free amod shipping root shipping raspberry cherokee scrub top pant raspberry compound cherokee compound scrub nsubj top root cc pant c top uggs size uggs compound size root nummod size essential oil diffuser essential amod oil compound diffuser root diffuser skylanders skylanders root skylanders sono human hair extension inch sono nmod human amod hair nsubj extension root nummod inch npadvmod extension nwt lularoe girl mae dress size nwt compound lularoe compound girl compound mae compound dress compound size e bra panty set b e compound bra nmod cc panty conj set root prep pobj set free shipping skeleton kid top free amod shipping compound skeleton compound kid compound top root top
Determine Transfer Learning Strategy for NER task,"<p>I worked on a Transfer Learning project in which I created a training dataset (labeled) and I used a pre-trained BERT model and fine-tuned it. The project was an NLP project in which I performed customized named entities recognition.
I'm working now on documenting the work so I have to specify which strategy of transfer learning I did.
i found this blog <a href=""https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a"" rel=""nofollow noreferrer"">https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a</a>
but even after reading it I still confused about the strategy and I want to make sure of my choice.</p>
",Named Entity Recognition (NER),determine transfer learning strategy ner task worked transfer learning project created training dataset labeled used pre trained bert model fine tuned project wa nlp project performed customized named entity recognition working documenting work specify strategy transfer learning found blog even reading still confused strategy want make sure choice
Extracting text from a passage using spacy or nltk,"<p>Sorry if this is a repeat but I couldn't find an answer or at least would like to know if there is a clean way to do this.
I have a passage from which I need to extract certain entities.</p>
<p>Any alphanumeric string like: PQ1234, Z123 etc
Any alphanumeric string followed by another number after a space: PQ1234 01, Z123 08
Any alphanumeric string followed by another number after a space: PQ1234 01 02, Z123 07 08. As a concrete example below, the strings in <strong>bold</strong> should be extracted:</p>
<p>01: Once, there was a boy named <strong>AZ009</strong> who became bored when he watched over the village <strong>PQ123 01</strong> sheep grazing on the <strong>B0199</strong>. To entertain himself, he sang out, “<strong>R0199 01 09</strong>! <strong>R0199 01 09</strong>! <strong>R0199 01 09</strong> is chasing the sheep!”</p>
<p>Rest all I want to ignore. I attempted this using spacy's NOUN, PROPN filter along with string functions like isalpha and isdigit to further filter it but it is becoming too rule based and not able to implement it too well.</p>
<p>I am a newbie to NLP and so wanted to know if there is a smarter way or if through some RegEx rule, I can get it done better.</p>
<p>Thanks</p>
",Named Entity Recognition (NER),extracting text passage using spacy nltk sorry repeat find answer least would like know clean way passage need extract certain entity alphanumeric string like pq z etc alphanumeric string followed another number space pq z alphanumeric string followed another number space pq z concrete example string bold extracted wa boy named az became bored watched village pq sheep grazing b entertain sang r r r chasing sheep rest want ignore attempted using spacy noun propn filter along string function like isalpha isdigit filter becoming rule based able implement well newbie nlp wanted know smarter way regex rule get done better thanks
Finding semantically related named entities in text,"<p>I have a set of text documents with tagged named entities like &quot;person&quot;, &quot;organization&quot;, &quot;location&quot;, &quot;product&quot;, &quot;amount&quot;, &quot;price&quot; etc. I have already done fine-tuning of the BERT model to recognize these named entities. But I also need to solve the problem of finding related named entities in the text. For example, let's say we have a part of text like this:</p>
<blockquote>
<p>Hey, <strong>Jack</strong>! There is work for you. <strong>Thomas Smith</strong> of the <strong>Big Corporation</strong> called this morning and ordered <strong>four</strong> <strong>pizzas</strong> for <strong>fifteen dollars</strong>, and <strong>Andy</strong> on <strong>28th Street</strong> ordered <strong>sushi</strong>.</p>
</blockquote>
<p>BERT will find the following named entities and their positions in this text:</p>
<ul>
<li>Jack - person</li>
<li>Thomas Smith - person</li>
<li>Big Corporation - organization</li>
<li>four - amount</li>
<li>pizzas - product</li>
<li>fifteen dollars - price</li>
<li>Andy - person</li>
<li>28th Street - location</li>
<li>sushi - product</li>
</ul>
<p>I need a model that can split these entities into groups, which contain semantically related entities as follows:</p>
<ul>
<li>{Jack}</li>
<li>{Thomas Smith, Big Corporation, four, pizzas, fifteen dollars}</li>
<li>{Andy, 28th Street, sushi}</li>
</ul>
<p>Is it possible to solve such a problem if I have a training dataset with links between entities? Is there any neural network architecture that can be used on top of the BERT model embeddings for solving this problem? Maybe a graph model?</p>
",Named Entity Recognition (NER),finding semantically related named entity text set text document tagged named entity like person organization location product amount price etc already done fine tuning bert model recognize named entity also need solve problem finding related named entity text example let say part text like hey jack work thomas smith big corporation called morning ordered four pizza fifteen dollar andy th street ordered sushi bert find following named entity position text jack person thomas smith person big corporation organization four amount pizza product fifteen dollar price andy person th street location sushi product need model split entity group contain semantically related entity follows jack thomas smith big corporation four pizza fifteen dollar andy th street sushi possible solve problem training dataset link entity neural network architecture used top bert model embeddings solving problem maybe graph model
Spacy ValueError [E103] Trying to set conflicting doc.ents while creating custom NER,"<p>As I am new to spacy, i am stuck with one problem statement. Read a lot of articles but doesn't help. I am constantly getting &quot;[E103] Trying to set conflicting doc.ents: &quot;. My code looks like this :</p>
<pre><code>train_data = ('We report the use of pamidronate for acute, severe hypercalcemia secondary to iatrogenic vitamin D poisoning.', {'entities': [(328, 348, 'ADE'), (373, 382, 'DRUG')]}), ('We report the use of pamidronate for acute, severe hypercalcemia secondary to iatrogenic vitamin D poisoning.', {'entities': [(373, 392, 'ADE'), (373, 382, 'DRUG')]})

for text,annotation in train_data:
  for ent in annotation.get('entities'):
    ner.add_label(ent[2])

disable_pipes =[pipe for pipe in nlp.pipe_names if pipe != 'ner']

with nlp.disable_pipes(*disable_pipes):
    #nlp.vocab.vectors.name = 'spacy_model' # without this, spaCy throws an &quot;unnamed&quot; error
    optimizer = nlp.begin_training()
    
    for itr in range(100):
        
        try:
        
            random.shuffle(train_data) # shuffle the training data before each iteration
            losses = {}
            
            batches = minibatch(train_data, size = compounding(16.0, 64.0, 1.5))

            for batch in batches:
                
                texts, annotations = zip(*batch)
                
                nlp.update(          
                    texts,
                    annotations,
                    drop = 0.5,  
                    sgd = optimizer,
                    losses = losses)
            
            print('Interation = '+str(itr))
            print('Losses = '+str(losses))
        
        except ValueError as ve:
            
            print(ve)
          
</code></pre>
<p>[E103] Trying to set conflicting doc.ents: '(373, 392, 'ADE')' and '(373, 382, 'DRUG')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap.</p>
<p>Please help me with how can I get this error sorted. If I need to remove this kind of record from training data then it is a huge number. How can I achieve this.</p>
",Named Entity Recognition (NER),spacy valueerror e trying set conflicting doc ents creating custom ner new spacy stuck one problem statement read lot article help constantly getting e trying set conflicting doc ents code look like e trying set conflicting doc ents ade drug token part one entity make sure entity setting overlap please help get error sorted need remove kind record training data huge number achieve
Author extraction in newspaper example is not working,"<p>I'm trying to use newspaper3k to extract speaker names from webpages containing speeches with no luck. Following the documentation of the package, <code>article.authors</code> seems to always return an empty list.</p>
<p>Using the example in the docs <a href=""https://newspaper.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">here</a>.</p>
<pre><code>In:

from newspaper import Article

url = 'http://fox13now.com/2013/12/30/new-year-new-laws-obamacare-pot-guns-and-drones/'
article = Article(url)
article.download()
article.parse()
article.authors
</code></pre>
<p>returns</p>
<pre><code>Out: []
</code></pre>
<p>Instead of the expected</p>
<pre><code>Out: ['Leigh Ann Caldwell', 'John Honway']
</code></pre>
<p>It's not working for many other examples too.</p>
",Named Entity Recognition (NER),author extraction newspaper example working trying use newspaper k extract speaker name webpage containing speech luck following documentation package seems always return empty list using example doc return instead expected working many example
Spacy - Use two trainable components with two different datasets,"<p>I was wondering if it is possible to train two trainable components in Spacy with two different datasets ?
In fact, I would like to use the NER and the text classifier but since the train datasets for these two components should be annotated differently so I don't know how can I train both components at once...</p>
<p>Should I train each task in a separate pipeline and assemble both pipelines at the end ?
Or should I train the NER, package this pipeline and then use this package as input to train the text classifier ?</p>
<p>Many thanks in advance for your help</p>
",Named Entity Recognition (NER),spacy use two trainable component two different datasets wa wondering possible train two trainable component spacy two different datasets fact would like use ner text classifier since train datasets two component annotated differently know train component train task separate pipeline assemble pipeline end train ner package pipeline use package input train text classifier many thanks advance help
Create a NER dictionary from a given text,"<p>I have the following variable</p>
<pre class=""lang-py prettyprint-override""><code>data = (&quot;Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country. Many people have been killed that day.&quot;,
        {&quot;entities&quot;: [(48, 54, 'Category 1'), (77, 81, 'Category 1'), (111, 118, 'Category 2'), (150, 173, 'Category 3')]})
</code></pre>
<p><code>data[1]['entities'][0] = (48, 54, 'Category 1')</code> stands for <code>(start_offset, end_offset, entity)</code>.</p>
<p>I want to read each word of <code>data[0]</code> and tag it according to <code>data[1]</code> entities. I am expecting to have as final output,</p>
<pre class=""lang-py prettyprint-override""><code>{
'Thousands': 'O', 
'of': 'O',
'demonstrators': 'O',
'have': 'O',
'marched': 'O',
'through': 'O',
'London': 'S-1',
'to': 'O', 
'protest': 'O', 
'the': 'O', 
'war': 'O', 
'in': 'O', 
'Iraq': 'S-1',
'and': 'O' 
'demand': 'O', 
'the': 'O', 
'withdrawal': 'O', 
'of': 'O', 
'British': 'S-2', 
'troops': 'O', 
'from': 'O',
'that': 'O', 
'country': 'O',
'.': 'O',
'Many': 'O', 
'people': 'S-3', 
'have': 'B-3', 
'been': 'B-3', 
'killed': 'E-3', 
'that': 'O', 
'day': 'O',
'.': 'O'
}
</code></pre>
<p>Here, 'O' stands for 'OutOfEntity', 'S' stands for 'Start', 'B' stands for 'Between', and 'E' stands for 'End' and are unique for every given text.</p>
<hr />
<p>I tried the following:</p>
<pre class=""lang-py prettyprint-override""><code>entities = {}
offsets = data[1]['entities']
for entity in offsets:
    entities[data[0][entity[0]:entity[1]]] = re.findall('[0-9]+', entity[2])[0]

tags = {}
for key, value in entities.items():
    entity = key.split()
    if len(entity) &gt; 1:
        bEntity = entity[1:-1]
        tags[entity[0]] = 'S-'+value
        tags[entity[-1]] = 'E-'+value
        for item in bEntity:
            tags[item] = 'B-'+value
    else:
        tags[entity[0]] = 'S-'+value
</code></pre>
<p>The output will be</p>
<pre class=""lang-py prettyprint-override""><code>{'London': 'S-1',
 'Iraq': 'S-1',
 'British': 'S-2',
 'people': 'S-3',
 'killed': 'E-3',
 'have': 'B-3',
 'been': 'B-3'}
</code></pre>
<p>From this point, I am stuck on how to deal with 'O' entities. Also, I want to build more efficient and readable code. I think dictionary data structure is not going to work more efficiently because I can have the same words which they'll be as keys.</p>
",Named Entity Recognition (NER),create ner dictionary given text following variable stand want read word tag according entity expecting final output stand outofentity stand start b stand e stand end unique every given text tried following output point stuck deal entity also want build efficient readable code think dictionary data structure going work efficiently word key
How to identify custom keywords of sentence and assign custom names in new column?,"<p>I'm new to the NLP NER (named-entity recognition) world (and programming in general) and am looking for some guidance with starting/knowing how to complete a project.</p>
<p>I have an excel file with about 5 columns of multiple sentences in each row (about 15000 rows). Each row has several paragraphs of words from people filling out a survey.</p>
<p>I want to iterate through each row to identify several certain words, like &quot;hot&quot;, &quot;cold&quot;, &quot;lukewarm&quot;, &quot;ball&quot;, &quot;paper&quot;, etc.</p>
<p>If any of these words are found in a sentence, I want to create a new word in a new column next to it to represent it - for example, words like &quot;hot&quot;, &quot;cold&quot;, and &quot;lukewarm&quot; found in the sentences would be given a new word like &quot;temperature&quot; in a new column; &quot;ball&quot; or &quot;bat&quot; would be given a new word &quot;toy&quot; in a new column; &quot;paper&quot; would be given a new word &quot;supply&quot; in a new column, and so on.</p>
<p>In addition, if there is more than one word found in each sentence, each new word would be inserted in the new cell/column separated by a comma. So if &quot;hot&quot;, &quot;ball&quot;, and &quot;paper&quot; were all in the same sentence, the new column would have <code>&quot;temperature&quot;,&quot;toy&quot;,&quot;supply&quot;</code>. So basically, I don't want to remove any words from the sentences - I want to add new tags/words in a new column next to them.</p>
<p>I need to do this in Python. So far I've been looking using at spacy.io, nltk, and StanfordNERTagger, but if there's a simpler way, that would be great too. There is some stuff/example online such as <a href=""https://aihub.cloud.google.com/u/0/p/products%2F2290fc65-0041-4c87-a898-0289f59aa8ba"" rel=""nofollow noreferrer"">https://aihub.cloud.google.com/u/0/p/products%2F2290fc65-0041-4c87-a898-0289f59aa8ba</a> and <a href=""https://spacy.io/usage/linguistic-features#named-entities"" rel=""nofollow noreferrer"">https://spacy.io/usage/linguistic-features#named-entities</a> but to be honest, I'm not sure where to start so I am looking for some direction. Here are some examples but how do I customize this to specific tags and keywords that I need?</p>
<pre><code>!pip install -U spacy
!python -m spacy download en_core_web_sm
import pandas as pd
import spacy
from spacy.lang.en import English
from spacy import displacy
from spacy.util import minibatch, compounding
import random
import time
import warnings
warnings.filterwarnings(&quot;ignore&quot;)
nlp = spacy.load('en')
TEST_DATA, _ = load_data_spacy(&quot;data/test.txt&quot;)

    test_sentences = [x[0] for x in TEST_DATA[0:15]] # extract the sentences from [sentence, entity]
    for x in test_sentences:
        doc = nlp(x)
        displacy.render(doc, jupyter = True, style = &quot;ent&quot;)
    warnings.filterwarnings(&quot;default&quot;)

def timer(method):
    def timed(*args, **kw):
        ts = time.time()
        result = method(*args, **kw)
        te = time.time()
        print(&quot;Completed in {} seconds&quot;.format(int(te - ts)))
        return result
    return timed

# Data must be of the form (sentence, {entities: [start, end, label]})
@timer
def train_spacy(train_data, labels, iterations, dropout = 0.2, display_freq = 1):
    ''' Train a spacy NER model, which can be queried against with test data
   
    train_data : training data in the format of (sentence, {entities: [(start, end, label)]})
    labels : a list of unique annotations
    iterations : number of training iterations
    dropout : dropout proportion for training
    display_freq : number of epochs between logging losses to console
    '''
    nlp = spacy.blank('en')
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner)
   
    # Add entity labels to the NER pipeline
    for i in labels:
        ner.add_label(i)

    # Disable other pipelines in SpaCy to only train NER
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):
        nlp.vocab.vectors.name = 'spacy_model' # without this, spaCy throws an &quot;unnamed&quot; error
        optimizer = nlp.begin_training()
        for itr in range(iterations):
            random.shuffle(train_data) # shuffle the training data before each iteration
            losses = {}
            batches = minibatch(train_data, size = compounding(4., 32., 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(          
                    texts,
                    annotations,
                    drop = dropout,  
                    sgd = optimizer,
                    losses = losses)
            if itr % display_freq == 0:
                print(&quot;Iteration {} Loss: {}&quot;.format(itr + 1, losses))
    return nlp

# Train (and save) the NER model
ner = train_spacy(TRAIN_DATA, LABELS,6)
ner.to_disk(&quot;models/spacy_example&quot;)
</code></pre>
<p>Thanks in advance.</p>
",Named Entity Recognition (NER),identify custom keywords sentence assign custom name new column new nlp ner named entity recognition world programming general looking guidance starting knowing complete project excel file column multiple sentence row row row ha several paragraph word people filling survey want iterate row identify several certain word like hot cold lukewarm ball paper etc word found sentence want create new word new column next represent example word like hot cold lukewarm found sentence would given new word like temperature new column ball bat would given new word toy new column paper would given new word supply new column addition one word found sentence new word would inserted new cell column separated comma hot ball paper sentence new column would basically want remove word sentence want add new tag word new column next need python far looking using spacy io nltk stanfordnertagger simpler way would great stuff example online honest sure start looking direction example customize specific tag keywords need thanks advance
How to train a spaCy model with line number as a feature?,"<p>I'm a newbie to nlp and <a href=""https://spacy.io/"" rel=""nofollow noreferrer"">spaCy</a> and I'm working on a project for extracting person and company names from business cards.</p>

<p>In order to extract text I am using a decent OCR function that I've made which gives me something like this:</p>

<pre><code>Sunny J. Mistry
Product Design Engineer

Apple
5 Infinite Loop, MS 305-1PH
Cupertino, CA 95014

T 408 974-5339
M 925 548-4585
sjmistry@apple.com
www.apple.com
</code></pre>

<p>At first I was trying process line by line using the default English NER for the job and soon realized that it's not enough.  </p>

<p>Eventually I've decided to create my own custom NER that will be trained with information about the position of text.</p>

<p>I haven't found any information in the official documentation on how to add custom features for the training data like line numbers, but I've found this <a href=""https://support.prodi.gy/t/incorporating-custom-position-feature-into-ner/160"" rel=""nofollow noreferrer"">answer</a> and <a href=""https://github.com/explosion/spaCy/blob/master/examples/training/ner_multitask_objective.py"" rel=""nofollow noreferrer"">example</a> of <strong>Matthew Honnibal</strong> which suggested to use a multi-task objective in order to train a model with a costume feature.</p>

<p>I'm still not sure:</p>

<ol>
<li><p>How the training data should look like?</p></li>
<li><p>How do I use spaCy's API to add a custom feature to the training process?</p></li>
<li><p>Is multi-task objective the right tool to train this kind of model?</p></li>
</ol>
",Named Entity Recognition (NER),train spacy model line number feature newbie nlp spacy working project extracting person company name business card order extract text using decent ocr function made give something like first wa trying process line line using default english ner job soon realized enough eventually decided create custom ner trained information position text found information official documentation add custom feature training data like line number found answer example matthew honnibal suggested use multi task objective order train model costume feature still sure training data look like use spacy api add custom feature training process multi task objective right tool train kind model
Can aws comprehend be used in splitting documents to sentences?,"<p>I started to try aws comprehend. One thing I noticed is that the sentences in the document will affect the sentiment analysis and entity extraction results especially when mixed sentiment sentences exist or some sentences are not capitalized in the document. So correctly splitting the sentences is an important step. However, I can't find an API in comprehend that splits the document in sentences. Is it because comprehend doesn't have the step? If there is, could someone points out how to obtain the splitting results? </p>

<p>BTW, I tried Stanford coreNLP and Google Language Cloud. They both make mistakes in some cases. </p>
",Named Entity Recognition (NER),aws comprehend used splitting document sentence started try aws comprehend one thing noticed sentence document affect sentiment analysis entity extraction result especially mixed sentiment sentence exist sentence capitalized document correctly splitting sentence important step however find api comprehend split document sentence comprehend step could someone point obtain splitting result btw tried stanford corenlp google language cloud make mistake case
How to extract and create new columns from specific match,"<p>I have a column <strong>bike_name</strong> and I want to know the easiest way to split it into <strong>year</strong> and <strong>CC</strong>.</p>
<p><strong>CC</strong> should contain the numeric data attached before the word <em>cc</em>. In some cases, where cc is not available, it should remain blank.</p>
<p>While <strong>year</strong> contains just the year in the last word.</p>
<pre><code>TVS Star City Plus Dual Tone 110cc 2018
Royal Enfield Classic 350cc 2017
Triumph Daytona 675R 2013
TVS Apache RTR 180cc 2017
Yamaha FZ S V 2.0 150cc-Ltd. Edition 2018
Yamaha FZs 150cc 2015
</code></pre>
",Named Entity Recognition (NER),extract create new column specific match column bike name want know easiest way split year cc cc contain numeric data attached word cc case cc available remain blank year contains year last word
How to extract a custom list of entities from a text file?,"<p>I have a list of entities which look something like this:</p>
<pre><code>[&quot;Bluechoice HMO/POS&quot;, &quot;Pathway X HMO/PPO&quot;, &quot;HMO&quot;, &quot;Indemnity/Traditional Health Plan/Standard&quot;]
</code></pre>
<p>It's not the exhaustive list, there are other similar entries.</p>
<p>I want to extract these entities, if present, from a text file (with over 30 pages of information). The crunch here is that this text file is generated using OCR and thus might not contain the exact entries. That is, for example, it might have:</p>
<pre><code>&quot;Out of all the entries the user made, BIueChoise HMOIPOS is the most prominent&quot;
</code></pre>
<p>Notice the spelling mistake in &quot;BIueChoise HMOIPOS&quot; w.r.t. &quot;Bluechoice HMO/POS&quot;.</p>
<p>I want those entities which are present in the text file even if the corresponding words do not match perfectly.</p>
<p>Any help, be it an algorithm or an approach, is welcomed. Thanks a lot!</p>
",Named Entity Recognition (NER),extract custom list entity text file list entity look something like exhaustive list similar entry want extract entity present text file page information crunch text file generated using ocr thus might contain exact entry example might notice spelling mistake biuechoise hmoipos w r bluechoice hmo po want entity present text file even corresponding word match perfectly help algorithm approach welcomed thanks lot
How to use Spacy nlp custom ner to identity 2 types of docs at once,"<p>I want to make a SPACY ner model that identifies and uses tags depending on what doc type it is.</p>
<p>The input is in json format. Example-</p>
<pre><code>{&quot;text&quot;:{&quot;a&quot;:&quot;ABC DEF.&quot;,&quot;b&quot;:&quot;CDE FG.&quot;},
  &quot;annotations&quot;:[
    {&quot;start&quot;:0,&quot;end&quot;:3,&quot;doc_type&quot;:&quot;a&quot;,&quot;label&quot;:{&quot;text&quot;:&quot;FIRST&quot;},&quot;text&quot;:&quot;ABC&quot;}, 
    {&quot;start&quot;:4,&quot;end&quot;:6,&quot;doc_type&quot;:&quot;b&quot;,&quot;label&quot;:{&quot;text&quot;:&quot;SECOND&quot;},&quot;text&quot;:&quot;FG&quot;}
  ]
}
</code></pre>
<p>In this I want the model to identify that the 1st text is of type &quot;a&quot; so the text should be tagged with tag FIRST. Similarly second text is of type &quot;b&quot; so it the ner must be SECOND</p>
<p>How can I go about this problem? Thanks!</p>
",Named Entity Recognition (NER),use spacy nlp custom ner identity type doc want make spacy ner model identifies us tag depending doc type input json format example want model identify st text type text tagged tag first similarly second text type b ner must second go problem thanks
NLP: How to train a spaCy NER Model using GoldParse objects,"<p>I am trying to train a spaCy NER model using GoldParse objects. This is what I have done:</p>
<p><em>Adding extra labels to NER model</em></p>
<pre><code>add_ents = ['A1', 'B1', 'C1', 'D1', 'E1', 'F1', 'G1'] # sample labels

# Create a pipe if it does not exist
if &quot;ner&quot; not in nlp.pipe_names:
    ner = nlp.create_pipe(&quot;ner&quot;) 
    nlp.add_pipe(ner)
else:
    ner = nlp.get_pipe(&quot;ner&quot;)

for e in add_ents:
    ner.add_label(e)
</code></pre>
<p><em>Training the NER Model</em></p>
<pre><code>other_pipes = [pipe for pipe in nlp.pipe_names if pipe != &quot;ner&quot;]
model = None # Since we training a fresh model not a saved model
with nlp.disable_pipes(*other_pipes):  # only train ner
    if model is None:
        optimizer = nlp.begin_training()
    else:
        optimizer = nlp.resume_training()
    for i in range(20):
        loss = {}
        nlp.update(X, y,  sgd=optimizer, drop=0.0, losses=loss)
        print(&quot;Loss: &quot;, loss)
</code></pre>
<p>Here X is a list of Doc objects and y is a list of corresponding GoldParse objects. While executing I am running into the following error:</p>
<pre><code>nn_parser.pyx in spacy.syntax.nn_parser.Parser.update()

nn_parser.pyx in spacy.syntax.nn_parser.Parser._init_gold_batch()

ner.pyx in spacy.syntax.ner.BiluoPushDown.preprocess_gold()

ner.pyx in spacy.syntax.ner.BiluoPushDown.lookup_transition()

ValueError: 'A1' is not in list
</code></pre>
<p>I tried searching for the solution but couldn't find anything relevant. Is there a way to fix this issue?</p>
",Named Entity Recognition (NER),nlp train spacy ner model using goldparse object trying train spacy ner model using goldparse object done adding extra label ner model training ner model x list doc object list corresponding goldparse object executing running following error tried searching solution find anything relevant way fix issue
How do i import the saved module in Google Colab?,"<p>I was build a NER module using Spacy in Google Colab. I saved it to the disk using nlp.to_disk() function.</p>
<p><code>nlp.to_disk(&quot;RCM.model&quot;)</code></p>
<p>This module is saved under the files. How should i import the RCM module for testing purpose?</p>
<p>i have tried the below code but it didn't work.</p>
<pre><code>from google.colab import drive
my_module = drive.mount('/content/RCM.model', force_remount=True)
</code></pre>
",Named Entity Recognition (NER),import saved module google colab wa build ner module using spacy google colab saved disk using nlp disk function module saved file import rcm module testing purpose tried code work
Elastic Architecture Full Text Searching on 1 million file&#39;s content,"<p><strong>Summary</strong></p>
<p>I am trying to design an elastic index(s) that will provide a solid foundation for indexing 1,000,000+ Files and full text searching on the contents. New files will be continuously added after the initial digitization process.</p>
<p><strong>Use Case</strong></p>
<p>Various File Types (Pdf, outlook email, mp3, txt, jpeg of handwritten things, ..etc) need to be searchable by their contents and meta-data. Users want to manually tag relationships between documents. ex Document A -&gt; contains information about -&gt; Document B. Users want to be able to see related/similar texts. Users want Named Entity Recognition on the text contents. The physical files are already stored on an external computer just waiting to be processed.</p>
<p><strong>Implementation</strong></p>
<ol>
<li>File Content extraction pipeline using Apache Tika</li>
<li>NER using Spacy</li>
<li>Upload File Contents + NER Tags to Elastic</li>
<li>Eventually we would run our own search models to gain better search insights + data science.</li>
</ol>
<p>How do I best store my extracted contents to fit the needs of the user and have a scalable foundation? Is it better to run our trained Named Entity Recognition on initial index or after text extraction has been uploaded to elastic?</p>
<p>Or does it make more sense to use an existing solution from below to not reinvent the wheel?</p>
<p><a href=""https://github.com/dadoonet/fscrawler"" rel=""nofollow noreferrer"">https://github.com/dadoonet/fscrawler</a></p>
<p><a href=""https://github.com/deepset-ai/haystack"" rel=""nofollow noreferrer"">https://github.com/deepset-ai/haystack</a></p>
<p><a href=""https://github.com/RD17/ambar"" rel=""nofollow noreferrer"">https://github.com/RD17/ambar</a></p>
",Named Entity Recognition (NER),elastic architecture full text searching million file content summary trying design elastic index provide solid foundation indexing file full text searching content new file continuously added initial digitization process use case various file type pdf outlook email mp txt jpeg handwritten thing etc need searchable content meta data user want manually tag relationship document ex document contains information document b user want able see related similar text user want named entity recognition text content physical file already stored external computer waiting processed implementation file content extraction pipeline using apache tika ner using spacy upload file content ner tag elastic eventually would run search model gain better search insight data science best store extracted content fit need user scalable foundation better run trained named entity recognition initial index text extraction ha uploaded elastic doe make sense use existing solution reinvent wheel
Name of training and test data files in NLP (BioBERT GitHub repo),"<p>I'm reading the <code>README.md</code> file of the <a href=""https://github.com/dmis-lab/biobert#named-entity-recognition-ner"" rel=""nofollow noreferrer"">BioBERT GitHub repo</a>:</p>
<blockquote>
<p>Let $NER_DIR indicate a folder for a single NER dataset which contains <strong>train_dev.tsv, train.tsv, devel.tsv and test.tsv</strong>. Also, set $OUTPUT_DIR as a directory for NER outputs (trained models, test predictions, etc). For example, when fine-tuning on the NCBI disease corpus,</p>
</blockquote>
<p>I don't understand what's the point of using 4 different files for training and testing. To me, only 2 are needed: <code>train.tsv</code> and <code>test.tsv</code> (and optionally <code>valid.tsv</code>).</p>
<p>Could someone explain the meaning of these files and why they are needed?</p>
",Named Entity Recognition (NER),name training test data file nlp biobert github repo reading file biobert github repo let ner dir indicate folder single ner dataset contains train dev tsv train tsv devel tsv test tsv also set output dir directory ner output trained model test prediction etc example fine tuning ncbi disease corpus understand point using different file training testing needed optionally could someone explain meaning file needed
how to unpickle an nlp model,"<p>I have trained a spacy blank NER NLP model and I want to save it in order to use it in an application, I thought about pickling it
I managed to that using those two lines</p>
<pre><code>import pickle
pickle.dump(nlp, open( &quot;nlp.p&quot;, &quot;wb&quot; ))
</code></pre>
<p>but when I wanted to use it I couldn't load it
I tried</p>
<pre><code>pickle_in = open('/content/nlp.p', 'rb')
pickle_clf = pickle.load(pickle_in)
</code></pre>
<p>it prints</p>
<pre><code>EOFError: Ran out of input
</code></pre>
<p>how can I unpickle it and test it on a text</p>
",Named Entity Recognition (NER),unpickle nlp model trained spacy blank ner nlp model want save order use application thought pickling managed using two line wanted use load tried print unpickle test text
VSCode BERT ValueError: Unable to access local path,"<p>I have written the code for an entity extraction model using <code>bert</code> but when I run the <code>train.py</code> file I get a value error.</p>
<p>This is the structure of my code with the configuration file in VSCode, I have downloaded bert models from <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">here</a></p>
<p><a href=""https://i.sstatic.net/RqHGB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RqHGB.png"" alt=""enter image description here"" /></a></p>
<p><strong>Error</strong></p>
<pre><code>&gt;&gt; (myenv) PS D:\Transformers\bert-entity-extraction&gt; python src/train.py

Configuration Complete!
Traceback (most recent call last):
  File &quot;src/train.py&quot;, line 83, in &lt;module&gt;
    model = EntityModel(num_tag = num_tag, num_pos = num_pos)
  File &quot;D:\Transformers\bert-entity-extraction\src\model.py&quot;, line 25, in __init__      
    self.bert = transformers.BertModel.from_pretrained(config.BASE_MODEL_PATH)
  File &quot;C:\Users\hp\anaconda3\envs\myenv\lib\site-packages\transformers\modeling_utils.py&quot;, line 1080, in from_pretrained
    **kwargs,
  File &quot;C:\Users\hp\anaconda3\envs\myenv\lib\site-packages\transformers\configuration_utils.py&quot;, line 427, in from_pretrained
    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File &quot;C:\Users\hp\anaconda3\envs\myenv\lib\site-packages\transformers\configuration_utils.py&quot;, line 492, in get_config_dict
    user_agent=user_agent,
  File &quot;C:\Users\hp\anaconda3\envs\myenv\lib\site-packages\transformers\file_utils.py&quot;, line 1289, in cached_path
    raise ValueError(f&quot;unable to parse {url_or_filename} as a URL or as a local path&quot;)
ValueError: unable to parse D:\Transformers\bert-entity-extraction\input\bert-base-uncased_L-12_H-768_A-12\config.json as a URL or as a local path
</code></pre>
<p>How to fix this?</p>
",Named Entity Recognition (NER),vscode bert valueerror unable access local path written code entity extraction model using run file get value error structure code configuration file vscode downloaded bert model error fix
NER model for lyrics[rap],"<p>I am looking for any NER model train to extract entity from rap lyrics. Presently I am using Spacy models for NER, but there's a lot of misclassification. This is because spacy models are pretrained on newspaper articles. So, the initial question; Is there any model that I can use for NER on rap lyrics.</p>
<p>This is how the present spacy model classifying some of the words</p>
<blockquote>
<p>('kanye west', 'GPE'),('2pac', 'PRODUCT'),('hoochie coochie', 'ORG'),('valley', 'LOC'),('talkin', 'NORP'),('nothin', 'PERSON'),('100k', 'PRODUCT')</p>
</blockquote>
<p>(In case if u are wondering what lyrics are these, these are from Kendrick Lamar)</p>
",Named Entity Recognition (NER),ner model lyric rap looking ner model train extract entity rap lyric presently using spacy model ner lot misclassification spacy model pretrained newspaper article initial question model use ner rap lyric present spacy model classifying word kanye west gpe pac product hoochie coochie org valley loc talkin norp nothin person k product case u wondering lyric kendrick lamar
Converting NER training data to Spacy training data format,"<p>I am creating an Indonesian NER model using Spacy. I'm using training data from <a href=""https://raw.githubusercontent.com/yohanesgultom/nlp-experiments/master/data/ner/training_data.txt"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/yohanesgultom/nlp-experiments/master/data/ner/training_data.txt</a></p>
<p>Above training data using this Tag format:</p>
<pre><code>Sementara itu Pengamat Pasar Modal &lt;ENAMEX TYPE=&quot;PERSON&quot;&gt;Dandossi Matram&lt;/ENAMEX&gt; mengatakan,
</code></pre>
<p>I wanted to convert this training data to Spacy format that is:</p>
<pre><code>[('Sementara itu Pengamat Pasar Modal Dandossi Matram mengatakan,',{&quot;entities:&quot;([35, 51, 'PERSON'])})]
</code></pre>
<p>I'm still new to Python library, any idea how to convert the train data? Or any idea to use which library?</p>
<p>Thank you.</p>
",Named Entity Recognition (NER),converting ner training data spacy training data format creating indonesian ner model using spacy using training data training data using tag format wanted convert training data spacy format still new python library idea convert train data idea use library thank
Spacy 3.0 Training custom NER --&gt; Validation of this custom NER model,"<p>I trained a custom SpaCy Named entity recognition model to detect biased words in job description. Now that I trained 8 variantions (using different base model, training model, and pipeline setting), I want to evaluate which model is performing best.</p>
<p>But.. I can't find any documentation on the validation of these models.
There are some numbers of recall, f1-score and precision on the meta.json file, in the output folder, but that is no sufficient.</p>
<p>Anyone knows how to validate or can link me to the correct documentation? The documentation seem nowhere to be found.</p>
<p>NOTE: Talking about SpaCy V3.x</p>
",Named Entity Recognition (NER),spacy training custom ner validation custom ner model trained custom spacy named entity recognition model detect biased word job description trained variantions using different base model training model pipeline setting want evaluate model performing best find documentation validation model number recall f score precision meta json file output folder sufficient anyone know validate link correct documentation documentation seem nowhere found note talking spacy v x
How to extract a keyword and its page number from a PDF file using NLP?,"<p><a href=""https://i.sstatic.net/UudYK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UudYK.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/t3Rb8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/t3Rb8.png"" alt=""enter image description here"" /></a></p>
<p>In the above PDF file, my code has to extract keywords and Table Names like <code>Table 1</code>, <code>Table 2</code>, Title with Bold Letters like <code>INTRODUCTION</code>, <code>CASE PRESENTATION</code> from all pages from the given PDF.</p>
<p>Wrote a small program to extract texts from the PDF file</p>
<pre><code>punctuations = ['(',')',';',':','[',']',',','^','=','-','!','.','{','}','/','#','^','&amp;']

stop_words = stopwords.words('English')

keywords = [word for word in tokens if not word in stop_words and not word in punctuations]

print(keywords)
</code></pre>
<p>and the output I got was as below</p>
<p><a href=""https://i.sstatic.net/3dQWp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3dQWp.png"" alt=""enter image description here"" /></a></p>
<p>From the above output, How to extract keywords like <code>INTRODUCTION</code>, <code>CASE PRESENTATION</code>, <code>Table 1</code> along with the page number and save them in a output file.</p>
<p><strong>Output Format</strong></p>
<pre><code>INTRODUCTION in Page 1

CASE PRESENTATION in Page 3

Table 1 (Descriptive Statistics) in Page 5
</code></pre>
<p>Need help in obtaining output of this format.</p>
<p><strong>Code</strong></p>
<pre class=""lang-py prettyprint-override""><code>def main():

        file_name = open(&quot;Test1.pdf&quot;,&quot;rb&quot;)
        readpdf = PyPDF2.PdfFileReader(file_name)
    

    #Parse thru each page to extract the texts
        pdfPages = readpdf.numPages
        count=0
        text=&quot;&quot;
        print()
        #The while loop will read each page.
        while count &lt; pdfPages:
            pageObj = readpdf.getPage(count)
            count +=1
            text += pageObj.extractText()

        #This if statement exists to check if the above library returned words. It's done because PyPDF2 cannot read scanned files.
        if text != &quot;&quot;:
            text = text
        #If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text.
        else:
            text = textract.process(fileurl, method='tesseract', language='eng')

        #PRINT THE TEXT EXTRACTED FROM GIVEN PDF
        #print(text)

        #The function will break text into individual words
    
        tokens = word_tokenize(text)
        #print('TOKENS')
        #print(tokens)

        #Clean the punctuations not required.
        punctuations = ['(',')',';',':','[',']',',','^','=','-','!','.','{','}','/','#','^','&amp;']
        
        stop_words = stopwords.words('English')
        
        keywords = [word for word in tokens if not word in stop_words and not word in punctuations]
   
        print(keywords)
</code></pre>
",Named Entity Recognition (NER),extract keyword page number pdf file using nlp pdf file code ha extract keywords table name like title bold letter like page given pdf wrote small program extract text pdf file output got wa output extract keywords like along page number save output file output format need help obtaining output format code
Using spaCy 3.0 to convert data from old Spacy v2 format to the brand new Spacy v3 format,"<p>I have the variable <code>trainData</code> which has the following simplified format.</p>
<pre class=""lang-py prettyprint-override""><code>[

('Paragraph_A', {&quot;entities&quot;: [(15, 26, 'DiseaseClass'), (443, 449, 'DiseaseClass'), (483, 496, 'DiseaseClass')]}),
('Paragraph_B', {&quot;entities&quot;: [(969, 975, 'DiseaseClass'), (1257, 1271, 'SpecificDisease')]}),
('Paragraph_C', {&quot;entities&quot;: [(0, 27, 'SpecificDisease')]})
]
</code></pre>
<p>I am trying to convert <code>trainData</code> to <code>.spacy</code> by converting it first in <code>doc</code> and then to <code>DocBin</code>. The whole <code>trainData</code> file is accessible via <a href=""https://drive.google.com/file/d/1Njb5hoPGU1sqaQzEgvx-Bld4LRUkrChm/view?usp=sharing"" rel=""nofollow noreferrer"">GoogleDocs</a>.</p>
<p>I tried to reproduce what was mentioned in this tutorial but did not work for me. The tutorial is: <a href=""https://towardsdatascience.com/using-spacy-3-0-to-build-a-custom-ner-model-c9256bea098"" rel=""nofollow noreferrer"">Using spaCy 3.0 to build a custom NER model</a></p>
<hr />
<p>I tried the following.</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.tokens import DocBin

nlp = spacy.blank(&quot;en&quot;) # load a new spacy model
db = DocBin() # create a DocBin object

for text, annot in trainData: # data in previous format
    doc = nlp.make_doc(text) # create doc object from text
    ents = []
    for start, end, label in annot[&quot;entities&quot;]: # add character indexes
        span = doc.char_span(start, end, label=label, alignment_mode=&quot;contract&quot;)
        ents.append(span)
    doc.ents = span # label the text with the ents
    db.add(doc)

db.to_disk(&quot;./train.spacy&quot;) # save the docbin object
</code></pre>
<p>But I am mistaken in my code of how to make conversion of the data from <code>Spacy v2</code> to <code>Spacy v3</code>.
In the above code snippet, I got a traceback:
<code>TypeError: 'spacy.tokens.token.Token' object is not iterable</code>.</p>
",Named Entity Recognition (NER),using spacy convert data old spacy v format brand new spacy v format variable ha following simplified format trying convert converting first whole file accessible via googledocs tried reproduce wa mentioned tutorial work tutorial using spacy build custom ner model tried following mistaken code make conversion data code snippet got traceback
"Create new columns pandas dataframe from function, fixing length ValueError: Length of values does not match length of index","<p>I have a dataset with a column containing a quote (string column in a dataframe).</p>
<p>I'm using spacy to extract the named entities and their types into a new column.
So, my function should take the first quote, extract entity names from that row and add them into a new column, and extract the entity types and add them into a new column</p>
<p>My function currently looks like this:</p>
<pre><code>def find_named_entities(df, quote):

  entity_text_list = []
  entity_label_list = []
  quotes = politifact_df.source_quote.to_list()
  print(&quot;quotes:&quot;, quotes)
  
  nlp = spacy.load(&quot;en_core_web_sm&quot;)
  for quote in quotes:
    doc = nlp(quote)
    for entity in doc.ents:
      entity_text_list.append(entity.text)
      entity_label_list.append(entity.label)

  df[&quot;entity_names&quot;] = entity_text_list
  df[&quot;entity_label&quot;] = entity_label_list
  
  return df
</code></pre>
<p>and I run with:</p>
<pre><code>politifact2_df = find_named_entities(politifact_df, politifact_df.source_quote)
</code></pre>
<p>But the problem with this is that the lists <code>entity_text_list</code> and <code>entity_label_list</code> is that it is trying to give each separate item in the list as its own column, which won't work because the length of the column will be greater than the length of the dataframe.</p>
<p>Can someone point out my mistake and how I could fix it?</p>
<p>I've looked around for similar questions and found that most of them aren't creating a new column from a function and aren't dealing with this ValueError</p>
",Named Entity Recognition (NER),create new column panda dataframe function fixing length valueerror length value doe match length index dataset column containing quote string column dataframe using spacy extract named entity type new column function take first quote extract entity name row add new column extract entity type add new column function currently look like run problem list trying give separate item list column work length column greater length dataframe someone point mistake could fix looked around similar question found creating new column function dealing valueerror
Spacy 3 Confidence Score on Named-Entity recognition,"<p>I need to get a confidence score for the tags predicted by NER 'de_core_news_lg' model. There was a well known solution to the problem in the Spacy 2:</p>
<pre><code>nlp = spacy.load('de_core_news_lg')
doc = nlp('ich möchte mit frau Mustermann in der Musterbank sprechen')
text = content
doc = nlp.make_doc(text)
beams = nlp.entity.beam_parse([doc], beam_width=16, beam_density=0.0001)
for score, ents in nlp.entity.moves.get_beam_parses(beams[0]):
    print (score, ents)
    entity_scores = defaultdict(float)
    for start, end, label in ents:
        # print (&quot;here&quot;)
        entity_scores[(start, end, label)] += score
        print ('entity_scores', entity_scores)
</code></pre>
<p>However, in Spacy 3 I get the following error:</p>
<pre><code>AttributeError: 'German' object has no attribute 'entity'
</code></pre>
<p>Obviously <code>language</code> object does not have <code>entity</code> attribute anymore.
Does anyone know how to get the confidence scores in Spacy 3?</p>
",Named Entity Recognition (NER),spacy confidence score named entity recognition need get confidence score tag predicted ner de core news lg model wa well known solution problem spacy however spacy get following error obviously object doe attribute anymore doe anyone know get confidence score spacy
Extract proper nouns from text in R?,"<p>Is there any better way of extracting proper nouns (e.g. &quot;London&quot;, &quot;John Smith&quot;, &quot;Gulf of Carpentaria&quot;) from free text?</p>
<p>That is, a function like</p>
<pre class=""lang-r prettyprint-override""><code>proper_nouns &lt;- function(text_input) {
  # ...
}
</code></pre>
<p>such that it would extract a list of proper nouns from the text input(s).</p>
<h3>Examples</h3>
<p>Here is a set of 7 text inputs (some easy, some harder):</p>
<pre class=""lang-r prettyprint-override""><code>text_inputs &lt;- c(&quot;a rainy London day&quot;,
  &quot;do you know John Smith?&quot;,
  &quot;sail the Adriatic&quot;,
  
  # tougher examples
  
  &quot;Hey Tom, where's Fred?&quot; # more than one proper noun in the sentence
  &quot;Hi Lisa, I'm Joan.&quot; # more than one proper noun in the sentence, separated by capitalized word
  &quot;sail the Gulf of Carpentaria&quot;, # proper noun containing an uncapitalized word
  &quot;The great Joost van der Westhuizen.&quot; # proper noun containing two uncapitalized words
  )
</code></pre>
<p>And here's what such a function, set of rules, or AI should return:</p>
<pre class=""lang-r prettyprint-override""><code>proper_nouns(text_inputs)

[[1]]
[1] &quot;London&quot;

[[2]]
[1] &quot;John Smith&quot; 

[[3]]
[1] &quot;Adriatic&quot;

[[4]]
[1] &quot;Tom&quot;    &quot;Fred&quot;

[[5]]
[1] &quot;Lisa&quot;    &quot;Joan&quot;

[[6]]
[1] &quot;Gulf of Carpentaria&quot;

[[7]]
[1] &quot;Joost van der Westhuizen&quot;
</code></pre>
<h3>Problems: simple regex are imperfect</h3>
<p>Consider some simple regex rules, which have obvious imperfections:</p>
<ul>
<li><p>Rule: take capitalized words, unless they're the first word in the sentence (which would ordinarily be capitalized). Problem: will miss proper nouns at start of sentence.</p>
</li>
<li><p>Rule: assume successive capitalized words are parts of the same proper noun (multi-part proper nouns like <code>&quot;John Smith&quot;</code>). Problem: <code>&quot;Gulf of Carpentaria&quot;</code> would be missed since it has an uncapitalized word in between.</p>
<ul>
<li>Similar problem with people's names containing uncapitalized words, e.g. <code>&quot;Joost van der Westhuizen&quot;</code>.</li>
</ul>
</li>
</ul>
<h3>Question</h3>
<p>The best approach I currently have is to simply use the regular expressions above and make do with a low success rate. Is there a better or more accurate way to extract the proper nouns from text in R? If I could get 80-90% accuracy on real text, that would be great.</p>
",Named Entity Recognition (NER),extract proper noun text r better way extracting proper noun e g london john smith gulf carpentaria free text function like would extract list proper noun text input example set text input easy harder function set rule ai return problem simple regex imperfect consider simple regex rule obvious imperfection rule take capitalized word unless first word sentence would ordinarily capitalized problem miss proper noun start sentence rule assume successive capitalized word part proper noun multi part proper noun like problem would missed since ha uncapitalized word similar problem people name containing uncapitalized word e g question best approach currently simply use regular expression make low success rate better accurate way extract proper noun text r could get accuracy real text would great
Problems with regex and lists comprehension with multiple functions and f-string,"<p>i have a probleme, two in fact. Firstable, i work on an NLP projet. I need to write a script to tag words, replace non standard POS (parts of speech) by universel POS and extract name entities with their tag. And i need to put the result in file like this. There are two files. The first one only prints the words with their POS and NER tag (named entity recognition). For the first file, i have this kind of output:</p>
<pre><code>Consuela    NOUN    B-PERSON
Washington  NOUN    B-ORGANIZATION
.   .   O
a   DET O
longtime    ADJ O
House   NOUN    B-ORGANIZATION
staffer NOUN    O
and CONJ    O
an  DET O
expert  NOUN    O
in  ADP O
securities  NOUN    O
laws    NOUN    O
.   .   O
is  VERB    O
a   DET O
leading VERB    O
candidate   NOUN    O
to  PRT O
be  VERB    O
chairwoman  NOUN    O
of  ADP O
the DET O
Securities  NOUN    B-ORGANIZATION
and CONJ    O
Exchange    NOUN    B-ORGANIZATION
Commission  NOUN    I-ORGANIZATION
</code></pre>
<p>It's exactly what i want but now i have to convert the NER tag to universal NER tag and created a dict with key/value (non standard/standard). Here the POS has been converted in universal format (NOUN, DET, ADP etc...) But now, when i want to apply the same method with the my second file to convert NER tag, i obtains that :</p>
<pre><code>Consuela    NOUN    BIBIBIB-LOCLOCPERSPERSORGORGPERSON
Washington  NOUN    BIBIBIB-LOCLOCPERSPERSORGORGORGANIZATION
.   .   BIBIB-LOCLOCPERSPERSORGBIBIB-LOCLOCPERSPERSORGORG
a   DET BIBIB-LOCLOCPERSPERSORGBIBIB-LOCLOCPERSPERSORGORG
longtime    ADJ BIBIB-LOCLOCPERSPERSORGBIBIB-LOCLOCPERSPERSORGORG
House   NOUN    BIBIBIB-LOCLOCPERSPERSORGORGORGANIZATION
staffer NOUN    BIBIB-LOCLOCPERSPERSORGBIBIB-LOCLOCPERSPERSORGORG
and CONJ    BIBIB-LOCLOCPERSPERSORGBIBIB-LOCLOCPERSPERSORGORG
an  DET BIBIB-LOCLOCPERSPERSORGBIBIB-LOCLOCPERSPERSORGORG
expert  NOUN    BIBIB-LOCLOCPERSPERSORGBIBIB-LOCLOCPERSPERSORGORG
in  ADP BIBIB-LOCLOCPERSPERSORGBIBIB-LOCLOCPERSPERSORGORG
securities  NOUN    BIBIB-LOCLOCPERSPERSORGBIBIB-LOCLOCPERSPERSORGORG
laws    NOUN    BIBIB-LOCLOCPERSPERSORGBIBIB-LOCLOCPERSPERSORGORG
.   .   BIBIB-LOCLOCPERSPERSORGBIBIB-LOCLOCPERSPERSORGORG
is  VERB    BIBIB-LOCLOCPERSPERSORGBIBIB-LOCLOCPERSPERSORGORG
a   DET BIBIB-LOCLOCPERSPERSORGBIBIB-LOCLOCPERSPERSORGORG
leading VERB    BIBIB-LOCLOCPERSPERSORGBIBIB-LOCLOCPERSPERSORGORG
candidate   NOUN    BIBIB-LOCLOCPERSPERSORGBIBIB-LOCLOCPERSPERSORGORG
to  PRT BIBIB-LOCLOCPERSPERSORGBIBIB-LOCLOCPERSPERSORGORG
</code></pre>
<p>It's for sure no what i want. It's when i want to change my dictionnary (correspondence table between NER) between the first one</p>
<pre><code>dict_ner = {'ORG':['I-ORGANIZATION', 'B-ORGANIZATION', 'FACILITY'], 'PERS':['I-PERSON', 'B-PERSON'], 'LOC':['I-LOCATION', 'B-LOCATION'],
'MISC':['DATE', 'TIME', 'MONEY', 'PERCENT'],'LOC':['I-GPE', 'B-GPE']}
</code></pre>
<p>and the second one because i want to print the I or B before (I-PERS, B-PERS etc...) It's important for my project. I use this dict and the result is awful.
My regexp module build  the entire regex to replace non standard tag in a dict where the regex is the key and the the value is the value by which to change the old one(s).</p>
<pre><code>dict_ner = {'I-ORG':'I-ORGANIZATION', 'B-ORG':'B-ORGANIZATION', 'ORG':'FACILITY', 'I-PERS':'I-PERSON', 'B-PERS':'B-PERSON', 'I-LOC':'I-LOCATION', 'B-LOC':'B-LOCATION',
'MISC':['DATE', 'TIME', 'MONEY', 'PERCENT'],'LOC':['I-GPE', 'B-GPE']}
</code></pre>
<p>Here is my code. Don't pay attention to french commentaries</p>
<pre><code># -*- coding: utf-8 -*-
#!usr/bin/env python3

from contextlib import ExitStack
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk import ne_chunk, pos_tag
from nltk.chunk import tree2conlltags
import re

# dictionnaire : table de correspondance des étiquètes NER (conll to standard)
dict_ner = {'I-ORG':'I-ORGANIZATION', 'B-ORG':'B-ORGANIZATION', 'ORG':'FACILITY', 'I-PERS':'I-PERSON', 'B-PERS':'B-PERSON', 'I-LOC':'I-LOCATION', 'B-LOC':'B-LOCATION',
'MISC':['DATE', 'TIME', 'MONEY', 'PERCENT'],'LOC':['I-GPE', 'B-GPE']}

# loading correspondance table between Penn Treebank POS and standard POS from file
def load_pos_table():

    try:
         
        with open('POSTags_PTB_Universal_Linux.txt', 'r', encoding='utf-8') as universal:

            # on commence par charger notre dictionnaire avec la table des étiquettes POS Penn Treebank, POS NLTK
            dict_pos = {}
            for sent in universal.readlines():
                for line in sent.splitlines():
                    cut = line.strip().split()
                    dict_pos[cut[1]] = dict_pos.get(cut[1], list()) + [cut[0]]

        return dict_pos

    except Exception as erreur:
        print(f'load_pos_table : {erreur}')

def convert_format(line, dic):

    try:
        rx_dctvals = {re.compile(&quot;|&quot;.join(sorted([to_regex(v) for v in val], key=len, reverse=True))):key for key, val in dic.items()}

        #Version 3.8+
        return [line := rx.sub(repl.replace('\\', '\\\\'), line) for rx, repl in rx_dctvals.items()][-1]
        &quot;&quot;&quot;
        version 3.7-
        for rx, repl in rx_dctvals.items():
            line = rx.sub(repl.replace('\\', '\\\\'), line)
        return line
        &quot;&quot;&quot;

    except Exception as erreur:
        print(f'convert_tag: {erreur}')

def to_regex(x):

    r = []
    if x[0].isalnum():
        r.append(r'(?&lt;![^\W])')
    else:
        if any(l.isalnum() for l in x):
            r.append(r'\B')

    r.append(re.escape(x))
    
    if x[-1].isalnum():
        r.append(r'\b')
    else:
        if any(l.isalnum() for l in x):
            r.append(r'\B')
    return &quot;&quot;.join(r)

def extract_entities(doc):
    return list(map(lambda sent: tree2conlltags(ne_chunk(pos_tag(word_tokenize(sent)))),
 sent_tokenize(doc)))

def main():

    try:

        with ExitStack() as stack:
            
            file = stack.enter_context(open('formal-tst.NE.key.04oct95_sample.txt', 'r', encoding='utf-8'))
            # fichier d'extraction des entités nommées avec étiquettes non standards 
            result_file_ner = stack.enter_context(open('wsj_0010_sample.txt.ne.nltk', 'w', encoding='utf-8'))
            # fichier avec étiquettes standards NER
            result_file_ner_standard = stack.enter_context(open('wsj_0010_sample.txt.ne.standard', 'w', encoding='utf-8'))

            pos_table = load_pos_table()
            content = file.read()

            [[result_file_ner.write(convert_format(f'{name}\t{tag}\t{ner}\n', pos_table)) for name, tag, ner in line] for line in extract_entities(content)]
            [[result_file_ner_standard.write(convert_format(f'{name}\t{tag}\t{ner}\n', {**pos_table, **dict_ner})) for name, tag, ner in line] for line in extract_entities(content)]

    except Exception as error:
        print(f'main error : {error}')

if __name__ == '__main__':
    main()
</code></pre>
<p>Furthermore, i'm not about these two list comprenhension</p>
<pre><code>            [[result_file_ner.write(convert_format(f'{name}\t{tag}\t{ner}\n', pos_table)) for name, tag, ner in line] for line in extract_entities(content)]
            [[result_file_ner_standard.write(convert_format(f'{name}\t{tag}\t{ner}\n', {**pos_table, **dict_ner})) for name, tag, ner in line] for line in extract_entities(content)]
</code></pre>
<p>extract_entities(content) returns a list of tuples of 3 elements (word, POS, NER or not (0))
I'm not sure if my problem concerns my regex module. I don't know.
If you could help me, i would be grateful</p>
",Named Entity Recognition (NER),problem regex list comprehension multiple function f string probleme two fact firstable work nlp projet need write script tag word replace non standard po part speech universel po extract name entity tag need put result file like two file first one print word po ner tag named entity recognition first file kind output exactly want convert ner tag universal ner tag created dict key value non standard standard po ha converted universal format noun det adp etc want apply method second file convert ner tag obtains sure want want change dictionnary correspondence table ner first one second one want print b pers b pers etc important project use dict result awful regexp module build entire regex replace non standard tag dict regex key value value change old one code pay attention french commentary furthermore two list comprenhension extract entity content return list tuples element word po ner sure problem concern regex module know could help would grateful
Entities extraction based on customized list in R,"<p>I have list of texts and I also have a list of entities.</p>
<p>The list of texts is typically in vectorized string.</p>
<p>The list of entities is a bit more complexed.
Some entities, can be listed out exhaustively such as the list of main cities of the world.
Some entities, while impossible to be listed out exhaustively, can be captured by regex pattern.</p>
<pre><code>
list_of_text &lt;- c('Lorem ipsum 12-01-2021 eat, Copenhagen 133.001.00.00 ...', 'Lorem ipsum 12-01-2021, Copenhagen www.stackoverflow.com swimming', ...)

entity_city &lt;- c('Copenhagen', 'Paris', 'New York', ...)

entity_IP_address &lt;- c('regex code for IP address')

entity_IP_address &lt;- c('regex code for URL')

entity_verb &lt;- c('verbs')

</code></pre>
<p>Given the <code>list_of_text</code> and the list of <code>entities</code>, I want to find matching entities for each text.</p>
<p>For example <code>c('Lorem ipsum 12-01-2021 eat drink sleep, Copenhagen 133.001.00.00 ...')</code>, it has <code>c(eat, drink, sleep)</code> for <code>entity_verb</code>, <code>c(133.001.00.00)</code> for <code>entity_IP</code>, etc.</p>
<pre><code>
res &lt;- extract_entity(text = c('Lorem ipsum 12-01-2021 eat drink sleep, Copenhagen 133.001.00.00 ...')
                      ,entities &lt;- c(entity_verb, entity_IP_address, entity_city))

res[['verb']]
c('eat', 'drink', 'sleep')

res[['IP']]
c('133.001.00.00')

res[['city']]
c('Copenhagen')

</code></pre>
<p>Is there a <code>R package</code> I can leverage on?</p>
",Named Entity Recognition (NER),entity extraction based customized list r list text also list entity list text typically vectorized string list entity bit complexed entity listed exhaustively list main city world entity impossible listed exhaustively captured regex pattern given list want find matching entity text example ha etc leverage
Train NER with Custom Training Data,"<p>I am new to NLP and started with Spacy. I want to train NER with custom data and I am looking for free tool that can be used for Annotation.
Please suggest if you are using any open-source and User-friendly tool</p>
<p>Thanks in advance</p>
",Named Entity Recognition (NER),train ner custom training data new nlp started spacy want train ner custom data looking free tool used annotation please suggest using open source user friendly tool thanks advance
BERT - Is that needed to add new tokens to be trained in a domain specific environment?,"<p>My question here is no how to add new tokens, or how to train using a domain-specific corpus, I'm already doing that.</p>
<p>The thing is, am I supposed to add the domain-specific tokens before the MLM training, or I just let Bert figure out the context? If I choose to not include the tokens, am I going to get a poor task-specific model like NER?</p>
<p>To give you more background of my situation, I'm training a Bert model on medical text using Portuguese language, so, deceased names, drug names, and other stuff are present in my corpus, but I'm not sure I have to add those tokens before the training.</p>
<p>I saw this one: <a href=""https://stackoverflow.com/questions/64816669/using-pretrained-bert-model-to-add-additional-words-that-are-not-recognized-by-t"">Using Pretrained BERT model to add additional words that are not recognized by the model</a></p>
<p>But the doubts remain, as other sources say otherwise.</p>
<p>Thanks in advance.</p>
",Named Entity Recognition (NER),bert needed add new token trained domain specific environment question add new token train using domain specific corpus already thing supposed add domain specific token mlm training let bert figure context choose include token going get poor task specific model like ner give background situation training bert model medical text using portuguese language deceased name drug name stuff present corpus sure add token training saw one href pretrained bert model add additional word recognized model doubt remain source say otherwise thanks advance
Unable to write content to csv in single row,"<p>I'm trying to write a list to csv format in python and facing a unique challenge with specific row which is in the format:</p>
<p>[('c6',
'',
'2021-01-21',
'Pak vs SA: Security  them play and that\u2019s almost like a generation and that\u2019s missed seeing them play.\n\u201cPakistan fans are very proud of their team, like any other nation, but it is when you tour the subcontinent, you realise people here are very passionate about cricket. They go to every match and support their team however they can. Singing, chanting, there\u2019s energy the whole day,\&quot; the Proteas player cherished. 2017 tour with the ICC World XI\nReflecting on the 2017 tour with the ICC World XI, du Plessis said: \u201cI think [the ICC World XI\u2019s tour of Pakistan] was really important in the context of bringing cricket back into Pakistan. There was zero cricket happening in Pakistan at that stage. Read more: Pak vs SA: Pacer Tabish Khan determined after finally being named in national squad\nThe second Test between the two teams will be played at the Pindi Cricket Stadium, Rawalpindi, from February 4, before they travel to Lahore for three T20Is on 11, 13, and 14 February at the Gaddafi Stadium.\nDu Plessis has played seven Tests against Pakistan in which he has scored 246 runs \u2013 including a century \u2013 at an average of 27.33. Two of those seven games were played in the United Arab Emirates, where Pakistan hosted their home cricket from 2010 until 2019, before the complete restoration of international and top-flight cricket in the country. More From Sports:')]</p>
<p>When I try to write it to a csv with following code:</p>
<pre><code>out = open('out.csv', 'w')
for row in l:
   out.write( '&quot;' + '&quot;,&quot;'.join(row) + '&quot;\n' )
   out = open('out.csv', 'w')
</code></pre>
<p>The output is splitting at exactly &quot;ICC World XI&quot; i.e. text before ICC World XI is in one cell and the rest text is pushed to the next cell in the same row. However, when I reduce the content (randomly) in the original text, the output looks fine. I'm unable to understand the reason behind it. Can anyone please explain. (I'm bit new to python)</p>
",Named Entity Recognition (NER),unable write content csv single row trying write list csv format python facing unique challenge specific row format c pak v sa security play u almost like generation u missed seeing play n u cpakistan fan proud team like nation tour subcontinent realise people passionate cricket go every match support team however singing chanting u energy whole day protea player cherished tour icc world xi nreflecting tour icc world xi du plessis said u ci think icc world xi u tour pakistan wa really important context bringing cricket back pakistan wa zero cricket happening pakistan stage read pak v sa pacer tabish khan determined finally named national squad second test two team played pindi cricket stadium rawalpindi february travel lahore three february gaddafi stadium ndu plessis ha played seven test pakistan ha scored run u including century u average two seven game played united arab emirate pakistan hosted home cricket complete restoration international top flight cricket country sport try write csv following code output splitting exactly icc world xi e text icc world xi one cell rest text pushed next cell row however reduce content randomly original text output look fine unable understand reason behind anyone please explain bit new python
how to extract a PERSON named entity after certain word with spacy?,"<p>I have this text ( text2 in code), it has 3 'by' word, I want to use Spacy to extract the person's name (full name, even if it is 3 words, some races use long names, in this case 2). The code is below, my pattern shows error. My intention: first fix the 'by' word with ORTH, then to tell program that whatever coming next is the Part of Speech entity called PERSON. I would be happy if anyone help it:</p>
<pre><code>import spacy
from spacy.matcher import Matcher
matcher = Matcher(nlp.vocab)
text2 = 'All is done by Emily Muller, the leaf is burned by fire. we were not happy, so we cut     relations by saying bye bye'
def extract_person(nlp_doc):
     pattern = [{'ORTH': 'by'}, {'POS': 'NOUN'}}]
     # second possible pattern:
     #pattern = [{&quot;TEXT&quot;: &quot;by&quot;}, {&quot;NER&quot;: &quot;PERSON&quot;}]
     matcher.add('person_only', None, pattern)
     matches = matcher(nlp_doc)
     for match_id, start, end in matches:
         span = nlp_doc[start:end]
         return span.text
target_doc = nlp(text2)
extract_person(target_doc)
</code></pre>
<p>I think this question can be asked other way around: how to use NER tags in pattern in Matcher in spacy?</p>
",Named Entity Recognition (NER),extract person named entity certain word spacy text text code ha word want use spacy extract person name full name even word race use long name case code pattern show error intention first fix word orth tell program whatever coming next part speech entity called person would happy anyone help think question asked way around use ner tag pattern matcher spacy
Build custom named entity recognition (NLP) models,"<p>I am trying to extract names of people from the text using OpenNLP in R. However whenever I use Indian names, the model fails to detect the names. Hence I understood that I need to build custom model. I have built my own <code>en-ner-customperson.bin</code> using Java.</p>

<p>I am not understanding how should I use this custom model in my R code?</p>

<p>I am using the following code:</p>

<pre><code>require(""NLP"")
## Some text.
s &lt;- paste(c(""Hardik, 61 years old, will join the board as a "",
             ""nonexecutive director Nov. 29.\n"",
             ""Mr. Vinken is chairman of Elsevier N.V., "",
             ""the Dutch publishing group.""),
           collapse = """")
s &lt;- as.String(s)
## Need sentence and word token annotations.
sent_token_annotator &lt;- Maxent_Sent_Token_Annotator()
word_token_annotator &lt;- Maxent_Word_Token_Annotator()
a2 &lt;- annotate(s, list(sent_token_annotator, word_token_annotator))
## Entity recognition for persons.
entity_annotator &lt;- Maxent_Entity_Annotator()
entity_annotator
annotate(s, entity_annotator, a2)
## Directly:
entity_annotator(s, a2)
## And slice ...
s[entity_annotator(s, a2)]
## Variant with sentence probabilities as features.
annotate(s, Maxent_Entity_Annotator(probs = TRUE), a2)
</code></pre>

<p>Is there any documentation available to build custom models in R? How to build custom models and use them along with R</p>
",Named Entity Recognition (NER),build custom named entity recognition nlp model trying extract name people text using opennlp r however whenever use indian name model fails detect name hence understood need build custom model built using java understanding use custom model r code using following code documentation available build custom model r build custom model use along r
How to transform unstructured text to rdf turtle in practice?,"<p>i am currently working on a study project, where i have to transform the vehicle complaints descriptions from the NHTSA Database (<a href=""https://catalog.data.gov/dataset/nhtsas-office-of-defects-investigation-odi-complaints"" rel=""nofollow noreferrer"">https://catalog.data.gov/dataset/nhtsas-office-of-defects-investigation-odi-complaints</a>) into rdf-turtle and later into a Knowledge Graph representation maybe using GraphDB, etc. A set of descriptions can be found in the attachment.</p>
<p>I have research topics like NER, Relation Extration, OpenIE, FRED, Knowledge Graph Construction, RDFS, OWL and Ontologies in theory.</p>
<p>Now, i come to a point where i just don't know how to practically do it.</p>
<p>Can someone help me with it and guide me a little bit through it?
Where should i start and with what?</p>
<p>Thanks you very much,
Dennis</p>
<p><a href=""https://i.sstatic.net/xzQaL.png"" rel=""nofollow noreferrer"">Examples customer complaints</a></p>
",Named Entity Recognition (NER),transform unstructured text rdf turtle practice currently working study project transform vehicle complaint description nhtsa database rdf turtle later knowledge graph representation maybe using graphdb etc set description found attachment research topic like ner relation extration openie fred knowledge graph construction rdfs owl ontology theory come point know practically someone help guide little bit start thanks much dennis example customer complaint
Creating NER model with Keras and Python,"<p>I have made Keras model that detects if string value is Address, Company or Date. I have used only different company names, different date formants, and different street addresses for training.
So each row in my dataset have between 1 and 5 words (some words can be numbers).</p>
<p>For preprocessing I have used vectorizers:</p>
<pre><code>transformerVectoriser = ColumnTransformer(transformers=[('vector char', CountVectorizer(analyzer='char', ngram_range=(3, 6), max_features = 2000), 'text'),
                                                        ('vector word', CountVectorizer(analyzer='word', ngram_range=(1, 1), max_features = 4000), 'text')],
                                          remainder='passthrough') # Default is to drop untransformed columns


features = transformerVectoriser.fit_transform(features)
</code></pre>
<p>This is my model:</p>
<pre><code>model = Sequential()
model.add(Dense(100, input_dim = features.shape[1], activation = 'relu')) # input layer requires input_dim param
model.add(Dense(200, activation = 'relu'))
model.add(Dense(100, activation = 'relu'))
model.add(Dense(50, activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(3, activation='softmax'))
</code></pre>
<p>I have achieved accuracy of 93%.
Is it possible to use that model for detecting where is that string (Address, Company or Date) in bigger text? I think that that kind of models is called NER models (named entity recognition).</p>
<p>My model takes string input and decides if its a company, person or address.
String input is 1-5 words long.</p>
<p>For example if I have text:</p>
<pre><code>text = &quot;What do you think about Amazon and their customer policy?&quot;
</code></pre>
<p>How can I detect start index and end index of &quot;Amazon&quot;, or how can I extract only Amazon from this text?
So required result should be:</p>
<pre><code>start_index = 21
end_index = 27
</code></pre>
<p>Or:</p>
<pre><code>result = {'company':'Amazon'}
</code></pre>
<p>Or for example if the input is:</p>
<pre><code>&quot;My name is Don John and I work in Amazon Inc.&quot;
</code></pre>
<p>as a result i want: <code>{&quot;company&quot;: &quot;Amazon Inc&quot;, &quot;name&quot;: &quot;Don John&quot;}</code></p>
<p>I want to use my model to extract specific entities from text, in this case company, address or date.
Basically I need some kind of <code>location search</code> thats based on Keras model, or searching for the pattern in large text.</p>
",Named Entity Recognition (NER),creating ner model kera python made kera model detects string value address company date used different company name different date formants different street address training row dataset word word number preprocessing used vectorizers model achieved accuracy possible use model detecting string address company date bigger text think kind model called ner model named entity recognition model take string input decides company person address string input word long example text detect start index end index amazon extract amazon text required result example input result want want use model extract specific entity text case company address date basically need kind thats based kera model searching pattern large text
Exporting annotated GATE file for further processing in Python causes character offset issues,"<p>I've used General Architecture for Text Engineering (GATE) to manually annotate data for a Named Entity Recognition (NER) task. The annotations are on corpus level and stored with a character start-and end offset, like this:</p>
<pre><code> startOffset endOffset annotationType
 12          17        personName
 21          28        organisationName
</code></pre>
<p>When I try to load the annotations and the original text file in Python, the character offsets mismatch, i.e. in GATE a given entity starting at offset 12 and ending at offset 20 may correctly resemble a person's name while the same offset in Python resembles something just slightly off.</p>
<p>Ideally, I would like to tokenize the corpus and replace annotations in the text with a tuple containing the word and its annotation type, like below:</p>
<pre><code>&quot;This&quot;, &quot;is&quot;, &quot;Jane&quot;, &quot;Doe&quot;, &quot;speaking&quot;
&quot;This&quot;, &quot;is&quot;, (&quot;Jane&quot;, personName), (&quot;Doe&quot;, personName), &quot;speaking&quot;
</code></pre>
<p>This would make it easier to achieve my ultimate goal of transforming the data in an I, O, B format where each token is written on a new line with an annotation tag or &quot;O&quot; if there is no annotation for that word next to it. An entity can be multiple tokens long, in that case, the first occurence of said entity receives the &quot;(B)eginning&quot; tag and the following occurences receive the &quot;(I)nside&quot; tag. Like shown below:</p>
<pre><code>This O
is O
Jane B-personName
Doe I-personName
speaking O
</code></pre>
<p>I am however unsure how to approach this problem: by using Python or by using GATE with one of the many available plugins, i.e. with the Groovy editor. There are multiple ways to export GATE files, for example by XML, json, or plain text using Groovy but importing either file type results in mismatching character offsets while using UTF-8 encoding. For this reason, I've created a Python script to repair the wrong annotations first, however this code is long, not efficient and it feels like I'm trying to solve a problem that should not be there in the first place. I've written my current high level approach at the bottom of this post.</p>
<p>For reference, a sample XML file snippet is shown below, with as input the following text where country and city names are labelled as &quot;Toponyms&quot;:</p>
<pre><code>Sweden is a country in Europe.
Paris is the capital of France.
In Spain it's often sunny.
</code></pre>
<p>This results in the following GATE output XML:</p>
<pre><code>&lt;?xml version='1.0' encoding='UTF-8'?&gt;
&lt;GateDocument&gt;
&lt;!-- The document content area with serialized nodes --&gt;

&lt;TextWithNodes&gt;&lt;Node id=&quot;0&quot;/&gt;Sweden&lt;Node id=&quot;6&quot;/&gt; is a country in &lt;Node id=&quot;23&quot;/&gt;Europe&lt;Node id=&quot;29&quot;/&gt;.&amp;#xd;
&lt;Node id=&quot;32&quot;/&gt;Paris&lt;Node id=&quot;37&quot;/&gt; is the capital of &lt;Node id=&quot;56&quot;/&gt;France&lt;Node id=&quot;62&quot;/&gt;.&amp;#xd;
In &lt;Node id=&quot;68&quot;/&gt;Spain&lt;Node id=&quot;73&quot;/&gt; it's often sunny.&lt;Node id=&quot;91&quot;/&gt;&lt;/TextWithNodes&gt;
&lt;!-- The default annotation set --&gt;

&lt;AnnotationSet&gt;
&lt;Annotation Id=&quot;1&quot; Type=&quot;gateFinal&quot; StartNode=&quot;0&quot; EndNode=&quot;6&quot;&gt;
&lt;Feature&gt;
  &lt;Name className=&quot;java.lang.String&quot;&gt;kind&lt;/Name&gt;
  &lt;Value className=&quot;java.lang.String&quot;&gt;Toponyms&lt;/Value&gt;
&lt;/Feature&gt;
&lt;/Annotation&gt;
&lt;Annotation Id=&quot;2&quot; Type=&quot;gateFinal&quot; StartNode=&quot;23&quot; EndNode=&quot;29&quot;&gt;
&lt;Feature&gt;
  &lt;Name className=&quot;java.lang.String&quot;&gt;kind&lt;/Name&gt;
  &lt;Value className=&quot;java.lang.String&quot;&gt;Toponyms&lt;/Value&gt;
&lt;/Feature&gt;
&lt;/Annotation&gt;
&lt;Annotation Id=&quot;3&quot; Type=&quot;gateFinal&quot; StartNode=&quot;56&quot; EndNode=&quot;62&quot;&gt;
&lt;Feature&gt;
  &lt;Name className=&quot;java.lang.String&quot;&gt;kind&lt;/Name&gt;
  &lt;Value className=&quot;java.lang.String&quot;&gt;Toponyms&lt;/Value&gt;
&lt;/Feature&gt;
&lt;/Annotation&gt;
&lt;Annotation Id=&quot;4&quot; Type=&quot;gateFinal&quot; StartNode=&quot;32&quot; EndNode=&quot;37&quot;&gt;
&lt;Feature&gt;
  &lt;Name className=&quot;java.lang.String&quot;&gt;kind&lt;/Name&gt;
  &lt;Value className=&quot;java.lang.String&quot;&gt;Toponyms&lt;/Value&gt;
&lt;/Feature&gt;
&lt;/Annotation&gt;
&lt;Annotation Id=&quot;5&quot; Type=&quot;gateFinal&quot; StartNode=&quot;68&quot; EndNode=&quot;73&quot;&gt;
&lt;Feature&gt;
  &lt;Name className=&quot;java.lang.String&quot;&gt;kind&lt;/Name&gt;
  &lt;Value className=&quot;java.lang.String&quot;&gt;Toponyms&lt;/Value&gt;
&lt;/Feature&gt;
&lt;/Annotation&gt;
&lt;/AnnotationSet&gt;
&lt;/GateDocument&gt;
</code></pre>
<p>Currently I &quot;repair&quot; the incorrect offsets by tokenizing the input corpus while remembering the character offset for each token. Then I retrieve the annotations from the &quot;Annotationsets&quot; part of the XML file. Then I loop through a list of all the annotations and find the first occurence of the annotated token and continue the loop from the last found token while storing the correct annotation offset. Ultimately I merge the corpus offsets with the annotation offsets. However, a small mistake can cause this script to crash which I why I'm curious to find a better solution.</p>
",Named Entity Recognition (NER),exporting annotated gate file processing python cause character offset issue used general architecture text engineering gate manually annotate data named entity recognition ner task annotation corpus level stored character start end offset like try load annotation original text file python character offset mismatch e gate given entity starting offset ending offset may correctly resemble person name offset python resembles something slightly ideally would like tokenize corpus replace annotation text tuple containing word annotation type like would make easier achieve ultimate goal transforming data b format token written new line annotation tag annotation word next entity multiple token long case first occurence said entity receives b eginning tag following occurences receive nside tag like shown however unsure approach problem using python using gate one many available plugins e groovy editor multiple way export gate file example xml json plain text using groovy importing either file type result mismatching character offset using utf encoding reason created python script repair wrong annotation first however code long efficient feel like trying solve problem first place written current high level approach bottom post reference sample xml file snippet shown input following text country city name labelled toponym result following gate output xml currently repair incorrect offset tokenizing input corpus remembering character offset token retrieve annotation annotationsets part xml file loop list annotation find first occurence annotated token continue loop last found token storing correct annotation offset ultimately merge corpus offset annotation offset however small mistake cause script crash curious find better solution
Tokenzing multi words in entire corpus,"<p>I'm new to NLTK and was wondering if anyone would be able to help me in solving the following situation:</p>
<p>I'm working on a corpus of new articles and have texts in the following format:</p>
<p>t= &quot;While at a rally in Wilmington, Republican presidential nominee Donald Trump made a clear insinuation that Democratic nominee Hillary Clinton has to keep away from becoming president&quot;</p>
<p>and have following code in place:</p>
<pre><code>from nltk.tokenize import word_tokenize
from nltk.tokenize import MWETokenizer
mwetokenizer = MWETokenizer([('Donald','Trump')], separator=' ')
mwetokenizer.add_mwe(('Hillary','Clinton'))
mwetokenized_sentence = mwetokenizer.tokenize(word_tokenize(t))
print(mwetokenized_sentence)
</code></pre>
<p>Which gives the following result:</p>
<p>['While', 'at', 'a', 'rally', 'in', 'Wilmington', ',', 'Republican', 'presidential', 'nominee', 'Donald Trump', 'made', 'a', 'clear', 'insinuation', 'that', 'Democratic', 'nominee', 'Hillary Clinton', 'has', 'to', 'keep', 'away', 'from', 'becoming', 'president']</p>
<p>As we can see the words &quot;Hillary Clinton&quot; and &quot;Donald trump&quot; are together which is as required/expected.</p>
<p>Is there any way that I can figure out similar multi-words in an entire corpus of sentences without looking at individual text?</p>
",Named Entity Recognition (NER),tokenzing multi word entire corpus new nltk wa wondering anyone would able help solving following situation working corpus new article text following format rally wilmington republican nominee donald trump made clear insinuation democratic nominee hillary clinton ha keep away becoming president following code place give following result rally wilmington republican nominee donald trump made clear insinuation democratic nominee hillary clinton ha keep away becoming president see word hillary clinton donald trump together required expected way figure similar multi word entire corpus sentence without looking individual text
Keep alignments in Named Entity Recognition tasks after cleaning text,"<p>I am working on a Named Entity Recognition (NER) task and the entities are annotated in BRAT format (.txt + .ann). I have implemented some regular expressions to clean the texts before using my model, but if I modify the text I have to align the entities' offsets of the annotations. This task is relatively straightforward and after this, I can use my NLP model to classify the different entity classes. However, once I get the classification of the model I need to re-align the recognized entities in the original text, i.e. change the offsets of the cleaned text to those I had before the use of regular expressions. Is there a way to keep track of the original offsets after cleaning texts?</p>
",Named Entity Recognition (NER),keep alignment named entity recognition task cleaning text working named entity recognition ner task entity annotated brat format txt ann implemented regular expression clean text using model modify text align entity offset annotation task relatively straightforward use nlp model classify different entity class however get classification model need align recognized entity original text e change offset cleaned text use regular expression way keep track original offset cleaning text
Name Entity Recognition (NER) for multiple languages,"<p>I am writing some code to perform Named Entity Recognition (NER), which is coming along quite nicely for English texts. However, I would like to be able to apply NER to <em>any</em> language. To do this, I would like to 1) identify the language of a text, and then 2) apply the NER for the identified language. For step 2, I'm doubting to A) translate the text to English, and then apply the NER (in English), or B) apply the NER in the language identified.</p>
<p>Below is the code I have so far. What I would like is for the NER to work for text2, or in any other language, after this language is first recognized:</p>
<pre><code>import spacy
from spacy_langdetect import LanguageDetector
from langdetect import DetectorFactory

text = 'In 1793, Alexander Hamilton recruited Webster to move to New York City and become an editor for a Federalist Party newspaper.'
text2 = 'Em 1793, Alexander Hamilton recrutou Webster para se mudar para a cidade de Nova York e se tornar editor de um jornal do Partido Federalista.'

# Step 1: Identify the language of a text
DetectorFactory.seed = 0
nlp = spacy.load('en_core_web_sm')
nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)
doc = nlp(text)
print(doc._.language)

# Step 2: NER
Entities = [(str(x), x.label_) for x in nlp(str(text)).ents]
print(Entities)
</code></pre>
<p>Does anyone have any experience with this? Much appreciated!</p>
",Named Entity Recognition (NER),name entity recognition ner multiple language writing code perform named entity recognition ner coming along quite nicely english text however would like able apply ner language would like identify language text apply ner identified language step doubting translate text english apply ner english b apply ner language identified code far would like ner work text language language first recognized doe anyone experience much appreciated
how to extract specific words in negspacy,"<p>When I apply negspacy to  my sentence, I want negspacy to consider specific phrase as a single entity and give me the output for it.</p>
<pre><code>import en_core_sci_lg
from negspacy.negation import Negex
nlp = en_core_sci_lg.load()

negex = Negex(nlp, language = &quot;en_clinical_sensitive&quot;)
nlp.add_pipe(negex, last=True)

doc = nlp(&quot;&quot;&quot; patient has no signs of shortness of breath. &quot;&quot;&quot;)

for word in doc.ents:
    print(word, word._.negex)
</code></pre>
<p>The output is -</p>
<pre><code>patient False
shortness True
</code></pre>
<p>I want the output to be -</p>
<pre><code>patient False
shortness of breath True
</code></pre>
<p>How can I consider phrases like &quot;shortness of breath&quot;, &quot;sore throat&quot;, &quot;respiratory distress&quot; as a single entity.</p>
<p>I have tried -</p>
<pre><code>import en_core_sci_lg
from negspacy.negation import Negex
nlp = en_core_sci_lg.load()
from spacy.pipeline import EntityRuler
ruler = EntityRuler(nlp)
patterns =  [{&quot;label&quot;: &quot;ENTITY&quot;, &quot;pattern&quot;: [{&quot;LOWER&quot;: &quot;shortness&quot;}, {&quot;LOWER&quot;: &quot;of&quot;}, {&quot;LOWER&quot;: &quot;breath&quot;}]}]

ruler.add_patterns(patterns)
nlp.add_pipe(ruler)
negex = Negex(nlp, language = &quot;en_clinical&quot;)
nlp.add_pipe(negex, last=True)

doc = nlp(&quot;&quot;&quot;patient has no signs of shortness of breath. &quot;&quot;&quot;)

for word in doc.ents:
    print(word, word._.negex)
</code></pre>
<p>The output is still coming -</p>
<pre><code>patient False
shortness True
</code></pre>
<p>What can I do to solve this problem</p>
",Named Entity Recognition (NER),extract specific word negspacy apply negspacy sentence want negspacy consider specific phrase single entity give output output want output consider phrase like shortness breath sore throat respiratory distress single entity tried output still coming solve problem
Updating spacy.tokenizer._get_regex_pattern(nlp.tokenizer.token_match) so that hashtags are tokenized as a single token,"<p>This is my first time using spacy and I am trying to learn how to edit the tokenizer on one of the pretrained models (en_core_web_md) so that when tweets are tokenized, the entire hashtag becomes a single token (e.g. I want one token '#hashtagText', the default would be two tokens, '#' and 'hashtagText').</p>
<p>I know I am not the first person that has faced this issue. I have tried implementing the advice other places online but after using their methods the output remains the same (#hashtagText is two tokens). These articles show the methods I have tried.</p>
<p><a href=""https://the-fintech-guy.medium.com/spacy-handling-of-hashtags-and-dollartags-ed1e661f203c"" rel=""nofollow noreferrer"">https://the-fintech-guy.medium.com/spacy-handling-of-hashtags-and-dollartags-ed1e661f203c</a></p>
<p><a href=""https://towardsdatascience.com/pre-processing-should-extract-context-specific-features-4d01f6669a7e"" rel=""nofollow noreferrer"">https://towardsdatascience.com/pre-processing-should-extract-context-specific-features-4d01f6669a7e</a></p>
<hr />
<p>Shown in the code below, my troubleshooting steps have been:</p>
<ol>
<li>save the default pattern matching regex (default_token_matching_regex)</li>
<li>save the regex that nlp (the pretrained model) is using before any updates (nlp_token_matching_regex_pre_update)</li>
</ol>
<p>Note: I originally suspected these would be the same, but they are not. See below for outputs.</p>
<ol start=""3"">
<li><p>Append the regex I need (#\w+) to the list that nlp is current using, save this combination as updated_token_matching_regex</p>
</li>
<li><p>Update the regex nlp is using with the variable created above (updated_token_matching_regex)</p>
</li>
<li><p>Save the new regex used by nlp to verify things were updated correctly (nlp_token_matching_regex_post_update).</p>
</li>
</ol>
<p>See code below:</p>
<pre><code>import spacy
import en_core_web_md
import re

nlp = en_core_web_md.load()

# Spacys default token matching regex.
default_token_matching_regex = spacy.tokenizer._get_regex_pattern(nlp.Defaults.token_match)

# Verify what regex nlp is using before changing anything.
nlp_token_matching_regex_pre_update = spacy.tokenizer._get_regex_pattern(nlp.tokenizer.token_match)

# Create a new regex that combines the default regex and a term to treat hashtags as a single token. 
updated_token_matching_regex = f&quot;({nlp_token_matching_regex_pre_update}|#\w+)&quot;

# Update the token matching regex used by nlp with the regex created in the line above.
nlp.tokenizer.token_match = re.compile(updated_token_matching_regex).match

# Verify that nlp is now using the updated regex.
nlp_token_matching_regex_post_update = spacy.tokenizer._get_regex_pattern(nlp.tokenizer.token_match)

# Now let's try again
s = &quot;2020 can't get any worse #ihate2020 @bestfriend &lt;https://t.co&gt;&quot;
doc = nlp(s)

# Let's look at the lemmas and is stopword of each token
print(f&quot;Token\t\tLemma\t\tStopword&quot;)
print(&quot;=&quot;*40)
for token in doc:
    print(f&quot;{token}\t\t{token.lemma_}\t\t{token.is_stop}&quot;)
</code></pre>
<p><a href=""https://i.sstatic.net/ul01t.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ul01t.png"" alt=""Output of print statements:"" /></a></p>
<p>As you can see above, the tokenization behavior is not as it should be with the addition of '#\w+'. See below for printouts of all the troubleshooting variables.</p>
<p><a href=""https://i.sstatic.net/bN981.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bN981.png"" alt=""default_token_matching_regex - Default regex, not used by my nlp"" /></a></p>
<p><a href=""https://i.sstatic.net/Gqmw6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Gqmw6.png"" alt=""nlp_token_matching_regex_pre_update - Regex used by my nlp before I change anything."" /></a></p>
<p><a href=""https://i.sstatic.net/MqYID.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MqYID.png"" alt=""updated_token_matching_regex - Shows the regex I will use to overwrite nlps regex with"" /></a></p>
<p><a href=""https://i.sstatic.net/3SpqO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3SpqO.png"" alt=""nlp_token_matching_regex_post_update - shows the regex nlp uses was successfully updated."" /></a></p>
<p>Since I feel like I have proven to myself above that I did correctly update the regex nlp is using, the only possible issue I could think of is that the regex itself was wrong. I tested the regex by itself and it seems to behave as intended, see below:</p>
<p><a href=""https://i.sstatic.net/PUxcq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PUxcq.png"" alt=""testing regex to pull out #hashTagText as a single token."" /></a></p>
<p>Is anyone able to see the error that is causing nlp to tokenize #hashTagText as two tokens after its nlp.tokenizer.token_match regex was updated to do it as a single token?</p>
<p>Thank you!!</p>
",Named Entity Recognition (NER),updating spacy tokenizer get regex pattern nlp tokenizer token match hashtags tokenized single token first time using spacy trying learn edit tokenizer one pretrained model en core web md tweet tokenized entire hashtag becomes single token e g want one token hashtagtext default would two token hashtagtext know first person ha faced issue tried implementing advice place online using method output remains hashtagtext two token article show method tried shown code troubleshooting step save default pattern matching regex default token matching regex save regex nlp pretrained model using update nlp token matching regex pre update note originally would see output append regex need w list nlp current using save combination updated token matching regex update regex nlp using variable created updated token matching regex save new regex used nlp verify thing updated correctly nlp token matching regex post update see code see tokenization behavior addition w see printout troubleshooting variable since feel like proven correctly update regex nlp using possible issue could think regex wa wrong tested regex seems behave intended see anyone able see error causing nlp tokenize hashtagtext two token nlp tokenizer token match regex wa updated single token thank
NER tagging schema for non-contiguous tokens,"<p>The most common tagging procedure for NER is IOB. But it seems that this kind of tagging is limited to cases where tokens from the same entity are contiguous.</p>
<p>So for instance,</p>
<p><code>Jane Smith is walking in the park</code> would be tagged as: <code>B-PER I-PER O O O O O</code></p>
<p>And here my PER entity is the concatenation of <code>[Jane, Smith]</code></p>
<p>If we tweak the example:</p>
<p><code>Jane and James Smith are walking in the park</code></p>
<p><code>B-PER O B-PER I-PER O O O O O</code></p>
<p>Now the issue is that the entities we would get are <code>[Jane]</code> and <code>[James, Smith]</code> because the IOB tagging does not allow to link Jane to Smith.</p>
<p>Is there any tagging schema that would allow to mark as entities both <code>[Jane, Smith]</code> and <code>[James, Smith]</code>?</p>
",Named Entity Recognition (NER),ner tagging schema non contiguous token common tagging procedure ner iob seems kind tagging limited case token entity contiguous instance would tagged per entity concatenation tweak example issue entity would get iob tagging doe allow link jane smith tagging schema would allow mark entity
Extracting Products Name from Unstructured text,"<p>I have unstructured text like this</p>
<pre><code>Sample1
From: Gujarat, IN 
To: Kerala, IN
Milk and egg, 100kg 
Please handle with care. 
</code></pre>
<pre><code>Sample2 
Hello,
(Tooth Brush and Paste)120KG in total
P/U: London, GB
Dest: QC, Canada
Keep the product away from heat
F:111-222 333
demo@gmail.com
http://www.sample123.com
</code></pre>
<p>Like these, I have hundreds of thousands of samples with different products</p>
<p>From the above 2 samples, I want to extract the product name</p>
<p>From sample 1 <strong>Milk</strong>,<strong>egg</strong> from sample 2 <strong>Tooth Brush</strong>,<strong>Paste</strong>.</p>
<p>Things I have done so far</p>
<p>I worked on Custom NER using Spacy to extract the product from the samples but it is not efficient, I watched some videos related to NER and also explored similar content in stack overflow but it didn't work well for my problem.</p>
<p>How can I approach this problem further ?</p>
",Named Entity Recognition (NER),extracting product name unstructured text unstructured text like like hundred thousand sample different product sample want extract product name sample milk egg sample tooth brush paste thing done far worked custom ner using spacy extract product sample efficient watched video related ner also explored similar content stack overflow work well problem approach problem
spaCy coreference resolution - named entity recognition (NER) to return unique entity ID&#39;s?,"<p>Perhaps I've skipped over a part of the docs, but what I am trying to determine is a unique ID for each entity in the standard NER toolset. For example:</p>

<pre><code>import spacy
from spacy import displacy
import en_core_web_sm
nlp = en_core_web_sm.load()

text = ""This is a text about Apple Inc based in San Fransisco. ""\
        ""And here is some text about Samsung Corp. ""\
        ""Now, here is some more text about Apple and its products for customers in Norway""

doc = nlp(text)

for ent in doc.ents:
    print('ID:{}\t{}\t""{}""\t'.format(ent.label,ent.label_,ent.text,))


displacy.render(doc, jupyter=True, style='ent')
</code></pre>

<p>returns:</p>

<blockquote>
<pre><code>ID:381    ORG ""Apple Inc"" 
ID:382    GPE ""San Fransisco"" 
ID:381    ORG ""Samsung Corp."" 
ID:381    ORG ""Apple"" 
ID:382    GPE ""Norway""
</code></pre>
</blockquote>

<p>I have been looking at <code>ent.ent_id</code> and <code>ent.ent_id_</code> but these are inactive according to the <a href=""https://spacy.io/api/token"" rel=""nofollow noreferrer"">docs</a>. I couldn't find anything in <code>ent.root</code> either. </p>

<p>For example, in <a href=""https://cloud.google.com/natural-language/"" rel=""nofollow noreferrer"">GCP NLP</a> each entity is returned with an ⟨entity⟩number that enables you to identify multiple instances of the same entity within a text.</p>

<blockquote>
  <p>This is a ⟨text⟩2 about ⟨Apple Inc⟩1 based in ⟨San Fransisco⟩4. And
  here is some ⟨text⟩3 about ⟨Samsung Corp⟩6. Now, here is some more
  ⟨text⟩8 about ⟨Apple⟩1 and its ⟨products⟩5 for ⟨customers⟩7 in
  ⟨Norway⟩9""</p>
</blockquote>

<p>Does spaCy support something similar? Or is there a way using NLTK or Stanford?</p>
",Named Entity Recognition (NER),spacy coreference resolution named entity recognition ner return unique entity id perhaps skipped part doc trying determine unique id entity standard ner toolset example return looking inactive according doc find anything either example gcp nlp entity returned entity number enables identify multiple instance entity within text text apple inc based san fransisco text samsung corp text apple product customer norway doe spacy support something similar way using nltk stanford
Trying to detect products from text while using a dictionary,"<p>I have a list of products names and a collection of text generated from random users. I  am trying to detect products mentioned in the text while talking into account spelling variation. For example the text</p>

<p>Text = i am interested in galxy s8</p>

<p>Mentions the product samsung galaxy s8</p>

<p>But note the difference in spellings.</p>

<p>I've implemented the following approaches:
1- max tokenized products names and users text (i split words by punctuation and digits so s8 will be tokenized into 's' and '8'. Then i did a check on each token in user's text to see if it is in my vocabulary with damerau levenshtein distance &lt;= 1 to allow for variation in spelling. Once i have detected a sequence of tokens that do exist in the vocabulary i do a search for the product that matches the query while checking the damerau levenshtein distance on each token. This gave poor results. Mainly because the sequence of tokens that exist in the vocabulary do not necessarily represent a product. For example since text is max tokenized numbers can be found in the vocabulary and as such dates are detected as products.</p>

<p>2- i constructed bigram and trigram indicies from the list of products and converted each user text into a query.. but also results weren't so great given the spelling variation</p>

<p>3- i manually labeled 270 sentences and trained a named entity recognizer with labels ('O' and 'Product'). I split the data into 80% training and 20% test. Note that I didn't use the list of products as part of the features. Results were okay.. not great tho </p>

<p>None of the above results achieved a reliable performance. I tried regular expressions but since there are so many different combinations to consider it became too complicated.. Are there better ways to tackle this problem? I suppose ner could give better results if i train more data but suppose there isn't enough training data, what do u think a better solution would be? </p>

<p>If i come up with a better alternative to the ones I've already mentioned, I'll add it to this post. In the meantime I'm open to suggestions</p>
",Named Entity Recognition (NER),trying detect product text using dictionary list product name collection text generated random user trying detect product mentioned text talking account spelling variation example text text interested galxy mention product samsung galaxy note difference spelling implemented following approach max tokenized product name user text split word punctuation digit tokenized check token user text see vocabulary damerau levenshtein distance allow variation spelling detected sequence token exist vocabulary search product match query checking damerau levenshtein distance token gave poor result mainly sequence token exist vocabulary necessarily represent product example since text max tokenized number found vocabulary date detected product constructed bigram trigram indicies list product converted user text query also result great given spelling variation manually labeled sentence trained named entity recognizer label product split data training test note use list product part feature result okay great tho none result achieved reliable performance tried regular expression since many different combination consider became complicated better way tackle problem suppose ner could give better result train data suppose enough training data u think better solution would come better alternative one already mentioned add post meantime open suggestion
"Extracting products models for products description text (not reviews, unstructured text)","<p>I'm looking for a method that will help me with this task. I'm familiar with NER but in this case it wont help me, the sentences are short and NER doesn't work well with it.</p>

<p>The input is products description (from Amazon for example, see the marked line in the image below), not something that POS tagging might help with.</p>

<p><a href=""https://i.sstatic.net/eygX7.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/eygX7.png"" alt=""enter image description here""></a></p>

<p>An example:</p>

<pre><code>1. Apple iPhone 6 64GB White
2. iRobot Scooba 390 Floor Scrubbing Robot
3. Samsung UN60J6200 60-Inch TV with HW-J450 Soundbar
</code></pre>

<p>And the results I'm looking for:</p>

<pre><code>1. Brand: Apple, Model: iPhone 6, Features: 64GB, White
2. Brand: iRobot, Model: Scooba 390, Features: Floor Scrubbing Robot
3. Brand: Samsung , Model: UN60J6200 , Features: 60-Inch, HW-J450 Soundbar
</code></pre>

<p>This is a really difficult problem for me and till now Iv'e solved it in a non-generic way and just by using NLP methods. So I wonder if there's a way of training a model for such task.</p>

<p>Some points I want to add:</p>

<ul>
<li>The brand name is not always at the beginning of the sentence</li>
<li>Some times the entire sentence contains only small letters.</li>
<li>In many cases different brands use really similar models for their products (iPhone 6s and Galaxy S6 for example).</li>
</ul>
",Named Entity Recognition (NER),extracting product model product description text review unstructured text looking method help task familiar ner case wont help sentence short ner work well input product description amazon example see marked line image something po tagging might help example result looking really difficult problem till iv e solved non generic way using nlp method wonder way training model task point want add brand name always beginning sentence time entire sentence contains small letter many case different brand use really similar model product iphone galaxy example
Can we find sentences around an entity tagged via NER?,"<p>We have a model ready which identifies a custom named entity. The problem is if the whole doc is given then the model does not work as per expecation if only a few sentences are given, it is giving amazing results.</p>

<p>I want to select two sentences before and after a tagged entity.</p>

<p>eg. If a part of the doc has world Colombo(which is tagged as GPE), I need to select two sentences before the tag and 2 sentences after the tag. I tried a couple of approaches but the complexity is too high.</p>

<p>Is there a built-in way in spacy with which we can address this problem? </p>

<p>I am using python and spacy.</p>

<p>I have tried parsing the doc by identifying the index of the tag. But that approach is really slow.</p>
",Named Entity Recognition (NER),find sentence around entity tagged via ner model ready identifies custom named entity problem whole doc given model doe work per expecation sentence given giving amazing result want select two sentence tagged entity eg part doc ha world colombo tagged gpe need select two sentence tag sentence tag tried couple approach complexity high built way spacy address problem using python spacy tried parsing doc identifying index tag approach really slow
NLTK Named Entity Recognition with Custom Data,"<p>I'm trying to extract named entities from my text using NLTK. I find that NLTK NER is not very accurate for my purpose and I want to add some more tags of my own as well. I've been trying to find a way to train my own NER, but I don't seem to be able to find the right resources. 
I have a couple of questions regarding NLTK-</p>

<ol>
<li>Can I use my own data to train an Named Entity Recognizer in NLTK?</li>
<li>If I can train using my own data, is the named_entity.py the file to be modified?</li>
<li>Does the input file format have to be in IOB eg. Eric NNP B-PERSON ?</li>
<li>Are there any resources - apart from the nltk cookbook and nlp with python that I can use?</li>
</ol>

<p>I would really appreciate help in this regard</p>
",Named Entity Recognition (NER),nltk named entity recognition custom data trying extract named entity text using nltk find nltk ner accurate purpose want add tag well trying find way train ner seem able find right resource couple question regarding nltk use data train named entity recognizer nltk train using data named entity py file modified doe input file format iob eg eric nnp b person resource apart nltk cookbook nlp python use would really appreciate help regard
Unexpected type of NER data when trying to train spacy ner pipe to add new named entity,"<p>I'm trying to add a new named entity to spacy but I couldn't have good examples of Example objects for ner training and I'm getting a value error.
Here is my code:</p>
<pre><code>import spacy
from spacy.util import minibatch, compounding
from pathlib import Path
from spacy.training import Example

nlp=spacy.load('en_core_web_lg')

ner=nlp.get_pipe(&quot;ner&quot;)
TRAIN_DATA=[('ABC is a worldwide organization',{'entities':[0,2,'CRORG']}),
           ('we stand with ABC',{'entities':[24,26,'CRORG']}),
           ('we supports ABC',{'entities':[15,17,'CRORG']})]
ner.add_label('CRORG')
# Disable pipeline components that dont need to change
pipe_exceptions = [&quot;ner&quot;]
unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]

with nlp.disable_pipes(*unaffected_pipes):
    for iteration in range(30):
        random.shuffle(TRAIN_DATA)
        for raw_text,entity_offsets in TRAIN_DATA:
            doc=nlp.make_doc(raw_text)
            nlp.update([Example.from_dict(doc,entity_offsets)])
</code></pre>
<p><a href=""https://i.sstatic.net/tzSyj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tzSyj.png"" alt=""Here is the error message I'm getting"" /></a></p>
",Named Entity Recognition (NER),unexpected type ner data trying train spacy ner pipe add new named entity trying add new named entity spacy good example example object ner training getting value error code
How to get sentence after chunking in NLTK?,"<p>I have a sentence as follow:</p>
<pre><code>txt =  &quot;i am living in the West Bengal and my brother live in New York. My name is John Smith&quot;
</code></pre>
<p>What I need is:</p>
<ol>
<li>Get the Chunks With GPE/location as labels and combine these chunks using &quot;_&quot;</li>
<li>Get the Chunks With PERSON label and remove those chunks.</li>
</ol>
<p>Output I needed:</p>
<pre><code>preprocessed_txt =  &quot;i am living in the West_Bengal and my brother live in New_York. My name is &quot;
</code></pre>
<p>I use code from <a href=""https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list/31837224#31837224"">NLTK Named Entity recognition to a Python list</a> to get the labels of the chunks.</p>
<pre><code>import nltk
for sent in nltk.sent_tokenize(sentence):
   for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):
      if hasattr(chunk, 'label'):
         print(chunk.label(), '_'.join(c[0] for c in chunk))
</code></pre>
<p>This returned me the output as:</p>
<pre><code>LOCATION West_Bengal
GPE New_York
PERSON John_Smith
</code></pre>
<p><strong>What to do next?</strong></p>
",Named Entity Recognition (NER),get sentence chunking nltk sentence follow need get chunk gpe location label combine chunk using get chunk person label remove chunk output needed use code href named entity recognition python list get label chunk returned output next
Finding the Start and End char indices in Spacy,"<p>I am training a custom model in Spacy to extract custom entities but while I need to provide an input train data that consists of my entities along with the index locations, I wanted to understand if there's a faster way to assign the index value for keywords I am looking for in a particular sentence in my training data.</p>
<p>An example of my traning data:</p>
<pre><code>TRAIN_DATA = [

('Behaviour Skills include Communication, Conflict Resolution, Work Life Balance,
 {'entities': [(25, 37, 'BS'),(40, ,60, 'BS'),(62, 79, 'BS')]
 })
            ]
</code></pre>
<p><strong>Now to pass the index location for specific keywords in my training data, I am presently counting it manually to give the location of my keyword.</strong></p>
<p>For example: in case of the first line where I am saying Behaviour skills include Communication etc, I am manually calculating the location of the index for the word &quot;Communication&quot; which is 25,37.</p>
<p>I am sure there must be another way to identify the location of these indices by some other methods instead counting it manually. Any ideas how can I achieve this?</p>
",Named Entity Recognition (NER),finding start end char index spacy training custom model spacy extract custom entity need provide input train data consists entity along index location wanted understand faster way assign index value keywords looking particular sentence training data example traning data pas index location specific keywords training data presently counting manually give location keyword example case first line saying behaviour skill include communication etc manually calculating location index word communication sure must another way identify location index method instead counting manually idea achieve
Is there a way to set &quot;rules&quot; for string matching in R?,"<p>I've been scratching my head trying to find a way to solve this problem without having to get into NLP and start training models. I have 2 rather large data sets that should be able to be matched by name, but the spellings and syntax of them are slightly different, easy enough for a human to understand, but complex enough that my fuzzy matching and levenshtein edit distances cannot. There is a large amount of duplicates in the data set, but enough where I cannot manually map them, so I am trying to create &quot;rules&quot; around what to match. Would a package such as FuzzyWuzzy allow more bespoke elements to solve this? Example below, thanks!</p>
<pre><code>a &lt;- c(&quot;The City of New York&quot;, &quot;The City of New York&quot;, &quot;Los Angeles City&quot;, &quot;The State of California&quot;, &quot;The State of California&quot;)

b &lt;- c(&quot;New York City&quot;, &quot;New York City&quot;, &quot;Los Angeles&quot;, &quot;California State&quot;, &quot;CA State&quot;) 
</code></pre>
<p>The closest I've gotten so far in matching the data sets is fuzzy string matching, but this only works decently well, and still misses a large chunk or makes quite a bit of errors as I increase the max edit distance.</p>
<pre><code>library(fuzzyjoin)

a &lt;- as.tibble(a)
b &lt;- as.tibble(b)
stringdist_inner_join(x = a, y = b, max_dist = 3, method = 'LV', ignore_case = T)
</code></pre>
<p>The dataset as a bit more depth than that. I was hoping to make a &quot;rule&quot; of some sort that &quot;The City of New York&quot; always equals &quot;New York City&quot;, but I'm not sure if there's a smarter way to go about this. I hope this specific text example helps. Thanks a bunch!</p>
",Named Entity Recognition (NER),way set rule string matching r scratching head trying find way solve problem without get nlp start training model rather large data set able matched name spelling syntax slightly different easy enough human understand complex enough fuzzy matching levenshtein edit distance large amount duplicate data set enough manually map trying create rule around match would package fuzzywuzzy allow bespoke element solve example thanks closest gotten far matching data set fuzzy string matching work decently well still miss large chunk make quite bit error increase max edit distance dataset bit depth wa hoping make rule sort city new york always equal new york city sure smarter way go hope specific text example help thanks bunch
Spacy to extract email address of a particular person,"<p>I need to extract email address of a person. I have trained the NER model in Spacy with a few examples but no luck. It has to be trained with thousands of examples to get satisfying results. So, I have now started to look at Token Matcher to fetch the email address. Did anyone work on this before? is there a better approach for this ?</p>
",Named Entity Recognition (NER),spacy extract email address particular person need extract email address person trained ner model spacy example luck ha trained thousand example get satisfying result started look token matcher fetch email address anyone work better approach
"Alternatives to NER taggers for long, heterogeneous phrases?","<p>I am looking for ideas/thoughts on the following problem:</p>
<p>I am working with food ingredient data such as: <em>milk, sugar, eggs, flour, may contain nuts</em></p>
<p>From such piece of text I want to be able to identify and extract phrases like <em>may contain nuts</em>, to preprocess them separately</p>
<p>These kinds of phrases can change quite a lot in terms of length and content. I thought of using NER taggers, but I don't know if they will do the job correctly as they are mainly used for identifying single-word entities...</p>
<p>Any ideas on what to use as a phrase-entity-recognition system? Also which package would you use? Cheers</p>
",Named Entity Recognition (NER),alternative ner tagger long heterogeneous phrase looking idea thought following problem working food ingredient data milk sugar egg flour may contain nut piece text want able identify extract phrase like may contain nut preprocess separately kind phrase change quite lot term length content thought using ner tagger know job correctly mainly used identifying single word entity idea use phrase entity recognition system also package would use cheer
named entity recognition with spacy,"<p>I'm working on natural language processing using spacy library in python.
From input i get several sentences that i work seperatly using this</p>
<pre><code>for sent in doc.sents:
</code></pre>
<p>for each sent i search for any named entity using .ents attribute.
What i would like to achieve is replacing the initial &quot;sent&quot; with a new one where every named entity recognized is replaced on the initial sentence.
Here an example:</p>
<pre><code>First sentence: Apple is looking at buying U.K. startup for $1 billion
After replacing: ORG is looking at buying GPE startup for MONEY
</code></pre>
<p>Of course using a simple string.replace doesnt work since i would like to have a new spacy.Doc
Any idea how to achieve this?</p>
",Named Entity Recognition (NER),named entity recognition spacy working natural language processing using spacy library python input get several sentence work seperatly using sent search named entity using ents attribute would like achieve replacing initial sent new one every named entity recognized replaced initial sentence example course using simple string replace doesnt work since would like new spacy doc idea achieve
Performing named-entity recognition on sentences that are poorly cased to extract company names,"<p>I have a database of sentences from which I am trying to extract any and all <strong>company names</strong>. As of now, I am using spaCy's Named Entity Recognition and achieve good results for the sentences that have standard capitalization.
The problem arises when I try to do the same thing for sentences that do NOT have standard capitalization. In particular, I get poor performance for the subset of the database that use &quot;title casing&quot; (i.e. all words apart from prepositions/articles/etc. are capitalized).</p>
<p>Here are some examples of these kind of sentences along with the current result I get with spaCy, and the result that I want:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Sentence</th>
<th style=""text-align: left;"">Current Extraction</th>
<th style=""text-align: left;"">Desired Extraction(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">Caribbean Airlines Transforms its Revenue Accounting Process</td>
<td style=""text-align: left;"">Caribbean Airlines Transforms its Revenue Accounting</td>
<td style=""text-align: left;"">Caribbean Airlines</td>
</tr>
<tr>
<td style=""text-align: left;"">Scoular Drives Employee Development With Absorb LMS</td>
<td style=""text-align: left;"">Scoular Drives Employee Development With Absorb</td>
<td style=""text-align: left;"">(Scoular, Absorb LMS)</td>
</tr>
<tr>
<td style=""text-align: left;"">Oracle Solution Reduces Operating Costs by 25 Percent</td>
<td style=""text-align: left;"">Oracle Solution Reduces Operating Costs</td>
<td style=""text-align: left;"">Oracle</td>
</tr>
<tr>
<td style=""text-align: left;"">Pandora CFO Cuts Procurement Time with Coupa</td>
<td style=""text-align: left;"">Pandora CFO Cuts Procurement Time</td>
<td style=""text-align: left;"">(Pandora, Coupa)</td>
</tr>
</tbody>
</table>
</div>
<p>As you can see the excess of capitalization makes spaCy believe that a lot more words are part of the entity name than is actually the case.
So my question is how can I mitigate this issue? Are there other libraries that are maybe not so sensitive to such capitalization or maybe I can preprocess the sentences by &quot;truecasing&quot; them. What are the standard procedures?</p>
<p>Just for completeness sake, here is how I use the spaCy library</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_md&quot;)
for sentence in sentences:
    doc = nlp(sentence)
    for ent in doc.ents:
        ... store in database (ORG) ...
</code></pre>
",Named Entity Recognition (NER),performing named entity recognition sentence poorly cased extract company name database sentence trying extract company name using spacy named entity recognition achieve good result sentence standard capitalization problem arises try thing sentence standard capitalization particular get poor performance subset database use title casing e word apart preposition article etc capitalized example kind sentence along current result get spacy result want sentence current extraction desired extraction caribbean airline transforms revenue accounting process caribbean airline transforms revenue accounting caribbean airline scoular drive employee development absorb lm scoular drive employee development absorb scoular absorb lm oracle solution reduces operating cost percent oracle solution reduces operating cost oracle pandora cfo cut procurement time coupa pandora cfo cut procurement time pandora coupa see excess capitalization make spacy believe lot word part entity name actually case question mitigate issue library maybe sensitive capitalization maybe preprocess sentence truecasing standard procedure completeness sake use spacy library
Spacy find text before specified word,"<p>I'm working on the sentence <code>Hall is a Tony Award winner and Grammy nominee</code> and would like to extract the awards won (<code>Tony Award</code>), using spaCy Rule-Matcher, but I can't seem to be able to tell spaCy to look for words that come before <code>winner</code>. Is that possible? If so how could one go about it?</p>
<pre><code>nlp = en_core_web_sm.load()

awards_lexical = [
            {'TEXT': {'REGEX': '\s*'}, 'OP': '*'},
            {'IS_PUNCT': True, 'OP': '*'},
            {'TEXT': {'REGEX': '^(winner|recipient)$'}},
            {'OP': '+'},
            ]
def matching(doc, pattern):
    result = []
    for sent in doc.sents:
        matcher = Matcher(nlp.vocab) 
        matcher.add(&quot;matching&quot;, None, pattern)  

        matches = matcher(nlp(str(sent))) 
        if len(matches)&gt;0:
            match = matches[-1]
            span = sent[match[1]:match[2]] 
            result.append(span.text)

    return result

csv_reader = csv.reader(open('Matheus_Schmitz_hw02_bios.csv', encoding='utf-8'))
limit = 500
count = 0

open(&quot;hw2_lexical.jl&quot;, &quot;w&quot;).close()
with open('hw2_lexical.jl', 'w') as hw2_lexical:
    for (idx, (url, bio)) in tqdm(enumerate(csv_reader), total=limit):
        count += 1
        result = {}
        result['url'] = url
        result['awards'] = matching(nlp(bio), awards_lexical)        
        hw2_lexical.write(str(result)+'\n')
        if count&gt;=limit:
            break
        pass
    hw2_lexical.close()
print(count)
</code></pre>
<p>From my code, I'd think that spaCy would include any text before the chosen word, but all variations I've are only giving me the text from winner|won|awarded onwards, not the text before, which is where the prize name most often is.</p>
",Named Entity Recognition (NER),spacy find text specified word working sentence would like extract award using spacy rule matcher seem able tell spacy look word come possible could one go code think spacy would include text chosen word variation giving text winner awarded onwards text prize name often
Python NER: add custom text and labels to update the NER model,"<p>I'm using NER to essentially scrub text so that each named entity is replaced with its label (PERSON, ORG, etc.). So &quot;John works at Apple&quot; would become &quot;PERSON works at ORG.&quot;</p>
<p><code>clause_text</code> is my list of sentences. I used the ner-d package to build my NER model and scrub text as follows:</p>
<pre><code>for text in clause_text:
    input_text = text
    doc = ner.name(input_text, language='en_core_web_sm')
    text_label = [(X.text, X.label_) for X in doc]

    # replace all named entities with their label (PERSON, ORG, etc)
    for text, label in text_label:
       input_text = input_text.replace(text, label)
    scrubbed_text.append(input_text)
</code></pre>
<p>Now, I am trying to add custom training data. Basically I want to be able to add a sentence with labels and update the NER model to make it more accurate/specific to what I need it to do. Right now I have this:</p>
<pre><code>nlp = spacy.load('en_core_web_sm')

if 'ner' not in nlp.pipe_names:
    ner = nlp.create_pipe('ner')
    nlp.add_pipe(ner)
else:
    ner = nlp.get_pipe('ner')
</code></pre>
<pre><code>from spacy.gold import GoldParse
from spacy.pipeline import EntityRecognizer

doc_list = [] 
doc = nlp('This EULA stipulates a contract for Hamilton Enterprises.') 
doc_list.append(doc) 
gold_list = [] 
gold_list.append(GoldParse(doc, [u'O', u'O', u'O', u'O', u'O', u'O', u'ORG'])) 
  
ner = EntityRecognizer(nlp.vocab, entity_types = ['ORG']) 
ner.update(doc_list, gold_list) 
</code></pre>
<p>But when I run this, I get this error:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-11-92c53f5c90b1&gt; in &lt;module&gt;
      9 
     10 ner = EntityRecognizer(nlp.vocab, entity_types = ['ORG'])
---&gt; 11 ner.update(doc_list, gold_list)

nn_parser.pyx in spacy.syntax.nn_parser.Parser.update()

nn_parser.pyx in spacy.syntax.nn_parser.Parser.require_model()

ValueError: [E109] Model for component 'ner' not initialized. Did you forget to load a model, or forget to call begin_training()?
</code></pre>
<p>Does anyone have any insight on how to best fix this code, or if there's a better way to add custom entries to update the NER model? Thanks so much!</p>
",Named Entity Recognition (NER),python ner add custom text label update ner model using ner essentially scrub text named entity replaced label person org etc john work apple would become person work org list sentence used ner package build ner model scrub text follows trying add custom training data basically want able add sentence label update ner model make accurate specific need right run get error doe anyone insight best fix code better way add custom entry update ner model thanks much
Using Conditional Random Fields for Nested named entity recognition,"<p>My question is the following.</p>
<p>When we work on Named entity recognition tasks, in most cases the classic LSTM-CRF architecture is used, where the CRF uses the Viterbi decoder and the transition matrix to find the best tag sequence associated to a sentence.</p>
<p>My question is, if a token is now associated to multiple entities and not just one (which is the case of Nested NER), as in the case of Bank of China, where China is a location and Bank of China is an organization. Can the CRF algorithm be adapted for this case? That is, finding more than one possible path in the sequence.</p>
",Named Entity Recognition (NER),using conditional random field nested named entity recognition question following work named entity recognition task case classic lstm crf architecture used crf us viterbi decoder transition matrix find best tag sequence associated sentence question token associated multiple entity one case nested ner case bank china china location bank china organization crf algorithm adapted case finding one possible path sequence
spaCy blank NER model underfitting even when trained on a large dataset,"<p>I am trying to create a custom NER model for identifying cybersecurity related entities (27 of them). I decided to go with a blank model because I think I have a large enough (not sure about this) training dataset (~11k sentences extracted from Wikipedia).</p>
<p>To create the training data required by spaCy, I used the <em>PhraseMatcher</em> utility. The idea is to match certain predefined words/phrases related to the entities I want to identify as illustrated below:</p>
<pre><code>import spacy
from spacy.matcher import PhraseMatcher
nlp = spacy.load(&quot;en&quot;)

import pandas as pd
from tqdm import tqdm

from collections import defaultdict
</code></pre>
<h2>Specify Matcher Labels</h2>
<pre><code>users_pattern = [nlp(text) for text in (&quot;user&quot;, &quot;human&quot;, &quot;person&quot;, &quot;people&quot;, &quot;end user&quot;)]
devices_pattern =  [nlp(text) for text in (&quot;device&quot;, &quot;peripheral&quot;, &quot;appliance&quot;, &quot;component&quot;, &quot;accesory&quot;, &quot;equipment&quot;, &quot;machine&quot;)]
accounts_pattern = [nlp(text) for text in (&quot;account&quot;, &quot;user account&quot;, &quot;username&quot;, &quot;user name&quot;, &quot;loginname&quot;, &quot;login name&quot;, &quot;screenname&quot;, &quot;screen name&quot;, &quot;account name&quot;)]
identifiers_pattern = [nlp(text) for text in (&quot;attribute&quot;, &quot;id&quot;, &quot;ID&quot;, &quot;code&quot;, &quot;ID code&quot;)]
authentication_pattern = [nlp(text) for text in (&quot;authentication&quot;, &quot;authenticity&quot;, &quot;certification&quot;, &quot;verification&quot;, &quot;attestation&quot;, &quot;authenticator&quot;, &quot;authenticators&quot;)]
time_pattern = [nlp(text) for text in (&quot;time&quot;, &quot;date&quot;, &quot;moment&quot;, &quot;present&quot;, &quot;pace&quot;, &quot;moment&quot;)]
unauthorized_pattern = [nlp(text) for text in (&quot;unauthorized&quot;, &quot;illegal&quot;, &quot;illegitimate&quot;, &quot;pirated&quot;, &quot;unapproved&quot;, &quot;unjustified&quot;, &quot;unofficial&quot;)]
disclosure_pattern = [nlp(text) for text in (&quot;disclosure&quot;, &quot;acknowledgment&quot;, &quot;admission&quot;, &quot;exposure&quot;, &quot;advertisement&quot;, &quot;divulgation&quot;)]
network_pattern = [nlp(text) for text in (&quot;network&quot;, &quot;net&quot;, &quot;networking&quot;, &quot;internet&quot;, &quot;Internet&quot;)]
wireless_pattern = [nlp(text) for text in (&quot;wireless&quot;, &quot;wifi&quot;, &quot;Wi-Fi&quot;, &quot;wireless networking&quot;)]
password_pattern = [nlp(text) for text in (&quot;password&quot;, &quot;passwords&quot;, &quot;passcode&quot;, &quot;passphrase&quot;)]
configuration_pattern = [nlp(text) for text in (&quot;configuration&quot;, &quot;composition&quot;)]
signatures_pattern = [nlp(text) for text in (&quot;signature&quot;, &quot;signatures&quot;, &quot;digital signature&quot;, &quot;electronic signature&quot;)]
certificates_pattern = [nlp(text) for text in (&quot;certificate&quot;, &quot;digital certificates&quot;, &quot;authorization certificate&quot;, &quot;public key certificates&quot;, &quot;PKI&quot;, &quot;X509&quot;, &quot;X.509&quot;)]
revocation_pattern = [nlp(text) for text in (&quot;revocation&quot;, &quot;annulment&quot;, &quot;cancellation&quot;)]
keys_pattern = [nlp(text) for text in (&quot;key&quot;, &quot;keys&quot;)]
algorithms_pattern = [nlp(text) for text in (&quot;algorithm&quot;, &quot;algorithms&quot;, &quot;formula&quot;, &quot;program&quot;)]
standard_pattern = [nlp(text) for text in (&quot;standard&quot;, &quot;standards&quot;, &quot;specification&quot;, &quot;specifications&quot;, &quot;norm&quot;, &quot;rule&quot;, &quot;rules&quot;, &quot;RFC&quot;)]
invalid_pattern = [nlp(text) for text in (&quot;invalid&quot;, &quot;false&quot;, &quot;unreasonable&quot;, &quot;inoperative&quot;)]
access_pattern = [nlp(text) for text in (&quot;access&quot;, &quot;connection&quot;, &quot;entry&quot;, &quot;entrance&quot;)]
blocking_pattern = [nlp(text) for text in (&quot;blocking&quot;, &quot;block&quot;, &quot;blacklist&quot;, &quot;blocklist&quot;, &quot;close&quot;, &quot;cut off&quot;, &quot;deter&quot;, &quot;prevent&quot;, &quot;stop&quot;)]
notification_pattern = [nlp(text) for text in (&quot;notification&quot;, &quot;notifications&quot;, &quot;notice&quot;, &quot;warning&quot;)]
messages_pattern = [nlp(text) for text in (&quot;message&quot;, &quot;messages&quot;, &quot;note&quot;, &quot;news&quot;)]
untrusted_pattern = [nlp(text) for text in (&quot;untrusted&quot;, &quot;malicious&quot;, &quot;unsafe&quot;)]
security_pattern = [nlp(text) for text in (&quot;security&quot;, &quot;secure&quot;, &quot;securely&quot;, &quot;protect&quot;, &quot;defend&quot;, &quot;guard&quot;)]
symmetric_pattern = [nlp(text) for text in (&quot;symmetric&quot;, &quot;symmetric crypto&quot;)]
asymmetric_pattern = [nlp(text) for text in (&quot;asymmetric&quot;, &quot;asymmetric crypto&quot;)]

matcher = PhraseMatcher(nlp.vocab)
matcher.add(&quot;USER&quot;, None, *users_pattern)
matcher.add(&quot;DEVICE&quot;, None, *devices_pattern)
matcher.add(&quot;ACCOUNT&quot;, None, *accounts_pattern)
matcher.add(&quot;IDENTIFIER&quot;, None, *identifiers_pattern)
matcher.add(&quot;AUTHENTICATION&quot;, None, *authentication_pattern)
matcher.add(&quot;TIME&quot;, None, *time_pattern)
matcher.add(&quot;UNAUTHORIZED&quot;, None, *unauthorized_pattern)
matcher.add(&quot;DISCLOSURE&quot;, None, *disclosure_pattern)
matcher.add(&quot;NETWORK&quot;, None, *network_pattern)
matcher.add(&quot;WIRELESS&quot;, None, *wireless_pattern)
matcher.add(&quot;PASSWORD&quot;, None, *password_pattern)
matcher.add(&quot;CONFIGURATION&quot;, None, *configuration_pattern)
matcher.add(&quot;SIGNATURE&quot;, None, *signatures_pattern)
matcher.add(&quot;CERTIFICATE&quot;, None, *certificates_pattern)
matcher.add(&quot;REVOCATION&quot;, None, *revocation_pattern)
matcher.add(&quot;KEY&quot;, None, *keys_pattern)
matcher.add(&quot;ALGORITHM&quot;, None, *algorithms_pattern)
matcher.add(&quot;STANDARD&quot;, None, *standard_pattern)
matcher.add(&quot;INVALID&quot;, None, *invalid_pattern)
matcher.add(&quot;ACCESS&quot;, None, *access_pattern)
matcher.add(&quot;BLOCKING&quot;, None, *blocking_pattern)
matcher.add(&quot;NOTIFICATION&quot;, None, *notification_pattern)
matcher.add(&quot;MESSAGE&quot;, None, *messages_pattern)
matcher.add(&quot;UNTRUSTED&quot;, None, *untrusted_pattern)
matcher.add(&quot;SECURITY&quot;, None, *security_pattern)
matcher.add(&quot;SYMMETRIC&quot;, None, *symmetric_pattern)
matcher.add(&quot;ASYMMETRIC&quot;, None, *asymmetric_pattern)
</code></pre>
<h2>Prepare Training Data</h2>
<pre><code>def offsetter(lbl, doc, matchitem):
    &quot;&quot;&quot;
    Convert PhaseMatcher result to the format required in training (start, end, label)
    &quot;&quot;&quot;
    o_one = len(str(doc[0:matchitem[1]]))
    subdoc = doc[matchitem[1]:matchitem[2]]
    o_two = o_one + len(str(subdoc))
    return (o_one, o_two, lbl)


to_train_ents = []
count_dic = defaultdict(int)

# Load the original sentences
df = pd.read_csv(&quot;sentences.csv&quot;, index_col=False)
phrases = df[&quot;sentence&quot;].values

for line in tqdm(phrases):

    nlp_line = nlp(line)
    matches = matcher(nlp_line)
    
    if matches:
        
        for match in matches:

            match_id = match[0]
            start = match[1]
            end = match[2]

            label = nlp.vocab.strings[match_id]  # get the unicode ID, i.e. 'COLOR'
            span = nlp_line[start:end]  # get the matched slice of the doc

            count_dic[label] += 1

            res = [offsetter(label, nlp_line, match)]
            to_train_ents.append((line, dict(entities=res)))
           
count_dic = dict(count_dic)
        
TRAIN_DATA =  to_train_ents
</code></pre>
<p>After executing the above code, I obtained the training data in the format required by spaCy. These sentences contain the entities I am interested which are distributed as shown below:</p>
<pre><code>print(sorted(count_dic.items(), key=lambda x:x[1], reverse=True), len(count_dic))
sum(count_dic.values())


[('NETWORK', 1962), ('TIME', 1489), ('USER', 1206), ('SECURITY', 981), ('DEVICE', 884), ('STANDARD', 796), ('ACCESS', 652), ('ALGORITHM', 651), ('MESSAGE', 605), ('KEY', 423), ('IDENTIFIER', 389), ('BLOCKING', 354), ('AUTHENTICATION', 141), ('WIRELESS', 109), ('UNAUTHORIZED', 99), ('CONFIGURATION', 89), ('ACCOUNT', 86), ('UNTRUSTED', 77), ('PASSWORD', 62), ('DISCLOSURE', 58), ('NOTIFICATION', 55), ('INVALID', 44), ('SIGNATURE', 41), ('SYMMETRIC', 23), ('ASYMMETRIC', 11), ('CERTIFICATE', 10), ('REVOCATION', 9)] 27
11306
</code></pre>
<p>I then used the <em>standard</em> training procedure to train a blank NER model in spaCy illustrated below.</p>
<h2>Training the Blank model</h2>
<pre><code># define variables
model = None  
n_iter = 100

if model is not None:
    nlp_new = spacy.load(model)  # load existing spaCy model
    print(&quot;Loaded model '%s'&quot; % model)
else:
    nlp_new = spacy.blank(&quot;en&quot;)  # create blank Language class
    print(&quot;Created blank 'en' model&quot;)

# Add entity recognizer to model if it's not in the pipeline
# nlp.create_pipe works for built-ins that are registered with spaCy
if &quot;ner&quot; not in nlp_new.pipe_names:
    ner = nlp_new.create_pipe(&quot;ner&quot;)
    nlp_new.add_pipe(ner)
# otherwise, get it, so we can add labels to it
else:
    ner = nlp_new.get_pipe(&quot;ner&quot;)


# add labels
for _, annotations in TRAIN_DATA:
    for ent in annotations.get(&quot;entities&quot;):
        ner.add_label(ent[2])
            
# get names of other pipes to disable them during training
other_pipes = [pipe for pipe in nlp_new.pipe_names if pipe != &quot;ner&quot;]

with nlp_new.disable_pipes(*other_pipes):  # only train NER
    
    if model is None:
        optimizer = nlp_new.begin_training()
    else:
        optimizer = nlp_new.resume_training()
    
    
    # Set this based on this resource: spacy compounding batch size
    sizes = compounding(1, 16, 1.001)
    
    # batch up the examples using spaCy's minibatch
    for itn in tqdm(range(n_iter)):
        losses = {}
        random.shuffle(TRAIN_DATA)
        batches = minibatch(TRAIN_DATA, size=sizes)
        for batch in batches:
            texts, annotations = zip(*batch)
            nlp_new.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)
        print(&quot;Losses&quot;, losses)
</code></pre>
<p>The final loss after this is about 500.</p>
<p>Finally, I tested how the new model performed using the training data. I would expect to recover as much as entities as specified originally in the training dataset. However, after running the below code I only get about ~600 instances out of ~11k in total.</p>
<h2>Test Trained Model</h2>
<pre><code>count_dic = defaultdict(int)

for text, _ in TRAIN_DATA:
    
    doc = nlp_new(text)
    
    for ent in doc.ents:
        count_dic[ent.label_] += 1
        
print(sorted(count_dic.items(), key=lambda x:x[1], reverse=True), len(count_dic))
sum(count_dic.values())

[('TIME', 369), ('NETWORK', 47), ('IDENTIFIER', 41), ('BLOCKING', 28), ('USER', 22), ('STANDARD', 22), ('SECURITY', 15), ('MESSAGE', 15), ('ACCESS', 7), ('CONFIGURATION', 7), ('DEVICE', 7), ('KEY', 4), ('ALGORITHM', 3), ('SYMMETRIC', 2), ('UNAUTHORIZED', 2), ('SIGNATURE', 2), ('WIRELESS', 1), ('DISCLOSURE', 1), ('INVALID', 1), ('PASSWORD', 1), ('NOTIFICATION', 1)] 21
598
</code></pre>
<p>I wonder why this procedure is producing a model with such underfitting behavior. I am aware of the comments in these posts: <a href=""https://stackoverflow.com/questions/58962469/ner-training-using-spacy"">NER training using Spacy</a> and <a href=""https://stackoverflow.com/questions/60008854/spacy-custom-ner-is-not-returning-any-entity"">SPACY custom NER is not returning any entity</a> but they do not address my issue.</p>
<p>I hope you can provide any feedback about what I have done and how I can improve detection of entities in the training set. I thought that 11k sentences would be enough unless I am doing something wrong. I am using Python 3.6.9 and spaCy 2.2.4.</p>
<p>Thank you so much for your help.</p>
<h2>Update</h2>
<p>I decided to train the model including both positive and negative samples. Now the training data has over 40k sentences. This change however, does dot improve the classification result in the training set. Any other suggestions?</p>
<h2>Training dataset</h2>
<p>The complete training dataset can be downloaded from <a href=""https://drive.google.com/file/d/1ZNwVrW4kyXYSEn4-tmZqFfAqRaVM9b56/view?usp=sharing"" rel=""nofollow noreferrer"">here</a>.</p>
",Named Entity Recognition (NER),spacy blank ner model underfitting even trained large dataset trying create custom ner model identifying cybersecurity related entity decided go blank model think large enough sure training dataset k sentence extracted wikipedia create training data required spacy used phrasematcher utility idea match certain predefined word phrase related entity want identify illustrated specify matcher label prepare training data executing code obtained training data format required spacy sentence contain entity interested distributed shown used standard training procedure train blank ner model spacy illustrated training blank model final loss finally tested new model performed using training data would expect recover much entity specified originally training dataset however running code get instance k total test trained model wonder procedure producing model underfitting behavior aware comment post address issue hope provide feedback done improve detection entity training set thought k sentence would enough unless something wrong using python spacy thank much help update decided train model including positive negative sample training data ha k sentence change however doe dot improve classification result training set suggestion training dataset complete training dataset downloaded
How can I find relationship between two entities or words using NLP?,"<p>I am fairly new to NLP. I am finding difficulty in finding the relationship between two words. For example, <em>Sarah spends $10. Max Spends $100. All of them spent some amount. In total, they spent $500.</em></p>
<p>Now, there are multiple persons spending some money, i.e., <code>Sarah, Max, All of them,</code> and <code>They</code>. And for some of them, there is an associated amount of money that they spend. I can train the NLP model to do NER and find out which represents a person and which word represents money, but how can I build a solution which tells me that <code>$10</code> is associated with <code>Sarah</code>, <code>$100</code> is associated with <code>Max</code>, <code>$500</code> is associated with <code>They</code>.</p>
<p>One more thing, what is this type of problem called in NLP?
I appreciate your help. It is also great if you can provide me just with the topics that I can research to explore this type of problem.
Thank you!</p>
",Named Entity Recognition (NER),find relationship two entity word using nlp fairly new nlp finding difficulty finding relationship two word example sarah spends max spends spent amount total spent multiple person spending money e associated amount money spend train nlp model ner find represents person word represents money build solution tell associated associated associated one thing type problem called nlp appreciate help also great provide topic research explore type problem thank
Access field values of an instance in AllenNLP,"<p>I´m using AllenNLP for a combined classifier (one of its task is NER tagging), and while designing some tests, I've come across this doubt: how do I access the values of the different fields of my AllenNLP instance? I want to compared them with the values introduced, to make sure nothing got lost in the preprocessing.</p>
<p>I managed to get to the text field of the instance using <code>instance.__getitem__(&quot;text&quot;)</code>, but I dont know how to get the value from there, or if there is anything quicker directly from the Instance class.</p>
<p>What I'm trying to do is something like <code>assert instance.getValueFromField(&quot;text) == training_dataset[&quot;text&quot;][0]</code></p>
",Named Entity Recognition (NER),access field value instance allennlp using allennlp combined classifier one task ner tagging designing test come across doubt access value different field allennlp instance want compared value introduced make sure nothing got lost preprocessing managed get text field instance using dont know get value anything quicker directly instance class trying something like
Difference between ne_chunk from NLTK and stanza for NER?,"<p>Sorry this is my first question!</p>
<p>I am starting named entity recognition on python and have used ne_chunk and stanza.</p>
<p>I would like to know what the difference is between their pre-trained models for the NER.  How do they recognize named entities?</p>
",Named Entity Recognition (NER),difference ne chunk nltk stanza ner sorry first question starting named entity recognition python used ne chunk stanza would like know difference pre trained model ner recognize named entity
Extraction of features such as skills and responsibilities from job advertisements using python,"<p>I am currently working on a project in information extraction from Job advertisements, we extracted the email addresses, telephone numbers, and addresses using regex but we are finding it difficult extracting features such as job title, name of the company, skills, and qualifications. Can anyone advise me on how we could extract them?</p>
<p>We found out that custom entities and custom dictionaries can be used as inputs to extract such attributes. When it comes to skills and responsibilities as they are sentences or paragraphs we are finding it difficult to extract them.
We have used spacy so far, is there a better package or methodology that can be used?</p>
",Named Entity Recognition (NER),extraction feature skill responsibility job advertisement using python currently working project information extraction job advertisement extracted email address telephone number address using regex finding difficult extracting feature job title name company skill qualification anyone advise could extract found custom entity custom dictionary used input extract attribute come skill responsibility sentence paragraph finding difficult extract used spacy far better package methodology used
How to merge multiword NER tags?,"<p>I am using allennlp for NER tagging currently. </p>

<p>Code:</p>

<pre><code>from allennlp.predictors.predictor import Predictor
predictor = Predictor.from_path(""...path to model..."")
sentence = ""Top Gun was inspired by a newspaper article.""
result = predictor.predict(sentence)
lang = {}
for word, tag in zip(result[""words""], result[""tags""]):
  if tag != ""O"":
    lang[word] = tag
</code></pre>

<p>Are there any parsers which could merge the output below so that it returns ""Top Gun"" and tag ""WORK_OF_ART"" ?</p>

<pre><code>{'Top': 'B-WORK_OF_ART', 'Gun': 'L-WORK_OF_ART'}
</code></pre>
",Named Entity Recognition (NER),merge multiword ner tag using allennlp ner tagging currently code parser could merge output return top gun tag work art
Extreme performance disparity between training output and evaluate function with flair NLP?,"<p>Ive trained a custom NER model in flair and after the training is completed, it outputs the results which were</p>
<pre><code>Results:
- F1-score (micro) 0.5714
- F1-score (macro) 0.4831

By class:
SymProp    tp: 13 - fp: 25 - fn: 21 - precision: 0.3421 - recall: 0.3824 - f1-score: 0.3611

SymRel     tp: 3 - fp: 3 - fn: 7 - precision: 0.5000 - recall: 0.3000 - f1-score: 0.3750

Symptom    tp: 46 - fp: 19 - fn: 18 - precision: 0.7077 - recall: 0.7188 - f1-score: 0.7132
</code></pre>
<p>then i used the evaluate function using this code:</p>
<pre><code>from flair.models import SequenceTagger

tagger = SequenceTagger.load('/content/flairmodels/ner/final-model.pt')


result, score = tagger.evaluate(corpus.test, mini_batch_size=1, out_path=f&quot;predictions.txt&quot;)

print(result.detailed_results)
</code></pre>
<p>which outputted:</p>
<pre><code>Results:
- F1-score (micro) 0.9580
- F1-score (macro) 0.9520
By class:
SymProp    tp: 48 - fp: 3 - fn: 4 - precision: 0.9412 - recall: 0.9231 - f1-score: 0.9320
SymRel     tp: 17 - fp: 0 - fn: 2 - precision: 1.0000 - recall: 0.8947 - f1-score: 0.9444
Symptom    tp: 72 - fp: 2 - fn: 1 - precision: 0.9730 - recall: 0.9863 - f1-score: 0.9796
</code></pre>
<p>This has confused me drastically. One performance is quite bad where as the other is incredible. It is performed on small data. let me know if im thoroughly misunderstanding something. Thanks so much.</p>
",Named Entity Recognition (NER),extreme performance disparity training output evaluate function flair nlp ive trained custom ner model flair training completed output result used evaluate function using code outputted ha confused drastically one performance quite bad incredible performed small data let know im thoroughly misunderstanding something thanks much
Stanford CoreNLP Output,"<p>Is there anyway to generate this output from the Stanford CoreNLP server?
<a href=""https://drive.google.com/drive/folders/1K2g7nBzHgOpiBQZFRQBNWbylIvCANsdQ?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/1K2g7nBzHgOpiBQZFRQBNWbylIvCANsdQ?usp=sharing</a></p>
<p>I have tried running the server on sample sentences with the following annotators:
'tokenize','ssplit','pos','lemma','depparse','natlog','openie', 'ner', 'parse'
and get similar data, just in a different format.</p>
<p>I am assuming that the format I am trying to get the output into is the default output from an older version of CoreNLP. Is there any way to get the output in the format needed?</p>
",Named Entity Recognition (NER),stanford corenlp output anyway generate output stanford corenlp server tried running server sample sentence following annotator tokenize ssplit po lemma depparse natlog openie ner parse get similar data different format assuming format trying get output default output older version corenlp way get output format needed
Distinguish Person&#39;s names from Organization names in structured table column,"<p>Are there any solutions to distinguish person names from organization names?</p>
<p>I was thinking NER, however the data are stored in a structured table (and are not unstructured sentences). Specifically, the <code>NAME</code> column lists person and organization names (which I'm trying to distinguish). In the below example, I would like to produce the values listed within the <code>PERSON</code> column, based on the values listed within the <code>NAME</code> column.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">NAME</th>
<th style=""text-align: center;"">PERSON</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">Tom Hanks</td>
<td style=""text-align: center;"">TRUE</td>
</tr>
<tr>
<td style=""text-align: center;"">Nissan Motors</td>
<td style=""text-align: center;"">FALSE</td>
</tr>
<tr>
<td style=""text-align: center;"">Ryan Reynolds</td>
<td style=""text-align: center;"">TRUE</td>
</tr>
<tr>
<td style=""text-align: center;"">Tesla</td>
<td style=""text-align: center;"">FALSE</td>
</tr>
<tr>
<td style=""text-align: center;"">Jeff's Cafe</td>
<td style=""text-align: center;"">FALSE</td>
</tr>
</tbody>
</table>
</div>",Named Entity Recognition (NER),distinguish person name organization name structured table column solution distinguish person name organization name wa thinking ner however data stored structured table unstructured sentence specifically column list person organization name trying distinguish example would like produce value listed within column based value listed within column name person tom hank true nissan motor false ryan reynolds true tesla false cafe false
SpaCy &#39;nlp.to_disk&#39; is not saving to disk,"<p>I am trying to figure out why my custom SpaCy NER model isn't saving to disk using <strong>nlp.to_disk</strong>. I am using this condition in my python script:</p>
<pre><code> # save model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.to_disk(output_dir)
        print(&quot;Saved model to&quot;, output_dir)
</code></pre>
<p>The <strong>output_dir</strong> is defined at the top of my script as:</p>
<pre><code>@plac.annotations(
    model=(&quot;Model name. Defaults to blank 'en' model.&quot;, &quot;option&quot;, &quot;m&quot;, str),
    output_dir=(&quot;Optional output directory&quot;, &quot;option&quot;, &quot;o&quot;, Path),
    n_iter=(&quot;Number of training iterations&quot;, &quot;option&quot;, &quot;n&quot;, int),
)
</code></pre>
<p>The model runs without any errors and all of the outputs are correct but it doesn't save to disk. I am not sure what I am missing here. Any help is greatly appreciated.</p>
",Named Entity Recognition (NER),spacy nlp disk saving disk trying figure custom spacy ner model saving disk using nlp disk using condition python script output dir defined top script model run without error output correct save disk sure missing help greatly appreciated
How does this for loop work in Spacy&#39;s custom NER training code?,"<p>I am writing a code to train custom entities in Spacy's NER engine. I am stuck in understanding a small part of the code from an online tutorial. Here's a <a href=""https://gist.github.com/DataTurks/f6035b1e58497d52bf88517ff7bf64cf"" rel=""nofollow noreferrer"">link to the tutorial</a>. The following is the code, I am stuck understanding the two for loops under the comment <code># add labels</code>. I am new to python.</p>
<pre><code>import spacy
################### Train Spacy NER.###########
def train_spacy():
    TRAIN_DATA = convert_dataturks_to_spacy(&quot;dataturks_downloaded.json&quot;);
    nlp = spacy.blank('en')  # create blank Language class
    # create the built-in pipeline components and add them to the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)

    # add labels
    for _, annotations in TRAIN_DATA:
        for ent in annotations.get('entities'):
            ner.add_label(ent[2])
</code></pre>
<p>Apparently, this for loop is adding custom labels to the NER. My questions are;</p>
<ol>
<li>What is an 'annotations', what is its data type? (I googled for 'spacy annotation' but couldn't find the answer)</li>
<li><strong>Why are there two variables to the left of 'in', ('_' and 'annotation')</strong>?</li>
<li>What does ent[2] return? What's at pos 2?</li>
</ol>
",Named Entity Recognition (NER),doe loop work spacy custom ner training code writing code train custom entity spacy ner engine stuck understanding small part code online tutorial link tutorial following code stuck understanding two loop comment new python apparently loop adding custom label ner question annotation data type googled spacy annotation find answer two variable left annotation doe ent return po
How to change the format of training data for custom NER model retraining using SpaCy?,"<p>I am working on this problem where the text data is in the a document file and the resulting 5 tags are in a csv file. So to train <code>spaCy NER</code> model, we have to tag dtaa something like :</p>
<pre><code>TRAIN_DATA = [
    (&quot;Who is Shaka Khan?&quot;, {&quot;entities&quot;: [(7, 17, &quot;PERSON&quot;)]}),
    (&quot;I like London and Berlin.&quot;, {&quot;entities&quot;: [(7, 13, &quot;LOC&quot;), (18, 24, &quot;LOC&quot;)]}),
]
</code></pre>
<p>But my data is in csv file like:</p>
<p><a href=""https://i.sstatic.net/B8wof.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/B8wof.png"" alt=""enter image description here"" /></a></p>
<p>I wrote a function which will search the first occurrences of <code>col query</code> in the text and add the length. Something like:</p>
<pre><code>train_data = []
for i,index in enumerate(df.index.tolist()):
    row_data = df.iloc[i,:].values.tolist()
    entities = {&quot;entities&quot;:[]}
    for file in dir_files:
        if file.split('.')[0] == row_data[0]:
            text = preprocess(textract.process(&quot;./Training_data/&quot;+file))
            
            for j,entry in enumerate(row_data[1:]):
                
                if not pd.isna(entry):
                    if isinstance(entry,str): # takes care of null values
                        entities['entities'].append((text.find(str(entry).strip()),len(str(entry)),ent_names[j]))
</code></pre>
<p>and the result is</p>
<pre><code>{'entities': [(-1, 7, 'Aggrement Value'),
  (-1, 10, 'Aggrement Start Date'),
  (-1, 10, 'Aggrement End Date'),
  (-1, 4, 'Renewal Notice (Days)'),
  (124, 22, 'Party One'),
  (540, 45, 'Party Two')]}
</code></pre>
<p>It is giving me decent results for the <code>STRING</code> but I have a huge problem for date as the are in format <code>12.08.2018</code> and price which is format <code>6000.00</code>. I can't compare directly so I have to change the price <code>str(int(price))</code> and then match. It'll work BUT the date is never in the format given in CSV. It's spmething like <code>1stDAY OF SEPTEMBER 2018 TWO THOUSAND EIGHTEEN</code>. How am I supposed to tag that one in format?</p>
<p>I tried using <code>Spacy's</code> inbuilt NER so that I could figure out but it is not giving me good results.</p>
<pre><code>nlp = spacy.load('en_core_web_sm')
doc = nlp(preprocess(text))
displacy.render(nlp(doc.text),style='ent',jupyter=True)
</code></pre>
<p>It gives me something like:</p>
<p><a href=""https://i.sstatic.net/TlJo3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TlJo3.png"" alt=""enter image description here"" /></a></p>
<p>How can I tag my data because without proper tagging of dates, it's all futile as it'll never learn to get the dates no matter what.
Is there any <code>Regular expression RE</code> or I saw that <a href=""https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da"" rel=""nofollow noreferrer"">NLTK POS based Queries to extract NER</a>  gives us something like:</p>
<p><a href=""https://i.sstatic.net/Uk5Ja.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Uk5Ja.png"" alt=""enter image description here"" /></a></p>
",Named Entity Recognition (NER),change format training data custom ner model retraining using spacy working problem text data document file resulting tag csv file train model tag dtaa something like data csv file like wrote function search first occurrence text add length something like result giving decent result huge problem date format price format compare directly change price match work date never format given csv spmething like supposed tag one format tried using inbuilt ner could figure giving good result give something like tag data without proper tagging date futile never learn get date matter saw nltk po based query extract ner give u something like
How to recognize names from a text using php,"<p>I want to extract name(firstnames and lastnames) from a text using php.
Example:
From text below I want to extract names(in this case Aline Wright and Jesse Wright)</p>

<blockquote>
  <p>Aline Wright is a cancer survivor,
  amputee and a newlywed.  Wednesday
  night she began to show signs she was
  having a stroke.</p>
  
  <p>""I started feeling some left arm
  numbness and a facial droop,"" said
  Aline.</p>
  
  <p>""It appeared to me that I was probably
  having a stroke.""</p>
  
  <p>That's when her husband of four days,
  Jesse Wright, put her in the car and
  rushed her to the Erlanger Medical
  Center. Wright knows an emergency. He
  is a nurse technician at Erlanger.</p>
</blockquote>
",Named Entity Recognition (NER),recognize name text using php want extract name firstnames lastnames text using php example text want extract name case aline wright jesse wright aline wright cancer survivor amputee newlywed wednesday night began show sign wa stroke started feeling left arm numbness facial droop said aline appeared wa probably stroke husband four day jesse wright put car rushed erlanger medical center wright know emergency nurse technician erlanger
Extract Named Entities using SpaCy and python lambda,"<p>I am using <a href=""https://stackoverflow.com/questions/65406519/how-to-select-only-first-entity-extracted-from-spacy-entities"">following</a> code to extract Named Entities using lambda.</p>
<pre><code>df['Place'] = df['Text'].apply(lambda x: [entity.text for entity in nlp(x).ents if entity.label_ == 'GPE'])
</code></pre>
<p>and</p>
<pre><code>df['Text'].apply(lambda x: ([entity.text for entity in nlp(x).ents if entity.label_ == 'GPE'] or [''])[0])
</code></pre>
<p>For a few hundred records it can extract results. But when it comes to thousands of records. It takes pretty much forever. Can someone help me to optimize this line of code?</p>
",Named Entity Recognition (NER),extract named entity using spacy python lambda using href code extract named entity using lambda p hundred record extract result come thousand record take pretty much forever someone help optimize line code
How Spacy NER verifies the rationality of entities?,"<p>When I use SpaCy NER, SpaCy will recognize 'TodoA' as PERSON. This is obviously unreasonable. Is there any way to verify whether the entity extracted by SpaCy is reasonable? Thanks!</p>
<p>Most of these unreasonable entities are extracted by spacy beam search. The beam search code is:</p>
<pre><code>import spacy
import sys
from collections import defaultdict

nlp = spacy.load('en')
text = u'Will Japan join the European Union? If yes, we should \ 
move to United States. Fasten your belts, America we are coming'


with nlp.disable_pipes('ner'):
    doc = nlp(text)

threshold = 0.2
(beams, somethingelse) = nlp.entity.beam_parse([ doc ], beam_width = 16, beam_density = 0.0001)

entity_scores = defaultdict(float)
for beam in beams:
    for score, ents in nlp.entity.moves.get_beam_parses(beam):
        for start, end, label in ents:
            entity_scores[(start, end, label)] += score

print ('Entities and scores (detected with beam search)')
for key in entity_scores:
    start, end, label = key
    score = entity_scores[key]
    if ( score &gt; threshold):
        print ('Label: {}, Text: {}, Score: {}'.format(label, doc[start:end], score))
</code></pre>
",Named Entity Recognition (NER),spacy ner verifies rationality entity use spacy ner spacy recognize todoa person obviously unreasonable way verify whether entity extracted spacy reasonable thanks unreasonable entity extracted spacy beam search beam search code
"pre-trained spacy model or spacy.blank,for custom NER which is the right way?","<p>I want train a spacy custom NER model,which is the best option?</p>
<p>the train data is ready (doccano)</p>
<p>option 1. use an existing pre-trained spacy model and update it with custom NER?.</p>
<p>option 2. create an empty model using spacy.blank() with custom NER?</p>
<p>I just want to identify my custom entity in a text, the other types of entities are not necessary...currently</p>
",Named Entity Recognition (NER),pre trained spacy model spacy blank custom ner right way want train spacy custom ner model best option train data ready doccano option use existing pre trained spacy model update custom ner option create empty model using spacy blank custom ner want identify custom entity text type entity necessary currently
Combining Spacy PoS and NER,"<p>I'm using Spacy for NLP and can't solve a Problem by myself.</p>

<p>This is my function, it recieves a list of lists with tokens:</p>

<pre><code>def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB']):
    """"""https://spacy.io/api/annotation""""""
    texts_out = []
    for sent in tqdm(texts):
        mytokens = nlp("" "".join(sent))
        # mytokens = [ent.label_.lower() if ent.label_ == ""DATE"" or ent.label_ == ""TIME"" for ent in mytokens]
        mytokens = [word.lemma_ if word.lemma_ != ""-PRON-"" else word.lower_ for word in mytokens if word.pos_ in allowed_postags]
        mytokens = [word for word in mytokens if word not in stopwords and word not in punctuations]
        mytokens = "" "".join([i for i in mytokens])
        texts_out.append(mytokens)
    return texts_out
</code></pre>

<p>The out-commented line should point to something I want to add.
The idea was to check if a token belongs to the entity ""DATE"" or ""TIME"". If yes the entity should be returned as lower case if no either the lemma or lower case of the word should be returned. (See line below)
Unfortunately, I cannot manage to implement this successfully. Could someone please help me?</p>

<p>Kind regards</p>
",Named Entity Recognition (NER),combining spacy po ner using spacy nlp solve problem function recieves list list token commented line point something want add idea wa check token belongs entity date time yes entity returned lower case either lemma lower case word returned see line unfortunately manage implement successfully could someone please help kind regard
Data preparation for NER in CONLL 2003 BIO format,"<p>To train my own NER over custom entities, I need my dataset preapared with CONLL-2003 format as specified in - <a href=""https://github.com/yongyuwen/sequence-tagging-ner"" rel=""nofollow noreferrer"">https://github.com/yongyuwen/sequence-tagging-ner</a>.</p>

<p>How would I convert my text documents (.txt) files to specified CONLL-U format - like [Word POS CHUNK NER]. </p>

<p>Note: For the given text documents, I am already having custom NER tags.</p>

<p>Sample data (training_data.txt):</p>

<pre><code>(Sample 1)
This Agreement of Work is made pursuant to the Global Developer Master Services Agreement effective as  of May 24, 2018, as amended on March 28, 2016, between MA[CUSTOM_ENTITY], lnc.[CUSTOM_ENTITY] whose registered office or principal place of  business is at 520 Madison Avenue, Ahmedabad, India, whose registered  office or principal place of business is at Building A, Atlantis de la,  Switzerland, collectively and ABC[CUSTOM_ENTITY] LLC[CUSTOM_ENTITY] a wholly owned subsidiary of  Amazon Services Ltd and having its registered office at 113 Red Avenue, 10th Floor, New York, NY 13027.

(Sample 2)
This Agreement of Work is subject to the terms and conditions of the Master Agreement for Technology  Consulting Services between Vignesh[CUSTOM_ENTITY] Services[CUSTOM_ENTITY] Limited[CUSTOM_ENTITY] and ABD[CUSTOM_ENTITY] LLC[CUSTOM_ENTITY], an  entity wholly owned by ABC[CUSTOM_ENTITY] Holdings[CUSTOM_ENTITY] LLC[CUSTOM_ENTITY].

(Sample 3)
This Agreement of Work dated October 22, 2013 between Google[CUSTOM_ENTITY] Services[CUSTOM_ENTITY] Limited[CUSTOM_ENTITY]  and Avaya[CUSTOM_ENTITY] Communications[CUSTOM_ENTITY] Management[CUSTOM_ENTITY], LLC[CUSTOM_ENTITY] and any of its operating subsidiaries and  affiliates which receive Services from Vendor incorporates and is governed by the terms and  conditions contained in the Master Services Agreement Services, by and between Avaya and Vendor.
</code></pre>

<p>Where [CUSTOM_ENTITY] is the tag for new entity to be trained with NER.</p>
",Named Entity Recognition (NER),data preparation ner conll bio format train ner custom entity need dataset preapared conll format specified would convert text document txt file specified conll u format like word po chunk ner note given text document already custom ner tag sample data training data txt custom entity tag new entity trained ner
named entity extraction python script with flair framework stuck while training,"<p>This is my code that I am trying to train in order to get a pretrained model for named entity extraction in french language . I am using google colab as an environment since my laptop has only 4GB GPU , whereas colab offers 12GB GPU . Sadly on colab , the script gets stuck in epoch 1 each time . can anyone suggest me a solution or a tip to solve my problem ? thanks a lot ! 
CODE : </p>

<pre><code>from flair.visual.training_curves import Plotter
from flair.trainers import ModelTrainer
from flair.models import SequenceTagger
from flair.data import TaggedCorpus
from flair.data_fetcher import NLPTaskDataFetcher, NLPTask
from flair.embeddings import TokenEmbeddings, WordEmbeddings, 
StackedEmbeddings
from flair.embeddings import FlairEmbeddings, BertEmbeddings
from typing import List
from flair.data import Sentence 
# import flair.datasets
# 1. get the corpus
corpus = NLPTaskDataFetcher.load_corpus(NLPTask.WIKINER_FRENCH)
# 2. what tag do we want to predict?
tag_type = 'ner'
# 3. make the tag dictionary from the corpus
tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)
print(tag_dictionary.idx2item)
# large embedding configuration - comment this in for a better model
embeddings = StackedEmbeddings(
   [WordEmbeddings('fr'),
   FlairEmbeddings('french-forward', use_cache=True),
   FlairEmbeddings('french-backward', use_cache=True)])
# 5. initialize sequence tagger

tagger: SequenceTagger = SequenceTagger(hidden_size=256,
                                    embeddings=embeddings,
                                    tag_dictionary=tag_dictionary,
                                    tag_type=tag_type,
                                    use_crf=True)
# # 6. initialize trainer

trainer: ModelTrainer = ModelTrainer(tagger, corpus)

# 7. start training
trainer.train('resources/taggers/example-ner',
          learning_rate=0.1,
          mini_batch_size=32,
          max_epochs=150)
</code></pre>
",Named Entity Recognition (NER),named entity extraction python script flair framework stuck training code trying train order get pretrained model named entity extraction french language using google colab environment since laptop ha gb gpu whereas colab offer gb gpu sadly colab script get stuck epoch time anyone suggest solution tip solve problem thanks lot code
how to extract contextual data from tables to train custom named entity recognizer?,"<p>I have document containing tables and I want to extract contextual information(ex. sentences or any other form) so I can tag them and build named entity recognizer.</p>

<p>Is anyone knows how can we build contextual training data to train named entity recognizer or how can we annotate table data to train named entity recognizer.</p>
",Named Entity Recognition (NER),extract contextual data table train custom named entity recognizer document containing table want extract contextual information ex sentence form tag build named entity recognizer anyone know build contextual training data train named entity recognizer annotate table data train named entity recognizer
Detecting arbitrary amount of intents using Dialogflow,"<p>In Dialogflow (ES version), we have an intent for detecting product names and optional quantities. E.g. <code>Do you have Pepsi</code> or <code>I need 4 apples</code>. We also have some training examples that contain more than one product. E.g: <code>I need 2 brush and 3 chocolates</code>.</p>
<p>The general idea is to make the entity extraction generic so that we can query with <code>n</code> number of products. For example, <code>1 Pepsi, 2 eggs, 5 ice cream, and 4 tomatoes</code> will extract the 4 different products correctly.</p>
<p>But we did some manual testing and found that the entity extraction does not generally extend to an arbitrary number of entities. Is that a limitation of Dialogflow or do we need to tune our training data to include more examples with 4/5+ products?</p>
<p>I am looking for suggestions on handling this type of query with an arbitrary amount of entities.</p>
<p><strong>Dialogflow Setup:</strong></p>
<pre><code>ML Threshold: 0.3
</code></pre>
<p><strong>Entities:</strong></p>
<pre><code>Product: Some product names as training data. **Automated expansion** and **Fuzzy matching** enabled.
Product-count: @sys.number. All options disabled 
</code></pre>
<p><strong>Product_Query intent parameters:</strong></p>
<pre><code>+----------------+----------------+------+
| Parameter Name |     Entity     | Type |
+----------------+----------------+------+
| products       | @Products      | List |
| product-count  | @Product-count | List |
+----------------+----------------+------+
</code></pre>
",Named Entity Recognition (NER),detecting arbitrary amount intent using dialogflow dialogflow e version intent detecting product name optional quantity e g also training example contain one product e g general idea make entity extraction generic query number product example extract different product correctly manual testing found entity extraction doe generally extend arbitrary number entity limitation dialogflow need tune training data include example product looking suggestion handling type query arbitrary amount entity dialogflow setup entity product query intent parameter
BERT NER: can&#39;t convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first,"<p>I want to train my BERT NER model on colab. But following error occurs</p>
<p>Code:</p>
<pre><code>tr_logits = tr_logits.detach().cpu().numpy()
tr_label_ids = torch.masked_select(b_labels, (preds_mask == 1))
tr_batch_preds = np.argmax(tr_logits[preds_mask.squeeze()], axis=1)
tr_batch_labels = tr_label_ids.to(device).numpy()
tr_preds.extend(tr_batch_preds)
tr_labels.extend(tr_batch_labels)
</code></pre>
<p>Error:</p>
<pre><code>Using TensorFlow backend.
Saved standardized data to ./data/en/combined/train_combined.txt.
Saved standardized data to ./data/en/combined/dev_combined.txt.
Saved standardized data to ./data/en/combined/test_combined.txt.
Constructed SentenceGetter with 25650 examples.
Constructed SentenceGetter with 8934 examples.
Loaded training and validation data into DataLoaders.
Initialized model and moved it to cuda.
Initialized optimizer and set hyperparameters.
Epoch:   0% 0/5 [00:00&lt;?, ?it/s]Starting training loop.
Epoch:   0% 0/5 [00:00&lt;?, ?it/s]
Traceback (most recent call last):
  File &quot;/content/FYP_Presentation/python/main.py&quot;, line 102, in &lt;module&gt;
    valid_dataloader,
  File &quot;/content/FYP_Presentation/python/utils/main_utils.py&quot;, line 431, in train_and_save_model
    tr_batch_preds = torch.max(tr_logits[preds_mask.squeeze()], axis=1)
  File &quot;/usr/local/lib/python3.6/dist-packages/torch/tensor.py&quot;, line 412, in __array__
    return self.numpy()
TypeError: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
</code></pre>
<p>How would I solve this issue?</p>
",Named Entity Recognition (NER),bert ner convert cuda tensor numpy use tensor cpu copy tensor host memory first want train bert ner model colab following error occurs code error would solve issue
Can I train NER in spaCy using annotations from a wordpad or text document,"<p>Can I train NER in spaCy using annotations from a wordpad or text document, because training with a sentence or paragraph doesn't meet my requirements. Thanks.</p>
",Named Entity Recognition (NER),train ner spacy using annotation wordpad text document train ner spacy using annotation wordpad text document training sentence paragraph meet requirement thanks
How to generate a list of tokens that are most likely to occupy the place of a missing token in a given sentence?,"<p>How to generate a list of tokens that are most likely to occupy the place of a missing token in a given sentence?</p>
<p>I've found this <a href=""https://stackoverflow.com/questions/56822991/an-nlp-model-that-suggest-a-list-of-words-in-an-incomplete-sentence?rq=1"">StackOverflow answer</a>, however, this only generates <strong>a</strong> possible word, and <strong>not a list</strong> of words that fits the sentence. I tried printing out every variable to see if he might have generated all the possible words, but no luck.</p>
<p>For example,</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; sentence = 'Cristiano Ronaldo dos Santos Aveiro GOIH ComM is a Portuguese professional [].' # [] is missing word
&gt;&gt;&gt; generate(sentence)
['soccer', 'basketball', 'tennis', 'rugby']
</code></pre>
",Named Entity Recognition (NER),generate list token likely occupy place missing token given sentence generate list token likely occupy place missing token given sentence found
NLP - Named Entity Recognition,"<p>What algorithm does Named Entity Recognition (NER) use? I mean how does it match and tag all the entities? </p>
",Named Entity Recognition (NER),nlp named entity recognition algorithm doe named entity recognition ner use mean doe match tag entity
create space KnowledgeBase for similar nouns,"<p>The entity linking examples in spacy's documentation are all based on named entities. Is it possible create a knowledgeable such that it links certain nouns with certain nouns?</p>
<p>For example, &quot;aeroplane&quot; with &quot;plane&quot; and &quot;aeroplane&quot; in case of a typing error? Such that I can pre-define the possible alternative terms that can be used for &quot;aeroplane&quot;. Are there any concrete examples?</p>
<p>I tried this with Knowledgebase:</p>
<pre><code>vocab = nlp.vocab
kb = KnowledgeBase(vocab=vocab, entity_vector_length=64)
kb.add_entity(entity=&quot;Aeroplane&quot;, freq=32, entity_vector=vector1)
</code></pre>
<p>as described here: <a href=""https://spacy.io/api/kb"" rel=""nofollow noreferrer"">https://spacy.io/api/kb</a></p>
<p>but I don't know what to use as the <code>entity_vector</code>, which is supposed to be a pre-trained vector of the entity.</p>
<p>Another example that I saw in the docs was this:</p>
<pre><code>nlp = spacy.load('en_core_web_sm')
kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=3)

# adding entities
kb.add_entity(entity=&quot;Q1004791&quot;, freq=6, entity_vector=[0, 3, 5])
kb.add_entity(entity=&quot;Q42&quot;, freq=342, entity_vector=[1, 9, -3])
kb.add_entity(entity=&quot;Q5301561&quot;, freq=12, entity_vector=[-2, 4, 2])

# adding aliases
kb.add_alias(alias=&quot;Douglas&quot;, entities=[&quot;Q1004791&quot;, &quot;Q42&quot;, &quot;Q5301561&quot;], probabilities=[0.6, 0.1, 0.2])
kb.add_alias(alias=&quot;Douglas Adams&quot;, entities=[&quot;Q42&quot;], probabilities=[0.9])
</code></pre>
<p>Can't we use anything else than wiki ids? and how do I get these vector lengths?</p>
",Named Entity Recognition (NER),create space knowledgebase similar noun entity linking example spacy documentation based named entity possible create knowledgeable link certain noun certain noun example aeroplane plane aeroplane case typing error pre define possible alternative term used aeroplane concrete example tried knowledgebase described know use supposed pre trained vector entity another example saw doc wa use anything else wiki id get vector length
Where is the trained NER model saved after training the Spacy model with new Entities,"<p>I'm still learning Python and creation of models and am very new to NLP using Spacy. I used <a href=""https://spacy.io/usage/training#ner"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#ner</a> to train Spacy's existing model - en_core_web_sm.</p>
<p>I've trained this model with my domain specific entities.</p>
<pre><code>def main(model=&quot;en_core_web_sm&quot;, new_model_name=&quot;new_ner_model&quot;, output_dir='/content/drive/My Drive/Data/new_model', n_iter=100):
.
.
(code to train the model)
.
.
    # save model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.meta[&quot;name&quot;] = new_model_name  # rename model
        nlp.to_disk(output_dir)
        print(&quot;Saved model to&quot;, output_dir)
</code></pre>
<p>Now I assumed that I would find a single model file within the output directory. Instead, what I have are 4 subfolders - <em>vocab, ner, tagger, parser</em>. And 2 files <em>meta.json and tokenizer</em>.
The <em>ner</em> subfolder has <em>cfg, moves, model</em>.</p>
<p>According to the website mentioned above, to load the new model, I need to use the entire folder (output directory), i.e.</p>
<p><code>nlp2 = spacy.load(output_dir)</code></p>
<p>Is the whole directory needed (is that the model) or is it the binary file named <em>model</em> within the <em>ner</em> subfolder?</p>
",Named Entity Recognition (NER),trained ner model saved training spacy model new entity still learning python creation model new nlp using spacy used train spacy existing model en core web sm trained model domain specific entity assumed would find single model file within output directory instead subfolders vocab ner tagger parser file meta json tokenizer ner subfolder ha cfg move model according website mentioned load new model need use entire folder output directory e whole directory needed model binary file named model within ner subfolder
How can we extract name using spacy nlp in rasa?,"<p>I tried what is found in the document and also put the spacynlp and the dimension I want which is <em>PERSON</em> but still rasa wont extract the name for me using spacynlp ..is there another way we can use spacynlp for extracting a person name ..please I need some help on how to extract name in rasa using spacynlp</p>
",Named Entity Recognition (NER),extract name using spacy nlp rasa tried found document also put spacynlp dimension want person still rasa wont extract name using spacynlp another way use spacynlp extracting person name please need help extract name rasa using spacynlp
unable to load NER pipeline with nlp.from_disk(),"<p>I am trying to load a pre-trained pipeline into my code like this:</p>
<pre><code>nlp = de_core_news_sm.load()
nlp = nlp.from_disk('./TRAINED/Background/')
</code></pre>
<p>but I get a versos error saying:</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-4-1f41fefa6daa&gt; in &lt;module&gt;
      1 nlp = de_core_news_sm.load()
----&gt; 2 nlp = nlp.from_disk('./TRAINED/Background/')
      3 print(nlp)

/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py in from_disk(self, path, exclude, disable)
    972             # Convert to list here in case exclude is (default) tuple
    973             exclude = list(exclude) + [&quot;vocab&quot;]
--&gt; 974         util.from_disk(path, deserializers, exclude)
    975         self._path = path
    976         return self

/opt/anaconda3/lib/python3.8/site-packages/spacy/util.py in from_disk(path, readers, exclude)
    688         # Split to support file names like meta.json
    689         if key.split(&quot;.&quot;)[0] not in exclude:
--&gt; 690             reader(path / key)
    691     return path
    692 

/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py in deserialize_vocab(path)
    948         def deserialize_vocab(path):
    949             if path.exists():
--&gt; 950                 self.vocab.from_disk(path)
    951             _fix_pretrained_vectors_name(self)
    952 

vocab.pyx in spacy.vocab.Vocab.from_disk()

strings.pyx in spacy.strings.StringStore.from_disk()

/opt/anaconda3/lib/python3.8/site-packages/srsly/_json_api.py in read_json(location)
     48         data = sys.stdin.read()
     49         return ujson.loads(data)
---&gt; 50     file_path = force_path(location)
     51     with file_path.open(&quot;r&quot;, encoding=&quot;utf8&quot;) as f:
     52         return ujson.load(f)

/opt/anaconda3/lib/python3.8/site-packages/srsly/util.py in force_path(location, require_exists)
     19         location = Path(location)
     20     if require_exists and not location.exists():
---&gt; 21         raise ValueError(&quot;Can't read file: {}&quot;.format(location))
     22     return location
     23 

ValueError: Can't read file: TRAINED/Background/vocab/strings.json
</code></pre>
<p>If I open the Vocab folder on my macOS, there's no string.json file. Just a few exec files. What can I do to properly read the model?</p>
",Named Entity Recognition (NER),unable load ner pipeline nlp disk trying load pre trained pipeline code like get verso error saying open vocab folder macos string json file exec file properly read model
How can we use Spacy minibatch and GoldParse to train NER model using BILUO tagging scheme?,"<p>My input data to the spacy ner model is in the <code>BILUO</code> tagging scheme and I wish to use the same as a part of some requirement. When I try to train the model simply without a minibatch, it works fine (the commented part). But I am unable to figure out how to use minibatch and GoldParse here in order to raise the model's accuracy. Are my expectations valid here as I could not find a single example with this kind of combination? Also, I have already trained the model with the approach of start, end, label format. Please help me to figure out this section. My code is as below,</p>
<pre><code>import spacy
from spacy.gold import offsets_from_biluo_tags
from spacy.gold import biluo_tags_from_offsets
import random
from spacy.util import minibatch, compounding
from os import path
from tqdm import tqdm


def train_spacy(data, iterations, model=None):
    TRAIN_DATA = data
    print(f&quot;downloads = {model}&quot;)
    if model is not None and path.exists(model):
        print(f&quot;training existing model&quot;)
        nlp = spacy.load(model)
        print(&quot;Model is Loaded '%s'&quot; % model)
    else:
        print(f&quot;Creating new model&quot;)

        nlp = spacy.blank('en')  # create blank Language class

    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)
    else:
        ner = nlp.get_pipe('ner')

    # Based on template, get labels and save those for further training
    LABEL = [&quot;Name&quot;, &quot;ORG&quot;]

    for i in LABEL:
        # print(i)
        ner.add_label(i)

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        if model is None:
            optimizer = nlp.begin_training()
        else:
            optimizer = nlp.entity.create_optimizer()
        tags = dict()
        for itn in range(iterations):
            print(&quot;Starting iteration &quot; + str(itn))
            random.shuffle(TRAIN_DATA)
            losses = {}
            # for text, annotations in tqdm(TRAIN_DATA):
            #     print(f&quot;text={text}, an={annotations}&quot;)
            #     tags['entities'] = offsets_from_biluo_tags(nlp(text), annotations)
            #     print(f&quot;a={tags}&quot;)
            #     nlp.update([text],  # batch of texts
            #                [tags],  # batch of annotations
            #                drop=0.5,  # dropout - make it harder to memorise data
            #                sgd=optimizer,  # callable to update weights
            #                losses=losses)
            # print(losses)
            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 16.0, 1.001))
            # type 2 with mini batch
            for batch in batches:
                texts, annotations = zip(*batch)
                print(texts)
                tags = {'entities': annotations}
                nlp.update(
                    texts,  # batch of texts
                    [tags],  # batch of annotations
                    drop=0.4,  # dropout - make it harder to memorise data
                    losses=losses,
                    sgd=optimizer
                )
            print(losses)
    return nlp

data_biluo = [
    ('I am Shah Khan, I work in MS Co', ['O', 'O', 'B-Name', 'L-Name', 'O', 'O', 'O', 'B-ORG', 'L-ORG']),
    ('I am Tom Tomb, I work in Telecom Networks', ['O', 'O', 'B-Name', 'L-Name', 'O', 'O', 'O', 'B-ORG', 'L-ORG'])
]


model = train_spacy(data_biluo, 10)
model.to_disk('./Vectors/')
</code></pre>
",Named Entity Recognition (NER),use spacy minibatch goldparse train ner model using biluo tagging scheme input data spacy ner model tagging scheme wish use part requirement try train model simply without minibatch work fine commented part unable figure use minibatch goldparse order raise model accuracy expectation valid could find single example kind combination also already trained model approach start end label format please help figure section code
Spacy - What does blank models contain?,"<p>Hy everyone,</p>
<p>I am trying to create a new NER model which labels custom entities.</p>
<p>As part of that I am using a new blank model for Italian with spacy.blank('it')</p>
<p>I'm creating a new NER pipeline with the labeled data to do the training, but I would like to  know what happens in the background.</p>
<p>If a blank model is used, how does the word embedding happen? I have read some where the core models use GloVe embedding but what happens when training a blank model?</p>
<p>In addition, if word embedding is happening through GloVe, can we use something like BERT to do that?</p>
<p>I'm just new to all this and finding hard to wrap my head around all that spacy offers.</p>
",Named Entity Recognition (NER),spacy doe blank model contain hy everyone trying create new ner model label custom entity part using new blank model italian spacy blank creating new ner pipeline labeled data training would like know happens background blank model used doe word embedding happen read core model use glove embedding happens training blank model addition word embedding happening glove use something like bert new finding hard wrap head around spacy offer
Transformers get named entity prediction for words instead of tokens,"<p>This is very basic question, but I spend hours struggling to find the answer. I built NER using Hugginface transformers.</p>
<p>Say I have input sentence</p>
<pre class=""lang-py prettyprint-override""><code>input = &quot;Damien Hirst oil in canvas&quot;
</code></pre>
<p>I tokenize it to get</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')
tokenized = tokenizer.encode(input) #[101, 12587, 7632, 12096, 3514, 1999, 10683, 102]
</code></pre>
<p>Feed tokenized sentence to the model to get predicted tags for the tokens</p>
<pre class=""lang-py prettyprint-override""><code>['B-ARTIST' 'B-ARTIST' 'I-ARTIST' 'I-ARTIST' 'B-MEDIUM' 'I-MEDIUM'
 'I-MEDIUM' 'B-ARTIST']
</code></pre>
<p><code>prediction</code> comes as output from the model. It assigns tags to different tokens.</p>
<p>How can I recombine this data to obtain tags for words instead of tokens? So I would know that</p>
<pre><code>&quot;Damien Hirst&quot; = ARTIST
&quot;Oil in canvas&quot; = MEDIUM
</code></pre>
",Named Entity Recognition (NER),transformer get named entity prediction word instead token basic question spend hour struggling find answer built ner using hugginface transformer say input sentence tokenize get feed tokenized sentence model get predicted tag token come output model assigns tag different token recombine data obtain tag word instead token would know
Train BERT with CLI commands,"<p>I have downloaded the HuggingFace BERT model from the transformer repository found <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">here</a> and would like to train the model on custom NER labels by using the run_ner.py script as it is referenced <a href=""https://huggingface.co/transformers/task_summary.html"" rel=""nofollow noreferrer"">here</a> in the section &quot;Named Entity Recognition&quot;.</p>
<p>I define model (&quot;bert-base-german-cased&quot;), data_dir (&quot;Data/sentence_data.txt&quot;) and labels (&quot;Data/labels.txt)&quot; as defaults in the code.</p>
<p>Now I'm using this input for the command line:</p>
<pre><code>python run_ner.py --output_dir=&quot;Models&quot; --num_train_epochs=3 --logging_steps=100 --do_train --do_eval --do_predict
</code></pre>
<p>But all it does is telling me:</p>
<pre><code>Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.w
eight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
<p>After that it just stops, not ending the script, but simply waiting.</p>
<p>Does anyone know what could be the problem here? Am I missing a parameter?</p>
<p>My sentence_data.txt in CoNLL format looks like this (small snippet):</p>
<pre><code>Strafverfahren O
gegen O
; O
wegen O
Diebstahls O
hat O
das O
Amtsgericht Ort
Leipzig Ort
- O
Strafrichter O
</code></pre>
<p>And that's how I defined my labels in labels.txt:</p>
<pre><code>&quot;Date&quot;, &quot;Delikt&quot;, &quot;Strafe_Tatbestand&quot;, &quot;Schadensbetrag&quot;, &quot;Geständnis_ja&quot;, &quot;Vorstrafe_ja&quot;, &quot;Vorstrafe_nein&quot;, &quot;Ort&quot;,
&quot;Strafe_Gesamtfreiheitsstrafe_Dauer&quot;, &quot;Strafe_Gesamtsatz_Dauer&quot;, &quot;Strafe_Gesamtsatz_Betrag&quot;
</code></pre>
",Named Entity Recognition (NER),train bert cli command downloaded huggingface bert model transformer repository found would like train model custom ner label using run ner py script referenced section named entity recognition define model bert base german cased data dir data sentence data txt label data label txt default code using input command line doe telling stop ending script simply waiting doe anyone know could problem missing parameter sentence data txt conll format look like small snippet defined label label txt
extending NLP entity extraction,"<p>We would like to identify from a simple search neighborhood and streets in various cities. We don't only use English but also various other Cyrillic languages. We need to be able to identify spelling mistakes of locations. When looking at python libraries, I found this one: 
<a href=""http://polyglot.readthedocs.io/en/latest/NamedEntityRecognition.html"" rel=""noreferrer"">http://polyglot.readthedocs.io/en/latest/NamedEntityRecognition.html</a></p>

<p>We tried to play around with it, but cannot find a way to extend the entity recognition database. How can that be done?<br>
If not is there any other suggestion for a multi lingual nlp that can help spell check and also extract various entities matching a custom database? </p>
",Named Entity Recognition (NER),extending nlp entity extraction would like identify simple search neighborhood street various city use english also various cyrillic language need able identify spelling mistake location looking python library found one tried play around find way extend entity recognition database done suggestion multi lingual nlp help spell check also extract various entity matching custom database
Entity Attribute Extraction On Unstructured Medical Text,"<p>I am working on Named entities and their attribute extraction. Where my objective is to extract attributes associated with a particular entity in the sentence.</p>
<p>For example - &quot;The Patient report is Positive for ABC disease&quot;</p>
<p>In above sentence, ABC is a Entity and Positive is a Attribute defining ABC.</p>
<p>I am looking for an concise approach to extract the attributes, I already formulated a solution to extract entities which is working seamlessly with respectable accuracy and now working on second part of the problem statement to extract its associated attributes.</p>
<p>I tried extracting attributes with rule based approach which providing descent result but having following cons:</p>
<ul>
<li>Source code is unmanageable.</li>
<li>Its not at all generic and difficult to manage new scenarios.</li>
<li>Time consuming.</li>
</ul>
<p>To portray a more generic solution I explored different NLP techniques and found Dependency Tree Parsing as a potential solution.</p>
<p>Looking for suggestion/inputs on how to solve this problem using dependency tree parsing using Python/Java.</p>
<p>Feel free to suggest any other technique which could potentially help here.</p>
",Named Entity Recognition (NER),entity attribute extraction unstructured medical text working named entity attribute extraction objective extract attribute associated particular entity sentence example patient report positive abc disease sentence abc entity positive attribute defining abc looking concise approach extract attribute already formulated solution extract entity working seamlessly respectable accuracy working second part problem statement extract associated attribute tried extracting attribute rule based approach providing descent result following con source code unmanageable generic difficult manage new scenario time consuming portray generic solution explored different nlp technique found dependency tree parsing potential solution looking suggestion input solve problem using dependency tree parsing using python java feel free suggest technique could potentially help
Finding custom entities using NLP,"<p>Basically, from a paragraph, I have to find two entities <code>Role</code> and <code>Oragnization</code>.</p>
<ul>
<li>Org should be captured along with their branch location, not their full address if provided in paragraph</li>
<li>Role can be in bracket and right before bracket at same time like
<code>Org as role(The &quot;Role&quot;)</code></li>
<li>Role can have multiple words</li>
</ul>
<p>Example paragraph would be:</p>
<blockquote>
<p>XXXX, dated as of November 10, 2050, among: (i) <code>&lt;ORG_NAME_1 PLC&gt;</code>,
a public company incorporated under the laws and having
its registered office at the 123 Penss Aven, NY, USA (the “<code>&lt;ROLE1&gt;</code>”), (ii) <code>&lt;ORG_NAME_2 PLC&gt;</code>, a public limited company incorporated
under the laws having its registered office at Manhattan Blvd, Seattle, WA, as <code>&lt;ROLE2&gt;</code> (the
“<code>&lt;ROLE2&gt;</code>”), (iii) the , Guarantors named in some random text hereto, (iv)
<code>&lt;ORG_NAME_3&gt;, N.A., Belfast Branch</code>, as <code>&lt;ROLE3&gt;</code> (the “<code>&lt;ROLE3&gt;</code>”), (v) <code>&lt;ORG_NAME_4&gt;</code> , as <code>&lt;ROLE4&gt;</code>, <code>&lt;ROLE5&gt;</code> and <code>&lt;ROLE6&gt;</code>, and (vi) <code>&lt;ORG_NAME_5&gt;</code> <code>Deutschland AG</code>, as <code>&lt;ROLE7&gt;</code>.</p>
</blockquote>
<p>After processing, desired result would be linking role with organization.</p>
<pre><code>&lt;ROLE1&gt; --&gt; &lt;ORG_NAME_1 PLC&gt;
&lt;ROLE2&gt; --&gt; &lt;ORG_NAME_2 PLC&gt;
&lt;ROLE3&gt; --&gt; &lt;ORG_NAME_3&gt;, N.A., Belfast Branch
&lt;ROLE4&gt;, &lt;ROLE5&gt; and &lt;ROLE6&gt; --&gt; &lt;ORG_NAME_4&gt; 
&lt;ROLE7&gt; --&gt; &lt;ORG_NAME_5&gt; Deutschland AG
</code></pre>
<p>Another example would be</p>
<blockquote>
<p>XXXX dated as of November 28, 2027 among <code>&lt;ORG_NAME_1&gt; A/S</code>, a company incorporated
under the laws of Australia (the “<code>&lt;ROLE1&gt;</code>”), the Guarantors (as defined herein), <code>&lt;ORG_NAME_2&gt;, N.A., Montreal</code>
<code>Branch</code>, as <code>&lt;ROLE2&gt;</code>, <code>&lt;ROLE3&gt;</code>, <code>&lt;ROLE4&gt;</code>, <code>&lt;ROLE5&gt;</code>, <code>&lt;ROLE6&gt;</code> and <code>&lt;ROLE7&gt;</code></p>
</blockquote>
<p>After processing, desired result should be:</p>
<pre><code>&lt;ROLE1&gt; --&gt; &lt;ORG_NAME_1&gt; A/S
&lt;ROLE2&gt;, &lt;ROLE3&gt;, &lt;ROLE4&gt;, &lt;ROLE5&gt;, &lt;ROLE6&gt; and &lt;ROLE7&gt; --&gt; &lt;ORG_NAME_2&gt;, N.A., Montreal Branch
</code></pre>
<p>I tried to use PoS, NER but not desired results.</p>
<ul>
<li>Played with stanford NLP for NER, but organizations are not detected properly, tried to train my own data but accuracy is not accepted enough. It does not detect all organizations properly and is tagged as <code>OTHER</code> rather. Did not tweak with actual CRF model.</li>
<li>Played with NLTK python and tried to make some rule around NNP(proper noun) but sometimes roles are detected as verb, sometimes noun and it depends on case also sometimes, so not sure if it is desired approach.</li>
</ul>
<p>There are not much varieties in paragraph patterns, I can post 1 or 2 more examples of different patterns if needed. Roles are fixed around 40 and organization would be dynamic.</p>
<p>Please suggest if I should read out some specific papers or models. Thanks.</p>
",Named Entity Recognition (NER),finding custom entity using nlp basically paragraph find two entity org captured along branch location full address provided paragraph role bracket right bracket time like role multiple word example paragraph would xxxx dated november among public company incorporated law registered office pen aven ny usa ii public limited company incorporated law registered office manhattan blvd seattle wa iii guarantor named random text hereto iv v vi processing desired result would linking role organization another example would xxxx dated november among company incorporated law australia guarantor defined herein processing desired result tried use po ner desired result played stanford nlp ner organization detected properly tried train data accuracy accepted enough doe detect organization properly tagged rather tweak actual crf model played nltk python tried make rule around nnp proper noun sometimes role detected verb sometimes noun depends case also sometimes sure desired approach much variety paragraph pattern post example different pattern needed role fixed around organization would dynamic please suggest read specific paper model thanks
pooled output vs sequence output for NER with BERT,"<p>It makes sense that pooled output is used for sentence level analysis (e.g classification). I am wondering, is the token level embedding better for named entity recongition? I would've thought so, but am unsure how to take the sequence output and manipulate for NER. Do I just take a slice <code>[:, :, -1]</code> for <code>[batch_size, num_tokens, 768]</code>?</p>
",Named Entity Recognition (NER),pooled output v sequence output ner bert make sense pooled output used sentence level analysis e g classification wondering token level embedding better named entity recongition would thought unsure take sequence output manipulate ner take slice
Entity extraction using POS and NER in spacy,"<p>I need to extract entities from sentences using NER and POS tags. For example,</p>
<p>Given the sentence below:</p>
<pre><code>docx = nlp(&quot;The two blue cars belong to the tall Lorry Jim.&quot;)
</code></pre>
<p>where the entities are (two blue cars, tall Lorry Jim). Running spacy NER on the sentence,</p>
<pre><code>for ent in docx.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)
</code></pre>
<p>It returns:</p>
<pre><code>two 4 7 CARDINAL
Lorry Jim 37 46 PERSON
</code></pre>
<p>My goal is to append adjectives/number in front of the entities identified by NER together, in the case above, <code>tall</code> is ADJ and should be appended to the <code>Lorry Jim</code> entity. And <code>two blue cars</code> should be extracted using <code>NUM ADJ NOUN</code> from POS tagger.</p>
",Named Entity Recognition (NER),entity extraction using po ner spacy need extract entity sentence using ner po tag example given sentence entity two blue car tall lorry jim running spacy ner sentence return goal append adjective number front entity identified ner together case adj appended entity extracted using po tagger
InvalidArgumentError: 2 root error(s) found,"<p>I am trying to train my model for a NER task with my custom dataset. My model structure is as follows:</p>
<pre><code>input_text = Input(shape=(max_len,), dtype=tf.string)
embedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(input_text)
x = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(embedding)
x_rnn = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(x)
x = add([x, x_rnn])
out = TimeDistributed(Dense(n_tags, activation=&quot;softmax&quot;))(x)

model = Model(input_text, out)
model.summary()
model.compile(optimizer=&quot;adam&quot;, loss=&quot;categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;])
</code></pre>
<p>The <code>ElmoEmbedding</code> function is defined as:</p>
<pre><code>def ElmoEmbedding(x):
return elmo_model(inputs={
                        &quot;tokens&quot;: tf.squeeze(tf.cast(x, tf.string)),
                        &quot;sequence_len&quot;: tf.constant(batch_size*[max_len])
                  },
                  signature=&quot;tokens&quot;,
                  as_dict=True)[&quot;elmo&quot;]
</code></pre>
<p>Now, when I am trying to fit the model, I am getting the following error:</p>
<pre><code>---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-184-1bcacedbc709&gt; in &lt;module&gt;
      1 history = model.fit(np.array(X_tr), y_tr_b, validation_data=(np.array(X_te), y_te_b),
----&gt; 2                     batch_size=batch_size, epochs=3, verbose=1)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    807         max_queue_size=max_queue_size,
    808         workers=workers,
--&gt; 809         use_multiprocessing=use_multiprocessing)
    810 
    811   def evaluate(self,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    664         validation_steps=validation_steps,
    665         validation_freq=validation_freq,
--&gt; 666         steps_name='steps_per_epoch')
    667 
    668   def evaluate(self,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    384 
    385         # Get outputs.
--&gt; 386         batch_outs = f(ins_batch)
    387         if not isinstance(batch_outs, list):
    388           batch_outs = [batch_outs]

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)
   3823 
   3824     fetched = self._callable_fn(*array_vals,
-&gt; 3825                                 run_metadata=self.run_metadata)
   3826     self._call_fetch_callbacks(fetched[-len(self._fetches):])
   3827     output_structure = nest.pack_sequence_as(

/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)
   1470         ret = tf_session.TF_SessionRunCallable(self._session._session,
   1471                                                self._handle, args,
-&gt; 1472                                                run_metadata_ptr)
   1473         if run_metadata:
   1474           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: Length of seq_lengths != input.dims(0), (32 vs. 25)
     [[{{node lambda_11/module_6_apply_tokens/bilm/ReverseSequence}}]]
     [[loss_12/mul/_1051]]
  (1) Invalid argument: Length of seq_lengths != input.dims(0), (32 vs. 25)
     [[{{node lambda_11/module_6_apply_tokens/bilm/ReverseSequence}}]]
0 successful operations.
0 derived errors ignored.
</code></pre>
<p>I doubt I might have a problem with importing the modules and configuring. So I am also adding following codes:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt</p>
<pre><code>from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split

from keras.models import Model, Input
from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda
from keras.layers.merge import add
from keras_contrib.layers import CRF

import tensorflow.compat.v1 as tf
import tensorflow_hub as hub
from keras import backend as K
tf.disable_v2_behavior()

sess = tf.Session()
tf.keras.backend.set_session(sess)
</code></pre>
",Named Entity Recognition (NER),invalidargumenterror root error found trying train model ner task custom dataset model structure follows function defined trying fit model getting following error doubt might problem importing module configuring also adding following code import numpy np import panda pd import matplotlib pyplot plt
NER model to recognize Indian names,"<p>I am planning to use Named Entity Recognition (NER) technique to identify person names (most of which are Indian names) from a given text. I have already explored the CRF-based NER model from Stanford NLP, however it is not quite accurate in recognizing Indian names. Hence I decided to create my own custom NER model via supervised training. I have a fair idea of how to create own NER model using the Stanford NER CRF, but creating a large training corpus with manual annotation is something I would like to avoid, as it is a humongous effort for an individual and secondly obtaining diverse people names from different states of India is also a challenge. Could anybody suggest any automation/programmatic way to prepare a labelled training corpus with at least 100k Indian names?<br>
I have already looked into Facebook and LinkedIn API, but did not find a way to extract 100k number of user's full name from a given location (e.g. India).</p>
",Named Entity Recognition (NER),ner model recognize indian name planning use named entity recognition ner technique identify person name indian name given text already explored crf based ner model stanford nlp however quite accurate recognizing indian name hence decided create custom ner model via supervised training fair idea create ner model using stanford ner crf creating large training corpus manual annotation something would like avoid humongous effort individual secondly obtaining diverse people name different state india also challenge could anybody suggest automation programmatic way prepare labelled training corpus least k indian name already looked facebook linkedin api find way extract k number user full name given location e g india
How to find if the topic is mentioned in the sentence ? - nlp,"<p>I am pretty new to NLP, and I am looking for the most appropriate solution for my problem.</p>
<p>In simplification, I want to create a &quot;tag list&quot; from the title.</p>
<p>Tags are predefined, and I can easily label examples for training.</p>
<p><strong>Simple examples:</strong></p>
<blockquote>
<p>Format &quot;exemplary sentence&quot; - &quot;exemplary tag list&quot;</p>
</blockquote>
<ul>
<li>&quot;The biggest elephant in the world&quot; - [Animals]</li>
<li>&quot;I like mangos and gorillas&quot; - [Animals, Fruits]</li>
<li>&quot;I have 3 cats and 4 dogs&quot; - [Animals]</li>
<li>&quot;I have diabetes&quot; - [Diseases]</li>
<li>&quot;I don't have diabetes but I have a cat&quot; - [Animals]</li>
</ul>
<p>I don't need the specific value of the tag</p>
<p>e.g. <code>tags = { Animal: Elephant }</code> is as useful as <code>tags = [Animals]</code></p>
<p>I could find the only solutions that extract the entity. I only came up with a building list of matcher and then trying them all, is there any clever and performant way to do it?</p>
<p>Thanks for any suggestions, tips, and resources, Have a nice day :)</p>
",Named Entity Recognition (NER),find topic mentioned sentence nlp pretty new nlp looking appropriate solution problem simplification want create tag list title tag predefined easily label example training simple example format exemplary sentence exemplary tag list biggest elephant world animal like mango gorilla animal fruit cat dog animal diabetes disease diabetes cat animal need specific value tag e g useful could find solution extract entity came building list matcher trying clever performant way thanks suggestion tip resource nice day
How to extract names from the resume in python,"<p>I am trying to extract the person name for Resume. I am not getting the correct output. What i have done till now is.</p>
<pre><code>import en_core_web_sm
import spacy
import pdfplumber
nlp = en_core_web_sm.load()

nlp = spacy.load(&quot;en_core_web_sm&quot;)
pdf = pdfplumber.open('C:/Person.pdf')
page = pdf.pages[0]
doc = nlp(page.extract_text())
print([(X.text, X.label_) for X in doc.ents if X.label_ == 'PERSON'])
</code></pre>
<p>and my output is :</p>
<pre><code>[('Mohamme mohammed24@yahoo.com\n', 'PERSON'), ('Mangalore', 'PERSON'), ('Demo Design1', 'PERSON'), ('Demo Design2', 'PERSON'), ('Demo Design3', 'PERSON'), ('Java', 'PERSON')]
</code></pre>
<p>I tried many thing but not able to get the only names. it includes many things like skills, email etc.</p>
<p>How can i extract all the details from resume example skills, phone no, name, years of experience, email.</p>
",Named Entity Recognition (NER),extract name resume python trying extract person name resume getting correct output done till output tried many thing able get name includes many thing like skill email etc extract detail resume example skill phone name year experience email
How to decide between NER and QA Model?,"<p>I am completing a task involving NLP and transformers. I would like to identify relevant features in a corpus of text. If i was to extract the relevant features from job description for instance the tools that would be used at the job (powerpoint, excel, java, etc..) and the level of proficiency required would this task be better suited for a Named Entity Recognition model or a Question Answering model.</p>
<p>If I was to approach it like a NER task I would attach a label to all the relevant tools in the training data and hope it would generalize well. I could approach the problem simialrly as a QA model and ask things like &quot;what tools does this job require&quot; and supply a description as context.</p>
<p>I plan to use the transformers library unless I am missing a better tool for this task. There are many features I am looking to extract so not all may be as simple as grabbing keywords from a list (programming languages, microsoft office etc...).</p>
<p>Would one of these approaches be a better fit or am I missing a better way to approach the proble.</p>
<p>Any help appreciated. Thank you!</p>
",Named Entity Recognition (NER),decide ner qa model completing task involving nlp transformer would like identify relevant feature corpus text wa extract relevant feature job description instance tool would used job powerpoint excel java etc level proficiency required would task better suited named entity recognition model question answering model wa approach like ner task would attach label relevant tool training data hope would generalize well could approach problem simialrly qa model ask thing like tool doe job require supply description context plan use transformer library unless missing better tool task many feature looking extract may simple grabbing keywords list programming language microsoft office etc would one approach better fit missing better way approach proble help appreciated thank
Train Spacy NER on Indian Names,"<p>I am trying to customize Spacy's NER to identify Indian names.
Following this guide <a href=""https://spacy.io/usage/training"" rel=""noreferrer"">https://spacy.io/usage/training</a> and this is the dataset I am using <a href=""https://gist.githubusercontent.com/mbejda/9b93c7545c9dd93060bd/raw/b582593330765df3ccaae6f641f8cddc16f1e879/Indian-Female-Names.csv"" rel=""noreferrer"">https://gist.githubusercontent.com/mbejda/9b93c7545c9dd93060bd/raw/b582593330765df3ccaae6f641f8cddc16f1e879/Indian-Female-Names.csv</a></p>

<p>As per the code , I am supposed to provide training data in following format:</p>

<pre><code>TRAIN_DATA = [
    ('Shivani', {
        'entities': [(0, 6, 'PERSON')]
    }),
    ('Isha ', {
        'entities': [(0,3 , 'PERSON')]
    })
]
</code></pre>

<p>How do I provide training data to Spacy for ~12000 names as manually specifying each entity will be a chore? Is there any other tool available to tag all the names ?</p>
",Named Entity Recognition (NER),train spacy ner indian name trying customize spacy ner identify indian name following guide dataset using per code supposed provide training data following format provide training data spacy name manually specifying entity chore tool available tag name
How to convert combined spacy ner tags to BIO format?,"<p>How can I convert this into BIO format? I have tried using spacy <code>biluo_tags_from_offsets</code> but it's failing to catch all entities and I think I know the reason why.</p>
<pre><code>tags = biluo_tags_from_offsets(doc, annot['entities'])
</code></pre>
<p>BSc(Bachelor of science) - These two are combined together but spacy split the text when there is a space. So now the words will be like ( <code>BSc(Bachelor, of, science</code> ) and this is why spacy <code>biluo_tags_from_offsets</code> failing  and return <code>-</code></p>
<p>Now, when it checks for <code>(80, 83, 'Degree')</code> It can't find BSc word alone. Similarly it will again fail for <code>(84, 103, 'Degree')</code>.</p>
<p>How can I fix these scenarios? Please help if anyone can.</p>
<hr />
<pre><code>EDUCATION: · Master of Computer Applications (MCA) from NV, *********, *****. · BSc(Bachelor of science) from NV, *********, *****

{'entities': [(13, 44, 'Degree'), (46, 49, 'Degree'), (80, 83, 'Degree'), (84, 103, 'Degree')]}
</code></pre>
",Named Entity Recognition (NER),convert combined spacy ner tag bio format convert bio format tried using spacy failing catch entity think know reason bsc bachelor science two combined together spacy split text space word like spacy failing return check find bsc word alone similarly fail fix scenario please help anyone
How to get last name based on condition in Pandas,"<p>I have a list of authors' name with irregular format as below:</p>
<pre><code>df = pd.DataFrame({'author':['fox district judge', 'louise w flanagan united states district judge', 'amy berman jackson united states district judge', 'rhesa hawkins barksdale, circuit judge','kanne, circuit judge']) 
</code></pre>
<p>When parsing out last name, I used <code>df['Last_Name'] = df['author'].apply(lambda x: x.split(',')[0].split(' ')[-1])</code> However, this line of code only worked on the last two author name. How do I extract last name such as <code>fox</code> and <code>flanagan</code> from the first two rows?</p>
",Named Entity Recognition (NER),get last name based condition panda list author name irregular format parsing last name used however line code worked last two author name extract last name first two row
How can I use Google Natural Language API to enrich data in a Bigquery table?,"<p>I want to use data stored in a BigQuery table as input to <a href=""https://cloud.google.com/natural-language"" rel=""nofollow noreferrer"">Google's Natural Language API</a>, perform entity extraction and sentiment analysis, and persist the result back to BigQuery. What tools/services could I use to handle this in GCP? Performance is not a concern, and running this in an overnight batch would be acceptable for this use-case.</p>
",Named Entity Recognition (NER),use google natural language api enrich data bigquery table want use data stored bigquery table input google natural language api perform entity extraction sentiment analysis persist result back bigquery tool service could use handle gcp performance concern running overnight batch would acceptable use case
Why spacy ner results are highly unpredictable?,"<p>I tried spacy for ner but the results are highly unpredictable.Sometimes spacy is not recognizing a particular country.Can anyone please explain why is it happening?
I tried on some random sentences.</p>

<p>CASE 1:</p>

<pre><code>nlp = spacy.load(""en_core_web_sm"")
print(nlp)
sent = ""hello china hello japan""
doc = nlp(sent)
for i in doc.ents:
  print(i.text,"" "",i.label_)
</code></pre>

<p>OUTPUT:no output in this case.</p>

<p>CASE 2:</p>

<pre><code>nlp = spacy.load(""en_core_web_sm"")
print(nlp)
sent = ""china is a populous nation in East Asia whose vast landscape encompasses grassland, desert, mountains, lakes, rivers and more than 14,000km of coastline.""
doc = nlp(sent)
for i in doc.ents:
  print(i.text,"" "",i.label_)
</code></pre>

<p>OUTPUT:</p>

<pre><code>&lt;spacy.lang.en.English object at 0x7f2213bde080&gt;
china   GPE
East Asia   LOC
more than 14,000km   QUANTITY
</code></pre>
",Named Entity Recognition (NER),spacy ner result highly unpredictable tried spacy ner result highly unpredictable sometimes spacy recognizing particular country anyone please explain happening tried random sentence case output output case case output
Not only extracting places from a text but also other names in geograpy(Python),"<p>I am trying to extract only the City names from a text so I am using geograpy library with python but in the output, some other names have been extracted.
Here is my code:</p>

<pre><code>from geograpy.extraction import Extractor
text6 = u""""""Some text...""""""
e6 = Extractor(text=text6)
e6.find_entities()
print(e6.places)
</code></pre>

<p>INPUT TEXT:- </p>

<blockquote>
  <p>Opposition Leader Mahinda Rajapaksa says that the whole public
  administration has collapsed due to the constitution council’s
  arbitrary actions. The Opposition Leader said so in response to a
  query a journalised raised after a meeting held...</p>
</blockquote>

<p>OUTPUT</p>

<pre><code>['Opposition', 'Leader Mahinda Rajapaksa', 'Opposition Leader']
</code></pre>

<p>There are no any city names in this text therefore the output shold be empty</p>
",Named Entity Recognition (NER),extracting place text also name geograpy python trying extract city name text using geograpy library python output name extracted code input text opposition leader mahinda rajapaksa say whole public administration ha collapsed due constitution council arbitrary action opposition leader said response query journalised raised meeting held output city name text therefore output shold empty
Rasa roles of entities,"<p>My team is building a bot for hotel reservations and we have recently decided to migrate from Wit.ai to Rasa. In Wit.ai, we used the build-in entities:<br>
 - <code>datetime</code> - with roles check-in and check-out<br>
-  <code>number</code> - with roles number of people, number of rooms, and length of stay.    </p>

<p>How can  I put roles to datetime and number entities in Rasa? I've seen entity roles are not supported: <a href=""https://github.com/RasaHQ/rasa_nlu/issues/1169"" rel=""nofollow noreferrer"">https://github.com/RasaHQ/rasa_nlu/issues/1169</a>. Any ideas for a workaround? I guess I need something like double named-entity recognition. </p>

<p>Thanks in advance. </p>
",Named Entity Recognition (NER),rasa role entity team building bot hotel reservation recently decided migrate wit ai rasa wit ai used build entity role check check role number people number room length stay put role datetime number entity rasa seen entity role supported idea workaround guess need something like double named entity recognition thanks advance
Python named entity recognition (NER): Replace named entities with labels,"<p>I'm new to Python NER and am trying to replace named entities in text input with their labels.</p>
<pre><code>from nerd import ner
input_text = &quot;&quot;&quot;Stack Overflow is a question and answer site for professional and enthusiast programmers. It is a privately held website, the flagship site of the Stack Exchange Network,[5][6][7] created in 2008 by Jeff Atwood and Joel Spolsky.&quot;&quot;&quot;
doc = ner.name(input_text, language='en_core_web_sm')
text_label = [(X.text, X.label_) for X in doc]
print(text_label)
</code></pre>
<p>The output is: <code>[('2008', 'DATE'), ('Jeff Atwood', 'PERSON'), ('Joel Spolsky', 'PERSON')]</code></p>
<p>I can then extract the people, for example:</p>
<pre><code>people = [i for i,label in text_label if 'PERSON' in label] 
print(people)
</code></pre>
<p>to get <code>['Jeff Atwood', 'Joel Spolsky']</code>.</p>
<p>My question is how can I replace identified named entities in the original input text so that the result is:</p>
<p><code>Stack Overflow is a question and answer site for professional and enthusiast programmers. It is a privately held website, the flagship site of the Stack Exchange Network,[5][6][7] created in DATE by PERSON and PERSON.</code></p>
<p>Thanks so much!</p>
",Named Entity Recognition (NER),python named entity recognition ner replace named entity label new python ner trying replace named entity text input label output extract people example get question replace identified named entity original input text result thanks much
"In Stanford CoreNlp, why are not all proper nouns (NNP) also named entities","<p>I use Stanford CoreNlp for Names Entity Recognition (NER). I've noticed that in some cases that it's not 100% which is fine and not surprising. However, even if a, say, single-word named entity is not recognized (i.e., the label is <code>O</code>), it has the tag <code>NNP</code> (proper noun).</p>
<p>For example, given the example sentence &quot;The RestautantName in New York is the best outlet.&quot;, <code>nerTags()</code> yields <code>[O, O, O, LOCATION, LOCATION, O, O, O, O, O]</code> only correctly recognizing &quot;New York&quot;. The parse tree for this sentence looks like</p>
<pre><code>(ROOT
  (S
    (NP
      (NP (DT The) (NNP RestautantName))
      (PP (IN in)
        (NP (NNP New) (NNP York))))
    (VP (VBZ is)
      (NP (DT the) (JJS best) (NN outlet)))
    (. .)))
</code></pre>
<p>so &quot;RestaurantName&quot; is a proper noun (<code>NNP</code>)</p>
<p>When I look up the definition of a proper noun, it sounds very close to a named entity. What's the difference?</p>
",Named Entity Recognition (NER),stanford corenlp proper noun nnp also named entity use stanford corenlp name entity recognition ner noticed case fine surprising however even say single word named entity recognized e label ha tag proper noun example given example sentence restautantname new york best outlet yield correctly recognizing new york parse tree sentence look like restaurantname proper noun look definition proper noun sound close named entity difference
Custom entities extraction from texts,"<p>What is the right approach for multi-label text information extraction/classification</p>
<p>Having texts that describe a caregiver/patient visit : (made-up example)</p>
<blockquote>
<p>Mr *** visits the clinic on 02/2/2018 complaining about pain in the
lower back for several days, No pathological findings in the x-ray or
in the blood tests. I suggest Mr *** 5 resting days.</p>
</blockquote>
<p>Now, that text can be even in a paragraph size where the <strong>only</strong> information I care about will be <strong>lower back pain</strong> and <strong>resting days</strong>.  I have 300-400 different labels but the number of labeled samples can be around 1000-1500 (total) . When I label the text I also mark the relevant words that create the &quot;label&quot; ,here it will be ['pain','lower','back'].</p>
<p>When I just use look-up for those words (or the other 300-400 labels) in other texts I manage to label a larger amount of texts but if the words are written in different patterns such as <strong>Ache in the lower back</strong> or &quot;lowerback pain&quot; and I've never added that pattern to the look-up table of &quot;lower back pain&quot; I won't find it.</p>
<p>Due to the fact that I can have large paragraph but the only information I need is just 3-4 words, DL/ML models do not manage to learn with that amount of data and a high number of labels.I am wondering if there is a way to use the lookup table as a feature in the training phase or to try other approaches</p>
",Named Entity Recognition (NER),custom entity extraction text right approach multi label text information extraction classification text describe caregiver patient visit made example mr visit clinic complaining pain lower back several day pathological finding x ray blood test suggest mr resting day text even paragraph size information care lower back pain resting day different label number labeled sample around total label text also mark relevant word create label pain lower back use look word label text manage label larger amount text word written different pattern ache lower back lowerback pain never added pattern look table lower back pain find due fact large paragraph information need word dl ml model manage learn amount data high number label wondering way use lookup table feature training phase try approach
How can I extract sentences from paragraphs in a pandas.DataFrame and keep the paragraph key?,"<p>I have a <code>pandas.DataFrame</code> with 1604 paragraphs as follows:</p>
<p><a href=""https://i.sstatic.net/s97KL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/s97KL.jpg"" alt=""enter image description here"" /></a></p>
<p>I want to extract all the sentences (even in a NAIVE way using dots) and provide a new data frame which has in each row one sentence and the previous column values especially the paragraph key (mainly index in the first column in the left)</p>
<p>I have worked on that and could provide the chapter column for each sentence as follows:</p>
<pre><code> # Create lists to fill with values
l_col1 = []
l_col2 = []

# iterate over each row and fill our lists
for ix, row in dfAstroNova.iterrows():
    for value in row['sentences']:
        l_col1.append(value)
        l_col2.append(row['chapter'])

# Create new dataframe from the two lists
df= pd.DataFrame({'sentences': l_col1 ,
                         'chapter': l_col2 })
df=df.rename(columns={&quot;sentences&quot;:&quot;sents&quot;});

</code></pre>
<p>which gives me this data frame(dfAstroNova is the name of the original data frame)</p>
<p>as you see I have the chapter key. My question is how to add paragraph key (which is the number of column text in main data frame to a new data frame)</p>
<p>Then I have one other column which shows that this sentence belong to which paragraph in the original data frame or better one additional column which includes for each sentence the corresponded paragraph?</p>
<p><a href=""https://i.sstatic.net/Fx8q3.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Fx8q3.jpg"" alt=""enter image description here"" /></a></p>
",Named Entity Recognition (NER),extract sentence paragraph panda dataframe keep paragraph key paragraph follows want extract sentence even naive way using dot provide new data frame ha row one sentence previous column value especially paragraph key mainly index first column left worked could provide chapter column sentence follows give data frame dfastronova name original data frame see chapter key question add paragraph key number column text main data frame new data frame one column show sentence belong paragraph original data frame better one additional column includes sentence corresponded paragraph
NLP- Finding the number of sentences which contain named entities,"<p>I am a beginner in NLP Python. I got a challenge to find the number of sentences that contain named entities. But I got a different answer by using two different methods. The first method I did it myself and the second one did by my instructor. I don't know what is wrong with my solution or <strong>did I do something stupid!</strong> Can anyone help me by explaining why I am getting two different answers?</p>
<p>By using this(First) method I got <code>34</code> as answer.</p>
<pre><code>list_of_ners=[nlp(sent.text) for sent in doc.sents if sent.ents]
print(len(list_of_ners))//34
</code></pre>
<p>By using this(Second) method I got <code>36</code> as answer.</p>
<pre><code>list_of_sents=[nlp(sent.text) for sent in doc.sents]
list_of_ners=[doc for doc in list_of_sents if doc.ents]
print(len(list_of_ners))//36
</code></pre>
",Named Entity Recognition (NER),nlp finding number sentence contain named entity beginner nlp python got challenge find number sentence contain named entity got different answer using two different method first method second one instructor know wrong solution something stupid anyone help explaining getting two different answer using first method got answer using second method got answer
What is the best way to extract the body of an article with Python?,"<p><strong>Summary</strong></p>
<p>I am building a text summarizer in Python. The kind of documents that I am mainly targeting are scholarly papers that are usually in pdf format.</p>
<p><strong>What I Want to Achieve</strong></p>
<p>I want to effectively extract the body of the paper (abstract to conclusion), excluding title of the paper, publisher names, images, equations and references.</p>
<p><strong>Issues</strong></p>
<p>I have tried looking for effective ways to do this, but I was not able to find something tangible and useful. The current code I have tries to split the pdf document by sentences and then filters out the entries that have less than average number of characters per sentence. Below is the code:</p>
<pre><code>from pdfminer import high_level

# input: string (path to the file)
# output: list of sentences
def pdf2sentences(pdf): 
    article_text = high_level.extract_text(pdf)
    sents = article_text.split('.') #splitting on '.', roughly splits on every sentence      
    run_ave = 0
    
    for s in sents:
        run_ave += len(s)
    run_ave /= len(sents)
    sents_strip = []
    
    for sent in sents:
        if len(sent.strip()) &gt;= run_ave:
            sents_strip.append(sent)

    return sents_strip
</code></pre>
<p><em>Note: I am using <a href=""http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf"" rel=""nofollow noreferrer"">this</a> article as input.</em></p>
<p>Above code seems to work fine, but I am still not effectively able to filter out thing like title and publisher names that come before the abstract section and things like the references section that come after the conclusion. Moreover, things like images are causing gibberish characters to show up in the text which is messing up the overall quality of the output. Due to the weird unicode characters I am not able to write the output to a txt file.</p>
<p><strong>Appeal</strong></p>
<p>Are there ways I can improve the performance of this parser and make it more consistent?</p>
<p>Thank you for your answers!</p>
",Named Entity Recognition (NER),best way extract body article python summary building text summarizer python kind document mainly targeting scholarly paper usually pdf format want achieve want effectively extract body paper abstract conclusion excluding title paper publisher name image equation reference issue tried looking effective way wa able find something useful current code try split pdf document sentence filter entry le average number character per sentence code note using article input code seems work fine still effectively able filter thing like title publisher name come abstract section thing like reference section come conclusion moreover thing like image causing gibberish character show text messing overall quality output due weird unicode character able write output txt file way improve performance parser make consistent thank answer
Should I use NLP to detect entities in metadata? How?,"<p>I have some meta-data about a building, here is one of the example:<br/>
<code>AHU-S-6F-01.RA.CO2.1</code><br/>
I am finding a way to make the computer able to recognize entities in metadata like:<br/>
<code>[Location].[Sensor-Type].[Sensor-Element].[Sensor-ID]</code><br/>
There have other data formats in the dataset, so I am thinking I can use Named Entity Recognition (NER) and implement it by Tensorflow.js to make the computer learn to recognize entities in metadata and give a tag to them. I found an example below:</p>
<p><a href=""https://i.sstatic.net/8Tcpk.png"" rel=""nofollow noreferrer"">https://monkeylearn.com/text-analysis/</a></p>
<p>Is Named Entity Recognition(NER) suitable to use to detect entities in metadata? How can I implement it by Tensorflow.js? <br/>
If not, what should I use to solve this problem?<br/>
Thanks!!!</p>
",Named Entity Recognition (NER),use nlp detect entity metadata meta data building one example finding way make computer able recognize entity metadata like data format dataset thinking use named entity recognition ner implement tensorflow j make computer learn recognize entity metadata give tag found example named entity recognition ner suitable use detect entity metadata implement tensorflow j use solve problem thanks
Ner tagging of variable in sentence using corenlp?,"<p>I am new to NLP. I am looking for a way to identify variable part from the sentence, for ex &quot;Show details of employee EMP01&quot;, I am able to tag employee as entity but didn't found any way to identify emp_id</p>
",Named Entity Recognition (NER),ner tagging variable sentence using corenlp new nlp looking way identify variable part sentence ex show detail employee emp able tag employee entity found way identify emp id
Extract text blocks with Python + NLTK,"<p>I'm looking for a way to extract text blocks by recognizing patterns with Python and NLTK.</p>
<p>I have the text below and need extract the corresponding text of the question and each alternative.</p>
<pre><code>QUESTION 01
For a p-type semiconductor, which of the following statements is true?
A Holes are the majority carriers and pentavalent atoms are the dopants.
B Electrons are the majority carriers and pentavalents atms are the dopants.
C Electrons are the majority carriers and trivalent atoms are the dopants.
</code></pre>
<p>The best way to do it is with a sort of patterns to the &quot;Question&quot; entity and other patterns to the &quot;Alternative&quot; entity?</p>
",Named Entity Recognition (NER),extract text block python nltk looking way extract text block recognizing pattern python nltk text need extract corresponding text question alternative best way sort pattern question entity pattern alternative entity
How to extract particular word(s) from the list of sentences using Python NLP. These word(s) are Parts of Medical equipments,"<p>I want to extract Medical Equipment part names from the list of sentences.
These sentences are recorded actions taken on a support request which might include replacement of a part or acknowledging a part is in a bad state.</p>
<p>Here are the sample sentences-</p>
<ol>
<li>Found [Some equipment part] on both side not working.</li>
<li>Narrowed problem down to the [Some equipment part].</li>
<li>Needed a [Some equipment part] replacement.</li>
<li>Assisted with troubleshooting, found [Some equipment part] is most likely bad.</li>
<li>[Some equipment part] won't go down, will order part.</li>
</ol>
<p>I want to extract &quot;[Some equipment part]&quot; from the above sentences.</p>
<p>Things I've already tried-
First, I've filtered the sentences using Sentiment Analysis. Considering only the ones which have a negative sentiment or have &quot;replace&quot; text in them.</p>
<ol>
<li>Using NLTK, After POS tagging using RegexpParser on a defined grammer = &quot;NP : {&lt;VB.<em>&gt;&lt;NN.</em>&gt;+&lt;NN.<em>&gt;+|&lt;VB.</em>&gt;<em>&lt;NN.</em>&gt;+}&quot;</li>
<li>Using Spacy, After POS tagging and dependency, filtering based on Verb, Noun relationship - token.dep_ in ['dobj'] and token.pos_ == 'NOUN'</li>
</ol>
<p>The above approaches gives me a lot of meaningless output.
Please, let me know if there is anything which can be of help.</p>
",Named Entity Recognition (NER),extract particular word list sentence using python nlp word part medical equipment want extract medical equipment part name list sentence sentence recorded action taken support request might include replacement part acknowledging part bad state sample sentence found equipment part side working narrowed problem equipment part needed equipment part replacement assisted troubleshooting found equipment part likely bad equipment part go order part want extract equipment part sentence thing already tried first filtered sentence using sentiment analysis considering one negative sentiment replace text using nltk po tagging using regexpparser defined grammer np vb nn nn vb nn using spacy po tagging dependency filtering based verb noun relationship token dep dobj token po noun approach give lot meaningless output please let know anything help
How to extract more than label text items in a single annotation using Google NLP,"<p>I have created dataset using Google NLP Entity extraction and I uploaded input data's(train, test, validation jsonl files) like NLP format that will be stored in google storage bucket.</p>

<p><strong>Sample Annotation:</strong></p>

<pre><code>   {
    ""annotations"": [{
        ""text_extraction"": {
            ""text_segment"": {
                ""end_offset"": 10,
                ""start_offset"": 0
            }
        },
        ""display_name"": ""Name""
    }],
    ""text_snippet"": {
        ""content"": ""JJ's Pizza\n ""
    }
} {
    ""annotations"": [{
        ""text_extraction"": {
            ""text_segment"": {
                ""end_offset"": 9,
                ""start_offset"": 0
            }
        },
        ""display_name"": ""City""
    }],
    ""text_snippet"": {
        ""content"": ""San Francisco\n ""
    }
}
</code></pre>

<p>Here is the input text to predict the label as ""Name"", ""City"" and ""State""</p>

<blockquote>
  <p>Best J J's Pizza in San Francisco, CA</p>
</blockquote>

<p>Result in the following screenshot,</p>

<p><a href=""https://i.sstatic.net/YVU6Q.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/YVU6Q.png"" alt=""predict-data""></a></p>

<p>I expect the predicted results would be in the following,</p>

<blockquote>
  <p><strong>Name</strong> : JJ's Pizza 
  <strong>City</strong> : San Francisco 
  <strong>State</strong>: CA</p>
</blockquote>
",Named Entity Recognition (NER),extract label text item single annotation using google nlp created dataset using google nlp entity extraction uploaded input data train test validation jsonl file like nlp format stored google storage bucket sample annotation input text predict label name city state best j j pizza san francisco ca result following screenshot expect predicted result would following name jj pizza city san francisco state ca
How to extract specific parts of messy PDFs in R?,"<p>I need to extract specific parts of a large corpus of PDF documents. The PDFs are large and messy reports containing all kinds of digital, alphabetic and other information. The files are of different length but have unified content and sections across them. The documents have a Table of Content with the section names in them. For example</p>
<pre><code>Table of Content:

Item 1. Business                                                                            1
Item 1A. Risk Factors                                                                       2
Item 1B. Unresolved Staff Comments                                                          5
Item 2. Properties                                                                          10
Item N........

..........text I do not care about...........

Item 1A. Risk Factors 

.....text I am interested in getting.......

(section ends)

Item 1B. Unresolved Staff Comments

..........text I do not care about...........
</code></pre>
<p>I have no problem reading them in and analyzing them as a whole but I need to pull out only the text between <strong>&quot;Item 1A. Risk Factors&quot;</strong> and <strong>&quot;Item 1B. Unresolved Staff Comments&quot;</strong>.
I used <em>pdftools, tm, quanteda and readtext package</em>
This is the part of code I use to read-in my docs. I created a directory where I placed my PDFs and called it &quot;PDF&quot; and another directory where R will place converted to &quot;.txt&quot; files.</p>
<pre><code>pdf_directory &lt;- paste0(getwd(), &quot;/PDF&quot;)
txt_directory &lt;- paste0(getwd(), &quot;/Texts&quot;)
</code></pre>
<p>Then I create a list of files using &quot;list.files&quot; function.</p>
<pre><code>files &lt;- list.files(pdf_directory, pattern = &quot;.pdf&quot;, recursive = FALSE, 
                    full.names = TRUE)
files
</code></pre>
<p>After that, I go on to create a function that extracts file names.</p>
<pre><code>extract &lt;- function(filename) {
  print(filename)
  try({
    text &lt;- pdf_text(filename)
  })
  f &lt;- gsub(&quot;(.*)/([^/]*).pdf&quot;, &quot;\\2&quot;, filename)
  write(text, file.path(txt_directory, paste0(f, &quot;.txt&quot;)))
}
</code></pre>
<pre><code>
for (file in files) {
  extract(file)
}
</code></pre>
<p>After this step, I get stuck and do not know how to proceed. I am not sure if I should try to extract the section of interest when I read data in, therefore, I suppose, I would have to wrestle with the chunk where I create the function --  <code>f &lt;- gsub(&quot;(.*)/([^/]*).pdf&quot;, &quot;\\2&quot;, filename)</code>? I apologize for such questions but I am self-teaching myself.
I also tried engaging the following code on just one file instead of a corpus:</p>
<pre><code>start &lt;- grep(&quot;^\\*\\*\\* ITEM 1A. RISK FACTORS&quot;, text_df$text) + 1

stop &lt;- grep(&quot;^ITEM 1B. UNRESOLVED STAFF COMMENTS&quot;, text_df$text) - 1

lines &lt;- raw[start:stop]

scd &lt;- paste0(&quot;.*&quot;,start,&quot;(.*)&quot;,&quot;\n&quot;,stop,&quot;.*&quot;)  
gsub(scd,&quot;\\1&quot;, name_of_file)
</code></pre>
<p>but it did not help me in any way.</p>
",Named Entity Recognition (NER),extract specific part messy pdfs r need extract specific part large corpus pdf document pdfs large messy report containing kind digital alphabetic information file different length unified content section across document table content section name example problem reading analyzing whole need pull text item risk factor item b unresolved staff comment used pdftools tm quanteda readtext package part code use read doc created directory placed pdfs called pdf another directory r place converted txt file create list file using list file function go create function extract file name step get stuck know proceed sure try extract section interest read data therefore suppose would wrestle chunk create function apologize question self teaching also tried engaging following code one file instead corpus help way
How can i use flair NER tagger with DKpro-core,"<p>There are already many taggers such as Stanford,opennlp, etc in DKpro but I would like to know how can I implement flair NER tagger in DKpro-core?</p>
",Named Entity Recognition (NER),use flair ner tagger dkpro core already many tagger stanford opennlp etc dkpro would like know implement flair ner tagger dkpro core
Machine Learning model to parse webpage data and extract fields,"<p>I need to extract common data from different websites. Like I want to scrape 100 event websites and extract the same information like event name, price, location etc. Every website has a different layout so I'm writing scraping rules by hand. There are some services like diffbot that can extract this automatically. They are using some sort of AI/ML model. I was wondering if this can be a Named Entity task or maybe LSTM can be used.</p>
",Named Entity Recognition (NER),machine learning model parse webpage data extract field need extract common data different website like want scrape event website extract information like event name price location etc every website ha different layout writing scraping rule hand service like diffbot extract automatically using sort ai ml model wa wondering named entity task maybe lstm used
How to compare noun chunks in python with spacy.io,"<p>I am trying to write a Python application which will take a natural language query and attempt to convert it into <a href=""https://learn.microsoft.com/en-us/analysis-services/multidimensional-models/mdx/mdx-query-the-basic-query"" rel=""nofollow noreferrer"">Microsoft MDX queries for SQL-Server, to query a Cube</a>.</p>
<p>I am using spacy.io and I am having trouble figuring out the best way to compare noun chunks with the names of datasources/measures/filters/etc.</p>
<p>Example input sentence:</p>
<pre><code>&quot;Give me household count split by net sales from the base customer cube&quot;
</code></pre>
<p>And from that I can extract the following noun chunks:</p>
<pre><code>NP:  household count split
NP:  net sales
NP:  the base customer cube
</code></pre>
<p>What's the best way to compare those noun chunks to the following display names from my test cube?</p>
<pre><code>Household Count
Net Sales
Base Customer
</code></pre>
<p>The trouble is one of the noun chunks contains the word &quot;split&quot;, I can't just remove it in case one of the display names from the cube also contains that word.</p>
<p>I have thought about trying a few things, for example calculating the Levenstein distance between the strings but the results are not really reliable enough to declare a match.</p>
<p>So my question is how can I solve this and get more reliable matches? Am I barking up the wrong tree? Or perhaps not using Spacy.io to its full potential?</p>
<p>I have thought about parsing the noun chunks and removing key words like &quot;split&quot; and unnecessary words such as &quot;the&quot; before getting the Levenshtein distance, but I don't feel comfortable with providing a list of possible words to remove, in the case that there are some genuine display names which contain one of the keywords.</p>
",Named Entity Recognition (NER),compare noun chunk python spacy io trying write python application take natural language query attempt convert microsoft mdx query sql server query cube using spacy io trouble figuring best way compare noun chunk name datasources measure filter etc example input sentence extract following noun chunk best way compare noun chunk following display name test cube trouble one noun chunk contains word split remove case one display name cube also contains word thought trying thing example calculating levenstein distance string result really reliable enough declare match question solve get reliable match barking wrong tree perhaps using spacy io full potential thought parsing noun chunk removing key word like split unnecessary word getting levenshtein distance feel comfortable providing list possible word remove case genuine display name contain one keywords
Improving spacy entity accuracy for location named entities,"<p>This is my first time working with Spacy and apologies if this question is pretty standard. I have gone through a lot of questions here but haven't found something that answers my question.</p>
<p>I have block of text (3-5 lines). The first 1-2 lines can contain names of individuals or organization. The next 2 and 3 contain addresses.There are some exceptions but this is generally the standard input.</p>
<p><strong>Examples</strong></p>
<p>1.
John Doe<br />
9308 EAST MILL AVE.<br />
ONALSKA WI 54650</p>
<p>2.
/1234567890<br />
JANE DOE<br />
277 LOWER RIVER RD<br />
CAMDEN, NJ 08105</p>
<p>3.
XYZ CONSTRUCTION GROUP<br />
4 VALLEY RD.<br />
SULPHUR, LA 70663</p>
<p>4.
/0987654321<br />
ABC FASHION BOARD<br />
3 ARNOLD ROAD<br />
SUITE 6598 MIAMI FL 33126<br />
UNITED STATES</p>
<p><strong>The objective is to identify all persons, organizations, and locations from this block of text.</strong></p>
<p>Spacy is having some difficulty classifying locations. I added a few patterns to the nlp pipe and classification improved marginally. Code below:</p>
<pre><code>ruler = EntityRuler(nlp, overwrite_ents=True)
patterns = [{&quot;label&quot;: &quot;LOC&quot;, &quot;pattern&quot;: [{'IS_ALPHA': True},{'LOWER': 'rd'}]}
ruler.add_patterns(patterns)
nlp.add_pipe(ruler)
nlp.pipe_names
</code></pre>
<p>I wanted to know:</p>
<ol>
<li><p>Since this input is so standard, can we train the spacy model to recognize this and maybe that improves classification?</p>
</li>
<li><p>Can we extract a list of Geographical entities to overwrite prediction. not sure if this is a good idea.</p>
</li>
</ol>
<p>Any other suggestions on improving the overall accuracy are welcome.</p>
",Named Entity Recognition (NER),improving spacy entity accuracy location named entity first time working spacy apology question pretty standard gone lot question found something answer question block text line first line contain name individual organization next contain address exception generally standard input example john doe east mill ave onalska wi jane doe lower river rd camden nj xyz construction group valley rd sulphur la abc fashion board arnold road suite miami fl united state objective identify person organization location block text spacy difficulty classifying location added pattern nlp pipe classification improved marginally code wanted know since input standard train spacy model recognize maybe improves classification extract list geographical entity overwrite prediction sure good idea suggestion improving overall accuracy welcome
Stanford CoreNLP Chinese model training,"<p>I am currently trying to train my own Chinese NER model with CoreNLP however when executing the training command I get an FileNotFoundException. I have seen post about this error being fixed in CoreNLP 3.5.0, however I am using 4.1.0 and it is still occurring.</p>
<pre><code>Exception in thread &quot;main&quot; edu.stanford.nlp.io.RuntimeIOException: java.io.FileNotFoundException: \u\nlp\data\chinese\distsim\xin_cmn_200907-201012.ldc.seg.utf8.c1000 (The system cannot find the path specified)
        at edu.stanford.nlp.io.IOUtils.inputStreamFromFile(IOUtils.java:523)
        at edu.stanford.nlp.io.IOUtils.readerFromFile(IOUtils.java:558)
        at edu.stanford.nlp.objectbank.ReaderIteratorFactory$ReaderIterator.setNextObject(ReaderIteratorFactory.java:189)
        at edu.stanford.nlp.objectbank.ReaderIteratorFactory$ReaderIterator.&lt;init&gt;(ReaderIteratorFactory.java:161)
        at edu.stanford.nlp.objectbank.ReaderIteratorFactory.iterator(ReaderIteratorFactory.java:98)
        at edu.stanford.nlp.objectbank.ObjectBank$OBIterator.&lt;init&gt;(ObjectBank.java:411)
        at edu.stanford.nlp.objectbank.ObjectBank.iterator(ObjectBank.java:250)
        at edu.stanford.nlp.ie.NERFeatureFactory.initLexicon(NERFeatureFactory.java:588)
        at edu.stanford.nlp.ie.NERFeatureFactory.init(NERFeatureFactory.java:389)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.reinit(AbstractSequenceClassifier.java:210)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.&lt;init&gt;(AbstractSequenceClassifier.java:190)
        at edu.stanford.nlp.ie.crf.CRFClassifier.&lt;init&gt;(CRFClassifier.java:181)
        at edu.stanford.nlp.ie.crf.CRFClassifier.chooseCRFClassifier(CRFClassifier.java:2919)
        at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:2930)
Caused by: java.io.FileNotFoundException: \u\nlp\data\chinese\distsim\xin_cmn_200907-201012.ldc.seg.utf8.c1000 (The system cannot find the path specified)
        at java.base/java.io.FileInputStream.open0(Native Method)
        at java.base/java.io.FileInputStream.open(FileInputStream.java:212)
        at java.base/java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:154)
        at edu.stanford.nlp.io.IOUtils.inputStreamFromFile(IOUtils.java:516)
        ... 13 more
</code></pre>
",Named Entity Recognition (NER),stanford corenlp chinese model training currently trying train chinese ner model corenlp however executing training command get filenotfoundexception seen post error fixed corenlp however using still occurring
StanfordNLP: Unable to identify Date with 7-class-ner,"<p>I'm using stanfordNLP to get date entities from text. Here's the code that i tried:-</p>
<pre><code>import java.io.IOException;
import java.util.List;
import edu.stanford.nlp.ie.AbstractSequenceClassifier;
import edu.stanford.nlp.ie.crf.CRFClassifier;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreLabel;

public class StanfordNLP_POC
{

    public static void main(String[] args) throws IOException
    {
        // TODO Auto-generated method stub
        String classifierPath = &quot;src//main//resources//classifiers//english.muc.7class.distsim.crf.ser.gz&quot;;

        String inputString = &quot;Appointment Facility: ABC Medicine Clinic 05/07/2020 Progress Notes: Niel Armstrong, DO Current Medications Reason for Appointment&quot;;

        AbstractSequenceClassifier classifier = CRFClassifier.getClassifierNoExceptions(classifierPath);

        List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(inputString);

        System.out.println(out.toString());

        for (List&lt;CoreLabel&gt; sentence : out)
        {
            for (CoreLabel word : sentence)
            {

                if (word.getString(CoreAnnotations.AnswerAnnotation.class).equals(&quot;O&quot;))
                    continue;
                System.out.println(word.word() + &quot; = &quot; + word.get(CoreAnnotations.AnswerAnnotation.class));
            }
        }

    }

}
</code></pre>
<p>I didn't get why it's not extracting Date even though it's very clearly identifiable in the text.</p>
",Named Entity Recognition (NER),stanfordnlp unable identify date class ner using stanfordnlp get date entity text code tried get extracting date even though clearly identifiable text
NLP-Name Entity Recognition : How to map different naming of the entity to the same entity? AMC vs AMC Entertainment vs AMC Theatres,"<p>I am doing a named entity recognition task where I am counting how many times a certain entity was mentioned in a document.</p>
<p>What I found is the different naming convention of the same entity has been counted separately.
For example AMC, AMC Entertainment, AMC theatres.
How can I know they are all referring to the same entity and count it 3 times, instead of counting 1 time for each?</p>
<p>Currently using Spacy, open to other python solutions.
<a href=""https://spacy.io/usage/spacy-101"" rel=""nofollow noreferrer"">https://spacy.io/usage/spacy-101</a></p>
",Named Entity Recognition (NER),nlp name entity recognition map different naming entity entity amc v amc entertainment v amc theatre named entity recognition task counting many time certain entity wa mentioned document found different naming convention entity ha counted separately example amc amc entertainment amc theatre know referring entity count time instead counting time currently using spacy open python solution
How to identify address location from text string php?,"<p>I'm trying to identify and extract any input address location (Not limited to US - <a href=""https://smartystreets.com/features#extract"" rel=""nofollow noreferrer"">SmartyStreet</a>) from a long string of text using php on my xampp.</p>
<p>I've read several topics/libraries regarding on how to do this, which revolves around using NLP, Google's Geocoding API and regex to perform the above mentioned task. These 3 links are some plausible link that may help <a href=""https://stackoverflow.com/questions/14087116/extract-address-from-string"">Link 1</a>, <a href=""https://gist.github.com/Jonathonbyrd/536049"" rel=""nofollow noreferrer"">Link 2</a>, <a href=""https://github.com/openvenues/libpostal"" rel=""nofollow noreferrer"">Link 3/GitHub Library(Seems Promising)</a>.</p>
<p>However, I do not know whether these links may be of any help with the implementation? Can anyone help me with it?</p>
",Named Entity Recognition (NER),identify address location text string php trying identify extract input address location limited u smartystreet long string text using php xampp read several topic library regarding revolves around using nlp google geocoding api regex perform mentioned task link plausible link may help link link github library seems promising however know whether link may help implementation anyone help
What is the meaning of &quot;drop&quot; and &quot;sgd&quot; while training custom ner model using spacy?,"<p>I am training a custom ner model to identify organization name in addresses. 
My training loop looks like this:-</p>

<pre><code>    for itn in range(100):
        random.shuffle(TRAIN_DATA)
        losses = {}
        batches = minibatch(TRAIN_DATA, size=compounding(15., 32., 1.001))
        for batch in batches
            texts, annotations = zip(*batch)
            nlp.update(texts, annotations, sgd=optimizer,
                       drop=0.25, losses=losses)

        print('Losses', losses)
</code></pre>

<p>Can someone explain the parameters ""drop"", ""sgd"", ""size"" and give some ideas to how should I change these values, so that my model performs better. </p>
",Named Entity Recognition (NER),meaning drop sgd training custom ner model using spacy training custom ner model identify organization name address training loop look like someone explain parameter drop sgd size give idea change value model performs better
Preprocessing of Labelled Text Data,"<p>To train NLP models for NER it is necessary to have text data that has named entity labels on the text. In many cases this is given by a character offset (eg.
<code>(&quot;Android Pay expands to Canada&quot;, [(0, 11, 'PRODUCT'), (23, 30, 'GPE')])</code>), BILUO format (eg. <code>([&quot;Facebook&quot;, &quot;released&quot;, &quot;React&quot;, &quot;in&quot;, &quot;2014&quot;], [&quot;U-ORG&quot;, &quot;O&quot;, &quot;U-TECHNOLOGY&quot;, &quot;O&quot;, &quot;U-DATE&quot;])</code>) or something similar (examples taken from the spaCy101). When one wants to do preprocessing on this data it is important to keeps the labels in the correct positions. Eg. for removing stop words or manipulating white-space characters and tokens will be removed. My question is:</p>
<blockquote>
<p>Is there a data structure that allows preprocessing of labelled text data while moving labels to the correct positions in the new text?</p>
</blockquote>
<p>If there is already an implementation of this in python I would also be interested in this. Otherwise I might also be willing to code it up myself.</p>
<p>I used NER as a motivation for this question but I would also be interested if there was a more general data-structure that can store different types of labels simultaneously.</p>
<p>For example I would like to do some operation like this to make all of my document lower case.</p>
<pre><code>import spacy
nlp = spacy.blank('en')
doc = nlp('Hello World! This is an amazing day!')
doc.text = doc.text.lower()
</code></pre>
<p>This is not possible and returning an error.</p>
<p><code>AttributeError: attribute 'text' of 'spacy.tokens.doc.Doc' objects is not writable</code></p>
",Named Entity Recognition (NER),preprocessing labelled text data train nlp model ner necessary text data ha named entity label text many case given character offset eg biluo format eg something similar example taken spacy one want preprocessing data important keep label correct position eg removing stop word manipulating white space character token removed question data structure allows preprocessing labelled text data moving label correct position new text already implementation python would also interested otherwise might also willing code used ner motivation question would also interested wa general data structure store different type label simultaneously example would like operation like make document lower case possible returning error
Dates when using StanfordCoreNLP pipeline,"<p>If I create an AnnotationPipeline with a TokenizerAnnotator, WordsToSentencesAnnotator, POSTaggerAnnotator, and sutime, I get TimexAnnotations attached to the resulting annotation.</p>

<p>But if I create a StanfordCoreNLP pipeline with the ""annotators"" property set to ""tokenize, ssplit, pos, lemma, ner"", I don't get TimexAnnotations even though the relevant individual tokens are NER-tagged as DATE.</p>

<p>Why is there this difference?</p>
",Named Entity Recognition (NER),date using stanfordcorenlp pipeline create annotationpipeline tokenizerannotator wordstosentencesannotator postaggerannotator sutime get timexannotations attached resulting annotation create stanfordcorenlp pipeline annotator property set tokenize ssplit po lemma ner get timexannotations even though relevant individual token ner tagged date difference
How to concatenate Glove 100d embedding and 1d array which contains additional signal?,"<p>I new to NLP and trying out some text classification algorithms. I have 100d GloVe vector representing each entry as a list of embeddings. Also, I have NER feature of shape (2234,) which shows if there is named entity or not. Array with GloVe embeddings is of shape (2234, 100).</p>
<p>How to correctly concatenate these array so each row represents its word?</p>
<p>Sorry for not including reproducible example. Please, use variables of your choice to explain the concatenation procedure.</p>
<p>Using <code>np.concatenate</code> did not work as I have expected but i don't know how to deal with dimensionality of embeddings.</p>
",Named Entity Recognition (NER),concatenate glove embedding array contains additional signal new nlp trying text classification algorithm glove vector representing entry list embeddings also ner feature shape show named entity array glove embeddings shape correctly concatenate array row represents word sorry including reproducible example please use variable choice explain concatenation procedure using work expected know deal dimensionality embeddings
How to extract pre-defined key words from a sentence in Python?,"<p>Consider the following example
&quot;10% of on all Artificial Intelligence courses.&quot;
In this example, I have to extract two predefined classes like Artificial Intelligence and courses. Even the program has to classify words like ANN, CNN, RNN, AI, etc. into the Artificial Intelligence category. I have used spacy to train but I am not impressed with the results as it is not labeling correctly. Is there any alternative to extract entities from a sentence in Python?</p>
",Named Entity Recognition (NER),extract pre defined key word sentence python consider following example artificial intelligence course example extract two predefined class like artificial intelligence course even program ha classify word like ann cnn rnn ai etc artificial intelligence category used spacy train impressed result labeling correctly alternative extract entity sentence python
"Is it possible to extract specific({Architect, Building}) information from unstructured text chunks using NLP?","<p>I am working on a task to extract architects and their buildings from unstructured pieces of texts with varying sizes. I started trying a NLP tool called SpaCy but annotations they provide sometimes mixes up.</p>
<p><strong>FAC</strong> Buildings, airports, highways, bridges, etc.</p>
<p><strong>ORG</strong> Companies, agencies, institutions, etc.</p>
<p><strong>GPE</strong> Countries, cities, states.</p>
<p><strong>LOC</strong> Non-GPE locations, mountain ranges, bodies of water.</p>
<p>Building names falls into those 4 annotations. My job would be so much easier if i could get only <strong>FAC</strong> for building names but it looks like it is not possible or i couldn't be able to make it work.</p>
<p>The question is, is it even possible to use NLP tools to extract such information tuples(in my case {Architect, Building}) from a chunk of text?</p>
<p>Edit: Some things i have done</p>
<p>Following bits are some examples of texts i am using at the moment</p>
<blockquote>
<p>He renovated Fatih Mosque and built Laleli Mosque in the name of
Sultan Mustafa III</p>
<p>Mehmed Tahir Ağa built Hamidiyye Complex in Bahçekapı for Sultan
Abdülhamid I.</p>
</blockquote>
<p>I am giving those texts as data to the spaCy, code bit is here:</p>
<pre><code>for i in range(len(data)):
text = data[i]
text = re.sub(r'\([^()]*\)', '', text)
doc = nlp(text)

#Extract ORG, GPE, LOC and FAC labels from phrases
for entity in doc.ents:
    if entity.label_ in ('ORG', 'GPE', 'LOC', 'FAC'):
        #Manual filtering of results
        if entity.text not in (&quot;Istanbul&quot;, &quot;Egypt&quot;, &quot;Hicaz&quot;, &quot;Palestine&quot;, &quot;Syria&quot;, &quot;Balkans&quot;, &quot;Albania&quot;, &quot;Malta&quot;, &quot;Spain&quot;, &quot;Bosnia&quot;, &quot;Frengistan&quot;, &quot;Kırım&quot;, &quot;Belgrade&quot;, &quot;Damascus&quot;):
            print(entity.text, entity.label_)
</code></pre>
<p>Output is:</p>
<p>Laleli Mosque ORG</p>
<p>Hamidiyye Complex ORG</p>
<p>Bahçekapı for Sultan Abdülhamid I. ORG</p>
",Named Entity Recognition (NER),possible extract specific architect building information unstructured text chunk using nlp working task extract architect building unstructured piece text varying size started trying nlp tool called spacy annotation provide sometimes mix fac building airport highway bridge etc org company agency institution etc gpe country city state loc non gpe location mountain range body water building name fall annotation job would much easier could get fac building name look like possible able make work question even possible use nlp tool extract information tuples case architect building chunk text edit thing done following bit example text using moment renovated fatih mosque built laleli mosque name sultan mustafa iii mehmed tahir built hamidiyye complex bah ekap sultan abd lhamid giving text data spacy code bit output laleli mosque org hamidiyye complex org bah ekap sultan abd lhamid org
how to extract text from PDF between some patterns,"<p>I have some thousands of documents (in German) in PDF format. I need to extract a portion of text from each one of them, which usually comes after date and ends before the Date, Location, Address info nearly at the page end. An example is attached. The text I need is highlighted.
<a href=""https://i.sstatic.net/PUrYl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PUrYl.png"" alt=""enter image description here"" /></a></p>
<p>What I tried so far is  <code>qdapRegex::rm_between</code>:</p>
<pre><code>library(pdftools)
library(qdapRegex)
t1 &lt;- pdf_text(&quot;textsample.pdf&quot;)
textIneed &lt;- rm_between(t1, &quot;Datum&quot;, &quot;, den&quot; )
</code></pre>
<p>which did not work. The output is the whole text content including Name, Vorame, etc. (I could live with the date 20.01.2019 at the beginning.) What I am doing wrong is not clear to me, as I am new to regex and nlp and cannot spot it reading the documentation of <code>rm_between</code> or <code>qdapRegex</code>.</p>
<p>My first problem is getting this working.</p>
<p>The further problem is, these documents are not standard, and some documents may have another type of info, e.g., <code>Ref:1234</code> instead of date before the highlighted area. This piece may be found in any page number of the document, so going for exact page numbers is not an option.</p>
<p>Is there any other solution, library, etc. which can be used to extract more or less this portion of text?</p>
",Named Entity Recognition (NER),extract text pdf pattern thousand document german pdf format need extract portion text one usually come date end date location address info nearly page end example attached text need highlighted tried far work output whole text content including name vorame etc could live date beginning wrong clear new regex nlp spot reading documentation first problem getting working problem document standard document may another type info e g instead date highlighted area piece may found page number document going exact page number option solution library etc used extract le portion text
Predicting next week incidents using text analysis on incident reports,"<p>I want to do a project but I'm not sure if it's really viable neither which path I should take to try and solve this.</p>
<p>I have a dataset with numerous incidents from different places and their risk classification. For example:</p>
<pre><code>Incident: &quot;John stumbled and fell down the stairs&quot;

Risk: Severe

Warehouse: A

Date: 2020-07-11

---

Incident: &quot;Mary left the door open&quot;

Risk: Low

Warehouse: B

Date: 2020-07-10
</code></pre>
<p>My idea is to compile the incidents by warehouse by week and give a probability of incidents happening in each warehouse for every risk.</p>
<pre><code>Warehouse A

Probability of low risk incidents next week: 60%

Probability of severe risk incidents next week: 30%
</code></pre>
<p>But I'm not really sure how to get around this problem. It's not really text classification because I know the classification of every report (risk). Is there a way to use this dataset and get any prediction for the next week?</p>
",Named Entity Recognition (NER),predicting next week incident using text analysis incident report want project sure really viable neither path take try solve dataset numerous incident different place risk classification example idea compile incident warehouse week give probability incident happening warehouse every risk really sure get around problem really text classification know classification every report risk way use dataset get prediction next week
"ner, spacy,sentence segmentation","<p>I want to break this sentences in order to process it using spacy</p>
<pre><code>Finally, on 1595 July 22 at 2h 40m am, when the sun was at 7° 59' 52&quot; Leo, 101,487 distant from earth, Mars's mean longitude 11s 14° 9' 5&quot;, and anomaly 164° 48' 55&quot;, and consequent eccentric position from the vicarious hypothesis 17° 16' 36&quot; Pisces: the apparent position of Mars, from the most select observations, was 4° 11' 10&quot; Taurus, lat. 2° 30' S ^37. Thus we twice have Mars in the most opportune position, in quadrature with the sun, while the positions of earth and Mars are also distant by a quadrant.\n
</code></pre>
<p>I want to result be like this :</p>
<pre><code>[
Finally, on 1595 July 22 at 2h 40m am, when the sun was at 7° 59' 52&quot; Leo, 101,487 distant from earth, Mars's mean longitude 11s 14° 9' 5&quot;, and anomaly 164° 48' 55&quot;, and consequent eccentric position from the vicarious hypothesis 17° 16' 36&quot; Pisces: the apparent position of Mars, from the most select observations, was 4° 11' 10&quot; Taurus, lat. 2° 30' S ^37. ,

  Thus we twice have Mars in the most opportune position, in quadrature with the sun, while the positions of earth and Mars are also distant by a quadrant.\n ]
</code></pre>
<p>It means two sentences, the first one should finish after lat. 2° 30' S ^37. but since lat. has a dote , it breaks the sentences after lat.</p>
<p>but I did not find the solution till now I have used these 2 approaches:</p>
<pre><code>def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text in (&quot;lat.&quot;):
            # print(&quot;Detected:&quot;, token.text)
            doc[token.i].is_sent_start = False
    return doc

nlp.add_pipe(set_custom_boundaries, before=&quot;parser&quot;)
nlp.pipeline
</code></pre>
<p>and</p>
<pre><code>a.split('.')
</code></pre>
<p>I  think some small mistakes in the first code.</p>
<p>both above methods do not work to split the sentences as desired!</p>
<p>generally, what do you recommend in order to segment paragraph to sentences? (especially when we have) such abbreviation cases lie</p>
<pre><code>lat. 
</code></pre>
",Named Entity Recognition (NER),ner spacy sentence segmentation want break sentence order process using spacy want result like mean two sentence first one finish lat since lat ha dote break sentence lat find solution till used approach think small mistake first code method work split sentence desired generally recommend order segment paragraph sentence especially abbreviation case lie
About training data for spaCy NER,"<p>I'm new to NLP.
Now I'm trying to create NER model for extracting music artist's name from some text.
But It hasn't gone well.This is what I've done.</p>
<ol>
<li><p>I got 1500,000 artist's name list.</p>
</li>
<li><p>I created training data with string template.like this &quot;{artist's name} is so sick.&quot;
All 1500,000 training data is like this string.</p>
</li>
</ol>
<pre><code>TRAIN_DATA = [
    (&quot;Nirvana is so sick&quot;, {&quot;entities&quot;: [(0, 7, &quot;ARTIST&quot;)]}),
    (&quot;City girls is so sick&quot;, {&quot;entities&quot;: [(0, 10, &quot;ARTIST&quot;)]}),
    (&quot;Taylor swift is so sick&quot;, {&quot;entities&quot;: [(0, 12, &quot;ARTIST&quot;)]}),
]
</code></pre>
<p>(Maybe this is the reason it doesn't go well?)</p>
<ol start=""3"">
<li><p>I used the model after training 30,000 datas.</p>
</li>
<li><p>But I didn't work at all.All sentence was extracted as ARTIST.
Below is example. 'Chris Thomas King' is artist's name in this case.</p>
</li>
</ol>
<pre><code>Entities [('Not sure how they handled it during filming, but Tim Blake Nelson did sing his own parts (as did Chris Thomas King).', 'ARTIST')]
</code></pre>
<p>Do you have any idea?
Thanks in advance.</p>
",Named Entity Recognition (NER),training data spacy ner new nlp trying create ner model extracting music artist name text gone well done got artist name list created training data string template like artist name sick training data like string maybe reason go well used model training data work sentence wa extracted artist example chris thomas king artist name case idea thanks advance
Extracting Product Attribute/Features from text,"<p>I've been assigned a task to extract features/attributes from product description. </p>

<pre><code>Levi Strauss slim fit jeans
Big shopping bag in pink and gold
</code></pre>

<p>I need to be able to extract out attributes such as ""Jeans"" and ""slim fit"" or ""shopping bag"" and ""pink"" and ""gold"".
The product description listings are not just for clothes, they can basically be anything.</p>

<p>I am not sure how to approach this problem. I tried implementing a Named Entity Recognizer solution and also a POS implementations, The NER implementation fails to recognize any token and most of the tokens show up as NNP(Proper Nouns) in he POS solution, which doesn't help me out a lot. I need a way to be able to distinguish between the brand name and the features of the Product(like if it is a t-shirt, the color or design(round neck, v-neck) etc). </p>

<p>I did implement a KMean solution which did cluster like products together, but then again it is not the result I am looking for.</p>

<p>Just looking for someone to direct me in the correct direction.</p>
",Named Entity Recognition (NER),extracting product attribute feature text assigned task extract feature attribute product description need able extract attribute jean slim fit shopping bag pink gold product description listing clothes basically anything sure approach problem tried implementing named entity recognizer solution also po implementation ner implementation fails recognize token token show nnp proper noun po solution help lot need way able distinguish brand name feature product like shirt color design round neck v neck etc implement kmean solution cluster like product together result looking looking someone direct correct direction
Training Data Format with Spacy,"<p>I am trying to build NLP with Spacy, but I am having trouble formatting the training data. I want my app to be able to recognize entities and intents. For example, in &quot;I want to place an order for pizza&quot;. The intent would be &quot;place_order&quot; and the entity would be pizza. How do I format the training data for BOTH entities and intents in Spacy?</p>
",Named Entity Recognition (NER),training data format spacy trying build nlp spacy trouble formatting training data want app able recognize entity intent example want place order pizza intent would place order entity would pizza format training data entity intent spacy
Extracting groups of unstructured text to for later NLP?,"<p>I am new to data mining / text mining so I am not sure that I am using the right terminology. I am attempting to come up with a process to extract groups of related content to later apply NLP and other techniques to extract the meaningful data out of it. I have starting data that looks something like this:</p>

<pre><code>Product Name - $-25- 15
Product Name - $3

Product Bundle $100
-Product 1
-Product 2 Condition
-Product 3 Condition

Product - Version - Condition $100

Product
Extras
Extras

More Info
$20 

Product
Extras
Condition
$15

Product (Condition) 50
Product (Condition) 25
Product (Condition) 10
</code></pre>

<p>The goal is to obtain a list like this with a unique entry for each ""listing"", grouped with relevant meta data:</p>

<pre><code>[Product Name - $-25- 15], [Product Name - $3], [Product Bundle $100 -Product 1 -Product 2 Condition -Product 3 Condition], [Product - Version - Condition $100] 
</code></pre>

<p>The full text is written by many different authors and often switches formats within a single post so I can't detect what format it is in and process the whole document. The one thing all formats have in common is that they have new line breaks rather than a dense paragraph of text. So working with that I have a few ideas on how to approach it:</p>

<p><strong>Option 1: Rudimentary</strong></p>

<ol>
<li>Split the document into an array by new lines (\n)</li>
<li>If there is an extra empty space between entries then group the previous ones</li>
<li>If there is no extra space, detect if there is a price, if so consider it's own group</li>
</ol>

<p>This option is very simple and could work when double spaced. However it fails by using a number as a heuristic to determine if it's a new group as the product names, extras, conditions could contain numbers when single spaced.</p>

<p><strong>Option 2: NLP</strong></p>

<p>This option would attempt to classify each word in the document as Product Name, Condition, Attribute, Price. Then process the document again to group text so that it has a Name and Price and optionally the condition and extra meta data.</p>

<p>The problem with this approach is that the extras and bundles are products as well so classifying them would determine that they are a unique entry with meta data when they belong under a ""parent"" product because of how they are spaced in the document. </p>

<p><strong>Option 3: Something else?</strong></p>

<p>My first thought was to process the document into groups first so that when NLP knows all words in this group are related to the same product. I have a list of all Product Names and a pretty good one of all Conditions. The extras, versions, and other text is unique so it may cause some issues attempting to determine how to group.</p>

<p>It seems like it might need a mix of the two because how the author spaces them is ultimately how everything is bundled. Yet we don't immediately know if the next set of content is related to the first or a new listing without some other process.</p>

<p><strong>INPUT</strong></p>

<pre><code>Mario Party - $10

Party Games Bundle $100
-Super Mario Bros
-Mario World - NEW

Donkey Kong - 2017 Version - Used $10

Wii Sports
Includes Controllers

Also includes memory card
$10 

Grand Theft Auto
San Andreas
Includes poster
Used
$10

Zelda (Unopened box) 10
</code></pre>

<p><strong>OUTPUT (JSON)</strong></p>

<pre><code>{ listings: [
    { name: 'Mario Party', condition: null, version: null, currency: '$', price: 10, includes: null },
    { name: 'Party Games Bundle', condition: null, version: null, currency: '$', price: 100, includes: ['Super Mario Bros', 'Mario World - NEW'] },
    { name: 'Donkey Kong', condition: 'Used', version: '2017 Version', currency: '$', price: 10, includes: null },
    { name: 'Wii Sports', condition: null, version: null, currency: '$', price: 10, includes: ['Includes Controllers', 'Also includes memory card'] },
    { name: 'Grand Theft Auto', condition: 'Used', version: 'San Andreas', currency: '$', price: 10, includes: 'Includes poster' },
    { name: 'Zelda', condition: 'Unopened box', version: null, currency: '$', price: 10, includes: null }
] }
</code></pre>
",Named Entity Recognition (NER),extracting group unstructured text later nlp new data mining text mining sure using right terminology attempting come process extract group related content later apply nlp technique extract meaningful data starting data look something like goal obtain list like unique entry listing grouped relevant meta data full text written many different author often switch format within single post detect format process whole document one thing format common new line break rather dense paragraph text working idea approach option rudimentary split document array new line n extra empty space entry group previous one extra space detect price consider group option simple could work double spaced however fails using number heuristic determine new group product name extra condition could contain number single spaced option nlp option would attempt classify word document product name condition attribute price process document group text ha name price optionally condition extra meta data problem approach extra bundle product well classifying would determine unique entry meta data belong parent product spaced document option something else first thought wa process document group first nlp know word group related product list product name pretty good one condition extra version text unique may cause issue attempting determine group seems like might need mix two author space ultimately everything bundled yet immediately know next set content related first new listing without process input output json
Extracting Related Date and Location from a sentence,"<p>I'm working with written text (paragraphs of articles and books) that includes both locations and dates. I want to extract from the texts pairs that contain locations and dates that are associated with one another. For example, given the following phrase:</p>

<p><strong>The man left Amsterdam on January and reached Nepal on October 21st</strong></p>

<p>I would have an output such as this:</p>

<p><code>&gt;&gt;&gt;[(Amsterdam, January), (Nepal, October 21st)]</code></p>

<p>I tried splitting the text through ""connecting words"" (such as ""and"" for example) and work on part as follows: find words that indicate a location (""at"", ""in"", ""from"",""to"" etc.) and words that indicate a date or time (""on"", ""during"" etc.), and join what you find. However, this proved to be problematic, as there are too much words that indicate location and date, and sometimes the basic ""find"" method cannot distinguish between them.</p>

<p>Assume that I am able to identify a date as such, and given a word that starts with a capital letter, I am able to determine if it is a location or not. The main issue is connecting between them, and making sure they are.</p>

<p>I figured that tools like <strong>ntlk</strong> and <strong>scapy</strong> will assist me here, but there isn't enough documentation to help me find an exact solution to this kind of problem.</p>

<p>Any help would be appreciated!</p>
",Named Entity Recognition (NER),extracting related date location sentence working written text paragraph article book includes location date want extract text pair contain location date associated one another example given following phrase man left amsterdam january reached nepal october st would output tried splitting text connecting word example work part follows find word indicate location etc word indicate date time etc join find however proved problematic much word indicate location date sometimes basic find method distinguish assume able identify date given word start capital letter able determine location main issue connecting making sure figured tool like ntlk scapy assist enough documentation help find exact solution kind problem help would appreciated
How to apply Named Entity Recognition function to all columns and return column names that meets criteria,"<p>I have the following code I am using to identify if a column type is &quot;GPE&quot; or not, which means that a field contains the name of a Geo-Political Entity.</p>
<pre><code>import spacy
import pandas as pd
import en_core_web_sm
nlp = en_core_web_sm.load()

text = [[&quot;Canada&quot;, 'University of California has great research', &quot;non-location&quot;],[&quot;China&quot;, 'MIT is at Boston', &quot;non-location&quot;]]
df = pd.DataFrame(text, columns = ['text', 'text2', 'text3'])
df['new_col'] = df['text2'].apply(lambda x: [[w.label_] for w in list(nlp(x).ents)])
df
</code></pre>
<p>However, it can only be applied to one column at a time and I am wondering how could I modify it so that it could be applied to all the columns within the data frame and then only return the name of columns that contain more than one &quot;GPE&quot; as the data type.</p>
<p>Desired output (Showing the column 'text' because both rows are GPE) :</p>
<pre><code>text
</code></pre>
",Named Entity Recognition (NER),apply named entity recognition function column return column name meet criterion following code using identify column type gpe mean field contains name geo political entity however applied one column time wondering could modify could applied column within data frame return name column contain one gpe data type desired output showing column text row gpe
How to improve word mover distance similarity in python and provide similarity score using weighted sentence,"<p>Word movers Distance can be used to identify similarity between text .
This similarity can be used to compare multiple text for finding nearest similar text.
However , I was unable to customise the algorithm to do the following 
1)eliminate location (GPE) - identified by spacy , in the text  to have any weightage in comparing similarity .
2)Give more weightage to features that are in first sentence of text rather than features in second sentence and second sentence over third and so on .</p>

<pre><code>instance = WmdSimilarity(wmd_corpus, loaded_model, num_best=10)
start = time()
sent = 'Abc hotel serves best in class drunken prawn in north america . ABC Hotel has branches in London, New York, Chicago and San Francisco.'
query = preprocess(sent)

sims = instance[query]  # A query is simply a ""look-up"" in the similarity class.

print('Cell took %.2f seconds to run.' % (time() - start))

print('Query:')
print(sent)
for i in range(num_best):
    print()
    print('sim = %.4f' % sims[i][1])
    print(documents[sims[i][0]])
</code></pre>

<p>In this particular example , where hotel description is passed for WMD similarity ,
The results identify descriptions such as </p>

<p>-DEF is a restaurant in Chicago serving vegan food since 1969 . 
-JKL now serving in London, New York, Chicago and San Francisco 
- Bestsellers of the hotel include drunken prawn , lasagne etc . (MNO Hotel)</p>

<p>Expected result 
Only MNO hotel from the above result is relevant accoring to the food aspect . </p>

<p>Query :
How to eliminate the other hotel which are mapped due to location ?</p>
",Named Entity Recognition (NER),improve word mover distance similarity python provide similarity score using weighted sentence word mover distance used identify similarity text similarity used compare multiple text finding nearest similar text however wa unable customise algorithm following eliminate location gpe identified spacy text weightage comparing similarity give weightage feature first sentence text rather feature second sentence second sentence third particular example hotel description passed wmd similarity result identify description def restaurant chicago serving vegan food since jkl serving london new york chicago san francisco bestseller hotel include drunken prawn lasagne etc mno hotel expected result mno hotel result relevant accoring food aspect query eliminate hotel mapped due location
Is it possible to subclass a spacy entity type?,"<p>I'd like to subclass the existing GPE, so that it differentiates between GPE-Nation, USA, and GPE-City, New York. I see in the docs how to create new entity types, but not how to subclass what's already there. Can this be done, and if so, how? Thanks.</p>
",Named Entity Recognition (NER),possible subclass spacy entity type like subclass existing gpe differentiates gpe nation usa gpe city new york see doc create new entity type subclass already done thanks
Training SpaCy NER with a custom dataset,"<p>I have followed <a href=""https://spacy.io/usage/training#ner"" rel=""nofollow noreferrer"">this</a> SpaCy tutorial for training a custom dataset. My dataset is a gazetteer. Therefore, I made my training data as the following.</p>
<pre><code>TRAIN_DATA = [
(&quot;Where is Abbess&quot;,{&quot;entities&quot;:[(9, 15,&quot;GPE&quot;)]}),
(&quot;Where is Abbey Pass&quot;,{&quot;entities&quot;:[(9, 19,&quot;LOC&quot;)]}),
(&quot;Where is Abbot&quot;,{&quot;entities&quot;:[(9, 14,&quot;GPE&quot;)]}),
(&quot;Where is Abners Head&quot;,{&quot;entities&quot;:[(9, 29,&quot;LOC&quot;)]}),
(&quot;Where is Acheron Flat&quot;,{&quot;entities&quot;:[(9, 21,&quot;LOC&quot;)]}),
(&quot;Where is Acheron River&quot;,{&quot;entities&quot;:[(9, 22,&quot;LOC&quot;)]})
]
</code></pre>
<p>I used <code>'en_core_web_sm'</code> for the training, not a blank model.</p>
<pre><code>model = 'en_core_web_sm'
output_dir=Path(path)
n_iter=20
</code></pre>
<p>After training for 20 epocs, I tried to make a prediction with the trained model. The following is the output that I get.</p>
<pre><code>test_text = &quot;Seven people, including teenagers, have been taken to hospital after their car crashed in the mid-Canterbury town of Rakaia.&quot;

Seven people, including teenagers 0 33 GPE
the mid-Canterbury town of Rakaia.. 90 125 GPE
</code></pre>
<p>I did a prediction using <code>'en_core_web_sm'</code> for the same test_text. The output is the following.</p>
<pre><code>Seven 0 5 CARDINAL
mid-Canterbury 94 108 DATE
Rakaia 117 123 GPE
</code></pre>
<p>Can someone please instruct me on the errors that I am making while training SpaCy?</p>
",Named Entity Recognition (NER),training spacy ner custom dataset followed spacy tutorial training custom dataset dataset gazetteer therefore made training data following used training blank model training epocs tried make prediction trained model following output get prediction using test text output following someone please instruct error making training spacy
How to assign an ID to a entity in a Google AuoML natural language entity extraction model?,"<p>How to give assign an ID to the entities added into AutoML Natural Language entity recognition model. For example, I Have added an entity ""Chelsea"" under ""Sports"" Label. How will I assign ""Chelsea"" an ID, so that whenever an article with an entity ""Chelsea"" comes in, it gets auto-tagged to a database?</p>
",Named Entity Recognition (NER),assign id entity google auoml natural language entity extraction model give assign id entity added automl natural language entity recognition model example added entity chelsea sport label assign chelsea id whenever article entity chelsea come get auto tagged database
How to understand LSTM performance generated on a graph?,"<p>I used LSTM by applying Keras for the task of Named Entity Recognition. The model present around 95% accuracy. Then I tried to create the graph for accuracy and loss. Now I do know, what do these two graphs present?</p>

<p><a href=""https://i.sstatic.net/wctKJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wctKJ.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/K7AAd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/K7AAd.png"" alt=""enter image description here""></a></p>
",Named Entity Recognition (NER),understand lstm performance generated graph used lstm applying kera task named entity recognition model present around accuracy tried create graph accuracy loss know two graph present
How can I prioritize Rule Based Matching over trained NER Model in Spacy?,"<p>I am building a Named Entity Recognition model for biomedical text (cancer papers from Pubmed). I trained a custom NER model using spacy for 3 entities (DISEASE, GENE, and DRUG) types. Further, I combined the model with <a href=""https://spacy.io/usage/rule-based-matching#models-rules"" rel=""nofollow noreferrer"">rule based components to improve the accuracy of my model</a>.</p>

<p>Here is my current code - </p>

<pre class=""lang-py prettyprint-override""><code>
# Loaded the trained NER Model
nlp = spacy.load(""my_spacy_model"")

# Define entity patterns for EntityRuler (just showing 2 relevant patterns here, it contains more patterns)
patterns = [{""label"": ""GENE"", ""pattern"": ""BRCA1""},
            {""label"": ""GENE"", ""pattern"": ""BRCA2""}]

ruler = EntityRuler(nlp)

ruler.add_patterns(patterns)

nlp.add_pipe(ruler)
</code></pre>

<p>When I test the above code on the following piece of text - </p>

<pre><code>text = ""Exceptional response to olaparib in BRCA2-altered breast cancer after PD-L1 inhibitor and chemotherapy failure""
</code></pre>

<p>I get the following result - </p>

<pre><code>DISEASE  BRCA2-altered breast cancer
DRUG  olaparib
GENE PD-L1
</code></pre>

<p>However, the correct answer is - </p>

<pre><code>GENE BRCA2
^^^^^^^^^^^
DISEASE breast cancer
^^^^^^^^^^^^^^^^^^^^^
DRUG  olaparib
GENE PD-L1
</code></pre>

<p>The model is not recognizing <code>BRCA2</code> as a gene, which I have added in the patterns for <code>EntitytRuler</code>.</p>

<p>Is there a way to prioritize predictions from rule-based matching over the trained model? Alternatively, is there something else I can do to get the correct results by combining rule-based matching?</p>
",Named Entity Recognition (NER),prioritize rule based matching trained ner model spacy building named entity recognition model biomedical text cancer paper pubmed trained custom ner model using spacy entity disease gene drug type combined model rule based component improve accuracy model current code test code following piece text get following result however correct answer model recognizing gene added pattern way prioritize prediction rule based matching trained model alternatively something else get correct result combining rule based matching
Distinguish name of a person vs of an organization,"<p>Given the display name from the From: header of an email, I'd like to determine whether that represents the name of a person or something else.  Something else might often be the name of an organization, or perhaps a title/role.</p>

<p>Some examples:</p>

<ul>
<li>Alice Smith => name of a person</li>
<li>Google Inc. => not the name of a person (it's an organization)</li>
<li>Amazon Customer Support => not the name of a person (it's a title/role)</li>
<li>Director of Engineering => not the name of a person (it's a title/role)</li>
</ul>

<p>How can I recognize whether the sender's name represents the name of an individual?</p>

<hr>

<p>One possible approach is to use machine learning and train a classifier.  But if I do that, what features should I use?  I've been reading the literature and haven't found anyone who has tackled exactly this issue, though I've seen some features that were used for related problems:</p>

<ul>
<li><p>A phrase of the form ""Word Word"" or ""Word, Word"" or ""Word, Word Letter"" or ""Word Letter Word"" or ""Word, Word Letter."" or ""Word Letter. Word"" are more likely to be a name (think ""First Last"" and ""Last, First"" and ""Last, First MI"" and ""First MI Last"").</p></li>
<li><p>A phrase that starts with a title (Mr., Mrs., Miss, Ms., Dr., Prof., Lt., Sgt., etc.) is more likely to be a name.  A phrase that ends with a common name-suffix (Jr., Sr., Phd, Esq) is more likely to be a name.  A phrase that ends with a company indicator (Inc., Corp., Corporation) is likely not a name (it's probably an organization).</p></li>
<li><p>It's possible to download a <a href=""https://stackoverflow.com/q/1803628/781723"">list</a> of the most common first names and last names from the US Census.  If the phrase contains one of those strings, it's more likely to be a name.  Or, one can use an existing lexicon, e.g., the BaLIE or Oak lexicons.</p></li>
<li><p>If the phrase contains a word found in the dictionary, it's more likely to be not a name (e.g., an organization/title/role).</p></li>
<li><p>If a word in the name matches a word in the domain name of the corresponding email address, it's more likely to be not a name of a person (e.g., Paypal Customer Service ).</p></li>
<li><p>If a word contains an internal apostrophe or hyphen, it's more likely to be a name of a person (e.g., O'Connor, Jean-Claude).</p></li>
</ul>

<p>But I don't know what will actually work.  What features should I use?  Alternatively, are there any pre-trained classifiers or any experience reports that describes what works?</p>

<p>I've read a bit about named entity recognition, but that seems like a harder problem.</p>
",Named Entity Recognition (NER),distinguish name person v organization given display name header email like determine whether represents name person something else something else might often name organization perhaps title role example alice smith name person google inc name person organization amazon customer support name person title role director engineering name person title role recognize whether sender name represents name individual one possible approach use machine learning train classifier feature use reading literature found anyone ha tackled exactly issue though seen feature used related problem phrase form word word word word word word letter word letter word word word letter word letter word likely name think first last last first last first mi first mi last phrase start title mr mr miss dr prof lt sgt etc likely name phrase end common name suffix jr sr phd esq likely name phrase end company indicator inc corp corporation likely name probably organization possible download href common first name last name u census phrase contains one string likely name one use existing lexicon e g balie oak lexicon p phrase contains word found dictionary likely name e g organization title role word name match word domain name corresponding email address likely name person e g paypal customer service word contains internal apostrophe hyphen likely name person e g connor jean claude know actually work feature use alternatively pre trained classifier experience report describes work read bit named entity recognition seems like harder problem
How can I extract GPE(location) using NLTK ne_chunk?,"<p>I am trying to implement a code to check for the weather condition of a particular area using OpenWeatherMap API and NLTK to find entity name recognition. But I am not able to find the method of passing the entity present in GPE(that gives the location), in this case, Chicago, to my API request. Kindly help me with the syntax.The code to given below. </p>

<p>Thank you for your assistance</p>

<pre><code>import nltk
from nltk import load_parser
import requests
import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords

sentence = ""What is the weather in Chicago today? ""
tokens = word_tokenize(sentence)

stop_words = set(stopwords.words('english'))

clean_tokens = [w for w in tokens if not w in stop_words]

tagged = nltk.pos_tag(clean_tokens)

print(nltk.ne_chunk(tagged))
</code></pre>
",Named Entity Recognition (NER),extract gpe location using nltk ne chunk trying implement code check weather condition particular area using openweathermap api nltk find entity name recognition able find method passing entity present gpe give location case chicago api request kindly help syntax code given thank assistance
NLTK Named Entity recognition to a Python list,"<p>I used NLTK's <code>ne_chunk</code> to extract named entities from a text:</p>

<pre><code>my_sent = ""WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.""


nltk.ne_chunk(my_sent, binary=True)
</code></pre>

<p>But I can't figure out how to save these entities to a list? E.g. –</p>

<pre><code>print Entity_list
('WASHINGTON', 'New York', 'Loretta', 'Brooklyn', 'African')
</code></pre>

<p>Thanks.</p>
",Named Entity Recognition (NER),nltk named entity recognition python list used nltk extract named entity text figure save entity list e g thanks
How to extract only a specific value from a python sublist that I got as an API response from Monkeylearn,"<p>I have been training a text classification model in Monkeylearn and as a response to my API query, I get a python list as a result. I want to extract only the specific text classification value from it. Attaching the code below.</p>

<pre><code>ml = MonkeyLearn('42b2344587')
data = reddittext[2]    # dataset in a python list
model_id = 'cl7C'
result = ml.classifiers.classify(model_id, data)
print(result.body)   #response from API in list format
</code></pre>

<p>Output I get is :</p>

<pre><code>[{'text': 'comment\n', 'external_id': None, 'error': False, 'classifications': []},
 {'text': 'So this is the worst series of Kohli like in years.\n', 'external_id': None, 'error': False, 'classifications': []}, 
 {'text': 'Saini ODI average at 53 😂\n', 'external_id': None, 'error': False, 'classifications': [{'tag_name': 'Batting', 'tag_id': 122983950, 'confidence': 0.64}]}]
</code></pre>

<p>I want to only print the classifications - tag_name ie ""Batting"" from this list.</p>

<pre><code>type(result.body)
</code></pre>

<p>the output I get is: List</p>
",Named Entity Recognition (NER),extract specific value python sublist got api response monkeylearn training text classification model monkeylearn response api query get python list result want extract specific text classification value attaching code output get want print classification tag name ie batting list output get list
How to extract artist&#39;s name from plain text?,"<p>I'm new to NLP.
I want to extract music artist's name from plain text like that is posted on social media.</p>

<p>The text looks like this. (this is just sample, not real)</p>

<blockquote>
  <p>Today bandcamp is waiving fees again! CHANGE, TAYLOR SWIFT and POP
  SMOKE will be using all funds collected through bandcamp to donate to
  Anti Repression Committee. No Justice No Peace.</p>
</blockquote>

<p>This time,I want to extract string ""CHANGE"",""TAYLOR SWIFT"",""POP SMOKE"".
I already tried NLTK and spaCy but it didn't work as desired.</p>

<p>Is there any other idea how I can achieve this?</p>

<p>Thanks in advance.</p>
",Named Entity Recognition (NER),extract artist name plain text new nlp want extract music artist name plain text like posted social medium text look like sample real today bandcamp waiving fee change taylor swift pop smoke using fund collected bandcamp donate anti repression committee peace time want extract string change taylor swift pop smoke already tried nltk spacy work desired idea achieve thanks advance
Rasa multiple lookups in same intent,"<p>I have been trying out Rasa NLU for a hotel booking chatbot. I am trying to extract two features, room type and number of rooms. 
Here is my nlu.md file </p>

<pre><code>## intent:greet
- hey
- hello
- hi
- good morning
- good evening
- hey there

## intent:goodbye
- bye
- goodbye
- see you around
- see you later

## intent:affirm
- yes
- indeed
- of course
- that sounds good
- correct

## intent:deny
- no
- never
- I don't think so
- don't like that
- no way
- not really

## intent: book_room
- i would like to book a room
- i want a room
- i want to book room
- i would like a room
- i want to have a room
- i want to book
- i would like to book


## intent: book_n_rooms
-  [num_rooms] rooms
-  I would like to book [num_rooms]
-  I want [num_rooms] rooms
-  I need [num_rooms] rooms
-  I want to have [num_rooms] rooms

## intent: room_type
-  I would like a [room_type_name] room
-  Well, I want [room_type_name]
-  I need [room_type_name] rooms
-  I prefer [room_type_name] rooms

## book_room_all_details
-  I would like a [room_type_name] [num_rooms]
-  I would like [num_rooms] of [room_type_name]
-  I want [num_rooms] of [room_type_name]
-  I prefer [room_type_name] [num_rooms]
-  I like [num_rooms] of [room_type_name]
-  book [num_rooms] [room_type_name] rooms
-  I would like to book [num_rooms] [room_type_name]
-  I would like to book [2](num_rooms) [deluxe](room_type_name) rooms
-  I would like to book [3](num_rooms) [deluxe](room_type_name) type rooms
-  I want [1](num_rooms) [simple](room_type_name) room
-  I want [1](num_rooms) [simple](room_type_name) type room
-  I want [2](num_rooms) [simple](room_type_name) rooms
-  book [3](num_rooms) [simple](room_type_name) type rooms
-  book [3](num_rooms) [simple](room_type_name) rooms
-  I would like [5](num_rooms) rooms [deluxe](room_type_name) 
-  I want [5](num_rooms) rooms [deluxe](room_type_name) 
-  I prefer [5](num_rooms) rooms [deluxe](room_type_name) 
-  I prefer [5](num_rooms) rooms [deluxe](room_type_name) 
## regex:num_rooms
- [0-9]+

## lookup:room_type_name
- Simple
- Deluxe
- simple
- deluxe
- SIMPLE
- DELUXE

## intent:bot_challenge
- are you a bot?
- are you a human?
- am I talking to a bot?
- am I talking to a human?
</code></pre>

<p>When I give the input ""book 2 deluxe rooms""
The bot asks me how many rooms are needed.
The intent is wrongly classified.
I tried this answer <a href=""https://stackoverflow.com/questions/59550980/rasa-nlu-multiple-entity-extraction-from-single-intent"">RASA NLU: Multiple entity extraction from Single intent</a>, but this has not been fruitful.
Please help.</p>
",Named Entity Recognition (NER),rasa multiple lookup intent trying rasa nlu hotel booking chatbot trying extract two feature room type number room nlu md file give input book deluxe room bot asks many room needed intent wrongly classified tried answer href nlu multiple entity extraction single intent ha fruitful please help
Check if the lines in the dataframe roughly correspond to each other,"<p>I have a data frame with names of cities in Morocco and another one with similar names but that was not well coded. Here's the first one:</p>

<pre><code>&gt;&gt;&gt; df[['new_regiononame']].head()

    new_regiononame
0   Grand Casablanca-Settat
1   Fès-Meknès
2   Souss-Massa
3   Laayoune-Sakia El Hamra
4   Fès-Meknès
</code></pre>

<p>and here's the other one I wanted to change to the names of the first one. At least they know a way to read it correctly:</p>

<pre><code>&gt;&gt;&gt;X_train[['S02Q03A_Region']].head()

    S02Q03A_Region
10918   FÃ¨s-MeknÃ¨s
1892    Rabat-SalÃ©-KÃ©nitra
6671    Casablanca-Settat
4837    Marrakech-Safi
6767    Casablanca-Settat
</code></pre>

<p>How can I check if the lines in the dataframe roughly correspond to each other and, if so, rename <code>X_train</code> rows by <code>df</code> ones?</p>

<p>So far I only know how to extract which rows in <code>X_train</code> have exact equivalents in <code>df</code>:</p>

<pre><code>X_train['S02Q03A_Region'][X_train['S02Q03A_Region'].isin(df['new_regiononame'].unique())]
</code></pre>
",Named Entity Recognition (NER),check line dataframe roughly correspond data frame name city morocco another one similar name wa well coded first one one wanted change name first one least know way read correctly check line dataframe roughly correspond rename row one far know extract row exact equivalent
Differences between using Lex and Alexa,"<p>I'm building an Alexa skill that will allow Alexa users to interact with a consumer facing e-commerce site.  There is functionality to call a representative that already exists on the site.  Now, I want to build out a voice app as a side project that extends that same option via a conversation.  There will be a need for slots like location, category of call, etc.  It's basically an Application/Transactional bot.</p>

<p>In the future, if this is successful, I'd like that same general app to be accessible on different IoT devices (like Google Home Assistant, etc.)  Therefore, I'd like to abstract out the voice interactions and have the same (general) flow and API to interact with.</p>

<p>This leaves me doing some research on different technologies like api.ai, wit.ai, Lex, etc.</p>

<p>But, since this is an app for Alexa and I already rely on AWS and Amazon in general, I think I'd prefer to use Lex or just write a native Alexa app for now.</p>

<p>I'm having a hard time understanding the differences between the two.  I understand that Alexa was built using Lex and I see that they have similar concepts like intent, slots, etc.</p>

<p>But, I'm looking for any differences between the two services:</p>

<ol>
<li><p>Would using Lex allow me to more easily integrate with other devices?  Or is there any benefit?</p></li>
<li><p>Would using Lex allow me greater flexibility in designing/modifying the flow of a conversation?  It seems like Lex is a little more complex and, therefore, might allow greater functionality.  </p></li>
<li><p>Or is it just that Lex offers nearly the exact same functionality and is just meant for devices that aren't Alexa?</p></li>
<li><p>Does Lex offer any more analytics processing than Alexa?  In Alexa I can only see intents/slots, but if I could see the actual text in Lex, that would be ideal.</p></li>
</ol>
",Named Entity Recognition (NER),difference using lex alexa building alexa skill allow alexa user interact consumer facing e commerce site functionality call representative already exists site want build voice app side project extends option via conversation need slot like location category call etc basically application transactional bot future successful like general app accessible different iot device like google home assistant etc therefore like abstract voice interaction general flow api interact leaf research different technology like api ai wit ai lex etc since app alexa already rely aws amazon general think prefer use lex write native alexa app hard time understanding difference two understand alexa wa built using lex see similar concept like intent slot etc looking difference two service would using lex allow easily integrate device benefit would using lex allow greater flexibility designing modifying flow conversation seems like lex little complex therefore might allow greater functionality lex offer nearly exact functionality meant device alexa doe lex offer analytics processing alexa alexa see intent slot could see actual text lex would ideal
SpaCy Custom NER Model: Dependency Parser Training Error,"<p>I was trying to build a custom NER model using spacy. After building the model for entities, it was necessary to train the model for dependency parsers. 
I tried following the sample code provided on the Spacy website given below: <a href=""https://spacy.io/usage/training#tagger-parser"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#tagger-parser</a></p>

<p>The sample code for the training data given the SpaCy website is:</p>

<pre><code>TRAIN_DATA = [
(
    ""They trade mortgage-backed securities."",
    {
        ""heads"": [1, 1, 4, 4, 5, 1, 1],
        ""deps"": [""nsubj"", ""ROOT"", ""compound"", ""punct"", ""nmod"", ""dobj"", ""punct""],
    },
)]
</code></pre>

<p>In this sample code, for the training data, there is a label called <strong>“heads”</strong>. I am not very particular on what it exactly is and <strong>what is its significance in the code.</strong></p>

<p>I tried to run the model without the ""heads"" label in the training data.  A sample of the training data is:</p>

<pre><code>TRAIN_PARSER = ('Mr Manjunath who is in-charge of the motor at their Goa location.', {'deps': ['compound',    'ROOT',    'nsubj',    'relcl',    'prep',    'punct',    'pobj',    'prep',    'det',    'pobj',    'prep',    'poss', 'compound','pobj', 'punct']})
</code></pre>

<p>When I try to run the model without the heads label given below:</p>

<pre><code>from __future__ import unicode_literals, print_function

import plac
import random
from pathlib import Path
import spacy
from spacy.util import minibatch, compounding


# training data
TRAIN_DATA = TRAIN_PARSER


@plac.annotations(
model=(""Model name. Defaults to blank 'en' model."", ""option"", ""m"", str),
output_dir=(""Optional output directory"", ""option"", ""o"", Path),
n_iter=(""Number of training iterations"", ""option"", ""n"", int),
)
def main(model='model1', output_dir='model2', n_iter=74):
""""""Load the model, set up the pipeline and train the parser.""""""
if model is not None:
    nlp = spacy.load(model)  # load existing spaCy model
    print(""Loaded model '%s'"" % model)
else:
    nlp = spacy.blank(""en"")  # create blank Language class
    print(""Created blank 'en' model"")

# add the parser to the pipeline if it doesn't exist
# nlp.create_pipe works for built-ins that are registered with spaCy
if ""parser"" not in nlp.pipe_names:
    parser = nlp.create_pipe(""parser"")
    nlp.add_pipe(parser, first=True)
# otherwise, get it, so we can add labels to it
else:
    parser = nlp.get_pipe(""parser"")

# add labels to the parser
for _, annotations in TRAIN_DATA:
    for dep in annotations.get('deps', []):
        parser.add_label(dep)

# get names of other pipes to disable them during training
pipe_exceptions = [""parser"", ""trf_wordpiecer"", ""trf_tok2vec""]
other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]
with nlp.disable_pipes(*other_pipes):  # only train parser
    optimizer = nlp.begin_training()
    for itn in range(n_iter):
        random.shuffle(TRAIN_DATA)
        losses = {}
        # batch up the examples using spaCy's minibatch
        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            texts, annotations = zip(*batch)
            nlp.update(texts, annotations, sgd=optimizer, losses=losses)
        print(""Losses"", losses)

# test the trained model
test_text = ""I like securities.""
doc = nlp(test_text)
print(""Dependencies"", [(t.text, t.dep_, t.head.text) for t in doc])

# save model to output directory
if output_dir is not None:
    output_dir = Path(output_dir)
    if not output_dir.exists():
        output_dir.mkdir()
    nlp.to_disk(output_dir)
    print(""Saved model to"", output_dir)

    # test the saved model
    print(""Loading from"", output_dir)
    nlp2 = spacy.load(output_dir)
    doc = nlp2(test_text)
    print(""Dependencies"", [(t.text, t.dep_, t.head.text) for t in doc])

    main(model='model1', output_dir='model2', n_iter=74)
</code></pre>

<p>I get the following error:</p>

<pre><code>IndexError: list index out of range
</code></pre>

<p>Can someone please explain it to me, what the exact problem is over here and how can I solve it? Also, how can I generate the ""heads"" label for my training data?</p>
",Named Entity Recognition (NER),spacy custom ner model dependency parser training error wa trying build custom ner model using spacy building model entity wa necessary train model dependency parser tried following sample code provided spacy website given sample code training data given spacy website sample code training data label called head particular exactly significance code tried run model without head label training data sample training data try run model without head label given get following error someone please explain exact problem solve also generate head label training data
Data preprocessing for Named Entity Recognition?,"<p>I'm working on a Named Entity Recognition on resume dataset and we have entities like dates, phone, email etc.,,</p>

<p>And I'm working how to preprocess those entities. I'm currently adding a space after each puncuation like this,</p>

<pre><code>DAVID B-Name
John I-Name
, O
IT O

Washington B-Address
, I-Address
DC I-Address
( B-Phone
107 I-Phone
) I-Phone
155
- I-Phone
4838 I-Phone
david B-Email
. I-Email
John I-Email
@ I-Email
gmail I-Email
. I-Email
com I-Email
</code></pre>

<p>But I'm starting to question the process on how to handle such text during inference. I'm assuming even at inference we have to preprocess text using same process that is adding a space after each puncuation isn't it?</p>

<p>But it won't be so readable right? </p>

<p>For example at inference I have to provide input text like <code>test @ example . com?</code> which is not readable isn't it? It only be able to predict entities in such format.</p>
",Named Entity Recognition (NER),data preprocessing named entity recognition working named entity recognition resume dataset entity like date phone email etc working preprocess entity currently adding space puncuation like starting question process handle text inference assuming even inference preprocess text using process adding space puncuation readable right example inference provide input text like readable able predict entity format
Mapping entities between two disparate company datasets,"<p>I have several datasets containing data about companies: 
- entity_structure (columns: entity_id, parent_entity_id, ultimate_parent_id)
- entity_addresses (columns: address_id, entity_id, location_city, state, postal_code, zip, street, ...)
- vendor (columns: vendor_id, parent_vendor_id, top_vendor_id, cnt_children, orgtype_id, geo_id, name, email, ...)
- geo (columns: geo_id, zipcode, is_primary, latitude, longtitude, elevation, state, ...)
- entity_coverage (entity_id, name, proper_name, sic_code, industry_code, sector_code, iso, ...)</p>

<p>I need to automatically map entities between the datasets, for example, there may be a company named ""Google"" in one data set, and a company named ""Google 123"" in another. I need to be able to determine with a high enough confidence that those are the same entities. In most cases, the data does not share a unique key. In most cases, the data does not share a unique key.</p>

<p>Would named entity linking be the best approach here? Are there any Python examples on how to approach this problem? </p>
",Named Entity Recognition (NER),mapping entity two disparate company datasets several datasets containing data company entity structure column entity id parent entity id ultimate parent id entity address column address id entity id location city state postal code zip street vendor column vendor id parent vendor id top vendor id cnt child orgtype id geo id name email geo column geo id zipcode primary latitude longtitude elevation state entity coverage entity id name proper name sic code industry code sector code iso need automatically map entity datasets example may company named google one data set company named google another need able determine high enough confidence entity case data doe share unique key case data doe share unique key would named entity linking best approach python example approach problem
How to add explanation/description for a newly defined label in SpaCy&#39;s NER?,"<p>I'm creating a new label called GADGET to identify gadgets like Apple iPhone, Samsung TV etc. How do I add a custom description for the new label ?</p>

<p>For example, if label='ORG' &amp; we give spacy.explain(label), it gives a description for ORG. How can I add similarly for a new label?</p>
",Named Entity Recognition (NER),add explanation description newly defined label spacy ner creating new label called gadget identify gadget like apple iphone samsung tv etc add custom description new label example label org give spacy explain label give description org add similarly new label
"Create tuples of (lemma, NER type) in python , Nlp problem","<p>I wrote the code below, and I made a dictionary for it, but I want Create tuples of (lemma, NER type) and Collect counts over the tuples I dont know how to do it? can you pls help me? NER type means name entity recognition</p>

<pre><code>text = """"""
Seville.
Summers in the flamboyant Andalucían capital often nudge 40C, but spring is a delight, with the parks in bloom and the scent of orange blossom and jasmine in the air. And in Semana Santa (Holy Week, 14-20 April) the streets come alive with floats and processions. There is also the raucous annual Feria de Abril – a week-long fiesta of parades, flamenco and partying long into the night (4-11 May; expect higher hotel prices if you visit then).
Seville is a romantic and energetic place, with sights aplenty, from the Unesco-listed cathedral – the largest Gothic cathedral in the world – to the beautiful Alcázar royal palace. But days here are best spent simply wandering the medieval streets of Santa Cruz and along the river to La Real Maestranza, Spain’s most spectacular bullring.
Seville is the birthplace of tapas and perfect for a foodie break – join a tapas tour (try devoursevillefoodtours.com), or stop at the countless bars for a glass of sherry with local jamón ibérico (check out Bar Las Teresas in Santa Cruz or historic Casa Morales in Constitución). Great food markets include the Feria, the oldest, and the wooden, futuristic-looking Metropol Parasol.
Nightlife is, unsurprisingly, late and lively. For flamenco, try one of the peñas, or flamenco social clubs – Torres Macarena on C/Torrijano, perhaps – with bars open across town until the early hours.
Book it: In an atmospheric 18th-century house, the Hospes Casa del Rey de Baeza is a lovely place to stay in lively Santa Cruz. Doubles from £133 room only, hospes.com
Trieste.
""""""

doc = nlp(text).ents
en = [(entity.text, entity.label_) for entity in doc]
en
#entities
#The list stored in variable entities is has type list[list[tuple[str, str]]],
#from pprint import pprint
pprint(en)

sum(filter(None, entities), [])

from collections import defaultdict
type2entities = defaultdict(list)
for entity, entity_type in sum(filter(None, entities), []):
  type2entities[entity_type].append(entity)

from pprint import pprint
pprint(type2entities)


</code></pre>
",Named Entity Recognition (NER),create tuples lemma ner type python nlp problem wrote code made dictionary want create tuples lemma ner type collect count tuples dont know pls help ner type mean name entity recognition
Cleaning Mixed Geographic Data (R),"<p>I have a very ugly column in a dataset that contains a mix of States and Cities (domestic and international). The rest of the data is all numbers and nothing correlating to anything geographic. Is there any method to do a text-analysis to determine what is what with the end goal of making columns to separate states and cities and have a 3rd column to show country? </p>

<pre><code>   c(""Arizona"", ""(not set)"", ""Arizona"", ""(not set)"", ""California"", 
""California"", ""New York"", ""Texas"", ""New York"", ""Texas"", ""England"", 
""Illinois"", ""Florida"", ""Maharashtra"", ""Massachusetts"", ""Virginia"", 
""Maryland"", ""Florida"", ""Karnataka"", ""Pennsylvania"", ""Arizona"", 
""New Jersey"", ""Illinois"", ""District of Columbia"", ""Delhi"", ""Ohio"", 
""Ontario"", ""Georgia"", ""Colorado"", ""Washington"", ""Michigan"", ""Virginia"", 
""North Carolina"", ""England"", ""Maryland"", ""Pennsylvania"", ""Colorado"", 
""Utah"", ""Arizona"", ""New Jersey"", ""District of Columbia"", ""Tamil Nadu"", 
""North Carolina"", ""Arizona"", ""Massachusetts"", ""Tokyo"", ""Andhra Pradesh"", 
""Minnesota"", ""Washington"", ""Tainan City"", ""Michigan"", ""Arizona"", 
""Maharashtra"", ""Federal District"", ""Ile-de-France"", ""Utah"", ""Georgia"", 
""Metro Manila"", ""Ontario"", ""Connecticut"")
</code></pre>

<p><a href=""https://i.sstatic.net/zM6qm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zM6qm.png"" alt=""enter image description here""></a></p>
",Named Entity Recognition (NER),cleaning mixed geographic data r ugly column dataset contains mix state city domestic international rest data number nothing correlating anything geographic method text analysis determine end goal making column separate state city rd column show country
Stanford CoreNLP Train custom NER model,"<p>I was making some tests by training custom models with crf, and since i don't have a proper training file i would like to make by myself a list of 5 tags and maybe 10 words only to start with and the plan is to keep improving the model with more incoming data in the future. but the results i get are plenty of false positives (it tags many words which have nothing to do with the original one in the training file) i imagine since the models created are probabilistic and take into considerarion more than just separate words</p>

<p>Let's say i want to train corenlp to detect a small list of words without caring about the context are there some special settings for that? if not, is there a way to calculate how much data is needed to get an accurate model?</p>
",Named Entity Recognition (NER),stanford corenlp train custom ner model wa making test training custom model crf since proper training file would like make list tag maybe word start plan keep improving model incoming data future result get plenty false positive tag many word nothing original one training file imagine since model created probabilistic take considerarion separate word let say want train corenlp detect small list word without caring context special setting way calculate much data needed get accurate model
How to extract relevant phrases from sentences regarding a particular topic using Neural networks?,"<p>I have training data as two columns
1.'Sentences'
2.'Relevant_text' (text in this column is a subset of text in the column 'Sentences')</p>

<p>I tried training a RNN with LSTM directly treating 'Sentences' as input and 'Relevant_text' and output but the results were disappointing.</p>

<p>I want to know how to approach this type of problem? Does this kind of problem have a name? Which models should I explore?</p>
",Named Entity Recognition (NER),extract relevant phrase sentence regarding particular topic using neural network training data two column sentence relevant text text column subset text column sentence tried training rnn lstm directly treating sentence input relevant text output result disappointing want know approach type problem doe kind problem name model explore
Creating training data set for named entity recognition for job titles,"<p>I want to recognise job titles from texts. How can I create a larger training data set by extending my small training data set?
Do some ready package or open projects for extend training set exist?</p>
",Named Entity Recognition (NER),creating training data set named entity recognition job title want recognise job title text create larger training data set extending small training data set ready package open project extend training set exist
Extract complete table from PDF using tabula in python,"<p>I have a PDF with the table in the below format, column names and data are separated by ""--------""</p>

<pre><code>col1 col2 col3 col4 col5 col6 col7 col8 col9 col10 col11 col12 col13
----------------------------------------------------------------------
B    ABC1      F1  SSSSSS 1   32WE 161A1 1     A   DU23   162.00 85
C    ABC2      F2  DDDDDD 1   WE32 161B1 1     B   DU20   162.00 86
C    ABC3      F3  FFFFFF 1   DF45 161C1 1     C   DU20   162.00 87
</code></pre>

<p><strong>current code:</strong></p>

<pre><code>import tabula
df = tabula.read_pdf(""example.pdf"", pages='all')
</code></pre>

<p>df is list of dataframes of all tables in the pdf</p>

<p>I was able to extract the table content using tabula but maybe because of the table format it ignores the column names and shows the first row of the table as column names.
How can I get the column names?
Also col3 is empty, tabula ignores this column completely.
How can i extract the complete table with column names including empty columns</p>

<p>I am not sure if this would work, but if I remove the ""-----------"" from the table, I believe tabula would be able to read the table correctly.But, I am not sure how to delete ""------------"" from pdf. I am trying to extract data from pdf using pypdf2 but not able to change the content.</p>

<p>Code :</p>

<pre><code>import PyPDF2
from PyPDF2 import PdfFileWriter, PdfFileReader

# creating a pdf file object 
pdfFileObj = open('example.pdf', 'rb') 

# creating a pdf reader object 
pdfReader = PyPDF2.PdfFileReader(pdfFileObj) 

# printing number of pages in pdf file 
i=0
pageData=''
for i in range(0,pdfReader.getNumPages()):
    # creating a page object 
    pageObj = pdfReader.getPage(i) 
    # extracting text from page 
    pageData = pageObj.extractText()
    print(pageData)

    #Modify PDF here
    #remove ""-----"" from the extracted text
    # rewrite modified text back to pdf

    i = i+1

# closing the pdf file object 
pdfFileObj.close() 
</code></pre>
",Named Entity Recognition (NER),extract complete table pdf using tabula python pdf table format column name data separated current code df list dataframes table pdf wa able extract table content using tabula maybe table format ignores column name show first row table column name get column name also col empty tabula ignores column completely extract complete table column name including empty column sure would work remove table believe tabula would able read table correctly sure delete pdf trying extract data pdf using pypdf able change content code
Extract time interval entities from sentence,"<p>I am trying to extract 3 pieces of information from a data set of a few thousand sentences in python. Specifically, for each sentence I would like to pull 1) the time interval (""interval""), 2) what the time interval is relative to (""relative_to""), 3) the name of the item or action the time interval is referring to (""item""). </p>

<p>The use case is that I am trying to create something to tag these 3 items, so that I can use this is as training data to teach an NER model to do it well.</p>

<p>Here are 2 examples of sentences and the 3 pieces of info I would want from them: </p>

<p>Example 1:</p>

<pre><code>sentence = ""Not more than five (5) days following the Opening of Escrow, Seller covenants and agrees to conduct a review of its files and deliver to Buyer copies of any pertinent and material Due Diligence Materials (as defined in Section 13.2.4) in Seller's possession relating to the Property.""

// expected extractions
interval = ""five (5) days following""
relative_to = ""the Opening of Escrow""
item = ""Due Diligence Materials""
</code></pre>

<p>Example 2:</p>

<pre><code>sentence = ""The foregoing representations and warranties shall survive Closing for a period of one hundred eighty (180) days""

# expected extractions
interval = ""one hundred eighty (180) days""
relative_to = ""Closing""
item = ""representations and warranties""
</code></pre>

<p>I can fairly reliably extract the interval using regex rules to look for words around digits and units.  However, I am having more difficulty extract the ""relative_to"" and ""item"" entities.</p>

<p>Previously on stack overflow I have seen posts like these that use a trained NLP model to tag the parts of speech, dependencies and named entities in the data to inform how once can extract them:
 - <a href=""https://stackoverflow.com/questions/32002386/extracting-triplets-from-a-sentence-in-python"">Extracting triplets from a sentence in Python</a>
 - <a href=""https://stackoverflow.com/questions/41208346/extract-entities-from-multiple-subject-passive-sentence-by-spacy?rq=1"">Extract entities from Multiple Subject passive sentence by Spacy</a>
 - <a href=""https://stackoverflow.com/questions/61258030/extract-name-entities-and-its-corresponding-numerical-values-from-sentence"">extract name entities and its corresponding numerical values from sentence</a></p>

<p>However, the tagging using spacy seem imprecise for locating the information I want from these sentences. </p>

<p>Is this still the approach that folks would recommend for my problem? Are there any other approaches you have tried that would work?</p>

<p>Thank you for any ideas you can share.</p>
",Named Entity Recognition (NER),extract time interval entity sentence trying extract piece information data set thousand sentence python specifically sentence would like pull time interval interval time interval relative relative name item action time interval referring item use case trying create something tag item use training data teach ner model well example sentence piece info would want example example fairly reliably extract interval using regex rule look word around digit unit however difficulty extract relative item entity previously stack overflow seen post like use trained nlp model tag part speech dependency named entity data inform extract href name entity corresponding numerical value sentence however tagging using spacy seem locating information want sentence still approach folk would recommend problem approach tried would work thank idea share
How to extract chemical entities with ChemDataExtractor?,"<p>I'm trying to process a text for extracting chemical entities through ChemDataExtractor (Python). A possible example is </p>

<pre><code>from chemdataextractor import Document
doc = Document('UV-vis spectrum of 5,10,15,20-Tetra(4-carboxyphenyl)porphyrin in Tetrahydrofuran (THF).')
</code></pre>

<p>and typing <code>doc.cems</code> the result is the following list</p>

<pre><code>[Span('THF', 82, 85),
 Span('5,10,15,20-Tetra(4-carboxyphenyl)porphyrin', 19, 61),
 Span('Tetrahydrofuran', 65, 80)]
</code></pre>

<p>I would like to extract only <code>'THF'</code>, <code>'5,10,15,20-Tetra(4-carboxyphenyl)porphyrin'</code>, <code>'Tetrahydrofuran'</code>, without having the ""span elements"". How can I do it?</p>
",Named Entity Recognition (NER),extract chemical entity chemdataextractor trying process text extracting chemical entity chemdataextractor python possible example typing result following list would like extract without span element
How to find all Wikipedia pages related to a named entity?,"<p>Given a text, I am looking to find links to all Wikipedia pages related to named entities mentioned in the text. Is there a reliable way to do this?  </p>

<p>For example, consider the text,</p>

<blockquote>
  <p>Mark Elliot Zuckerberg is an American internet entrepreneur and
  philanthropist.</p>
</blockquote>

<p>"" Given this, I am looking at output with the following links:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Mark_Zuckerberg"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Mark_Zuckerberg</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Americans"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Americans</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Internet"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Internet</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Entrepreneurship"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Entrepreneurship</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Philanthropy"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Philanthropy</a></li>
</ul>

<p>Is this possible at all given the current state of NLP? 
Many thanks!</p>
",Named Entity Recognition (NER),find wikipedia page related named entity given text looking find link wikipedia page related named entity mentioned text reliable way example consider text mark elliot zuckerberg american internet entrepreneur philanthropist given looking output following link possible given current state nlp many thanks
Prevent nested entity extraction in LUIS application,"<p>In a hypothetical ""Contacts"" LUIS.AI application with three list entities</p>

<pre><code>ContactType: [Phone, Email]
Country: [UK, Germany, US, Canada, Brazil, Venezuela]
Region: [Europe, North America, South America]
</code></pre>

<p>Where the Country entity ""US"" has synonyms ""America, American, USA"".</p>

<p>Is it possible to enforce that if an utterance contains the Region ""South America"", e.g. ""What are my phone contacts in South America"" that it should (correctly) match the South America ""Region"" entity, and not (incorrectly) include the US ""Country"" entity?</p>

<p>I am seeing utterances where the intent is being correctly identified, with the ContactType, and Region; but the Country is also being identified, leading to contextually incorrect responses from the business logic of the app.</p>

<p>In the utterance review of the intent, I see all three entities highlighted in the intent review, but can't figure out how to <em>remove</em> a labelled entity if LUIS decides that it is present.</p>

<p>If it is not possible for LUIS to learn rules such that an entity cannot contain a nested entity, is there a strategy for the business logic to identify that the entity is nested, and should be ignored, other than special casing?</p>

<p>Edit to provide more information:</p>

<p>The defined Entities:
<a href=""https://i.sstatic.net/UcOVG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UcOVG.png"" alt=""Contact Method entity definition""></a>
<a href=""https://i.sstatic.net/5csJ2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5csJ2.png"" alt=""Contact Type entity definition""></a>
<a href=""https://i.sstatic.net/O9kpw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/O9kpw.png"" alt=""Country entity definition""></a>
<a href=""https://i.sstatic.net/ir9Td.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ir9Td.png"" alt=""Region entity definition""></a></p>

<p>An example of a correctly processed utterance; The intent is correct, the contact method is phone numbers, the contact type is family, and the country is US.</p>

<p><a href=""https://i.sstatic.net/LaEJo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LaEJo.png"" alt=""Correctly processed utterance""></a></p>

<p>Here is an incorrectly processed utterance;  The intent is correct, the contact method and type are correct, but ""South America"" is incorrectly producing two entitites, a Region (correct) of South America, but also the Country US, which is contextually incorrect.</p>

<p><a href=""https://i.sstatic.net/HvbiG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HvbiG.png"" alt=""Example of incorrectly processed utterance""></a></p>
",Named Entity Recognition (NER),prevent nested entity extraction luis application hypothetical contact luis ai application three list entity country entity u ha synonym america american usa possible enforce utterance contains region south america e g phone contact south america correctly match south america region entity incorrectly include u country entity seeing utterance intent correctly identified contacttype region country also identified leading contextually incorrect response business logic app utterance review intent see three entity highlighted intent review figure remove labelled entity luis decides present possible luis learn rule entity contain nested entity strategy business logic identify entity nested ignored special casing edit provide information defined entity example correctly processed utterance intent correct contact method phone number contact type family country u incorrectly processed utterance intent correct contact method type correct south america incorrectly producing two entitites region correct south america also country u contextually incorrect
How to get Named Entity Extraction using GATE Annie in Java?,"<p>I am newbie in <strong>GATE ANNIE</strong>. I tried <strong>GATE GUI interface</strong> and got experience to do task on it. I wanted to know how can I implement <strong>Named Entity Extraction</strong> in Java? </p>

<p>I did R&amp;D but unable to find any tutorial regarding <strong>Named Entity Extraction</strong>. </p>

<p>Is there any code available to find out <strong>Named Entity Extraction</strong> in <strong>GATE ANNIE</strong> in <strong>Java</strong>?</p>
",Named Entity Recognition (NER),get named entity extraction using gate annie java newbie gate annie tried gate gui interface got experience task wanted know implement named entity extraction java r unable find tutorial regarding named entity extraction code available find named entity extraction gate annie java
Custom entity recognition using Azure Text Analytics API?,"<p>Is is possible to define custom/special entities to be used for entity recognition within the Azure Text Analytics API?</p>

<p>NER (Names Entities Recognition) allows to discover a wide range of entities but for our purposes we're focusing on some model-specific entities (e.g. brand and product names) which we need to relate to the overall sentiment. General NER might not be enough for our purposes since we're looking for very specific appreciation/criticism terms during the topic generation.</p>

<p>The theme has already been presented in different flavors with no answers so far:</p>

<ul>
<li>in 2016 it seemed to be an ""upcoming"" feature: <a href=""https://stackoverflow.com/questions/39071771/customizing-the-named-entity-recogntition-model-in-azure-ml"">Customizing the Named Entity Recogntition model in Azure ML</a></li>
<li>in 2018 someone was searching for much more specialized version capable to physically locate custom entities spatials position within documents: <a href=""https://stackoverflow.com/questions/53876142/documentation-examples-for-custom-entity-detection-azure-nlp-text-analytics"">Documentation / Examples for Custom Entity Detection (Azure NLP / Text Analytics)</a></li>
</ul>
",Named Entity Recognition (NER),custom entity recognition using azure text analytics api possible define custom special entity used entity recognition within azure text analytics api ner name entity recognition allows discover wide range entity purpose focusing model specific entity e g brand product name need relate overall sentiment general ner might enough purpose since looking specific appreciation criticism term topic generation theme ha already presented different flavor answer far seemed upcoming feature
Problem adding custom entities to SpaCy&#39;s NER,"<ul>
<li>I added a new entity called ""orgName"" to en_core_web_lg using <a href=""https://spacy.io/usage/training#example-new-entity-type"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#example-new-entity-type</a></li>
<li>All my training data (26k sentences) have the ""orgName"" labeled in them.</li>
<li>To deal with the catastrophic forgetting problem, I ran en_core_web_lg on those 26k raw sentences and added the ORG, PROD, FAC, etc. entities as labels and not face the colliding entities, I created duplicates.
So, for a sentence A which was labeled by ""orgName"", I created a duplicate A2 which has ORG, PROD, FAC, etc. ending up with about 52k sentences.</li>
<li>I trained using 100 iterations.</li>
</ul>

<p>Now, the problem is that testing the model even on the training sentences, it's not showing the ORG, PROD, FAC, etc. but only showing ""orgName"".</p>

<p>Where do you think the problem is?</p>
",Named Entity Recognition (NER),problem adding custom entity spacy ner added new entity called orgname en core web lg using training data k sentence orgname labeled deal catastrophic forgetting problem ran en core web lg k raw sentence added org prod fac etc entity label face colliding entity created duplicate sentence wa labeled orgname created duplicate ha org prod fac etc ending k sentence trained using iteration problem testing model even training sentence showing org prod fac etc showing orgname think problem
List-based Named Entity Recognition for search engine: how to scale?,"<p>I'm working on a search engine for documents stored in Solr.</p>

<p><strong>In the user query, I want to detect Named Entitities</strong> (persons, organizations, cities...).</p>

<p>The example query is:</p>

<blockquote>
  <p>barack obama wife age</p>
</blockquote>

<p>In this query, I want to detect that ""barack obama"" is a person.</p>

<p>Since queries are not real phrases, it is difficult for classic NER (Spacy, Stanford NER...) to work properly. 
So, I adopted <strong>this approach</strong>: </p>

<ul>
<li>store in a dictionary all entities found in the documents (sorted by decreasing length)</li>
<li><p>loop the dictionary, to see if the user query contains entities</p>

<pre><code>def find_entities(query,entities_dict):

    entities=[]
    new_query=query.lower()

    for entity in entities_dict:
        if find_substring(entity,new_query):
            entities.append({entity:entities_dict[entity]})
            new_query = re.sub(r'\b{}\b'.format(entity), '', new_query)
    return(new_query,entities)
</code></pre></li>
</ul>

<p><strong>At the moment, I have about 200k entities in my Solr index</strong>: dictionary creation takes a few minutes; after the loading, this approach works well, is fast and not so memory consuming.</p>

<p><strong>In the near future, I will have 50-100 million entities</strong>.</p>

<p>I think that it will be impossible to store these entities in memory.</p>

<p><strong>How can I change my approach?</strong> 
I'm looking for advice for the <em>algorithm</em>, the <em>memory management</em> and <em>data structures</em> to be used.</p>
",Named Entity Recognition (NER),list based named entity recognition search engine scale working search engine document stored solr user query want detect named entitities person organization city example query barack obama wife age query want detect barack obama person since query real phrase difficult classic ner spacy stanford ner work properly adopted approach store dictionary entity found document sorted decreasing length loop dictionary see user query contains entity moment k entity solr index dictionary creation take minute loading approach work well fast memory consuming near future million entity think impossible store entity memory change approach looking advice algorithm memory management data structure used
How to clean up messy &quot;Country&quot; attribute from biopython pubmed extracts?,"<p>I have extracted ~60,000 PubMed abstracts into a data frame using <a href=""https://biopython.org/"" rel=""nofollow noreferrer"">Biopython</a>. The attributes include ""Authors"", ""Title"", ""Year"", ""Journal"", ""Country"", and ""Abstract"". 
The attribute ""Country"" is very messy, with a mixture of countries, cities, names, addresses, free-text items (e.g., ""freelance journalist with interest in Norwegian science""), faculties, etc.
I want to clean up the column only to contain the country - and ""NA"" for those records that are missing the entry, or have a free-text item that does not make sense.</p>

<p>Currently, my clean-up process of this column is very cumbersome:</p>

<pre><code>pub = df['Country']
chicago = pub.str.contains('Chicago')
df['Country'] = np.where(chicago, 'USA', pub.str.replace('-', ' '))
au = pub.str.contains('@edu.au')
df['Country'] = np.where(au, 'Australia', pub.str.replace('-', ' '))
... and so on
</code></pre>

<p>Are you aware of some python libraries, or have some ideas for a more automated way of cleaning up this column? </p>
",Named Entity Recognition (NER),clean messy country attribute biopython pubmed extract extracted pubmed abstract data frame using biopython attribute include author title year journal country abstract attribute country messy mixture country city name address free text item e g freelance journalist interest norwegian science faculty etc want clean column contain country na record missing entry free text item doe make sense currently clean process column cumbersome aware python library idea automated way cleaning column
Is there any way to improve Google NLP API result accuracy?,"<p>I am using Google-Vision API to get text from images and using its result into NLP API.
So far i intend to get Name,Location,address,email,contact number,job title,company name etc.
when i scan a business card.Results so far are not that much accurate as sometimes the results are too vague,also NLP API return multiple entries for the same content text <em>i.e Multiple value in names field,location field sometimes incorrect classifications too</em>.Any suggestions on how i can improve its results?</p>

<p><strong>Reference</strong> </p>

<ol>
<li><a href=""https://cloud.google.com/vision/docs/ocr"" rel=""nofollow noreferrer"">Google vision API</a> </li>
<li><p><a href=""https://cloud.google.com/natural-language/docs/analyzing-entities"" rel=""nofollow noreferrer"">Google language processing API</a> </p>

<p>Say for this business card  <a href=""https://i.sstatic.net/9CryP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9CryP.jpg"" alt=""enter image description here""></a></p>

<p>VISION API results into <a href=""https://i.sstatic.net/3zglA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3zglA.png"" alt=""enter image description here""></a></p>

<p>NLP results into <a href=""https://i.sstatic.net/A9mly.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/A9mly.png"" alt=""enter image description here""></a></p></li>
</ol>
",Named Entity Recognition (NER),way improve google nlp api result accuracy using google vision api get text image using result nlp api far intend get name location address email contact number job title company name etc scan business card result far much accurate sometimes result vague also nlp api return multiple entry content text e multiple value name field location field sometimes incorrect classification suggestion improve result reference google vision api google language processing api say business card vision api result nlp result
Mapping word tags/labels to their index,"<p>Here is a sample word/tag sentence pair sampled from a dataframe in NER</p>

<pre><code>B-per   Mr
B-per   .
I-per   Trump
O        ’
O        s
O       tweets
O       began
O       just
O       moments
O       after
O       a
B-org   Fox
I-org   News
O       report
O       by
B-per   Mike
I-per   Tobin
0       who 
0       never
0       arrived

this for example has index of 1 to 61, by manually counting down the columns from B-per to I-per 
</code></pre>

<p>now, I want to locate the start and end span for different sentences across a dataframe, which in this case is the first B-per to the last I-per. </p>

<p>Thus I feel like i might break it down into a series of steps</p>

<ol>
<li>locate the first person tag B-per</li>
<li>locate the last I-per</li>
<li>index the sentence and find the index of the first letter of the word B-per</li>
<li>find the index of the last letter of the word I-per.</li>
</ol>

<p>How do i accomplish this?</p>
",Named Entity Recognition (NER),mapping word tag label index sample word tag sentence pair sampled dataframe ner want locate start end span different sentence across dataframe case first b per last per thus feel like might break series step locate first person tag b per locate last per index sentence find index first letter word b per find index last letter word per accomplish
NER Dataset For Phone Number,"<p>I am trying to make NER dataset For phone numbers like '<strong>+44 (0)845 257 0422</strong>',</p>

<p>i have a tags of phone numbers detected by regex. when i am trying to sentence tokenizer and then word tokenizer, the phone number splits. so i am not able to match this phone with the tag 
anyone have idea to make a number as one word without splinting by nlp.</p>

<pre><code>def custom_tokenizer(nlp):
infix_re = re.compile(r'''[.\,\?\:\;\...\‘\’\`\“\”\""\'~]''')
prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)
suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)
return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,
                            suffix_search=suffix_re.search,
                            infix_finditer=infix_re.finditer,
                            token_match=None)
</code></pre>

<p>i tried this function so it only work for hyphen <strong>239-734-1608</strong> the numbe is not split.
i am looking for solution for this number <strong>+44 (0)845 257 0452</strong></p>
",Named Entity Recognition (NER),ner dataset phone number trying make ner dataset phone number like tag phone number detected regex trying sentence tokenizer word tokenizer phone number split able match phone tag anyone idea make number one word without splinting nlp tried function work hyphen numbe split looking solution number
Losses in NER training loop not decreasing in spacy,"<p>I am trying to train a new entity type 'HE INST'--to recognize colleges.
That is the only new label. I have a long document as raw text. I ran NER on it and saved the entities to the TRAIN DATA and then added the new entity labels to the TRAIN_DATA( i replaced in places where there was overlap).</p>

<p>The training loop is constant at a loss value(~4000 for all the 15 texts) and (~300) for a single data. Why does this happen, how do I train the model properly. I have around 18 texts with 40 annotated new entities.Even after all iterations, the model still doesn't predict the output correctly.</p>

<p>I haven't changed the script much. Just added en_core_web_lg, the new label and my TRAIN_DATA</p>

<p>I am trying to tag institutes from resume(C.V) data:</p>

<p>This would be one of my text in TRAIN_DATA: (soory for the long text)
I have around 18 such texts concantenated to form TRAIN_DATA</p>

<pre><code>[(""To perform better in my work each day. To increase my knowledge. To bring out my best by hardworking and improving my skills. To serve my parents and my family. To contribute my skills to my country. Marital ; Single Status Nationality \xe2\x80\x94: Indian Known . Parr . English, Malayalam, Hindi, Tamil Languages Hobby Playing cricket and football, Listening to music, Movies, Games. Father's ; V.N. Balappan Nair Name Mother's ; Saraswathy B Nair Name Believers Church Caarmel Engineering College R-Perunad Btech Electronics and communication engineering 6.09(Upto S6) 2015 - 2019 Marthoma Senior Secondary School Kozhencherry All India Senior School Certificate Examination 75% 2014 - 2015 Marthoma Senior Secondary School Kozhencherry Secondary School Examination 8.2 2012 - 2013 s@ INTERESTS Electronics, Sports s@ PERSONAL STRENGTHS Hardworking Loyal Good Team Spirit Good in mathematics ees IAA eM LANL NUL e (2 Problem Solving Skills rg DUS \\ TRAININGS completed the Vocational Industrial Training on Long Distance Communication Systems conducted by Southern Telecom Region, Bharat Sanchar Nigam Limited. Completed the internship training in Power Electronics Group(PEG), Tool Room, Fabrication Shop, Transform Winding, Electro Plating, Security And Surveillance Group(SSG), Special Products Group(SPG), Search And Rescue Beacon(SRB), Intelligent Tracking and Communication Project and Technology Development Center of Keltron Equipment Complex, Thiruvananthapuram. PROJECTS Final Year Project: Life Detection Using Quadcopter This project is useful at the time of natural calamities like flood earthquake etc... And can also be used in military applications as this device detects life signals using a PIR sensor and a thermal sensor. The components used in this are: PIR sensor, Thermal sensor, Arduino Nano, BEC, ESC, Quadcopter. Design project: Wireless Power Bank Wireless Power Bank enables us to charge our phone wordlessly. It can charge a device which is kept 10m(maximum) away from the adaptor without any obstacles in between. It uses the IR technology for power transmission. ACHIEVEMENTS &amp; AWARDS Participated in Pecardio Debugging Conducted as a part of NAKSHATRA 2019, The Annual National Level Techno Cultural Fest held at Saingits College of Engineering, kottayam. Volunteered in Alexa One day workshop on Artificial intelligence. Completed a period of two year tenue with a total of 240 hours in the National Service Scheme activities and has attended NSS Annual Special Camp. Participant in Cricket and football at the Annual Sports Meets. DECLARATION do here by confirm that the information given in this form is true to the best of my knowledge and belief."", {'entities': [(29, 37, 'DATE'), (210, 223, 'ORG'), (241, 247, 'NORP'), (256, 260, 'PERSON'), (263, 270, 'LANGUAGE'), (272, 281, 'PERSON'), (283, 288, 'PERSON'), (290, 295, 'NORP'), (362, 375, 'EVENT'), (388, 401, 'PERSON'), (402, 420, 'PERSON'), (423, 445, 'PERSON'), (446, 490, 'HE INST'), (563, 574, 'DATE'), (575, 620, 'ORG'), (625, 668, 'ORG'), (669, 672, 'PERCENT'), (673, 684, 'DATE'), (685, 717, 'ORG'), (764, 775, 'DATE'), (779, 800, 'ORG'), (890, 893, 'ORG'), (909, 910, 'CARDINAL'), (963, 997, 'ORG'), (1001, 1036, 'ORG'), (1050, 1073, 'ORG'), (1075, 1103, 'ORG'), (1142, 1169, 'ORG'), (1172, 1181, 'ORG'), (1183, 1199, 'ORG'), (1201, 1218, 'ORG'), (1220, 1235, 'ORG'), (1275, 1301, 'ORG'), (1304, 1332, 'ORG'), (1335, 1355, 'ORG'), (1360, 1415, 'ORG'), (1419, 1444, 'ORG'), (1446, 1464, 'LOC'), (1475, 1494, 'EVENT'), (1797, 1809, 'GPE'), (1811, 1814, 'GPE'), (1816, 1819, 'ORG'), (1821, 1831, 'ORG'), (1849, 1888, 'ORG'), (1969, 1980, 'CARDINAL'), (2050, 2052, 'ORG'), (2088, 2122, 'ORG'), (2126, 2154, 'ORG'), (2168, 2182, 'EVENT'), (2188, 2194, 'DATE'), (2239, 2270, 'HE INST'), (2297, 2302, 'GPE'), (2303, 2310, 'DATE'), (2358, 2369, 'DATE'), (2370, 2378, 'DATE'), (2401, 2410, 'TIME'), (2414, 2441, 'ORG'), (2470, 2493, 'ORG'), (2534, 2557, 'EVENT')]})]
</code></pre>

<p>The script is given below: (Note:- eval function is used to parse the TRAIN_DATA to list after reading it as string from text file-----you most probably know that but just in case)</p>

<pre><code>from __future__ import unicode_literals, print_function

import plac
import random
from pathlib import Path
import spacy
import en_core_web_lg
from spacy.util import minibatch, compounding


# new entity label
LABEL = ""HE INST""

with open('train_dump-backup.txt', 'r') as i_file:
    t_data = i_file.read()
TRAIN_DATA=eval(t_data)

@plac.annotations(
    model=(""en_core_web_lg"", ""option"", ""m"", str),
    new_model_name=(""NLP_INST"", ""option"", ""nm"", str),
    output_dir=(""/home/drbinu/Downloads/NLP_INST"", ""option"", ""o"", Path),
    n_iter=(""30"", ""option"", ""n"", int),
)

def main(model=None, new_model_name=""animal"", output_dir=None, n_iter=30):
    """"""Set up the pipeline and entity recognizer, and train the new entity.""""""
    random.seed(0)
    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank(""en"")  # create blank Language class
        print(""Created blank 'en' model"")
    # Add entity recognizer to model if it's not in the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if ""ner"" not in nlp.pipe_names:
        ner = nlp.create_pipe(""ner"")
        nlp.add_pipe(ner)
    # otherwise, get it, so we can add labels to it
    else:
        ner = nlp.get_pipe(""ner"")

    ner.add_label(LABEL)  # add new entity label to entity recognizer
    # Adding extraneous labels shouldn't mess anything up
    ner.add_label(""VEGETABLE"")
    if model is None:
        optimizer = nlp.begin_training()
    else:
        optimizer = nlp.resume_training()
    move_names = list(ner.move_names)
    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]
    with nlp.disable_pipes(*other_pipes):  # only train NER
        sizes = compounding(1.0, 4.0, 1.001)
        # batch up the examples using spaCy's minibatch
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            batches = minibatch(TRAIN_DATA, size=sizes)
            losses = {}
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)
            print(""Losses"", losses)

    # test the trained model
    test_text = ""B.Tech from Believers Church Caarmel Engineering College CGPA of 8.9""
    doc = nlp(test_text)
    print(""Entities in '%s'"" % test_text)
    for ent in doc.ents:
        print(ent.label_, ent.text)

    # save model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.meta[""name""] = new_model_name  # rename model
        nlp.to_disk(output_dir)
        print(""Saved model to"", output_dir)

        # test the saved model
        print(""Loading from"", output_dir)
        nlp2 = spacy.load(output_dir)
        # Check the classes have loaded back consistently
        assert nlp2.get_pipe(""ner"").move_names == move_names
        doc2 = nlp2(test_text)
        for ent in doc2.ents:
            print(ent.label_, ent.text)


if __name__ == ""__main__"":
    plac.call(main)
</code></pre>
",Named Entity Recognition (NER),loss ner training loop decreasing spacy trying train new entity type inst recognize college new label long document raw text ran ner saved entity train data added new entity label train data replaced place wa overlap training loop constant loss value text single data doe happen train model properly around text annotated new entity even iteration model still predict output correctly changed script much added en core web lg new label train data trying tag institute resume c v data would one text train data soory long text around text concantenated form train data script given note eval function used parse train data list reading string text file probably know case
Geo tersm in Luis synonyms generating,"<p>I'm facing this issue with geo places. For example.
USA, United states, United states of America, US.
Luis is able to detect them as built in entity of geographyV2
but I want NLP to pass all the similar terms of the user input. Ex user said US we send back geographyV2 entity and the similar synonyms of it , can luis do that by any chance? I'm desperate for this</p>
",Named Entity Recognition (NER),geo tersm luis synonym generating facing issue geo place example usa united state united state america u luis able detect built entity geographyv want nlp pas similar term user input ex user said u send back geographyv entity similar synonym luis chance desperate
Cleaning up text in an inconsistent format in Python,"<p>I'm trying to learn textual analysis in Python by analyzing Congressional testimony transcripts. I've been able to extract the text from the PDF transcripts, but the text is very dirty. I'm not sure how to clean it up. I'm trying to convert this:</p>

<pre><code>`[""HOUSE COMMITTEE ON BANKING, FINANCE AND URBAN AFFAIRSHENRY S. REUSS, Wisconsin, ChairmanTHOMAS'L. 
ASHLEY, OhioWILLIAM S. MOORHEAD, PennsylvaniaFERNAND J. ST GERMAIN, Rhode Island"", ""HENRY B. 
GONZALEZ, TexasPAUL NELSON, Clerk and Staff DirectorMICHAEL P. FLAHERTY, General CounselMERCER L. 
JACKSON, Minority Staff Director(H)']`
</code></pre>

<p>to a list of names, states, and title (ex: ""Chairman""), like this:</p>

<pre><code>`[[""HENRY S. REUSS"", ""Wisconsin"", ""Chairman""],
[""THOMAS'L. ASHLEY"", ""Ohio""],
[""WILLIAM S. MOORHEAD"", ""Pennsylvania""],
[""FERNAND J. ST GERMAIN"", ""Rhode Island""], 
[""HENRY B. GONZALEZ"", ""Texas""], 
[""PAUL NELSON"", ""Clerk and Staff Director""],
[""MICHAEL P. FLAHERTY"", ""General Counsel""],
[""MERCER L. JACKSON"", ""Minority Staff Director(H)""]]`
</code></pre>

<p>Note that some individuals have a state and title associated with their name, while some have just a state or title associated with their name. Is this type of conversion doable? Any recommendations on how to approach this? Any help will be greatly appreciated. Cheers!
-Mike</p>
",Named Entity Recognition (NER),cleaning text inconsistent format python trying learn textual analysis python analyzing congressional testimony transcript able extract text pdf transcript text dirty sure clean trying convert list name state title ex chairman like note individual state title associated name state title associated name type conversion doable recommendation approach help greatly appreciated cheer mike
"In R, how to do named entity recognition?","<p>I have looked at this guide <a href=""https://lincolnmullen.com/projects/dh-r2/nlp.html"" rel=""nofollow noreferrer"">https://lincolnmullen.com/projects/dh-r2/nlp.html</a> and this youtube video <a href=""https://www.youtube.com/watch?v=0lpQludiI-0"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=0lpQludiI-0</a> which both use the same method. When I try and replicate the results from the biography example in the first link, all I get as the output of </p>

<pre><code>entities(bio_doc, kind = ""person"")
</code></pre>

<p>is </p>

<pre><code>character(0)
</code></pre>

<p>This was the same result when I used a different input string as well. I saw this thread <a href=""https://stackoverflow.com/questions/58169707/efficient-named-entity-recognition-in-r"">Efficient Named Entity Recognition in R</a> and they got the same result as well (character(0)). I see that it went unanswered, so I was wondering if perhaps someone now has an answer to it? Has this method just been deprecated? I would very much appreciate anyone's insight.</p>
",Named Entity Recognition (NER),r named entity recognition looked guide youtube video use method try replicate result biography example first link get output wa result used different input string well saw thread href named entity recognition r got result well character see went unanswered wa wondering perhaps someone ha answer ha method deprecated would much appreciate anyone insight
Named Entity Recognition For Product Names Of Clothes,"<p>I am trying to extract product names from a plain text, the problem with product names is that they don't have a specific pattern and I don't want to give the algorithm a set of data that has fixed names I want it to be generic.</p>

<p>I'm looking for a way to make it detect the product names as an Entity.</p>

<p>Any help please?</p>

<p>Here's an example of the text </p>

<blockquote>
  <p>Order dispatched Your new clothes are on their way. Track your
  delivery with Royal Mail: VB 9593 7366 0GB</p>
  
  <p>Order Details</p>
  
  <p>Men's Dark Navy Jersey Cotton Lounge Shorts Size: XL</p>
  
  <p>£45.00</p>
  
  <p>Men's Navy Cotton Jersey Lounge Pants Size: XL</p>
  
  <p>£60.00</p>
  
  <p>Delivery £0.00</p>
  
  <p>Total £95.00</p>
</blockquote>

<p>I want to extract </p>

<blockquote>
  <p>Men's Navy Cotton Jersey Lounge 
  and 
  Men's Dark Navy Jersey Cotton Lounge Shorts</p>
</blockquote>

<p>Another example</p>

<blockquote>
  <p>Your order summary Delivery between 18/11/2019 and 19/11/2019 Shipping
  from O' adidas Lxcon sneakers £80.96 Delivery between 18/11/2019 and
  19/11/2019 Shipping from BOUTIQUE ANTONIA MARCELO BURLON COUNTY OF
  MILAN Confidencial striped swimsuit £97.58 Shipping Total Payment
  method £20.00 £153.90 VISA</p>
</blockquote>

<p>I want to extract</p>

<blockquote>
  <p>adidas Lxcon sneakers</p>
</blockquote>

<p>And</p>

<blockquote>
  <p>MARCELO BURLON COUNTY OF MILAN</p>
</blockquote>

<p>For your information this text is an email of orders and I have a lot of different patterns of emails.</p>
",Named Entity Recognition (NER),named entity recognition product name clothes trying extract product name plain text problem product name specific pattern want give algorithm set data ha fixed name want generic looking way make detect product name entity help please example text order dispatched new clothes way track delivery royal mail vb gb order detail men dark navy jersey cotton lounge short size xl men navy cotton jersey lounge pant size xl delivery total want extract men navy cotton jersey lounge men dark navy jersey cotton lounge short another example order summary delivery shipping adidas lxcon sneaker delivery shipping boutique antonia marcelo burlon county milan confidencial striped swimsuit shipping total payment method visa want extract adidas lxcon sneaker marcelo burlon county milan information text email order lot different pattern email
How to understand the patterns of section names in a resume?,"<h1>Python 2.6</h1>

<p>Recently I am doing some text mining works with resumes. The objective is to divide the resume into several sections based on its headings and contents and then classify it for required jds. Eg. We know that a resume generally contains these sections :</p>

<p>1) Personal Information</p>

<p>2) Summary</p>

<p>3) Technical Skills</p>

<p>4) Earlier Projects and Experience</p>

<p>5) Education.</p>

<p>Now all I want is to build a database where I have contents of resume under each category for all the resumes.</p>

<p>The structure is something like this :</p>

<pre><code>    Personal Information   Summary  Technical Skills    Experience/Projects Education
</code></pre>

<p>Resume 1 Relevant info Relevant info Relevant info Relevant info Relevant info</p>

<p>Resume 2 "" "" "" "" ""</p>

<p>Resume 3 "" "" "" "" ""</p>

<p>The relevant info shall be the contents under those specific sections in the resumes.</p>

<p>I have done some researches and finally my problem boils down to identify the section names. Idea is to Find where a section name begins and where the next section name begins , so that the text in this interval comes under the first section name. Here in lies the problem.</p>

<p>Problem: Suppose in resume 1 we have section names ""Technical skills"" and ""Experience"" . We take the data in between the two and put under the Technical skills column of Resume1. But when we look at Resume2 we find the same section names are named ""Expertise in Software"" and ""Earlier works &amp; Project Profiles"" and we cannot extract the names by the keywords which we used earlier. So, if I have to extract the sections for different cv Each time I have to do it by different section names for which I cannot generalize my code.</p>

<p>I have tried using dictionary of similar words i.e. the synonyms for words like ""Soft Skills"" are ""Technical Expertise , Software Expertise, Technical Knowledge etc. "" similar are ""Academics"" , ""Educational Qualification"",""Education"" , same for Experience , Projects and other sections too . But this list is not exhaustive there can be some other words for these section names , because people can write anything in their cv. Under one section too there can be subsections with different names.</p>

<p>Generally a section name ends with a colon or a semi-colon we can also find by that .</p>

<p>These are just the approaches but nothing concrete to build the database I want to. Now most of the resumes are in PDF format , so I convert them to text first and then read those. So the section names which are sometimes in a larger font or maybe different than the rest of the resume become the same font as the rest , so no way to identify them by these criteria.</p>

<p>These are the problems I am facing, if I can have a general algorithm to choose the section names then it would ease my work a lot. I know this is a forum for coding problems and it has been a great help to me since I have started my career, but I am posting this here if anyone can give me any insight on how to proceed. I am coding in Python , also any suggestion on some other languages like R or SAS would help. Mostly a general algorithm to choose the section names will do best for me. Please help if you have some idea.By tagging conditional random field. Thanks a in advance.</p>

<p>PS: I have already tried NER approach and converting all formats to html to extract headers but all efforts were fruitless...</p>
",Named Entity Recognition (NER),understand pattern section name resume python recently text mining work resume objective divide resume several section based heading content classify required jds eg know resume generally contains section personal information summary technical skill earlier project experience education want build database content resume category resume structure something like resume relevant info relevant info relevant info relevant info relevant info resume resume relevant info shall content specific section resume done research finally problem identify section name idea find section name begin next section name begin text interval come first section name lie problem problem suppose resume section name technical skill experience take data two put technical skill column resume look resume find section name named expertise software earlier work project profile extract name keywords used earlier extract section different cv time different section name generalize code tried using dictionary similar word e synonym word like soft skill technical expertise software expertise technical knowledge etc similar academic educational qualification education experience project section list exhaustive word section name people write anything cv one section subsection different name generally section name end colon semi colon also find approach nothing concrete build database want resume pdf format convert text first read section name sometimes larger font maybe different rest resume become font rest way identify criterion problem facing general algorithm choose section name would ease work lot know forum coding problem ha great help since started career posting anyone give insight proceed coding python also suggestion language like r sa would help mostly general algorithm choose section name best please help idea tagging conditional random field thanks advance p already tried ner approach converting format html extract header effort fruitless
Jupyter Kernel dies/Spyder console stops while training custom NER model in Spacy 2.0.11,"<p>I was trying to train a custom NER model in spacy. Initially I had installed the latest spacy version but was getting the following error during the training</p>

<blockquote>
  <p>ValueError: [E103] Trying to set conflicting doc.ents: A token can only be part of one entity, so make sure the entities you're setting don't overlap.</p>
</blockquote>

<p>After that I installed spacy version <code>spacy==2.0.11</code> and tried running my code. When I am having around 10 rows of data to train, the model is working fine and it's saving to my output directory. But when there is more data(5K rows) which is the original training data, my jupyter kernel dies or when I run in spyder, the console just exists!!</p>

<p>I understand that the deprecated version of <code>spacy</code> is not throwing the value error but still it's of no use as I am unable to train my model.</p>

<p>Sample data:</p>

<pre><code>CarryBag    09038820815c.txt
Stopperneedle   0903882080f4.txt
Foilbags    09038820819.txt
</code></pre>

<p>I have around 700 files like this with data to be tagged and in each file multiple entities need tagging.
Code for reference:</p>

<pre><code>import spacy
# import en_core_web_sm
import re
import csv
from spacy.matcher import PhraseMatcher
import plac
from pathlib import Path
import random

#Function to convert PhraseMatcher return value to string indexes 
def str_index_conversion(lbl, doc, matchitem):
    o_one = len(str(doc[0:matchitem[1]]))
    subdoc = doc[matchitem[1]:matchitem[2]]
    o_two = o_one + len(str(subdoc))
    return (o_one, o_two, lbl)

# nlp = spacy.load('en')
nlp = spacy.load('en_core_web_sm')

if 'ner' not in nlp.pipe_names:
    ner = nlp.create_pipe('ner')
    nlp.add_pipe(ner)
else:
    ner = nlp.get_pipe('ner')

ner.add_label('PRODUCT')     

DIR = 'D:/Docs/'
matcher = PhraseMatcher(nlp.vocab)


list_str_index = []
to_train_ents = []
with open(r'D:\ner_dummy_pack.csv', newline='', encoding ='utf-8') as myFile:

    reader = csv.reader(myFile)
    for row in reader:
        try:
            product = row[0].lower()
            #print('K---'+ product)
            filename = row[1]
            file = open(DIR+filename, ""r"", encoding ='utf-8')
            print(file)
            filecontents = file.read()
            for s in filecontents:
                filecontents = re.sub(r'\s+', ' ', filecontents)
                filecontents = re.sub(r'^https?:\/\/.*[\r\n]*', '', filecontents, flags=re.MULTILINE)
                filecontents = re.sub(r""http\S+"", """", filecontents)
                filecontents = re.sub(r""[-\""#/@;:&lt;&gt;?{}*`• ?+=~|$.!‘?“”?,_]"", "" "", filecontents)
                filecontents = re.sub(r'\d+', '', filecontents)#removing all numbers
                filecontents = re.sub(' +', ' ',filecontents)
                #filecontents = filecontents.encode().decode('unicode-escape')
                filecontents = ''.join([line.lower() for line in filecontents])
                if "","" in product:
                    product_patterns = product.split(',')
                    product_patterns = [i.strip() for i in product_patterns]

                    for elem in product_patterns:
                        matcher.add('PRODUCT', None, nlp(elem)) 

                else:
                    matcher.add('PRODUCT', None, nlp(product))                
                print(filecontents)
                doc = nlp(filecontents)
                matches = matcher(doc)
                        #print(matches)
                list_str_index = [str_index_conversion('PRODUCT', doc, x) for x in matches]
                to_train_ents.append((filecontents, dict(entities=list_str_index)))
                break


        except Exception as e:
            print(e)
            pass

to_train_entsfinal=to_train_ents      




def main(model=None, output_dir=None, n_iter=100):
    # nlp.vocab.vectors.name = 'spacy_pretrained_vectors'
    optimizer = nlp.begin_training()
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']

    with nlp.disable_pipes(*other_pipes):  # only train NER
        for itn in range(10):
            losses = {}
            random.shuffle(to_train_entsfinal)
            for item in to_train_entsfinal:
                nlp.update([item[0]],
                           [item[1]],
                           sgd=optimizer,
                           drop=0.50,
                           losses=losses)
            print(losses)
            print(""OUTTTTT"")


    if output_dir is None:
        output_dir = ""C:\\Users\\APRIL""


    noutput_dir = Path(output_dir)
    if not noutput_dir.exists():
        noutput_dir.mkdir()

    #nlp.meta['name'] = new_model_name
    nlp.to_disk(output_dir)


    random.shuffle(to_train_entsfinal)

if __name__ == '__main__':
    main()   
</code></pre>

<p>Can anyone help me solve this. Even when I removed conflicting entities in a sample of 10+ rows, example:</p>

<pre><code>Blister       abc.txt
Blisterpack   abc.txt
Blisters      abc.txt   
</code></pre>

<p>the same issue is happening and the model is not training</p>

<p>Suggested changes:</p>

<pre><code>def main(model=None, output_dir=None, n_iter=100):
    top_memory_precentage_use = 75 # or what ever number you choose

    def handle_memory(ruler):
        if psutil.virtual_memory().percent &lt; top_memory_precentage_use:
            dump_ruler_nonascii(ruler)
            ruler = nlp.begin_training() #or just init the nlp object again
        return ruler

    # This fitted for my use case
    def dump_ruler_nonascii(ruler):
        path = Path(os.path.join(self.data_path, 'config.jsonl'))
        pattern = ruler.patterns
        with open(path, ""a"", encoding=""utf-8"") as f:
            for line in pattern:
                f.write(json.dumps(line, ensure_ascii=False) + ""\n"")
        return ruler
    # nlp.vocab.vectors.name = 'spacy_pretrained_vectors'
    optimizer = nlp.begin_training()
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']

    with nlp.disable_pipes(*other_pipes):  # only train NER
        for itn in range(10):
            losses = {}
            random.shuffle(to_train_entsfinal)
            for item in to_train_entsfinal:
                nlp.update([item[0]],
                           [item[1]],
                           sgd=optimizer,
                           drop=0.50,
                           losses=losses)
            print(losses)
            print(""OUTTTTT"")


    if output_dir is None:
        output_dir = ""C:\\Users\\APRIL""


    noutput_dir = Path(output_dir)
    if not noutput_dir.exists():
        noutput_dir.mkdir()

    #nlp.meta['name'] = new_model_name
    nlp.to_disk(output_dir)


    random.shuffle(to_train_entsfinal)

if __name__ == '__main__':
    main()   
</code></pre>
",Named Entity Recognition (NER),jupyter kernel dy spyder console stop training custom ner model spacy wa trying train custom ner model spacy initially installed latest spacy version wa getting following error training valueerror e trying set conflicting doc ents token part one entity make sure entity setting overlap installed spacy version tried running code around row data train model working fine saving output directory data k row original training data jupyter kernel dy run spyder console exists understand deprecated version throwing value error still use unable train model sample data around file like data tagged file multiple entity need tagging code reference anyone help solve even removed conflicting entity sample row example issue happening model training suggested change
&quot;ImportError: cannot import name StanfordNERTagger&quot; in NLTK,"<p>I'm unable to import the NER Stanford Tagger in NLTK. This is what I have done:</p>

<p>Downloaded the java code from <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow"">here</a> 
and added an environment variable <code>STANFORD_MODELS</code> with the path to the folder where the java code is stored.</p>

<p>That should be sufficient according to the information that is provided on the NLTK site. It says: </p>

<p>""Tagger models need to be downloaded from <a href=""http://nlp.stanford.edu/software"" rel=""nofollow"">http://nlp.stanford.edu/software</a> and the STANFORD_MODELS environment variable set (a colon-separated list of paths).""</p>

<p>Would anybody be kind enough to help me, please?</p>

<p>EDIT: The downloaded folder is located at /Users/-----------/Documents/JavaJuno/stanford-ner-2015-04-20 and contains these files:</p>

<pre><code>LICENSE.txt         lib             ner.sh              stanford-ner-3.5.2-javadoc.jar
NERDemo.java            ner-gui.bat         sample-conll-file.txt       stanford-ner-3.5.2-sources.jar
README.txt          ner-gui.command         sample-w-time.txt       stanford-ner-3.5.2.jar
build.xml           ner-gui.sh          sample.ner.txt          stanford-ner.jar
classifiers         ner.bat             sample.txt
</code></pre>

<p>Then I have added an environment variable STANFORD_MODELS:</p>

<pre><code>os.environ[""STANFORD_MODELS""] = ""/Users/-----------/Documents/JavaJuno/stanford-ner-2015-04-20""
</code></pre>

<p>Calling from nltk.tag import StanfordNERTagger yields the error: </p>

<pre><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-356-f4287e573edc&gt; in &lt;module&gt;()
----&gt; 1 from nltk.tag import StanfordNERTagger

ImportError: cannot import name StanfordNERTagger
</code></pre>

<p>Also in case that this may be relevant, this is what is in my nltk.tag folder:</p>

<pre><code>__init__.py api.pyc     crf.py      hmm.pyc     senna.py    sequential.pyc  stanford.py tnt.pyc
__init__.pyc    brill.py    crf.pyc     hunpos.py   senna.pyc   simplify.py stanford.pyc    util.py
api.py      brill.pyc   hmm.py      hunpos.pyc  sequential.py   simplify.pyc    tnt.py      util.pyc
</code></pre>

<p>EDIT2: I have managed to import the NER Tagger, by using:</p>

<pre><code>from nltk.tag.stanford import NERTagger
</code></pre>

<p>but now when calling an example call from the NLTK website, I get:</p>

<pre><code>In [360]: st = NERTagger('english.all.3class.distsim.crf.ser.gz')
---------------------------------------------------------------------------
LookupError                               Traceback (most recent call last)
&lt;ipython-input-360-0c0ab770b0ff&gt; in &lt;module&gt;()
----&gt; 1 st = NERTagger('english.all.3class.distsim.crf.ser.gz')

/Library/Python/2.7/site-packages/nltk/tag/stanford.pyc in __init__(self, *args, **kwargs)
    158 
    159     def __init__(self, *args, **kwargs):
--&gt; 160         super(NERTagger, self).__init__(*args, **kwargs)
    161 
    162     @property

/Library/Python/2.7/site-packages/nltk/tag/stanford.pyc in __init__(self, path_to_model, path_to_jar, encoding, verbose, java_options)
     40                 self._JAR, path_to_jar,
     41                 searchpath=(), url=_stanford_url,
---&gt; 42                 verbose=verbose)
     43 
     44         self._stanford_model = find_file(path_to_model,

/Library/Python/2.7/site-packages/nltk/__init__.pyc in find_jar(name, path_to_jar, env_vars, searchpath, url, verbose)
    595                     (name, url))
    596     div = '='*75
--&gt; 597     raise LookupError('\n\n%s\n%s\n%s' % (div, msg, div))
    598 
    599 ##########################################################################

LookupError: 

===========================================================================
  NLTK was unable to find stanford-ner.jar! Set the CLASSPATH
  environment variable.

  For more information, on stanford-ner.jar, see:
    &lt;http://nlp.stanford.edu/software&gt;
===========================================================================
</code></pre>

<p>So I have incorrectly set the environment variable. Can anybody help me with that?</p>
",Named Entity Recognition (NER),importerror import name stanfordnertagger nltk unable import ner stanford tagger nltk done downloaded java code added environment variable path folder java code stored sufficient according information provided nltk site say tagger model need downloaded stanford model environment variable set colon separated list path would anybody kind enough help please edit downloaded folder located user document javajuno stanford ner contains file added environment variable stanford model calling nltk tag import stanfordnertagger yield error also case may relevant nltk tag folder edit managed import ner tagger using calling example call nltk website get incorrectly set environment variable anybody help
How to load BertforSequenceClassification models weights into BertforTokenClassification model?,"<p>Initially, I have a fine-tuned BERT base cased model using a text classification dataset and I have used BertforSequenceClassification class for this. </p>

<pre><code>from transformers import BertForSequenceClassification, AdamW, BertConfig

# Load BertForSequenceClassification, the pretrained BERT model with a single 
# linear classification layer on top. 
model = BertForSequenceClassification.from_pretrained(
    ""bert-base-uncased"", # Use the 12-layer BERT model, with an uncased vocab.
    num_labels = 2, # The number of output labels--2 for binary classification.
                    # You can increase this for multi-class tasks.   
    output_attentions = False, # Whether the model returns attentions weights.
    output_hidden_states = False, # Whether the model returns all hidden-states.
)
</code></pre>

<p>Now I want to use this fine-tuned BERT model weights for Named Entity Recognition and I have to use BertforTokenClassification class for this. I'm unable to figure out how to load the fine-tuned BERT model weights into the new model created using BertforTokenClassification.</p>

<p>Thanks in advance.......................</p>
",Named Entity Recognition (NER),load bertforsequenceclassification model weight bertfortokenclassification model initially fine tuned bert base cased model using text classification dataset used bertforsequenceclassification class want use fine tuned bert model weight named entity recognition use bertfortokenclassification class unable figure load fine tuned bert model weight new model created using bertfortokenclassification thanks advance
Named Entity Recognition in aspect-opinion extraction using dependency rule matching,"<p>Using Spacy, I extract aspect-opinion pairs from a text, based on the grammar rules that I defined. Rules are based on POS tags and dependency tags, which is obtained by <code>token.pos_</code> and <code>token.dep_</code>. Below is an example of one of the grammar rules. If I pass the sentence <code>Japan is cool,</code> it returns <code>[('Japan', 'cool', 0.3182)]</code>, where the value represents the polarity of <code>cool</code>.</p>

<p>However I don't know how I can make it recognise the Named Entities. For example, if I pass <code>Air France is cool</code>, I want to get <code>[('Air France', 'cool', 0.3182)]</code> but what I currently get is <code>[('France', 'cool', 0.3182)]</code>. </p>

<p>I checked Spacy online documentation and I know how to extract NE(<code>doc.ents</code>). But I want to know what the possible workaround is to make my extractor work. Please note that I don't want a forced measure such as concatenating strings <code>AirFrance</code>, <code>Air_France</code> etc.</p>

<p>Thank you!</p>

<pre><code>import spacy

nlp = spacy.load(""en_core_web_lg-2.2.5"")
review_body = ""Air France is cool.""
doc=nlp(review_body)

rule3_pairs = []

for token in doc:

    children = token.children
    A = ""999999""
    M = ""999999""
    add_neg_pfx = False

    for child in children :
        if(child.dep_ == ""nsubj"" and not child.is_stop): # nsubj is nominal subject
            A = child.text

        if(child.dep_ == ""acomp"" and not child.is_stop): # acomp is adjectival complement
            M = child.text

        # example - 'this could have been better' -&gt; (this, not better)
        if(child.dep_ == ""aux"" and child.tag_ == ""MD""): # MD is modal auxiliary
            neg_prefix = ""not""
            add_neg_pfx = True

        if(child.dep_ == ""neg""): # neg is negation
            neg_prefix = child.text
            add_neg_pfx = True

    if (add_neg_pfx and M != ""999999""):
        M = neg_prefix + "" "" + M

    if(A != ""999999"" and M != ""999999""):
        rule3_pairs.append((A, M, sid.polarity_scores(M)['compound']))
</code></pre>

<p>Result</p>

<pre><code>rule3_pairs
&gt;&gt;&gt; [('France', 'cool', 0.3182)]
</code></pre>

<p>Desired output</p>

<pre><code>rule3_pairs
&gt;&gt;&gt; [('Air France', 'cool', 0.3182)]
</code></pre>
",Named Entity Recognition (NER),named entity recognition aspect opinion extraction using dependency rule matching using spacy extract aspect opinion pair text based grammar rule defined rule based po tag dependency tag obtained example one grammar rule pas sentence return value represents polarity however know make recognise named entity example pas want get currently get checked spacy online documentation know extract ne want know possible workaround make extractor work please note want forced measure concatenating string etc thank result desired output
Flair NER Metrics interpretation,"<p>I'm currently preparing a NER Task with Flair and I'm looking for some information about metrics used for NER task.</p>

<p>What are the most used metrics and how to interpretate them ?</p>
",Named Entity Recognition (NER),flair ner metric interpretation currently preparing ner task flair looking information metric used ner task used metric interpretate
NLP - amazon reviews feature extraction,"<p>I'm working on the amazon reviews dataset.
the goal is to extract the positive and negative features of each product. </p>

<p>for example: for the sentence ""this product has great battery life"" I would like to extract the word 'battery' as a positive feature.</p>

<p>the dataset contains the fields:</p>

<p><strong>reviewerID</strong> - ID of the reviewer, e.g. A2SUAM1J3GNN3B</p>

<p><strong>asin</strong> - ID of the product, e.g. 0000013714</p>

<p><strong>reviewerName</strong> - name of the reviewer</p>

<p><strong>helpful</strong> - helpfulness rating of the review, e.g. 2/3</p>

<p><strong>reviewText</strong> - text of the review</p>

<p><strong>overall</strong> - rating of the product</p>

<p><strong>summary</strong> - summary of the review</p>

<p><strong>unixReviewTime</strong> - time of the review (unix time)</p>

<p><strong>reviewTime</strong> - time of the review (raw)</p>

<p>So far I separated the reviews for two lists: positive_reviews and negative_reviews.</p>

<p>positive_reviews = all the reviews that have a rating > 3
, negative_reviews = all the reviews that have a rating &lt; 3</p>

<p>I cleaned the text and tokenized it. after that, I extracted the nouns that came to after adjectives hoping that those specific nouns will be the features that I'm looking for.
after that, I tried to use clustering algorithms (k-means, DBSCAN) hoping that it will create a group that will represent the features I want to extract. </p>

<p>The results aren't good at all and I'm hoping that someone here might have an idea for a way for this to work</p>
",Named Entity Recognition (NER),nlp amazon review feature extraction working amazon review dataset goal extract positive negative feature product example sentence product ha great battery life would like extract word battery positive feature dataset contains field reviewerid id reviewer e g suam j gnn b asin id product e g reviewername name reviewer helpful helpfulness rating review e g reviewtext text review overall rating product summary summary review unixreviewtime time review unix time reviewtime time review raw far separated review two list positive review negative review positive review review rating negative review review rating cleaned text tokenized extracted noun came adjective hoping specific noun feature looking tried use clustering algorithm k mean dbscan hoping create group represent feature want extract result good hoping someone might idea way work
Named Entity Recognition for Python error,"<p>So Im trying to write a text pre-processor and trying to get nltk.ne_chunk() to work but Im getting a host of errors for the following code</p>

<pre><code>z = ""Francois Legault of the CAQ will now become the new premier of Quebec. This is possible as his party defeated the Liberals in the Provincial elections held on October 1st 2018.""

def preprocess_pipe1(doc1):
sent1 = nltk.sent_tokenize(doc1)
#print(sent1)
print("" "")
print (""SENTENCE SPLITTER"")
for x in sent1:
    print(x)

print("" "")

sent1 = [nltk.word_tokenize(sent2) for sent2 in sent1]
#print(sent1)
print("" "")
print (""TOKENIZER"")
for x in sent1:
    print(x)

print("" "")

sent1 = [nltk.pos_tag(sent2) for sent2 in sent1]
#print(sent1)
print("" "")
print (""POS TAGGER"")
for x in sent1:
    print(x)

return(sent1)


sent2=preprocess_pipe1(z)
sent3=nltk.ne_chunk(sent2)
print(sent3)
</code></pre>

<p>`
The errors are as follows</p>

<p>SENTENCE SPLITTER
Francois Legault of the CAQ will now become the new premier of Quebec.
This is possible as his party defeated the Liberals in the Provincial elections held on October 1st 2018.</p>

<p>TOKENIZER</p>

<pre><code>['Francois', 'Legault', 'of', 'the', 'CAQ', 'will', 'now', 'become', 'the', 'new', 'premier', 'of', 'Quebec', '.']
['This', 'is', 'possible', 'as', 'his', 'party', 'defeated', 'the', 'Liberals', 'in', 'the', 'Provincial', 'elections', 'held', 'on', 'October', '1st', '2018', '.']


POS TAGGER
[('Francois', 'NNP'), ('Legault', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('CAQ', 'NNP'), ('will', 'MD'), ('now', 'RB'), ('become', 'VB'), ('the', 'DT'), ('new', 'JJ'), ('premier', 'NN'), ('of', 'IN'), ('Quebec', 'NNP'), ('.', '.')]
[('This', 'DT'), ('is', 'VBZ'), ('possible', 'JJ'), ('as', 'IN'), ('his', 'PRP$'), ('party', 'NN'), ('defeated', 'VBD'), ('the', 'DT'), ('Liberals', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('Provincial', 'NNP'), ('elections', 'NNS'), ('held', 'VBD'), ('on', 'IN'), ('October', 'NNP'), ('1st', 'CD'), ('2018', 'CD'), ('.', '.')]
</code></pre>

<p>Error:</p>

<blockquote>
  <p>Traceback (most recent call last):   File ""C:/Users/Robin
  Karlose/PycharmProjects/NLTK Test 1/Code 5 - NER test.py"", line 71, in
  
      sent3=nltk.ne_chunk(sent2)   File ""C:\Users\Robin Karlose\PycharmProjects\NLTK Test
  1\venv\lib\site-packages\nltk\chunk__init__.py"", line 177, in
  ne_chunk
      return chunker.parse(tagged_tokens)   File ""C:\Users\Robin Karlose\PycharmProjects\NLTK Test
  1\venv\lib\site-packages\nltk\chunk\named_entity.py"", line 123, in
  parse
      tagged = self._tagger.tag(tokens)   File ""C:\Users\Robin Karlose\PycharmProjects\NLTK Test
  1\venv\lib\site-packages\nltk\tag\sequential.py"", line 63, in tag
      tags.append(self.tag_one(tokens, i, tags))   File ""C:\Users\Robin Karlose\PycharmProjects\NLTK Test
  1\venv\lib\site-packages\nltk\tag\sequential.py"", line 83, in tag_one
      tag = tagger.choose_tag(tokens, index, history)   File ""C:\Users\Robin Karlose\PycharmProjects\NLTK Test
  1\venv\lib\site-packages\nltk\tag\sequential.py"", line 632, in
  choose_tag
      featureset = self.feature_detector(tokens, index, history)   File ""C:\Users\Robin Karlose\PycharmProjects\NLTK Test
  1\venv\lib\site-packages\nltk\tag\sequential.py"", line 680, in
  feature_detector
      return self._feature_detector(tokens, index, history)   File ""C:\Users\Robin Karlose\PycharmProjects\NLTK Test
  1\venv\lib\site-packages\nltk\chunk\named_entity.py"", line 56, in
  _feature_detector
      pos = simplify_pos(tokens[index][1])   File ""C:\Users\Robin Karlose\PycharmProjects\NLTK Test
  1\venv\lib\site-packages\nltk\chunk\named_entity.py"", line 186, in
  simplify_pos
      if s.startswith('V'): return ""V"" AttributeError: 'tuple' object has no attribute 'startswith'</p>
</blockquote>

<p>Funnily enough when Im running this code the NER is working just fine</p>

<pre><code>import nltk
import nltk.corpus

sent = nltk.corpus.treebank.tagged_sents()[22]
print(sent)
print(nltk.ne_chunk(sent))
</code></pre>

<p>As far as I understand it - in both cases Im sending POS tagged text to the NLTK named entity recognition function (i.e. nltk.ne_chunk() ) but for the life of me I cannot understand why there are so many errors in the first case.</p>

<p>I would be grateful if anyone can provide some insight into this matter!</p>
",Named Entity Recognition (NER),named entity recognition python error im trying write text pre processor trying get nltk ne chunk work im getting host error following code error follows sentence splitter francois legault caq become new premier quebec possible party defeated liberal provincial election held october st tokenizer error traceback recent call last file c user robin karlose pycharmprojects nltk test code ner test py line sent nltk ne chunk sent file c user robin karlose pycharmprojects nltk test venv lib site package nltk chunk init py line ne chunk return chunker parse tagged token file c user robin karlose pycharmprojects nltk test venv lib site package nltk chunk named entity py line parse tagged self tagger tag token file c user robin karlose pycharmprojects nltk test venv lib site package nltk tag sequential py line tag tag append self tag one token tag file c user robin karlose pycharmprojects nltk test venv lib site package nltk tag sequential py line tag one tag tagger choose tag token index history file c user robin karlose pycharmprojects nltk test venv lib site package nltk tag sequential py line choose tag featureset self feature detector token index history file c user robin karlose pycharmprojects nltk test venv lib site package nltk tag sequential py line feature detector return self feature detector token index history file c user robin karlose pycharmprojects nltk test venv lib site package nltk chunk named entity py line feature detector po simplify po token index file c user robin karlose pycharmprojects nltk test venv lib site package nltk chunk named entity py line simplify po v return v attributeerror tuple object ha attribute funnily enough im running code ner working fine far understand case im sending po tagged text nltk named entity recognition function e nltk ne chunk life understand many error first case would grateful anyone provide insight matter
Rule-based NER in Spacy: Remove patterns,"<p>I have been adding rules to my custom Spacy named entity recognition model, using the new EntityRuler ( <a href=""https://spacy.io/usage/rule-based-matching#entityruler"" rel=""nofollow noreferrer"">https://spacy.io/usage/rule-based-matching#entityruler</a> ).</p>

<p>I added 1 million names of proteins, which took hours to run,  and now realized that many of them have names which are common words (like 'FOR' and '11').</p>

<p>I would like to remove some of the patterns from the EntityRuler object ( <a href=""https://spacy.io/api/entityruler"" rel=""nofollow noreferrer"">https://spacy.io/api/entityruler</a> ). But I'm not sure how to do that...</p>

<p>How can I remove rules/patterns from my EntityRuler object? 
Without unloading everything and loading the ones that should remain.</p>
",Named Entity Recognition (NER),rule based ner spacy remove pattern adding rule custom spacy named entity recognition model using new entityruler added million name protein took hour run realized many name common word like would like remove pattern entityruler object sure remove rule pattern entityruler object without unloading everything loading one remain
Custom names detection,"<p>This is a project in really early phase and I'm trying to find ideas on where to start.<br>
Any help or pointers would be greatly appreciated!</p>

<p><strong>My problem:</strong><br>
I have text on one side, and a list of named GraphDB elements on the other (usually the name is either an acronym or a multi-word expression). My texts are not annotated.<br>
I want to detect whenever a name is <strong>explicitly</strong> used in the text. The trick is that it will not necessarily be a perfect string match (for example an acronym can be used to shorten a multi-word expression, or a small part can be left out). So a simple string search will not have a 100% recall (even though it can be used as a starter).</p>

<p>If I just had an input and I wanted it to match it to one of the names, I would do a simple <strong>edit distance</strong> computation and that's it. What bugs me is that I have to do this for a whole text, and I don't know how to approach/break down the problem. <br>
I cannot break down everything in N-grams because my named entities can be a single word or up to seven words long... Or can I?<br>
I have thousands of Graph elements so I don't think NER can be applied here... Or can it?</p>

<p><strong>An example could be:</strong><br>
My list of names is ['Graph Database', 'Manager', 'Employee Number 1']<br>
The text is:</p>

<blockquote>
  <p>Every morning, the <strong><em>Manager</em></strong> browse through the <strong><em>Graph Database</em></strong> to look for updates. Every evening, <strong><em>Employee 1</em></strong> updates the <strong><em>GraphDB</em></strong>.</p>
</blockquote>

<p>I want in this block of text to map the 4 highlighted portions to their corresponding item in the list.</p>

<p><br><br>
I have a small background in Machine Learning but I haven't really ever done NLP. To be clear, I do not care about the meaning of these words, I just want to be able to <strong>detect</strong> them.<br></p>

<p>Thanks</p>
",Named Entity Recognition (NER),custom name detection project really early phase trying find idea start help pointer would greatly appreciated problem text one side list named graphdb element usually name either acronym multi word expression text annotated want detect whenever name explicitly used text trick necessarily perfect string match example acronym used shorten multi word expression small part left simple string search recall even though used starter input wanted match one name would simple edit distance computation bug whole text know approach break problem break everything n gram named entity single word seven word long thousand graph element think ner applied example could list name graph database manager employee number text every morning manager browse graph database look update every evening employee update graphdb want block text map highlighted portion corresponding item list small background machine learning really ever done nlp clear care meaning word want able detect thanks
Spacy Custom NER model training Error when using more than a single token for an entity,"<p>I am training Spacy custom NER (Named Entity Recognition) model. I followed the steps in the links <a href=""https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718"" rel=""nofollow noreferrer"">https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718</a>
and 
<a href=""https://spacy.io/usage/training#ner"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#ner</a></p>

<p>As per the examples provided, each word has a label for entity. I completed it successfully when I trained using a single word for each label. <strong>But in my scenario I need to train it with more than a single word or a sentence for an entity.</strong> Sometimes it can be 7 words or more, for example: ""Had partial loss of 30 BBLs"" or ""Total losses = 50 BBLs"" ...etc. However when I did that the code is given me following error:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-45-a9c9ec92bab3&gt; in &lt;module&gt;
    110 n_iter= 30 #(""Number of training iterations"", ""option"", ""n"", int))
    111 
--&gt; 112 train_test(model, new_model_name, output_dir, n_iter)

&lt;ipython-input-45-a9c9ec92bab3&gt; in train_test(model, new_model_name, output_dir, n_iter)
     74                 texts, annotations = zip(*batch)
     75                 nlp.update(texts, annotations, sgd=optimizer, drop=0.35,
---&gt; 76                            losses=losses)
     77             print('Losses', losses)
     78 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\spacy\language.py in update(self, docs, golds, drop, sgd, losses, component_cfg)
    513             kwargs = component_cfg.get(name, {})
    514             kwargs.setdefault(""drop"", drop)
--&gt; 515             proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)
    516             for key, (W, dW) in grads.items():
    517                 sgd(W, dW, key=key)

nn_parser.pyx in spacy.syntax.nn_parser.Parser.update()

nn_parser.pyx in spacy.syntax.nn_parser.Parser._init_gold_batch()

ner.pyx in spacy.syntax.ner.BiluoPushDown.preprocess_gold()

ner.pyx in spacy.syntax.ner.BiluoPushDown.has_gold()

TypeError: object of type 'NoneType' has no len()
</code></pre>

<p>my python code is: </p>

<pre><code>#!/usr/bin/env python
# coding: utf8

# Training additional entity types using spaCy
from __future__ import unicode_literals, print_function
import pickle
import plac
import random
from pathlib import Path
import spacy
from spacy.util import minibatch, compounding


# New entity labels
# Specify the new entity labels which you want to add here
LABELSS = [""MY_CUSTOM_ENTITY"" , ""U-Tag""] 

# Loading training data 
with open ('C:\\Users\\NER\\\ner_corpus_spacy_format_data.json', 'rb') as fp:
    TRAIN_DATA = pickle.load(fp)

@plac.annotations(
    model=(""Model name. Defaults to blank 'en' model."", ""option"", ""m"", str),
    new_model_name=(""New model name for model meta."", ""option"", ""nm"", str),
    output_dir=(""Optional output directory"", ""option"", ""o"", Path),
    n_iter=(""Number of training iterations"", ""option"", ""n"", int))

def train_test(model=None, new_model_name='AnyName43', output_dir=None, n_iter=30):
    """"""Setting up the pipeline and entity recognizer, and training the new entity.""""""
    if model is not None:
        nlp = spacy.load(model)  # load existing spacy model
        print(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank('en')  # create blank Language class
        print(""Created blank 'en' model"")
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)
    else:
        ner = nlp.get_pipe('ner')


    for _, annotations in TRAIN_DATA:
        for ent in annotations.get('entities'):
            ner.add_label(ent[2])


    #for i in LABELSS:
        #ner.add_label(i)   # Add new entity labels to entity recognizer

    if model is None:
        optimizer = nlp.begin_training()
    else:
        optimizer = nlp.entity.create_optimizer()

    # Get names of other pipes to disable them during training to train only NER
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            losses = {}
            batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(texts, annotations, sgd=optimizer, drop=0.35,
                           losses=losses)
            print('Losses', losses)

    # Test the trained model
    test_text = 'This is the text that has the instance of my custom entity. I am not using actual data since it is confidential, it can be something like: Had total loss of 60 BBLs or total losses = 85 BBLs. I have dataframe which consists of thousands of records.' 
    doc = nlp(test_text)
    print(""Entities in '%s'"" % test_text)
    for ent in doc.ents:
        print(ent.label_, ent.text)

    # Save model 
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.meta['name'] = new_model_name  # rename model
        nlp.to_disk(output_dir)
        print(""Saved model to"", output_dir)

        # Test the saved model
        print(""Loading from"", output_dir)
        nlp2 = spacy.load(output_dir)
        doc2 = nlp2(test_text)
        for ent in doc2.ents:
            print(ent.label_, ent.text)


#if __name__ == '__main__':
#plac.call(train_test)


model= None #""en_core_web_sm"" #(""Model name. Defaults to blank 'en' model."", ""option"", ""m"", str),
new_model_name= ""MyModelName"" #(""New model name for model meta."", ""option"", ""nm"", str),
output_dir= 'C:\\Users\\NER\\TRAIN_TEST_OUTPUT' #(""Optional output directory"", ""option"", ""o"", Path),
n_iter= 30 #(""Number of training iterations"", ""option"", ""n"", int))

train_test(model, new_model_name, output_dir, n_iter)
</code></pre>

<p>Appreciate your help as I am stuck on this point and I couldn't find a solution on the internet. </p>
",Named Entity Recognition (NER),spacy custom ner model training error using single token entity training spacy custom ner named entity recognition model followed step link per example provided word ha label entity completed successfully trained using single word label scenario need train single word sentence entity sometimes word example partial loss bbl total loss bbl etc however code given following error python code appreciate help stuck point find solution internet
ImportError: Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training,"<p>cannot install apex for distributed and fp16 training of bert model
i have tried to install by cloning the apex from github and tried to install packages using pip</p>

<p>i have tried to install apex by cloning from git hub using following command:</p>

<p>git clone <a href=""https://github.com/NVIDIA/apex.git"" rel=""nofollow noreferrer"">https://github.com/NVIDIA/apex.git</a></p>

<p>and cd apex to goto apex directory and tried to install package using following pip command:</p>

<p>pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext""</p>

<p>full code is:</p>

<pre><code>def main(server_ip,server_port,local_rank,no_cuda,fp16,train_batch_size,gradient_accumulation_steps,seed,do_train,do_eval,output_dir,task_name,data_dir,do_lower_case,bert_model,num_train_epochs,cache_dir,learning_rate,warmup_proportion,loss_scale,max_seq_length):
        if server_ip and server_port:
            # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script
            import ptvsd
            print(""Waiting for debugger attach"")
            ptvsd.enable_attach(address=(server_ip, server_port), redirect_output=True)
            ptvsd.wait_for_attach()

        processors = {""ner"":NerProcessor}
        print(processors)

        if local_rank == -1 or no_cuda:
            device = torch.device(""cuda"" if torch.cuda.is_available() and not no_cuda else ""cpu"")
            n_gpu = torch.cuda.device_count()
        else:
            torch.cuda.set_device(local_rank)
            device = torch.device(""cuda"", local_rank)
            n_gpu = 1
            # Initializes the distributed backend which will take care of sychronizing nodes/GPUs
            torch.distributed.init_process_group(backend='nccl')
        logger.info(""device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}"".format(
            device, n_gpu, bool(local_rank != -1), fp16))

        if gradient_accumulation_steps &lt; 1:
            raise ValueError(""Invalid gradient_accumulation_steps parameter: {}, should be &gt;= 1"".format(
                                args.gradient_accumulation_steps))

        train_batch_size = train_batch_size // gradient_accumulation_steps

        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)

        if not do_train and not do_eval:
            raise ValueError(""At least one of `do_train` or `do_eval` must be True."")

        if os.path.exists(output_dir) and os.listdir(output_dir) and do_train:
            raise ValueError(""Output directory ({}) already exists and is not empty."".format(output_dir))
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        task_name = task_name.lower()

        if task_name not in processors:
            raise ValueError(""Task not found: %s"" % (task_name))

        processor = processors[task_name]()
        label_list = processor.get_labels()
        num_labels = len(label_list) + 1

        tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)

        train_examples = None
        num_train_optimization_steps = None
        if do_train:
            train_examples = processor.get_train_examples(data_dir)
            num_train_optimization_steps = int(
                len(train_examples) / train_batch_size / gradient_accumulation_steps) * num_train_epochs
            if local_rank != -1:
                num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()

    #     # Prepare model
        cache_dir = cache_dir if cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(local_rank))
        model = Ner.from_pretrained(bert_model,
                  cache_dir=cache_dir,
                  num_labels = num_labels)
        if fp16:
            model.half()
        # model.cuda()
        model.to(device)
        if local_rank != -1:
            try:
                from apex.parallel import DistributedDataParallel as DDP
            except ImportError:
                raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training."")

            model = DDP(model)
        elif n_gpu &gt; 1:
            model = torch.nn.DataParallel(model)

        param_optimizer = list(model.named_parameters())
        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
        optimizer_grouped_parameters = [
            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
             ]
        if fp16:
            try:
                from apex.optimizers import FP16_Optimizer
                from apex.optimizers import FusedAdam
            except ImportError:
                raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training."")

            optimizer = FusedAdam(optimizer_grouped_parameters,
                                  lr=learning_rate,
                                  bias_correction=False,
                                  max_grad_norm=1.0)
            if loss_scale == 0:
                optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)
            else:
                optimizer = FP16_Optimizer(optimizer, static_loss_scale=loss_scale)

        else:
            optimizer = BertAdam(optimizer_grouped_parameters,
                                 lr=learning_rate,
                                 warmup=warmup_proportion,
                                 t_total=num_train_optimization_steps)

        global_step = 0
        nb_tr_steps = 0
        tr_loss = 0
        label_map = {i : label for i, label in enumerate(label_list,1)}
        if do_train:
            train_features = convert_examples_to_features(
                train_examples, label_list, max_seq_length, tokenizer)
            logger.info(""***** Running training *****"")
            logger.info(""  Num examples = %d"", len(train_examples))
            logger.info(""  Batch size = %d"", train_batch_size)
            logger.info(""  Num steps = %d"", num_train_optimization_steps)
            all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)
            all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)
            all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)
            all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)
            all_valid_ids = torch.tensor([f.valid_ids for f in train_features], dtype=torch.long)
            all_lmask_ids = torch.tensor([f.label_mask for f in train_features], dtype=torch.long)
            train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids,all_valid_ids,all_lmask_ids)
            if local_rank == -1:
                train_sampler = RandomSampler(train_data)
            else:
                train_sampler = DistributedSampler(train_data)
            train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)

            model.train()
            for _ in trange(int(num_train_epochs), desc=""Epoch""):
                tr_loss = 0
                nb_tr_examples, nb_tr_steps = 0, 0
                for step, batch in enumerate(tqdm(train_dataloader, desc=""Iteration"")):
                    batch = tuple(t.to(device) for t in batch)
                    input_ids, input_mask, segment_ids, label_ids, valid_ids,l_mask = batch
                    loss = model(input_ids, segment_ids, input_mask, label_ids,valid_ids,l_mask)
                    del loss
                    if n_gpu &gt; 1:
                        loss = loss.mean() # mean() to average on multi-gpu.
                    if gradient_accumulation_steps &gt; 1:
                        loss = loss / gradient_accumulation_steps

                    if fp16:
                        optimizer.backward(loss)
                    else:
                        loss.backward()

                    tr_loss += loss.item()
                    nb_tr_examples += input_ids.size(0)
                    nb_tr_steps += 1
                    if (step + 1) % gradient_accumulation_steps == 0:
                        if fp16:
                            # modify learning rate with special warm up BERT uses
                            # if args.fp16 is False, BertAdam is used that handles this automatically
                            lr_this_step = learning_rate * warmup_linear(global_step/num_train_optimization_steps, warmup_proportion)
                            for param_group in optimizer.param_groups:
                                param_group['lr'] = lr_this_step
                        optimizer.step()
                        optimizer.zero_grad()
                        global_step += 1
</code></pre>

<p>main('','',-1,True,True,8,1,42,True,True,'jpt','ner','data/',True,'bert-base-cased',5,'cache_dir',5e-5,0.4,0,128)</p>
",Named Entity Recognition (NER),importerror please install apex use distributed fp training install apex distributed fp training bert model tried install cloning apex github tried install package using pip tried install apex cloning git hub using following command git clone cd apex goto apex directory tried install package using following pip command pip install v cache dir global option cpp ext global option cuda ext full code main true true true true jpt ner data true bert base cased cache dir e
how to speed up lemmatization using spacy.pipe on text?,"<p>How can I speed up lemmatization on text set using spacy pipe? Currently I'm using like this,</p>

<pre><code>nlp = spacy.load('en_core_web_lg')
df['text'].apply(lambda x: len(nlp(x).ents)) # returns number of named entities
</code></pre>

<p>How can I use extract number of named entities using nlp.pipe with batch_size, threads etc.. and take advantage of multiprocesses?</p>

<pre><code>spacy_nlp.pipe(df['text'], n_threads=6, batch_size=10)
</code></pre>
",Named Entity Recognition (NER),speed lemmatization using spacy pipe text speed lemmatization text set using spacy pipe currently using like use extract number named entity using nlp pipe batch size thread etc take advantage multiprocesses
Predicting NER with BertForTokenClassification model,"<p>I have built my model using this tutorial on NER with bert:</p>

<p><a href=""https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/#resources"" rel=""nofollow noreferrer"">https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/#resources</a></p>

<p>However, I could not figure out how to parse in a input data into the model to predict its ner value.</p>

<p>The following links are some of the resources I have looked through</p>

<p><a href=""https://stackoverflow.com/questions/60220842/how-should-properly-formatted-data-for-ner-in-bert-look-like"">How should properly formatted data for NER in BERT look like?</a></p>

<p><a href=""https://huggingface.co/transformers/model_doc/bert.html#bertfortokenclassification"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/bert.html#bertfortokenclassification</a></p>
",Named Entity Recognition (NER),predicting ner bertfortokenclassification model built model using tutorial ner bert however could figure parse input data model predict ner value following link resource looked
Replace names with serial number using Spacy Python,"<p>I am trying to replace the names in a text a dataframe (two columns: Name and Comments) with serial numbers. To do so, I am using Spacy's NeuralCoref for the NER. My current code is only good enough to replace the names in the dataframe with ""[REDACTED]"".</p>

<pre><code>import spacy
import pandas as pd
`Load the large English NLP model`
nlp = spacy.load('en_core_web_lg-2.1.0')

`load neural coref to Spacy pipe`
 import neuralcoref
 neuralcoref.add_to_pipe(nlp)

df = pd.read_excel(r'D:\XXX\YYY\try.xlsx')

def replace_name_with_index(token):
if token.ent_iob != 0 and token.ent_type_ == ""PERSON"":

    return ""[REDACTED] ""

else:
    return token.string

def scrub(text):
    doc = nlp(text)
    for ent in doc.ents:
       ent.merge()
    tokens = map(replace_name_with_index, doc)
    return """".join(tokens)

for ind in df.index:
    name = df['Name'][ind]
    comments = df['Comments'][ind]
    textDoc = name + "" ; "" + comments #concat text from both columns
    #remove stopwords and punctuations
    print (scrub(textDoc), end = '\n')
    print(""---"")
</code></pre>

<p>I am trying another set of codes in an attempt to replace the names with index numbers first. </p>

<pre><code>import spacy
import pandas as pd


# Load the large English NLP model
nlp = spacy.load('en_core_web_lg-2.1.0')

#load neural coref to Spacy's pipe
import neuralcoref
neuralcoref.add_to_pipe(nlp)

text= ""Carolina Smith Henderson is my sister. Carolina Smith Henderson is a dog lover. Carolina loves to eat ice cream even though she is lactose intolerent. She has a brother named Andrew and a sister named Brenda""

doc = nlp(text)

newString = text
index = 0

for e in reversed(doc.ents): #reversed to not modify the offsets of other entities when substituting
    start = e.start_char
    end = start + len(e.text)
    #newString = newString[:start] + e.label_ + newString[end:]

    newString = newString[:start] + str(index) + newString[end:]

    index = index + 1

print(newString)
</code></pre>

<p>But the problem with the above code is that ""Carolina Smith Henderson"", ""Carolina Smith Henderson"" (2nd mention) and ""Carolina"" is considered as 3 different ""PERSON"" though they are referring to the same person. Could you kindly advise how the code could be improved such that the NeuralCoref could identify it as one individual and only assign one index to it? Subsequently, this is to be done for all the records in the dataframe using for loop as shown in the first half of the post. </p>

<p>Thanks in advance!</p>
",Named Entity Recognition (NER),replace name serial number using spacy python trying replace name text dataframe two column name comment serial number using spacy neuralcoref ner current code good enough replace name dataframe redacted trying another set code attempt replace name index number first problem code carolina smith henderson carolina smith henderson nd mention carolina considered different person though referring person could kindly advise code could improved neuralcoref could identify one individual assign one index subsequently done record dataframe using loop shown first half post thanks advance
How to create Affix(Prefix + Suffix) embeddings in NLP,"<p>I am working on a named entity recognition task. Traditional method is to concatenate word embeddings and character level embeddings for creating a word representation first. I want to also use affix embeddings to better understand the relation between the tags and the words. </p>

<p>For example the words ""Afghanistan"" and ""Kajikistan"" are clear examples of Location. Here the suffix ""istan"" or ""tan"" will be useful to identify future ""location"" tags. So I want to extract the suffixes and prefixes of all the words and create embeddings for them, and then concatenate with the initial word representation. How to achieve this?</p>
",Named Entity Recognition (NER),create affix prefix suffix embeddings nlp working named entity recognition task traditional method concatenate word embeddings character level embeddings creating word representation first want also use affix embeddings better understand relation tag word example word afghanistan kajikistan clear example location suffix istan tan useful identify future location tag want extract suffix prefix word create embeddings concatenate initial word representation achieve
custom pipelines in RASA : getting pipeline output,"<p>Suppose I make a pipeline that only extracts entities, then how do I get these output entities ??
for example, this is my pipeline :</p>

<pre><code>pipeline:
- name: ""SpacyNLP""
- name: ""CRFEntityExtractor""
- name: ""EntitySynonymMapper""
</code></pre>

<p>how do I get the output entities?</p>
",Named Entity Recognition (NER),custom pipeline rasa getting pipeline output suppose make pipeline extract entity get output entity example pipeline get output entity
How to match a text based on string from a list and extract the subsection in Python?,"<p>I am trying to generate the structure from an earnings call text which looks like the following sample:</p>

<pre><code>""Operator

Ladies and gentlemen, thank you for standing by. And welcome to XYZ Fourth Quarter 2019 Earning Conference Call. At this time, all participants are in a listen-only mode. After the speaker presentation, there will be a question-and-answer session. [Operator Instructions] Please be advised that today’s conference is being recorded. [Operator Instructions]
I would now like to hand the conference to your speaker today,Person1, Head of Investor Relations. Please go ahead, ma’am**

Person1

Hello everyone, blablablablabla. Now let's see what Person2 has to say.

Person2

Thank you and hello everyone. Blablablabla

Person3

I have no further remarks....thank you once again""
</code></pre>

<p>From this I have generated a list called <code>list1 = ['Person1','Person2','Person3']</code>. I have generated an empty dataframe which has column names as <code>Person1</code>, <code>Person2</code> and <code>Person3</code>. I now have to extract the text below <code>Person1</code>, <code>Person2</code> and <code>Person3</code> based on the values from list and fill in the dataframe. Is that possible?</p>
",Named Entity Recognition (NER),match text based string list extract subsection python trying generate structure earnings call text look like following sample generated list called generated empty dataframe ha column name extract text based value list fill dataframe possible
Label custom entities in Resume (NER),"<p>How I can perform NER for custom named entity. e.g. If I want to identify if particular word is skill in resume. If (Java, c++) is occurring in my text i should be able to label them as skill. I don't want to use spacy with custom corpus.I want to create the dataset e.g.
words will be my features and label(skill) will be my dependent variable. </p>

<p>what is the best approach to handle these kinda problems.</p>
",Named Entity Recognition (NER),label custom entity resume ner perform ner custom named entity e g want identify particular word skill resume java c occurring text able label skill want use spacy custom corpus want create dataset e g word feature label skill dependent variable best approach handle kinda problem
How to extract unique string in between string pattern in full text in R?,"<p><strong>I'm looking to extract names and professions of those who testified in front of Congress from the following text:</strong> </p>

<p>text &lt;- c((""FULL COMMITTEE HEARINGS\\"", \\"" 2017\\"",\n\\"" April 6, 2017—‘‘The 2017 Tax Filing Season: Internal Revenue\\"", \"", \""\\""\nService Operations and the Taxpayer Experience.’’ This hearing\\"", \\"" examined\nissues related to the 2017 tax filing season, including\\"", \\"" IRS performance,\ncustomer service challenges, and information\\"", \\"" technology. Testimony was\nheard from the Honorable John\\"", \\"" Koskinen, Commissioner, Internal Revenue\nService, Washington,\\"", \"", \""\\"" DC.\\"", \\"" May 25, 2017—‘‘Fiscal Year 2018 Budget\nProposals for the Depart-\\"", \\"" ment of Treasury and Tax Reform.’’ The hearing\ncovered the\\"", \\"" President’s 2018 Budget and touched on operations of the De-\n\\"", \\"" partment of Treasury and Tax Reform. Testimony was heard\\"", \\"" from the\nHonorable Steven Mnuchin, Secretary of the Treasury,\\"", \"", \""\\"" United States\nDepartment of the Treasury, Washington, DC.\\"", \\"" July 18, 2017—‘‘Comprehensive\nTax Reform: Prospects and Chal-\\"", \\"" lenges.’’ The hearing covered issues\nsurrounding potential tax re-\\"", \\"" form plans including individual, business,\nand international pro-\\"", \\"" posals. Testimony was heard from the Honorable\nJonathan Talis-\\"", \"", \""\\"" man, former Assistant Secretary for Tax Policy 2000–\n2001,\\"", \\"" United States Department of the Treasury, Washington, DC; the\\"",\n\\"" Honorable Pamela F. Olson, former Assistant Secretary for Tax\\"", \\"" Policy\n2002–2004, United States Department of the Treasury,\\"", \\"" Washington, DC; the\nHonorable Eric Solomon, former Assistant\\"", \"", \""\\"" Secretary for Tax Policy\n2006–2009, United States Department of\\"", \\"" the Treasury, Washington, DC; and\nthe Honorable Mark J.\\"", \\"" Mazur, former Assistant Secretary for Tax Policy\n2012–2017,\\"", \\"" United States Department of the Treasury, Washington, DC.\\"",\n\\"" (5)\\"", \\""VerDate Sep 11 2014 14:16 Mar 28, 2019 Jkt 000000 PO 00000 Frm 00013\nFmt 6601 Sfmt 6601 R:\\\\DOCS\\\\115ACT.000 TIM\\""\"", \"")\"")""
)</p>

<p>The full text is available here: <a href=""https://www.congress.gov/116/crpt/srpt19/CRPT-116srpt19.pdf"" rel=""nofollow noreferrer"">https://www.congress.gov/116/crpt/srpt19/CRPT-116srpt19.pdf</a></p>

<p><strong>It seems that the names are in between ""Testimony was heard from"" until the next ""."". So, how can I extract the names between these two patterns? The text is much longer (50 page document), but I figured that if I can do it one, I'll do it for the rest of the text.</strong> </p>

<p><strong>I know I can't use NLP for name extraction because they are names of persons that didn't testify, for example.</strong> </p>
",Named Entity Recognition (NER),extract unique string string pattern full text r looking extract name profession testified front congress following text text c full committee hearing n april tax filing season internal revenue nservice operation taxpayer experience hearing examined nissues related tax filing season including irs performance ncustomer service challenge information technology testimony wa nheard honorable john koskinen commissioner internal revenue nservice washington dc may fiscal year budget nproposals depart ment treasury tax reform hearing ncovered president budget touched operation de n partment treasury tax reform testimony wa heard nhonorable mnuchin secretary treasury united state ndepartment treasury washington dc july comprehensive ntax reform prospect chal lenges hearing covered issue nsurrounding potential tax form plan including individual business nand international pro posals testimony wa heard honorable njonathan talis man former assistant secretary tax policy n united state department treasury washington dc n honorable pamela f olson former assistant secretary tax policy n united state department treasury washington dc nhonorable eric solomon former assistant secretary tax policy n united state department treasury washington dc honorable mark j mazur former assistant secretary tax policy n united state department treasury washington dc n verdate sep mar jkt po frm nfmt sfmt r doc act tim full text available seems name testimony wa heard next extract name two pattern text much longer page document figured one rest text know use nlp name extraction name person testify example
Text semantic preprocessing,"<p>Let assume that I have a dataset of car accidents. Each accident has a textual description made using a set of cameras and other sensors. </p>

<p>Suppose now I have only the data of a single camera (e.g. the frontal) and I want to remove all the sentences of the description that are not related to it. I think a basic and easy solution could be to use a boolean retrieval system using a set of specific keywords to remove unwanted sentences, but I don't know neither if it is a good idea ner if it could work; could someone suggest me any idea? What kind of statistics might be useful to study this problem? Thanks</p>
",Named Entity Recognition (NER),text semantic preprocessing let assume dataset car accident accident ha textual description made using set camera sensor suppose data single camera e g frontal want remove sentence description related think basic easy solution could use boolean retrieval system using set specific keywords remove unwanted sentence know neither good idea ner could work could someone suggest idea kind statistic might useful study problem thanks
Train Spacy NER model with &#39;en_core_web_sm&#39; as base model,"<p>I am using Spacy to train my NER model with new entities and I am using <code>en_core_web_sm</code> model as my base model because I also want to detect the basic entities (<code>ORG</code>, <code>PERSON</code>, <code>DATE</code>, etc). I ran <code>en_core_web_sm</code> model over unlabelled sentences, and adding their annotations to my training set.</p>

<p>After I finished with that, now I want to create the training data for the new entities. For example, I want to add a new entity called <code>FRUIT</code>. I have a bunch of sentences (in addition to those that were annotated using <code>en_core_web_sm</code> earlier) that I am going to annotate. The sentence example is:  </p>

<blockquote>
  <p>""James likes eating apples"".</p>
</blockquote>

<p><strong><em>My question is</em></strong>: Do I still need to annotate ""<em>James</em>"" as <code>PERSON</code> as well as annotating ""<em>apples</em>"" as <code>FRUIT</code>? Or whether I don't need to do it because I already have another bunch of sentences that were annotated with <code>PERSON</code> entity using <code>en_core_web_sm</code> model earlier.</p>
",Named Entity Recognition (NER),train spacy ner model en core web sm base model using spacy train ner model new entity using model base model also want detect basic entity etc ran model unlabelled sentence adding annotation training set finished want create training data new entity example want add new entity called bunch sentence addition annotated using earlier going annotate sentence example james like eating apple question still need annotate james well annotating apple whether need already another bunch sentence annotated entity using model earlier
How to retrieve the fully qualified name from certain functions while parsing the python file,"<p>I've noticed that there's a '<strong>qualname</strong>' for functions to get their fully qualified name. However, what I'm trying to achieve is to extract a functions call sequence from a given python file, which means to get the fully qualified name while reading the file, but not execute it. Here's an example of the problem.</p>

<p>Given code: </p>

<pre><code>import libA
class A():
    bar = libA.Bar()
    bar.functionA()
</code></pre>

<p>I'm expecting the output of: libA.Bar.functionA.</p>

<p>Is there a way to use the <strong>inspect</strong> or <strong>ast</strong> tool to solve this issue?</p>
",Named Entity Recognition (NER),retrieve fully qualified name certain function parsing python file noticed qualname function get fully qualified name however trying achieve extract function call sequence given python file mean get fully qualified name reading file execute example problem given code expecting output liba bar functiona way use inspect ast tool solve issue
Named Entity Extraction of dates,"<p>I am absolutely new to the NER and Extraction and programming in general. I am trying to figure out a way where I can extract due dates and start date of certain documents. Is there a way to do this? A place where I can start? I have been looking around but the problem  I run into is the same. Can extract dates but not whether the date is due or post. If it only has 1 date, is it post or due. Stuff like that. Any help would be appreciated.</p>

<p>Example: </p>

<p>""Essay on Medieval Asia was due on September 3rd.""</p>

<p>""Your last assignment that was given on April 6th was supposed to be submitted in 10 days.""</p>

<p>""The bid is due no later than a month from the date it was posted(today).""</p>
",Named Entity Recognition (NER),named entity extraction date absolutely new ner extraction programming general trying figure way extract due date start date certain document way place start looking around problem run extract date whether date due post ha date post due stuff like help would appreciated example essay medieval asia wa due september rd last assignment wa given april th wa supposed submitted day due later month date wa posted today
SPACY custom NER is not returning any entity,"<p>I am trying to train a Spacy model to recognize a few custom NERs, the training data is given below, it is mostly related to recognizing a few server models, date in the FY format and Types of HDD:</p>

<pre><code>TRAIN_DATA = [('Send me the number of units shipped in FY21 for A566TY server', {'entities': [(39, 42, 'DateParse'),(48,53,'server')]}),
            ('Send me the number of units shipped in FY-21 for A5890Y server', {'entities': [(39, 43, 'DateParse'),(49,53,'server')]}),              
          ('How many systems sold with 3.5 inch drives in FY20-Q2 for F567?', {'entities': [(46, 52, 'DateParse'),(58,61,'server'),(27,29,'HDD')]}),              
          ('Total revenue in FY20Q2 for 3.5 HDD', {'entities': [(17, 22, 'DateParse'),(28,30,'HDD')]}),
          ('How many systems sold with 3.5 inch drives in FY20-Q2 for F567?', {'entities': [(46, 52, 'DateParse'),(58,61,'server'),(27,29,'HDD')]}),

          ('Total units shipped in FY2017-FY2021', {'entities': [(23, 28, 'DateParse'),(30,35,'DateParse')]}),
          ('Total units shipped in FY 18', {'entities': [(23, 27, 'DateParse')]}),
          ('Total units shipped between FY16 and FY2021', {'entities': [(28, 31, 'DateParse'),(37,42,'DateParse')]})
         ]
def train_spacy(data,iterations):
TRAIN_DATA = data
nlp = spacy.blank('en')  # create blank Language class
# create the built-in pipeline components and add them to the pipeline
# nlp.create_pipe works for built-ins that are registered with spaCy
if 'ner' not in nlp.pipe_names:
    ner = nlp.create_pipe('ner')
    nlp.add_pipe(ner, last=True)


# add labels
for _, annotations in TRAIN_DATA:
     for ent in annotations.get('entities'):
        ner.add_label(ent[2])

# get names of other pipes to disable them during training
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
with nlp.disable_pipes(*other_pipes):  # only train NER
    optimizer = nlp.begin_training()
    for itn in range(iterations):
        print(""Statring iteration "" + str(itn))
        random.shuffle(TRAIN_DATA)
        losses = {}
         # batch up the examples using spaCy's minibatch
        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            texts, annotations = zip(*batch)
            nlp.update(
                texts,  # batch of texts
                annotations,  # batch of annotations
                drop=0.2,  # dropout - make it harder to memorise data
                losses=losses,
            )
        print(""Losses"", losses)
return nlp
</code></pre>

<p>But on running the code even on training data no entity is being returned.</p>

<pre><code>prdnlp = train_spacy(TRAIN_DATA, 100)
for text, _ in TRAIN_DATA:
    doc = prdnlp(text)
    print(""Entities"", [(ent.text, ent.label_) for ent in doc.ents])
    print(""Tokens"", [(t.text, t.ent_type_, t.ent_iob) for t in doc])
</code></pre>

<p>The Output is coming as below:</p>

<p><a href=""https://i.sstatic.net/a1OnL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/a1OnL.png"" alt=""Output""></a></p>
",Named Entity Recognition (NER),spacy custom ner returning entity trying train spacy model recognize custom ners training data given mostly related recognizing server model date fy format type hdd running code even training data entity returned output coming
pull full names and surname/surnames form the consecutive paragraphs in Excel / Word VBA,"<p>First Things First: The macro is run from Excel VBA editor, but performs the biggest part of its job on the previously opened Word document, where it's goal is to find the full names of the people who are the contracting parties in the agreement being analized.</p>

<p>The issue I'm experiencing with the code is that it is variable number of words, that I need to pull from every consecutive paragraph. If the name is Will SMITH, then its two words I need to pull, when it's Carrie Ann MOSS, then it's three words, sometimes it can be Anna Nicole SMITH BURKE, than its four words but when it's Anna Nicole SMITH-BURKE, than its five words and so on.</p>

<p>The other idea to get this <strong>full name</strong> is, that it always ends with a coma, and this coma is always the first coma in this paragraph, where the full name appears.</p>

<p>ATTENTION !!! The Paragraphs we work with are not <code>ListParagraphs</code>. They are the normal/ordinary ones albeit indented and numbered. I get these contracts from people who don't care to use numbered list :-(<br>
  So for the last time: The numbered list is not enabled on those paragraphs we work with.</p>

<p>This is how it looks like in Word and the selected words are the names and surnames that the macro is supposed to extract from the document - excluding the coma after the last surname.</p>

<p><a href=""https://i.sstatic.net/llGrh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/llGrh.png"" alt=""enter image description here""></a> </p>

<pre><code>Sub FindNamesCleanDraftWithLoop()
    'Variables declaration
    Dim WordApp As Word.Application
    Dim WordDoc As Word.Document
    Dim ExcelApp As Excel.Application
    Dim MySheet As Excel.Worksheet
    Dim Para As Word.Paragraph
    Dim Rng As Word.Range
    Dim RngStart As Word.Range
    Dim RngEnd As Word.Range

    Dim TextToFind1 As String
    Dim TextToFind2 As String
    Dim firstName As String
    Dim startPos As Long
    Dim endPos As Long

    Application.ScreenUpdating = False

    'Assigning object variables
    Set WordApp = GetObject(, ""Word.Application"")
    Set ExcelApp = GetObject(, ""Excel.Application"")
    Set WordDoc = WordApp.ActiveDocument
    Set MySheet = Application.ActiveWorkbook.ActiveSheet
    'Set MySheet = ExcelApp.ActiveWorkbook.ActiveSheet
    Set Rng = WordApp.ActiveDocument.Content
    TextToFind1 = ""REGON 364061169, NIP 951-24-09-783,""
    TextToFind2 = ""- ad.""

    'InStr function returns a Variant (Long) specifying the position of the first occurrence of one string within another.
    startPos = InStr(1, Rng, TextToFind1) - 1    'here we get 1421, we're looking 4 ""TextToFind1""
    endPos = InStr(1, Rng, TextToFind2) - 1      'here we get 2246, we're looking 4 ""- ad.""
    If startPos = 0 Or endPos = 0 Then Exit Sub
    Rng.SetRange Start:=startPos, End:=endPos
    Debug.Print Rng.Paragraphs.Count

    If startPos = 0 Or endPos = 0 Then
        MsgBox (""Client's names were not found!"")
    Else
        'somewhere here I need your help to write some lines that will
        'recognize how many words need to be pulled to extract the full
        'name/names + surname/surnames and nothing else - we end on the first coma.
        For Each Para In Rng.Paragraphs
            firstName = Trim$(Para.Range.Words(3))
            Debug.Print Para.Range.Words(1) &amp; Para.Range.Words(2) &amp; _
                        Para.Range.Words(3) &amp; Para.Range.Words(4) &amp; _
                        Para.Range.Words(5) &amp; Para.Range.Words(6)
        Next Para
    End If
End Sub
</code></pre>

<p>There in the <code>For Each Para ... Next Para</code> loop, I need your help to write some lines that will recognize how many words need to be pulled to extract the <strong>full name/names + surname/surnames and nothing else</strong> - we end on the first coma - that means excluding the coma after the last surname.</p>
",Named Entity Recognition (NER),pull full name surname surname form consecutive paragraph excel word vba first thing first macro run excel vba editor performs biggest part job previously opened word document goal find full name people contracting party agreement analized issue experiencing code variable number word need pull every consecutive paragraph name smith two word need pull carrie ann moss three word sometimes anna nicole smith burke four word anna nicole smith burke five word idea get full name always end coma coma always first coma paragraph full name appears attention paragraph work normal ordinary one albeit indented numbered get contract people care use numbered list last time numbered list enabled paragraph work look like word selected word name surname macro supposed extract document excluding coma last surname loop need help write line recognize many word need pulled extract full name name surname surname nothing else end first coma mean excluding coma last surname
Named Entity recognition relative date,"<p>I'm using spaCy as NLP library to detect named entities. I want to extract dates and time reference fom the text automatically. For example, capture the date in this sentence: <code>I will go to the show on 1/1/2020</code> and detect that <code>1/1/2020</code> is a DATE named entity.</p>

<p>But I also want to understand relative time phrases, for example <code>I will go to the show tomorrow</code>. <code>tomorrow</code> is detected as DATE named entity, but I don't know which time it refers - if today is <code>1/1/2020</code> than tomorrow is <code>1/2/2020</code>. I want to get the <code>1/2/2020</code> directly from the named entity, even if it relative.</p>

<p>I tried to do that manually by created a dictionary, but the date named entities are very wide and I miss them with a static dictionary.</p>

<p>Is there any way to receive the actual time from a relative date named entity?</p>
",Named Entity Recognition (NER),named entity recognition relative date using spacy nlp library detect named entity want extract date time reference fom text automatically example capture date sentence detect date named entity also want understand relative time phrase example detected date named entity know time refers today tomorrow want get directly named entity even relative tried manually created dictionary date named entity wide miss static dictionary way receive actual time relative date named entity
Python Library for Finding Duplicate Sub-strings Between Textual Files in Python,"<p>I'm trying to identify duplicate blocks of text contained at arbitrary locations within several larger bodies of text, without knowing anything about the texts beforehand. That's to say, given n-many bodies of text, ignore the portions which are duplicate between texts.</p>

<p><strong>An example:</strong></p>

<p>5 webpages, each containing an article, but also a navigation menu, footer, sidebar etc.</p>

<p>The article would be unique to each page, but the content in the sidebar, footer, and navigation menus would be the same.</p>

<p>My goal would be to identify the non-article content as being duplicate..</p>

<p><strong>Some Notes:</strong></p>

<p>I do not know the contents of the duplicate content beforehand.</p>

<p>The duplicate content could be anywhere within the larger bodies of text between files. It could proceed, precede, or be mixed within.</p>

<p>The duplicate content should be identified from start to finish. i.e. if an entire paragraph is duplicated between files, a substring of that paragraph shouldn't be flagged as duplicate.</p>

<p>I'm digging through the <a href=""https://biopython.org/"" rel=""nofollow noreferrer"">BioPython</a> library and its sequence-matching features seem to be on track. I would love to find one more geared towards natural language.</p>
",Named Entity Recognition (NER),python library finding duplicate sub string textual file python trying identify duplicate block text contained arbitrary location within several larger body text without knowing anything text beforehand say given n many body text ignore portion duplicate text example webpage containing article also navigation menu footer sidebar etc article would unique page content sidebar footer navigation menu would goal would identify non article content duplicate note know content duplicate content beforehand duplicate content could anywhere within larger body text file could proceed precede mixed within duplicate content identified start finish e entire paragraph duplicated file substring paragraph flagged duplicate digging biopython library sequence matching feature seem track would love find one towards natural language
Replace specific text with a redacted version using Python,"<p>I am looking to do the opposite of what has been done here:</p>

<pre><code>import re

text = '1234-5678-9101-1213 1415-1617-1819-hello'

re.sub(r""(\d{4}-){3}(?=\d{4})"", ""XXXX-XXXX-XXXX-"", text)

output = 'XXXX-XXXX-XXXX-1213 1415-1617-1819-hello'
</code></pre>

<p><em><a href=""https://stackoverflow.com/questions/16327590/partial-replacement-with-re-sub"">Partial replacement with re.sub()</a></em></p>

<p>My overall goal is to replace all <code>XXXX</code> within a text using a neural network. <code>XXXX</code> can represent names, places, numbers, dates, etc. that are in a .csv file.</p>

<p>The end result would look like:</p>

<pre><code>XXXX went to XXXX XXXXXX
</code></pre>

<p>Sponge Bob went to Disney World.</p>

<p>In short, I am unmasking text and replacing it with a generated dataset using fuzzy.</p>
",Named Entity Recognition (NER),replace specific text redacted version using python looking opposite ha done href replacement sub overall goal replace within text using neural network represent name place number date etc csv file end result would look like sponge bob went disney world short unmasking text replacing generated dataset using fuzzy
"How do I get the correct NER using SpaCy from text like &quot;F.B.I. Agent Peter Strzok, Who Criticized Trump in Texts, Is Fired&quot;?","<p>How do I get the correct NER using SpaCy from text like ""F.B.I. Agent Peter Strzok, Who Criticized Trump in Texts, Is Fired - The New York Times SectionsSEARCHSkip to contentSkip to site."" 
here ""Criticized Trump"" is recognized as person instead of ""Trump"" as person.</p>

<p>How to pre-process and lower case the text like ""Criticized"" or ""Texts"" from the above string to overcome above issue or any other technique to do so. </p>

<pre><code>import spacy
from spacy import displacy
from collections import Counter
import en_core_web_sm
nlp = en_core_web_sm.load()
from pprint import pprint

sent = (""F.B.I. Agent Peter Strzok, Who Criticized Trump in Texts, Is Fired - The New York Times SectionsSEARCHSkip to contentSkip to site"")
doc = nlp(sent)
pprint([(X, X.ent_iob_, X.ent_type_) for X in doc])
</code></pre>

<p>Result from above code:-
""Criticized Trump"" as 'PERSON' and ""Texts"" as 'GPE'</p>

<p>Expected result should be:-
""Trump"" as 'PERSON' instead of ""Criticized Trump"" as 'PERSON'  and ""Texts"" as '' instead of ""Texts"" as 'GPE'</p>
",Named Entity Recognition (NER),get correct ner using spacy text like f b agent peter strzok criticized trump text fired get correct ner using spacy text like f b agent peter strzok criticized trump text fired new york time sectionssearchskip contentskip site criticized trump recognized person instead trump person pre process lower case text like criticized text string overcome issue technique result code criticized trump person text gpe expected result trump person instead criticized trump person text instead text gpe
How can I use regex in Python to extract location information from tweets about activism/protests?,"<p>I'm working with a corpus I've scraped from Twitter activist communities in order to study the modern era of community organizing. I'm trying to run these data through re.findall in order to identify the tweets focused on location. I think that using the keyword ""at"" may be the easiest way to accomplish this. </p>

<p>Basically, if the entire tweet is (for example) ""all who wish 2 join, meet at city hall 3pm"", my code should print out something like ""meet at city hall"" for that line. Is this possible, or am I fundamentally misunderstanding the utility of regex? I've only ever really used them for extracting email information previously, so I'm used to writing code like this:</p>

<pre><code>match = re.findall(r'[\w\.-]+@[\w\.-]+', line)
</code></pre>

<p>However, attempting to exchange the '@' in the code above for an 'at' doesn't yield any results.</p>

<p>I'm probably not even asking the right question here. Apologies for any confusion I cause and I appreciate any and all help!</p>
",Named Entity Recognition (NER),use regex python extract location information tweet activism protest working corpus scraped twitter activist community order study modern era community organizing trying run data findall order identify tweet focused location think using keyword may easiest way accomplish basically entire tweet example wish join meet city hall pm code print something like meet city hall line possible fundamentally misunderstanding utility regex ever really used extracting email information previously used writing code like however attempting exchange code yield result probably even asking right question apology confusion cause appreciate help
How to train completely new entities instead of pre-trained entities using Spacy NER model?,"<p>How do I do transfer learning i.e. take pre-trained Spacy NER model and make it learn new entities specific to my use case? </p>

<p>For this, I have 100 new annotated training samples. The new retrained model should only predict the new entities and not any of the existing entities in the pre-trained spacy model. Just adding/updating new entities to existing models and ignoring the old entities during prediction doesn't make sense. </p>

<p>This <a href=""https://github.com/explosion/spaCy/blob/master/examples/training/train_new_entity_type.py"" rel=""nofollow noreferrer"">official example</a> describes how to add new entities to existing pre-trained entities but that's not what I want. I also have very few examples i.e. 100 to completely built a new NER model from scratch.</p>

<p>Edit: I want to identify all account numbers in an unstructured document.</p>

<p>Example 
(""I would like to change address corresponding to my account 12345. Kindly let me know how to do it. "" [34, 39, 'accountnumber'])</p>
",Named Entity Recognition (NER),train completely new entity instead pre trained entity using spacy ner model transfer learning e take pre trained spacy ner model make learn new entity specific use case new annotated training sample new retrained model predict new entity existing entity pre trained spacy model adding updating new entity existing model ignoring old entity prediction make sense official example describes add new entity existing pre trained entity want also example e completely built new ner model scratch edit want identify account number unstructured document example would like change address corresponding account kindly let know accountnumber
Custom NER for identifying products,"<p>I am trying to buld a custom named entity extractor for product names and their model numbers. </p>

<p>My use case contains sentences like:
""Microsoft used product ABC-300 and also integrated it with ASQ"".
Product mentioned in the above sentence are: ABC-300 and ASQ</p>

<p>I have already tried using Stanford and Spacy NER, accuracy of both is less than desired.</p>

<p>Are there any datasets that contain product names in paragraphs or sentences I can use for training custom NER model? The sentences for training can be simple or complex. Any kind of data will be useful. </p>

<p>Any leads on how to approach this problem with less training data will also be appreciated.</p>
",Named Entity Recognition (NER),custom ner identifying product trying buld custom named entity extractor product name model number use case contains sentence like microsoft used product abc also integrated asq product mentioned sentence abc asq already tried using stanford spacy ner accuracy le desired datasets contain product name paragraph sentence use training custom ner model sentence training simple complex kind data useful lead approach problem le training data also appreciated
numerical entity extraction from unstructured texts using python,"<p>I want to extract numerical entities like temperature and duration mentioned in unstructured formats of texts using neural models like CRF using python. I would like to know how to proceed for numerical extraction as most of the examples available on the internet are for specific words or strings extraction. </p>

<p>Input: 'For 5 minutes there, I felt like baking in an oven at 350 degrees F'
Output: temperature: 350
        duration: 5 minutes</p>
",Named Entity Recognition (NER),numerical entity extraction unstructured text using python want extract numerical entity like temperature duration mentioned unstructured format text using neural model like crf using python would like know proceed numerical extraction example available internet specific word string extraction input minute felt like baking oven degree f output temperature duration minute
Creating a custom location-centric NER using SpaCy,"<p>So, I am creating a system where I need to be able to get the location information on restaurant/grocery receipts which have processed through an OCR (so I am working on raw-text) around the world.</p>

<p>I have opted to use SpaCy's NER engine to detect location. However, the existing <code>en_core_web_sm</code> model is only good in detecting limited set of locations (<code>GPE</code>'s as they're called) like <code>New York</code> and <code>Washington</code>, etc... which is kind of expected as it has been trained on a dataset involving broadcast news, etc.</p>

<p>Now, I have a dataset where I have information of restaurants located in various cities. The first <code>column</code> hosts the information on the complete <code>address</code> and the other is the <code>city</code>. 
Like,</p>

<p><a href=""https://i.sstatic.net/JHGqX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JHGqX.png"" alt=""restaurant address""></a></p>

<p>I am currently interested in only detecting location up to the level of a city (not further down than that). That is why I am interested in further training my <code>spaCy</code> model.</p>

<p>My question is,</p>

<p>Is it okay in using an existing <code>pre-trained</code> model (<code>en_core_web_sm</code> in my case) which would be fine-tuned when I train it further using the above dataset?</p>
",Named Entity Recognition (NER),creating custom location centric ner using spacy creating system need able get location information restaurant grocery receipt processed ocr working raw text around world opted use spacy ner engine detect location however existing model good detecting limited set location called like etc kind expected ha trained dataset involving broadcast news etc dataset information restaurant located various city first host information complete like currently interested detecting location level city interested training model question okay using existing model case would fine tuned train using dataset
Why does Spacy&#39;s NER trainer return tokens but not entities?,"<p>Thanks for looking.  I am trying to train a custom Named Entity Recognizer, using code from Spacy's website.  My problem is that after I run my examples through the trainer, it returns the tokens, but no entities.  Here are my examples, saved in the variable <code>to_train_ents</code>:</p>

<pre class=""lang-py prettyprint-override""><code>[('""We’re at the beginning of what we could do with laser ultrasound,"" says Brian W. Anthony, a principal research scientist in MIT’s Department of Mechanical Engineering and Institute for Medical Engineering and Science (IMES), a senior author on the paper.',
  {'entities': [(72, 88, 'PERSON')]}),
 ('Early concepts for noncontact laser ultrasound for medical imaging originated from a Lincoln Laboratory program established by Rob Haupt of the Active Optical Systems Group and Chuck Wynn of the Advanced Capabilities and Technologies Group, who are co-authors on the new paper along with Matthew Johnson.',
  {'entities': [(126, 135, 'PERSON'),
    (176, 186, 'PERSON'),
    (287, 302, 'PERSON')]}),
 ('From there, the research grew via collaboration with Anthony and his students, Xiang (Shawn) Zhang, who is now an MIT postdoc and is the paper’s first author, and recent doctoral graduate Jonathan Fincke, who is also a co-author.',
  {'entities': [(78, 97, 'PERSON'), (187, 202, 'PERSON')]})]
</code></pre>

<p>From what I can tell, they are formatted correctly to pass into the trainer.  Here is the code used to train the NER model, from spacy.io:</p>

<pre class=""lang-py prettyprint-override""><code>def main(model = None, output_dir = None, n_iter = 100):
    # Load the model, set up the pipeline and train the entity recognizer
    if model is not None:   # If model was specified...
        nlp = spacy.load(model)   # ...load the existing spaCy model
        pprint(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank(""en"")   # ...otherwise, create a blank language class
        print(""Created blank 'en' model"")

    # Create the built-in pipeline components and add them to the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if ""ner"" not in nlp.pipe_names:   # If Named Entity Recognition is not part of the pipeline...
        ner = nlp.create_pipe(""ner"")
        nlp.add_pipe(ner, last = True)   # ...add it to the pipeline
    else:
        ner = nlp.get_pipe(""ner"")

    # Add labels
    for _, annotations in to_train_ents:
        for ent in annotations.get(""entities""):  # ""get"" is a way of retrieving items from dictionaries
            ner.add_label(ent[2])

    # Get names of other pipes to disable them during training (we want only NER)
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""] # other_pipes is any pipe that is not NER
    with nlp.disable_pipes(*other_pipes):  # Train only NER
        # Reset and initialize the weights randomly - but only if we're training a new model
        if model is None:
            nlp.begin_training()
        for itn in range(n_iter):
            random.shuffle(to_train_ents)
            losses = {}
            # Batch up the examples using spaCy's minibatch
            batches = minibatch(to_train_ents, size = compounding(4.0, 32.0, 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(
                texts,  # Batch of texts
                annotations,  # Batch of annotations
                drop = 0.5,  # Dropout - make it harder to memorize data (adjustable variable)
                losses = losses,
                )
            print(""Losses"", losses)

    # Test the trained model
    for text, _ in to_train_ents:
        doc = nlp(text)
        print(""Entities"", [(ent.text, ent.label_) for ent in doc.ents])
        print(""Tokens"", [(t.text, t.ent_type_, t.ent_iob) for t in doc])

    # Save the model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.to_disk(output_dir)
        print(""Saved model to"", output_dir)

    # Test the saved model
    print(""Loading from"", output_dir)
    nlp2 = spacy.load(output_dir)
    for text, _ in to_train_ents:
        doc = nlp2(text)
        print(""Entities"", [(ent.text, ent.label_) for ent in doc.ents])
        print(""Tokens"", [(t.text, t.ent_type_, t.ent_iob) for t in doc])
</code></pre>

<p>I tell this function to use the english model and to save in output directory 'nih_ner':</p>

<pre class=""lang-py prettyprint-override""><code>main(model = 'en', output_dir = 'nih_ner')
</code></pre>

<p>Here is the result:</p>

<pre><code>""Loaded model 'en'""
Losses {'ner': 52.71057402440056}
Losses {'ner': 43.944127584481976}
Losses {'ner': 40.92080506101935}
~snip~
Losses {'ner': 8.647840025578502}
Losses {'ner': 0.001753763942560257}
Entities []
Tokens [('From', '', 2), ('there', '', 2), (',', '', 2), ('the', '', 2), ('research', '', 2), ('grew', '', 2), ('via', '', 2), ('collaboration', '', 2), ('with', '', 2), ('Anthony', '', 2), ('and', '', 2), ('his', '', 2), ('students', '', 2), (',', '', 2), ('Xiang', '', 2), ('(', '', 2), ('Shawn', '', 2), (')', '', 2), ('Zhang', '', 2), (',', '', 2), ('who', '', 2), ('is', '', 2), ('now', '', 2), ('an', '', 2), ('MIT', '', 2), ('postdoc', '', 2), ('and', '', 2), ('is', '', 2), ('the', '', 2), ('paper', '', 2), ('’s', '', 2), ('first', '', 2), ('author', '', 2), (',', '', 2), ('and', '', 2), ('recent', '', 2), ('doctoral', '', 2), ('graduate', '', 2), ('Jonathan', '', 2), ('Fincke', '', 2), (',', '', 2), ('who', '', 2), ('is', '', 2), ('also', '', 2), ('a', '', 2), ('co', '', 2), ('-', '', 2), ('author', '', 2), ('.', '', 2)]
Entities []
Tokens [('""', '', 2), ('We', '', 2), ('’re', '', 2), ('at', '', 2), ('the', '', 2), ('beginning', '', 2), ('of', '', 2), ('what', '', 2), ('we', '', 2), ('could', '', 2), ('do', '', 2), ('with', '', 2), ('laser', '', 2), ('ultrasound', '', 2), (',', '', 2), ('""', '', 2), ('says', '', 2), ('Brian', '', 2), ('W.', '', 2), ('Anthony', '', 2), (',', '', 2), ('a', '', 2), ('principal', '', 2), ('research', '', 2), ('scientist', '', 2), ('in', '', 2), ('MIT', '', 2), ('’s', '', 2), ('Department', '', 2), ('of', '', 2), ('Mechanical', '', 2), ('Engineering', '', 2), ('and', '', 2), ('Institute', '', 2), ('for', '', 2), ('Medical', '', 2), ('Engineering', '', 2), ('and', '', 2), ('Science', '', 2), ('(', '', 2), ('IMES', '', 2), (')', '', 2), (',', '', 2), ('a', '', 2), ('senior', '', 2), ('author', '', 2), ('on', '', 2), ('the', '', 2), ('paper', '', 2), ('.', '', 2)]
Entities []
Tokens [('Early', '', 2), ('concepts', '', 2), ('for', '', 2), ('noncontact', '', 2), ('laser', '', 2), ('ultrasound', '', 2), ('for', '', 2), ('medical', '', 2), ('imaging', '', 2), ('originated', '', 2), ('from', '', 2), ('a', '', 2), ('Lincoln', '', 2), ('Laboratory', '', 2), ('program', '', 2), ('established', '', 2), ('by', '', 2), ('Rob', '', 2), ('Haupt', '', 2), ('of', '', 2), ('the', '', 2), ('Active', '', 2), ('Optical', '', 2), ('Systems', '', 2), ('Group', '', 2), ('and', '', 2), ('Chuck', '', 2), ('Wynn', '', 2), ('of', '', 2), ('the', '', 2), ('Advanced', '', 2), ('Capabilities', '', 2), ('and', '', 2), ('Technologies', '', 2), ('Group', '', 2), (',', '', 2), ('who', '', 2), ('are', '', 2), ('co', '', 2), ('-', '', 2), ('authors', '', 2), ('on', '', 2), ('the', '', 2), ('new', '', 2), ('paper', '', 2), ('along', '', 2), ('with', '', 2), ('Matthew', '', 2), ('Johnson', '', 2), ('.', '', 2)]
Saved model to nih_ner
Loading from nih_ner
Entities []
Tokens [('From', '', 2), ('there', '', 2), (',', '', 2), ('the', '', 2), ('research', '', 2), ('grew', '', 2), ('via', '', 2), ('collaboration', '', 2), ('with', '', 2), ('Anthony', '', 2), ('and', '', 2), ('his', '', 2), ('students', '', 2), (',', '', 2), ('Xiang', '', 2), ('(', '', 2), ('Shawn', '', 2), (')', '', 2), ('Zhang', '', 2), (',', '', 2), ('who', '', 2), ('is', '', 2), ('now', '', 2), ('an', '', 2), ('MIT', '', 2), ('postdoc', '', 2), ('and', '', 2), ('is', '', 2), ('the', '', 2), ('paper', '', 2), ('’s', '', 2), ('first', '', 2), ('author', '', 2), (',', '', 2), ('and', '', 2), ('recent', '', 2), ('doctoral', '', 2), ('graduate', '', 2), ('Jonathan', '', 2), ('Fincke', '', 2), (',', '', 2), ('who', '', 2), ('is', '', 2), ('also', '', 2), ('a', '', 2), ('co', '', 2), ('-', '', 2), ('author', '', 2), ('.', '', 2)]
Entities []
Tokens [('""', '', 2), ('We', '', 2), ('’re', '', 2), ('at', '', 2), ('the', '', 2), ('beginning', '', 2), ('of', '', 2), ('what', '', 2), ('we', '', 2), ('could', '', 2), ('do', '', 2), ('with', '', 2), ('laser', '', 2), ('ultrasound', '', 2), (',', '', 2), ('""', '', 2), ('says', '', 2), ('Brian', '', 2), ('W.', '', 2), ('Anthony', '', 2), (',', '', 2), ('a', '', 2), ('principal', '', 2), ('research', '', 2), ('scientist', '', 2), ('in', '', 2), ('MIT', '', 2), ('’s', '', 2), ('Department', '', 2), ('of', '', 2), ('Mechanical', '', 2), ('Engineering', '', 2), ('and', '', 2), ('Institute', '', 2), ('for', '', 2), ('Medical', '', 2), ('Engineering', '', 2), ('and', '', 2), ('Science', '', 2), ('(', '', 2), ('IMES', '', 2), (')', '', 2), (',', '', 2), ('a', '', 2), ('senior', '', 2), ('author', '', 2), ('on', '', 2), ('the', '', 2), ('paper', '', 2), ('.', '', 2)]
Entities []
Tokens [('Early', '', 2), ('concepts', '', 2), ('for', '', 2), ('noncontact', '', 2), ('laser', '', 2), ('ultrasound', '', 2), ('for', '', 2), ('medical', '', 2), ('imaging', '', 2), ('originated', '', 2), ('from', '', 2), ('a', '', 2), ('Lincoln', '', 2), ('Laboratory', '', 2), ('program', '', 2), ('established', '', 2), ('by', '', 2), ('Rob', '', 2), ('Haupt', '', 2), ('of', '', 2), ('the', '', 2), ('Active', '', 2), ('Optical', '', 2), ('Systems', '', 2), ('Group', '', 2), ('and', '', 2), ('Chuck', '', 2), ('Wynn', '', 2), ('of', '', 2), ('the', '', 2), ('Advanced', '', 2), ('Capabilities', '', 2), ('and', '', 2), ('Technologies', '', 2), ('Group', '', 2), (',', '', 2), ('who', '', 2), ('are', '', 2), ('co', '', 2), ('-', '', 2), ('authors', '', 2), ('on', '', 2), ('the', '', 2), ('new', '', 2), ('paper', '', 2), ('along', '', 2), ('with', '', 2), ('Matthew', '', 2), ('Johnson', '', 2), ('.', '', 2)]
</code></pre>

<p>As you can see, the model returns the tokens to me, but there are empty lists, [], where the recognized entities should be.  Any suggestions as to why this is happening would be helpful.</p>

<p>Thanks again!</p>
",Named Entity Recognition (NER),doe spacy ner trainer return token entity thanks looking trying train custom named entity recognizer using code spacy website problem run example trainer return token entity example saved variable tell formatted correctly pas trainer code used train ner model spacy io tell function use english model save output directory nih ner result see model return token empty list recognized entity suggestion happening would helpful thanks
Spacy NER - Train a model only having a collection of entities,"<p>I have a database with a collection of thousands of products and we are working on a NLP module on our system that should be capable of read text and identify all the products from it.</p>

<p>The problem is: We have many entities to be tagged but we don't have enough samples to train the model. So what we are doing is just to generate some random text and adding those entities like this:</p>

<pre><code>Training_data = [
    (""I'm looking for the P1.\n"", {'entities': [(20, 22, 'Product')]}),
    ('I bought P2 last week.\n', {'entities': [(9, 11, 'Product')]}),
    ('P1 is better than P2.', {'entities': [(0, 2, 'Product'), (18, 20, 'Product')]})
]
</code></pre>

<p>I'm struggling to find a way to just pass a collection with all of our products to perform the spacy training. Is this possible? Any better solution or approach? </p>
",Named Entity Recognition (NER),spacy ner train model collection entity database collection thousand product working nlp module system capable read text identify product problem many entity tagged enough sample train model generate random text adding entity like struggling find way pas collection product perform spacy training possible better solution approach
Rasa Stack Dynamic Entites,"<p>Can we extract dynamic entities that we not defined in the nlu file or data file?</p>

<p>Below is my NLU File</p>

<p>intent:benename
ahsan
ali
ahsan
mohsin
ahmed
qaseem
yasir
qaiser
salman
daniyal</p>

<p>For example: above bene_names are easily extract by nlu engine, but what if when user enter a new name? how we can get that name?</p>
",Named Entity Recognition (NER),rasa stack dynamic entites extract dynamic entity defined nlu file data file nlu file intent benename ahsan ali ahsan mohsin ahmed qaseem yasir qaiser salman daniyal example bene name easily extract nlu engine user enter new name get name
Set validation data in SpaCy NER training,"<p>Is it possible to train SpaCy NER with validation data?
Or split some data to validation set like in Keras (validation_split in model.fit)? Thanks</p>

<pre><code>with nlp.disable_pipes(*other_pipes):  # only train NER
        for itn in tqdm(range(n_iter)):
            random.shuffle(train_data_list)
            losses = {}
            # batch up the examples using spaCy's minibatch
            batches = minibatch(train_data_list, size=compounding(8., 64., 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(texts, annotations, sgd=optimizer, drop=0.35,
                           losses=losses)
</code></pre>
",Named Entity Recognition (NER),set validation data spacy ner training possible train spacy ner validation data split data validation set like kera validation split model fit thanks
dataset creation using feature extraction from text,"<p>I am trying to extract a few features from text data of terrorist events to create a dataset. Using name entity recognition, I have successfully extracted the features like <strong>name, place, organization</strong> now I want to extract the number of members involved in the incidence.</p>

<pre><code>The 2008 Mumbai attacks (also referred to as 26/11) were a series of terrorist attacks that took place in
November 2008, when 10 members of Lashkar-e-Taiba, a terrorist organization based in Pakistan,
carried out 12 coordinated shooting and bombing attacks lasting four days across Mumbai.
</code></pre>

<p>from the above text how can I extract <strong>10 members of Lashkar-e-Taiba</strong> and place 10 in the column of the number of attackers.
Is that even possible using nlp techniques?</p>
",Named Entity Recognition (NER),dataset creation using feature extraction text trying extract feature text data terrorist event create dataset using name entity recognition successfully extracted feature like name place organization want extract number member involved incidence text extract member lashkar e taiba place column number attacker even possible using nlp technique
How to do dependency parsing with custom lexicons,"<p>I have a load of medical text. I also have two lexicons. One contains a list of standardised anatomical locations eg <code>anatomy&lt;-c(""oesophagus"", ""stomach"",""duodenum"", ""pylorus"", ""antrum"")</code> as well as a list of events <code>events&lt;-c(""clip"",""inject"",""emr"",""rfa"")</code>.  The lists are actually much longer but I'm simplifying it here.</p>

<p>I would like to do dependency parsing on the medical text using the two lexicons so I can pick up events related to medical sites..for example:</p>

<p><strong>Input text</strong></p>

<p>The pylorus looked indurated. It underwent emr and rfa. The stomach started to bleed so a clip was placed. The bleeding stopped. Finally the oesophagus looked normal but the duodenum required a further clip</p>

<p><strong>Desired output 1</strong> </p>

<p>which should give me:
 pylorus:emr, pylorus:rfa, stomach:clip, duodenum:clip</p>

<p>I think I have understood that I need to train a model using the lexicons and that this model needs to be provided in CONLL-U format (according to the <a href=""https://bnosac.github.io/udpipe/docs/doc3.html"" rel=""nofollow noreferrer""><code>udpipe</code></a> package. documentation). I might be wrong in assuming this however.</p>

<p>I have tried following the instructions but I do not know how to create the CONLL-U file in the first place using my lexicons. If I have misinterpreted how to do my custom dependency parsing please can you direct me to an explanation in r?...</p>

<p>As a further example, I have created a function in R that extracts elements from two lists if they are present in the same sentence, and another function if they are in adjoining functions. I have created this as part of a package I have written but here is the code:</p>

<p><strong>Dependency parsing attempts</strong></p>

<pre><code>#' See if words from two lists co-exist within a sentence 
#'
#' See if words from two lists co-exist within a sentence. Eg site and tissue type.
#' This function only looks in one sentence for the two terms. If you suspect the terms may
#' occur in adjacent sentences then use the EntityPairs_TwoSentence function.
#' @keywords PathPairLookup
#' @param inputText The relevant pathology text column
#' @param list1 First list to refer to
#' @param list2 The second list to look for
#' @importFrom purrr flatten_chr map_chr map map_if
#' @export
#' @family Basic Column mutators 
#' @examples # tbb&lt;-EntityPairs_OneSentence(Mypath$Histology,HistolType(),LocationList())

EntityPairs_OneSentence&lt;-function(inputText,list1,list2){

  #dataframe&lt;-data.frame(dataframe,stringsAsFactors = FALSE)
  list1&lt;-paste0(unlist(list1,use.names=F),collapse=""|"")
  list2&lt;-paste0(unlist(list2,use.names=F),collapse=""|"")

  #text&lt;-textPrep(inputText)
  text&lt;-standardisedTextOutput&lt;-stri_split_boundaries(inputText, type=""sentence"")
  r1 &lt;-lapply(text,function(x) Map(paste, str_extract_all(tolower(x),tolower(list2)), 
                                   str_extract_all(tolower(x),tolower(list1)), MoreArgs = list(sep="":"")))

  r1&lt;-lapply(r1,function(x) unlist(x))
  #Unlist into a single row-This should output a character vector
  out&lt;-lapply(r1,function(x) paste(x,collapse="",""))

  return(out)
}

#' Look for relationships between site and event
#' 
#' This is used to look for relationships between site and event especially for endoscopy events
#' where sentences such as 'The stomach polyp was large. It was removed with a snare' ie the therapy
#' and the site are in two different locations.
#' @keywords Find and replace
#' @param inputString The relevant pathology text column
#' @param list1 The intial list to assess
#' @param list2 The other list to look for
#' @importFrom stringr str_replace_na str_c str_split str_which str_extract_all regex str_subset
#' @importFrom stringi stri_split_boundaries
#' @importFrom rlang is_empty
#' @importFrom purrr flatten_chr map_chr map map_if
#' @export
#' @family Basic Column mutators 
#' @examples # tbb&lt;-EntityPairs_TwoSentence(Myendo$Findings,EventList(),HistolType())

EntityPairs_TwoSentence&lt;-function(inputString,list1,list2){

  #Prepare the text to be back into a tokenised version.
  #text&lt;-textPrep(inputText)
  text&lt;-standardisedTextOutput&lt;-stri_split_boundaries(inputString, type=""sentence"")
  text&lt;-lapply(text,function(x) tolower(x))


  #Some clean up to get rid of white space- all of this prob already covered in the ColumnCleanUp function but for investigation later
  text&lt;-lapply(text,function(x) gsub(""[[:punct:]]+"","" "",x))
  #Prepare the list to use as a lookup:
  tofind &lt;-paste(tolower(list2),collapse=""|"")

  #Prepare the second list to use as a lookup
  EventList&lt;-unique(tolower(unlist(list1,use.names = FALSE)))


  text&lt;-sapply(text,function(x) {

    #Cleaning
    x&lt;-trimws(x)



    #Prepare the text so that all empty text is replaced with NA and 
    #ready for processing
    try(words &lt;-
          x %&gt;%
          unlist() %&gt;%
          str_replace_na()%&gt;%
          str_c(collapse = ' ') %&gt;%
          str_split(' ') %&gt;%
          `[[`(1))


    words&lt;-words[words != """"] 
    x1 &lt;- str_extract_all(tolower(x),tolower(paste(unlist(list1), collapse=""|"")))
    i1 &lt;- which(lengths(x1) &gt; 0)


    try(if(any(i1)) {
      EventList %&gt;%
        map(
          ~words %&gt;%
            str_which(paste0('^.*', .x)) %&gt;%
            map_chr(
              ~words[1:.x] %&gt;%
                str_c(collapse = ' ') %&gt;%

                str_extract_all(regex(tofind, ignore_case = TRUE)) %&gt;%
                map_if(rlang::is_empty, ~ NA_character_) %&gt;%
                flatten_chr()%&gt;%
                `[[`(length(.)) %&gt;%

                .[length(.)]
            ) %&gt;%
            paste0(':', .x)
        ) %&gt;%
        unlist() %&gt;%
        str_subset('.+:')

    } else """")

  }
  )
  return(text)
}
</code></pre>

<p><strong>Lexicons for the functions above</strong></p>

<p>The lexicons I am using for the two functions above are:</p>

<pre><code>EventList &lt;- function() {
  Event &lt;- list(
    ""radiofrequency ablation"" = ""RFA"",
    ""(argon plasma coagulation)|apc"" = ""APC"",
    ""halo| rfa"" = ""RFA"",
    ""dilatation|dilated"" = ""dilat"",
    ""clip"" = ""clip"",
    ""grasp"" = ""grasp"",
    ""iodine"" = ""iodine"",
    ""acetic"" = ""acetic"",
    ""NAC"" = ""NAC"",
    ""Brushings"" = ""brushings"",
    "" cryo"" = ""cryablation""
  )

  # To get the list as a list of values only in a regex use
  # paste0(unlist(Event,use.names=F),collapse=""|"")

  return(Event)
}


HistolType &lt;- function() {

  # First standardise the terms

  tissue &lt;- list(
    ""Resection"" = ""Resection"",
    ""bx|biopsies"" = ""Biopsy"",
    ""(endoscopic mucosal resection)|(endoscopic mucosectomy)"" = ""EMR"",
    ""endoscopic submucosal (dissection|resection)"" = ""ESD"",
    ""nodul"" = ""nodul"",
    ""polyp"" = ""polyp"",
    ""emr"" = ""EMR""
  )


  # To get the list as a list of values only in a regex use
  # paste0(unlist(HistolType(),use.names=F),collapse=""|"")

  return(tissue)
}
</code></pre>

<p><strong>A larger synthetic example dataset is given here:</strong></p>

<pre><code>c(""OESOPHAGUS: tight proximal anastomotic stricture - dilated 20mm\n,STOMACH: minimal non-erosive gastritis - CLO test negative\n,Flumazenil given\n,This couldn't be resolved with the coag graspers so 2 clips were deploted over the vessel and 1:10,000 adrenaline x10ml was injected with haemostasis achieved\n,Oesophagus and stomach normal\n,Stomach- Antrum Gastritis- Nodular\n,Normal duodenum\n,There was loss of pit pattern on surface,STOMACH: significant amount of food residue in the stomach, therefore the procedure was stopped due to risk of aspiration and poor views\n,Examined under white light and NBI\n\n\n"",
 Duodenum treated with HALO RFA Channel at 12J\n,Sliding hiatus hernia 3cm, GOJ biopsies taken\n,Intubation of efferent limb for a length of the scope\n,Linear erosions in oesophagus\n,OESOPHAGUS: Normal to GOJ at 41 cm\n,\n,Treated with HALO 90 RFA at 12J\n\n\n""
    )
</code></pre>

<p><strong>And the desired output 2:</strong></p>

<pre><code>[[1]]
[1] ""Oesophagus:dilat""  ""stomach:clip""   ""stomach:grasp"" 

[[2]]
[1] ""Duodenum:rfa"" ""Oesophagus:rfa""
</code></pre>
",Named Entity Recognition (NER),dependency parsing custom lexicon load medical text also two lexicon one contains list standardised anatomical location eg well list event list actually much longer simplifying would like dependency parsing medical text using two lexicon pick event related medical site example input text pylorus looked indurated underwent emr rfa stomach started bleed clip wa placed bleeding stopped finally oesophagus looked normal duodenum required clip desired output give pylorus emr pylorus rfa stomach clip duodenum clip think understood need train model using lexicon model need provided conll u format according package documentation might wrong assuming however tried following instruction know create conll u file first place using lexicon misinterpreted custom dependency parsing please direct explanation r example created function r extract element two list present sentence another function adjoining function created part package written code dependency parsing attempt lexicon function lexicon using two function larger synthetic example dataset given desired output
How to extract unusual unknown words in NLP,"<p>Im new to NLP and try to work with spacy, other libraries are also ok. 
In my dataset i have a lot of names of computerlocations and computer names like: WRN212 asnd WRN800 SKh1038 PHk2gd
This names are free formatted text in my dataset, does someone know how to extract such words.  </p>

<p>Is this possible in Spacy?
Is there somewhere and example?</p>

<p>Thank you in advance. </p>

<p>Greets </p>
",Named Entity Recognition (NER),extract unusual unknown word nlp im new nlp try work spacy library also ok dataset lot name computerlocations computer name like wrn asnd wrn skh phk gd name free formatted text dataset doe someone know extract word possible spacy somewhere example thank advance greets
How to convert XML NER data from the CRAFT corpus to spaCy&#39;s JSON format?,"<p><strong>How to build a named entity recognition(NER) model using spaCy for biomedical NER on <a href=""https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocxml?pmcids=PMC6207735"" rel=""nofollow noreferrer"">CRAFT corpus</a>?</strong> </p>

<p>It is difficult for me to pre-process the <code>xml</code> files given in that corpus to any format used by <code>spacy</code>, any little help would be highly appreciated.
I first converted the <code>xml</code> files to <code>json</code> format but that was not accepted by <code>spacy</code>. <strong>What format of training data does <code>spacy</code> expect?</strong> I even tried to build my own <code>NER</code> model but was not able to pre-process the <code>xml</code> files as given in this  <a href=""https://towardsdatascience.com/parsing-xml-named-entity-recognition-in-one-shot-629a8b9846ee"" rel=""nofollow noreferrer"">article</a>.</p>

<p>Here is an example of training an NER model using spacy, including the expected format of training data (from <a href=""https://spacy.io/usage/training"" rel=""nofollow noreferrer"">spacy's docs</a>):</p>

<pre class=""lang-py prettyprint-override""><code>import random

import spacy


TRAIN_DATA = [
        (""Uber blew through $1 million a week"", {""entities"": [(0, 4, ""ORG"")]}),
        (""Google rebrands its business apps"", {""entities"": [(0, 6, ""ORG"")]})]

nlp = spacy.blank(""en"")
optimizer = nlp.begin_training()
for i in range(20):
    random.shuffle(TRAIN_DATA)
    for text, annotations in TRAIN_DATA:
        nlp.update([text], [annotations], sgd=optimizer)
nlp.to_disk(""/model"")
</code></pre>

<p>The XML file I am using is available online <a href=""https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocxml?pmcids=PMC6207735"" rel=""nofollow noreferrer"">here</a>. An example record looks like:</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;passage&gt;
&lt;infon key=""section_type""&gt;ABSTRACT&lt;/infon&gt;
&lt;infon key=""type""&gt;abstract&lt;/infon&gt;
&lt;offset&gt;141&lt;/offset&gt;
&lt;text&gt;
Breast cancer is the most frequent tumor in women, and in nearly two-thirds of cases, the tumors express estrogen receptor alpha (ERalpha, encoded by ESR1). Here, we performed whole-exome sequencing of 16 breast cancer tissues classified according to ESR1 expression and 12 samples of whole blood, and detected 310 somatic mutations in cancer tissues with high levels of ESR1 expression. Of the somatic mutations validated by a different deep sequencer, a novel nonsense somatic mutation, c.2830 C&gt;T; p.Gln944*, in transcriptional regulator switch-independent 3 family member A (SIN3A) was detected in breast cancer of a patient. Part of the mutant protein localized in the cytoplasm in contrast to the nuclear localization of ERalpha, and induced a significant increase in ESR1 mRNA. The SIN3A mutation obviously enhanced MCF7 cell proliferation. In tissue sections from the breast cancer patient with the SIN3A c.2830 C&gt;T mutation, cytoplasmic SIN3A localization was detected within the tumor regions where nuclear enlargement was observed. The reduction in SIN3A mRNA correlates with the recurrence of ER-positive breast cancers on Kaplan-Meier plots. These observations reveal that the SIN3A mutation has lost its transcriptional repression function due to its cytoplasmic localization, and that this repression may contribute to the progression of breast cancer.
&lt;/text&gt;
&lt;annotation id=""38""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""246"" length=""23""/&gt;
&lt;text&gt;estrogen receptor alpha&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""39""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""271"" length=""7""/&gt;
&lt;text&gt;ERalpha&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""40""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""291"" length=""4""/&gt;
&lt;text&gt;ESR1&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""41""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""392"" length=""4""/&gt;
&lt;text&gt;ESR1&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""42""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""512"" length=""4""/&gt;
&lt;text&gt;ESR1&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""43""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""720"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""44""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""868"" length=""7""/&gt;
&lt;text&gt;ERalpha&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""45""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""915"" length=""4""/&gt;
&lt;text&gt;ESR1&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""46""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""930"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""47""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""1048"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""48""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""1087"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""49""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""1201"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""50""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""1331"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""51""&gt;
&lt;infon key=""identifier""&gt;9606&lt;/infon&gt;
&lt;infon key=""type""&gt;Species&lt;/infon&gt;
&lt;location offset=""185"" length=""5""/&gt;
&lt;text&gt;women&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""52""&gt;
&lt;infon key=""identifier""&gt;9606&lt;/infon&gt;
&lt;infon key=""type""&gt;Species&lt;/infon&gt;
&lt;location offset=""762"" length=""7""/&gt;
&lt;text&gt;patient&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""53""&gt;
&lt;infon key=""identifier""&gt;9606&lt;/infon&gt;
&lt;infon key=""type""&gt;Species&lt;/infon&gt;
&lt;location offset=""1031"" length=""7""/&gt;
&lt;text&gt;patient&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""54""&gt;
&lt;infon key=""identifier""&gt;29278&lt;/infon&gt;
&lt;infon key=""type""&gt;Species&lt;/infon&gt;
&lt;location offset=""397"" length=""10""/&gt;
&lt;text&gt;expression&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""55""&gt;
&lt;infon key=""identifier""&gt;29278&lt;/infon&gt;
&lt;infon key=""type""&gt;Species&lt;/infon&gt;
&lt;location offset=""517"" length=""10""/&gt;
&lt;text&gt;expression&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""56""&gt;
&lt;infon key=""identifier""&gt;c.2830C&gt;T&lt;/infon&gt;
&lt;infon key=""type""&gt;DNAMutation&lt;/infon&gt;
&lt;location offset=""1054"" length=""10""/&gt;
&lt;text&gt;c.2830 C&gt;T&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""57""&gt;
&lt;infon key=""identifier""&gt;CVCL:0031&lt;/infon&gt;
&lt;infon key=""type""&gt;CellLine&lt;/infon&gt;
&lt;location offset=""964"" length=""4""/&gt;
&lt;text&gt;MCF7&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""58""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""1494"" length=""13""/&gt;
&lt;text&gt;breast cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""59""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""346"" length=""13""/&gt;
&lt;text&gt;breast cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""60""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""743"" length=""13""/&gt;
&lt;text&gt;breast cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""61""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""1017"" length=""13""/&gt;
&lt;text&gt;breast cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""62""&gt;
&lt;infon key=""identifier""&gt;MESH:D009369&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""477"" length=""6""/&gt;
&lt;text&gt;cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""63""&gt;
&lt;infon key=""identifier""&gt;p.Q944*&lt;/infon&gt;
&lt;infon key=""type""&gt;ProteinMutation&lt;/infon&gt;
&lt;location offset=""642"" length=""9""/&gt;
&lt;text&gt;p.Gln944*&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""64""&gt;
&lt;infon key=""identifier""&gt;MESH:D009369&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""1130"" length=""5""/&gt;
&lt;text&gt;tumor&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""65""&gt;
&lt;infon key=""identifier""&gt;MESH:D009369&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""176"" length=""5""/&gt;
&lt;text&gt;tumor&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""66""&gt;
&lt;infon key=""identifier""&gt;c.2830C&gt;T&lt;/infon&gt;
&lt;infon key=""type""&gt;DNAMutation&lt;/infon&gt;
&lt;location offset=""630"" length=""10""/&gt;
&lt;text&gt;c.2830 C&gt;T&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""67""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""1258"" length=""14""/&gt;
&lt;text&gt;breast cancers&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""68""&gt;
&lt;infon key=""identifier""&gt;MESH:D009369&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""231"" length=""6""/&gt;
&lt;text&gt;tumors&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""69""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""141"" length=""13""/&gt;
&lt;text&gt;Breast cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;/passage&gt;
</code></pre>
",Named Entity Recognition (NER),convert xml ner data craft corpus spacy json format build named entity recognition ner model using spacy biomedical ner craft corpus difficult pre process file given corpus format used little help would highly appreciated first converted file format wa accepted format training data doe expect even tried build model wa able pre process file given article example training ner model using spacy including expected format training data spacy doc xml file using available online example record look like
Why Spacy always recognising apple as an Organisation,"<p>In named entity recognition Spacy always recognizing apple as an organization, never recognizing as a fruit.</p>

<p>I tried the below code: </p>

<pre><code>import spacy
nlp=spacy.load('en_core_web_lg')

doc= nlp(u'Apple is my favourite fruit , it\'s cost around \u20B9 10 in India')
for token in doc:
    print(token,end='|')
print('\n....')

for ent in doc.ents:
    print(ent.text+'-'+ent.label_+'-'+str(spacy.explain(ent.label_)))
</code></pre>

<p>The output was : 
<a href=""https://i.sstatic.net/TdmEY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TdmEY.png"" alt=""![output image][1]][1""></a>
Why Spacy is not recognizing apple as fruit and ₹10 as monetary value.</p>

<p>Please help me on this.</p>

<p>Regards,</p>

<p>Venkat</p>
",Named Entity Recognition (NER),spacy always recognising apple organisation named entity recognition spacy always recognizing apple organization never recognizing fruit tried code output wa spacy recognizing apple fruit monetary value please help regard venkat
Meaning of &quot;drop&quot; in SpaCy custom NER model training?,"<p>Below code is an example training loop for SpaCy's named entity recognition(<code>NER</code>). </p>

<pre><code>for itn in range(100):
    random.shuffle(train_data)
    for raw_text, entity_offsets in train_data:
        doc = nlp.make_doc(raw_text)
        gold = GoldParse(doc, entities=entity_offsets)
        nlp.update([doc], [gold], drop=0.5, sgd=optimizer)
nlp.to_disk(""/model"")
</code></pre>

<p><code>drop</code>  as per <code>spacy</code> is the drop out rate. Can somebody explain the meaning of the same in detail?</p>
",Named Entity Recognition (NER),meaning drop spacy custom ner model training code example training loop spacy named entity recognition per drop rate somebody explain meaning detail
Why spacy forgetting old trained data and how to solve,"<p>I am trying to train a spacy model for ner. I have a data set with 2940 rows and i trained a base model let it's name be <code>current_model</code> with these data and i got another 10 distinct dataset each have rows ranging from 200 to 530 rows so i loaded my current_model using spacy's <code>spacy.load(""current_model"")</code> then i trained using my each dataset. and i tried to predict ner using <code>test data</code> it recognises ner in new dataset but it seems forgetting ner in oldest dataset. i did this to reduce training time. please see my code below to see the what have i tried to do</p>

<p><strong>Code for base model training</strong></p>

<pre><code>import spacy
from spacy.util import minibatch,compounding
import random
from pathlib import Path
from spacy import displacy
import re
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
imporcytoolz import partition_all
import os
from os import path
import shutil
import json


df = pd.read_csv(""new_annotations/dataset_transfer_learning1.csv"")
def populate_train_data(df):
train_data = []
i =0
for d_index, row in df.iterrows():
    print(row[""annotations""])
    content = row[""annotations""].replace(""\\n"", ""\n"").replace(""\n"", "" "")
    content = re.sub(r""(?&lt;=[:])(?=[^\s])"", r"" "", content)

# Finding tags and entities and store values in a entity list-----
soup = BeautifulSoup(content, ""html.parser"")
text = soup.get_text()
entities = []
for tag in soup.find_all():
    if tag.string is None:
        # failing silently for invalid tag
        print(f'Tagging is invalid: {row[""_id""], tag.name}, on row {i+2}skipping..')
        continue

tag_index = content.split(str(tag))[0].count(tag.string)
try:

for index, match in enumerate(re.finditer(tag.string.replace(""*"", "" ""), text)):

if index == tag_index:

entities.append((match.start(), match.end(), tag.name))

except Exception as e:

print(e, f""at line no {i+2}"")

continue

i += 1

if entities:

train_data.append((text, {""entities"": entities}))

return train_data



def train(training_data,old_training_data=None,model_name=None):

nlp = """"

pretrained_weights = Path('weights/model999.bin')

if model_name is not None:

nlp = spacy.load(model_name,weights=pretrained_weights)

else:

print(""no model specified using default model"")

nlp = spacy.load(""en_core_web_sm"")

if ""ner"" not in nlp.pipe_names:

print(""there is no ner creating ner"")

ner = nlp.create_pipe(""ner"")

nlp.add_pipe(ner,last=True)

else:

print(""there is ner"")

ner = nlp.get_pipe(""ner"")

for _,annotations in training_data:

for ent in annotations.get(""entities""):

ner.add_label(ent[2])

start_time = time.time()

if model_name is not None:

# nlp.resume_training()

# TRAINING_DATA = populate_train_data(pd.read_csv(old_training_data))

TRAINING_DATA = old_training_data

revision_data =[]

for doc in nlp.pipe(list(zip(*TRAINING_DATA))[0]):

tags = [w.tag_ for w in doc]

heads = [w.head.i for w in doc]

deps = [w.dep_ for w in doc]

entities = [(e.start_char, e.end_char, e.label_) for e in doc.ents]

revision_data.append((doc, GoldParse(doc, entities=entities)))

fine_tune_data = []

for raw_text, entity_offsets in training_data:

doc = nlp.make_doc(raw_text)

try:

gold = GoldParse(doc,entities=entity_offsets['entities'])

except ValueError:

pass

fine_tune_data.append((doc,gold))

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]

optimizer = nlp.entity.create_optimizer()

with nlp.disable_pipes(*other_pipes):

# pretrained_weights = Path('weights/model999.bin')

# with pretrained_weights.open(""rb"") as file_:

# ner.model.tok2vec.from_bytes(file_.read())

for i in range(20):

example_data = revision_data+fine_tune_data

# example_data = training_data

losses = {}

random.shuffle(example_data)

for batch in partition_all(2,example_data):

docs, golds = zip(*batch)

# print(docs, golds)

try:


nlp.update(docs,golds)

except ValueError:

pass

# print(losses)

else:

for i in range(20):

random.shuffle(training_data)

correct = 1

for text, annotations in training_data:

try:

nlp.update([text],[annotations])

print(correct)

correct +=1

except ValueError:

pass

# print(""skipping.."")

no_of_stars = i

print(""*""*no_of_stars)

end_time = time.time()

print(""this code took {}"".format(end_time - start_time))

return nlp


def save_to_directory(nlp,directory_name):

save_directory = directory_name

for directory in save_directory:

if directory is not None:

directory_full_path = Path(directory+""_""+datetime.today().strftime('%Y_%m_%d'))

if path.exists(directory_full_path):

shutil.rmtree(directory_full_path)

print(""folder already existed so removed"")

if not directory_full_path.exists():

directory_full_path.mkdir()

nlp.to_disk(directory_full_path)

print(""Saved model to output directory"",directory)



if __name__ == ""__main__"":

training_data = populate_train_data(df)


# training_data = [

# (""I Like Today and Evening"", {""entities"":[(7,12,""DAY""),(17,24,""DAY"")]}),

# (""Today is my lucky day"", {""entities"":[(1,5,""DAY"")]}),

# (""Yesterday and Today are two same days of a month"",{""entities"":[(14,19,""DAY"")]}),

# (""May Today is Best Day"",{""entities"":[(4,9,""DAY"")]}),

# (""Have a Nice Today and Every Day"",{""entities"":[(12,17,""DAY"")]}),

# (""Hey How are feeling Today"",{""entities"":[(20,25,""DAY"")]}),

# ]

# print(training_data)

nlp = train(training_data)

save_to_directory(nlp,[""trained_model_with_transfer_learning""])cytoolz import partition_all

import os

from os import path

import shutil

import json



df = pd.read_csv(""new_annotations/dataset_transfer_learning1.csv"")

def populate_train_data(df):

train_data = []

i =0

for d_index, row in df.iterrows():

print(row[""annotations""])

content = row[""annotations""].replace(""\\n"", ""\n"").replace(""\n"", "" "")

content = re.sub(r""(?&lt;=[:])(?=[^\s])"", r"" "", content)


# Finding tags and entities and store values in a entity list-----

soup = BeautifulSoup(content, ""html.parser"")

text = soup.get_text()

entities = []

for tag in soup.find_all():

if tag.string is None:

# failing silently for invalid tag

print(f'Tagging is invalid: {row[""_id""], tag.name}, on row {i+2}skipping..')

continue


tag_index = content.split(str(tag))[0].count(tag.string)

try:

for index, match in enumerate(re.finditer(tag.string.replace(""*"", "" ""), text)):

if index == tag_index:

entities.append((match.start(), match.end(), tag.name))

except Exception as e:

print(e, f""at line no {i+2}"")

continue

i += 1

if entities:

train_data.append((text, {""entities"": entities}))

return train_data



def train(training_data,old_training_data=None,model_name=None):

nlp = """"

pretrained_weights = Path('weights/model999.bin')

if model_name is not None:

nlp = spacy.load(model_name,weights=pretrained_weights)

else:

print(""no model specified using default model"")

nlp = spacy.load(""en_core_web_sm"")

if ""ner"" not in nlp.pipe_names:

print(""there is no ner creating ner"")

ner = nlp.create_pipe(""ner"")

nlp.add_pipe(ner,last=True)

else:

print(""there is ner"")

ner = nlp.get_pipe(""ner"")

for _,annotations in training_data:

for ent in annotations.get(""entities""):

ner.add_label(ent[2])

start_time = time.time()

if model_name is not None:

# nlp.resume_training()

# TRAINING_DATA = populate_train_data(pd.read_csv(old_training_data))

TRAINING_DATA = old_training_data

revision_data =[]

for doc in nlp.pipe(list(zip(*TRAINING_DATA))[0]):

tags = [w.tag_ for w in doc]

heads = [w.head.i for w in doc]

deps = [w.dep_ for w in doc]

entities = [(e.start_char, e.end_char, e.label_) for e in doc.ents]

revision_data.append((doc, GoldParse(doc, entities=entities)))

fine_tune_data = []

for raw_text, entity_offsets in training_data:

doc = nlp.make_doc(raw_text)

try:

gold = GoldParse(doc,entities=entity_offsets['entities'])

except ValueError:

pass

fine_tune_data.append((doc,gold))

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]

optimizer = nlp.entity.create_optimizer()

with nlp.disable_pipes(*other_pipes):

# pretrained_weights = Path('weights/model999.bin')

# with pretrained_weights.open(""rb"") as file_:

# ner.model.tok2vec.from_bytes(file_.read())

for i in range(20):

example_data = revision_data+fine_tune_data

# example_data = training_data

losses = {}

random.shuffle(example_data)

for batch in partition_all(2,example_data):

docs, golds = zip(*batch)

# print(docs, golds)

try:


nlp.update(docs,golds)

except ValueError:

pass

# print(losses)

else:

for i in range(20):

random.shuffle(training_data)

correct = 1

for text, annotations in training_data:

try:

nlp.update([text],[annotations])

print(correct)

correct +=1

except ValueError:

pass

# print(""skipping.."")

no_of_stars = i

print(""*""*no_of_stars)

end_time = time.time()

print(""this code took {}"".format(end_time - start_time))

return nlp


def save_to_directory(nlp,directory_name):

save_directory = directory_name

for directory in save_directory:

if directory is not None:

directory_full_path = Path(directory+""_""+datetime.today().strftime('%Y_%m_%d'))

if path.exists(directory_full_path):

shutil.rmtree(directory_full_path)

print(""folder already existed so removed"")

if not directory_full_path.exists():

directory_full_path.mkdir()

nlp.to_disk(directory_full_path)

print(""Saved model to output directory"",directory)



if __name__ == ""__main__"":

training_data = populate_train_data(df)


# training_data = [

# (""I Like Today and Evening"", {""entities"":[(7,12,""DAY""),(17,24,""DAY"")]}),

# (""Today is my lucky day"", {""entities"":[(1,5,""DAY"")]}),

# (""Yesterday and Today are two same days of a month"",{""entities"":[(14,19,""DAY"")]}),

# (""May Today is Best Day"",{""entities"":[(4,9,""DAY"")]}),

# (""Have a Nice Today and Every Day"",{""entities"":[(12,17,""DAY"")]}),

# (""Hey How are feeling Today"",{""entities"":[(20,25,""DAY"")]}),

# ]

# print(training_data)

nlp = train(training_data)

save_to_directory(nlp,[""trained_model_with_transfer_learning""])


#to do train using batched

#add drop rate


</code></pre>

<p><strong>Code for training with new dataset and save to another directory</strong></p>

<p>note: below code is written new file.</p>

<pre><code>
import spacy

from spacy import displacy

import pandas as pd

from annotations_training_spacy_31_oct_2019 import populate_train_data,train,save_to_directory



# test_texts = ""I Like Today and Evening""

# base_training_data = [

# (""I Like Today and Evening"", {""entities"":[(7,12,""DAY""),(17,24,""DAY"")]}),

# (""Today is my lucky day"", {""entities"":[(1,5,""DAY"")]}),

# (""Yesterday and Today are two same days of a month"",{""entities"":[(14,19,""DAY"")]}),

# (""May Today is Best Day"",{""entities"":[(4,9,""DAY"")]}),

# (""Have a Nice Today and Every Day"",{""entities"":[(12,17,""DAY"")]}),

# (""Hey How are feeling Today"",{""entities"":[(20,25,""DAY"")]}),

# ]

test_text = test_texts


# new_data_set = [

# (""Today is an Awsome Day"", {""entities"":[(1,5,""DAY"")]}),

# ]


nlp = train(training_data=new_data_set,old_training_data=base_training_data,model_name=""trained_model_with_transfer_learning_8_2019_12_05"")

save_to_directory(nlp,[""trained_model_with_transfer_learning_9""])


doc = nlp(test_text)

print(""ENTITIES in '%s'"" % test_text)

nlp.add_pipe(nlp.create_pipe('sentencizer'))

sentence = list(doc.sents)

for ent in doc.ents:

print(ent.label_,ent.text)



displacy.serve(sentence, style='ent')


</code></pre>

<p>as you can see i also tried to load old datasets tags. but still i have this problem</p>

<p>i know some peoples faced this problem please if anybody solved this problem help me.</p>

<p>thanks in advance for you help friends.</p>

<p>Hi,</p>

<p>I am trying to train a spacy model for ner. I have a data set with 2940 rows and i trained a base </p>

<p>model let it's name be <code>current_model</code> with these data and i got another 10 distinct dataset </p>

<p>each have rows ranging from 200 to 530 rows so i loaded my current_model using spacy's <code>spacy.load(""current_model"")</code> then i trained using my each dataset. and i tried to predict ner </p>

<p>using <code>test data</code> it recognises ner in new dataset but it seems forgetting ner in oldest dataset</p>

<p>i did this because to reduce training time. please see my code below to see the what have i tried </p>

<p>to do and i </p>

<p><strong>code for base model training</strong></p>

<pre><code>
import spacy

from spacy.util import minibatch,compounding

import random

from pathlib import Path

from spacy import displacy

import re

import pandas as pd

from bs4 import BeautifulSoup

from datetime import datetime

imporcytoolz import partition_all

import os

from os import path

import shutil

import json



df = pd.read_csv(""new_annotations/dataset_transfer_learning1.csv"")

def populate_train_data(df):

train_data = []

i =0

for d_index, row in df.iterrows():

print(row[""annotations""])

content = row[""annotations""].replace(""\\n"", ""\n"").replace(""\n"", "" "")

content = re.sub(r""(?&lt;=[:])(?=[^\s])"", r"" "", content)


# Finding tags and entities and store values in a entity list-----

soup = BeautifulSoup(content, ""html.parser"")

text = soup.get_text()

entities = []

for tag in soup.find_all():

if tag.string is None:

# failing silently for invalid tag

print(f'Tagging is invalid: {row[""_id""], tag.name}, on row {i+2}skipping..')

continue


tag_index = content.split(str(tag))[0].count(tag.string)

try:

for index, match in enumerate(re.finditer(tag.string.replace(""*"", "" ""), text)):

if index == tag_index:

entities.append((match.start(), match.end(), tag.name))

except Exception as e:

print(e, f""at line no {i+2}"")

continue

i += 1

if entities:

train_data.append((text, {""entities"": entities}))

return train_data



def train(training_data,old_training_data=None,model_name=None):

nlp = """"

pretrained_weights = Path('weights/model999.bin')

if model_name is not None:

nlp = spacy.load(model_name,weights=pretrained_weights)

else:

print(""no model specified using default model"")

nlp = spacy.load(""en_core_web_sm"")

if ""ner"" not in nlp.pipe_names:

print(""there is no ner creating ner"")

ner = nlp.create_pipe(""ner"")

nlp.add_pipe(ner,last=True)

else:

print(""there is ner"")

ner = nlp.get_pipe(""ner"")

for _,annotations in training_data:

for ent in annotations.get(""entities""):

ner.add_label(ent[2])

start_time = time.time()

if model_name is not None:

# nlp.resume_training()

# TRAINING_DATA = populate_train_data(pd.read_csv(old_training_data))

TRAINING_DATA = old_training_data

revision_data =[]

for doc in nlp.pipe(list(zip(*TRAINING_DATA))[0]):

tags = [w.tag_ for w in doc]

heads = [w.head.i for w in doc]

deps = [w.dep_ for w in doc]

entities = [(e.start_char, e.end_char, e.label_) for e in doc.ents]

revision_data.append((doc, GoldParse(doc, entities=entities)))

fine_tune_data = []

for raw_text, entity_offsets in training_data:

doc = nlp.make_doc(raw_text)

try:

gold = GoldParse(doc,entities=entity_offsets['entities'])

except ValueError:

pass

fine_tune_data.append((doc,gold))

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]

optimizer = nlp.entity.create_optimizer()

with nlp.disable_pipes(*other_pipes):

# pretrained_weights = Path('weights/model999.bin')

# with pretrained_weights.open(""rb"") as file_:

# ner.model.tok2vec.from_bytes(file_.read())

for i in range(20):

example_data = revision_data+fine_tune_data

# example_data = training_data

losses = {}

random.shuffle(example_data)

for batch in partition_all(2,example_data):

docs, golds = zip(*batch)

# print(docs, golds)

try:


nlp.update(docs,golds)

except ValueError:

pass

# print(losses)

else:

for i in range(20):

random.shuffle(training_data)

correct = 1

for text, annotations in training_data:

try:

nlp.update([text],[annotations])

print(correct)

correct +=1

except ValueError:

pass

# print(""skipping.."")

no_of_stars = i

print(""*""*no_of_stars)

end_time = time.time()

print(""this code took {}"".format(end_time - start_time))

return nlp


def save_to_directory(nlp,directory_name):

save_directory = directory_name

for directory in save_directory:

if directory is not None:

directory_full_path = Path(directory+""_""+datetime.today().strftime('%Y_%m_%d'))

if path.exists(directory_full_path):

shutil.rmtree(directory_full_path)

print(""folder already existed so removed"")

if not directory_full_path.exists():

directory_full_path.mkdir()

nlp.to_disk(directory_full_path)

print(""Saved model to output directory"",directory)



if __name__ == ""__main__"":

training_data = populate_train_data(df)


# training_data = [

# (""I Like Today and Evening"", {""entities"":[(7,12,""DAY""),(17,24,""DAY"")]}),

# (""Today is my lucky day"", {""entities"":[(1,5,""DAY"")]}),

# (""Yesterday and Today are two same days of a month"",{""entities"":[(14,19,""DAY"")]}),

# (""May Today is Best Day"",{""entities"":[(4,9,""DAY"")]}),

# (""Have a Nice Today and Every Day"",{""entities"":[(12,17,""DAY"")]}),

# (""Hey How are feeling Today"",{""entities"":[(20,25,""DAY"")]}),

# ]

# print(training_data)

nlp = train(training_data)

save_to_directory(nlp,[""trained_model_with_transfer_learning""])cytoolz import partition_all

import os

from os import path

import shutil

import json



df = pd.read_csv(""new_annotations/dataset_transfer_learning1.csv"")

def populate_train_data(df):

train_data = []

i =0

for d_index, row in df.iterrows():

print(row[""annotations""])

content = row[""annotations""].replace(""\\n"", ""\n"").replace(""\n"", "" "")

content = re.sub(r""(?&lt;=[:])(?=[^\s])"", r"" "", content)


# Finding tags and entities and store values in a entity list-----

soup = BeautifulSoup(content, ""html.parser"")

text = soup.get_text()

entities = []

for tag in soup.find_all():

if tag.string is None:

# failing silently for invalid tag

print(f'Tagging is invalid: {row[""_id""], tag.name}, on row {i+2}skipping..')

continue


tag_index = content.split(str(tag))[0].count(tag.string)

try:

for index, match in enumerate(re.finditer(tag.string.replace(""*"", "" ""), text)):

if index == tag_index:

entities.append((match.start(), match.end(), tag.name))

except Exception as e:

print(e, f""at line no {i+2}"")

continue

i += 1

if entities:

train_data.append((text, {""entities"": entities}))

return train_data



def train(training_data,old_training_data=None,model_name=None):

nlp = """"

pretrained_weights = Path('weights/model999.bin')

if model_name is not None:

nlp = spacy.load(model_name,weights=pretrained_weights)

else:

print(""no model specified using default model"")

nlp = spacy.load(""en_core_web_sm"")

if ""ner"" not in nlp.pipe_names:

print(""there is no ner creating ner"")

ner = nlp.create_pipe(""ner"")

nlp.add_pipe(ner,last=True)

else:

print(""there is ner"")

ner = nlp.get_pipe(""ner"")

for _,annotations in training_data:

for ent in annotations.get(""entities""):

ner.add_label(ent[2])

start_time = time.time()

if model_name is not None:

# nlp.resume_training()

# TRAINING_DATA = populate_train_data(pd.read_csv(old_training_data))

TRAINING_DATA = old_training_data

revision_data =[]

for doc in nlp.pipe(list(zip(*TRAINING_DATA))[0]):

tags = [w.tag_ for w in doc]

heads = [w.head.i for w in doc]

deps = [w.dep_ for w in doc]

entities = [(e.start_char, e.end_char, e.label_) for e in doc.ents]

revision_data.append((doc, GoldParse(doc, entities=entities)))

fine_tune_data = []

for raw_text, entity_offsets in training_data:

doc = nlp.make_doc(raw_text)

try:

gold = GoldParse(doc,entities=entity_offsets['entities'])

except ValueError:

pass

fine_tune_data.append((doc,gold))

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]

optimizer = nlp.entity.create_optimizer()

with nlp.disable_pipes(*other_pipes):

# pretrained_weights = Path('weights/model999.bin')

# with pretrained_weights.open(""rb"") as file_:

# ner.model.tok2vec.from_bytes(file_.read())

for i in range(20):

example_data = revision_data+fine_tune_data

# example_data = training_data

losses = {}

random.shuffle(example_data)

for batch in partition_all(2,example_data):

docs, golds = zip(*batch)

# print(docs, golds)

try:


nlp.update(docs,golds)

except ValueError:

pass

# print(losses)

else:

for i in range(20):

random.shuffle(training_data)

correct = 1

for text, annotations in training_data:

try:

nlp.update([text],[annotations])

print(correct)

correct +=1

except ValueError:

pass

# print(""skipping.."")

no_of_stars = i

print(""*""*no_of_stars)

end_time = time.time()

print(""this code took {}"".format(end_time - start_time))

return nlp


def save_to_directory(nlp,directory_name):

save_directory = directory_name

for directory in save_directory:

if directory is not None:

directory_full_path = Path(directory+""_""+datetime.today().strftime('%Y_%m_%d'))

if path.exists(directory_full_path):

shutil.rmtree(directory_full_path)

print(""folder already existed so removed"")

if not directory_full_path.exists():

directory_full_path.mkdir()

nlp.to_disk(directory_full_path)

print(""Saved model to output directory"",directory)



if __name__ == ""__main__"":

training_data = populate_train_data(df)


# training_data = [

# (""I Like Today and Evening"", {""entities"":[(7,12,""DAY""),(17,24,""DAY"")]}),

# (""Today is my lucky day"", {""entities"":[(1,5,""DAY"")]}),

# (""Yesterday and Today are two same days of a month"",{""entities"":[(14,19,""DAY"")]}),

# (""May Today is Best Day"",{""entities"":[(4,9,""DAY"")]}),

# (""Have a Nice Today and Every Day"",{""entities"":[(12,17,""DAY"")]}),

# (""Hey How are feeling Today"",{""entities"":[(20,25,""DAY"")]}),

# ]

# print(training_data)

nlp = train(training_data)

save_to_directory(nlp,[""trained_model_with_transfer_learning""])


#to do train using batched

#add drop rate


</code></pre>

<p><strong>Code for training with new dataset and save to another directory</strong></p>

<p>note: below code is written new file.</p>

<pre><code>
import spacy

from spacy import displacy

import pandas as pd

from annotations_training_spacy_31_oct_2019 import populate_train_data,train,save_to_directory



# test_texts = ""I Like Today and Evening""

# base_training_data = [

# (""I Like Today and Evening"", {""entities"":[(7,12,""DAY""),(17,24,""DAY"")]}),

# (""Today is my lucky day"", {""entities"":[(1,5,""DAY"")]}),

# (""Yesterday and Today are two same days of a month"",{""entities"":[(14,19,""DAY"")]}),

# (""May Today is Best Day"",{""entities"":[(4,9,""DAY"")]}),

# (""Have a Nice Today and Every Day"",{""entities"":[(12,17,""DAY"")]}),

# (""Hey How are feeling Today"",{""entities"":[(20,25,""DAY"")]}),

# ]

test_text = test_texts


# new_data_set = [

# (""Today is an Awsome Day"", {""entities"":[(1,5,""DAY"")]}),

# ]


nlp = train(training_data=new_data_set,old_training_data=base_training_data,model_name=""trained_model_with_transfer_learning_8_2019_12_05"")

save_to_directory(nlp,[""trained_model_with_transfer_learning_9""])


doc = nlp(test_text)

print(""ENTITIES in '%s'"" % test_text)

nlp.add_pipe(nlp.create_pipe('sentencizer'))

sentence = list(doc.sents)

for ent in doc.ents:

print(ent.label_,ent.text)



displacy.serve(sentence, style='ent')


</code></pre>

<p>as you can see i also tried to load old datasets tags. but still i have this problem</p>

<p>i know some peoples faced this problem please if anybody solved this problem help me.</p>

<p>thanks in advance for you help friends.</p>
",Named Entity Recognition (NER),spacy forgetting old trained data solve trying train spacy model ner data set row trained base model let name data got another distinct dataset row ranging row loaded current model using spacy trained using dataset tried predict ner using recognises ner new dataset seems forgetting ner oldest dataset reduce training time please see code see tried code base model training code training new dataset save another directory note code written new file see also tried load old datasets tag still problem know people faced problem please anybody solved problem help thanks advance help friend hi trying train spacy model ner data set row trained base model let name data got another distinct dataset row ranging row loaded current model using spacy trained using dataset tried predict ner using recognises ner new dataset seems forgetting ner oldest dataset reduce training time please see code see tried code base model training code training new dataset save another directory note code written new file see also tried load old datasets tag still problem know people faced problem please anybody solved problem help thanks advance help friend
How to determine relationship between two entities when there is more than one relation while creating distant supervision training data?,"<p>I got the concepts of distant supervision. As for my understanding, the creating training data process is like;</p>

<ul>
<li>Extract named entities from sentences </li>
<li>Find two entities named ""e1"" and ""e2"" from each sentence. </li>
<li>Search these two entities in knowledge base (freebase etc.) to find relationship between them</li>
</ul>

<p>I got confused at this step. What if there is more than 1 relation between these two entities (e1 and e2) ? If so which relation should I select?</p>
",Named Entity Recognition (NER),determine relationship two entity one relation creating distant supervision training data got concept distant supervision understanding creating training data process like extract named entity sentence find two entity named e e sentence search two entity knowledge base freebase etc find relationship got confused step relation two entity e e relation select
Extracting Proper Lake Names from Text in R,"<p>I am trying to extract the names of Lakes from some text that I have in R. The lakes are proper (capitalized) but will require me extracting a few words on either side of the word ""Lake"". </p>

<p>I tried a few things but nothing is working quite the way I want it to... in some cases, a sentence or the article may begin with ""Lake"" so there is no text before it. In some cases, the proper name may be 3 words (Lake St. Clair or Red Hawk Lake). </p>

<p>Example code to work with:</p>

<pre><code>text &lt;- paste(""Lake Erie is located on the border of the United States and Canada."",
          ""It is located nearby to Lake St. Clair and Lake Michigan."",
          ""All three lakes have a history of high levels of Phosphorus."",
          ""One lake that has not yet been impacted is Lake Ontario."")
</code></pre>

<p>This was maybe the closest I got-- pulling from another stack overflow but it's still not working out. </p>

<pre><code>context &lt;- function(text){splittedText &lt;-strsplit(text,'',T) print(splitted Text) data.frame(before = head(c('',splittedText),-1),words=splittedText,after=tail(c(splittedText,''),-1))}

info &lt;- context(text)
print(subset(info, words == 'Lake')
</code></pre>

<p>I would like to get either: 1) the proper lakes names extracted (""Lake Erie"", ""Lake St. Clair"", etc.) OR 2) a dataframe with the words before and after ""Lake"". Ideally the first but I'm flexible at this point.</p>

<pre><code>before &lt;- c("""",""nearby to"", ""Clair and"",""impacted is"")
Lake &lt;- c(""Lake"",""Lake"",""Lake"",""Lake"")
after &lt;- c(""Erie is"",""St. Clair"", ""Michigan "",""Ontario "")
output &lt;- data.frame(cbind(before,Lake,after)); print(output)
</code></pre>

<p>Thanks in advance for the help!</p>
",Named Entity Recognition (NER),extracting proper lake name text r trying extract name lake text r lake proper capitalized require extracting word either side word lake tried thing nothing working quite way want case sentence article may begin lake text case proper name may word lake st clair red hawk lake example code work wa maybe closest got pulling another stack overflow still working would like get either proper lake name extracted lake erie lake st clair etc dataframe word lake ideally first flexible point thanks advance help
What&#39;s the ideal way to include dictionaries (gazetteer) in spaCy to improve NER?,"<p>I'm currently working on replacing a system based on nltk entity extraction combined with regexp matching where I have several named entity dictionaries. The dictionary entities are both of common type (PERSON (employees) etc.) as well as custom types (e.g. SKILL). I want to use the pre-trained spaCy model and include my dictionaries somehow, to increase the NER accuracy. Here are my thoughts on possible methods:</p>

<ul>
<li><p>Use spaCy's Matcher API, iterate through the dictionary and add each phrase with a callback to add the entity?</p></li>
<li><p>I've just found spacy-lookup, which seems like an easy way to provide long lists of words/phrases to match.</p></li>
<li><p>But what if I want to have fuzzy matching? Is there a way to add directly to the Vocab and thus have some fuzzy matching through Bloom filter / n-gram word vectors, or is there some extension out there that suits this need? Otherwise I guess I could copy spacy-lookup and replace the flashtext machinery with something else, e.g. Levenshtein distance.</p></li>
<li><p>While playing around with spaCy I did try just training the NER directly with a single word from the dictionary (without any sentence context), and this did ""work"". But I would, of course, have to take much care to keep the model from forgetting everything. </p></li>
</ul>

<p>Any help appreciated, I feel like this must be a pretty common requirement and would love to hear what's working best for people out there. </p>
",Named Entity Recognition (NER),ideal way include dictionary gazetteer spacy improve ner currently working replacing system based nltk entity extraction combined regexp matching several named entity dictionary dictionary entity common type person employee etc well custom type e g skill want use pre trained spacy model include dictionary somehow increase ner accuracy thought possible method use spacy matcher api iterate dictionary add phrase callback add entity found spacy lookup seems like easy way provide long list word phrase match want fuzzy matching way add directly vocab thus fuzzy matching bloom filter n gram word vector extension suit need otherwise guess could copy spacy lookup replace flashtext machinery something else e g levenshtein distance playing around spacy try training ner directly single word dictionary without sentence context work would course take much care keep model forgetting everything help appreciated feel like must pretty common requirement would love hear working best people
How to make Microsoft LUIS case sensitive?,"<p>I have a Azure LUIS instance for NLP, 
tried to extract Alphanumberic values using RegEx Expression. it worked well but the output had output in lowercase alphabets. </p>

<p>For example: </p>

<p><strong>CASE 1*</strong></p>

<p>My Input: "" run job for AE0002"" <code>RegExCode = [a-zA-Z]{2}\d+</code></p>

<p>Output: </p>

<pre><code>{
  ""query"": "" run job for AE0002"",
  ""topScoringIntent"": {
    ""intent"": ""Run Job"",
    ""score"": 0.7897274
  },
  ""intents"": [
    {
      ""intent"": ""Run Job"",
      ""score"": 0.7897274
    },
    {
      ""intent"": ""None"",
      ""score"": 0.00434472738
    }
  ],
  ""entities"": [
    {
      ""entity"": ""ae0002"",
      ""type"": ""Alpha Number"",
      ""startIndex"": 15,
      ""endIndex"": 20
    }
  ]
} 
</code></pre>

<p>I need to maintain the case of the input.</p>

<p><strong><em>CASE 2</em></strong></p>

<p>My Input : ""Extract only abreaviations like HP and IBM""   <code>RegExCode = [A-Z]{2,}</code></p>

<p>Output : </p>

<pre><code>{
  ""query"": ""extract only abreaviations like hp and ibm"", // Query accepted by LUIS test window
  ""query"": ""extract only abreaviations like HP and IBM"", // Query accepted as an endpoint url
  ""prediction"": {
    ""normalizedQuery"": ""extract only abreaviations like hp and ibm"",
    ""topIntent"": ""None"",
    ""intents"": {
      ""None"": {
        ""score"": 0.09844558
      }
    },
    ""entities"": {
      ""Abbre"": [
        ""extract"",
        ""only"",
        ""abreaviations"",
        ""like"",
        ""hp"",
        ""and"",
        ""ibm""
      ],
      ""$instance"": {
        ""Abbre"": [
          {
            ""type"": ""Abbre"",
            ""text"": ""extract"",
            ""startIndex"": 0,
            ""length"": 7,
            ""modelTypeId"": 8,
            ""modelType"": ""Regex Entity Extractor"",
            ""recognitionSources"": [
              ""model""
            ]
          },
          {
            ""type"": ""Abbre"",
            ""text"": ""only"",
            ""startIndex"": 8,
            ""length"": 4,
            ""modelTypeId"": 8,
            ""modelType"": ""Regex Entity Extractor"",
            ""recognitionSources"": [
              ""model""
            ]
          },....          
          {
            ""type"": ""Abbre"",
            ""text"": ""ibm"",
            ""startIndex"": 39,
            ""length"": 3,
            ""modelTypeId"": 8,
            ""modelType"": ""Regex Entity Extractor"",
            ""recognitionSources"": [
              ""model""
            ]
          }
        ]
      }
    }
  }
}
</code></pre>

<p>This makes me doubt if the entire training is happening in lowercase, What shocked me was all the words that were trained initially to their respective entities were retrained as <strong><em>Abbre</em></strong></p>

<p>Any input would be of great help :) </p>

<p><em>Thank you</em></p>
",Named Entity Recognition (NER),make microsoft luis case sensitive azure luis instance nlp tried extract alphanumberic value using regex expression worked well output output lowercase alphabet example case input run job ae output need maintain case input case input extract abreaviations like hp ibm output make doubt entire training happening lowercase shocked wa word trained initially respective entity retrained abbre input would great help thank
Linear CRF Versus Word2Vec for NER,"<p>I have done lots of reading around Linear CRF and Word2Vec and wanted to know which one is the best to do Named Entity Recognition. I trained my model using Stanford NER(Which is a Linear CRF Implementation) and got a precision of 85%. I know that Word2vec groups similar words together but is it a good model to do NER?</p>
",Named Entity Recognition (NER),linear crf versus word vec ner done lot reading around linear crf word vec wanted know one best named entity recognition trained model using stanford ner linear crf implementation got precision know word vec group similar word together good model ner
NLP - Speed of Named Entity Recognition (StanfordNER),"<p>I'm doing Natural Language Processing (NLP) in Python 3 and more specifically Named Entity Recognition (NER) on the Harry Potter set of books. I'm using StanfordNER, which works pretty well but takes incredible amounts of time...</p>

<p>I have done some research online on why it would be this slow but I can't seem to find anything that truly suits my code, and I honestly think the problem lays more in the (bad) way I have written the code.</p>

<p>So here's what I wrote for now :</p>

<pre><code>import string
from nltk.tokenize import sent_tokenize, word_tokenize
import nltk.tag.stanford as st

tagger = st.StanfordNERTagger('_path_/stanford-ner-2017-06-09/classifiers/english.all.3class.distsim.crf.ser.gz', '_path_/stanford-ner-2017-06-09/stanford-ner.jar')

#this is just to read the file

hp = open(""books/hp1.txt"", 'r', encoding='utf8')
lhp = hp.readlines()

#a small function I wrote to divide the book in sentences

def get_sentences(lbook):
    sentences = []
    for k in lbook:
        j = sent_tokenize(k)
        for i in j:
            if bool(i):
                sentences.append(i)
    return sentences

#a function to divide a sentence into words

def get_words(sentence):
    words = word_tokenize(sentence)
    return words

sentences = get_sentences(lhp)

#and now the code I wrote to get all the words labeled as PERSON by the StanfordNER tagger

characters = []
    for i in sentence:
    characters = [tag[0] for tag in tagger.tag(get_words(sentences[i])) if tag[1]==""PERSON""]
    print(characters)
</code></pre>

<p>Now the problem, as I explained, is that the code takes huge amounts of time... So I'm wondering, is it normal or <strong>can I save time by rewriting the code in a better way ?</strong> If so, could you help me out ?</p>
",Named Entity Recognition (NER),nlp speed named entity recognition stanfordner natural language processing nlp python specifically named entity recognition ner harry potter set book using stanfordner work pretty well take incredible amount time done research online would slow seem find anything truly suit code honestly think problem lay bad way written code wrote problem explained code take huge amount time wondering normal save time rewriting code better way could help
Longest match only with Spacy Phrasematcher,"<p>I have created a <a href=""https://spacy.io/api/phrasematcher"" rel=""nofollow noreferrer"">Spacy Phrasematcher</a> to match names in a document, following the <a href=""https://spacy.io/usage/rule-based-matching#phrasematcher"" rel=""nofollow noreferrer"">tutorial</a>. I want to use the resulting matches as additional training data in order to train a Spacy NER model.
My patterns, however, contain both full names (e.g. 'Barack Obama') and last names ('Obama') separately.</p>

<p>Hence, in a sentence that contains 'Barack Obama', both patterns match, resulting in overlapping matches. This overlap, however, triggers an exception when I try to use the data for training, e.g.:</p>

<pre><code>ValueError: [E103] Trying to set conflicting doc.ents: '(19, 33, 'PERSON')' and '(29, 33, 'PERSON')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap.
</code></pre>

<p>I've been considering to filter out overlapping matches before using the data for training, but this seems like a very inefficient approach, resulting in a significant increase in processing time for large data.</p>

<p>Is there a way to set up a <code>PhraseMatcher</code> so that it only matches the longest match for overlapping matches?</p>
",Named Entity Recognition (NER),longest match spacy phrasematcher created spacy phrasematcher match name document following tutorial want use resulting match additional training data order train spacy ner model pattern however contain full name e g barack obama last name obama separately hence sentence contains barack obama pattern match resulting overlapping match overlap however trigger exception try use data training e g considering filter overlapping match using data training seems like inefficient approach resulting significant increase processing time large data way set match longest match overlapping match
Interpret / Extract location values from text in JavaScript - NodeJS,"<p>I have a string: ""I would like to ship a sofa to Heathrow weighing 60lB from LAX by the 29th of November with AWB 12381140743""</p>

<p>I pass the above string to my Google NLP API and it spits out a list of json objects.</p>

<p>Looking for the tag LOCATION I am able to extract the location from the JSON object, 
however if I don't look for the previous word (i.e. to, from), I can't understand which one is the origin and which one is the destination.
Furthermore if values are inverted (i.e. from - to) the current code is not able to identify the origin and destination correctly.</p>

<pre><code>entities.forEach(element =&gt; {
        if (element.type === 'LOCATION') {

            const index = getTokenElementIndex(tokens, element.name);
            const wordBefore = getPreviousWord(tokens, index);

            if (wordBefore === 'from') {
                origin = element.name;
            } else if (wordBefore === 'to') {
                destination = element.name;
            }
        }
    });
</code></pre>

<p>Is there a better way to do it? Any suggestion or libraries that I can use? Any thoughts on how can I improve my code to better understand the text context?</p>
",Named Entity Recognition (NER),interpret extract location value text javascript nodejs string would like ship sofa heathrow weighing lb lax th november awb pas string google nlp api spit list json object looking tag location able extract location json object however look previous word e understand one origin one destination furthermore value inverted e current code able identify origin destination correctly better way suggestion library use thought improve code better understand text context
Process of performing NER (Named Enitity Recognition) - NLP,"<p>So I have texts that look like the one below:</p>

<blockquote>
  <p>He also may have
  recurrent seizures which should be treated with ativan IV or IM
  and do not neccessarily indicate patient needs to return to
  hospital unless they continue for greater than 5 minutes or he
  has multiple recurrent seizures or complications such as
  aspiration.</p>
</blockquote>

<p>and also annotation files which are like:</p>

<blockquote>
  <p>T1    Reason 16 33 recurrent seizures</p>
</blockquote>

<p>The above annotation tells the ID of the entity, the span (character position) and the entity itself. My goal is to do NER (Named Entity Recongnition) on the above data. From my research I know that I have to do BIO (Beginning, Inside and Outside) tagging on the data which will make my data look as follows:</p>

<p><code>
O - also 
O - may 
O - have
B - recurrent 
I - seizures
</code></p>

<p>After the BIO tagging I want to use the data to get some word embeddings and input it to a classifier which will let me get the Entity types with the test data. </p>

<p>Is the process outline that I gave right or can anyone please explain how I can go about this problem?</p>
",Named Entity Recognition (NER),process performing ner named enitity recognition nlp text look like one also may recurrent seizure treated ativan iv im neccessarily indicate patient need return hospital unless continue greater minute ha multiple recurrent seizure complication aspiration also annotation file like reason recurrent seizure annotation tell id entity span character position entity goal ner named entity recongnition data research know bio beginning inside outside tagging data make data look follows bio tagging want use data get word embeddings input classifier let get entity type test data process outline gave right anyone please explain go problem
Token sequence labeling,"<p>I have a task which is half a matcher and half an entity extraction. I want to label some words that in some contexts refer to some label. Named entity extraction would be the way to go, but these words do not necessarily share structure (they can be verbs, nouns... etc). I could simply use a dictionary, but I would like to use context to label them. I am having trouble finding a solution to this problem. Can NER be used for this or is this a completely different task?</p>

<p>To give some examples, consider that I am interested in the following category ""customer acceptance"". Now, these can be possible sentences: ""this is a fair amount of data!"" and ""this condition is not fair"". I want my word extractor to find only the second 'fair'.</p>

<p>In other words, it is like a dictionary that takes context into account.</p>
",Named Entity Recognition (NER),token sequence labeling task half matcher half entity extraction want label word context refer label named entity extraction would way go word necessarily share structure verb noun etc could simply use dictionary would like use context label trouble finding solution problem ner used completely different task give example consider interested following category customer acceptance possible sentence fair amount data condition fair want word extractor find second fair word like dictionary take context account
NER training using Spacy,"<p>When running a train on an empty NER model, should I include only labeled data (data that contain necessarily at least one entity), or should I also include data that do not contain any label at all (in this case, teaching the model that in some circunstances these words do not have any label)?</p>
",Named Entity Recognition (NER),ner training using spacy running train empty ner model include labeled data data contain necessarily least one entity also include data contain label case teaching model circunstances word label
How to extract sender name from emails containing multiple human names?,"<p>Suppose I have a set of emails which contains human names. I managed to extract named entity using natural language processing. But I want to specifically extract sender name alone from this set of name. What is the standard method for doing this?</p>
",Named Entity Recognition (NER),extract sender name email containing multiple human name suppose set email contains human name managed extract named entity using natural language processing want specifically extract sender name alone set name standard method
Extract entities from text using Knowledge Bases in Python,"<p>I have an entity extraction tasks which needs KBs like wikidata, freebase, DBpedia. Given the huge size of them, it is hard to download and extract entities from them. Is there a python client which can make API calls to get the extractions through them with unstructured text as input?</p>
",Named Entity Recognition (NER),extract entity text using knowledge base python entity extraction task need kb like wikidata freebase dbpedia given huge size hard download extract entity python client make api call get extraction unstructured text input
Train NER SpaCy using en_trf_bertbaseuncased_lg model,"<p>I am currently working on NER project and I would like to improve my NER performance by trying new SpaCy model <code>en_trf_bertbaseuncased_lg</code> but it gave me error <code>KeyError: ""[E001] No component 'trf_tok2vec' found in pipeline. Available names: ['ner']""</code>. Is it that SpaCy currently does not support NER for this language model? Thanks!</p>

<pre><code>   # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        for itn in tqdm(range(n_iter)):
            random.shuffle(train_data_list)
            losses = {}
            # batch up the examples using spaCy's minibatch
            batches = minibatch(train_data_list, size=compounding(8., 64., 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(texts, annotations, sgd=optimizer, drop=0.35,
                           losses=losses)
            tqdm.write('Iter: ' + str(itn + 1) + ' Losses: ' + str(losses['ner']))
            if itn == 30 or itn == 40:
                output_dir = Path(output_dir)
                if not output_dir.exists():
                    output_dir.mkdir()
                nlp.to_disk(Path(output_dir))
</code></pre>

<p>It gave error on </p>

<pre><code>nlp.update(texts, annotations, sgd=optimizer, drop=0.35,
                           losses=losses)
</code></pre>
",Named Entity Recognition (NER),train ner spacy using en trf bertbaseuncased lg model currently working ner project would like improve ner performance trying new spacy model gave error spacy currently doe support ner language model thanks gave error
How to train Stanford NLP NER Extraction model to skip the repeating words?,"<p>I am trying to extract the NER from the text using <strong>.NET Framework</strong> and <strong>StanFord NER Model</strong>.
I have a text like </p>

<p>Hello, I am John Doe. Body Mass index is 27. And Body Surface Area is 2.3m.</p>

<p>For this i did create tsv file to train the model. Which is as under:</p>

<pre><code>Hello   O
,   O
I   O
am  O
John    PERSON
Doe.    PERSON
Body    BMI
Mass    BMI
index   BMI
is  O
27. O
And O
Body    O
Surface O
Area    O
is  O
2.3m.   O
</code></pre>

<p>prop file is as under</p>

<pre><code>trainFileList = train/standford_train.tsv
serializeTo = dummy-ner-model-eng.ser.gz
map = word=0,answer=1

useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
useDisjunctive=true
</code></pre>

<p>and using below java command</p>

<pre><code>java -mx1g -cp stanford-ner.jar;lib/* edu.stanford.nlp.ie.crf.CRFClassifier -annotators 'tokenize,ssplit,pos,lemma,ner,regexner' -prop train/prop.txt
</code></pre>

<p>So, the problem i am facing is Body with tagging BMI is coming two times because of repetition in <strong>Body Mass Index</strong> and <strong>Body Surface Area</strong>.</p>

<p>Is there any way that i can omit this second body tag?</p>
",Named Entity Recognition (NER),train stanford nlp ner extraction model skip repeating word trying extract ner text using net framework stanford ner model text like hello john doe body mass index body surface area create tsv file train model prop file using java command problem facing body tagging bmi coming two time repetition body mass index body surface area way omit second body tag
How to extract limited lines of data from specific keyword using python,"<p>I have a text file where I need to extract first five lines ones a specified keyword occurs in the paragraph.</p>

<p>I am able to find keywords but not able to write next five lines from that keyword.</p>

<pre class=""lang-py prettyprint-override""><code>mylines = []                              

with open ('D:\\Tasks\\Task_20\\txt\\CV (4).txt', 'rt') as myfile:  

    for line in myfile:                   

        mylines.append(line)             

    for element in mylines:               

        print(element, end='')  

print(mylines[0].find(""P""))
</code></pre>

<p>Please help if anybody have any idea on how to do so.</p>

<p>Input Text File Example:-</p>

<p>Philippine Partner Agency: ALL POWER STAFFING SOLUTIONS, INC.</p>

<p>Training Objectives: : To have international cultural exposure and hands-on experience in the field
of hospitality management as a gateway to a meaningful hospitality career. To develop my hospitality
management skills and become globally competitive.</p>

<p>Education
Institution Name: SOUTHVILLE FOREIGN UNIVERSITY - PHILIPPINES
Location Hom as Pinas City, Philippine Institution start date: (June 2007</p>

<p>Required Output:-</p>

<p>Training Objectives: : To have international cultural exposure and hands-on experience in the field
of hospitality management as a gateway to a meaningful hospitality career. To develop my hospitality
management skills and become globally competitive.</p>

#

<p>I have to search Training Objective Keyword in text file and ones it find that it should write next 5 lines only.</p>
",Named Entity Recognition (NER),extract limited line data specific keyword using python text file need extract first five line one specified keyword occurs paragraph able find keywords able write next five line keyword please help anybody idea input text file example philippine partner agency power staffing solution inc training objective international exposure hand experience field hospitality management gateway meaningful hospitality career develop hospitality management skill become globally competitive education institution name southville foreign university philippine location hom pinas city philippine institution start date june required output training objective international exposure hand experience field hospitality management gateway meaningful hospitality career develop hospitality management skill become globally competitive search training objective keyword text file one find write next line
Is there a python Regex to find street name followed by one or multiple persons followed by house number?,"<p>I have an image dataset that I am extracting text data from. I have the text as a string but now want to separate this text into a more structured form. </p>

<p>The data looks like this:</p>

<pre><code>Camden Row,Camberwell, S.E—A. Massey, M.D.4.

Campden Hill, Kensington.
(Hornton House).

Campden Hill Road, Kensington.
James, M.D. 6.

Canning Town. E—R. J. Carey (Widdicombe-
co ee

Cannon Street. E.C.—R. Cresswell, 151.

Cannon Street Road. E.—R. W. Lammiman, 106.
—J. R. Morrison, 57.—B. R. Rygate, 126.—
J. J. Rygate, M.B. 126.

Canonbury N. (see foot note)—J. Cheetham, M.D.
(Springjield House),

Canonbury Lane, N.—H. Bateman,
Roberts, 10.—J. Rose, 3.
</code></pre>

<p>As you can see it involves a street name followed by a letter representing (N/S/E/W/NW/SE etc.) and then a persons name and house number.</p>

<p>So far I have been using python NLTK. I am able to extract streets, names and numbers as individual entities using:</p>

<pre><code>tagged = nltk.pos_tag(tokens)
</code></pre>

<p>What I would like to achieve is a list of:</p>

<pre><code>[street name, person, house_number]
</code></pre>

<p>For example:</p>

<pre><code>[[Cannon Street Road, R. W. Lammiman, 106][Cannon Street Road, J. R. Morrison, 57]]
</code></pre>

<p>My plan was to use the street names as an anchor for the start and then the digit for an anchor at the end but this is a bit more complicated due to multiple house numbers on each street.</p>

<p>Can anyone suggest a method/regex that might work for this?</p>

<p>Thank you kindly if so!
James.</p>
",Named Entity Recognition (NER),python regex find street name followed one multiple person followed house number image dataset extracting text data text string want separate text structured form data look like see involves street name followed letter representing n e w nw se etc person name house number far using python nltk able extract street name number individual entity using would like achieve list example plan wa use street name anchor start digit anchor end bit complicated due multiple house number street anyone suggest method regex might work thank kindly james
Why is spacy failing at tokenizing a particular quotation mark?,"<p>I am running spacy on a paragraph of text and it's not extracting text in quote the same way for each, and I don't understand why that is</p>

<pre><code>nlp = spacy.load(""en_core_web_lg"")

doc = nlp(""""""A seasoned TV exec, Greenblatt spent eight years as chairman of NBC Entertainment before WarnerMedia. He helped revive the broadcast network's primetime lineup with shows like ""The Voice,"" ""This Is Us,"" and ""The Good Place,"" and pushed the channel to the top of the broadcast-rating ranks with 18-49-year-olds, Variety reported. He also drove Showtime's move into original programming, with series like ""Dexter,"" ""Weeds,"" and ""Californication."" And he was a key programming exec at Fox Broadcasting in the 1990s."""""")
</code></pre>

<p>Here's the whole output:</p>

<pre><code>A
seasoned
TV
exec
,
Greenblatt
spent
eight years
as
chairman
of
NBC Entertainment
before
WarnerMedia
.
He
helped
revive
the
broadcast
network
's
primetime
lineup
with
shows
like
""
The Voice
,
""
""
This
Is
Us
,
""
and
""The Good Place
,
""
and
pushed
the
channel
to
the
top
of
the
broadcast
-
rating
ranks
with
18-49-year-olds
,
Variety
reported
.
He
also
drove
Showtime
's
move
into
original
programming
,
with
series
like
""
Dexter
,
""
""
Weeds
,
""
and
""
Californication
.
""
And
he
was
a
key
programming
exec
at
Fox Broadcasting
in
the 1990s
.
</code></pre>

<p>The one that bothers me the most is The Good Place, which is extracted as <code>""The Good Place</code>. Since the quotation is part of the token, I then can't extract text in quote with a <a href=""https://spacy.io/usage/rule-based-matching"" rel=""nofollow noreferrer"">Token Matcher</a> later on… Any idea what's going on here?</p>
",Named Entity Recognition (NER),spacy failing tokenizing particular quotation mark running spacy paragraph text extracting text quote way understand whole output one bother good place extracted since quotation part token extract text quote token matcher later idea going
What should be the format of training data for Stanford NER CRF Classifier?,"<p>I am trying to train my own Address classifier model using Stanford <code>CRF-NER</code> but the performance is very low. I am confused about the format of the training data I have trained with. The training data is typically the list of districts, cities, provinces and their respective labels. But the model is not tagging the respective address tags to its tokens.</p>

<p>The format of the training data is as below:</p>

<ul>
<li>BARAT    PROVINCE</li>
<li>MALUKU    PROVINCE</li>
<li>MALUKU    PROVINCE</li>
<li>KABUPATEN REGENCY</li>
<li>SIMEULUE  REGENCY</li>
<li>KABUPATEN REGENCY</li>
<li>ACEH  REGENCY</li>
</ul>

<p>This is the just a sample of training data in csv format, There are 3 labels <strong>PROVINCE, REGENCY and DISTRICT</strong></p>

<p>Here is the output of tagged tokens:</p>

<p><a href=""https://i.sstatic.net/QQKCA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QQKCA.png"" alt=""OUTPUT of the Stanford NER Tgger""></a></p>

<p>You can all tokens has been tagged as DISTRICT though I have REGENCY, DISTRICT AND PROVINCE as labelled data.</p>

<p><strong>I wanted to know if my format of training data is correct is only works on contextual data at sentence level Since I saw Stanford <code>NER</code> working well on sentence level.</strong></p>
",Named Entity Recognition (NER),format training data stanford ner crf classifier trying train address classifier model using stanford performance low confused format training data trained training data typically list district city province respective label model tagging respective address tag token format training data barat province maluku province maluku province kabupaten regency simeulue regency kabupaten regency aceh regency sample training data csv format label province regency district output tagged token token ha tagged district though regency district province labelled data wanted know format training data correct work contextual data sentence level since saw stanford working well sentence level
Spacy: Incorrect date identified by ner,"<p>I am using the spacy library to identify the entity from the text. When I passed the text to the nlp object it is not identifying the date properly.</p>

<p><strong>text</strong> : meet me 9 Oct. - 8am</p>

<p><strong>Identified</strong> -></p>

<p>9 (as Cardinal)</p>

<p>Oct. - 8 (as Date)</p>

<p><strong>Required</strong> -> </p>

<p>9 Oct. (as Date)</p>

<p>8am (as Time)</p>

<p><a href=""https://i.sstatic.net/RCzyh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RCzyh.png"" alt=""enter image description here""></a></p>

<p>So could you please help me out how could I resolve this issue. I am beginner in nlp.</p>

<p>Regards,
Aman</p>
",Named Entity Recognition (NER),spacy incorrect date identified ner using spacy library identify entity text passed text nlp object identifying date properly text meet oct identified cardinal oct date required oct date time could please help could resolve issue beginner nlp regard aman
How to use BERT just for ENTITY extraction from a Sequence without classification in the NER task?,"<p>My requirement here is given a sentence(sequence), I would like to just extract the entities present in the sequence without classifying them to a type in the NER task. I see that BertForTokenClassification for NER does the classification. Can this be adapted for just the extraction?</p>

<p>Can BERT just be used to do <strong>entity extraction/identification</strong>?</p>
",Named Entity Recognition (NER),use bert entity extraction sequence without classification ner task requirement given sentence sequence would like extract entity present sequence without classifying type ner task see bertfortokenclassification ner doe classification adapted extraction bert used entity extraction identification
How to influence NLTK to tag city as GPE instead of Verb,"<p>I am looking for a way to influence the behavior of the IoB tagging of pythons NLTK.
Consider the following piece of code:</p>

<pre><code>from nltk import word_tokenize, pos_tag, ne_chunk
from nltk.stem import PorterStemmer 
from nltk.tag import untag, str2tuple, tuple2str
from nltk.chunk import tree2conllstr, conllstr2tree, conlltags2tree, tree2conlltags
import nltk

text = ""Drive me from Seattle to Brussels""
# Morphology - tagging the words
tokens = word_tokenize(text)

# Part of speech tagging
tagged_tokens = pos_tag(tokens)

# Create named entity tree of tagged tokens
ner_tree = ne_chunk(tagged_tokens)

# Get tag structure
iob_tagged = tree2conlltags(ner_tree)
print(iob_tagged)
</code></pre>

<p>This outputs the following values:</p>

<p>[('Drive', 'VB', 'O'), ('me', 'PRP', 'O'), ('from', 'IN', 'O'), ('Seattle', 'NNP', 'B-GPE'), ('to', 'TO', 'O'), ('Brussels', 'VB', 'O')]</p>

<p>Is there a way I can influence or tune the NLTK algorithm/model in a way that the last word (Brussels) is tagged as a Geopolitical Entity (GPE), instead of a verb?  I understand the verb is there, because it is following the To word, which is often used prior to verbs.  </p>
",Named Entity Recognition (NER),influence nltk tag city gpe instead verb looking way influence behavior iob tagging python nltk consider following piece code output following value drive vb prp seattle nnp b gpe brussels vb way influence tune nltk algorithm model way last word brussels tagged geopolitical entity gpe instead verb understand verb following word often used prior verb
Spacy Entity from PhraseMatcher only,"<p>I'm using <a href=""http://spacy.io/"" rel=""nofollow noreferrer"">Spacy</a> for a NLP project. I have a list of phrases I'd like to mark as a new entity type. I originally tried training a NER model but since there's a finite terminology list, I think simply using a Matcher should be easier. I see in the <a href=""https://spacy.io/usage/linguistic-features#on_match"" rel=""nofollow noreferrer"">documentation</a> that you can add entities to a document based on a Matcher. My question is: how do I do this for a <strong>new</strong> entity and not have the NER pipe label any other tokens as this entity? Ideally only tokens found via my matcher should be marked as the entity but I need to add it as a label to the NER model which then ends up labeling some as the entity.</p>

<p>Any suggestions on how to best accomplish this? Thanks!</p>
",Named Entity Recognition (NER),spacy entity phrasematcher using spacy nlp project list phrase like mark new entity type originally tried training ner model since finite terminology list think simply using matcher easier see documentation add entity document based matcher question new entity ner pipe label token entity ideally token found via matcher marked entity need add label ner model end labeling entity suggestion best accomplish thanks
spaCy: How to write named entities to an existing Doc object using some loaded model for this?,"<p>I created a <code>Doc</code> object from a custom list of tokens according to documentation like so:</p>

<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.tokens import Doc

nlp = spacy.load(""my_ner_model"")
doc = Doc(nlp.vocab, words=[""Hello"", "","", ""world"", ""!""])
</code></pre>

<p>How do I write named entities tags to <code>doc</code> with my NER model now?</p>

<p>I tried to do <code>doc = nlp(doc)</code>, but that didn't work for me raising a <code>TypeError</code>.</p>

<p>I can't just join my list of words into a plain text to do <code>doc = nlp(text)</code> as usual because in this case <code>spaCy</code> splits some words in my texts into two tokens which I can not accept.</p>
",Named Entity Recognition (NER),spacy write named entity existing doc object using loaded model created object custom list token according documentation like write named entity tag ner model tried work raising join list word plain text usual case split word text two token accept
Information Extraction,"<p>I am looking for steps/process to extract information from a Invoice using machine learning/NLP/Deep learning techniques. What will be the steps/process to be followed ?</p>

<p>The approach would need clarification on below </p>

<p>Suppose there are invoices from 2 Vendors, how a model needs to be created to extract the value mentioned for below fields? Will it have Keyword extraction ? Does custom NER needs to be implemented, if so how ? How should the training data be created for this ?</p>

<p>Invoice Number
Invoice Date
Invoice Amount
Address </p>
",Named Entity Recognition (NER),information extraction looking step process extract information invoice using machine learning nlp deep learning technique step process followed approach would need clarification suppose invoice vendor model need created extract value mentioned field keyword extraction doe custom ner need implemented training data created invoice number invoice date invoice amount address
Do I need to provide sentences for training Spacy NER or are paragraphs fine?,"<p>I am trying to train a new Spacy model to recognize references to law articles. I start using a blank model, and train the ner pipe according to the example given in the documentation.</p>

<p>The performance of the trained model is really poor, even with several thousands on input points. I am tryong to figure out why.</p>

<p>One possible answer is that I am giving full paragraphs to train on, instead of sentences that are in the examples. Each of these paragraphs can have multiple references to law articles. Is this a possible issue?</p>

<p>Turns out I was making a huge mistake in my code. There is nothing wrong with paragraphs. As long as your code actually supplies them to spacy.</p>
",Named Entity Recognition (NER),need provide sentence training spacy ner paragraph fine trying train new spacy model recognize reference law article start using blank model train ner pipe according example given documentation performance trained model really poor even several thousand input point tryong figure one possible answer giving full paragraph train instead sentence example paragraph multiple reference law article possible issue turn wa making huge mistake code nothing wrong paragraph long code actually supply spacy
How can I keep multi-word names in tokenization together?,"<p>I want to classify documents using TF-IDF features. One way to do it:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import string
import re
import nltk


def tokenize(document):
    document = document.lower()
    for punct_char in string.punctuation:
        document = document.replace(punct_char, "" "")
    document = re.sub('\s+', ' ', document).strip()

    tokens = document.split("" "")

    # Contains more than I want:
    # from spacy.lang.de.stop_words import STOP_WORDS
    stopwords = nltk.corpus.stopwords.words('german')
    tokens = [token for token in tokens if token not in stopwords]
    return tokens

# How I intend to use it
transformer = TfidfVectorizer(tokenizer=tokenize)

example = ""Jochen Schweizer ist eines der interessantesten Unternehmen der Welt, hat den Sitz allerdings nicht in der Schweizerischen Eidgenossenschaft.""
transformer.fit([example])

# Example of the tokenizer
print(tokenize(example))
</code></pre>

<p>One flaw of this tokenizer is that it splits words that belong together: ""Jochen Schweizer"" and ""schweizerische Eidgenossenschaft"". Also lemmatization (word stemming) is missing. I would like to get the following tokens:</p>

<pre><code>[""Jochen Schweizer"", ""interessantesten"", ""unternehmen"", ""Welt"", ""Sitz"", ""allerdings"", ""nicht"", ""Schweizerische Eidgenossenschaft""]
</code></pre>

<p>I know that Spacy can identify those named entities (NER):</p>

<pre><code>import en_core_web_sm  # python -m spacy download en_core_web_sm --user
parser = en_core_web_sm.load()
doc = parser(example)
print(doc.ents)  # (Jochen Schweizer, Welt, Sitz)
</code></pre>

<p>Is there a good way to use spacy to tokenize in a way that keeps the named entity words together?</p>
",Named Entity Recognition (NER),keep multi word name tokenization together want classify document using tf idf feature one way one flaw tokenizer split word belong together jochen schweizer schweizerische eidgenossenschaft also lemmatization word stemming missing would like get following token know spacy identify named entity ner good way use spacy tokenize way keep named entity word together
Disabling Gensim&#39;s removal of punctuation etc. when parsing a wiki corpus,"<p>I want to train a word2vec model on the english wikipedia using python with gensim. I closely followed <a href=""https://groups.google.com/forum/#!topic/gensim/MJWrDw_IvXw"" rel=""noreferrer"">https://groups.google.com/forum/#!topic/gensim/MJWrDw_IvXw</a> for that.</p>

<p>It works for me but what I don't like about the resulting word2vec model is that named entities are split which makes the model unusable for my specific application. The model I need has to represent named entities as a single vector. </p>

<p>Thats why I planned to parse the wikipedia articles with spacy and merge entities like ""north carolina"" into ""north_carolina"", so that word2vec would represent them as a single vector. So far so good.</p>

<p>The spacy parsing has to be part of the preprocessing, which I originally did as recommended in the linked discussion using:</p>

<pre><code>...
wiki = WikiCorpus(wiki_bz2_file, dictionary={})
for text in wiki.get_texts():
    article = "" "".join(text) + ""\n""
    output.write(article)
...
</code></pre>

<p>This removes punctuation, stop words, numbers and capitalization and saves  each article in a separate line in the resulting output file. The problem is that spacy's NER doesn't really work on this preprocessed text, since I  guess it relies on punctuation and capitalization for NER (?).</p>

<p><strong>Does anyone know if I can ""disable"" gensim's preprocessing so that it doesn't remove punctuation etc. but still parses the wikipedia articles to text directly from the compressed wikipedia dump? Or does someone know a better way to accomplish this? Thanks in advance!</strong></p>
",Named Entity Recognition (NER),disabling gensim removal punctuation etc parsing wiki corpus want train word vec model english wikipedia using python gensim closely followed work like resulting word vec model named entity split make model unusable specific application model need ha represent named entity single vector thats planned parse wikipedia article spacy merge entity like north carolina north carolina word vec would represent single vector far good spacy parsing ha part preprocessing originally recommended linked discussion using remove punctuation stop word number capitalization save article separate line resulting output file problem spacy ner really work preprocessed text since guess relies punctuation capitalization ner doe anyone know disable gensim preprocessing remove punctuation etc still par wikipedia article text directly compressed wikipedia dump doe someone know better way accomplish thanks advance
How can I improve massively classification report of one class using ensemble model?,"<p>I have a dataset including</p>

<p>{0: 6624, 1: 75} 0 for nonobservational sentences and 1 for observational sentences. (basically, I annotate my sentences using Named Entity Recognition, If there is a specific entity like DATA, TIME, LONG (coordinate) I put label 1)</p>

<p>Now I want to make a model to classify them, the best model (CV =3 FOR ALL) that I made is the ensembling model of </p>

<pre><code>clf= SGDClassifier()
trial_05=Pipeline([(""vect"",vec),(""clf"",clf)])
</code></pre>

<p>which has:</p>

<pre><code>                  precision    recall  f1-score   support

           0       1.00      1.00      1.00      6624
           1       0.73      0.57      0.64        75

   micro avg       0.99      0.99      0.99      6699
   macro avg       0.86      0.79      0.82      6699
weighted avg       0.99      0.99      0.99      669

</code></pre>

<pre><code>[[6611   37]
 [  13   38]]

</code></pre>

<p>and this model which used resampled sgd for classifcation </p>

<pre><code>                  precision    recall  f1-score   support

           0       1.00      0.92      0.96      6624
           1       0.13      1.00      0.22        75

   micro avg       0.92      0.92      0.92      6699
   macro avg       0.56      0.96      0.59      6699
weighted avg       0.99      0.92      0.95      6699

</code></pre>

<pre><code>[[6104    0]
 [ 520   75]]
</code></pre>

<p>As you see the problem in both cases is class 1, but in forst one we have fairly good precision and f1 score versus in the second one we have a very good recall</p>

<p>So I decided to use ensemble model using both in this way: </p>

<pre><code>from sklearn.ensemble import VotingClassifier#create a dictionary of our models
estimators=[(""trail_05"",trial_05), (""resampled"", SGD_RESAMPLED_Model)]#create our voting classifier, inputting our models
ensemble = VotingClassifier(estimators, voting='hard')
</code></pre>

<p>now I have this result: </p>

<pre><code>                precision    recall  f1-score   support

           0       0.99      1.00      1.00      6624
           1       0.75      0.48      0.59        75

   micro avg       0.99      0.99      0.99      6699
   macro avg       0.87      0.74      0.79      6699
weighted avg       0.99      0.99      0.99      6699
</code></pre>

<pre><code>[[6612   39]
 [  12   36]]
</code></pre>

<p>As you the ensembe model has better precision regarding to class 1,but worse recall and f1 socre which caused to worse confusion matrix regarding classed 1 (36 TP vs 38 TP for class 1)</p>

<p>MY aim is to improve TP for class one (f1 score, recall for class 1)</p>

<p>what do you recommend to improve TP for class one (f1score, recall for class 1?
generaly do you have any idea regarding my workflow? </p>

<p>I have tried parameter tuning, it i does not improve sgd model.</p>
",Named Entity Recognition (NER),improve massively classification report one class using ensemble model dataset including nonobservational sentence observational sentence basically annotate sentence using named entity recognition specific entity like data time long coordinate put label want make model classify best model cv made ensembling model ha model used resampled sgd classifcation see problem case class forst one fairly good precision f score versus second one good recall decided use ensemble model using way result ensembe model ha better precision regarding class worse recall f socre caused worse confusion matrix regarding classed tp v tp class aim improve tp class one f score recall class recommend improve tp class one f score recall class generaly idea regarding workflow tried parameter tuning doe improve sgd model
Do spaces between words matter while extracting entities with SpaCy?,"<p>I'm using spaCy for NER task. My training data looks like below:</p>

<pre><code>Total HDL Cholestrol &lt;mulitple-spaces&gt;        Photometry &lt;multiple-spaces&gt;          12.3 &lt;multiple-spaces&gt;        mg/dl &lt;multiple-spaces&gt;       0-45
</code></pre>

<p>(Note: Multiple spaces between words.)</p>

<p>I annotated each word as below:</p>

<pre><code>Total-BTest HDL-ITest Cholestrol-LTest &lt;mulitple-spaces&gt;       Photometry-UTech &lt;mulitple-spaces&gt;        12.3-UVal  &lt;mulitple-spaces&gt;       mg/dl-UUnit &lt;mulitple-spaces&gt;      0-45-O
</code></pre>

<p>Should my test data also be in the same format (words with multiple spaces) for spacy to exactly recognize entities?</p>

<p>I tested the trained spacy model with below inputs:</p>

<pre><code>Total Cholestrol &lt;single-space&gt;       Photometry &lt;single-space&gt;         56.9 &lt;single-space&gt;        mg/dl &lt;single-space&gt;           0-45
</code></pre>

<p>My model was able to recognize entities correctly in the above case. But in the below case:</p>

<pre><code>Total Cholestrol Photometry 56.9 mg/dl 0-45
</code></pre>

<p>(Note: No multiple spaces between words)</p>

<p>The model was not able to recognize the entities.</p>

<p>My question is, Does spacy take a number of whitespaces between words also into account while training?</p>

<p>If so, would removing multiple spaces in the training set be a possible solution?</p>
",Named Entity Recognition (NER),space word matter extracting entity spacy using spacy ner task training data look like note multiple space word annotated word test data also format word multiple space spacy exactly recognize entity tested trained spacy model input model wa able recognize entity correctly case case note multiple space word model wa able recognize entity question doe spacy take number whitespaces word also account training would removing multiple space training set possible solution
Is there anyway that I can load a model trained in eBrevia into CoreNLP or Spacy?,"<p>I have a trained model for Name Entity Recognition (NER) from eBrevia. I am wondering if there is a way that I can load it into CoreNLP or Spacy using Python or Java programmatically. </p>

<p>Edit: If the pretrained model is a pickle model, is there a way that I can use Corenlp or Spacy to load it? </p>

<p>Thanks in advance! </p>
",Named Entity Recognition (NER),anyway load model trained ebrevia corenlp spacy trained model name entity recognition ner ebrevia wondering way load corenlp spacy using python java programmatically edit pretrained model pickle model way use corenlp spacy load thanks advance
NLP Named Entity Recognition using NLTK and Spacy,"<p>I used the NER for the following sentence on both NLTK and Spacy and below are the results:</p>

<pre><code>""Zoni I want to find a pencil, a eraser and a sharpener""
</code></pre>

<p>I ran the following code on Google Colab.</p>

<pre><code>import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

ex = ""Zoni I want to find a pencil, a eraser and a sharpener""

def preprocess(sent):
    sent = nltk.word_tokenize(sent)
    sent = nltk.pos_tag(sent)
    return sent

sent = preprocess(ex)
sent

#Output:
[('Zoni', 'NNP'),
 ('I', 'PRP'),
 ('want', 'VBP'),
 ('to', 'TO'),
 ('find', 'VB'),
 ('a', 'DT'),
 ('pencil', 'NN'),
 (',', ','),
 ('a', 'DT'),
 ('eraser', 'NN'),
 ('and', 'CC'),
 ('a', 'DT'),
 ('sharpener', 'NN')]
</code></pre>

<p>But when i used spacy on the same text, it didn't return me any result</p>

<pre><code>import spacy
from spacy import displacy
from collections import Counter
import en_core_web_sm
nlp = en_core_web_sm.load()

text = ""Zoni I want to find a pencil, a eraser and a sharpener""

doc = nlp(text)
doc.ents

#Output:
()
</code></pre>

<p>Its only working for some sentences.</p>

<pre><code>import spacy
from spacy import displacy
from collections import Counter
import en_core_web_sm
nlp = en_core_web_sm.load()

# text = ""Zoni I want to find a pencil, a eraser and a sharpener""

text = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'

doc = nlp(text)
doc.ents

#Output:
(European, Google, $5.1 billion, Wednesday)
</code></pre>

<p>Please let me know if there is something wrong.</p>
",Named Entity Recognition (NER),nlp named entity recognition using nltk spacy used ner following sentence nltk spacy result ran following code google colab used spacy text return result working sentence please let know something wrong
Is it feasible to extract persons associated to named entity organizations?,"<p>I am new to text extraction and I would like to extract company names from a text and people associated to the company. I'm thinking of using Spacy to extract these individual parts but I'm not sure whether I'll be able to find the associations, since multiple companies and people will be mentioned within the same text. For example, from the following text: </p>

<p><em>Wozniak left Apple in 1983 due to a diminishing interest in the day-to-day running of Apple Computers. Jobs then hired PepsiCo's John Sculley to be president. However, this move backfired and after much controversy with Sculley, Jobs left in 1985 and went on to new and bigger things. He founded his own company NeXT Software and he also bought Pixar from George Lucas</em></p>

<p>I would like to pull out companies and people to make the associations (at the very least):
Apple - Wozniak,
PepsiCo - John Sculley, 
Jobs - NeXT Software, Pixar - George Lucas.</p>

<p>Any guidance would be appreciated, thank you.</p>
",Named Entity Recognition (NER),feasible extract person associated named entity organization new text extraction would like extract company name text people associated company thinking using spacy extract individual part sure whether able find association since multiple company people mentioned within text example following text wozniak left apple due diminishing interest day day running apple computer job hired pepsico john sculley president however move backfired much controversy sculley job left went new bigger thing founded company next software also bought pixar george lucas would like pull company people make association least apple wozniak pepsico john sculley job next software pixar george lucas guidance would appreciated thank
Spacy Remove stopwords without affecting Named Entities,"<p>I am trying to remove stopwords from a string but the condition I want to achieve is that the named entities in the string should not be removed.</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_sm')
text = ""The Bank of Australia has an agreement according to the Letter Of Offer which states that the deduction should be made at the last date of each month""
doc = nlp(text)
</code></pre>

<p>If i check the named entities in the text, i get the below</p>

<pre><code>print(doc.ents)
(The Bank of Australia, the Letter Of Offer, the last date of each month)
</code></pre>

<p>The usual way to remove the stopwords would be like below </p>

<pre><code>[token.text for token in doc if not token.is_stop]
['Bank',
 'Australia',
 'agreement',
 'according',
 'Letter',
 'Offer',
 'states',
 'deduction',
 'date',
 'month']
</code></pre>

<p>The normal way completely took the meaning away which is needed for my task.
I would want to retain the Named Entities.</p>

<p>I tried adding the named entities with the same list.</p>

<pre><code>list1 = [token.text for token in doc if not token.is_stop]
list2 = [str(a) for a in doc.ents]

list1 + list2

['Bank',
 'Australia',
 'agreement',
 'according',
 'Letter',
 'Offer',
 'states',
 'deduction',
 'date',
 'month',
 'The Bank of Australia',
 'the Letter Of Offer',
 'the last date of each month']
</code></pre>

<p>Is there any other approach to this?</p>
",Named Entity Recognition (NER),spacy remove stopwords without affecting named entity trying remove stopwords string condition want achieve named entity string removed check named entity text get usual way remove stopwords would like normal way completely took meaning away needed task would want retain named entity tried adding named entity list approach
Using spacy&#39;s Matcher without a model,"<p>I want to use spaCy's Matcher class on a new language (Hebrew) for which spaCy does not yet have a working model.</p>

<p>I found a working tokenizer + POS tagger (from Stanford NLP), yet I would prefer spaCy as its Matcher can help me do some rule-based NER.</p>

<p>Can the rule-based Matcher be fed with POS-tagged text instead of the standard NLP pipeline?</p>
",Named Entity Recognition (NER),using spacy matcher without model want use spacy matcher class new language hebrew spacy doe yet working model found working tokenizer po tagger stanford nlp yet would prefer spacy matcher help rule based ner rule based matcher fed po tagged text instead standard nlp pipeline
"Extract personal information of a subject from emails using Python, Spacy","<p>I need to extract personal information of a particular person from the below email </p>

<pre><code>Hi Alex,



Please find my personal details below,



Name: Bill Smith

Company: Apple

Insurance number: AB 654321C

Phone: +447677679999

Birth date: 21-11-1990

City: California


Regards,

Bill
</code></pre>

<p>In the above email I need to find where Bill works, stays, can be contacted at and his date of birth. </p>

<p>I have tried to extract using Python3 and Spacy but unfortunately Spacy cannot identify entities (Apple, California, Bill Smith) in the above email.(may be because the data is in tabular format)</p>
",Named Entity Recognition (NER),extract personal information subject email using python spacy need extract personal information particular person email email need find bill work stay contacted date birth tried extract using python spacy unfortunately spacy identify entity apple california bill smith email may data tabular format
Chatbot to use pdf documents as source,"<p>I am looking at creating a simple chatbot which can use a pdf file as it's source.
For example, the input to the chatbot can be a bank's terms and conditions document and the chatbot would respond to a question which are related to the contents of the document.</p>

<p>Sample Q&amp;A.
Q : What is my monthly fee for my savings account?
A : Your monthly fees is $5 for the savings account if no deposit is made above $2000, else free.</p>

<p>I used pdfminer to read the pdf document and convert it into processed data and spaCy to identify the NER, POS etc.</p>

<p>I learnt about RASA and all the links which I saw uses a defined text response and not using any pdf document as a source.</p>

<p>Can someone provide any guidance on which approach i could follow?</p>

<p>I don't want to use Dialogflow or Lex and want to be in the open source world.</p>
",Named Entity Recognition (NER),chatbot use pdf document source looking creating simple chatbot use pdf file source example input chatbot bank term condition document chatbot would respond question related content document sample q q monthly fee saving account monthly fee saving account deposit made else free used pdfminer read pdf document convert processed data spacy identify ner po etc learnt rasa link saw us defined text response using pdf document source someone provide guidance approach could follow want use dialogflow lex want open source world
"Is a Conditional Random Field, on a Named Entity Recognition task, bi-directional?","<p>I am currently working on a Named Entity Recognition task. I am using a Conditional Random Field algorithm to classify my marked entites. I was wondering if this algorithm is bi-directional like BERT ?</p>

<p>The features the algorithm has for each word include the previous and the next word, so I guess it is the case. Does that also mean that the CRF is predicting on the whole sentence ? Or on each word ? </p>

<p>Thank you for any lead on this question !</p>
",Named Entity Recognition (NER),conditional random field named entity recognition task bi directional currently working named entity recognition task using conditional random field algorithm classify marked entites wa wondering algorithm bi directional like bert feature algorithm ha word include previous next word guess case doe also mean crf predicting whole sentence word thank lead question
RuntimeError: Expected object of backend CPU but got backend CUDA for argument #3 &#39;index&#39;,"<p>I'm experimenting with argumentation mining with BERT and try to identify argument components (BIO-classification task). I closely followed the code from this article, <a href=""https://www.depends-on-the-definition.com/named-entity-recognition-with-bert"" rel=""nofollow noreferrer"">Named Entity Recognition With Bert</a>, but adapted it to fit my data. I run the code on colab (hardware accelerator is set to GPU). </p>

<p>Does anybody know a solution to my problem? Thanks in advance!</p>

<pre><code>epochs = 5
max_grad_norm = 1.0

for _ in trange(epochs, desc=""Epoch""):
    # TRAIN loop
    model.train()
    tr_loss = 0
    nb_tr_examples, nb_tr_steps = 0, 0
    for step, batch in enumerate(train_dataloader):
        # add batch to gpu
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        # forward pass
        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
        # backward pass
        loss.backward()
        # track train loss
        tr_loss += loss.item()
        nb_tr_examples += b_input_ids.size(0)
        nb_tr_steps += 1
        # gradient clipping
        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
        # update parameters
        optimizer.step()
        model.zero_grad()
    # print train loss per epoch
    print(""Train loss: {}"".format(tr_loss/nb_tr_steps))
    # VALIDATION on validation set
    model.eval()
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels = [], []
    for batch in valid_dataloader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch

        with torch.no_grad():
            tmp_eval_loss = model(b_input_ids, token_type_ids=None,
                                  attention_mask=b_input_mask, labels=b_labels)
            logits = model(b_input_ids, token_type_ids=None,
                           attention_mask=b_input_mask)
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.append(label_ids)

        tmp_eval_accuracy = flat_accuracy(logits, label_ids)

        eval_loss += tmp_eval_loss.mean().item()
        eval_accuracy += tmp_eval_accuracy

        nb_eval_examples += b_input_ids.size(0)
        nb_eval_steps += 1
    eval_loss = eval_loss/nb_eval_steps
    print(""Validation loss: {}"".format(eval_loss))
    print(""Validation Accuracy: {}"".format(eval_accuracy/nb_eval_steps))
    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]
    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]
    print(""F1-Score: {}"".format(f1_score(pred_tags, valid_tags)))
</code></pre>

<p>Although I closely followed the original code, it does not seem to work for me and I keep on getting this mistake...</p>

<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-45-aa659bbf8fac&gt; in &lt;module&gt;()
     12 
     13         # forward pass
---&gt; 14         loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
     15         # backward pass
     16         loss.backward()

8 frames
/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   1465         # remove once script supports set_grad_enabled
   1466         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 1467     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   1468 
   1469 

RuntimeError: Expected object of backend CPU but got backend CUDA for argument #3 'index'
</code></pre>

<p><strong>SOLUTION</strong>:
setting model = model.to(device) plus reducing the batch size from 32 to 4 allowed the program to run without any runtimeerrors. </p>
",Named Entity Recognition (NER),runtimeerror expected object backend cpu got backend cuda argument index experimenting argumentation mining bert try identify argument component bio classification task closely followed code article named entity recognition bert adapted fit data run code colab hardware accelerator set gpu doe anybody know solution problem thanks advance although closely followed original code doe seem work keep getting mistake solution setting model model device plus reducing batch size allowed program run without runtimeerrors
SpaCy NER: Can a same word be part of two different entities?,"<p>For example:</p>
<p>Sentence: The best product in the world is Nestle Cookies.</p>
<blockquote>
<p>Entities:</p>
<p>BRAND: Nestle</p>
<p>PRODUCT: Nestle Cookie</p>
</blockquote>
<p>Are the above entities valid, or should I tag them as:</p>
<blockquote>
<p>Entities:</p>
<p>BRAND: Nestle</p>
<p>PRODUCT: Cookie</p>
</blockquote>
<p>And will it affect model performance?</p>
",Named Entity Recognition (NER),spacy ner word part two different entity example sentence best product world nestle cooky entity brand nestle product nestle cookie entity valid tag entity brand nestle product cookie affect model performance
Add custom entity in addition to NER basic model,"<p>I am using spacy to train my own NER model. In addition to entities trained by spacy basic 'en_core_web_sm' model (ORG, PERSON, DATE, etc), I want to add my own entities. I trained my model with 'en_core_web_sm' as my base model, but then the model can only detect my own custom entities only, not the basic entities. Is there any way to do this? Thanks.</p>
",Named Entity Recognition (NER),add custom entity addition ner basic model using spacy train ner model addition entity trained spacy basic en core web sm model org person date etc want add entity trained model en core web sm base model model detect custom entity basic entity way thanks
Multiline regex: How to extract text between dates in pandas dataframe?,"<p>I have dataframe with description column, under one row of description there are multiple lines of texts, basically those are set of information for each record. </p>

<p>Example: Regarding information no 1 at 07-01-2019  we got update as the sky is blue and at 05-22-2019 we again got update as Apples are red, that are arranged between two dates. Firstly, I would like to extract text between the date and split the respective details in new columns as date, name, description.</p>

<p>The raw description looks like</p>

<pre><code>info no|           Description
--------------------------------------------------------------------------
1      |07-01-2019 12:59:41 - XYZ (Work notes) The sky is blue in color.
       |                                        Clouds are looking lovely.
       | 05-22-2019 12:00:49 - MNX  (Work notes) Apples are red in color.
--------------------------------------------------------------------------    
       |  02-26-2019 12:53:18 - ABC (Work notes) Task is to separate balls.
2      |  02-25-2019 16:57:57 - lMN (Work notes) He came by train.
       |                                         That train was 15 min late.
       |                                         He missed the concert.
       |  02-25-2019 11:08:01 - sbc (Work notes) She is my grandmother.
</code></pre>

<p>Desired output is </p>

<pre><code>info No |DATE                   |  NAME |   DESCRIPTION
--------|------------------------------------------------------
   1    |07-01-2019 12:59:41    |   xyz  |  The sky is blue in color.
        |                       |        |  Clouds are looking lovely.
--------|---------------------------------------------------------
   1    |05-22-2019 12:00:49    |   MNX  |  Apples are red in color                     
--------|---------------------------------------------------------
   2    | 02-26-2019 12:53:18   |   ABC  |  Task is to separate blue balls.
--------|---------------------------------------------------------
   2    |  02-25-2019 16:57:57  |   IMN   |  He came by train
        |                       |         |  That train was 15 min late.
        |                       |         |  He missed the concert.
--------|---------------------------------------------------------
        |  02-25-2019 11:08:01  |   sbc   | She is my grandmother.
</code></pre>

<p>I tried:</p>

<pre><code> myDf = pd.DataFrame(re.split('(\d{2}-\d{2}-\d{4} \d{2}:\d{2}:\d{2} -.*)',Description),columns = ['date'])
 myDf['date'] = myDf['date'].replace('(Work notes)','-', regex=True)
 newQueue = newQueue.date.str.split(-,n=3)
</code></pre>
",Named Entity Recognition (NER),multiline regex extract text date panda dataframe dataframe description column one row description multiple line text basically set information record example regarding information got update sky blue got update apple red arranged two date firstly would like extract text date split respective detail new column date name description raw description look like desired output tried
Storing NLP corpora in databases rather than csv?,"<p>While implementing an NLP system, I wonder why CSV files are often used to store text Corpora in Academia and common Python Examples (in particular: NLTK-based). I have personally ran into issues, using a system that generates a number of corpora automatically and accesses them later.</p>

<p>These are issues that come from <code>CSV files</code>:
- Difficult to automate back up
- Difficult to ensure availability
- Potential transaction race and thread accessing issues
- Difficult to distribute/shard over multiple servers
- Schema not clear or defined, if corpora becomes complicated
- Accessing via a filename is risky. It could be altered.
- File Corruption possible
- Fine grained permissions not typically used for file-access</p>

<p>Issues from using <code>MySQL</code>, or <code>MongooseDB</code>:
- Initial set up, keeping a dediated server running with DB instance online
- Requires spending time creating and defining a Schema</p>

<p>Pros of CSV:
- Theoretically easier to automate zip and unzipping of contents
- More familiar to some programmers
- Easier to transfer to another academic researcher, via FTP or even e-mail</p>

<p>Viewing multiple academic articles, even in cases of more advanced NLP research, for example undertaking Named Entity Recognition or statement extraction, research seems to use CSV.</p>

<p>Are there other advantages to the CSV format, that make it so widely used? What should an Industry system use?</p>
",Named Entity Recognition (NER),storing nlp corpus database rather csv implementing nlp system wonder csv file often used store text corpus academia common python example particular nltk based personally ran issue using system generates number corpus automatically access later issue come difficult automate back difficult ensure availability potential transaction race thread accessing issue difficult distribute shard multiple server schema clear defined corpus becomes complicated accessing via filename risky could altered file corruption possible fine grained permission typically used file access issue using initial set keeping dediated server running db instance online requires spending time creating defining schema pro csv theoretically easier automate zip unzipping content familiar programmer easier transfer another academic researcher via ftp even e mail viewing multiple academic article even case advanced nlp research example undertaking named entity recognition statement extraction research seems use csv advantage csv format make widely used industry system use
Any examples of actually using (not training) pre-training BERT models for NER?,"<p>I'd like to use one of pre-trained BERT models to extract NER tags from sentences.  I can't seem to find any examples of doing so.  Every link I find on the web is about re-training models.  I just want to use them.  Basically, given a sentence such as ""John Smith from Texas"", I want tags such as [""PERSON"",""PERSON"",""GEO""].  Does anyone know how to do this?</p>

<p>Thank you!</p>
",Named Entity Recognition (NER),example actually using training pre training bert model ner like use one pre trained bert model extract ner tag sentence seem find example every link find web training model want use basically given sentence john smith texas want tag person person geo doe anyone know thank
Extract Wikipedia Entities from Text,"<p>Is there any way we can extract all the wikipedia entities from the text using Wikipedia2Vec? Or is there any other way to do the same. </p>

<p>Example:  </p>

<pre><code>Text : ""Scarlett Johansson is an American actress.""  
Entities : [ 'Scarlett Johansson' , 'American' ]
</code></pre>

<p>I want to do it in Python</p>

<p>Thanks</p>
",Named Entity Recognition (NER),extract wikipedia entity text way extract wikipedia entity text using wikipedia vec way example want python thanks
Shoud i use Spacy Named Entity Recognition for this case?,"<p>I have a set of names, a fixed set of names which can extend up-to 50,000 names.</p>

<p>""John"",""Mike"",""Josh"",""Peter"",""Karl"".</p>

<p>And I have a document, this document is dynamic. I need to find whether this document has
the predefined name or not ?</p>

<p>Is defining everything as a entity in spacy nlp the right way to do it ?</p>
",Named Entity Recognition (NER),shoud use spacy named entity recognition case set name fixed set name extend name john mike josh peter karl document document dynamic need find whether document ha predefined name defining everything entity spacy nlp right way
Getting full names from NER,"<p>From reading through the docs and playing with the API, it looks like CoreNLP will tell me the NER tags per token, but it won't help me extract out full names from a sentence. For example:</p>

<pre><code>Input: John Wayne and Mary have coffee
CoreNLP Output: (John,PERSON) (Wayne,PERSON) (and,O) (Mary,PERSON) (have,O) (coffee,O)
Desired Result: list of PERSON ==&gt; [John Wayne, Mary]
</code></pre>

<p>Unless there is some flag I missed, I believe to do this I will need to parse the tokens and glue together successive tokens tagged PERSON.</p>

<p>Can someone confirm that this is indeed what I need to do? I mostly want to know if there is some flag or utility in CoreNLP that does something like this for me. Bonus points if someone has a utility (ideally Java, since I'm using the Java API) that does this and wants to share :)</p>

<p>Thanks!</p>

<p>PS: There was a very similar question <a href=""https://stackoverflow.com/questions/25842982/tagging-full-name-in-stanford-ner"">here</a>, which seems to suggest the answer is ""roll your own"", but it was never confirmed by anyone else.</p>
",Named Entity Recognition (NER),getting full name ner reading doc playing api look like corenlp tell ner tag per token help extract full name sentence example unless flag missed believe need parse token glue together successive token tagged person someone confirm indeed need mostly want know flag utility corenlp doe something like bonus point someone ha utility ideally java since using java api doe want share thanks p wa similar question
Train Spacy default english model,"<p>I am trying to train Spacy's <code>en_core_web_lg</code> model.
I got the code from official documentation for trainig the new model. But I want to do training on top of <code>en_core_web_lg</code> model.</p>

<p>Here is the code:</p>

<pre><code>from __future__ import unicode_literals, print_function

import sys
import plac
import random
from pathlib import Path
import spacy
from spacy.util import minibatch, compounding

# training data Start
TRAIN_DATA = [(""The model of machine is PC-234w and its serial number is 322424-AGX."", {""entities"": [(24, 31, ""PRODUCT"")]}),(""The model of machine is PC-234w and its serial number is 322424-AGX."", {""entities"": [(57, 67, ""PRODUCT"")]})]
#Train data End
def main(model=""en_core_web_lg"", output_dir=None, n_iter=100):
    """"""Load the model, set up the pipeline and train the entity recognizer.""""""
    nlp = spacy.load(model)  # load existing spaCy model
    print(""Loaded model '%s'"" % model)


    # create the built-in pipeline components and add them to the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if ""ner"" not in nlp.pipe_names:
        ner = nlp.create_pipe(""ner"")
        nlp.add_pipe(ner, last=True)
    # otherwise, get it so we can add labels
    else:
        ner = nlp.get_pipe(""ner"")

    # add labels
    for _, annotations in TRAIN_DATA:
        for ent in annotations.get(""entities""):
            ner.add_label(ent[2])

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]
    with nlp.disable_pipes(*other_pipes):  # only train NER
        # reset and initialize the weights randomly – but only if we're
        # training a new model
        nlp.begin_training()
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            losses = {}
            # batch up the examples using spaCy's minibatch
            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(
                    texts,  # batch of texts
                    annotations,  # batch of annotations
                    drop=0.5,  # dropout - make it harder to memorise data
                    losses=losses,
                )
            print(""Losses"", losses)

    # test the trained model
    for text, _ in TRAIN_DATA:
        doc = nlp(text)
        print(""Entities"", [(ent.text, ent.label_) for ent in doc.ents])
        print(""Tokens"", [(t.text, t.ent_type_, t.ent_iob) for t in doc])

    # save model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.to_disk(output_dir)
        print(""Saved model to"", output_dir)

        # test the saved model
        print(""Loading from"", output_dir)
        nlp2 = spacy.load(""en_core_web_lg"")
        for text, _ in TRAIN_DATA:
            doc = nlp2(text)
            print(""Entities"", [(ent.text, ent.label_) for ent in doc.ents])



if __name__ == ""__main__"":
    plac.call(main)
</code></pre>

<p>And here is the ouput</p>

<pre><code>Entities [('PC-234w', 'PRODUCT'), ('322424-AGX', 'PRODUCT')]
Tokens [('The', '', 2), ('model', '', 2), ('of', '', 2), ('machine', '', 2), ('is', '', 2), ('PC-234w', 'PRODUCT', 3), ('and', '', 2), ('its', '', 2), ('serial', '', 2), ('number', '', 2), ('is', '', 2), ('322424-AGX', 'PRODUCT', 3), ('.', '', 2)]
</code></pre>

<p>But when I run the same model in different script it gives me different NER results.</p>

<p>Code:</p>

<pre><code>import spacy

nlp = spacy.load(""en_core_web_lg"")
doc = nlp(u""The model of machine is PC-234w and its serial number is 322424-AGX."")

for ent in doc.ents:
      print(ent.text, ent.label_)
</code></pre>

<p>Output:</p>

<pre><code>PC-234w ORG
</code></pre>

<p>Definitely I am doing something wrong, but I am unable to figure out what it is.</p>
",Named Entity Recognition (NER),train spacy default english model trying train spacy model got code official documentation trainig new model want training top model code ouput run model different script give different ner result code output definitely something wrong unable figure
Prepare data for scikit-learn,"<p>I am working on a small NLP project of authorship attribution: I have some texts from two authors and I want to say who wrote them. </p>

<p>I have some pre-processed text (tokenized, pos-tagged, ect.) and I want to load it into sciki-learn.</p>

<p>The documents have this shape:</p>

<pre><code>Testo   -   SPN Testo   testare+v+indic+pres+nil+1+sing testo+n+m+sing  O
:   -   XPS colon   colon+punc  O
""   -   XPO ""   quotation_mark+punc O
Buongiorno  -   I   buongiorno  buongiorno+inter buongiorno+n+m+_   O
a   -   E   a   a+prep  O
tutti   -   PP  tutto   tutto+adj+m+plur+pst+ind tutto+pron+_+m+_+plur+ind  O
.   &lt;eos&gt;   XPS full_stop   full_stop+punc  O
Ci  -   PP  pro loc+pron+loc+_+3+_+clit pro+pron+accdat+_+1+plur+clit   O
sarebbe -   VI  essere  essere+v+cond+pres+nil+2+sing   O
molto   -   B   molto   molto+adj+m+sing+pst+ind
</code></pre>

<p>So it's a tab separeted text file of 6 columns (word, end of sentence marker, part of speech, lemma, morphological information and named entity recognition marker).</p>

<p>Every file represents a document to classify.</p>

<p>What would be the best way to shape them for scikit learn?</p>
",Named Entity Recognition (NER),prepare data scikit learn working small nlp project authorship attribution text two author want say wrote pre processed text tokenized po tagged ect want load sciki learn document shape tab separeted text file column word end sentence marker part speech lemma morphological information named entity recognition marker every file represents document classify would best way shape scikit learn
How to train custom NER in Spacy with single words data set?,"<p>I am trying to train a custom ner in Spacy with the new entity 'ANIMAL'. But I have a data set with single words as:</p>

<pre><code>TRAIN_DATA = [(""Whale_ Blue"", {""entities"": [(0,11,LABEL)]}), (""Shark_ whale"", {""entities"": [(0,12,LABEL)]}), (""Elephant_ African"", {""entities"": [(0,17,LABEL)]}), (""Elephant_ Indian"", {""entities"": [(0,16,LABEL)]}), (""Giraffe_ male"", {""entities"": [(0,13,LABEL)]}), (""Mule"", {""entities"": [(0,4,LABEL)]}), (""Camel"", {""entities"": [(0,5,LABEL)]}), (""Horse"", {""entities"": [(0,5,LABEL)]}), (""Cow"", {""entities"": [(0,3,LABEL)]}), (""Dolphin_ Bottlenose"", {""entities"": [(0,19,LABEL)]}), (""Donkey"", {""entities"": [(0,6,LABEL)]}), (""Tapir"", {""entities"": [(0,5,LABEL)]}), (""Shark_ Hammerhead"", {""entities"": [(0,17,LABEL)]}), (""Seal_ fur"", {""entities"": [(0,9,LABEL)]}), (""Manatee"", {""entities"": [(0,7,LABEL)]}), (""Bear_ Grizzly"", {""entities"": [(0,13,LABEL)]}), (""Alligator_ American"", {""entities"": [(0,19,LABEL)]}), (""Sturgeon_ Atlantic"", {""entities"": [(0,18,LABEL)]}), (""Lion"", {""entities"": [(0,4,LABEL)]}), (""Bear_ American Black"", {""entities"": [(0,20,LABEL)]}), (""Ostrich"", {""entities"": [(0,7,LABEL)]}), (""Crocodile_ Saltwater"", {""entities"": [(0,20,LABEL)]}), (""Pig"", {""entities"": [(0,3,LABEL)]}), (""Sheep"", {""entities"": [(0,5,LABEL)]}), (""Dog_ Saint Bernard"", {""entities"": [(0,18,LABEL)]}), (""Human"", {""entities"": [(0,5,LABEL)]}), (""Deer_ white-tailed"", {""entities"": [(0,18,LABEL)]}), (""Tuna"", {""entities"": [(0,4,LABEL)]}), (""Salamander_ Japanese"", {""entities"": [(0,20,LABEL)]}), (""Carp"", {""entities"": [(0,4,LABEL)]}), (""Dog_ Foxhound"", {""entities"": [(0,13,LABEL)]}), (""Goat_ Milch"", {""entities"": [(0,11,LABEL)]}), (""Sting Ray"", {""entities"": [(0,9,LABEL)]}), (""Dog_ Pointer"", {""entities"": [(0,12,LABEL)]}), (""Kangaroo_ Red"", {""entities"": [(0,13,LABEL)]}), (""Cod_ Atlantic"", {""entities"": [(0,13,LABEL)]}), (""Dog_ Collie"", {""entities"": [(0,11,LABEL)]}), (""Pike_ Northern"", {""entities"": [(0,14,LABEL)]}), (""Trout_ brown"", {""entities"": [(0,12,LABEL)]}), (""Dog_ Basset Hound"", {""entities"": [(0,17,LABEL)]}), (""Turkey"", {""entities"": [(0,6,LABEL)]}), (""Porcupine"", {""entities"": [(0,9,LABEL)]}), (""Trout_ Rainbow"", {""entities"": [(0,14,LABEL)]}), (""Gar_ longnose"", {""entities"": [(0,13,LABEL)]}), (""Beaver"", {""entities"": [(0,6,LABEL)]}), (""Dog_ Irish Terrier"", {""entities"": [(0,18,LABEL)]}), (""Dog_ Beagle"", {""entities"": [(0,11,LABEL)]}), (""Bass_ Large Mouth Black"", {""entities"": [(0,23,LABEL)]}), (""Dog_ Whippet"", {""entities"": [(0,12,LABEL)]}), (""Dog_ Boston Terrier"", {""entities"": [(0,19,LABEL)]}), (""Nutria"", {""entities"": [(0,6,LABEL)]}), (""Dog_ Fox Terrier"", {""entities"": [(0,16,LABEL)]}), (""Armadillo_ Nine-banded"", {""entities"": [(0,22,LABEL)]}), (""Fox_ Arctic"", {""entities"": [(0,11,LABEL)]}), (""Woodchuck (Groundhog)"", {""entities"": [(0,21,LABEL)]}), (""Rabbit_ Domestic"", {""entities"": [(0,16,LABEL)]}), (""Chicken"", {""entities"": [(0,7,LABEL)]}), (""Dog_ Pekingese"", {""entities"": [(0,14,LABEL)]}), (""Haddock"", {""entities"": [(0,7,LABEL)]}), (""Cat_ domestic"", {""entities"": [(0,13,LABEL)]}), (""Salmon_ Chum"", {""entities"": [(0,12,LABEL)]}), (""Vulture_ Turkey"", {""entities"": [(0,15,LABEL)]}), (""Opossum_ Large American"", {""entities"": [(0,23,LABEL)]}), (""Flounder_ Winter"", {""entities"": [(0,16,LABEL)]}), (""Pheasant_ Ringnecked"", {""entities"": [(0,20,LABEL)]}), (""Perch"", {""entities"": [(0,5,LABEL)]}), (""Duck_ Mallard"", {""entities"": [(0,13,LABEL)]}), (""Mackerel_ Spanish"", {""entities"": [(0,17,LABEL)]}), (""Platypus_ Duck-billed"", {""entities"": [(0,21,LABEL)]}), (""Sea lamprey"", {""entities"": [(0,11,LABEL)]}), (""Bullhead_ Brown"", {""entities"": [(0,15,LABEL)]}), (""Mink_ American"", {""entities"": [(0,14,LABEL)]}), (""Falcon_ Peregrin"", {""entities"": [(0,16,LABEL)]}), (""Goshawk"", {""entities"": [(0,7,LABEL)]}), (""Bat_ Flying fox"", {""entities"": [(0,15,LABEL)]}), (""Duck_ Wood"", {""entities"": [(0,10,LABEL)]}), (""Buzzard"", {""entities"": [(0,7,LABEL)]}), (""Bass_ Rock"", {""entities"": [(0,10,LABEL)]}), (""Squirrel_ Gray"", {""entities"": [(0,14,LABEL)]}), (""Guinea Pig"", {""entities"": [(0,10,LABEL)]}), (""Rat_ Norway"", {""entities"": [(0,11,LABEL)]}), (""Gull_ Herring"", {""entities"": [(0,13,LABEL)]}), (""Crow_ Hooded"", {""entities"": [(0,12,LABEL)]}), (""Rook"", {""entities"": [(0,4,LABEL)]}), (""Pumpkinseed"", {""entities"": [(0,11,LABEL)]}), (""Pigeon"", {""entities"": [(0,6,LABEL)]}), (""Guinea fowl"", {""entities"": [(0,11,LABEL)]}), (""Quail_ Bobwhite"", {""entities"": [(0,15,LABEL)]}), (""Magpie_ Black-billed"", {""entities"": [(0,20,LABEL)]}), (""European Jackdaw"", {""entities"": [(0,16,LABEL)]}), (""Hamster"", {""entities"": [(0,7,LABEL)]}), (""Kestrel_ lesser"", {""entities"": [(0,15,LABEL)]}), (""Hawk_ Night"", {""entities"": [(0,11,LABEL)]}), (""Chipmunk_ Eastern"", {""entities"": [(0,17,LABEL)]}), (""Bat_ little brown"", {""entities"": [(0,17,LABEL)]}), (""Starling_ Common"", {""entities"": [(0,16,LABEL)]}), (""Frog_ leopard"", {""entities"": [(0,13,LABEL)]}), (""Weasel_ least"", {""entities"": [(0,13,LABEL)]}), (""Mouse_ White-footed"", {""entities"": [(0,19,LABEL)]}), (""Mouse_ House"", {""entities"": [(0,12,LABEL)]}), (""Canary"", {""entities"": [(0,6,LABEL)]}), (""Hummingbird"", {""entities"": [(0,11,LABEL)]}), (""Hummingbird_ Cuban bee"", {""entities"": [(0,22,LABEL)]}), (""Shrew_ Musked"", {""entities"": [(0,13,LABEL)]}), (""Shrew_ dwarf"", {""entities"": [(0,12,LABEL)]}), (""Goby_ Philippine"", {""entities"": [(0,16,LABEL)]}), (""Goldfish"", {""entities"": [(0,8,LABEL)]}), (""Toad_ American"", {""entities"": [(0,14,LABEL)]}), (""Frog_ Bull"", {""entities"": [(0,10,LABEL)]}), (""Eel_ American"", {""entities"": [(0,13,LABEL)]}), (""Penguin_ Adelie"", {""entities"": [(0,15,LABEL)]}), (""Robin"", {""entities"": [(0,5,LABEL)]}), (""Kiwi"", {""entities"": [(0,4,LABEL)]}), (""Fighting Fish_ Siamese"", {""entities"": [(0,22,LABEL)]}), (""Skate"", {""entities"": [(0,5,LABEL)]}), (""Quail_ Japanese/European"", {""entities"": [(0,24,LABEL)]}), (""Gila Monster"", {""entities"": [(0,12,LABEL)]}), (""Chameleon"", {""entities"": [(0,9,LABEL)]}), (""Cobra_ Indian"", {""entities"": [(0,13,LABEL)]}), (""Boa Constrictor"", {""entities"": [(0,15,LABEL)]}), (""Guppy"", {""entities"": [(0,5,LABEL)]}), (""Salamander_ Tiger"", {""entities"": [(0,17,LABEL)]}), (""Swordtail_ Mexican"", {""entities"": [(0,18,LABEL)]}), (""Stickleback_ three spine"", {""entities"": [(0,24,LABEL)]}), (""Sea horse"", {""entities"": [(0,9,LABEL)]}), (""Hellbender"", {""entities"": [(0,10,LABEL)]}), (""Herring_ Atlantic"", {""entities"": [(0,17,LABEL)]}), (""Chameleon_ Madagascar"", {""entities"": [(0,21,LABEL)]}), (""Frog_ Cuban"", {""entities"": [(0,11,LABEL)]}), ]
</code></pre>

<p>I have used the python script mention here <a href=""https://github.com/explosion/spaCy/blob/master/examples/training/train_new_entity_type.py"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/master/examples/training/train_new_entity_type.py</a></p>

<p>After training the model, I am getting the incorrect result as Spacy also detects other words 'ANIMAL'.</p>

<p>Can anyone guide me, how to do this in the right way? 
Spacy ver: 2.1.8</p>
",Named Entity Recognition (NER),train custom ner spacy single word data set trying train custom ner spacy new entity animal data set single word used python script mention training model getting incorrect result spacy also detects word animal anyone guide right way spacy ver
Spacy NER inferring GPE type,"<p>When using Spacy for NER, is there a way to extract information  if the entity type is city, country or state when entity type returned is GPE ?</p>
",Named Entity Recognition (NER),spacy ner inferring gpe type using spacy ner way extract information entity type city country state entity type returned gpe
Calculating confidence score for Entity in NLP Named-entity recognition,"<p>I am working on named-entity extraction from documents(pdfs). Each pdf contains set of entities (nearly 16 different type entities)</p>

<p>Here are my steps to build the NLP and ML models:</p>

<ul>
<li><code>Step 1</code> : Parsed documents. Got nearly 2 Million tokens (words). Used these words and CBOW method for building word2vec model.</li>
<li><code>Step 2</code> : By used word2vec model, generated vectors for words in douments.</li>
<li><code>Step 3</code> : As per the domain, i labeled words(vectors) for training, validation and testing.</li>
<li><code>Step 4</code> : With labeled data, train the Neural Network model.</li>
<li><code>Step 5</code>: Once model got build, given testing data (words) to the model. Got 85% accuracy.</li>
</ul>

<p>Till now everything going good. But problem is in next step. :(</p>

<ul>
<li><code>Step 6</code> : Now i want to make entities with confidence score from words which are classified from the trained model. </li>
</ul>

<p>Neural network model using <code>SOFTMAX</code> to classify input. From this model getting score for each word.</p>

<p>But my question is, my entities contains minimum 3 words. How can i calculate confidence score for generated entity.</p>

<p>right now i am using <code>P(entity) = P(w1)*P(w2)*(w3)</code>  if entity has three words.</p>

<p>Kindly help me. this approach wont make sense all the time.</p>

<p>suppose, if model predict only two words in entity then entity confidence will be  <code>P(entity) = P(w1)*P(w2)</code>.</p>

<p>And if model predict only one word in a entity then <code>P(entity) = P(w1)</code>. :(</p>
",Named Entity Recognition (NER),calculating confidence score entity nlp named entity recognition working named entity extraction document pdfs pdf contains set entity nearly different type entity step build nlp ml model parsed document got nearly million token word used word cbow method building word vec model used word vec model generated vector word douments per domain labeled word vector training validation testing labeled data train neural network model model got build given testing data word model got accuracy till everything going good problem next step want make entity confidence score word classified trained model neural network model using classify input model getting score word question entity contains minimum word calculate confidence score generated entity right using entity ha three word kindly help approach wont make sense time suppose model predict two word entity entity confidence model predict one word entity
"Is there a way to identify cities in a text without maintaining a prior vocabulary, in Python?","<p>I have to identify cities in a document (has only characters), I do not want to maintain an entire vocabulary as it is not a practical solution. I also do not have Azure text analytics api account. </p>

<p>I have already tried using Spacy,  I did ner and identified geolocation and that output is passed to spellchecker() to train the model. But the issue with this is that ner requires sentences and my input has words. </p>

<p>I am relatively new to this field.</p>
",Named Entity Recognition (NER),way identify city text without maintaining prior vocabulary python identify city document ha character want maintain entire vocabulary practical solution also azure text analytics api account already tried using spacy ner identified geolocation output passed spellchecker train model issue ner requires sentence input ha word relatively new field
Clarification on the use of Vocab file in NER,"<p>I am learning Named Entity Recognition, and i see that the training script uses a variable called <code>vocab</code> which looks like this </p>

<pre><code>vocab = ""0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\'-/\t \n\r\x0b\x0c:""
</code></pre>

<p>My Guess is that it is supposed to learn all these characters present in the text like abcd... etc, what i dont understand is the use of char like <code>/n /t</code>  what is the use of these char? and in general this variable?</p>

<p>Thanks in advance. </p>
",Named Entity Recognition (NER),clarification use vocab file ner learning named entity recognition see training script us variable called look like guess supposed learn character present text like abcd etc dont understand use char like use char general variable thanks advance
How to extract variables from sentences with node-nlp,"<p>I'm trying to extract variable(s) from sentences with node-nlp using the following code:</p>

<pre><code>const { NlpManager, ConversationContext } = require('node-nlp');

const manager = new NlpManager({ languages: ['en'] });
const context = new ConversationContext();

(async () =&gt; {
  manager.addDocument('en', 'Hello my name is %name%', 'greeting.hello');
  manager.addDocument('en', 'I have to go', 'greeting.bye');
  manager.addAnswer('en', 'greeting.hello', 'Hey there!');
  manager.addAnswer('en', 'greeting.bye', 'Till next time, {{name}}!');

  manager.train();

  const result1 = await manager.process('en', 'Hello my name is Thierry', context);
  console.log(result1);
  console.log(context);

  const result2 = await manager.process('en', 'I have to go', context);
  console.log(result2);
})();
</code></pre>

<p>The context doesn't contain a <code>name</code> variable...</p>

<p>I opened an issue directly on the github project page but the answer suggested to add this:</p>

<pre><code>manager.addNamedEntityText(
  'name',
  'John',
  ['en'],
  ['john', 'John'], 
);
</code></pre>

<p>But it's not exactly what I want because a variable <code>name</code> is added only if there is either ""John"" or ""john"" for the name in the sentence.</p>

<p>I also saw in this issue <a href=""https://github.com/axa-group/nlp.js/issues/133#issuecomment-503223171"" rel=""nofollow noreferrer"">https://github.com/axa-group/nlp.js/issues/133#issuecomment-503223171</a> what there are restrictions on variable names.</p>

<p>Thanks for your help!
Thierry</p>
",Named Entity Recognition (NER),extract variable sentence node nlp trying extract variable sentence node nlp using following code context contain variable opened issue directly github project page answer suggested add exactly want variable added either john john name sentence also saw issue restriction variable name thanks help thierry
Named entity recognization using multiple lines using Spacy NLP,"<p>In Spacy NLP, I am not able to get exact output for named entity. My string value is on multiple lines. Please check below code:</p>

<pre><code>from spacy import displacy
from collections import Counter
import en_core_web_sm
nlp = en_core_web_sm.load()
m = (u""""""Release the container 6th August

USG11223
USG12224
USG21113"""""")

doc = nlp(m)
print([(X.text, X.label_) for X in doc.ents])
</code></pre>

<p>OUTPUT: <code>[('6th August', 'DATE')]</code></p>

<p>But i want output like</p>

<p><code>['USG11223', 'USG12224', 'USG21113',6th August]</code></p>
",Named Entity Recognition (NER),named entity recognization using multiple line using spacy nlp spacy nlp able get exact output named entity string value multiple line please check code output want output like
How to prepare data for spacy&#39;s custom named entity recognition?,"<p>I'm trying to prepare a training dataset for custom named entity recognition using spacy. My data has a variable 'Text', which contains some sentences, a variable 'Names', which has names of people from the previous variable (sentences). After going through some examples and spacy's documentation, I realised that one has to pass index of the entity while preparing the dataset. I want to know if there's any way to pass the entity as a string directly while preparing the dataset ?</p>

<p>Reference: ""<a href=""https://medium.com/@manivannan_data/how-to-train-ner-with-custom-training-data-using-spacy-188e0e508c6"" rel=""nofollow noreferrer"">https://medium.com/@manivannan_data/how-to-train-ner-with-custom-training-data-using-spacy-188e0e508c6</a>""</p>
",Named Entity Recognition (NER),prepare data spacy custom named entity recognition trying prepare training dataset custom named entity recognition using spacy data ha variable text contains sentence variable name ha name people previous variable sentence going example spacy documentation realised one ha pas index entity preparing dataset want know way pas entity string directly preparing dataset reference
How to increase speed of this ner model implemented from scratch using 1 million labeled sentences,"<p>I would like to use spacy's NER model to train a model from scratch using 1 Million sentences. The model has only two types of entities. This is the code I am using. Since, I can't share the data, I created a dummy dataset. </p>

<p>My main issue is that the model is taking too long to train. I would appreciate it if you can highlight any error in my code or suggest other methods to try to fasten training. </p>

<pre><code>TRAIN_DATA = [ ('Ich bin in Bremen', {'entities': [(11, 17, 'loc')]})] * 1000000



import spacy
import random
from spacy.util import minibatch, compounding

def train_spacy(data,iterations):
    TRAIN_DATA = data
    nlp = spacy.blank('de')  
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)


    # add labels
    for _, annotations in TRAIN_DATA:
         for ent in annotations.get('entities'):
            ner.add_label(ent[2])

    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  
        optimizer = nlp.begin_training()
        for itn in range(iterations):
            print(""Statring iteration "" + str(itn))
            random.shuffle(TRAIN_DATA)
            losses = {}  
            batches = minibatch(TRAIN_DATA, size=compounding(100, 64.0, 1.001))
            for batch in batches:        
                texts, annotations = zip(*batch)
                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)
            print(""Losses"", losses)

    return nlp



model = train_spacy(TRAIN_DATA, 20)



</code></pre>
",Named Entity Recognition (NER),increase speed ner model implemented scratch using million labeled sentence would like use spacy ner model train model scratch using million sentence model ha two type entity code using since share data created dummy dataset main issue model taking long train would appreciate highlight error code suggest method try fasten training
How can I improve my spaCy model to perfectly recognise coordinates?,"<p>I trained a model for custom named entity recognition by using wonderful prodigy but It can not recognise coordinate in this:</p>

<p>""But the sun is 7° LONG 51’ 24“ removed from opposition  to Mars ""</p>

<p>as you see it gives me:</p>

<p>[7° LONG 51’ 24“] </p>

<p>but it should be like this</p>

<p>[7°  51’ 24“ LONG]
can anyone help me why it is so?</p>

<p>more detail:</p>

<p>I have done an annotation by regex for labels as following: Date, Time, Coordinate, then I trained my custom named entity recognition using Prodigy </p>

<p><a href=""https://prodi.gy/?gclid=Cj0KCQjwyLDpBRCxARIsAEENsrLs2bbv7QT-d7lq88ZHkYaxPjI9L3aR23uRQGgWOBV1spM5WGV_LrIaAv6pEALw_wcB"" rel=""nofollow noreferrer"">https://prodi.gy/?gclid=Cj0KCQjwyLDpBRCxARIsAEENsrLs2bbv7QT-d7lq88ZHkYaxPjI9L3aR23uRQGgWOBV1spM5WGV_LrIaAv6pEALw_wcB</a></p>

<p>I followed this step  for each label first I</p>

<p>1-edit annotation by this command (for instance for longitude)</p>

<pre><code>python -m prodigy ner.manual an_ner_date_time_02 en_core_web_sm AN_NER_DATE_TIME_01.jsonl  --label LONG
</code></pre>

<p>to edit (by an annotator) my per-annotated data by regex.  Then I merged my dataset. the next stage was to build a model by </p>

<p>2-bulid a model (for all merged data)</p>

<pre><code>python -m prodigy ner.batch-train data_merged_v06 en_core_web_sm --output Model_12 --n-iter 10 --eval-split 0.2 --dropout 0.2 --no-missing
</code></pre>

<p>question: 
how can I have better annotation regarding long,</p>

<p>So now I have a model which has this result:</p>

<pre><code>BEFORE     0.008
Correct    36
Incorrect  4438
Entities   2802
Unknown    0


#          LOSS       RIGHT      WRONG      ENTS       SKIP       ACCURACY
01         110.744    1614       163        1683       0          0.908
02         98.563     1660       107        1719       0          0.939
03         98.472     1668       96         1724       0          0.946
04         96.250     1673       93         1731       0          0.947
05         96.192     1679       80         1730       0          0.955
06         96.108     1678       71         1719       0          0.959
07         94.347     1681       67         1721       0          0.962
08         96.472     1679       66         1716       0          0.962
09         98.936     1681       57         1711       0          0.967
10         96.175     1681       57         1711       0          0.967

Correct    1681
Incorrect  57
Baseline   0.008
Accuracy   0.967
</code></pre>

<p>It can annotate the data as you see here (click the link)</p>

<p>![annotation text]</p>

<p>(<a href=""https://ibb.co/rtQm5B9"" rel=""nofollow noreferrer"">https://ibb.co/rtQm5B9</a>)</p>

<p>![annotation text]</p>

<p>the problem is the model can not learn perfectly coordinate as you see 
7° LONG 51’ 24“ it should be
7°  51’ 24“ LONG</p>

<p>is wrongly  annotated (however It annotated by regex correct in the training set but by the model in the test set no!)</p>

<p>do you have any idea how to improve this?</p>

<p>second question, How can I use the result of ner for the classification of sentences?</p>
",Named Entity Recognition (NER),improve spacy model perfectly recognise coordinate trained model custom named entity recognition using wonderful prodigy recognise coordinate sun long removed opposition mar see give long like long anyone help detail done annotation regex label following date time coordinate trained custom named entity recognition using prodigy followed step label first edit annotation command instance longitude edit annotator per annotated data regex merged dataset next stage wa build model bulid model merged data question better annotation regarding long model ha result annotate data see click link annotation text annotation text problem model learn perfectly coordinate see long long wrongly annotated however annotated regex correct training set model test set idea improve second question use result ner classification sentence
How to cluster Named Entity using StanfordNER using python,"<p>Stanford NER provides it NER jars to detect POS tags and NERs. But I am facing one issue with one of the sentences when trying to parse. The sentence is as follows:</p>

<pre><code>Joseph E. Seagram &amp; Sons, INC said on Thursday that it is merging its two United States based wine companies
</code></pre>

<p>Below is my code</p>

<pre><code>st = StanfordNERTagger('./stanford- ner/classifiers/english.all.3class.distsim.crf.ser.gz',
                       './stanford-ner/stanford-ner.jar',
                       encoding='utf-8')
ne_in_sent = []
with open(""./CCAT/2551newsML.txt"") as fd:
    lines = fd.readlines()
    for line in lines:
        print(line)
        tokenized_text = word_tokenize(line)
        classified_text = st.tag(tokenized_text)
        ne_tree = stanfordNE2tree(classified_text)
        for subtree in ne_tree:
            # If subtree is a noun chunk, i.e. NE != ""O""
            if type(subtree) == Tree:
                ne_label = subtree.label()
                ne_string = "" "".join([token for token, pos in subtree.leaves()])
                ne_in_sent.append((ne_string, ne_label))
                print(ne_in_sent)
</code></pre>

<p>when I parse it I get the following entities as the organization.
(Joseph E. Seagram &amp; Sons, Organization) and (Inc, Organization)</p>

<p>Also for some other texts in the file like</p>

<pre><code>TransCo has a very big plane. Transco is moving south.
</code></pre>

<p>It differentiates the organizations due to capitalization hence I get
2 entities (TransCo, organization) and (Transco, organization).</p>

<p>Is it possible to convert these into one entity?</p>
",Named Entity Recognition (NER),cluster named entity using stanfordner using python stanford ner provides ner jar detect po tag ners facing one issue one sentence trying parse sentence follows code parse get following entity organization joseph e seagram son organization inc organization also text file like differentiates organization due capitalization hence get entity transco organization transco organization possible convert one entity
Cross Validation with Spacy for Named Entity Recognition,"<p>I am trying to train a custom NER Model on 50,000 million samples. I am using <a href=""https://spacy.io/api/top-level#util.minibatch"" rel=""nofollow noreferrer"">minibatch</a> with 20 iterations for modeling. I want to understand if I should be using use Cross-Validation for more accurate out of sample accuracy. If yes then where should the cross-validation step take place? If no, then how do I split/distribute my training and testing data, since I am using annotations and 6 custom entities and it is hard to keep track of the percentages of annotated labels in each of training and test data since and evenly distribute it.</p>

<p>Here is the code I am using for training - </p>

<pre class=""lang-py prettyprint-override""><code>def train_spacy(data, iterations):
    TRAIN_DATA = data

    # create blank Language class
    nlp = spacy.blank('en')  

    # create the built-in pipeline components and add them to the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)

    # Add LABELS
    for _, annotations in TRAIN_DATA:
         for ent in annotations.get('entities'):
            ner.add_label(ent[2])

    # Get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']

    # only train NER
    with nlp.disable_pipes(*other_pipes):  
        optimizer = nlp.begin_training()
        for itn in range(iterations):
            print(""Starting iteration "" + str(itn))

            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(texts, annotations, sgd=optimizer, 
                           drop=0.20,losses=losses)
            print('Losses', losses)

    return nlp


if __name__ == ""__main__"":

    # Train formatted data
    model = train_spacy(data, 10)

</code></pre>

<p>I think cross-validation step should take place somewhere inside the for loop for iterations but I am not sure. Can someone throw some light on how to use cross-validation with Spacy NER or whether is it not needed at all?</p>
",Named Entity Recognition (NER),cross validation spacy named entity recognition trying train custom ner model million sample using minibatch iteration modeling want understand using use cross validation accurate sample accuracy yes cross validation step take place split distribute training testing data since using annotation custom entity hard keep track percentage annotated label training test data since evenly distribute code using training think cross validation step take place somewhere inside loop iteration sure someone throw light use cross validation spacy ner whether needed
NameError: name &#39;named_entity_extractor&#39; is not defined,"<pre><code>import sys, os
parent = os.path.dirname(os.path.realpath(""cobaie""))
sys.path.append(parent + '/../../mitielib')

from mitie import *
</code></pre>

<ul>
<li>The training process for a binary relation detector requires a <code>MITIE NER</code> object as</li>
<li>input.  So we load the saved <code>NER</code> model first.</li>
</ul>

<p><code>ner = named_entity_extractor(""../../MITIE-models/english/ner_model.dat"")</code></p>
",Named Entity Recognition (NER),nameerror name named entity extractor defined training process binary relation detector requires object input load saved model first
Large difference between overall F Score for a custom Spacy NER model and Individual Entity F Score,"<p>I am training a custom NER Model using Spacy on a sample of 5000 text entries with 6 entities. While evaluating the trained model on an unseen sample (500 text entries), the F Score that I get for the overall model (<code>93.8</code>) has a large difference between F Score for any individual entities. Can someone help me understand how is the overall F Score calculates and why is there so much difference between overall F Score and individual entity Score? </p>

<p>I built my own custom named entity recognition (NER) model using Spacy.  The size of my training data set was 5000 with 6 entities. Further, I tested my model on 500 samples and evaluated the model using the <code>Scorer</code> and <code>GoldParse</code>.</p>

<p>Here is my code for evaluating performance on my test data - </p>

<pre class=""lang-py prettyprint-override""><code>def evaluate(ner_model, examples):
    scorer = Scorer()
    for input_, annot in examples:
        doc_gold_text = ner_model.make_doc(input_)
        gold = GoldParse(doc_gold_text, entities=annot.get('entities'))
        pred_value = ner_model(input_)
        scorer.score(pred_value, gold)
    return scorer.scores
</code></pre>

<p>Here is the result that I get - </p>

<pre><code>{'uas': 0.0, 'las': 0.0, 'ents_p': 93.62838106164233, 'ents_r': 
 93.95728476332452, 'ents_f': 93.79254457050243,
 'ents_per_type': {
 'ENTITY1': {'p': 6.467595956926736, 'r': 54.51002227171492, 'f': 
 11.563219748420247},
 'ENTITY2': {'p': 6.272470243289469, 'r': 49.219391947411665, 'f': 
 11.126934984520123}, 
 'ENTITY3': {'p': 18.741109530583213, 'r': 85.02742820264602, 'f': 
 30.712745497989392}, 
 'ENTITY4': {'p': 13.413228854574788, 'r': 70.58823529411765, 'f': 
 22.54284884283916}, 
 'ENTITY5': {'p': 19.481765834932823, 'r': 82.85714285714286, 'f': 
 31.546231546231546}, 
'ENTITY6': {'p': 24.822695035460992, 'r': 64.02439024390245, 'f': 35.77512776831346}},
 'tags_acc': 0.0, 'token_acc': 100.0}
</code></pre>

<p>Here you can see a large difference between <code>ents_f</code> and <code>f</code> for any other entity type. What is the relationship of the overall F Score of the model with individual entity scores?</p>
",Named Entity Recognition (NER),large difference overall f score custom spacy ner model individual entity f score training custom ner model using spacy sample text entry entity evaluating trained model unseen sample text entry f score get overall model ha large difference f score individual entity someone help understand overall f score calculates much difference overall f score individual entity score built custom named entity recognition ner model using spacy size training data set wa entity tested model sample evaluated model using code evaluating performance test data result get see large difference entity type relationship overall f score model individual entity score
Selective text extraction in Python based on certain topics or keywords,"<p>I have a quite long text document describing behaviours of different animals. I want to extract text about a specific animal and haven't figured out how this can be done.</p>

<p>So for example, if the document descibes 15 different animals, I want my alorithm to output all information from the input file that related to lions. Lions described and discussed in several different places of the document - how do I do ""selective extraction"" for text that is only related to lions, does anyone know?</p>

<h1>EDIT - inputs and outputs</h1>

<p>Inputs:
(1) Text file (e.g. ""document.txt"")
(2) Key word(s) (e.g. ""lion"")</p>

<p>Output (example):
""Lions are large felines that are traditionally depicted as the 'king of the jungle.' These big cats once roamed Africa, Asia and Europe. [...] Males are generally larger than females and have a distinctive mane of hair around their heads [...] Asiatic lions eat large animals as well, such as goats, nilgai, chital, sambhar and buffaloes. [...] Females have a gestation period of around four months. She will give birth to her young away from others and hide the cubs for the first six weeks of their lives.""</p>
",Named Entity Recognition (NER),selective text extraction python based certain topic keywords quite long text document describing behaviour different animal want extract text specific animal figured done example document descibes different animal want alorithm output information input file related lion lion described discussed several different place document selective extraction text related lion doe anyone know edit input output input text file e g document txt key word e g lion output example lion large feline traditionally depicted king jungle big cat roamed africa asia europe male generally larger female distinctive mane hair around head asiatic lion eat large animal well goat nilgai chital sambhar buffalo female gestation period around four month give birth young away others hide cub first six week life
Are there any approaches/suggestiosn for classifying a keyword so the search space will be reduced in elasticsearch?,"<p>I was wondering is there any way to classify a single word before applying a search on elasticsearch. Let's say I have 4 indexes each one holds few millions documents about a specific category. </p>

<p><a href=""https://i.sstatic.net/CuTWA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CuTWA.png"" alt=""enter image description here""></a></p>

<p>I'd like to avoid searching the whole search space each time. 
This problem becomes more challenging since it's not a sentence, 
The search query usually consists only a single or two words, so some nlp <code>magic</code> (Named-entity recognition, POS etc) can't be applied. </p>

<p>I have read few questions on stackoverflow like:</p>

<p><a href=""https://stackoverflow.com/questions/8772692/semantic-search-with-nlp-and-elasticsearch"">Semantic search with NLP and elasticsearch</a></p>

<p><a href=""https://stackoverflow.com/questions/12290667/identifying-a-persons-name-vs-a-dictionary-word"">Identifying a person's name vs. a dictionary word</a></p>

<p>and few more, but couldn't find an approach. are there any suggestions I should try?</p>
",Named Entity Recognition (NER),approach suggestiosn classifying keyword search space reduced elasticsearch wa wondering way classify single word applying search elasticsearch let say index one hold million document specific category like avoid searching whole search space time problem becomes challenging since sentence search query usually consists single two word nlp named entity recognition po etc applied read question stackoverflow like find approach suggestion try
How the model is retrain by spacy?,"<p>Earlier token 'Modi' is recognised as an Org by spacy to I retrain it with the following code:</p>

<pre><code>import spacy 
import random
nlp = spacy.load('en')
nlp.entity.add_label('CELEBRITY')
TRAIN_DATA = [
        (u""Modi"", {""entities"": [(0, 4, ""PERSON"")]}),
        (u""India"", {""entities"": [(0, 5, ""GPE"")]})]

optimizer = nlp.begin_training()
for i in range(20):
    random.shuffle(TRAIN_DATA)
    for text, annotations in TRAIN_DATA:
        nlp.update([text], [annotations],drop=0.3, sgd=optimizer)


text = ""But Modi is starting India. The company made a late push\ninto hardware, and Apple’s Siri and Google available on iPhones, and Amazon’s Alexa\nsoftware, which runs on its Echo and Dot devices, have clear leads in\nconsumer adoption.""
doc = nlp(text)
for ent in doc.ents:
    print(ent.text,ent.label_)

</code></pre>

<p>And I got the following answer:</p>

<pre><code>Modi PERSON
India GPE
Apple’s Siri ORG
Google ORG
iPhones ORG
Amazon GPE
Echo PERSON
Dot PERSON

</code></pre>

<p>It changes the Modi to the person at the same time it doing incorrect NER as compare to the previous mode. In the previous model, Amazon was recognized as ORG but now change to GPE.
Now I add the extra-label CELEBRITY and categorize Modi to CELEBRITY with this following code </p>

<pre><code>
import spacy 
import random
nlp = spacy.load('en')
nlp.entity.add_label('CELEBRITY')
TRAIN_DATA = [
        (u""Modi"", {""entities"": [(0, 4, ""CELEBRITY"")]})]

optimizer = nlp.begin_training()
for i in range(20):
    random.shuffle(TRAIN_DATA)
    for text, annotations in TRAIN_DATA:
        nlp.update([text], [annotations],drop=0.3, sgd=optimizer)


text = ""But Modi is starting India. The company made a late push\ninto hardware, and Apple’s Siri and Google available on iPhones, and Amazon’s Alexa\nsoftware, which runs on its Echo and Dot devices, have clear leads in\nconsumer adoption.""
doc = nlp(text)
for ent in doc.ents:
    print(ent.text,ent.label_)
</code></pre>

<p>But looks like it crashes my model and getting the following result:</p>

<pre><code>But CELEBRITY
Modi CELEBRITY
is CELEBRITY
starting CELEBRITY
India GPE
. CELEBRITY
The CELEBRITY
company CELEBRITY
made CELEBRITY
a CELEBRITY
late CELEBRITY
push CELEBRITY
into CELEBRITY
hardware CELEBRITY
, CELEBRITY
and CELEBRITY
Apple CELEBRITY
</code></pre>

<p>Please let me know the behind the seen reason and also how can I achieve that only entity which I label should change while all other should be according to spacy.</p>
",Named Entity Recognition (NER),model retrain spacy earlier token modi recognised org spacy retrain following code got following answer change modi person time incorrect ner compare previous mode previous model amazon wa recognized org change gpe add extra label celebrity categorize modi celebrity following code look like crash model getting following result please let know behind seen reason also achieve entity label change according spacy
Extract the relative sentence for entity recognised by TextRazor,"<p>I am using Textrazor and want to figure out the sentence from which the keywords are identified, which I am not able to. 
The documentation doesn't contain much information on it and nor found anywhere on the internet.</p>

<p>How can I extract the sentence related to the keyword identified. </p>

<pre><code>import textrazor
key = ""key""

textrazor.api_key = key

client = textrazor.TextRazor(extractors=[""word"",""entities"", ""topics"",""sentence"",""words""])

for entity,sentence in zip(response.entities(),response.sentences()):
        print(sentence.words)
</code></pre>

<p>The print statement do results into words of the sentence but in textRazor class format and are not interpritable by python.</p>

<p>Output is as follow:</p>

<pre><code>[TextRazor Word:""b'If'"" at position 196, TextRazor Word:""b'aggression'"" at position 197, TextRazor Word:""b'helps'"" at position 198, TextRazor Word:""b'in'"" at position 199, TextRazor Word:""b'the'"" at position 200, TextRazor Word:""b'survival'"" at position 201, TextRazor Word:""b'of'"" at position 202, TextRazor Word:""b'our'"" at position 203, TextRazor Word:""b'genes'"" at position 204, TextRazor Word:""b','"" at position 205, TextRazor Word:""b'then'"" at position 206, TextRazor Word:""b'the'"" at position 207, TextRazor Word:""b'process'"" at position 208, TextRazor Word:""b'of'"" at position 209, TextRazor Word:""b'natural'"" at position 210, TextRazor Word:""b'selection'"" at position 211, TextRazor Word:""b'may'"" at position 212, TextRazor Word:""b'well'"" at position 213, TextRazor Word:""b'have'"" at position 214, TextRazor Word:""b'caused'"" at position 215, TextRazor Word:""b'humans'"" at position 216, TextRazor Word:""b','"" at position 217, TextRazor Word:""b'as'"" at position 218, TextRazor Word:""b'it'"" at position 219, TextRazor Word:""b'would'"" at position 220, TextRazor Word:""b'any'"" at position 221, TextRazor Word:""b'other'"" at position 222, TextRazor Word:""b'animal'"" at position 223, TextRazor Word:""b','"" at position 224, TextRazor Word:""b'to'"" at position 225, TextRazor Word:""b'be'"" at position 226, TextRazor Word:""b'aggressive'"" at position 227, TextRazor Word:""b'-LRB-'"" at position 228, TextRazor Word:""b'Buss'"" at position 229, TextRazor Word:""b'&amp;'"" at position 230, TextRazor Word:""b'Duntley'"" at position 231, TextRazor Word:""b','"" at position 232, TextRazor Word:""b'2006'"" at position 233, TextRazor Word:""b'-RRB-'"" at position 234, TextRazor Word:""b'.'"" at position 235]
</code></pre>
",Named Entity Recognition (NER),extract relative sentence entity recognised textrazor using textrazor want figure sentence keywords identified able documentation contain much information found anywhere internet extract sentence related keyword identified print statement result word sentence textrazor class format interpritable python output follow
Extract text from .txt file and save into .csv files with columns and header,"<p>I have approximately 100 text files with clinical notes that consist of 1-2 paragraphs. Each file is named doc_1.txt to doc_179.txt accordingly. I would like to save the text from each file into a .csv file with 2 columns w/ headers (id, text). The  <code>id</code> columns are the name of each files. </p>

<p>For example <code>doc_1</code> is the record file name and will become the id. The text in <code>doc_1</code> will be stored the <code>text column</code>. The desired results is below</p>

<pre><code>
|   id  | text |
|:-----:|:----:|
| doc_1 | abcf |
| doc_2 | efrf |
| doc_3 | gvni |


</code></pre>

<p>So far I am to just viewed the text and have not determine the best practical way to achieve my results. </p>
",Named Entity Recognition (NER),extract text txt file save csv file column header approximately text file clinical note consist paragraph file named doc txt doc txt accordingly would like save text file csv file column w header id text column name file example record file name become id text stored desired result far viewed text determine best practical way achieve result
"How to perform NER on true case, then lemmatization on lower case, with spaCy","<p>I try to lemmatize a text using spaCy 2.0.12 with the French model <code>fr_core_news_sm</code>. Morevoer, I want to replace people names by an arbitrary sequence of characters, detecting such names using <code>token.ent_type_ == 'PER'</code>. Example outcome would be ""Pierre aime les chiens"" -> ""~PER~ aimer chien"".</p>

<p>The problem is I can't find a way to do both. I only have these two partial options:</p>

<ul>
<li>I can feed the pipeline with the original text: <code>doc = nlp(text)</code>. Then, the NER will recognize most people names but the lemmas of words starting with a capital won't be correct. For example, the lemmas of the simple question ""Pouvons-nous faire ça?"" would be <code>['Pouvons', '-', 'se', 'faire', 'ça', '?']</code>, where ""Pouvons"" is still an inflected form.</li>
<li>I can feed the pipeline with the lower case text: <code>doc = nlp(text.lower())</code>. Then my previous example would correctly display <code>['pouvoir', '-', 'se', 'faire', 'ça', '?']</code>, but most people names wouldn't be recognized as entities by the NER, as I guess a starting capital is a useful indicator for finding entities.</li>
</ul>

<p>My idea would be to perform the standard pipeline (tagger, parser, NER), then lowercase, and then lemmatize only at the end.</p>

<p>However, lemmatization doesn't seem to have its own pipeline component and the documentation doesn't explain how and where it is performed. <a href=""https://stackoverflow.com/a/52560132/11774841"">This</a> answer seem to imply that lemmatization is performed independent of any pipeline component and possibly at different stages of it. </p>

<p>So my question is: how to choose when to perform the lemmatization and which input to give to it?</p>
",Named Entity Recognition (NER),perform ner true case lemmatization lower case spacy try lemmatize text using spacy french model morevoer want replace people name arbitrary sequence character detecting name using example outcome would pierre aime le chiens per aimer chien problem find way two partial option feed pipeline original text ner recognize people name lemma word starting capital correct example lemma simple question pouvons nous faire would pouvons still inflected form feed pipeline lower case text previous example would correctly display people name recognized entity ner guess starting capital useful indicator finding entity idea would perform standard pipeline tagger parser ner lowercase lemmatize end however lemmatization seem pipeline component documentation explain performed href answer seem imply lemmatization performed independent pipeline component possibly different stage p question choose perform lemmatization input give
Training spacy model not working: running the train_ner script has no effect,"<p>I am writing a program that uses the spacy model en_core_web_md for Named Entity Recognition. It was not identifying all my entities correctly: for instance, there were some names of people and organisations that were not being recognised as such.</p>

<p>I looked up how to train the model and found this script: <a href=""https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py</a></p>

<p>I downloaded the script, put it in the same folder as my program, replaced their training data with my own (containing the names I wanted it to recognise) and ran it, with <code>model=""en_core_web_md""</code> and <code>output_dir=""model""</code> instead of <code>None</code>.</p>

<p>My project involves video game characters so my training data was:</p>

<pre class=""lang-py prettyprint-override""><code>TRAIN_DATA = [
    (""Who is Cave Johnson?"", {""entities"": [(7, 19, ""PERSON"")]}),
    (""I work for Aperture Science."", {""entities"":[(11, 27, ""ORG"")]}),
    (""Wallace Breen is CEO of Black Mesa."", {""entities"":[(0, 13, ""PERSON""), (25, 35, ""ORG"")]}),

]
</code></pre>

<p>The train_ner script outputs the expected results. However, when I run my other program, it still does not recognise ""Cave Johnson"" as a <code>PERSON</code> or ""Black Mesa"" as an <code>ORG</code>. Why is the script not working?</p>

<p>Update: still haven't got it working. I ran the script again, to no apparent effect.</p>
",Named Entity Recognition (NER),training spacy model working running train ner script ha effect writing program us spacy model en core web md named entity recognition wa identifying entity correctly instance name people organisation recognised looked train model found script downloaded script put folder program replaced training data containing name wanted recognise ran instead project involves video game character training data wa train ner script output expected result however run program still doe recognise cave johnson black mesa script working update still got working ran script apparent effect
how to use gazetteer features as input in other models with BILOU,"<p>I was working on a gazetteer list to use for NER tagging but with CRF or LSTM models. I had a doubt if I have a bigram as a gazetteer then do both individual words have the feature as 1 ? For example I saw ""Bring Apple Juice"" and Apple Juice is in gazetteer list for Products then do both Apple and Juice have an entry of 1 or only the second word Juice has it ? </p>
",Named Entity Recognition (NER),use gazetteer feature input model bilou wa working gazetteer list use ner tagging crf lstm model doubt bigram gazetteer individual word feature example saw bring apple juice apple juice gazetteer list product apple juice entry second word juice ha
How to extract coefficients for TF-IDF features?,"<p>I have a dataset where I am using a text column to predict some numerical column. </p>

<p><strong>My ultimate question is this: Which words in the text column are associated with a higher/lower score?</strong> </p>

<p>So my pipeline is to first vectorize my text column, and then use a ridge regression. But after I build this pipeline how do I extract the coefficents on the vectorizer feature names?</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split

# This is my toy data 
d = {'text': [""I am a a string"", ""And I am a string"", ""I, too am string"", ""And me"", ""Me too""], 
     'target': [3, 4, 14, 6, 7]}
df = pd.DataFrame(d)

X_train, X_test, y_train, y_test= train_test_split(df['text'], df['target'], 
                                                   test_size=0.3, random_state=42)


# Here is a vectorizer 
vect = TfidfVectorizer(stop_words='english')
X_train_vect = vect.fit_transform(X_train)

# Here is a ridge regressor
model = Ridge(random_state=42)
model.fit(X_train_vect, y_train)

# Now we make a pipeline
pipe = Pipeline([('vect',vect),('model',model)])
y_pred = pipe.predict(X_test)
</code></pre>

<p>How would I go about about extracting words as coefficents from here? 
E.g: <code>""I am"": 0.05</code> or whatever</p>
",Named Entity Recognition (NER),extract coefficient tf idf feature dataset using text column predict numerical column ultimate question word text column associated higher lower score pipeline first vectorize text column use ridge regression build pipeline extract coefficents vectorizer feature name would go extracting word coefficents e g whatever
Extract relevant sentences to entity,"<p>Do you know some paper or algorithm in NLP that is able to extract sentences from text that are related to given entity (term). I would like to process some reviews (mainly tech), but I found out that many reviews mention more then one product (they do comparation). I would like to extract from that text just sentences that are relevant to one product, or delete sentences that are irrelevant to particular named entity (product).</p>

<p>My questin is how to do it? Is there some related papers? Is something like this done by some toolkit or api?</p>
",Named Entity Recognition (NER),extract relevant sentence entity know paper algorithm nlp able extract sentence text related given entity term would like process review mainly tech found many review mention one product comparation would like extract text sentence relevant one product delete sentence irrelevant particular named entity product questin related paper something like done toolkit api
Perform Named Entity Recognition - NLP,"<p>I am trying to learn how to perform Named Entity Recognition.</p>

<p>I have a set of discharge summaries containing medical information about patients. I converted my unstructured data into structured data. Now, I have a <code>DataFrame</code> that looks like this:</p>

<pre><code>Text                        |   Target
normal coronary arteries...     R060
</code></pre>

<p>The <code>Text</code> column contains information about the diagnosis of a patient, and the <code>Target</code> column contains the code that will need to be predicted in a further task.</p>

<p>I have also constructed a dictionary that looks like this:</p>

<pre><code>Code (Key) | Term (Value)
A00          Cholera
</code></pre>

<p>This dictionary brings information about each diagnosis and the afferent code. The <code>term</code> column will be used to identify the clinical entities in the corpus.</p>

<p>I will need to train a classifier and predict the code in order to automate the process of assigning codes for the discharge summaries (I am explaining this to have an idea about the task I'm performing).</p>

<p>Until now I have converted my data into a structured one. I am trying to understand how I should perform Named Entity Recognition to label the medical terminology. I would like to try direct matching and fuzzy matching but I am not sure what are the previous steps. Should I perform tokenizing, stemming, lemmatizing before? Or firstly should I find the medical terminology as clinical named entities are often multi-token terms with nested structures that include other named entities inside them? Also what packages or tools are you recommending me to use in Python?</p>

<p>I am new in this field so any help will be appreciated! Thanks!</p>
",Named Entity Recognition (NER),perform named entity recognition nlp trying learn perform named entity recognition set discharge summary containing medical information patient converted unstructured data structured data look like column contains information diagnosis patient column contains code need predicted task also constructed dictionary look like dictionary brings information diagnosis afferent code column used identify clinical entity corpus need train classifier predict code order automate process assigning code discharge summary explaining idea task performing converted data structured one trying understand perform named entity recognition label medical terminology would like try direct matching fuzzy matching sure previous step perform tokenizing stemming lemmatizing firstly find medical terminology clinical named entity often multi token term nested structure include named entity inside also package tool recommending use python new field help appreciated thanks
for loop not iterating through every row,"<p>I have a corpus of text, consisting of multiple MS Word files, that I would like to analyse. As the corpus is large (~10,000 lines) and nlp (using the <code>cleanNLP</code> package) analysis takes a long time and frequently crashes, I thought I could loop through the text line by line and analyse each one separately.</p>

<p>I've written the following loop, which aims to take each line of the initial text, extract any location entities and store the details in the next empty line of the matrix <code>text_mat</code>.</p>

<pre><code>#read in text corpus
all &lt;- read_dir(""N:/data/All"")

#convert into dataframe usable by text packages
all_df &lt;- tibble(line = 1:nrow(all), text = all$content)

#loop through every line for location extraction
#create unpopulated matrix
text_mat &lt;- matrix(NA, nrow = nrow(all_df), ncol = 3)

#loop through each line, fill matrix with location output
for (i in nrow(all_df)) {
  line &lt;- all_df[i, ]
  obj_line &lt;- cnlp_annotate(line, as_strings = TRUE)
  loc &lt;- cnlp_get_entity(obj_line) %&gt;%
    filter(entity_type == ""CITY"" | entity_type == ""LOCATION"") %&gt;%
    group_by(entity) %&gt;%
    tally() %&gt;%
    arrange(desc(n)) %&gt;%
    rename(""Count"" = ""n"")
  text_mat[i, ] &lt;- c(i, loc$entity, loc$Count)
  next 
}

#convert matrix to data frame
entity_df &lt;- as.data.frame(text_mat)  
</code></pre>

<p>When I run the loop it completes very quickly - I would expect this to take at least a few minutes, and the <code>text_mat</code> matrix remains empty. This makes me think that the loop is only analysing the first line of text and then completing but I'm not sure why. Any help as to why this would be the case would be greatly appreciated.</p>
",Named Entity Recognition (NER),loop iterating every row corpus text consisting multiple word file would like analyse corpus large line nlp using package analysis take long time frequently crash thought could loop text line line analyse one separately written following loop aim take line initial text extract location entity store detail next empty line matrix run loop completes quickly would expect take least minute matrix remains empty make think loop analysing first line text completing sure help would case would greatly appreciated
"How do I either remove &#39;\n&#39; from my nltk tokens, or prevent it from appearing in the first place, after converting a string to a list?","<p>I've converted a column from a CSV to a list, and then a string for tokenization. After it's converted to a string I get '\n' throughout. I'm looking to either prevent that from happening completely, or remove it after it happens. </p>

<p>So far, I've tried replace, strip, and rstrip to no avail.</p>

<p>Here's a version where I tried .replace() after converting the list to a string. </p>

<pre><code>df = pd.read_csv('raw_da_qs.csv')
question = df['question_only']
question = question.str.replace(r'\d+','')
question = str(question.tolist())
question = question.replace('\n','')
tokenizer = nltk.tokenize.RegexpTokenizer('\w+')
tokens = tokenizer.tokenize(question)
</code></pre>

<p>and I end up with tokens like this 'nthere', and 'nsuicide'</p>
",Named Entity Recognition (NER),either remove n nltk token prevent appearing first place converting string list converted column csv list string tokenization converted string get n throughout looking either prevent happening completely remove happens far tried replace strip rstrip avail version tried replace converting list string end token like nthere nsuicide
What are the core parts for making an impressive autosuggestion like Quora?,"<p>I have recently been asked a design question in an interview:</p>

<blockquote>
  <p>How do you think the Quora's autosuggestions works?</p>
</blockquote>

<p>I tried my best, but I'm not sure I nailed it :/ I drew a diagram sketch with a few Elasticsearch components, digesting the node part of the pipeline which is responsible to the <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">named-entity recognition</a> (NER) before indexing the data, and synonyms for the token filter.</p>

<p>Can somebody from the field, elaborate more in depth the core components for Quora's search box, including some essential parts of the classification process?</p>

<p>(I would like to get better in this field.)</p>

<p>Examples:</p>

<p>I have entered <code>Shrek</code>, and it suggested <code>Shrek (character)</code>.</p>

<p>I have entered <code>einstein</code>, and it suggested <code>Albert Einstein (physicist)</code>.</p>

<p>I have entered <code>john</code>, and it suggested many people profiles which start with ""John"".</p>

<p>Amazingly this retrieval process takes a few milliseconds. That's impressive. How it is done?</p>
",Named Entity Recognition (NER),core part making impressive autosuggestion like quora recently asked design question interview think quora autosuggestion work tried best sure nailed drew diagram sketch elasticsearch component digesting node part pipeline responsible named entity recognition ner indexing data synonym token filter somebody field elaborate depth core component quora search box including essential part classification process would like get better field example entered suggested entered suggested entered suggested many people profile start john amazingly retrieval process take millisecond impressive done
Disease named entity recognition,"<p>I have a bunch of text documents that describe diseases. Those documents are in most cases quite short and often only contain a single sentence. An example is given here:</p>

<blockquote>
  <p>Primary pulmonary hypertension is a progressive disease in which widespread occlusion of the smallest pulmonary arteries leads to increased pulmonary vascular resistance, and subsequently right ventricular failure.</p>
</blockquote>

<p>What I need is a tool that finds all disease terms (e.g. ""pulmonary hypertension"" in this case) in the sentences and maps them to a controlled vocabulary like <a href=""http://www.ncbi.nlm.nih.gov/mesh"" rel=""noreferrer"">MeSH</a>.</p>

<p>Thanks in advance for your answers!</p>
",Named Entity Recognition (NER),disease named entity recognition bunch text document describe disease document case quite short often contain single sentence example given primary pulmonary hypertension progressive disease widespread occlusion smallest pulmonary artery lead increased pulmonary vascular resistance subsequently right ventricular failure need tool find disease term e g pulmonary hypertension case sentence map controlled vocabulary like mesh thanks advance answer
What model is Rasa NLU entity extraction using? Is it LSTM or just a simple neural network?,"<p>What kind of model is RASA NLU using to extract the entities and intents after word embedding?</p>
",Named Entity Recognition (NER),model rasa nlu entity extraction using lstm simple neural network kind model rasa nlu using extract entity intent word embedding
Extract verbs from sentence in R?,"<p>Please note that I am aware of <a href=""https://stackoverflow.com/questions/2970829/extracting-nouns-and-verbs-from-text"">Extracting Nouns and Verbs from Text</a>
and it doesn't work for me because the function they use doesn't exist in <code>openNLP</code> package.</p>

<p>Here is my column of strings:</p>

<pre><code>tibble(recipe_name = c(""Easter Leftover Sandwich"", ""Pasta with Pesto Cream Sauce"", 
""Herb Roasted Pork Tenderloin with Preserves"", ""Chicken Florentine Pasta"", 
""Perfect Iced Coffee"", ""Easy Green Chile Enchiladas"", ""Krispy Easter Eggs"", 
""Patty Melts"", ""Yum. Doughnuts!"", ""Buttery Lemon Parsley Noodles"", 
""Roast Chicken"", ""Baked French Toast"", ""Yummy Slice-and-Bake Cookies"", 
""Yummy Grilled Zucchini"", ""Chocolate Covered S’mores"", ""T-Bone Steaks with Hotel Butter"", 
""Mango Margaritas!"", ""Tuscan Bean Soup with Shrimp"", ""Hoppin’ John"", 
""Turkey Bagel Burger""))
</code></pre>

<p>I want to run an analysis that will find out all the verbs/nouns etc. in each of the names.</p>

<p>How can I do this in R?
I have checked <code>qdap</code> and <code>tm</code> packages but didn't find a function that will extract it.</p>

<p>Please advise how to do this.</p>
",Named Entity Recognition (NER),extract verb sentence r please note aware href noun verb text work function use exist package column string want run analysis find verb noun etc name r checked package find function extract please advise
RASA NLU overfitting entity extraction,"<p><strong>Rasa version:</strong> 0.1.1</p>

<p><strong>Python version:</strong> 3.7</p>

<p><strong>Operating system:</strong> osx</p>

<p><strong>Issue:</strong></p>

<p>I am making an application that can pick entities from a grocery list. For the first version I am trying only with a couple of products/brands.</p>

<p>My training data has been generated using chatito and these are the query shapes:</p>

<pre><code>%[inventory_count]('training': '0.99', 'testing': '')
    order @[count] @[units?] of @[brand?] @[product]
    @[count] @[units?] @[product] by @[brand]
    @[brand] @[product] is @[count] @[units?]
    add @[brand] @[product] @[count] @[units?]
    @[brand] @[product] @[count] @[units?] on floor
</code></pre>

<p>The product list is something like this:</p>

<pre><code>spinach 5 oz
spinach 5 ounces
spinach 16 oz
spinach 16 ounces
spinach
spinach sixteen ounces
spinach five ounces
gala apples
gala apple
apples
apple
</code></pre>

<p>The trained model is able to detect all the products in the list above but it is also detecting arbitrary products like this (there can be potential errors in the text and I wanted to test for the negative case):</p>

<pre><code>spinach 50 oz
call apples
</code></pre>

<p>Is this overfitting? What are the possible solutions?</p>

<p>Content of configuration file (config.yml):</p>

<pre><code>language: ""en""

pipeline:
- name: ""tokenizer_whitespace""
- name: ""ner_crf""
- name: ""intent_featurizer_count_vectors""
- name: ""intent_classifier_tensorflow_embedding""
  droprate: 0.5
  epochs: 300
  C2: 0.02
</code></pre>
",Named Entity Recognition (NER),rasa nlu overfitting entity extraction rasa version python version operating system osx issue making application pick entity grocery list first version trying couple product brand training data ha generated using chatito query shape product list something like trained model able detect product list also detecting arbitrary product like potential error text wanted test negative case overfitting possible solution content configuration file config yml
Spacy NER doesn&#39;t identify lowercase entities,"<p>I am facing problem to detect named entities which starts with lowercase letter. I have tried the solution provided on link <a href=""https://github.com/explosion/spaCy/issues/701"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/issues/701</a>. It seems to be not working for me.</p>

<p>===== Info about spaCy=============</p>

<pre><code>spaCy version    2.1.4
Platform         Darwin-16.7.0-x86_64-i386-64bit
Python version   3.6.5
Models           en
</code></pre>

<pre><code>import spacy
from spacy import displacy
nlp = spacy.load('en_core_web_sm')
sk = nlp.vocab[u'south korea']
SK = nlp.vocab[u'South Korea']
sk.is_lower = SK.is_lower
sk.shape = SK.shape
sk.shape_ = SK.shape_
sk.is_upper =SK.is_upper
sk.cluster = SK.cluster
sk.is_title = SK.is_title
doc = nlp(u'south korea is a country in asia')
for word in doc:
    print(word.text, word.tag_, word.ent_type_) 

</code></pre>

<p>The expected output is: </p>

<pre><code>south NNP GPE
korea NNP GPE
is VBZ 
a DT 
country NN 
in IN 
asia NNP 
</code></pre>

<p>But the output of above code is:</p>

<pre><code>south JJ 
korea NN 
is VBZ 
a DT 
country NN 
in IN 
asia NNP 
</code></pre>
",Named Entity Recognition (NER),spacy ner identify lowercase entity facing problem detect named entity start lowercase letter tried solution provided link seems working info spacy expected output output code
Use OpenIE to extract relations given entities,"<p>I want to know if it is possible to use OpenIE or if there is an available option with which I can specify the entities instead of OpenIE extracting them from Text. And given the entities it finds relation between them?</p>

<p>Eg. Obama was president of US. 
Input - Obama, US
Output - president of</p>
",Named Entity Recognition (NER),use openie extract relation given entity want know possible use openie available option specify entity instead openie extracting text given entity find relation eg obama wa president u input obama u output president
How to prepare text for more successful &quot;person&quot; entity type classification when using stanford Named Entity Recognition,"<p>I am using Stanford NER classification as part of a PHI De-identification process running on laboratory text notes. I am noticing that in some cases, the classification tags e.g <code>&lt;PERSON&gt;&lt;/PERSON&gt;</code> tags can find a person name, but then continue to tag much more text either side of the found name. This loss of precision means that we could potentially lose a lot of non-PHI and valuable info. Is there a way to prepare text in such a way that entities are more precisely discovered?</p>
",Named Entity Recognition (NER),prepare text successful person entity type classification using stanford named entity recognition using stanford ner classification part phi de identification process running laboratory text note noticing case classification tag e g tag find person name continue tag much text either side found name loss precision mean could potentially lose lot non phi valuable info way prepare text way entity precisely discovered
How to train my own NER model with stanford libraries?,"<p>I have been suffering from hundreds of emails about travel information. One of my job is saving some of the information in the emails into out system db.<br>
My plan is to make this happen automatically, and this is why I started to study StanfordNER and IE stuffs.</p>

<p>Here we go,<br>
This is my email example. It is not a sentence, and contains even some code.  </p>

<h3>sample email</h3>

<pre><code>NO. PETER 17 HIGHSCHOOL/2TH/OPEN
LONDON,ENGLAND STY 12-13TH JUNE

NO. JAKE 12 HIGHSCHOOL/OPEN
LIVERPOOL,ENGLAND 12,13 JUNE
</code></pre>

<p>I need only name, location and dates from these
So I made my tsv</p>

<h3>dummy-vess-corpus.tsv</h3>

<pre><code>NO  O
.   O
PETER   PERSON
JAKE    PERSON
17  O
12  O
HIGHSCHOOL  O
2TH O
OPEN    O
LONDON  CITY
LIVERPOOL   CITY
ENGLAND COUNTRY
12-13TH DATE
12  DATE
13  DATE
JUNE    MONTH
</code></pre>

<h3>prop.txt</h3>

<pre><code>trainFile = train/dummy-vess-corpus.tsv
serializeTo = dummy-ner-model-vess.ser.gz
map = word=0,answer=1

useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
useDisjunctive=true
</code></pre>

<h3>build model cmd</h3>

<pre><code>java -cp ""stanford-ner.jar:lib/*"" -mx4g edu.stanford.nlp.ie.crf.CRFClassifier -prop train/prop.txt
</code></pre>

<h3>output</h3>

<pre><code>[('NO', 'O'), ('.', 'O'), ('PETER', 'O'), ('17', 'O'), 
('HIGHSCHOOL2THOPEN', 'O'), ('LONDON', 'CITY'), (',', 'CITY'), 
('ENGLAND','COUNTRY'), ('STY', 'DATE'), ('12-13TH', 'DATE'), ('JUNE', 'MONTH'), 
('NO', 'O'), ('.', 'O'), ('JAKE', 'O'), ('12', 'O'), ('HIGHSCHOOLOPEN', 'O'), 
('LIVERPOOL', 'O'), (',', 'O'), ('ENGLAND', 'COUNTRY'), ('12,13', 'DATE'), ('JUNE', 'MONTH')]
</code></pre>

<p>It does not work at all. I have been looking for the goole to find out the way of traing, but I can find only simple examples...</p>
",Named Entity Recognition (NER),train ner model stanford library suffering hundred email travel information one job saving information email system db plan make happen automatically started study stanfordner ie stuff go email example sentence contains even code sample email need name location date made tsv dummy f corpus tsv prop txt build model cmd output doe work looking goole find way traing find simple example
Additional Named Entity Recognition models for Stanford CoreNLP?,"<p>The <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">Stanford CoreNLP</a> library is packaged with models to recognize Time, Location, Organization, Person, Money, Percent, and Dates.  Are there any other general-use models available from other groups that recognize additional things?</p>

<p>Also, if we were to train <a href=""http://nlp.stanford.edu/software/crf-faq.shtml"" rel=""nofollow"">a new model</a> to recognize <em>just</em> band names (for instance), could we run our new model in addition to the packaged ones, or would be have to train the new model to recognize Time, Location, Organization, Person, Money, Percent, Dates, and <em>Bands</em> all together if we wanted to do that?  The documentation does say the existing models themselves cannot be extended.</p>
",Named Entity Recognition (NER),additional named entity recognition model stanford corenlp stanford corenlp library packaged model recognize time location organization person money percent date general use model available group recognize additional thing also train new model recognize band name instance could run new model addition packaged one would train new model recognize time location organization person money percent date band together wanted documentation doe say existing model extended
Running stanford NER classifier in server mode,"<p>To make the NER classification faster I am trying to execute it in server mode listerning on port xxxx, so that it can give faster result when request is send.</p>

<p>Here is the original execution command without server that I am using.</p>

<pre><code>java -mx1500m -cp $1/stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier $1/classifiers/ner-eng-ie.crf-3-all2008-distsim.ser.gz -textFile $2
</code></pre>

<p>(this command is in .sh file and executed by python script. $1 is input file name)</p>

<p>This documentation explain how it can be run in server mode - <a href=""https://github.com/dat/stanford-ner"" rel=""nofollow noreferrer"">Link</a>
Here how the server get started:</p>

<pre><code>java -mx400m -cp stanford-ner.jar edu.stanford.nlp.ie.NERServer -loadClassifier classifiers/ner-eng-ie.crf-3-all2008.ser.gz 1234
</code></pre>

<p>Now server is in listerning mode on port 1234.</p>

<p><strong>How can I make call using input text file for this server?</strong></p>

<p>I followed this tut : <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#cc"" rel=""nofollow noreferrer"">Link</a> and executed this command:</p>

<pre><code>java -cp stanford-ner.jar edu.stanford.nlp.ie.NERServer -port 1234 -client 
</code></pre>

<p>But it just print this message:</p>

<pre><code>Usage: NERServer [-loadFile file|-loadJarFile resource] portNumber
</code></pre>

<p>I am working on linux system.</p>
",Named Entity Recognition (NER),running stanford ner classifier server mode make ner classification faster trying execute server mode listerning port xxxx give faster result request send original execution command without server using command sh file executed python script input file name documentation explain run server mode link server get started server listerning mode port make call using input text file server followed tut link executed command print message working linux system
Running the Stanford NLP Pipeline in stages,"<p>I am trying to run the core pipeline in multiple steps to cut down on expensive parsing and annotation steps.</p>

<p>I have a collection of documents, currently I am tokenizing and Sentence breaking them with the pipeline. This is working great and I can do what I need to with this information, up to a point.</p>

<p>Depending on processing I do of the tokenized sentences, some need to dependency parsed, some need NER, some can just be thrown out as unimportant.</p>

<p>I can see that the dependency parsers can take in a CoreMap of a sentence and give me back the dependency graph. But reading online I see that having POS tagging will improve the parsing, which of course makes sense. The NER is also going to need POS most likely?</p>

<p>Sentences that need NER do not need to be dependency parsed, for me.</p>

<p>The POS tagger though does not appear to be able to take in a CoreMap representing a tokenized sentence though.</p>

<p>Is there a way I can run POS tagging and then either NER or dependency parsing on a tokenized sentence from the core pipeline only running tokenize and ssplit?</p>
",Named Entity Recognition (NER),running stanford nlp pipeline stage trying run core pipeline multiple step cut expensive parsing annotation step collection document currently tokenizing sentence breaking pipeline working great need information point depending processing tokenized sentence need dependency parsed need ner thrown unimportant see dependency parser take coremap sentence give back dependency graph reading online see po tagging improve parsing course make sense ner also going need po likely sentence need ner need dependency parsed po tagger though doe appear able take coremap representing tokenized sentence though way run po tagging either ner dependency parsing tokenized sentence core pipeline running tokenize ssplit
Passing parameters to the Stanford NER in C#,"<p>I am loading the Stanford NER module in my C# code list this:</p>

<pre><code>var classifier = CRFClassifier.getClassifierNoExceptions(@""..\..\NER\english.all.3class.distsim.crf.ser.gz"");
</code></pre>

<p>I can see from the documentation (<a href=""https://stanfordnlp.github.io/CoreNLP/memory-time.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/memory-time.html</a>) that there are a number of parameters that would be useful in optimising my processing of text.</p>

<p>For example:</p>

<pre><code>edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz -ner.useSUTime false -ner.applyNumericClassifiers false
</code></pre>

<p>Can anyone advise how I can pass these parameters into my classifier in c# please?</p>
",Named Entity Recognition (NER),passing parameter stanford ner c loading stanford ner module c code list see documentation number parameter would useful optimising processing text example anyone advise pas parameter classifier c please
Extracting entities from utterances in node-nlp,"<p>I'm using node-nlp to create a chatbot for helping a user to book a package/trip and I want to extract destination_name, start_date, end_date, adults and children from utterances. Currently, I'm using the following approach :</p>

<pre><code>   nlp_manager.addDocument('en', 'planning %destination_name% from %start_date% till %end_date% with %adult% parents and %child% kids','book_package');
   nlp_manager.addDocument('en', 'visiting %destination_name% from %start_date% to %end_date% with %adult% adult and %child% childs','book_package');
</code></pre>

<p>And for pulling entities I'm using trim named entities :</p>

<pre><code>   const destination_name = manager.addTrimEntity('destination_name','trim');
   destination_name.addBetweenCondition('en', 'visit', 'with');
   destination_name.addBetweenCondition('en', 'visit', 'from');
   destination_name.addBetweenCondition('en', 'going to', 'from');
   destination_name.addBetweenCondition('en', 'going to', 'with');
   destination_name.addBetweenCondition('en', 'packages to', 'with');
   destination_name.addBetweenCondition('en', 'package to', 'with');
   destination_name.addBetweenCondition('en', 'package to', 'from');

   const adult = manager.addTrimEntity('adult','trim');
   adult.addBetweenCondition('en','with','adults and');
   adult.addBetweenCondition('en','and','adults');

   const child = manager.addTrimEntity('child','trim');
   child.addBetweenCondition('en','and','childs');
   child.addBetweenCondition('en','with','childs and');

   const start_date = manager.addTrimEntity('start_date','trim');
   start_date.addBetweenCondition('en','from','to');
   start_date.addBetweenCondition('en','from','till');

   const end_date = manager.addTrimEntity('end_date','trim');
   end_date.addBetweenCondition('en','to','with');
   end_date.addBetweenCondition('en','till','with');
</code></pre>

<p>After training the model and giving the bot an utterance the NER manager is not able to extract the entities from the utterance. An example of utterance :</p>

<pre><code>   i am planning to visit europe from 21 june to 25 june with 2 adults and 3 childs
</code></pre>

<p>Can you suggest a better way of doing it.
Here's a link to the package <a href=""https://www.npmjs.com/package/node-nlp"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/node-nlp</a></p>
",Named Entity Recognition (NER),extracting entity utterance node nlp using node nlp create chatbot helping user book package trip want extract destination name start date end date adult child utterance currently using following approach pulling entity using trim named entity training model giving bot utterance ner manager able extract entity utterance example utterance suggest better way link package
how to find strings left and right from named entity with spacy,"<p>I'm new in Python and NLP (using SpaCy), so I'm hoping someone can help me out. I want to detect the Named Entities in my text and right after I want to get five words left and right from the NEs. </p>

<p>I found already the NEs but I'm stuck finding the ""surrounding words""</p>

<pre><code>import spacy

nlp=spacy.load(""en_core_web_sm"")

doc = nlp(open(path to my text).read())

for index, token in enumerate(doc.ents): 
    if token.label_ == ""PERSON"" and token.text == ""Frodo"" or token.text == ""Frodo Beutlin"":
        print(token[:index])
        print(token[index])
        print(token[index:])
</code></pre>

<pre><code>Frodo Beutlin
think
</code></pre>

<p>This is my result, as you can see the strings before my NE are not shown. Also I am confused how to get more than one string (before and after).</p>
",Named Entity Recognition (NER),find string left right named entity spacy new python nlp using spacy hoping someone help want detect named entity text right want get five word left right ne found already ne stuck finding surrounding word result see string ne shown also confused get one string
Is there a way to turn off specific built-in tokenization rules in Spacy?,"<p>Spacy automatically tokenizes word contractions such as ""dont"" and ""don't"" into ""do"" and ""nt""/""n't"".  For instance, a sentence like ""I dont understand"" would be tokenized into: <b>[""I"", ""do"", ""nt"", ""understand""]</b>. </p>

<p>I understand this is usually helpful in many NLP tasks, but is there a way to suppress this <a href=""https://spacy.io/usage/linguistic-features#tokenization"" rel=""nofollow noreferrer"">special tokenization rule in Spacy</a> such that the result is <b>[""I"", ""dont"", ""understand""]</b> instead? </p>

<p>This is because I am trying to evaluate the performance (f1-score for BIO tagging scheme) of my custom Spacy NER model, and the mismatch in the number of tokens in the input sentence and the number of predicated token tags is causing problems for my evaluation code down the line:</p>

<ul>
<li><p>Input (3 tokens): <b>[(""I"", ""O""), (""dont"", ""O""), (""understand"", ""O"")]</b> </p></li>
<li><p>Predicted (4 tokens): <b>[(""I"", ""O""), (""do"", ""O""), (""nt"", ""O""), (""understand"", ""O"")]</b> </p></li>
</ul>

<p>Of course, if anyone has any suggestions for a better way to perform evaluation on sequential tagging tasks in Spacy (perhaps like the <a href=""https://github.com/chakki-works/seqeval"" rel=""nofollow noreferrer"">seqeval</a> package but more compatible with Spacy's token format), that would be greatly appreciated as well.</p>
",Named Entity Recognition (NER),way turn specific built tokenization rule spacy spacy automatically tokenizes word contraction dont nt n instance sentence like dont understand would tokenized nt understand understand usually helpful many nlp task way suppress special tokenization rule spacy result dont understand instead trying evaluate performance f score bio tagging scheme custom spacy ner model mismatch number token input sentence number predicated token tag causing problem evaluation code line input token dont understand predicted token nt understand course anyone ha suggestion better way perform evaluation sequential tagging task spacy perhaps like seqeval package compatible spacy token format would greatly appreciated well
Spacy Named Entity Recognition Issue,"<p>I am trying to tag ORG's from a bunch of text I am parsing through</p>

<p>What I have so far is as follows:</p>

<pre><code>import spacy
import en_core_web_sm
nlp = en_core_web_sm.load()

file = open(""C:\\sample.txt"")

doc = nlp(file.read())
print([(X.text, X.label_) for X in doc.ents])
</code></pre>

<p>Now, my result prints all possible tags, I just want it to print ORGs instead. Any suggestions on how to do that?</p>
",Named Entity Recognition (NER),spacy named entity recognition issue trying tag org bunch text parsing far follows result print possible tag want print orgs instead suggestion
Extract the most relevant location corresponding to a keyword,"<p>I'm implementing an application that tracks the locations of Australia's sharks through analysing a Twitter dataset. So I'm using shark as the keyword and search for the Twitts that contains ""shark"" and a location phrase.  </p>

<p>So the question is how to identify that ""Airlie Beach at Hardy Reef"" is the one that is correlated to ""shark""? If it's possible, can anyone provide a working code of Python to demonstrate​? Thank you so much!</p>
",Named Entity Recognition (NER),extract relevant location corresponding keyword implementing application track location australia shark analysing twitter dataset using shark keyword search twitts contains shark location phrase question identify airlie beach hardy reef one correlated shark possible anyone provide working code python demonstrate thank much
Is 100 training examples sufficient for training custom NER using spacy?,"<p>I have trained NER model for names data. I generated some random sentences which contain names of the person. I generated some 70 sentences and annotated the data in spacy's format.</p>

<p>I trained custom NER using both blank 'en' model and 'en_core_web_sm' but when I tested on any string. It is able to detect in very few examples.</p>

<p>Is this number of examples are insufficient?</p>

<pre><code>My data looks like this -:

[(""'Hi, I am looking for a house on rent for a year. Best Regards, Rajesh',\r"",
  {'entities': [(56, 63, 'name')]}),
 (""'Hello everyone, I am Gunjan Arora',\r"", {'entities': [(22, 34, 'name')]}),
 (""'Greetings!, I am 34 years old. I want a car for my wife Bella Roy',\r"",
  {'entities': [(60, 69, 'name')]}),
 (""'Heyo, I lived with my family comprises 4 people and myself Randy Lao',\r"",
  {'entities': [(60, 69, 'name')]}),
 (""'I am Geetanjali. ',\r"", {'entities': [(6, 16, 'name')]})]

I have generated some 70 examples like this.

Losses during training -:

 - 1.Losses {'ner': 6.307317615201415} 
 - 2.Losses {'ner': 11.182436657139132}
 - 3.Losses {'ner': 6.014345924849759}
 - 4.Losses {'ner': 6.442589285506237}
 - 5.Losses {'ner': 5.328383899880891}
 - 6.Losses {'ner': 1.706726450400089}
 - 7.Losses {'ner': 3.9960324752880005}
 - 8.Losses {'ner': 5.415169572852782}

These losses when I am using blank 'en' model
</code></pre>

<p>Please suggest.</p>

<p>I wanted to detect names as the pre-trained model itself is not able to detect names in most of the cases as well.</p>
",Named Entity Recognition (NER),training example sufficient training custom ner using spacy trained ner model name data generated random sentence contain name person generated sentence annotated data spacy format trained custom ner using blank en model en core web sm tested string able detect example number example insufficient please suggest wanted detect name pre trained model able detect name case well
"StanfordNLP, CoreNLP, spaCy - different dependency graphs","<p>I'm trying to use simple rules/patterns defined over a dependency graph to extract very basic informations from sentences (e.g., triples such as subject->predicate->object). I started using <a href=""https://stanfordnlp.github.io/stanfordnlp/pipeline.html"" rel=""nofollow noreferrer"">StanfordNLP</a> since it was easy to set up and utlizes the GPU for better performance. However, I've noticed that for some sentences, the resulting dependency graph looked not as I would have expected -- I'm no expert though. I therefore tried two other solutions: <a href=""https://spacy.io/usage/linguistic-features"" rel=""nofollow noreferrer"">spaCy</a> and <a href=""https://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">Stanford CoreNLP</a> (I understand that these are maintained by different groups?)</p>

<p>For the example sentence <em>""Tom made Sam believe that Alice has cancer.""</em> I've printed the dependencies for all three approaches. CoreNLP and spaCy yield the same dependencies, and they are different from the ones of StanfordNLP. Hence, I'm inclined to swich to CoreNLP and spaCy (another advantage would be that they come with NER out of the box).</p>

<p>Does anyone have some more experience or feedback that would help where to go from here? I don't expect that CoreNLP and spaCy will always yield in the same dependency graphs, but in the example sentence, considering <code>Sam</code> as <code>obj</code> as StandfordNLP is doing compared to being <code>nsubj</code> (CoreNLP, spaCy) seems to be a significant difference</p>

<pre><code>Format:
token   dependency_tag   parent_token

StanfordNLP
Tom     nsubj   made
made    ROOT    ROOT
Sam     obj     made
believe ccomp   made
that    mark    has
Alice   nsubj   has
has     ccomp   believe
cancer  obj     has
.       punct   made

CoreNLP
Tom     nsubj   made
made    ROOT    ROOT
Sam     nsubj   believe
believe ccomp   made
that    mark    has
Alice   nsubj   has
has     ccomp   believe
cancer  dobj    has
.       punct   made

spaCy
Tom     nsubj   made
made    ROOT    ROOT
Sam     nsubj   believe
believe ccomp   made
that    mark    has
Alice   nsubj   has
has     ccomp   believe
cancer  dobj    has
.       punct   made
</code></pre>
",Named Entity Recognition (NER),stanfordnlp corenlp spacy different dependency graph trying use simple rule pattern defined dependency graph extract basic information sentence e g triple subject predicate object started using stanfordnlp since wa easy set utlizes gpu better performance however noticed sentence resulting dependency graph looked would expected expert though therefore tried two solution spacy stanford corenlp understand maintained different group example sentence tom made sam believe alice ha cancer printed dependency three approach corenlp spacy yield dependency different one stanfordnlp hence inclined swich corenlp spacy another advantage would come ner box doe anyone experience feedback would help go expect corenlp spacy always yield dependency graph example sentence considering standfordnlp compared corenlp spacy seems significant difference
"UIMA, extraction semi-structured (tabular) data out of the text","<p>I am working on application using Apache UIMA for NLP task about domain specific entity extraction. </p>

<p><strong>The use case is following:</strong></p>

<p>There is Office document or PDF (both scanned, non-scanned) as the input, the application needs to get domain specific data out of it. The document could have free text or/and key-values,  tables,  pictures </p>

<p><strong>What are the challenges</strong>:</p>

<p>Sometimes the original document can contain tables (w/ metadata or w/o). There is no problem to annotate specific standalone token. However, I am looking for some example of building relationships between annotated tokens inside the table (say, it has headers with some business attributes and rows underneath contains the attributes values so I need to create proper relationships as well as to define groups so I can later extract instances of information, say, each row of the table is a one business entity instance compiled of some primitive entities and bounded by relationships).</p>

<p>So there are questions:</p>

<ol>
<li>I am looking for something that is more flexible and human readable in terms of the annotation rules i.e. can I use Ruta in such scenarios when table-form data needs to be annotated? Any rule examples would be very help. The research over this topic did not give much yet. </li>
<li>I am looking for approach how to extract the data if no metadata exist (see below. Would Ruta suites here or anything else? Any examples would be appreciated</li>
<li>I am looking for the tools which will simplify work with annotated text i.e. for profiling, testings purposes. Again, would Ruta solve it?</li>
</ol>

<p><strong>Examples:</strong></p>

<ol>
<li>OCR w/ metadata, data after extract stage:</li>
</ol>

<pre><code>&lt;table&gt; 
    &lt;tr&gt; 
      &lt;th&gt;Name&lt;/th&gt; 
      &lt;th&gt;Favorite Color&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
      &lt;td&gt;Bob&lt;/td&gt; 
      &lt;td&gt;Yellow&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
      &lt;td&gt;Michelle&lt;/td&gt; 
      &lt;td&gt;Purple&lt;/td&gt; 
    &lt;/tr&gt; 
&lt;/table&gt;
</code></pre>

<ol start=""2"">
<li>OCR <strong>w/o</strong> metadata, data after extract stage:</li>
</ol>

<pre><code>Name    Favorite Color
Bob Yellow
Michelle    Purple
</code></pre>
",Named Entity Recognition (NER),uima extraction semi structured tabular data text working application using apache uima nlp task domain specific entity extraction use case following office document pdf scanned non scanned input application need get domain specific data document could free text key value table picture challenge sometimes original document contain table w metadata w problem annotate specific standalone token however looking example building relationship annotated token inside table say ha header business attribute row underneath contains attribute value need create proper relationship well define group later extract instance information say row table one business entity instance compiled primitive entity bounded relationship question looking something flexible human readable term annotation rule e use ruta scenario table form data need annotated rule example would help research topic give much yet looking approach extract data metadata exist see would ruta suite anything else example would appreciated looking tool simplify work annotated text e profiling testing purpose would ruta solve example ocr w metadata data extract stage ocr w metadata data extract stage
Extract FAQ content from websites of different domain name,"<p>Currently, I have used Scrapy and bs4 to do web crawling on individual website's faq contents.</p>

<p><strong>However, as different websites format their html structures differently, I will need to adjust the tags or xpath whenever I crawl a new faq page.</strong></p>

<p>I have discovered that google's Dialogflow has introduced ""Knowledge base"" feature, in which the user just need to parse a faq website to their system and they will generate list of faq. </p>

<p>Now, I am thinking of ways to speed up my goal:</p>

<ul>
<li><strong><em>Crawl FAQ data from different websites.</em></strong></li>
</ul>

<p><strong>May I know if there's any lib I can use to speed up the process?
Or if there's a way for me to download the generated Q&amp;A pairs from Dialogflow client.</strong></p>
",Named Entity Recognition (NER),extract faq content website different domain name currently used scrapy b web crawling individual website faq content however different website format html structure differently need adjust tag xpath whenever crawl new faq page discovered google dialogflow ha introduced knowledge base feature user need parse faq website system generate list faq thinking way speed goal crawl faq data different website may know lib use speed process way download generated q pair dialogflow client
Extracting nationalities and countries from text,"<p>I want to extract all country and nationality mentions from text using nltk, I used POS tagging to extract all GPE labeled tokens but the results were not satisfying. </p>

<pre><code> abstract=""Thyroid-associated orbitopathy (TO) is an autoimmune-mediated orbital inflammation that can lead to disfigurement and blindness. Multiple genetic loci have been associated with Graves' disease, but the genetic basis for TO is largely unknown. This study aimed to identify loci associated with TO in individuals with Graves' disease, using a genome-wide association scan (GWAS) for the first time to our knowledge in TO.Genome-wide association scan was performed on pooled DNA from an Australian Caucasian discovery cohort of 265 participants with Graves' disease and TO (cases) and 147 patients with Graves' disease without TO (controls). ""

  sent = nltk.tokenize.wordpunct_tokenize(abstract)
  pos_tag = nltk.pos_tag(sent)
  nes = nltk.ne_chunk(pos_tag)
  places = []
  for ne in nes:
      if type(ne) is nltk.tree.Tree:
         if (ne.label() == 'GPE'):
            places.append(u' '.join([i[0] for i in ne.leaves()]))
      if len(places) == 0:
          places.append(""N/A"")
</code></pre>

<p>The results obtained are :</p>

<pre><code>['Thyroid', 'Australian', 'Caucasian', 'Graves']
</code></pre>

<p>Some are nationalities but others are just nouns.</p>

<p>So what am I doing wrong or is there another way to extract such info?</p>
",Named Entity Recognition (NER),extracting nationality country text want extract country nationality mention text using nltk used po tagging extract gpe labeled token result satisfying result obtained nationality others noun wrong another way extract info
Is there a way to extract the &#39;NAME of the company&#39; &#39;TITLE of the job&#39; and &#39;LOCATION of the job&#39; from each line of string below,"<p>From each line of string below, I want to extract company name, job title and location of the job. Is there a way to do so? as the pattern is not consistent. Thanks.</p>

<pre><code>""Jerry (YC S17) Is Hiring Senior Software Dev, Data Engineer (Toronto/Remote)""

""Iris Automation Is Hiring an Account Executive for B2B Flying Vehicle Software""

""Strikingly (YC W13) is hiring in our Shanghai office""

""BuildZoom (YC W13) is hiring  help make remodeling cheaper""

""EquipmentShare (YC W15) Is Looking for an Experienced React Native Dev""

""Saleswhale (YC S16) AI Assistant Startup Is Hiring Customer Success Managers""

""Streak (YC S11) is profitable, well funded and hiring in Vancouver""

""Tesorio (YC S15) Is Hiring Engineering Managers, Senior Python Engineer""

""Checkr (YC S14) is hiring engineers to build the future of online trust""

""Rescale Is Hiring a Senior DevOps Engineer in San Francisco""

""Tremendous.com is hiring its first engineer""

""Remix is looking for a front-end engineer to help build better public transit""

""Atomwise (YC W15) Is Hiring a Senior Machine Learning Research Scientist in SF""

""Confident Cannabis (YC S15) Is Hiring Engineers""

""WaystoCap (YC W17) is hiring a software engineer in Spain""

""Smarking (YC W15) Is Hiring a Customer Service Manager""

""Sunsama (YC W19) Is Hiring a Senior Full Stack Engineer (RN/GraphQL/Node)""

""Pachyderm Raised $10M and Is Looking for a Senior Full-Stack Engineer""

""Picktrace (YC S15) is hiring a senior Android engineer""

""Segment is hiring engineers to create our developer platform""

""XIX Is Hiring a Senior Front End Engineer""

""Athelas (YC S16) is hiring software engineers""

""Dyneti (YC W19) is hiring software engineers""

""ZeroCater (YC W11) Is Hiring a Principal Engineer in SF: Must Love Food""

""Mux is looking for developers who want to help developers build better video""

""Munich, Germany: Demodesk (YC W19) Is Hiring Software Engineers""

""New Story (YC Nonprofit) Hiring a JavaScript Software Engineer""

""Quit Genius (YC W18) Is Hiring a Product Manager in London""

""Flexport is hiring senior engineers in SF  Come get to know us""

""OneSignal Is Hiring Ruby on Rails and DevOps Engineers in San Mateo""
</code></pre>

<p><b>************* This is what I want **************</b></p>

<p><em>Example 1</em></p>

<p>""Jerry (YC S17) Is Hiring Senior Software Dev, Data Engineer (Toronto/Remote)""</p>

<p><b>Company Name: </b> Jerry</p>

<p><b>Job Title:</b> Senior Software Dev, Data Engineer</p>

<p><b>Location:</b> Toronto/Remote</p>

<p><em>Example 2</em></p>

<p>""Remix is looking for a front-end engineer to help build better public transit""</p>

<p><b>Company Name: </b> Remix</p>

<p><b>Job Title:</b> front-end engineer</p>

<p><b>Location:</b> </p>

<p><em>Example 3</em></p>

<p>""Munich, Germany: Demodesk (YC W19) Is Hiring Software Engineers""</p>

<p><b>Company Name: </b> Demodesk</p>

<p><b>Job Title:</b> Software engineer</p>

<p><b>Location:</b> Munich, Germany</p>
",Named Entity Recognition (NER),way extract name company title job location job line string line string want extract company name job title location job way pattern consistent thanks want example jerry yc hiring senior software dev data engineer toronto remote company name jerry job title senior software dev data engineer location toronto remote example remix looking front end engineer help build better public transit company name remix job title front end engineer location example munich germany demodesk yc w hiring software engineer company name demodesk job title software engineer location munich germany
Extracting a particular type of data from unstructured text namely Institutes,"<p>I need to extract the names of Institutes from the given data. Institues names will look similar ( Anna University, Mashsa Institute of Techology , Banglore School of Engineering, Model Engineering College). It will be a lot of similar data. I want to extract these from text. How can I create a model to extract these names from data(I need to extract from resumes-C.V)</p>

<p>I tried adding new NER in spacy but even after training, the loss doesnt decrease and predictions are wrong. That is why I want to make a new model just for this.</p>
",Named Entity Recognition (NER),extracting particular type data unstructured text namely institute need extract name institute given data institues name look similar anna university mashsa institute techology banglore school engineering model engineering college lot similar data want extract text create model extract name data need extract resume c v tried adding new ner spacy even training loss doesnt decrease prediction wrong want make new model
Extract specific type of word from paragraph using natural language processing,"<p>I am developing model which extract specific type of words. consider my data-set is as follow: </p>

<pre><code>  1. I want to book movie tickets for 2 peoples.
  2. I need to book movie tickets for 2 seats.
  3. I required two seat for movie
</code></pre>

<p>from above three statement, I want to extract number 2 it may be in integer or string or it refers to seats or people.</p>

<p>I have tried Named entity recognition but i did not get required output.  and for another output I used sentimental analysis. but problem is to extract number of people. </p>

<p>I expect number of people from paragraph and which is may be in integer or string. Thanks for help.</p>
",Named Entity Recognition (NER),extract specific type word paragraph using natural language processing developing model extract specific type word consider data set follow three statement want extract number may integer string refers seat people tried named entity recognition get required output another output used sentimental analysis problem extract number people expect number people paragraph may integer string thanks help
"RASA NLU- I want to extract anything(Words, numbers or special characters) as an entity after a word","<p>Is there a way we can extract anything after a word as an entity; for eg:</p>

<p>I want to extract anything after <code>about</code> or <code>go to</code> or <code>learn</code> as an entity.</p>

<pre><code>##intent:navigate
-I want to learn about linear regression
-I want to read about SVM
-I want to go to Python 2.6
-Take me to logistic regression: eval

##regex:topic
-^[A-Za-z0-9 :_ -][A-Za-z0-9 :_ -][A-Za-z0-9 :_ -]$
</code></pre>
",Named Entity Recognition (NER),rasa nlu want extract anything word number special character entity word way extract anything word entity eg want extract anything entity
How to apply a function to the whole dataset - Python?,"<p>I have a dataframe called ""data"" like that : </p>

<pre><code>id    email_body
1      text_1
2      text_2
3      text_3
4      text_4
5      text_5
6      text_6
7      text_7
8      text_8
9      text_9
10     text_10
</code></pre>

<p>I'm using the following code to extract from the different rows , the full name(s), the first name(s) and the last name(s) which are contained in the different ""text_i"" : </p>

<pre><code>import nltk
from nameparser.parser import HumanName
from nltk.corpus import wordnet 

def get_human_names(text):
  tokens = nltk.tokenize.word_tokenize(text)
  pos = nltk.pos_tag(tokens)
  sentt = nltk.ne_chunk(pos, binary = False)
  person_list = []
  lastname = []
  firstname = []
  person = []
  name = """"
  for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):
      for leaf in subtree.leaves():
          person.append(leaf[0])
      if len(person) &gt; 1: #avoid grabbing lone surnames
          for part in person:
              name += part + ' '
          if name[:-1] not in person_list:
              person_list.append(name[:-1])
              for person in person_list:
                person_split = person.split("" "")
                for name in person_split:
                  if wordnet.synsets(name):
                    if(name in person):
                      person_list.remove(person)

                      break
      firstname = [i.split(' ')[0] for i in person_list]
      lastname = [i.split(' ')[1] for i in person_list]            
      name = ''
      person = []

  return person_list, firstname, lastname


names = data.email_body.apply(get_human_names) 

columns = ['names','firstname','lastname' ]

data_2 = pd.DataFrame([names[0],names[1],names[2]], columns = columns)


data_2
</code></pre>

<p>I'm obtaining the following dataset : </p>

<pre><code>id          names                                                firstname                   lastname
0   [Lesley Kirchman, Milap Majmundar, Segoe UI]    [Lesley, Milap, Segoe]  [Kirchman, Majmundar, UI]
1   [Gerrit Boerman, Lesley Kirchman, Segoe UI] [Gerrit, Lesley, Segoe] [Boerman, Kirchman, UI]
2   [Lesley Kirchman]                                  [Lesley]    [Kirchman]
</code></pre>

<p>You can observe that I have only the 3 first rows, how to apply the function to the whole initial dataframe ""data"" and thus obtain a resulting dataframe with 10 rows ?</p>

<p>Regards,</p>
",Named Entity Recognition (NER),apply function whole dataset python dataframe called data like using following code extract different row full name first name last name contained different text obtaining following dataset observe first row apply function whole initial dataframe data thus obtain resulting dataframe row regard
How to properly update a model in spaCy?,"<p>I want to update a model with new entities. I'm loading the ""pt"" NER model, and trying to update it.
Before doing anything, I tried this phrase: ""meu nome é Mário e hoje eu vou para academia"". (in English this phrase is ""my name is Mário and today I'm going to go to gym).
Before the whole process I got this: </p>

<pre><code>Entities [('Mário', 'PER')]
Tokens [('meu', '', 2), ('nome', '', 2), ('é', '', 2), ('Mário', 'PER', 3), ('e', '', 2), ('hoje', '', 2), ('eu', '', 2), ('vou', '', 2), ('pra', '', 2), ('academia', '', 2)]
</code></pre>

<p>Ok, Mário is a name and it's correct. 
But I want the model recognize ""hoje (today)"" as DATE, then I ran the script below.</p>

<p>After I ran the script, I've tried the same setence and got this:</p>

<pre><code>Entities [('hoje', 'DATE')]
Tokens [('meu', '', 2), ('nome', '', 2), ('é', '', 2), ('Mário', '', 2), ('e', '', 2), ('hoje', 'DATE', 3), ('eu', '', 2), ('vou', '', 2), ('pra', '', 2), ('academia', '', 2)]
</code></pre>

<p>The model is recognizing ""hoje"" as DATE, but totally forgot about Mário as Person.</p>

<pre class=""lang-py prettyprint-override""><code>from __future__ import unicode_literals, print_function
import plac
import random
from pathlib import Path
import spacy
from spacy.util import minibatch, compounding

# training data
TRAIN_DATA = [
    (""Infelizmente não, eu briguei com meus amigos hoje"", {""entities"": [(45, 49, ""DATE"")]}),
    (""hoje foi um bom dia."", {""entities"": [(0, 4, ""DATE"")]}),
    (""ah não sei, hoje foi horrível"", {""entities"": [(12, 16, ""DATE"")]}),
    (""hoje eu briguei com o Mário"", {""entities"": [(0, 4, ""DATE"")]})
]


@plac.annotations(
    model=(""Model name. Defaults to blank 'en' model."", ""option"", ""m"", str),
    output_dir=(""Optional output directory"", ""option"", ""o"", Path),
    n_iter=(""Number of training iterations"", ""option"", ""n"", int),
)

def main(model=""pt"", output_dir=""/model"", n_iter=100):
    """"""Load the model, set up the pipeline and train the entity recognizer.""""""
    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank(""pt"")  # create blank Language class
            print(""Created blank 'en' model"")

    doc = nlp(""meu nome é Mário e hoje eu vou pra academia"")
    print(""Entities"", [(ent.text, ent.label_) for ent in doc.ents])
    print(""Tokens"", [(t.text, t.ent_type_, t.ent_iob) for t in doc])

    # create the built-in pipeline components and add them to the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if ""ner"" not in nlp.pipe_names:
        ner = nlp.create_pipe(""ner"")
        nlp.add_pipe(ner, last=True)
    # otherwise, get it so we can add labels
    else:
        ner = nlp.get_pipe(""ner"")

    # add labels
    for _, annotations in TRAIN_DATA:
        for ent in annotations.get(""entities""):
            ner.add_label(ent[2])

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]
    with nlp.disable_pipes(*other_pipes):  # only train NER
        # reset and initialize the weights randomly – but only if we're
        # training a new model
        if model is None:
            nlp.begin_training()
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            losses = {}
            # batch up the examples using spaCy's minibatch
            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(
                    texts,  # batch of texts
                    annotations,  # batch of annotations
                    drop=0.5,  # dropout - make it harder to memorise data
                    losses=losses,
                )
            print(""Losses"", losses)

    # test the trained model
   # for text, _ in TRAIN_DATA:
    doc = nlp(""meu nome é Mário e hoje eu vou pra academia"")
    print(""Entities"", [(ent.text, ent.label_) for ent in doc.ents])
    print(""Tokens"", [(t.text, t.ent_type_, t.ent_iob) for t in doc])

    # save model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.to_disk(output_dir)
        print(""Saved model to"", output_dir)

        # test the saved model
        print(""Loading from"", output_dir)
        nlp2 = spacy.load(output_dir)
        # for text, _ in TRAIN_DATA:
        #     doc = nlp2(text)
        #     print(""Entities"", [(ent.text, ent.label_) for ent in doc.ents])
        #     print(""Tokens"", [(t.text, t.ent_type_, t.ent_iob) for t in doc])
</code></pre>
",Named Entity Recognition (NER),properly update model spacy want update model new entity loading pt ner model trying update anything tried phrase meu nome rio e hoje eu vou para academia english phrase name rio today going go gym whole process got ok rio name correct want model recognize hoje today date ran script ran script tried setence got model recognizing hoje date totally forgot rio person
What kind of neural network should I use for extract key information from a sentence for RDF rules?,"<p>I am working on my paper, and one of the tasks is to extract the company name and location from the sentence of the following type:</p>

<p>""Google shares resources with Japan based company.""</p>

<p>Here, I want the output to be ""Google Japan"". The sentence structure may also be varied like ""Japan based company can access the resources of Google"". I have tried an Attention based NN, but the error rate is around 0.4. <strong>Can anyone give me a little bit of hint about which model I should use?</strong></p>

<p><strong>And I printed out the validation process like this:</strong>
<a href=""https://i.sstatic.net/8oQbp.png"" rel=""nofollow noreferrer"">validation print</a></p>

<p><strong>And I got the graphs of the loss and accuracy:</strong>
<a href=""https://i.sstatic.net/NKIOO.png"" rel=""nofollow noreferrer"">lass and accuracy</a></p>

<p>It shows that the val_acc is 0.99. Is this mean my model is pretty good at predicting? But why do I get 0.4 error rate when I use my own validation function to show error rate? I am very new to ML. What does the val_acc actually mean?</p>

<p><strong>Here is my model:</strong></p>

<pre><code>encoder_input = Input(shape=(INPUT_LENGTH,))
decoder_input = Input(shape=(OUTPUT_LENGTH,))

encoder = Embedding(input_dict_size, 64, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)
encoder = LSTM(64, return_sequences=True, unroll=True)(encoder)
encoder_last = encoder[:, -1, :]

decoder = Embedding(output_dict_size, 64, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)
decoder = LSTM(64, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])

attention = dot([decoder, encoder], axes=[2, 2])
attention = Activation('softmax')(attention)

context = dot([attention, encoder], axes=[2, 1])
decoder_combined_context = concatenate([context, decoder])

output = TimeDistributed(Dense(64, activation=""tanh""))(decoder_combined_context)  # equation (5) of the paper
output = TimeDistributed(Dense(output_dict_size, activation=""softmax""))(output)

model = Model(inputs=[encoder_input, decoder_input], outputs=[output])
model.compile(optimizer='adam', loss=""binary_crossentropy"", metrics=['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200, min_delta=0.0005)
</code></pre>
",Named Entity Recognition (NER),kind neural network use extract key information sentence rdf rule working paper one task extract company name location sentence following type google share resource japan based company want output google japan sentence structure may also varied like japan based company access resource google tried attention based nn error rate around anyone give little bit hint model use printed validation process like validation print got graph loss accuracy lass accuracy show val acc mean model pretty good predicting get error rate use validation function show error rate new ml doe val acc actually mean model
How to fine tune BERT on its own tasks?,"<p>I wanted to pre-train BERT with the data from my own language since multilingual (which includes my language) model of BERT is not successful. Since whole pre-training costs a lot, I decided to fine tune it on its own 2 tasks: masked language model and next sentence prediction. There are previous implementation on different tasks (NER, sentiment analysis etc.), but I couldn't find any fine tuning on its own tasks. Is there an implementation that I couldn't see? If not, where should I start? I need some initial help.</p>
",Named Entity Recognition (NER),fine tune bert task wanted pre train bert data language since multilingual includes language model bert successful since whole pre training cost lot decided fine tune task masked language model next sentence prediction previous implementation different task ner sentiment analysis etc find fine tuning task implementation see start need initial help
Spacy Pipeline?,"<p>So lately I've been playing around with a WikiDump.
I preprocessed it and trained it on Word2Vec + Gensim</p>

<p>Does anyone know if there is only one script within Spacy that would generate
tokenization, sentence recognition, part of speech tagging, lemmatization, dependency parsing, and named entity recognition all at once</p>

<p>I have not been able to find clear documentation
Thank you </p>
",Named Entity Recognition (NER),spacy pipeline lately playing around wikidump preprocessed trained word vec gensim doe anyone know one script within spacy would generate tokenization sentence recognition part speech tagging lemmatization dependency parsing named entity recognition able find clear documentation thank
Is there a way in Polyglot to permanently &quot;fix&quot; the language code of an Hebrew text from &#39;&#39;iw&#39;&#39; to &#39;&#39;he&#39;&#39;?,"<p>I want to make a simple sentiment analysis on a Hebrew text using Polyglot in python 3.6.
The problem is that Polyglot recognizes the text language code as ""iw"" and not as ""he"", and therefore is not able to process it.</p>

<p>As shown at:
<a href=""https://stackoverflow.com/questions/38296602/use-polyglot-package-for-named-entity-recognition-in-hebrew"">use polyglot package for Named Entity Recognition in hebrew</a> I've already added <code>hint_language_code = 'he'</code> to the <code>Text</code> function call, but it only changes the initial form of the text, not its sub-forms (like sentences or words).</p>

<p>For example:<br/></p>

<p>Input:</p>

<pre><code>import polyglot
from polyglot.text import Text, Word

article='איך ניתן לנתח טקסט בעברית? והאם ניתן לשנות את הקידוד?'
txt = Text(article)
print(txt.language.code)

txt = Text(article,hint_language_code = 'he')
print(txt.language.code)

sent=txt.sentences[1]
print(sent.language.code)
print(sent)
</code></pre>

<p>Output: </p>

<pre><code>iw
he
iw
והאם ניתן לשנות את הקידוד?
</code></pre>

<p>How can I permanently change the text <code>language_code</code> from <code>'iw'</code> to <code>'he'</code>?</p>
",Named Entity Recognition (NER),way polyglot permanently fix language code hebrew text iw want make simple sentiment analysis hebrew text using polyglot python problem polyglot recognizes text language code iw therefore able process shown href polyglot package named entity recognition hebrew already added function call change initial form text sub form like sentence word example input output permanently change text
Is it possible to load a pre-trained model into spacy?,"<p>I want to be able to use spacy's full functionality, but my language of choice does not currently have it's own model on spacy. By full functionality, I mean  convolutional layer that is shared between the tagger, parser and NER; and be able to update all of these different models by extending the vocabulary.</p>

<p>I need my model to do dependency parsing and named entity recognition to begin with, but I may try text classification and so on as well. </p>

<p>There are models in stanfordNLP that I can use for dependency parsing. I have discovered the spacy_stanfordnlp module. And this is good for getting tags and visualizing the dependency model. Is there a way to import this model (or any pre-trained model) to spacy's pipeline as as the parser? Or do I need to re-train entirely on the spacy side by using this parser's outcomes as spacy's tags and train with spacy? I want to move the dependency parser to spacy to train a NER model with a specific vocabulary on top of it, and do other tasks in the future. </p>

<p>I tried something like this, but it doesn't work</p>

<pre><code>    snlp = stanfordnlp.Pipeline(lang=""tr"")
    nlp1 = StanfordNLPLanguage(snlp)
    nlp1.to_disk(""./stanfordnlp-spacy-model"")    
    nlp = spacy.load(""./stanfordnlp-spacy-model"", snlp=snlp)
    parser = nlp.create_pipe(""parser"")
    nlp.add_pipe(parser)
    # this doesnt work, with a pytorch model from tr_imst tree bank, which is used by stanfodnlp
    nlp.parser.from_bytes(""tr_imst_parser.pt"")[""model""]
</code></pre>
",Named Entity Recognition (NER),possible load pre trained model spacy want able use spacy full functionality language choice doe currently model spacy full functionality mean convolutional layer shared tagger parser ner able update different model extending vocabulary need model dependency parsing named entity recognition begin may try text classification well model stanfordnlp use dependency parsing discovered spacy stanfordnlp module good getting tag visualizing dependency model way import model pre trained model spacy pipeline parser need train entirely spacy side using parser outcome spacy tag train spacy want move dependency parser spacy train ner model specific vocabulary top task future tried something like work
Is there a library to see the weights being learned by RASA NLU interpreter after training?,"<p>I'm using RASA NLU for entity extraction from healthcare invoices. I have manually tagged and trained OCR extracted data from these invoices with 10 relevant entities(Clinic name, clinic address...). RASA implicitly learns several features from the data. Is there any way to see these features and weights being learned for these features?</p>

<p>I have already tried <a href=""https://eli5.readthedocs.io/en/latest/index.html"" rel=""nofollow noreferrer"">ELI5</a> but it doesn't support rasa interpreter it seems.</p>

<p>Has anyone come across this? Thanks in advance</p>
",Named Entity Recognition (NER),library see weight learned rasa nlu interpreter training using rasa nlu entity extraction healthcare invoice manually tagged trained ocr extracted data invoice relevant entity clinic name clinic address rasa implicitly learns several feature data way see feature weight learned feature already tried eli support rasa interpreter seems ha anyone come across thanks advance
Natural Language Entity extraction,"<p>I have a text which requires entities to be tagged.</p>

<p>Example:</p>

<p><b>David is specialised in bipolar disorder and works in institution University of California.</b></p>

<p>I have table with all the entities in one table ,called <b>entities</b> and table contains 1 million records.</p>

<p>Of course i would write and scan each one of those to find the entities,but problem would be performance.</p>

<p>What is the best approach to identify entities like bipolar disorder and University of California from the table list.</p>

<pre><code> I expect the out put to be in format
 array(""bipolar disorder"",""California"");
</code></pre>
",Named Entity Recognition (NER),natural language entity extraction text requires entity tagged example david specialised bipolar disorder work institution university california table entity one table called entity table contains million record course would write scan one find entity problem would performance best approach identify entity like bipolar disorder university california table list
Is it possible to train \ tune a spacy NER model with &quot;hints&quot; based on rules \ patterns,"<p>Dummy example:
I want the NER to be able to detect locations, animals and sport groups
a Matcher \ PhraseMatcher\ EntityRuler (<em>which is more relevant for this use case?</em>) could be used to add ""simple"" rules
like: 
locations: Chicago, New York
animals: Bull, Chicken
groups: Chicago Bulls</p>

<p>The NER layer should be able to learn that Chicago Bulls is a group and not a location and animal (like using a matcher alone would give)
and that other combinations of location + animal are sport groups and not location animal pairs (even if the specific combination didn't exists in the training set)</p>

<p>TLDR: I don't want to use the rule based extracted entities as-is, but as hints for another layer that will use them to improve the entity extraction</p>
",Named Entity Recognition (NER),possible train tune spacy ner model hint based rule pattern dummy example want ner able detect location animal sport group matcher phrasematcher entityruler relevant use case could used add simple rule like location chicago new york animal bull chicken group chicago bull ner layer able learn chicago bull group location animal like using matcher alone would give combination location animal sport group location animal pair even specific combination exists training set want use rule based extracted entity hint another layer use improve entity extraction
"Do double quotes, dots and commas modify the forget weights in LSTM if retained?","<p>I am trying to implement custom NER with LSTM. In the pre processing steps is it required to remove the punctuation marks like double quotes, dots and commas? Do they add any significance if retained? Since each document is a collection of sentences.</p>
",Named Entity Recognition (NER),double quote dot comma modify forget weight lstm retained trying implement custom ner lstm pre processing step required remove punctuation mark like double quote dot comma add significance retained since document collection sentence
"Named Entity Recognition using NLTK: Extract Auditor name, address and organisation","<p>I am trying to use nltk to identify Person, Organization and Place from a sentence.</p>

<p>My Use Case is to basically extract Auditor name, organization and Place from an annual financial report</p>

<p>With nltk in python the results don't seem to be really satisfactory</p>

<pre><code>import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

ex='Alastair John Richard Nuttall (Senior statutory auditor) for and on behalf of Ernst &amp; Young LLP (Statutory auditor) Leeds'

ne_tree = ne_chunk(pos_tag(word_tokenize(ex)))

print(ne_tree)

Tree('S', [Tree('PERSON', [('Alastair', 'NNP')]), Tree('PERSON', [('John', 'NNP'), ('Richard', 'NNP'), ('Nuttall', 'NNP')]), ('(', '('), Tree('ORGANIZATION', [('Senior', 'NNP')]), ('statutory', 'NNP'), ('auditor', 'NN'), (')', ')'), ('for', 'IN'), ('and', 'CC'), ('on', 'IN'), ('behalf', 'NN'), ('of', 'IN'), Tree('GPE', [('Ernst', 'NNP')]), ('&amp;', 'CC'), Tree('PERSON', [('Young', 'NNP'), ('LLP', 'NNP')]), ('(', '('), ('Statutory', 'NNP'), ('auditor', 'NN'), (')', ')'), ('Leeds', 'NNS')])

</code></pre>

<p>As seen above 'Leeds' is not identified as place nor is Ernst &amp; Young LLP recognized as Organization</p>

<p>Are there any better ways of achieving this in Python?</p>
",Named Entity Recognition (NER),named entity recognition using nltk extract auditor name address organisation trying use nltk identify person organization place sentence use case basically extract auditor name organization place annual financial report nltk python result seem really satisfactory seen leeds identified place ernst young llp recognized organization better way achieving python
Is there a mechanism to filter common nouns from a bag of nouns?,"<p><code>""the bride and the groom got married at the church.""</code></p>

<p>Applying a simple POS-tagger on the above sentence yields:</p>

<p><code>""the bride(NOUN) and the groom(NOUN) got married at the church(NOUN).""</code></p>

<p>I am interested in filtering out <code>bride, groom</code>. I have tried using various NLP tricks like NER (extracting noun-phrases) but in vain.</p>

<p>I would like to know if this problem is already solved.</p>
",Named Entity Recognition (NER),mechanism filter common noun bag noun applying simple po tagger sentence yield interested filtering tried using various nlp trick like ner extracting noun phrase vain would like know problem already solved
Named Entity Recognition using context of the sentence,"<p>I have a problem in which I want to know how can we extract or name the entity based on the context in which it is getting used in a sentence.</p>

<p>For example: If we have to extract date field which is used in the context of the date of birth only then how can we do that.</p>

<p>I know that we can use regular expression, spacy, NLTK to extract date field from a document. But I am unable to determine the approach to extract date based on the context in which it is getting used.</p>

<p>Example 1 : My birthday is on 9th December. Here 9th December will be marked as date field if we use spacy or regex, but I want it to be marked as a custom entity 'date of birth'. 
Example 2: I am going for a movie on 1st April. Here 1st April should be marked as normal date field.</p>
",Named Entity Recognition (NER),named entity recognition using context sentence problem want know extract name entity based context getting used sentence example extract date field used context date birth know use regular expression spacy nltk extract date field document unable determine approach extract date based context getting used example birthday th december th december marked date field use spacy regex want marked custom entity date birth example going movie st april st april marked normal date field
"Error in e(s, a) : no sentence token annotations found. Error while doing NER in R","<p>I am trying to do Named Entity Recognition in R. Its throwing below error after doing sentence token annotation and word token annotation. Cannot find a way out. Please help if anyone has faced the same error.</p>

<p>Below is my code followed by the error.</p>

<pre><code>library(openNLP)
sent_token_annotator = Maxent_Sent_Token_Annotator()
word_token_annotator = Maxent_Word_Token_Annotator()
pos_tag_annotator = Maxent_POS_Tag_Annotator()
pos_annotation = NLP::annotate(wc_text1, list(pos_tag_annotator, sent_token_annotator, word_token_annotator))
# Error in e(s, a) : no sentence token annotations found
</code></pre>
",Named Entity Recognition (NER),error e sentence token annotation found error ner r trying named entity recognition r throwing error sentence token annotation word token annotation find way please help anyone ha faced error code followed error
Concepts to measure text &quot;relevancy&quot; to a subject?,"<p>I do side work writing/improving a research project web application for some political scientists. This application collects articles pertaining to the U.S. Supreme Court and runs analysis on them, and after nearly a year and half, we have a database of around 10,000 articles (and growing) to work with.</p>

<p>One of the primary challenges of the project is being able to determine the ""relevancy"" of an article - that is, the primary focus is the federal U.S. Supreme Court (and/or its justices), and not a local or foreign supreme court. Since its inception, the way we've addressed it is to primarily parse the title for various explicit references to the federal court, as well as to verify that ""supreme"" and ""court"" are keywords collected from the article text. Basic and sloppy, but it actually works fairly well. That being said, irrelevant articles can find their way into the database - usually ones with headlines that don't explicitly mention a state or foreign country (the Indian Supreme Court is the usual offender).</p>

<p>I've reached a point in development where I can focus on this aspect of the project more, but I'm not quite sure where to start. All I know is that I'm looking for a method of analyzing article text to determine its relevance to the federal court, and nothing else. I imagine this will entail some machine learning, but I've basically got no experience in the field. I've done a little reading into things like tf-idf weighting, vector space modeling, and word2vec (+ CBOW and Skip-Gram models), but I'm not quite seeing a ""big picture"" yet that shows me how just how applicable these concepts can be to my problem. Can anyone point me in the right direction?</p>
",Named Entity Recognition (NER),concept measure text relevancy subject side work writing improving research project web application political scientist application collect article pertaining u supreme court run analysis nearly year half database around article growing work one primary challenge project able determine relevancy article primary focus u supreme court local foreign supreme court since inception way addressed primarily parse title various explicit reference court well verify supreme court keywords collected article text basic actually work fairly well said irrelevant article find way database usually one headline explicitly mention state foreign country indian supreme court usual offender reached point development focus aspect project quite sure start know looking method analyzing article text determine relevance court nothing else imagine entail machine learning basically got experience field done little reading thing like tf idf weighting vector space modeling word vec cbow skip gram model quite seeing big picture yet show applicable concept problem anyone point right direction
"Using NLP or Spacy, How can we extract contextual data from a text given entity as the input?","<p>For example, there is a text given (in the form of a document) along with person name ""John"". We need to extract all sentences from the text where there is a mention of John by his name or otherwise.</p>
",Named Entity Recognition (NER),using nlp spacy extract contextual data text given entity input example text given form document along person name john need extract sentence text mention john name otherwise
Extracting Dead Name Entities from Obituaries - NLP,"<p>I have a continuous strings of ads , which are extracted from some newspaper. The ads may appear in a format as shown below:My task here is to extract the deceased person's names.</p>

<pre><code>John, the small son of Mr. and Mrs.&lt;br&gt;
Elmer Cleppfer, died at their home in&lt;br&gt;
Lewistown on Wednesday. The funeral&lt;br&gt;
will He held on Saturday afternoon&lt;br&gt;
from the home of the grandparents&lt;br&gt;
on the child, Mr. and Mrs. John&lt;br&gt;
Kiopper, 224 Locust street, tortiorrow&lt;br&gt;
afternoon at 2 o'clock. Interment witt&lt;br&gt;
take place at Oberlin.&lt;br&gt;

Mrs. Lydia Mintch, aged 6S years &lt;br&gt;
died yesterday afternoon at the home&lt;br&gt;
of Fred Flowerfleld at Enhaut. Mrs.&lt;br&gt;
Mlnlch contracted a severe attack of&lt;br&gt;
pneumonia aggravated by other illness&lt;br&gt;
Several days ago which resulted in her&lt;br&gt;
death. Funeral arrangements have not&lt;br&gt;
yet been completed.&lt;br&gt;
</code></pre>

<p>The whole of the para is made up of 2 ads.. Can any one tell me how to classify such kind of text into paragraphs if there are more than 1 such ads?</p>
",Named Entity Recognition (NER),extracting dead name entity obituary nlp continuous string ad extracted newspaper ad may appear format shown task extract deceased person name whole para made ad one tell classify kind text paragraph ad
How to get back incorrect NER predictions in sklearn-crfsuite,"<p>I am performing NER using sklearn-crfsuite. I am trying to report back on an entity mention by entity mention case as a true positive (both prediction and expected correct even if no entity), false positive (prediction says yes, expected no) or false negative (prediction says no, expected yes).  </p>

<p>I cannot see how to get anything other than tag/token based summary statistics for NER performance.  </p>

<p>I would be OK with a different way of grouping entity mentions such as: correct, incorrect, partial, missing, spurious.  I can write a whole bunch of code around it myself to try to accomplish this (and might have to), but there has to be a single call to get this info?</p>

<p>Here are some of the calls that are being made to get the summary statistics:</p>

<pre><code>from sklearn import metrics
report = metrics.classification_report(targets, predictions,
                                       output_dict=output_dict)
precision = metrics.precision_score(targets, predictions,
                                    average='weighted')
f1 = metrics.f1_score(targets, predictions, average='weighted')
accuracy = metrics.accuracy_score(targets, predictions)
</code></pre>
",Named Entity Recognition (NER),get back incorrect ner prediction sklearn crfsuite performing ner using sklearn crfsuite trying report back entity mention entity mention case true positive prediction expected correct even entity false positive prediction say yes expected false negative prediction say expected yes see get anything tag token based summary statistic ner performance would ok different way grouping entity mention correct incorrect partial missing spurious write whole bunch code around try accomplish might ha single call get info call made get summary statistic
Can we test or evaluate entity extraction in Rasa NLU?,"<p>Is it possible to evaluate how well my model extracts entities (and maps synonym values) in Rasa NLU?</p>

<p>I have tried the <code>rasa_nlu -evaluate</code> mode however, it seems to only work for intent classification, although my JSON data file contains entities information and I'd really like to know if my entity extraction is up to the mark given various scenarios. I've used Tracy to generate test dataset.</p>
",Named Entity Recognition (NER),test evaluate entity extraction rasa nlu possible evaluate well model extract entity map synonym value rasa nlu tried mode however seems work intent classification although json data file contains entity information really like know entity extraction mark given various scenario used tracy generate test dataset
Training spaCy&#39;s NER model from scratch on CoNLL 2003 data got very weird results,"<p>I'm trying to try training NER models using spaCy from scratch. I wanted to first try it out on <a href=""https://github.com/Franck-Dernoncourt/NeuroNER/tree/master/data/conll2003/en"" rel=""nofollow noreferrer"">CoNLL 2003 data</a>, which is widely used as a baseline for NER systems.</p>

<p>The following are the commands I ran:</p>

<pre><code>spacy convert -c ner train.txt valid.txt test.txt spacyConverted
cd spacyConverted
python -m spacy train en trained train.txt.json valid.txt.json --no-tagger --no-parser
mkdir displacy
python -m spacy evaluate trained/model-final test.txt.json --displacy-path displacy
</code></pre>

<p>However, the evaluation results on test data are very weird and totally off, as seen in the following displacy output.</p>

<p><a href=""https://i.sstatic.net/nednr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nednr.png"" alt=""results""></a></p>

<p>The precision, recall and f1 scores are very low both during the training and the evaluation.</p>

<p><a href=""https://i.sstatic.net/Cgvys.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Cgvys.png"" alt=""training""></a></p>

<p>I do believe that the commands are correct and in accordance with the documentation. What could be the possible problem here? Could it be that I must supply some word vectors as well? If so, how do I supply those that come by default in spaCy? Or could it be that one cannot use <code>--no-tagger --no-parser</code>?</p>

<p>The converted <code>.json</code> files look like the following:</p>

<pre><code>[
  {
    ""id"":0,
    ""paragraphs"":[
      {
        ""sentences"":[
          {
            ""tokens"":[
              {
                ""orth"":""-DOCSTART-"",
                ""tag"":""-X-"",
                ""ner"":""O""
              }
            ]
          },
          {
            ""tokens"":[
              {
                ""orth"":""EU"",
                ""tag"":""NNP"",
                ""ner"":""U-ORG""
              },
              {
                ""orth"":""rejects"",
                ""tag"":""VBZ"",
                ""ner"":""O""
              },
              {
                ""orth"":""German"",
                ""tag"":""JJ"",
                ""ner"":""U-MISC""
              },
              {
                ""orth"":""call"",
                ""tag"":""NN"",
                ""ner"":""O""
              },
              {
                ""orth"":""to"",
                ""tag"":""TO"",
                ""ner"":""O""
              },
              ...
</code></pre>

<hr>

<p>EDIT: It seemed that I actually needed to pass in the <code>--gold-preproc</code> flag for the training to properly work. But I'm not sure what it actually means in this context.</p>
",Named Entity Recognition (NER),training spacy ner model scratch conll data got weird result trying try training ner model using spacy scratch wanted first try conll data widely used baseline ner system following command ran however evaluation result test data weird totally seen following displacy output precision recall f score low training evaluation believe command correct accordance documentation could possible problem could must supply word vector well supply come default spacy could one use converted file look like following edit seemed actually needed pas flag training properly work sure actually mean context
Manual Tagging of Words for NLP,"<p>I am a newbie in machine learning, named entity recognition and am assigned a task to manually tag my data in hundreds of paragraphs to retrain a Bidirectional LSTM model. Is there a better approach to this or i have to go through the whole content and manually tag each organization, person?</p>
",Named Entity Recognition (NER),manual tagging word nlp newbie machine learning named entity recognition assigned task manually tag data hundred paragraph retrain bidirectional lstm model better approach go whole content manually tag organization person
"K-means, bag of word, Word embedded text classification CSV file and retrieve data associated","<p>I have two tasks to do.</p>

<p>1)I have to extract the headers of any CVS file containing invoices data.
In specific: invoice number, address, location, physical good. 
I have been asked to create a text classifier for this task, therefore the classifier will go over any CVS file and identify those 4 headers. </p>

<p>2)After the classifier identifies the 4 words I have to find the attach the data of that column and create a class.</p>

<p>I researched the matter and the three methodologies that I thought were must be appropriated are: 
1)bad of words
2)word embedded
3)K-means clustering </p>

<p>Bag of words can identify the word but it does not give me the location of the word itself to go and grab the column and create the class.</p>

<p>Word embedded is over complicated for this task, I believe, and even if give me the position of the word in the file is too time-consuming for this</p>

<p>K-means seems simple and effective it tells me where the word is.</p>

<p>My question before I start coding</p>

<p>did I miss something. Is my reasoning correct?
And most important the second question 
Once the position of the word is identified in the CSV file how I translate that into coding so I can attach the data in that column  </p>
",Named Entity Recognition (NER),k mean bag word word embedded text classification csv file retrieve data associated two task extract header cv file containing invoice data specific invoice number address location physical good asked create text classifier task therefore classifier go cv file identify header classifier identifies word find attach data column create class researched matter three methodology thought must appropriated bad word word embedded k mean clustering bag word identify word doe give location word go grab column create class word embedded complicated task believe even give position word file time consuming k mean seems simple effective tell word question start coding miss something reasoning correct important second question position word identified csv file translate coding attach data column
"Stanford CoreNLP: How do you integrate a standard, but Russian-trained NER model with completely custom models for lemmatisation?","<p>I am currently working on adapting a coreference tagging algorithm to Russian, as part of my university project, based on Stanford CoreNLP. For the most part, it was easy enough: there already exist Russian models for lemmatisation and PoS-tagging. However, there was no NER model for the language, at least based on CoreNLP, and as such, I had to train such a model myself using CoreNLP's statistical methods.</p>

<p>The problem's in adapting this new trained NER model to the lemmatisation and PoS-tagging models. While my model was purely statistical and was still based on the original CoreNLP code, the two other ones are completely different. Therefore, I cannot actually make my NER integrate them. How can you accomplish that? The relevant code is:</p>

<pre><code>props.setProperty(""annotators"", ""tokenize,ssplit,pos,custom.lemma,custom.morpho,custom.ner,depparse, mention, coref"");
props.setProperty(""pos.model"", ""edu/stanford/nlp/models/pos-tagger/russian-ud-pos.tagger"");
props.setProperty(""customAnnotatorClass.custom.lemma"", ""edu.stanford.nlp.international.russian.process.RussianLemmatizationAnnotator"");
props.setProperty(""custom.lemma.dictionaryPath"", ""edu/stanford/nlp/international/russian/process/dict.tsv"");
props.setProperty(""customAnnotatorClass.custom.morpho"", ""edu.stanford.nlp.international.russian.process.RussianMorphoAnnotator"");
props.setProperty(""customAnnotatorClass.custom.ner"", ""edu.stanford.nlp.international.russian.process.RussianMorphoAnnotator"");
props.setProperty(""custom.morpho.model"", ""edu/stanford/nlp/models/pos-tagger/russian-ud-mf.tagger"");
props.setProperty(""ner.model"", ""C:/Users/Admin/eclipse-workspace/Coreference-Evaluation-master/libs/russian-new-model.ser.gz"");
props.setProperty(""depparse.model"", ""edu/stanford/nlp/models/parser/nndep/nndep.rus.model.wiki.txt.gz"");
props.setProperty(""depparse.language"", ""russian"");
props.setProperty(""parse.maxlen"", ""100"");
props.setProperty(""ssplit.eolonly"", ""true"");
props.setProperty(""tokenize.whitespace"",""true"");
props.setProperty(""coref.removeSingletonClusters"",""false"");
pipeline = new StanfordCoreNLP(props);
System.out.println(pipeline);
</code></pre>

<p>Before you suggest I change the algorithm altogether: my knowledge of Java as of now is not good enough to make such radical changes to the already existing code.</p>
",Named Entity Recognition (NER),stanford corenlp integrate standard russian trained ner model completely custom model lemmatisation currently working adapting coreference tagging algorithm russian part university project based stanford corenlp part wa easy enough already exist russian model lemmatisation po tagging however wa ner model language least based corenlp train model using corenlp statistical method problem adapting new trained ner model lemmatisation po tagging model model wa purely statistical wa still based original corenlp code two one completely different therefore actually make ner integrate accomplish relevant code suggest change algorithm altogether knowledge java good enough make radical change already existing code
How can I check if the first word of a sentence is a proper noun?,"<p>I want to remove all the proper nouns from a large corpus. Due to the large volume, I take a shortcut and remove all words starting with capital letters. For the first word of each sentence, I also want to check if it is a proper noun. How can I do this without using a tagger. One option is to do a screening using a list of common proper nouns. Is there a better way and where can I get such a list?  Thanks. </p>

<p>I tried NLTK pos_tag and Standford NER. Without context, they do not work well.  </p>

<pre><code> ner_tagger = StanfordNERTagger(model,jar)
 names = ner_tagger.tag(first_words)
 types = [""DATE"", ""LOCATION"", ""ORGANIZATION"", ""PERSON"", ""TIME""]

 for name, type in names:
     if type in types:
        print(name, type)
</code></pre>

<p>Below are some results. </p>

<pre><code>  Abnormal ORGANIZATION
  Abnormally ORGANIZATION
  Abraham ORGANIZATION
  Absorption ORGANIZATION
  Abundant ORGANIZATION
  Abusive ORGANIZATION
  Academic ORGANIZATION
  Acadia ORGANIZATION
</code></pre>

<p>There are too many false positives since the first letter of a sentence is always capitalized. After I changed the words to all lower cases, NER even missed common entities such as America and American. </p>
",Named Entity Recognition (NER),check first word sentence proper noun want remove proper noun large corpus due large volume take shortcut remove word starting capital letter first word sentence also want check proper noun without using tagger one option screening using list common proper noun better way get list thanks tried nltk po tag standford ner without context work well result many false positive since first letter sentence always capitalized changed word lower case ner even missed common entity america american
SpaCy tags new line (\n) as GPE named entities,"<p>I am using SpaCy to get named entities. However, it always mis-tags new line symbols as named entities. </p>

<p>Below is the input text.</p>

<pre><code>mytxt = """"""&lt;?xml version=""1.0""?&gt;

&lt;nitf&gt;

&lt;head&gt;
&lt;title&gt;KNOW YOUR ROLE ON SUPER BOWL LIII.&lt;/title&gt;
&lt;/head&gt;

&lt;body&gt;

&lt;body.head&gt;

&lt;hedline&gt;
&lt;hl1&gt;KNOW YOUR ROLE ON SUPER BOWL LIII.&lt;/hl1&gt;
&lt;/hedline&gt;

&lt;distributor&gt;Gale Group&lt;/distributor&gt;

&lt;/body.head&gt;

&lt;body.content&gt;
&lt;p&gt;Montpelier: &lt;org&gt;Department of Motor Vehicles&lt;/org&gt;, has issued the following
news release:&lt;/p&gt;

&lt;p&gt;Be a designated sober driver, help save lives. Remember these tips
on game night:&lt;/p&gt;

&lt;p&gt;Know your State&amp;apos;s laws: refusing to take a breath test in many
jurisdictions could result in arrest, loss of your driver&amp;apos;s
license, and impoundment of your vehicle. Not to mention the
embarrassment in explaining your situation to family, friends, and
employers.&lt;/p&gt;

&lt;p&gt;In case of any query regarding this article or other content needs
please contact: &lt;a href=""mailto:editorial@plusmediasolutions.com""&gt;editorial@plusmediasolutions.com&lt;/a&gt;&lt;/p&gt;
&lt;/body.content&gt;

&lt;/body&gt;
&lt;/nitf&gt;


""""""
</code></pre>

<p>Below is my code:</p>

<pre><code>    CONTENT_XML_TAG = ('p', 'ul', 'h3', 'h1', 'h2', 'ol')
    soup = BeautifulSoup(mytxt, 'xml')
    spacy_model = spacy.load('en_core_web_sm')
    content = ""\n"".join([p.get_text() for p in soup.find('body.content').findAll(CONTENT_XML_TAG)])
    print(content)

    section_spacy = spacy_model(content)
    tokenized_sentences = []
    for sent in section_spacy.sents:
        tokenized_sentences.append(sent)
    for s in tokenized_sentences:
        labels = [(ent.text, ent.label_) for ent in s.ents]
        print(Counter(labels))
</code></pre>

<p>The print out:</p>

<pre><code>Counter({('\n', 'GPE'): 2, ('Department of Motor Vehicles', 'ORG'): 1})
Counter({('\n', 'GPE'): 1})
Counter({('\n', 'GPE'): 2, ('State', 'ORG'): 1})
Counter({('\n', 'GPE'): 3})
Counter({('\n', 'GPE'): 1})
</code></pre>

<p>I cannot believe SpaCy has such kind of misclassification. Did I miss anything?</p>
",Named Entity Recognition (NER),spacy tag new line n gpe named entity using spacy get named entity however always mi tag new line symbol named entity input text code print believe spacy ha kind misclassification miss anything
Limiting the number of iterations in Stanford NER,"<p>I am training the Stanford NER CRF model, on a customised dataset but the number of iterations that is being used to train the model have now gone to 
333 iterations-i.e. and this training process has now gone for hours.
Below is the message printed in the terminal -</p>

<pre><code>Iter 335 evals 400 &lt;D&gt; [M 1.000E0] 2.880E3 38054.87s |5.680E1| {6.652E-6} 4.488E-4 - 
Iter 336 evals 401 &lt;D&gt; [M 1.000E0] 2.880E3 38153.66s |1.243E2| {1.456E-5} 4.415E-4 -
 - 
</code></pre>

<p>The properties file being used is given below - is there some way I can limit the number of iterations to say 20.</p>

<pre><code>location of the training file
trainFile = TRAIN5000.tsv
#location where you would like to save (serialize to) your
#classifier; adding .gz at the end automatically gzips the file,
#making it faster and smaller
serializeTo = ner-model_TRAIN5000.ser.gz

#structure of your training file; this tells the classifier
#that the word is in column 0 and the correct answer is in
#column 1
map = word=0,answer=1

#these are the features we'd like to train with
#some are discussed below, the rest can be
#understood by looking at NERFeatureFactory
useClassFeature=true
useWord=true
useNGrams=true
#no ngrams will be included that do not contain either the
#beginning or end of the word
noMidNGrams=true
useDisjunctive=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
#the next 4 deal with word shape features
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
saveFeatureIndexToDisk = true
printFeatures=true
flag useObservedSequencesOnly=true
featureDiffThresh=0.05
</code></pre>
",Named Entity Recognition (NER),limiting number iteration stanford ner training stanford ner crf model customised dataset number iteration used train model gone iteration e training process ha gone hour message printed terminal property file used given way limit number iteration say
python regex to get numbers placed in different position of a string,"<p>These are the possible cases of text I have,</p>

<pre><code>4 bedrooms 2 bathrooms 3 carparks
3 bedroom house
Bedrooms 2, 
beds 5,
Bedrooms 1, 
2 bedrooms, 1 bathroom, 
Four bedrooms home, double garage
Four bedrooms home
Three double bedrooms home, garage
Three bedrooms home,
2 bedroom home unit with single carport.
Garage car spaces: 2, Bathrooms: 4, Bedrooms: 7,
</code></pre>

<p>I am trying to get the number of bedrooms out of this text. I managed to write the below ones,</p>

<pre><code>  def get_bedroom_num(s):
    if ':' in s:
        out = re.search(r'(?:Bedrooms:|Bedroom:)(.*)', s,re.I).group(1)
    elif ',' in s:
        out = re.search(r'(?:bedrooms|bedroom|beds)(.*)', s,re.I).group(1)
    else:
        out = re.search(r'(.*)(?:bedrooms|bedroom).*', s,re.I).group(1)
    out = filter(lambda x: x.isdigit(), out)
    return out
</code></pre>

<p>But it is not capturing all the possible cases. The key here is the word 'bedroom', text will always have the text bedroom either in the front or back of the number. Any better approach to handle this? If not through regex, may be Named Entity Recognition in NLP?</p>

<p>Thanks.</p>

<p>EDIT : - </p>

<p>For case 7 to 10, I managed to convert the word numbers to integer using the below function,</p>

<pre><code>#Convert word to number
def text2int (textnum, numwords={}):
    if not numwords:
        units = [
        ""zero"", ""one"", ""two"", ""three"", ""four"", ""five"", ""six"", ""seven"", ""eight"",
        ""nine"", ""ten"", ""eleven"", ""twelve"", ""thirteen"", ""fourteen"", ""fifteen"",
        ""sixteen"", ""seventeen"", ""eighteen"", ""nineteen"",
        ]

        tens = ["""", """", ""twenty"", ""thirty"", ""forty"", ""fifty"", ""sixty"", ""seventy"", ""eighty"", ""ninety""]

        scales = [""hundred"", ""thousand"", ""million"", ""billion"", ""trillion""]

        numwords[""and""] = (1, 0)
        for idx, word in enumerate(units):  numwords[word] = (1, idx)
        for idx, word in enumerate(tens):       numwords[word] = (1, idx * 10)
        for idx, word in enumerate(scales): numwords[word] = (10 ** (idx * 3 or 2), 0)

    ordinal_words = {'first':1, 'second':2, 'third':3, 'fifth':5, 'eighth':8, 'ninth':9, 'twelfth':12}
    ordinal_endings = [('ieth', 'y'), ('th', '')]

    textnum = textnum.replace('-', ' ')

    current = result = 0
    curstring = """"
    onnumber = False
    for word in textnum.split():
        if word in ordinal_words:
            scale, increment = (1, ordinal_words[word])
            current = current * scale + increment
            if scale &gt; 100:
                result += current
                current = 0
            onnumber = True
        else:
            for ending, replacement in ordinal_endings:
                if word.endswith(ending):
                    word = ""%s%s"" % (word[:-len(ending)], replacement)

            if word not in numwords:
                if onnumber:
                    curstring += repr(result + current) + "" ""
                curstring += word + "" ""
                result = current = 0
                onnumber = False
            else:
                scale, increment = numwords[word]

                current = current * scale + increment
                if scale &gt; 100:
                    result += current
                    current = 0
                onnumber = True

    if onnumber:
        curstring += repr(result + current)

    return curstring
</code></pre>

<p>so, 'Four bedrooms home, double garage' can be converted to '4 bedrooms home, double garage' with this function before doing any regex to get the number.</p>
",Named Entity Recognition (NER),python regex get number placed different position string possible case text trying get number bedroom text managed write one capturing possible case key word bedroom text always text bedroom either front back number better approach handle regex may named entity recognition nlp thanks edit case managed convert word number integer using function four bedroom home double garage converted bedroom home double garage function regex get number
Extract personal information about a person from a list of documents and summarize it,"<p>I need to extract personal information about a person from a list of documents and summarize it to the user. If there are 2 people with the same name, the correct person should be identified. If the person has a nickname, that also needs to be identified. The input to the program can be the name of the person, address, organization name etc. I have extracted named entities like person, org, location etc from the text using NLTK library. The output after extracting the named entities is mentioned below,</p>

<p>[('Michael', 'NNP', 'B-PERSON'), ('Joseph', 'NNP', 'B-PERSON'), ('Jackson', 'NNP', 'I-PERSON'), ('was', 'VBD', 'O'), ('born', 'VBN', 'O'), ('in', 'IN', 'O'), ('Gary', 'NNP', 'B-GPE'), (',', ',', 'O'), ('Indiana', 'NNP', 'B-GPE')....</p>

<p>Now, I want to extract relationships between those entities.</p>
",Named Entity Recognition (NER),extract personal information person list document summarize need extract personal information person list document summarize user people name correct person identified person ha nickname also need identified input program name person address organization name etc extracted named entity like person org location etc text using nltk library output extracting named entity mentioned michael nnp b person joseph nnp b person jackson nnp person wa vbd born vbn gary nnp b gpe indiana nnp b gpe want extract relationship entity
Evaluation of stanford crf classifier,"<p>I am doing NER classifier for three tags (PER, ORG, LOC). I am checking my results based on stanford crf classifier. Therefore I would like to know how does the stanford crf classifier evaluation part works. Does it take into account all of the TAGS (PER, ORG, LOC, O) or only (PER, ORG, LOC)??</p>
",Named Entity Recognition (NER),evaluation stanford crf classifier ner classifier three tag per org loc checking result based stanford crf classifier therefore would like know doe stanford crf classifier evaluation part work doe take account tag per org loc per org loc
Custom Named entity recognition,"<p>So i have task in front of me to make a custom ner model for the pharmaceutical industry where in i have a finite list of drugs and over 4000 text files from where NER is supposed to be done. I have also tried entity matching using spacy but it is showing some error. So now i plan on using SKlearn crfsuite but in order to do that my data needs to be in conll format and should be annotated.Would really appreciate if someone could guide me in annotating my text files! is there any way i can initiate automatic annotation on the text files using the drug list i have ? as it is a humongous effort for an individual to achieve the same manually.I also had a look at the question asked in the link mentioned below.
<a href=""https://stackoverflow.com/questions/32073018/ner-model-to-recognize-indian-names"">NER model to recognize Indian names</a>
But no one has actually addressed my question.Would really appreciate if someone could help me out</p>

<p>Spacy code:-</p>

<pre><code>import spacy
from spacy.matcher import PhraseMatcher
from spacy.tokens import Span



class EntityMatcher(object):
    name = 'entity_matcher'

    def __init__(self, nlp, terms, label):
        patterns = [nlp(term) for term in terms]
        self.matcher = PhraseMatcher(nlp.vocab)
        self.matcher.add(label, None, *patterns)

    def __call__(self, doc):
        matches = self.matcher(doc)
        spans = []
        for label, start, end in matches:
            span = Span(doc, start, end, label=label)
            spans.append(span)
        doc.ents = spans
        return doc

data=pd.read_excel(r'C:\Users\xyz\pname.xlsx')
ld=list(set(data['Product']))
nlp = spacy.load('en')

entity_matcher = EntityMatcher(nlp, ld, 'DRUG')
nlp.add_pipe(entity_matcher)
print(nlp.pipe_names)    

doc=nlp('Hi bnbbn, ope all is well.  In preparation for the bcbcb is there anything that BGTD requires specifically?  We had sent you the US centric Briefing Package to align with our previous discussion on having bkjnsd included in the Wave 1  IMOVAX POLIO submission plan. If you would like, we can set-up a BGTD specific meeting after the June 20th meeting to discuss any jk specific product questions you may have as the product mix is a bit different between countries.')

for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)

when i run my script, this is the error i  get :-

[T002] Pattern length (11) &gt;= phrase_matcher.max_length (10). Length can be set on initialization, up to 10.
</code></pre>
",Named Entity Recognition (NER),custom named entity recognition task front make custom ner model pharmaceutical industry finite list drug text file ner supposed done also tried entity matching using spacy showing error plan using sklearn crfsuite order data need conll format annotated would really appreciate someone could guide annotating text file way initiate automatic annotation text file using drug list humongous effort individual achieve manually also look question asked link mentioned href model recognize indian name one ha actually addressed question would really appreciate someone could help spacy code
I want to extract text values from text in spacy,"<p>I am new in using spacy. I want to extract text values from sentences </p>

<pre><code>training_sentence=""I want to add a text field having name as new data""
        OR
training_sentence="" add a field and label it as advance data""
</code></pre>

<p>So from the above sentence, I want to extract ""new data"" and ""advance data""</p>

<p>For now, I am able to extract entities like ""add"", ""field"" and ""label"" using Custom NER.</p>

<p>But I am unable to extract text values as these value can be anything and I am not sure how to extract it using custom NER in spacy.</p>

<p>I have seen code snippet <a href=""https://spacy.io/usage/examples#entity-relations"" rel=""nofollow noreferrer"">here</a> of <strong>entity relations</strong> in the spacy documentation
But don't know to implement it as per my use case.</p>

<p>I can't share the code. Please assist how to tackle this problem</p>
",Named Entity Recognition (NER),want extract text value text spacy new using spacy want extract text value sentence sentence want extract new data advance data able extract entity like add field label using custom ner unable extract text value value anything sure extract using custom ner spacy seen code snippet entity relation spacy documentation know implement per use case share code please assist tackle problem
How to handle two entity extraction methods in NLP,"<p>I am using two different entity extraction methods (<a href=""https://rasa.com/docs/nlu/entities/"" rel=""nofollow noreferrer"">https://rasa.com/docs/nlu/entities/</a>) while building my NLP model in the RASA framework to build a chatbot. 
The bot should handle different questions which have custom entities as well as some general ones like location or organisation. 
So I use both components ner_spacy and ner_crf to create the model. After that I build a small helper script in python to evaluate the model performance. There I noticed that the model struggles to choose the correct enity. </p>

<p>For example for a word 'X' it choosed the pre-defined enity 'ORG' from SpaCy, but it should be recogniced as a custom enity which I defined in the training data. </p>

<p>If I just use the ner_crf extractor I face huge problems in identifing location enities like capitals. Also one of my biggest problems are single answer enities.</p>

<p>Q : ""What´s your favourite animal?""</p>

<p>A : Dog</p>

<p>My model is not able to extract this single entity 'animal' for this single answer. If I answer this question with two words like 'The Dog', the model has no problems to extract the animal entity with the value 'Dog'. </p>

<p>So my question is, is it clever to use two different components to extract entities? One for custom enities and the other one for pre-defined enities. 
If I use two methods, what´s the mechanism in the model which extractor is used?</p>

<p>By the way, currently I´m just testing things out, so my training samples are not that huge it should be (less then 100 examples). Could the problem been solved if I have much more training examples?</p>
",Named Entity Recognition (NER),handle two entity extraction method nlp using two different entity extraction method building nlp model rasa framework build chatbot bot handle different question custom entity well general one like location organisation use component ner spacy ner crf create model build small helper script python evaluate model performance noticed model struggle choose correct enity example word x choosed pre defined enity org spacy recogniced custom enity defined training data use ner crf extractor face huge problem identifing location enities like capital also one biggest problem single answer enities q favourite animal dog model able extract single entity animal single answer answer question two word like dog model ha problem extract animal entity value dog question clever use two different component extract entity one custom enities one pre defined enities use two method mechanism model extractor used way currently testing thing training sample huge le example could problem solved much training example
NER for predefined entities,"<p>I'm developing a application to categorize requirements in a requirement specification in to categories like database, front end, back end, etc. Requirement specification is a single document where I want to see the underlying categories in it. Can I use NER to get the categories? Sentences are divided in to categories if they contain certain words that match that particular category.  </p>

<p><strong>Example</strong></p>

<blockquote>
  <p>data should be stored in a secured database.</p>
</blockquote>

<p>If we consider above given sentence is a requirement it should be categorized in to database category considering the words it contains (database, data). </p>
",Named Entity Recognition (NER),ner predefined entity developing application categorize requirement requirement specification category like database front end back end etc requirement specification single document want see underlying category use ner get category sentence divided category contain certain word match particular category example data stored secured database consider given sentence requirement categorized database category considering word contains database data
Grouping Similar words in python,"<p>I'm trying to extract keywords/entity names from a text using spacy.</p>

<p>I'm able to extract all the entity names but I'm getting a lot of duplicates.</p>

<p>For example, </p>

<pre><code>def keywords(text): 
    tags = bla_bla(text)
    return tags
article = ""Donald Trump. Trump. Trump. Donald. Donald J Trump.""
tags = keywords(article)
</code></pre>

<p>The output I'm getting is:
['Donald Trump', 'Trump', 'Trump', 'Donald', 'Donald J Trump']</p>

<p>How do I cluster all these tags under a master tag 'Donald J Trump'?</p>
",Named Entity Recognition (NER),grouping similar word python trying extract keywords entity name text using spacy able extract entity name getting lot duplicate example output getting donald trump trump trump donald donald j trump cluster tag master tag donald j trump
NLP to find relationship between entities,"<p>My current understanding is that it's possible to extract entities from a text document using toolkits such as OpenNLP, Stanford NLP. </p>

<p>However, is there a way to find <em>relationships</em> between these entities? </p>

<p>For example consider the following text : </p>

<p><em>""As some of you may know, I spent last week at CERN, the European high-energy physics laboratory where the famous Higgs boson was discovered last July. Every time I go to CERN I feel a deep sense of reverence. Apart from quick visits over the years, I was there for three months in the late 1990s as a visiting scientist, doing work on early Universe physics, trying to figure out how to connect the Universe we see today with what may have happened in its infancy.""</em> </p>

<p>Entities: <strong>I</strong> (author), <strong>CERN</strong>, <strong>Higgs boson</strong></p>

<p>Relationships : 
- I ""<strong>visited</strong>"" CERN
- CERN ""<strong>discovered</strong>"" Higgs boson</p>

<p>Thanks. </p>
",Named Entity Recognition (NER),nlp find relationship entity current understanding possible extract entity text document using toolkits opennlp stanford nlp however way find relationship entity example consider following text may know spent last week cern european high energy physic laboratory famous higgs boson wa discovered last july every time go cern feel deep sense reverence apart quick visit year wa three month late visiting scientist work early universe physic trying figure connect universe see today may happened infancy entity author cern higgs boson relationship visited cern cern discovered higgs boson thanks
Dbpedia Extract Concept/Entity/ GRAPH in Python,"<p>I am having a hard time understanding how to fetch data with SPARQL. I looked at several tutorials but still have a few questions</p>

<p>1) How can we determine the headers in the dataset. Say there's a dataset A, and I want to extract the names and locations of people in that dataset. How can I determine the header in which the names is stored?</p>

<p>2) How could I extract a sub-graph containing statements about entities within 2 hops from <strong>Donald Trump</strong> A code example or web link would be very helpful.</p>
",Named Entity Recognition (NER),dbpedia extract concept entity graph python hard time understanding fetch data sparql looked several tutorial still question determine header dataset say dataset want extract name location people dataset determine header name stored could extract sub graph containing statement entity within hop donald trump code example web link would helpful
Specify the provenance of FHIR Resources generated by applying NLP over medical narratives,"<p><a href=""https://www.hl7.org/fhir/"" rel=""nofollow noreferrer"">FHIR</a>  </p>

<blockquote>
  <p>is a standard for health care data exchange, published by HL7®. </p>
</blockquote>

<p>The <a href=""https://www.hl7.org/fhir/Documentreference.html"" rel=""nofollow noreferrer"">DocumentReference</a> </p>

<blockquote>
  <p>Provides metadata about the document so that the document can be
  discovered and managed</p>
</blockquote>

<p>Through the <a href=""https://www.hl7.org/fhir/provenance.html"" rel=""nofollow noreferrer"">Provenance</a> one can </p>

<blockquote>
  <p>describe entities and processes involved in producing and delivering
  or otherwise influencing that resource</p>
</blockquote>

<p><a href=""https://artificial-intelligence.healthcaretechoutlook.com/cxoinsights/unstructured-data-in-healthcare-nid-506.html"" rel=""nofollow noreferrer""> Nearly 80 percent of clinical information in electronic health
 records(EHRs) is ""unstructured"" and in a format that health
 information technology systems cannot use.</a></p>

<p>It is therefore natural to apply computer techniques to automatically generate structured data from the medical records. For that there are several implementations available both on the market and also fully open source. For example cTAKES, CLAMP, NOBLE, ClarityNLP and others are all freely available solutions targeting this task.</p>

<p>They all address the specific need of generating structured data from unstructured medical notes, however they all deliver the structure using their own format, that eventually could be converted into FHIR.</p>

<p>However, a central problem is on how to represent the Provenance of the extracted information, since FHIR is - to the best of my knowledge - missing the way of connecting to the precise location within the DocumentReference object of where the information has been extracted from , with which technology, and which is the level of ""quality"" of the extracted information.</p>

<p>Before submitting a Change Request <a href=""https://gforge.hl7.org/gf/project/fhir/tracker/?action=TrackerItemBrowse"" rel=""nofollow noreferrer"">https://gforge.hl7.org/gf/project/fhir/tracker/?action=TrackerItemBrowse</a> to the FHIR normative, it is recommended to expose the issue to the widest community and the stackoverflow.com is one of the main recommended channels. </p>

<p>For this purpose I am hereby looking forward opinions on the matter, and namely on how to specify the provenance of  FHIR Resources generated by applying NLP over medical narratives. For example, taking an example from the Adverse Event Corpus of Gurulingappa et al <a href=""https://doi.org/10.1016/j.jbi.2012.04.008"" rel=""nofollow noreferrer"">https://doi.org/10.1016/j.jbi.2012.04.008</a> , </p>

<pre><code>10030778|Intravenous azithromycin-induced ototoxicity.|ototoxicity|43|54|azithromycin|22|34
123456789012345678901234567890123456789012345678901234567890
         1         2         3         4         5
</code></pre>

<p>The question is how to represent into FHIR that such drug induced problem has been extracted from the specific bytes positions 22-34 (drug) and 43-54 (problem) from the text (the Title of the paper 1999 in this example).</p>

<pre><code>{
  ""resourceType"": ""AdverseEvent"",
  ""id"": ""example"",
  ""actuality"": ""actual"",
  ""category"": [
    {
      ""coding"": [
        {
          ""system"": ""http://terminology.hl7.org/CodeSystem/adverse-event-category"",
          ""code"": ""product-use-error"",
          ""display"": ""Product Use Error""
        }
      ]
    }
  ],
  ""event"": {
    ""coding"": [
      {
        ""system"": ""http://snomed.info/sct"",
        ""code"": ""9062008"",
        ""display"": ""Ototoxicity (disorder)""
      }
    ],
    ""text"": ""10030778|Intravenous azithromycin-induced ototoxicity.""
  },
  ""subject"": {
    ""reference"": ""Patient/example""
  },
  ""date"": ""1999-02-29T00:00:00+00:00"",
  ""seriousness"": {
    ""coding"": [
      {
        ""system"": ""http://terminology.hl7.org/CodeSystem/adverse-event-seriousness"",
        ""code"": ""Non-serious"",
        ""display"": ""Non-serious""
      }
    ]
  },
  ""severity"": {
    ""coding"": [
      {
        ""system"": ""http://terminology.hl7.org/CodeSystem/adverse-event-severity"",
        ""code"": ""mild"",
        ""display"": ""Mild""
      }
    ]
  },
  ""recorder"": {
    ""reference"": ""Pharmacotherapy. 1999 Feb;19(2):245-8.""
  },
  ""suspectEntity"": [
    {
      ""instance"": {
        ""reference"": ""Azithromycin""
      }
    }
  ]
}
</code></pre>

<p>Currently the FHIR standard does not allow to represent the precise byte position, the quality of the extraction, and the method used to perform it.</p>
",Named Entity Recognition (NER),specify provenance fhir resource generated applying nlp medical narrative fhir standard health care data exchange published hl documentreference provides metadata document document discovered managed provenance one describe entity process involved producing delivering otherwise influencing resource nearly percent clinical information electronic health record ehrs unstructured format health information technology system use therefore natural apply computer technique automatically generate structured data medical record several implementation available market also fully open source example ctakes clamp noble claritynlp others freely available solution targeting task address specific need generating structured data unstructured medical note however deliver structure using format eventually could converted fhir however central problem represent provenance extracted information since fhir best knowledge missing way connecting precise location within documentreference object information ha extracted technology level quality extracted information submitting change request fhir normative recommended expose issue widest community stackoverflow com one main recommended channel purpose hereby looking forward opinion matter namely specify provenance fhir resource generated applying nlp medical narrative example taking example adverse event corpus gurulingappa et al question represent fhir drug induced problem ha extracted specific byte position drug problem text title paper example currently fhir standard doe allow represent precise byte position quality extraction method used perform
How can I get substring or 2 words from the list on matching text?,"<p>I have a list where all the company name is there</p>

<pre><code>organizations={'mahindra &amp; mahindra','atametica','cognizant Technology','Tata Cosultancy Services'}
</code></pre>

<p>I have a text where I have 1 or 2 company names and I want to extract those company names from the organizations.
example:</p>

<pre><code>text = 'XXX has worked in Tata Consultancy Services and currently working in cognizant technology.He has experience in Java Technology as well'
</code></pre>

<p>How can I fetch company from the text.</p>
",Named Entity Recognition (NER),get substring word list matching text list company name text company name want extract company name organization example fetch company text
Can stanford ner tagger process hindi or nepali language?,"<p>I would like to train a ner model using <strong>stanford-ner.jar</strong> CRFClassifier for Nepali or Hindi language. Can I simply use the java command line mentioned in the <a href=""https://nlp.stanford.edu/software/crf-faq.html#a"" rel=""nofollow noreferrer"">here</a></p>
",Named Entity Recognition (NER),stanford ner tagger process hindi nepali language would like train ner model using stanford ner jar crfclassifier nepali hindi language simply use java command line mentioned
extract name entity from unstructured data,"<p>I have highly unstructured data and I want to extract full name out of It. The data is something like this </p>

<pre><code>txt = "" 663555 murphy rd suite 106 richardson tx 7508 usa 111 it park indore 452 010 india ph 91 987 4968420 123456789 sumeetlogikviewcom  Nirali Khoda cofounder analytics pvt ltd ideata  a comprehensive data analytics platform""

text = ""dicictay  8 8 8 bf infotech pvt ltd manager infotech pvt ltd  redefining technologies 91 12345 12345 zoeb fatemi ""
</code></pre>

<p>I tried spacy and standfordNER but It is not giving good results. It gives me name from address like this </p>

<pre><code>en = spacy.load('en_core_web_md')

txt = txt.title().strip()

sents = en(txt)

people = [ee for ee in sents.ents if ee.label_ == 'PERSON']
</code></pre>

<p>out put is this :</p>

<pre><code>[663555 Murphy Rd Suite, Analytics Pvt Ltd Ideata]
</code></pre>

<p>expected output :</p>

<pre><code>[Nirali Khoda]
</code></pre>

<p>Help would be appreciated. Thanks :) </p>
",Named Entity Recognition (NER),extract name entity unstructured data highly unstructured data want extract full name data something like tried spacy standfordner giving good result give name address like put expected output help would appreciated thanks
Is there a way to determine sensitive data in HTTP/HTTPS POST Body?,"<p>I'm working on developing the system that can determine the chance of sensitive data leakage via HTTPS POST requests. To be honest, I'm really newbie in this field. Anyway, I realised that mostly the data like text will be sent as a key-value pairs. So, I tried to bring something like keyword matching to detect the key like username, password, or GPS Location. BUT it doesn't work well because the key do not have to be labeled correctly, like ""username"".</p>

<p>I stuck here. Is there anyway to determine what the type the value is?</p>

<p>ALSO, can I use text analysis with this kind of system? I want to inspect on HTTPS POST requests that I used Proxie (application) to export it as raw TCP text files.</p>
",Named Entity Recognition (NER),way determine sensitive data http http post body working developing system determine chance sensitive data leakage via http post request honest really newbie field anyway realised mostly data like text sent key value pair tried bring something like keyword matching detect key like username password gps location work well key labeled correctly like username stuck anyway determine type value also use text analysis kind system want inspect http post request used proxie application export raw tcp text file
Extracting chunk from sentence,"<p>I am trying to extract a chunk from a sentence based on a pattern sequence.</p>

<pre><code>import re
import spacy           
nlp = spacy.load('en')
s = ""His name is Robinson.""
doc = nlp(s)
pattern = re.compile(r'(&lt;PRP$|POS&gt;+&lt;RB.?&gt;)*(&lt;JJ.?&gt;)*(&lt;NN.?|VBG|VBN&gt;+&lt;VB.? 
|MD|RP&gt;+)')
for chunk in doc:

   if(re.search(pattern,chunk.tag_)):
       print(chunk, chunk.tag_)
</code></pre>

<p>The chunk acquired through this should be is ""His name is""</p>

<p>How can I implement this?</p>
",Named Entity Recognition (NER),extracting chunk sentence trying extract chunk sentence based pattern sequence chunk acquired name implement
Determining geo location by arbitrary body of text,"<p>I am working on a project that I am not exactly sure how to approach. The problem can be summarized as following:</p>

<ul>
<li>Given an arbitrary body of text(kind of like a report), determine what geographic location that each part of the report is referring to.</li>
</ul>

<p>Geographic locations range from states to counties(all within US), so their number is limited, but each report generally contains references to multiple locations. For example, first 5 paragraphs of report might be about a state as a whole, and then then next 5 would be about individual counties within that state, or something like that.</p>

<p>I am curious what would be the best way of approaching a problem like that, perhaps with a specific recommendation in terms of NLP or ML frameworks(Python or Java)?</p>
",Named Entity Recognition (NER),determining geo location arbitrary body text working project exactly sure approach problem summarized following given arbitrary body text kind like report determine geographic location part report referring geographic location range state county within u number limited report generally contains reference multiple location example first paragraph report might state whole next would individual county within state something like curious would best way approaching problem like perhaps specific recommendation term nlp ml framework python java
Extract text based on character position returned from gregexpr,"<p>I'm working in R, trying to prepare text documents for analysis. Each document is stored in a column (aptly named, ""document"") of dataframe called ""metaDataFrame."" The documents are strings containing articles and their BibTex citation info.  Data frame looks like this: </p>

<pre><code>[1] filename         document                          doc_number
[2] lithuania2016    Commentary highlights Estonian...    1
[3] lithuania2016    Norwegian police, immigration ...    2
[4] lithuania2016    Portugal to deply over 1,000 m...    3
</code></pre>

<p>I want to extract the BibTex information from each document into a new column. The citation information begins with ""Credit:"" but some articles contain multiple ""Credit:"" instances, so I need to extract all of the text after the last instance. Unfortunately, the string is only sometimes preceded by a new line. </p>

<p>My solution so far has been to find all of the instances of the string and save the location of the last instance of ""Credit:"" in each document in a list:</p>

<pre><code>locate.last.credit &lt;- lapply(gregexpr('Credit:', metaDataFrame$document), tail, 1)
</code></pre>

<p>This provides a list of integer locations of the last ""Credit:"" string in each document or a value of ""-1"" where no instance is found. (Those missing values pose a separate but related problem I think I can tackle after resolving this issue). </p>

<p>I've tried variations of strsplit, substr, stri_match_last, and rm_between...but can't figure out a way to use the character position in lieu of regular expression to extract this part of the string.  </p>

<p>How can I use the location of characters to manipulate a string instead of regular expressions? Is there a better approach to this (perhaps with regex)?</p>
",Named Entity Recognition (NER),extract text based character position returned gregexpr working r trying prepare text document analysis document stored column aptly named document dataframe called metadataframe document string containing article bibtex citation info data frame look like want extract bibtex information document new column citation information begin credit article contain multiple credit instance need extract text last instance unfortunately string sometimes preceded new line solution far ha find instance string save location last instance credit document list provides list integer location last credit string document value instance found missing value pose separate related problem think tackle resolving issue tried variation strsplit substr stri match last rm figure way use character position lieu regular expression extract part string use location character manipulate string instead regular expression better approach perhaps regex
ML based domain specific named enitty recognition (NER)?,"<p>I need to build a classifier which identifies NEs in a specific domain. So for instance if my domain is Hockey or Football, the classifier should go accept NEs in that domain but NOT all pronouns it sees on web pages. My ultimate goal is to improve text classification through NER. </p>

<p>For people working in this area please suggest me how should I build such a classifier?
thanks!</p>
",Named Entity Recognition (NER),ml based domain specific named enitty recognition ner need build classifier identifies ne specific domain instance domain hockey football classifier go accept ne domain pronoun see web page ultimate goal improve text classification ner people working area please suggest build classifier thanks
How to use NLTK DependencyGrammar package In NER,"<p>How to use NLTK DependencyGrammar package in Named Entity Recognition (NER)?</p>

<p>Eg. sentence that I am looking is:
“<em>what is the status of my invoice P1234U?</em>”</p>

<p>And I would like to extract P1234U as the invoice number.</p>

<p>Without much training I would like to do this, so no SPACY solution please.</p>

<p>I would prefer to go in a knowledge graph way.</p>
",Named Entity Recognition (NER),use nltk dependencygrammar package ner use nltk dependencygrammar package named entity recognition ner eg sentence looking status invoice p u would like extract p u invoice number without much training would like spacy solution please would prefer go knowledge graph way
How to analyse an NER that is trained using Spacy?,"<p>This is a simple code that is picked up from the tutorial documentation (more or less). Once I train the NER model using the below training code, I use <code>nlp(sentence).ents</code> inside the for loop to get the named entities. As you can see, I used a blank model <code>spacy.blank('en')</code> this was because I am adding new entities. But no entities are detected from the test set.</p>

<pre><code>import spacy
import random
from spacy.util import compounding
from spacy.util import minibatch
def get_batches(train_data, model_type):
    max_batch_sizes = {'tagger': 32, 'parser': 16, 'ner': 16, 'textcat': 64}
    max_batch_size = max_batch_sizes[model_type]
    if len(train_data) &lt; 1000:
        max_batch_size /= 2
    if len(train_data) &lt; 500:
        max_batch_size /= 2
    batch_size = compounding(1, max_batch_size, 1.001)
    batches = minibatch(train_data, size=batch_size)
    return batches

nlp = spacy.blank('en')
nlp.vocab.vectors.name = 'blank_vector'
optimizer = nlp.begin_training()
for i in range(20):
    random.shuffle(TRAIN_DATA)
    batches = get_batches(TRAIN_DATA, 'ner')
    for batch in batches:
        texts, annotations = zip(*batch)
        nlp.update(texts, annotations, drop=0.5, sgd=optimizer)
#     for text, annotations in TRAIN_DATA:
#         nlp.update([text], [annotations], drop=0.5, sgd=optimizer)
nlp.to_disk('model')
</code></pre>

<p><strong><em>How does one go about analysing a model created in spacy?</em></strong> I did try to understand some of it by looking at the <code>model</code> that was created by <code>nlp.to_disk('model')</code>. But unfortunately I did not know how to add the necessary information that I need.</p>

<p><strong>My Requirement:</strong> Consider percentages like [20%, 0.5% etc] and dollar amounts like [$100, 100 dollars etc] such occurences will be picked up as <em>MONEY, PERCENT</em> by the pre-trained NER but I need them to detect entities based on usage such as ['HOME_LOAN_INTEREST_RATE', 'CAR_LOAN_INTEREST_RATE' etc]. Now my problem could still be because all the dollar amounts are not present in the vocabulary. If that is the case, how do I go about fixing this problem.</p>

<p>Any help on this will be much appreciated. </p>
",Named Entity Recognition (NER),analyse ner trained using spacy simple code picked tutorial documentation le train ner model using training code use inside loop get named entity see used blank model wa adding new entity entity detected test set doe one go analysing model created spacy try understand looking wa created unfortunately know add necessary information need requirement consider percentage like etc dollar amount like dollar etc occurences picked money percent pre trained ner need detect entity based usage home loan interest rate car loan interest rate etc problem could still dollar amount present vocabulary case go fixing problem help much appreciated
NLP :: Extract entities and their values from string in Python,"<p>I have a certain list/dictionary of fruits names like below:</p>

<pre><code>fruits = [""Banana"",""Apples"",""Oranges""]
</code></pre>

<p>This is just a sample list, there can be many more fruits in it.
My input text looks like below:</p>

<pre><code>text1 = "" I want to 2 Apples and 3 Bananas""
text2 = "" I need Apples 2, Bananas 5""
text3 = ""want to have 1 orange""
</code></pre>

<p>I want to note that my input string is a free flowing text and hence doesn't follow <strong>any certain format</strong>. </p>

<p><strong>Problem</strong>: I want to parse the string and get a table/list with what quantity of which fruit I have. The amount can be before or after the fruit. Below can be seen as output for input ""<em>text1</em>"" I want:</p>

<pre><code>Apple   2

Banana  3
</code></pre>

<p>I have gone through similar problem statements in various links but there the input string has certain format which my problem doesn't.
Some of the links which I have gone through are:</p>

<p><a href=""https://stackoverflow.com/questions/42536406/python-extracting-variables-from-string-templates"">python: extracting variables from string templates</a></p>

<p><a href=""https://stackoverflow.com/questions/35144597/how-to-extract-variable-name-and-value-from-string-in-python"">How to extract variable name and value from string in python</a></p>

<p><a href=""https://stackoverflow.com/questions/22359216/extracting-key-value-pairs-from-a-string-containing-escaped-characters"">extracting key value pairs from a string containing escaped characters</a></p>

<p><a href=""https://stackoverflow.com/questions/10380992/get-python-dictionary-from-string-containing-key-value-pairs"">get python dictionary from string containing key value pairs</a></p>
",Named Entity Recognition (NER),nlp extract entity value string python certain list dictionary fruit name like sample list many fruit input text look like want note input string free flowing text hence follow certain format problem want parse string get table list quantity fruit amount fruit seen output input text want gone similar problem statement various link input string ha certain format problem link gone
"Using Pandas, Could i detect wrong element in a fixed column and return that value?","<p>I am new to Pandas. My goal is to detect the wrong element  in a fixed column  and return that  row value 
Here is the sample scenario</p>

<p><a href=""https://i.sstatic.net/gKOcU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gKOcU.png"" alt=""enter image description here""></a></p>

<p>45 dollar is the wrong element in the country column. so i want to detect this value and return the row number(if possible) in my program. My first thought was to create a list and match with this or do i need to search NLP solution here. Kindly help me to solve it out</p>
",Named Entity Recognition (NER),using panda could detect wrong element fixed column return value new panda goal detect wrong element fixed column return row value sample scenario dollar wrong element country column want detect value return row number possible program first thought wa create list match need search nlp solution kindly help solve
How to extract specific information from emails using machine learning?,"<p>I have multiple emails with a list of stock, price and quantity. Each day, the list is formatted a little differently and I was hoping to use NLP to try to understand read in the data and reformat it to show the information in a correct format.</p>

<p>Here is a sample of the emails I receive:</p>

<pre><code>Symbol  Quantity    Rate
AAPL    16        104
MSFT    8.3k      56.24
GS      34        103.1
RM      3,400     -10
APRN    6k        11
NP      14,000    -44
</code></pre>

<p>As we can see, the quantity is in varying formats, the ticker always is standard but the rate is either positive or negative or could have decimals. Another issue is that the headers are not always the same so that is not an identifier that I can rely on.</p>

<p>So far I've seen some examples online where this works for names but I am unable to implement this for stock ticker, quantity and price. The code I've tried so far is below: </p>

<pre><code>import re
import nltk
from nltk.corpus import stopwords
stop = stopwords.words('english')

string = """"""

To: ""Anna Jones"" &lt;anna.jones@mm.com&gt;
From: James B.

Hey,
This week has been crazy. Attached is my report on IBM. Can you give it a quick read and provide some feedback.
Also, make sure you reach out to Claire (claire@xyz.com).
You're the best.
Cheers,
George W.
212-555-1234
""""""


def extract_phone_numbers(string):
    r = re.compile(r'(\d{3}[-\.\s]??\d{3}[-\.\s]??\d{4}|\(\d{3}\)\s*\d{3}[-\.\s]??\d{4}|\d{3}[-\.\s]??\d{4})')
    phone_numbers = r.findall(string)
    return [re.sub(r'\D', '', number) for number in phone_numbers]


def extract_email_addresses(string):
    r = re.compile(r'[\w\.-]+@[\w\.-]+')
    return r.findall(string)


def ie_preprocess(document):
    document = ' '.join([i for i in document.split() if i not in stop])
    sentences = nltk.sent_tokenize(document)
    sentences = [nltk.word_tokenize(sent) for sent in sentences]
    sentences = [nltk.pos_tag(sent) for sent in sentences]
    return sentences


def extract_names(document):
    names = []
    sentences = ie_preprocess(document)
    for tagged_sentence in sentences:
        for chunk in nltk.ne_chunk(tagged_sentence):
            if type(chunk) == nltk.tree.Tree:
                if chunk.label() == 'PERSON':
                    names.append(' '.join([c[0] for c in chunk]))
    return names


if __name__ == '__main__':
    numbers = extract_phone_numbers(string)
    emails = extract_email_addresses(string)
    names = extract_names(string)

    print(numbers)
    print(emails)
    print(names)
</code></pre>

<p>This code does a good job with numbers, emails and names but I am unable to replicate this for the example I have and do not really know how to go about it. Any tips will be more than helpful. </p>
",Named Entity Recognition (NER),extract specific information email using machine learning multiple email list stock price quantity day list formatted little differently wa hoping use nlp try understand read data reformat show information correct format sample email receive see quantity varying format ticker always standard rate either positive negative could decimal another issue header always identifier rely far seen example online work name unable implement stock ticker quantity price code tried far code doe good job number email name unable replicate example really know go tip helpful
Ensure the presence of a word/token/noun in Encoder-Decoder text generation deep learning models,"<p>I am stuck with a problem where in I want to ensure that specific tokens/words are produced while decoding and generating abstractive-style sentences. </p>

<p>I am working with deep learning models like LSTM and transformer model for generating short sentences(100-200 characters). I want that some words like places or nouns(like brand names) be present in the generated texts.</p>

<p>I am not sure if there has been any research on this, I couldn't really find a paper after an extensive search on it.</p>

<p>TIA, any leads or suggestions are appreciated. :)</p>
",Named Entity Recognition (NER),ensure presence word token noun encoder decoder text generation deep learning model stuck problem want ensure specific token word produced decoding generating abstractive style sentence working deep learning model like lstm transformer model generating short sentence character want word like place noun like brand name present generated text sure ha research really find paper extensive search tia lead suggestion appreciated
Named Entity Recognition in NLP using Python,"<p>I have lots of CVs text documents. In that, there is different formats of dates are available e.g. <em>Birthdate</em> - 12-12-1995, <em>Experience-year</em> - 2000 PRESENT <em>or</em> 1995-2005 <em>or</em> 5 years of experience <em>or</em> 1995/2005, <em>Date-of-Joining</em> - 5th March, 2015 etc. From these data I want to extract only years of experience. <strong>How can I do this in Python using NLP?</strong> Please answer.</p>

<p>I have tried with following :</p>

<pre><code>#This gives me all the dates from documents
import datefinder
data = open(""/home/system/Desktop/samplecv/5c22fcad79fcc1.33753024.txt"")
str1 = ''.join(str(e) for e in data)
matches = datefinder.find_dates(str1)
for match in matches:
    print(match)
</code></pre>
",Named Entity Recognition (NER),named entity recognition nlp using python lot cv text document different format date available e g birthdate experience year present year experience date joining th march etc data want extract year experience python using nlp please answer tried following
rule based information extraction from raw text,"<p>I have a some phrases from aviation communication domain for eg: "" Metro tower, 4 Delta Tango Charlie, request climb to flight level 350, wind 220"" In this case </p>

<pre><code>""metro tower"" = Air traffic control tower name,
""four Delta Tango Charlie"" = airplane call sign ,
""requset climb to flight level 350"" = type of clearance request,
""350"" = flight level
""wind 220"" = wind info
</code></pre>

<p>I need to separate and extract these values corresponding to the tag names mentioned above to be used in later processing. As per my research I have figured out that this could be achieved by using custom Named Entity Recognition classes and rules, but I am not clear if this is the most efficient way to do it since this is to be used in a chat application and the processing time and response time has to be really quick. Please tell me if there are any other algorithms or techniques to do this.</p>

<p>Next problem is ""four Delta Tango Charlie"" part which consist of numbers and phonetic alphabet (A=Alpha, B=Bravo, C=Charlie,P=papa etc). What are the possible ways of creating a term dictionary for this alphabet and use the dictionary to extract the call sign from the raw text ?</p>

<p>Please also tell me what sought of algorithms could be used to solve my problem</p>
",Named Entity Recognition (NER),rule based information extraction raw text phrase aviation communication domain eg metro tower delta tango charlie request climb flight level wind case need separate extract value corresponding tag name mentioned used later processing per research figured could achieved using custom named entity recognition class rule clear efficient way since used chat application processing time response time ha really quick please tell algorithm technique next problem four delta tango charlie part consist number phonetic alphabet alpha b bravo c charlie p papa etc possible way creating term dictionary alphabet use dictionary extract call sign raw text please also tell sought algorithm could used solve problem
Getting Type Error:&#39;Nonetype&#39; is not iterable in Spacy to build Custom NER model,"<p>I am a beginner in Spacy. Recently I am making entity recognition model using spacy with small dataset. I have made csv file which contains canadian urban information like Country, City, Province, Postal address etc. I have 
  <a href=""https://dataturks.com"" rel=""nofollow noreferrer"">https://dataturks.com</a> free NER labeling service to label my  row elements
they have provided a convertDataturkSpacy() method to give spacy comaptible json format.
Up to this point, all are going good but  i am getting </p>

<blockquote>
  <p>TypeError: 'NoneType' object is not iterable</p>
</blockquote>

<p>here is my  snippet</p>

<pre><code>import json
import logging
import spacy
import random
from spacy.util import minibatch, compounding
trainingfilename=""C:/Users/codemen/Desktop/Timeseries Analytics/Canadianinfo.json""

logging.basicConfig(level=logging.INFO)




def ConvertDataturkToSpacy(trainingfilename):

    try:
        trainingData=[]
        lines=[]
        # reading file  and  formating  part
        with open(trainingfilename,'r') as f:
            lines=f.readlines()
        for line in lines:
            data=json.loads(line)
            text=data['content']
            entities=[]
            print('entties',entities)
            for annotation in data['annotation']:
                #print(""Here is the thing"")
                point=annotation['points'][0] #single point annotation part
                #print(point)
                labels=annotation['label']
                print(""isintance"",labels)
                if not isinstance(labels,list):#handling both list of labels or single label
                    labels=[labels]
                    print(labels)

                for label in labels:
                    #dataturks indices are inclusive but spacy indices are not so dealing with it by adding  with +1
                    #print(""Test here"")
                    entities.append((point['start'],point['end']+1,label))

            trainingData.append((text,{""entities"":entities}))
        return trainingData
    except Exception as e:
        logging.exception(""Unable to process item"" + trainingfilename +""\n""+ ""errror =""+str(e))
        return None

TrainingData=ConvertDataturkToSpacy(trainingfilename)
</code></pre>

<p>So far i have figured out that my initialized empty list entity[] is not appending and updating over iteration. </p>

<p><a href=""https://i.sstatic.net/4IB93.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4IB93.png"" alt=""enter image description here""></a></p>
",Named Entity Recognition (NER),getting type error nonetype iterable spacy build custom ner model beginner spacy recently making entity recognition model using spacy small dataset made csv file contains canadian urban information like country city province postal address etc free ner labeling service label row element provided convertdataturkspacy method give spacy comaptible json format point going good getting typeerror nonetype object iterable snippet far figured initialized empty list entity appending updating iteration
Identifying text using NLP,"<p>I'm trying to find the courses in the below line of text using some NLP technique.</p>

<pre><code>from nltk import word_tokenize, pos_tag, ne_chunk
sentence = ""SDGI is offering courses like Electronics,Mechatronics, Physics,Mechanical Engineering""    
print ne_chunk(pos_tag(word_tokenize(sentence)))
</code></pre>

<p>Out put of this is </p>

<pre><code>(S
  (ORGANIZATION SDGI/NNP)
  is/VBZ
  offering/VBG
  courses/NNS
  like/IN
  Electronics/NNS
  ,/,
  Mechatronics/NNS
  ,/,
  (PERSON Physics/NNPS)
  ,/,
  (PERSON Mechanical/NNP Engineering/NNP))
</code></pre>

<p>Is there any way I can extract the courses from this line?</p>

<p>In my real project I will be getting so many documents from which I need to get the course names.</p>

<p>Any help is appreciated!</p>
",Named Entity Recognition (NER),identifying text using nlp trying find course line text using nlp technique put way extract course line real project getting many document need get course name help appreciated
Automatic Summarization using Named Entity Recognition,"<p>I would like to use <strong>Named Entity Recognition</strong> (NER) to auto summarize <strong>Airline ticket</strong> based on a given dataset.</p>

<p>So basically this is my dataset.</p>

<p><a href=""https://i.sstatic.net/ajUcG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ajUcG.png"" alt=""enter image description here""></a></p>

<p>Here i need to create a summary about the details of passenger in a pdf like :</p>

<blockquote>
  <p>The PNR Number ____(PNRNum) refers to the passenger name
  ____(Name) travelling from ____(Dep Airport),____(Start Country) to ____(Arr Airport),____(End Country) starting at ____(Start Time). The flight number is ____(Flight No) which is _____(Int Dom) using
  _____(Cabin Class) ticket of base fare _____(Base Fare).</p>
</blockquote>

<p>Here when the PNR Number should be given as input to enter in the first blank space and the corresponding data from dataset should be filled in remaining blank spaces.</p>

<pre><code>airline = pd.read_csv(""AIR-LINE.csv"")
def create_airline_ticket():
    c = canvas.Canvas('AIRlines.pdf')

    c.setFont(""Courier"", 20)
    c.drawCentredString(300, 700, 'Airline Ticket')
    c.setFont(""Courier"", 14)
    form = c.acroForm

    c.drawString(10, 650, 'The PNR Number')
    options = [('airline.loc[[0, 10], :]')]
    form.choice(name='choice1', tooltip='Field choice1',
                value='A',
                x=165, y=645, width=72, height=20,
                borderColor=magenta, fillColor=pink, 
                textColor=blue, forceBorder=True, options=options)

    c.save()
</code></pre>

<p>I thought of using ReportLabs module in order to use listbox available in it. But it didn't go accordingly. I have to do with some other way.</p>

<p>So could you suggest me a step by step procedure? Since i'm a beginner in python, i could learn easily. Thanks. </p>
",Named Entity Recognition (NER),automatic summarization using named entity recognition would like use named entity recognition ner auto summarize airline ticket based given dataset basically dataset need create summary detail passenger pdf like pnr number pnrnum refers passenger name name travelling dep airport start country arr airport end country starting start time flight number flight int dom using cabin class ticket base fare base fare pnr number given input enter first blank space corresponding data dataset filled remaining blank space thought using reportlabs module order use listbox available go accordingly way could suggest step step procedure since beginner python could learn easily thanks
Recognize partial/complete address with NLP framework,"<p>I was wondering the amount of work on NLP framework to get partial (without city) or complete postal address extraction with NLP frameworks from unstructured text? Are NLP frameworks efficient to do this? Also, how difficult is it to ""train"" Named Entity Recognition modules to match new locations ?</p>
",Named Entity Recognition (NER),recognize partial complete address nlp framework wa wondering amount work nlp framework get partial without city complete postal address extraction nlp framework unstructured text nlp framework efficient also difficult train named entity recognition module match new location
Runnig DeepPavlov Named Entity Recognition,"<p>How I can run NER from DeepPavlov? </p>

<p>I have an input file with sentences ""sentences.txt"" and need to put results in ""result.txt"". </p>
",Named Entity Recognition (NER),runnig deeppavlov named entity recognition run ner deeppavlov input file sentence sentence txt need put result result txt
Named Entity Recognition in practice,"<p>I am a NLP novice trying to learn, and would like to better understand how Named Entity Recognition (NER) is implemented in practice, for example in popular python libraries such as <a href=""https://spacy.io"" rel=""nofollow noreferrer"">spaCy</a>.</p>

<p>I understand the basic concept behind it, but I suspect I am missing some details.
From the documentation, it is not clear to me for example how much preprocessing is done on the text and annotation data; and what statistical model is used.</p>

<p>Do you know if:</p>

<ul>
<li>In order to work, the text has to go through chunking before the model is trained, right? Otherwise it wouldn't be able to perform anything useful?</li>
<li>Are the text and annotations typically normalized prior to the training of the model? So that if a named entity is at the beginning or middle of a sentence it can still work?</li>
<li>Specifically in spaCy, how are things implemented concretely? Is it a HMM, CRF or something else that is used to build the model?</li>
</ul>

<p>Apologies if this is all trivial, I am having some trouble finding easy to read documentation on NER implementations.</p>
",Named Entity Recognition (NER),named entity recognition practice nlp novice trying learn would like better understand named entity recognition ner implemented practice example popular python library spacy understand basic concept behind suspect missing detail documentation clear example much preprocessing done text annotation data statistical model used know order work text ha go chunking model trained right otherwise able perform anything useful text annotation typically normalized prior training model named entity beginning middle sentence still work specifically spacy thing implemented concretely hmm crf something else used build model apology trivial trouble finding easy read documentation ner implementation
How do I use Conll 2003 corpus in python crfsuite,"<p>I have downloaded Conll 2003 corpus (""eng.train""). I want to use it to extract entity using python crfsuite training. But I don't know how to load this file for training.</p>

<p>I found this example, but it is not for English.</p>

<pre><code>train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))
test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))
</code></pre>

<p>Also in future I would like to train new entities other than POS or location. How can I add those.</p>

<p>Also please suggest how to handle multiple words.</p>
",Named Entity Recognition (NER),use conll corpus python crfsuite downloaded conll corpus eng train want use extract entity using python crfsuite training know load file training found example english also future would like train new entity po location add also please suggest handle multiple word
"IOB tagging scheme, usage of BI scheme","<p>I often see variants of IOB tagging scheme such as IOB, BIO, IOBES mentioned in the literature for chunking, NER etc. I tried using only BI tags for detecting morpheme boundaries (segmentation) in a binary classification setting and got high F1 score while adding O tag gave much lower score. Is there any drawback with using only BI tags ? Why I am not seeing people using it ?</p>
",Named Entity Recognition (NER),iob tagging scheme usage bi scheme often see variant iob tagging scheme iob bio iobes mentioned literature chunking ner etc tried using bi tag detecting morpheme boundary segmentation binary classification setting got high f score adding tag gave much lower score drawback using bi tag seeing people using
Human annotation tool for corpora in NLP,"<p>I am trying to build my own training corpus for Named Entity Recognition, but I don't know if there is already an existing tool for this or if I have to implement one myself. </p>

<p>Basically, what I need to do is take a corpus and manually tag it word by word, which is pretty tedious, but it has to be done.</p>

<p>Can anyone tell me if there is already an existing one and where to get it?</p>
",Named Entity Recognition (NER),human annotation tool corpus nlp trying build training corpus named entity recognition know already existing tool implement one basically need take corpus manually tag word word pretty tedious ha done anyone tell already existing one get
I have a question regarding practical implementation of Named Entity Recognition in NLP,"<p>If we Consider two Named Entity Relation systems, one based on the use of word embedding and the other using both word and character embedding jointly. How can we intuitively conclude which model is better for NER task? Is any illustration possible for above case?</p>
",Named Entity Recognition (NER),question regarding practical implementation named entity recognition nlp consider two named entity relation system one based use word embedding using word character embedding jointly intuitively conclude model better ner task illustration possible case
NLP: Extract shape names and shape dimensions,"<p>I am an NLP beginner. I am working on a task where I have to process a text to draw shapes. </p>

<p>I did some research and found that tokensregex could be a good fit.</p>

<p>For example</p>

<p>Text : Could you please draw an Isosceles triangle with an altitude of 150 and a base of 100.</p>

<p>In here, my approach is to write tokensregex rule to extract shape name and dimensions. Once i get these two data, I will draw the extracted shape with the given dimensions.</p>

<p>Is my choice of using tokensregex correct?</p>

<p>Or is there any other way to do it smartly?</p>
",Named Entity Recognition (NER),nlp extract shape name shape dimension nlp beginner working task process text draw shape research found tokensregex could good fit example text could please draw isosceles triangle altitude base approach write tokensregex rule extract shape name dimension get two data draw extracted shape given dimension choice using tokensregex correct way smartly
Confidence prediction in Stanford NER,"<p>Confidence level to sequence prediction in Stanford NER Tagger. It's possible? Confidence for a given predicted sequence.</p>
",Named Entity Recognition (NER),confidence prediction stanford ner confidence level sequence prediction stanford ner tagger possible confidence given predicted sequence
use polyglot package for Named Entity Recognition in hebrew,"<p>I am trying to use the polyglot package for Named Entity Recognition in hebrew. <br>
this is my code:</p>

<pre><code># -*- coding: utf8 -*-
import polyglot
from polyglot.text import Text, Word
from polyglot.downloader import downloader
downloader.download(""embeddings2.iw"")
text = Text(u""in france and in germany"")
print(type(text))
text2 = Text(u""נסעתי מירושלים לתל אביב"")
print(type(text2))
print(text.entities)
print(text2.entities)
</code></pre>

<p>this is the output:</p>

<pre><code>&lt;class 'polyglot.text.Text'&gt;
&lt;class 'polyglot.text.Text'&gt;
[I-LOC([u'france']), I-LOC([u'germany'])]
Traceback (most recent call last):
  File ""C:/Python27/Lib/site-packages/IPython/core/pyglot.py"", line 15, in &lt;module&gt;
    print(text2.entities)
  File ""C:\Python27\lib\site-packages\polyglot\decorators.py"", line 20, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File ""C:\Python27\lib\site-packages\polyglot\text.py"", line 132, in entities
    for i, (w, tag) in enumerate(self.ne_chunker.annotate(self.words)):
  File ""C:\Python27\lib\site-packages\polyglot\decorators.py"", line 20, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File ""C:\Python27\lib\site-packages\polyglot\text.py"", line 100, in ne_chunker
    return get_ner_tagger(lang=self.language.code)
  File ""C:\Python27\lib\site-packages\polyglot\decorators.py"", line 30, in memoizer
    cache[key] = obj(*args, **kwargs)
  File ""C:\Python27\lib\site-packages\polyglot\tag\base.py"", line 191, in get_ner_tagger
    return NEChunker(lang=lang)
  File ""C:\Python27\lib\site-packages\polyglot\tag\base.py"", line 104, in __init__
    super(NEChunker, self).__init__(lang=lang)
  File ""C:\Python27\lib\site-packages\polyglot\tag\base.py"", line 40, in __init__
    self.predictor = self._load_network()
  File ""C:\Python27\lib\site-packages\polyglot\tag\base.py"", line 109, in _load_network
    self.embeddings = load_embeddings(self.lang, type='cw', normalize=True)
  File ""C:\Python27\lib\site-packages\polyglot\decorators.py"", line 30, in memoizer
    cache[key] = obj(*args, **kwargs)
  File ""C:\Python27\lib\site-packages\polyglot\load.py"", line 61, in load_embeddings
    p = locate_resource(src_dir, lang)
  File ""C:\Python27\lib\site-packages\polyglot\load.py"", line 43, in locate_resource
    if downloader.status(package_id) != downloader.INSTALLED:
  File ""C:\Python27\lib\site-packages\polyglot\downloader.py"", line 738, in status
    info = self._info_or_id(info_or_id)
  File ""C:\Python27\lib\site-packages\polyglot\downloader.py"", line 508, in _info_or_id
    return self.info(info_or_id)
  File ""C:\Python27\lib\site-packages\polyglot\downloader.py"", line 934, in info
    raise ValueError('Package %r not found in index' % id)
ValueError: Package u'embeddings2.iw' not found in index
</code></pre>

<p>The english worked but not the hebrew.<br>
Whether I try to download the package <code>u'embeddings2.iw'</code>  or not I get: <br></p>

<pre><code>ValueError: Package u'embeddings2.iw' not found in index
</code></pre>
",Named Entity Recognition (NER),use polyglot package named entity recognition hebrew trying use polyglot package named entity recognition hebrew code output english worked hebrew whether try download package get
spacy updating NER,"<p>Spacy <a href=""https://spacy.io/usage/training#ner"" rel=""nofollow noreferrer"">documentation</a> shows how to update an NER with additional training examples. However, it trains using the entities offsets. How can I perform the same task but using <strong>BILUO scheme</strong>? I want to use training examples that contain for each sentence the list of tokens and the respective <strong>BILUO tags</strong>.</p>
",Named Entity Recognition (NER),spacy updating ner spacy documentation show update ner additional training example however train using entity offset perform task using biluo scheme want use training example contain sentence list token respective biluo tag
How can I find expected target phrase or keywords from given sentence in Python?,"<p>I am wondering that is there any efficient way to extract expected target phrase or key phrase from given sentence. So far I tokenized the given sentence and get POS tag for each word. Now I am not sure how to extract target key phrase or keyword from given sentence. The way of doing this is not intuitive to me. </p>

<p>Here is my input sentence list:</p>

<pre><code>sentence_List= {""Obviously one of the most important features of any computer is the human interface."", ""Good for everyday computing and web browsing."",
""My problem was with DELL Customer Service"", ""I play a lot of casual games online[comma] and the touchpad is very responsive""}
</code></pre>

<p>here is the tokenized sentence:</p>

<pre><code>from nltk.tokenize import word_tokenize
tokenized_sents = [word_tokenize(i) for i in sentence_List]
tokenized=[i for i in tokenized_sents]
</code></pre>

<p>Here I used <code>Spacy</code> to get POS tag of words:</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_sm')

res=[]
for i in range(len(sentence_list.index)):
    for token in i:
        res.append(token.pos_)
</code></pre>

<p>so I may use <code>NER</code> (a.k.a, name entity relation) from <code>spacy</code> but its output is not the same thing with my pre-defined expected target phrase. Does anyone know how to accomplish this task either using <code>Spacy</code> or <code>stanfordcorenlp</code> module in python? what is an efficient solution to make this happen? Any idea? Thanks in advance :)</p>

<p><strong>desired output</strong>:</p>

<p>I want to get the list of target phrase from respective sentence list as follow:</p>

<pre><code>target_phraseList={""human interface"",""everyday computing"",""DELL Customer Service"",""touchpad""}
</code></pre>

<p>so I concatenate my input <code>sentence_list</code> with an expected target phrase, my final desired output would be like this: </p>

<pre><code>import pandas as pd
df=pd.Series(sentence_List, target_phraseList)
df=pd.DataFrame(df)
</code></pre>

<p>How can I get my expected target phrases from a given input sentence list by using <code>spacy</code>? Any idea?</p>
",Named Entity Recognition (NER),find expected target phrase keywords given sentence python wondering efficient way extract expected target phrase key phrase given sentence far tokenized given sentence get po tag word sure extract target key phrase keyword given sentence way intuitive input sentence list tokenized sentence used get po tag word may use k name entity relation output thing pre defined expected target phrase doe anyone know accomplish task either using module python efficient solution make happen idea thanks advance desired output want get list target phrase respective sentence list follow concatenate input expected target phrase final desired output would like get expected target phrase given input sentence list using idea
Extracting the person names in the named entity recognition in NLP using Python,"<p>I have a sentence for which i need to identify the Person names alone:</p>

<p>For example:</p>

<pre><code>sentence = ""Larry Page is an American business magnate and computer scientist who is the co-founder of Google, alongside Sergey Brin""
</code></pre>

<p>I have used the below code to identify the NERs.</p>

<pre><code>from nltk import word_tokenize, pos_tag, ne_chunk
print(ne_chunk(pos_tag(word_tokenize(sentence))))
</code></pre>

<p>The output i received was:</p>

<pre><code>(S
  (PERSON Larry/NNP)
  (ORGANIZATION Page/NNP)
  is/VBZ
  an/DT
  (GPE American/JJ)
  business/NN
  magnate/NN
  and/CC
  computer/NN
  scientist/NN
  who/WP
  is/VBZ
  the/DT
  co-founder/NN
  of/IN
  (GPE Google/NNP)
  ,/,
  alongside/RB
  (PERSON Sergey/NNP Brin/NNP))
</code></pre>

<p>I want to extract all the person names, such as</p>

<pre><code>Larry Page
Sergey Brin
</code></pre>

<p>In order to achieve this, I refereed this <a href=""https://stackoverflow.com/questions/30664677/extract-list-of-persons-and-organizations-using-stanford-ner-tagger-in-nltk"">link</a> and tried this. </p>

<pre><code>from nltk.tag.stanford import StanfordNERTagger
st = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz','/usr/share/stanford-ner/stanford-ner.jar')
</code></pre>

<p>However i continue to get this error: </p>

<pre><code>LookupError: Could not find stanford-ner.jar jar file at /usr/share/stanford-ner/stanford-ner.jar
</code></pre>

<p>Where can i download this file?</p>

<p>As informed above, the result that i am expecting in the form of list or dictionary is :</p>

<pre><code>Larry Page
Sergey Brin
</code></pre>
",Named Entity Recognition (NER),extracting person name named entity recognition nlp using python sentence need identify person name alone example used code identify ners output received wa want extract person name order achieve refereed href tried p however continue get error download file informed result expecting form list dictionary
How to recognize Named Entity from a python list using Stanford NERTagger,"<p>I am a beginner in NLP and first time using StanfordNERTagger. For learning purpose I am playing with Stanford NERTagger. I have a python list of country name </p>

<pre><code>['France', 'India', 'Bangladesh', 'England', 'Germany', 'Brazil', 'Egypt', 'Bhutan', 'Srilanka']
</code></pre>

<p>I want to get 'location' entity which belongs to  NERTagger but i am getting the 'Organization'  Entity</p>

<p>[('France', 'ORGANIZATION'),
 ('India', 'ORGANIZATION'),
 ('Bangladesh', 'ORGANIZATION'),
 ('England', 'ORGANIZATION'),
 ('Germany', 'ORGANIZATION'),
 ('Brazil', 'ORGANIZATION'),
 ('Egypt', 'ORGANIZATION'),
 ('Bhutan', 'ORGANIZATION'),
 ('Srilanka', 'ORGANIZATION')]</p>

<p>May be i am missing something here</p>
",Named Entity Recognition (NER),recognize named entity python list using stanford nertagger beginner nlp first time using stanfordnertagger learning purpose playing stanford nertagger python list country name want get location entity belongs nertagger getting organization entity france organization india organization bangladesh organization england organization germany organization brazil organization egypt organization bhutan organization srilanka organization may missing something
Python Spacy&#39;s Lemmatizer: getting all options for lemmas with maximum efficiency,"<p>When using spacy, the lemma of a token (lemma_) depends on the POS. Therefore, a specific string can have more than one lemmas. For example:</p>

<pre><code>import spacy
nlp = spacy.load('en')
for tok in nlp(u'He leaves early'):
    if tok.text == 'leaves':
        print (tok, tok.lemma_)
for tok in nlp(u'These are green leaves'):
    if tok.text == 'leaves':
        print (tok, tok.lemma_) 
</code></pre>

<p>Will yield that the lemma for 'leaves' can be either 'leave' or 'leaf', depending on context. I'm interested in:</p>

<p>1) Get all possible lemmas for a specific string, regardless of context. Meaning, applying the Lemmatizer without depending on the POS or exceptions, just get all feasible options.</p>

<p>In addition, but independently, I would also like to apply tokenization and get the ""correct"" lemma.</p>

<p>2) Running over a large corpus only tokenization and lemmatizer, as efficiently as possible, without damaging the lemmatizer at all. I know that I can drop the 'ner' pipeline for example, and shouldn't drop the 'tagger', but didn't receive a straightforward answer regarding parser etc. From a simulation over a corpus, it seems like results yielded the same, but I thought that the 'parser' or 'sentenzicer' should affect? My current code at the moment is:</p>

<pre><code>import multiprocessing
our_num_threads = multiprocessing.cpu_count()
corpus = [u'this is a text', u'this is another text'] ## just an example
nlp = spacy.load('en', disable = ['ner', 'textcat', 'similarity', 'merge_noun_chunks', 'merge_entities', 'tensorizer', 'parser', 'sbd', 'sentencizer'])
nlp.pipe(corpus, n_threads = our_num_threads)
</code></pre>

<p>If I have a good answer on 1+2, I can then for my needs use for words that were ""lemmatized"", consider other possible variations.</p>

<p>Thanks!</p>
",Named Entity Recognition (NER),python spacy lemmatizer getting option lemma maximum efficiency using spacy lemma token lemma depends po therefore specific string one lemma example yield lemma leaf either leave leaf depending context interested get possible lemma specific string regardless context meaning applying lemmatizer without depending po exception get feasible option addition independently would also like apply tokenization get correct lemma running large corpus tokenization lemmatizer efficiently possible without damaging lemmatizer know drop ner pipeline example drop tagger receive straightforward answer regarding parser etc simulation corpus seems like result yielded thought parser sentenzicer affect current code moment good answer need use word lemmatized consider possible variation thanks
NLTK for Named Entity Recognition,"<p>I am trying to use NLTK toolkit to get extract place, date and time from text messages. I just installed the toolkit on my machine and I wrote this quick snippet to test it out:</p>

<pre><code>sentence = ""Let's meet tomorrow at 9 pm"";
tokens = nltk.word_tokenize(sentence)
pos_tags = nltk.pos_tag(tokens)
print nltk.ne_chunk(pos_tags, binary=True)
</code></pre>

<p>I was assuming that it will identify the date (tomorrow) and time (9 pm). But, surprisingly it failed to recognize that. I get the following result when I run my above code:</p>

<pre><code>(S (GPE Let/NNP) 's/POS meet/NN tomorrow/NN at/IN 9/CD pm/NN)
</code></pre>

<p>Can someone help me understand if I am missing something or NLTK is just not mature enough to tag time and date properly. Thanks!</p>
",Named Entity Recognition (NER),nltk named entity recognition trying use nltk toolkit get extract place date time text message installed toolkit machine wrote quick snippet test wa assuming identify date tomorrow time pm surprisingly failed recognize get following result run code someone help understand missing something nltk mature enough tag time date properly thanks
What are the preprocessing steps to be taken before passing text into Stanford NER tagger?,"<p>Initially I had followed preprocessing steps like, stop words removal, HTML stripping, removing punctuation. However when I don't do this, the NER seems to perform better. Can anyone tell me what are preprocessing steps to be followed?</p>
",Named Entity Recognition (NER),preprocessing step taken passing text stanford ner tagger initially followed preprocessing step like stop word removal html stripping removing punctuation however ner seems perform better anyone tell preprocessing step followed
Extract Information/cleaning data from CSV file using Python,"<p>I have a dataset provided properties.csv (4000 rows and 6 columns). The csv file including many features some of these features are numerical and some of them are nominal (features contain text). Suppose the features in this dataset are</p>

<pre><code>id
F1 
F2
F3
F4 
Price 
</code></pre>

<p>Examples of the content of each feature:</p>

<p>id (row 1 to 3 in CSV File) ---> 44525
                                 44859
                                 45465</p>

<p>F1 (row 1 to 3 in CSV File) ---> ""Stunning 6 bedroom villa in the heart of the 
                                  Golden Mile, Marbella"" 
                                 ""Villa for sale in Rocio de NagÃ¼eles, Marbella 
                                  Golden Mile""
                                 ""One level 5 bedroom villa for sale in 
                                  NagÃ¼eles""</p>

<p>F2 (row 1 to 3 in CSV File) --->  ""Fireplace, Elevator, Terrace, Mountain view, 
                                   Freight Elevator, Air conditioning, Patio, 
                                   Guest toilet, Garden, Balcony, Sea/lake view, 
                                   Built-in kitchen""
                                   ""Mountain view""
                                   ""Elevator, Terrace, Alarm system, Mountain 
                                    view, Swimming pool, Air conditioning, 
                                    Basement, Sea/lake view""</p>

<p>F3 (row 1 to 3 in CSV File) - contains numerical values --->  0
                                                              0
                                                              0</p>

<p>F4 (row 1 to 3 in CSV File) - contains numerical values ---> 393
                                                             640
                                                             4903
F3 (row 1 to 3 in CSV File) - contains numerical values ---> 4400000
                                                             2400000
                                                             1900000</p>

<p>In F1, I am looking to do the following:</p>

<p>1- Extract the type of the properties (apartment’, ‘house’ or ‘Villa’) and put it in a separate feature (independent variable) calls ""Type"" in CSV file. After that, I want to separate them in groups (apartments group, houses group, Vilas group) with calculating the mean price of each type group.
2- Extract the location of each property (locations can be: Alenquer, Quinta da Marinha, Golden Mile, Nagüeles) and put it in a separate feature (independent variable) calls ""Location"" in csv file. </p>

<p>I am a beginner in NLP. I tried to write this code to extract information ""Apartment"" from F1, but it does not work probably:</p>

<pre><code>import pandas as pd
from pandas import DataFrame
import re

properties = pd.read_csv (r'C:/Users/User/Desktop/properties.csv')
Extract ""Apartment"" from F1
Title= DataFrame(properties,columns= ['F1'])

for line in F1: 
        #return list of apartments in that line
        x = re.findall(""\apartment"", line) 
        #if a date is found
         if len(x) != 0:
         print(x)
</code></pre>

<p>I need your help to fix this code and what should I do to extract the other information ‘houses’ and ‘Villa’ from F1.</p>

<p>After that, Create a property dataset in this format and save it as a csv file:</p>

<pre><code>id
Location (Information extracted from F1)
type (information extracted from F1 in groups ""apartments’, ‘houses’, ‘Villas’"")
F1
F2
F3
F4
Price
</code></pre>

<p>In case, F1 does not contain the type of some properties ""Blank field (no text)"", what should I do to deal with the blanks fields (no text) in F1 and extract the type of the properties from other properties?   </p>
",Named Entity Recognition (NER),extract information cleaning data csv file using python dataset provided property csv row column csv file including many feature feature numerical nominal feature contain text suppose feature dataset example content feature id row csv file f row csv file stunning bedroom villa heart golden mile marbella villa sale rocio de nag eles marbella golden mile one level bedroom villa sale nag eles f row csv file fireplace elevator terrace mountain view freight elevator air conditioning patio guest toilet garden balcony sea lake view built kitchen mountain view elevator terrace alarm system mountain view swimming pool air conditioning basement sea lake view f row csv file contains numerical value f row csv file contains numerical value f row csv file contains numerical value f looking following extract type property apartment house villa put separate feature independent variable call type csv file want separate group apartment group house group vila group calculating mean price type group extract location property location alenquer quinta da marinha golden mile nag eles put separate feature independent variable call location csv file beginner nlp tried write code extract information apartment f doe work probably need help fix code extract information house villa f create property dataset format save csv file case f doe contain type property blank field text deal blank field text f extract type property property
Extracting Specific Information from Scientific Papers,"<p>I am looking for specific information that I need to extract from scientific papers. The information mostly resides in the ""Evaluation"" or ""Implementation"" sections of the papers. I need to extract any function name, parameter, file name, application name, application version in the content.
Is there any NLP technique/machine learning algorithm to do this type of information extraction from scientific papers?</p>
",Named Entity Recognition (NER),extracting specific information scientific paper looking specific information need extract scientific paper information mostly resides evaluation implementation section paper need extract function name parameter file name application name application version content nlp technique machine learning algorithm type information extraction scientific paper
Spacy identifying blank spaces as entities,"<p>I am just starting to work with Spacy and have put a text through to test how it is working on a pdf I OCR'd with AntFileConverter.</p>

<p>The txt file (sample below - would attach but unsure how) seems fine, is in UTF-8. However when I output the file in CONLL format, for some reason there are various apparent gaps, which have no original word, but seem to have been identified. This happens both at the end and within sentences. </p>

<p>""species in many waters in the northern hemisphere. In
most countries in the region pike has both commercial
and recreational value (Crossman &amp; Casselman 1987;
Raat 1988). Pike is a typical sit-and-wait predator
which usually hunts prey by ambushing (Webb &amp;
Skadsen 1980).""</p>

<p>The output us as so:</p>

<pre><code>        GPE 24  
26  species specie  NNS     20  attr
27  in  in  IN      26  prep
28  many    many    JJ      29  amod
29  waters  water   NNS     27  pobj
30  in  in  IN      29  prep
31  the the DT      33  det
32  northern    northern    JJ      33  amod
33  hemisphere  hemisphere  NN      30  pobj
34  .   .   .       20  punct
1   In  in  IN      9   prep
2   

        GPE 1   
3   most    most    JJS     4   amod
4   countries   country NNS     9   nsubj
5   in  in  IN      4   prep
6   the the DT      8   det
7   region  region  NN      8   compound
8   pike    pike    NN      5   pobj
9   has have    VBZ     0   ROOT
10  both    both    DT      11  preconj
11  commercial  commercial  JJ      9   dobj
12  

        GPE 11  
13  and and CC      11  cc
14  recreational    recreational    JJ      15  amod
15  value   value   NN      11  conj
16  (   (   -LRB-       15  punct
17  Crossman    crossman    NNP ORG 15  appos
18  &amp;   &amp;   CC  ORG 17  cc
19  Casselman   casselman   NNP ORG 17  conj
20  1987    1987    CD  DATE    17  nummod
21  ;   ;   :       15  punct
22  

        GPE 21  
23  Raat    raat    NNP     15  appos
24  1988    1988    CD  DATE    23  nummod
25  )   )   -RRB-       15  punct
26  .   .   .       9   punct
1   Pike    pike    NNP     2   nsubj
2   is  be  VBZ     0   ROOT
3   a   a   DT      10  det
4   typical typical JJ      10  amod
5   sit sit NN      10  nmod
6   -   -   HYPH        5   punct
7   and and CC      5   cc
8   -   -   HYPH        9   punct
9   wait    wait    VB      5   conj
10  predator    predator    NN      2   attr
11  

        GPE 10  
12  which   which   WDT     14  nsubj
13  usually usually RB      14  advmod
14  hunts   hunt    VBZ     10  relcl
15  prey    prey    NN      14  dobj
16  by  by  IN      14  prep
17  ambushing   ambush  VBG     16  pcomp
18  (   (   -LRB-       17  punct
19  Webb    webb    NNP     17  conj
20  &amp;   &amp;   CC      19  cc
21  
</code></pre>

<p>I also tried without the NER print out but  these gaps continue to be marked. I thought it might be related to the line breaks, so I also tried with a Linux-style EOL but that didn't make any difference.</p>

<p>This is the code I am using:</p>

<pre><code>import spacy
import en_core_web_sm
nlp_en = en_core_web_sm.load()
input = open('./input/55_linux.txt', 'r').read()
doc = nlp_en(input)
for sent in doc.sents:
        for i, word in enumerate(sent):
              if word.head == word:
                 head_idx = 0
              else:
                 head_idx = word.head.i - sent[0].i + 1
              output = open('CONLL_output.txt', 'a')
              output.write(""%d\t%s\t%s\t%s\t%s\t%s\t%s\n""%(
                 i+1, # There's a word.i attr that's position in *doc*
                  word,
                  word.lemma_,
                  word.tag_, # Fine-grained tag
                  word.ent_type_,
                  str(head_idx),
                  word.dep_ # Relation
                 ))
</code></pre>

<p>Has anyone else had this problem? If so, do you know how I can solve it? </p>
",Named Entity Recognition (NER),spacy identifying blank space entity starting work spacy put text test working pdf ocr antfileconverter txt file sample would attach unsure seems fine utf however output file conll format reason various apparent gap original word seem identified happens end within sentence specie many water northern hemisphere country region pike ha commercial recreational value crossman casselman raat pike typical sit wait predator usually hunt prey ambushing webb skadsen output u also tried without ner print gap continue marked thought might related line break also tried linux style eol make difference code using ha anyone else problem know solve
Write a regular expression based named entity recognition module (NER),"<p>I am new to Python and just started using it with NLP, I need to write a regular expression based named entity recognition module.
Anyone provide me with helpful links or examples will be appreciated. </p>
",Named Entity Recognition (NER),write regular expression based named entity recognition module ner new python started using nlp need write regular expression based named entity recognition module anyone provide helpful link example appreciated
Difference between Stanford CoreNLP and Stanford NER,"<p>What is the difference between using CoreNLP (<a href=""https://stanfordnlp.github.io/CoreNLP/ner.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/ner.html</a>) and the standalone distribution Stanford NER (<a href=""https://nlp.stanford.edu/software/CRF-NER.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/CRF-NER.html</a>) for doing Named Entity Recognition? I noticed that the standalone distribution comes with a GUI, but are there any other differences in terms of supported functionality? </p>

<p>I'm trying to decide which one to use for a commercial purpose. I'm working on English models only.</p>
",Named Entity Recognition (NER),difference stanford corenlp stanford ner difference using corenlp standalone distribution stanford ner named entity recognition noticed standalone distribution come gui difference term supported functionality trying decide one use commercial purpose working english model
How to incorporate metadata into NLTK corpus for efficient processing,"<p>I have a folder of txt files and also a csv file with additional data like categories a particular txt document belongs to and the original source file (pdf) path. The Txt file name is used as key into the CSV file.</p>

<p>I have created a basic nltk corpus but I would like to know if that's the best way of structuring my data given I want to carry out a range of NLP tasks like NER on the corpus and eventually identify the entities which occur in each category and be able to link back to the source pdf files so each entity can be seen in context.</p>

<p>Most NLP examples (find NERs) go from corpus to python lists of entities but doesn't that mean I will loose the association back to the txt file which contained the entities and all the other associated data?</p>

<p>Categorical corpus appears to help with keeping the category data but my question is </p>

<p>What is the best way to structure and work with my corpus that avoids having to roundtrip between
- process corpus to identify interesting information outputted to list
-  search corpus again to get files which contains the interested element from the list
- search CSV (data frame) by file id to get the rest of the metadata</p>
",Named Entity Recognition (NER),incorporate metadata nltk corpus efficient processing folder txt file also csv file additional data like category particular txt document belongs original source file pdf path txt file name used key csv file created basic nltk corpus would like know best way structuring data given want carry range nlp task like ner corpus eventually identify entity occur category able link back source pdf file entity seen context nlp example find ners go corpus python list entity mean loose association back txt file contained entity associated data categorical corpus appears help keeping category data question best way structure work corpus avoids roundtrip process corpus identify interesting information outputted list search corpus get file contains interested element list search csv data frame file id get rest metadata
How can I extract scene/place words in a sentence by using python packages like NLTK or stanfordcorenlp?,"<p>Given sentences like:</p>

<pre><code>'the people are all watching and listening to the bikers on the corner of the road'

'woman on snow skis being pulled by dogs.'

(Actually, the sentences I want to process are captions in MSCOCO datatset)
</code></pre>

<p>I want to <strong>extract the scene/place words in the sentence</strong>. For example, 'road', 'snow' here are the scene/place words.</p>

<p>I have tried NER in stanfordcorenlp, but it can only extract the location name, not a scene word.</p>

<p>Could anyone give me a clue about how to extract such words?
Thanks.</p>
",Named Entity Recognition (NER),extract scene place word sentence using python package like nltk stanfordcorenlp given sentence like want extract scene place word sentence example road snow scene place word tried ner stanfordcorenlp extract location name scene word could anyone give clue extract word thanks
How to link NE with it&#39;s dependent?,"<p>I need to extract descriptions of locations from a text. For now, I am trying to get location with it's adjectival modifier.<br>
For example from  </p>

<blockquote>
  <p>In compact Durham you don't need transport to get around.  </p>
</blockquote>

<p>I want to get  </p>

<blockquote>
  <p>compact Durham  </p>
</blockquote>

<p>I have <code>CoreEntityMention</code> and <code>SemanticGraph</code> of my sentence. I can get index of NE's token to find <code>IndexedWord</code> in <code>SemanticGraph</code>, but NE may contain more than one token so I don't know hot to build the link. I saw <a href=""https://stackoverflow.com/questions/40479342/how-to-extract-named-entity-verb-from-text"">this similar question</a>, but didn't understand suggested solution. Do need I to check dependence for each token?<br>
Here is my approach written in Kotlin (no big difference from Java):  </p>

<pre><code>    val dependencies = mutableListOf&lt;String&gt;()
    val depGraph = entityMention.sentence().dependencyParse()

    for (token in entityMention.tokens()) {
        val node = depGraph.getNodeByIndex(token.index())
        for (dependence in depGraph.childPairs(node)) {
            if (dependence.first.shortName == ""amod"") {
                dependencies.add(dependence.second.toString())
            }
        }
    }
</code></pre>

<p>Is it correct and simplest way?</p>
",Named Entity Recognition (NER),link ne dependent need extract description location text trying get location adjectival modifier example compact durham need transport get around want get compact durham sentence get index ne token find ne may contain one token know hot build link saw href similar question understand suggested solution need check dependence token approach written kotlin big difference java correct simplest way
"How to extract the location name, country name, city name, tourist places by using nlp or spacy in python","<p>I am trying to extract the location name, country name, city name, tourist places from txt file by using nlp or scapy library in python. </p>

<p>I have tried below: </p>

<pre><code>import spacy
en = spacy.load('en')

sents = en(open('subtitle.txt').read())
place = [ee for ee in sents.ents]
</code></pre>

<p>Getting output:</p>

<pre><code>[1, 
, three, London, 
, 
, 
, 
, first, 
, 
, 00:00:20,520, 
, 
, London, the

4
00:00:20,520, 00:00:26,130
, Buckingham Palace, 
, 
</code></pre>

<p>I just want only location name, country name, city name and any place within city. </p>

<p>I also tried by using NLP:</p>

<pre><code>import nltk
nltk.download('maxent_ne_chunker')
nltk.download('words')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

with open('subtitle.txt', 'r') as f:
    sample = f.read()


sentences = nltk.sent_tokenize(sample)
tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]
tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]
chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)

def extract_entity_names(t):
    entity_names = []

    if hasattr(t, 'label') and t.label:
        if t.label() == 'NE':
            entity_names.append(' '.join([child[0] for child in t]))
        else:
            for child in t:
                entity_names.extend(extract_entity_names(child))

    return entity_names

entity_names = []
for tree in chunked_sentences:
    # Print results per sentence
    #print (extract_entity_names(tree))

    entity_names.extend(extract_entity_names(tree))

# Print all entity names
#print (entity_names)

# Print unique entity names
print (set(entity_names))
</code></pre>

<p>Output Getting: </p>

<pre><code>{'Okay', 'Buckingham Palace', 'Darwin Brasserie', 'PDF', 'London', 'Local Guide', 'Big Ben'}
</code></pre>

<p>Here, also getting unwanted words like 'Okay', 'PDF', 'Local Guide' and some places are missing. </p>

<p>Please suggest.</p>

<p><strong>Edit-1</strong></p>

<p><strong>Script</strong></p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_lg')

gpe = [] # countries, cities, states
loc = [] # non gpe locations, mountain ranges, bodies of water


doc = nlp(open('subtitle.txt').read())
for ent in doc.ents:
    if (ent.label_ == 'GPE'):
        gpe.append(ent.text)
    elif (ent.label_ == 'LOC'):
        loc.append(ent.text)

cities = []
countries = []
other_places = []
import wikipedia
for text in gpe:
    summary = str(wikipedia.summary(text))
    if ('city' in summary):
        cities.append(text)
        print (cities)
    elif ('country' in summary):
        countries.append(text)
        print (countries)
    else:
        other_places.append(text)
        print (other_places)

for text in loc:
    other_places.append(text)
    print (other_places)
</code></pre>

<p>By using answered script: getting below output</p>

<pre><code>['London', 'London']
['London', 'London', 'London']
['London', 'London', 'London', 'London']
['London', 'London', 'London', 'London', 'London']
['London', 'London', 'London', 'London', 'London', 'London']
['London', 'London', 'London', 'London', 'London', 'London', 'London']
['London', 'London', 'London', 'London', 'London', 'London', 'London', 'London']
['London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London']
['London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London']
['London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London']
['London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London']
</code></pre>
",Named Entity Recognition (NER),extract location name country name city name tourist place using nlp spacy python trying extract location name country name city name tourist place txt file using nlp scapy library python tried getting output want location name country name city name place within city also tried using nlp output getting also getting unwanted word like okay pdf local guide place missing please suggest edit script using answered script getting output
Extracting italic text from a document,"<p>I have a word document with a list of species names and then various text about each species. I'd like to just extract all the species names. The obvious way to do this is to just extract all text in italics. However, I can't find a way to do this in python, does anyone have any ideas?</p>

<p>E.g. input: <em>Acanthognathus
rudis</em>
Small prey Solitary – 1 ? 1 ? Recruitment: Solitary, frequently catch collembola and other
small prey (GRONENBERG &amp; al. 1998).
Size: Small, can be retrieved by one <em>Acromyrmex
coronatus</em></p>

<p>ouput: Acanthognathus
rudis, Acromyrmex
coronatus</p>
",Named Entity Recognition (NER),extracting italic text document word document list specie name various text specie like extract specie name obvious way extract text italic however find way python doe anyone idea e g input acanthognathus rudis small prey solitary recruitment solitary frequently catch collembola small prey gronenberg al size small retrieved one acromyrmex coronatus ouput acanthognathus rudis acromyrmex coronatus
Methods to extract keywords from large documents that are relevant to a set of predefined guidelines using NLP/ Semantic Similarity,"<p>I'm in need of suggestions how to extract keywords from a large document. The keywords should be inline what we have defined as the intended search results. </p>

<p>For example, </p>

<p>I need the owner's name, where the office is situated, what the operating industry is when a document about a company is given, and the defined set of words would be, </p>

<blockquote>
  <p>{owner, director, office, industry...}-(1)</p>
</blockquote>

<p>the intended output has to be something like, </p>

<blockquote>
  <p>{Mr.Smith James, ,Main Street, Financial Banking}-(2)</p>
</blockquote>

<p>I was looking for a method related to Semantic Similarity where sentences containing words similar to the given corpus (1), would be extracted, and using POS tagging to extract nouns from those sentences.</p>

<p>It would be a useful if further resources could be provided that support this approach.  </p>
",Named Entity Recognition (NER),method extract keywords large document relevant set predefined guideline using nlp semantic similarity need suggestion extract keywords large document keywords inline defined intended search result example need owner name office situated operating industry document company given defined set word would owner director office industry intended output ha something like mr smith james main street financial banking wa looking method related semantic similarity sentence containing word similar given corpus would extracted using po tagging extract noun sentence would useful resource could provided support approach
Better named-entity recognition and similarity using spaCy,"<p>I've been trying out spaCy for a small side-project, and had a few questions &amp; concerns.</p>

<p>I noticed that spaCy's named-entity recognition results (with its largest <code>en_vectors_web_lg</code> model) don't seem to be as accurate as that of Google Cloud Natural Language API [1]. Google's API is able to extract more entities, more accurately, most likely because their model is even larger. So, is there a way to improve spaCy's NER results using a different model if possible, or through some other technique?</p>

<p>Secondly, Google's API also returns Wikipedia article links for relevant entities. Is this possible with spaCy too, or using some other technique on top of spaCy's NER results?</p>

<p>Thirdly, I noticed that spaCy has a <code>similarity()</code> method [2] that uses GloVe word vectors. But being new to it, I'm not sure what's the best way to frequently perform similarity comparison between each document in a set of documents (say 5000-10000 text documents of under 500 characters each) to generate buckets of similar documents?</p>

<p>Hoping for someone to have any suggestions or tips.</p>

<p>Many thanks!</p>

<hr>

<p>[1] <a href=""https://cloud.google.com/natural-language/"" rel=""noreferrer"">https://cloud.google.com/natural-language/</a></p>

<p>[2] <a href=""https://spacy.io/usage/vectors-similarity"" rel=""noreferrer"">https://spacy.io/usage/vectors-similarity</a></p>
",Named Entity Recognition (NER),better named entity recognition similarity using spacy trying spacy small side project question concern noticed spacy named entity recognition result largest model seem accurate google cloud natural language api google api able extract entity accurately likely model even larger way improve spacy ner result using different model possible technique secondly google api also return wikipedia article link relevant entity possible spacy using technique top spacy ner result thirdly noticed spacy ha method us glove word vector new sure best way frequently perform similarity comparison document set document say text document character generate bucket similar document hoping someone suggestion tip many thanks
Tree representation of Entity-Relationship in Python using NLTK,"<p>My aim is to extract entity relationship from a text file. I don't know much how the output should be. I took the input (aaa.txt):</p>

<blockquote>
  <p>My school situated in Kolkata. The color of the sky is pink. I wonder why?</p>
</blockquote>

<p>and got the output:</p>

<pre><code>(S
  My/PRON
  (NP school/NOUN)
  situated/VERB
  in/ADP
  (NP Kolkata/NOUN)
  ./.
  (NP The/DET color/NOUN)
  of/ADP
  (NP the/DET sky/NOUN)
  is/VERB
  (NP pink/NOUN)
  ./.
  I/PRON
  wonder/VERB
  why/ADV
  ?/.)
</code></pre>

<p>Is it completely correct? These kinds of problems are not very clear to me, as I am a beginner in both Python and NLTK, and could not find any good clarification from Google. Can the output be presented in a better format (Tree, Tabular etc.) ?</p>
",Named Entity Recognition (NER),tree representation entity relationship python using nltk aim extract entity relationship text file know much output took input aaa txt school situated kolkata color sky pink wonder got output completely correct kind problem clear beginner python nltk could find good clarification google output presented better format tree tabular etc
Keyword-Search with synonyms in information retrieval systems,"<p>I have development a chatbot to give right answer for user input. Right now, I struggle how to read the DB/Knowledge Base or just the json file properly to extract the right answer. In my use case I have very much Keyword names/entities together with synonyms. So it would be a bad idea to write the synonyms in the NLU training file manually. My database file has first listed the keyword and afterwards the answers are separated for each intent belonging to this entity. How do you handle in practice such keyword search together with slightly different keywords/synonyms given by the user?</p>

<p>I am interested in standard approach to this and in future I would like to apply NLP maybe like word emebedding for my custom case.</p>
",Named Entity Recognition (NER),keyword search synonym information retrieval system development chatbot give right answer user input right struggle read db knowledge base json file properly extract right answer use case much keyword name entity together synonym would bad idea write synonym nlu training file manually database file ha first listed keyword afterwards answer separated intent belonging entity handle practice keyword search together slightly different keywords synonym given user interested standard approach future would like apply nlp maybe like word emebedding custom case
Why do CoreNLP ner tagger and ner tagger join the separated numbers together?,"<p>Here is the code snippet:   </p>

<pre><code>In [390]: t
Out[390]: ['my', 'phone', 'number', 'is', '1111', '1111', '1111']

In [391]: ner_tagger.tag(t)
Out[391]: 
[('my', 'O'),
 ('phone', 'O'),
 ('number', 'O'),
 ('is', 'O'),
 ('1111\xa01111\xa01111', 'NUMBER')]
</code></pre>

<p>What I expect is: </p>

<pre><code>Out[391]: 
[('my', 'O'),
 ('phone', 'O'),
 ('number', 'O'),
 ('is', 'O'),
 ('1111', 'NUMBER'),
 ('1111', 'NUMBER'),
 ('1111', 'NUMBER')]
</code></pre>

<p>As you can see the artificial phone number is joined by \xa0 which is said to be a non-breaking space. Can I separate that by setting the CoreNLP without changing other default rules. </p>

<p>The ner_tagger is defined as: </p>

<pre><code>ner_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
</code></pre>
",Named Entity Recognition (NER),corenlp ner tagger ner tagger join separated number together code snippet expect see artificial phone number joined xa said non breaking space separate setting corenlp without changing default rule ner tagger defined
How to find city names and person names from unstructured data in python,"<p>I am trying to find the city names and person names from another unstructured data files. We have many text files.</p>

<p>How to find such string in using pandas or python</p>

<p>for example:</p>

<p>I have to find a string <code>Ram</code> and <code>Mumbai</code> from another unstructured data file. </p>
",Named Entity Recognition (NER),find city name person name unstructured data python trying find city name person name another unstructured data file many text file find string using panda python example find string another unstructured data file
How to create a good NER training model in OpenNLP?,"<p>I just have started with OpenNLP. I need to create a simple training model to recognize name entities. </p>

<p>Reading the doc here <a href=""https://opennlp.apache.org/docs/1.8.0/apidocs/opennlp-tools/opennlp/tools/namefind"" rel=""nofollow noreferrer"">https://opennlp.apache.org/docs/1.8.0/apidocs/opennlp-tools/opennlp/tools/namefind</a> I see this simple text to train the model:</p>

<pre><code>&lt;START:person&gt; Pierre Vinken &lt;END&gt; , 61 years old , will join the board as a nonexecutive director Nov. 29 .
Mr . &lt;START:person&gt; Vinken &lt;END&gt; is chairman of Elsevier N.V. , the Dutch publishing group .
&lt;START:person&gt; Rudolph Agnew &lt;END&gt; , 55 years old and former chairman of Consolidated Gold Fields PLC ,
    was named a director of this British industrial conglomerate .
</code></pre>

<p>The questions are two:</p>

<ul>
<li><p>Why should i have to put the names of the persons in a text (phrase) context ? Why not write person's name one for each line? like:</p>

<pre><code>&lt;START:person&gt; Robert &lt;END&gt;

&lt;START:person&gt; Maria &lt;END&gt;

&lt;START:person&gt; John &lt;END&gt;
</code></pre></li>
<li><p>How can I also add extra information to that name?
For example I would like to save the information Male/Female for each name.</p></li>
</ul>

<p>(I know there are systems that try to understand it reading the last letter, like the ""a"" for <strong>Female</strong> etc but i would like to add it myself)</p>

<p>Thanks.</p>
",Named Entity Recognition (NER),create good ner training model opennlp started opennlp need create simple training model recognize name entity reading doc see simple text train model question two put name person text phrase context write person name one line like also add extra information name example would like save information male female name know system try understand reading last letter like female etc would like add thanks
SpaCy doesn&#39;t recognize money and countries as expected,"<p>I'm working with SpaCy in order to extract the different named entities from a sample text. The problem is that SpaCy doesn't recognize all the expected entities. It has problems with money and some locations. This is my code:</p>

<pre><code># encoding: utf-8
import spacy
from spacy import displacy

nlp = spacy.load('es') #cargo el modelo en español.
text = u""Una vez un personaje le preguntó a Agustín Chirichigno si estaba en su casa. El nombre de este personaje es Lucas Picchi y es de Mar del Plata. Junto a SU PRIMO, DE ESTADOS UNIDOS hacen cosas re locas como por ejemplo comprar un Fernet Branca a AR$2.500 cuando en realidad está a $180.""
doc = nlp(text)
displacy.serve(doc, style='ent')
</code></pre>

<p>Note that I'm loading a spanish model. My configuration is as follows:
spacy info</p>

<pre><code>Info about spaCy

Python version     2.7.6          
Platform           Linux-4.4.0-112-generic-x86_64-with-Ubuntu-14.04-trusty
spaCy version      2.0.9          
Location           /usr/local/lib/python2.7/dist-packages/spacy
Models             es, en
</code></pre>

<p>So, the output is:
<a href=""https://i.sstatic.net/POi9y.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/POi9y.png"" alt=""enter image description here""></a></p>

<p>What is expected is that SpaCy recognize <code>MONEY</code> correctly (AR$2500 is 2500 argentinian pesos and $180 is money too). The word ""Junto"" is not a <code>LOCATION</code> and neither ""SU PRIMO"". ""Junto a su primo"" would be like ""with his cousin"" in english. Also, Fernet Branca is a <code>BRAND</code>, and not a person.</p>

<p>So, my question is: what am I doing wrong? Should I use additional libraries?</p>

<p>Thanks in advance.</p>
",Named Entity Recognition (NER),spacy recognize money country expected working spacy order extract different named entity sample text problem spacy recognize expected entity ha problem money location code note loading spanish model configuration follows spacy info output expected spacy recognize correctly ar argentinian peso money word junto neither su primo junto su primo would like cousin english also fernet branca person question wrong use additional library thanks advance
Spacy multiple NER tags for a single word,"<p>I successfully trained Spacy to recognize some custom named entities.</p>

<p>Let's say that the model successfully recognizes companies (ORG) and fruits (FRUIT).</p>

<p>I would like to get the probability for a word to be each of this entities. Something like:</p>

<pre><code>doc = nlp(""Apple to ship highest number of new iPhones this fall"")

print(getProbabilities(doc))
// [
//   (0,0, 'Apple', [ ['ORG', 0.99], ['FRUIT', 0.01] ])
// ]

doc = nlp(""Apple picking machine provides potential future of agriculture"")

print(getProbabilities(doc))
// [
//   (0,0, 'Apple', [ ['FRUIT', 0.99], ['ORG', 0.01] ])
// ]
</code></pre>

<p>Using the technique described <a href=""https://github.com/explosion/spaCy/issues/881"" rel=""nofollow noreferrer"">here</a> I can get the score of a particular named entity, but only the most likely one, not the others.</p>
",Named Entity Recognition (NER),spacy multiple ner tag single word successfully trained spacy recognize custom named entity let say model successfully recognizes company org fruit fruit would like get probability word entity something like using technique described get score particular named entity likely one others
Stanford&#39;s coreNLP Name Entity Recogniser throwing error 500 Server Error: Internal Server Error for url,"<p>I have a set of text files. I am using Stanford's coreNLP Name Entity Recogniser to extract details of the lines where patient name is mentioned out of those files. When I am running NER on a single sentence, it is printing results correctly but when I am running it on set of files, it is printing the results along with error and also I am not able to write the results on a text file because of this:</p>

<pre><code>500 Server Error: Internal Server Error for url: http://localhost:9000/?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cssplit%2Cner%22%2C+%22ssplit.isOneSentence%22%3A+%22true%22%7D
</code></pre>

<p>Here is the code which I am using:</p>

<pre><code>import re
import os
from nltk.parse import CoreNLPParser
tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')

def name_detail_extracter():    

    data_location=""D:\Data"" # folder containing all the data
    for root, dirs, files in os.walk(data_location):
    for filename in files:
        with open(os.path.join(root, filename), encoding=""utf8"",mode=""r"") as f:
            patient_name_check=re.compile(r"".*\s+(patient name)\s*:*\s*(.*)"",re.I)                
            for line_number, line in enumerate(f, 1):

                patient_name_matches=patient_name_check.findall(line)
                for match in patient_name_matches:

                    name_details=match[1]
                    tokens = name_details.split()
                    result=tagger.tag(tokens)
                    for m in result:
                        print(m)

name_detail_extracter()
</code></pre>
",Named Entity Recognition (NER),stanford corenlp name entity recogniser throwing error server error internal server error url set text file using stanford corenlp name entity recogniser extract detail line patient name mentioned file running ner single sentence printing result correctly running set file printing result along error also able write result text file code using
How to annotate and train data for predominantly numeric data extraction,"<p>I am trying to extract information from unstructured text. For example</p>

<blockquote>
  <p>The CEO has recently sought suggestions for features to add to the truck under development and he revealed some planned features, like an option for 400 to 500 miles of range, Dual Motor All-wheel-drive powertrain with dynamic suspension, ‘300,000 lbs of towing capacity’, and more. When asked about the release date, the CEO gave an estimated time of Q2 2021.</p>
</blockquote>

<p>Ideal output would be something like</p>

<pre><code>[minRange = 400, maxRange = 500, allWheelDrive = TRUE, susepnsionType = 'Dynamic', releaseDate = 2021-04-01 00:00]
</code></pre>

<p>Because the data I want to extract won't always be structured in well-behaved pairs (e.g., the releaseDate extraction above requires semantically processing the entire sentence) regex and a few rules probably won't suffice. I believe I need to annotate my datasets and then use NLP and learning tools such as spaCy, NLTK, or Stanford NLP to extract the attribute-value pairs from future examples.</p>

<p>I am trying to use online resources / tutorials but they all seem to be focused on either named entity recognition alone or extracting relations on to string-based entities (e.g., LOC:Paris, REL:is capital of, LOC:France). Most (not all) of the data I need to extract is just attribute-numeric value pair. How do I accomplish this with NLP tools? For the most part I'm just not sure if the numbers should be considered entities, and if so, should they be annotated as an entity named ""Value"" or given an entity name with the associated attribute. For example, which of these two is the better way to annotate?:</p>

<ol>
<li>The &lt;\PRODUCT>vehicle&lt;\PRODUCT> has a towing capacity of &lt;\TOWING_CAP>300,000 lbs&lt;\TOWING_CAP></li>
<li>The &lt;\PRODUCT>vehicle&lt;\PRODUCT> has a towing capacity of &lt;\VALUE>300,000 lbs&lt;\VALUE>, RELATION:""has towing capacity""</li>
</ol>
",Named Entity Recognition (NER),annotate train data predominantly numeric data extraction trying extract information unstructured text example ceo ha recently sought suggestion feature add truck development revealed planned feature like option mile range dual motor wheel drive powertrain dynamic suspension lb towing capacity asked release date ceo gave estimated time q ideal output would something like data want extract always structured well behaved pair e g releasedate extraction requires semantically processing entire sentence regex rule probably suffice believe need annotate datasets use nlp learning tool spacy nltk stanford nlp extract attribute value pair future example trying use online resource tutorial seem focused either named entity recognition alone extracting relation string based entity e g loc paris rel capital loc france data need extract attribute numeric value pair accomplish nlp tool part sure number considered entity annotated entity named value given entity name associated attribute example two better way annotate product vehicle product ha towing capacity towing cap lb towing cap product vehicle product ha towing capacity value lb value relation ha towing capacity
Stanford Name Entity Recognizer(NER) using PyNER not working,"<p>I am trying using Stanford's Name Entity Recognizer(NER). </p>

<p>I downloaded the zip file from : <a href=""https://github.com/dat/pyner"" rel=""nofollow noreferrer"">https://github.com/dat/pyner</a>.</p>

<p>Installed it using: python setup.py install.</p>

<p>Now when I am running the below command, I am getting blank output</p>

<pre><code>import ner
tagger =ner.SocketNER(host='localhost',port=31752,output_format='slashTags')
tagger.get_entities(""University of California is located in California, United States"")

Output:
{}
</code></pre>

<p>am I missing anything?</p>
",Named Entity Recognition (NER),stanford name entity recognizer ner using pyner working trying using stanford name entity recognizer ner downloaded zip file installed using python setup py install running command getting blank output missing anything
Custom entity extraction in R using openNLP,"<p>I am currently using openNLP (openNLPmodels.en) to extract biographical information about different high ranking executives. I can extract dates, organizations, people and locations,  however I want to extract a list of job roles. I have a list of job roles and I was just wondering if I can do a custom entity recognition? Or something of the sort and how I would go about doing such a thing.
Thanks in advance</p>
",Named Entity Recognition (NER),custom entity extraction r using opennlp currently using opennlp opennlpmodels en extract biographical information different high ranking executive extract date organization people location however want extract list job role list job role wa wondering custom entity recognition something sort would go thing thanks advance
Named entity recognition for Arabic corpus using Stanford NER,"<p>I wish to find out how to use Stanford NER to train a corpus in Arabic. I was hoping to use freely available corpus such as ANERCorp available here :</p>

<p><a href=""http://www1.ccls.columbia.edu/~ybenajiba/downloads.html"" rel=""nofollow noreferrer"">http://www1.ccls.columbia.edu/~ybenajiba/downloads.html</a></p>

<p>I used the following prop file:</p>

<pre><code>trainFile = ANERCorp
serializeTo = aner-model.ser.gz
map = word=0,answer=1
maxLeft=1
useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useDisjunctive=true
useSequences=true
usePrevSequences=true
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
</code></pre>

<p>I then trained the model using:</p>

<pre><code>java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop t.prop
</code></pre>

<p>The training ran successfully and was serialized to my model. But when I test the model I always get a blank dataset, i.e. no entity is recognised. I know I am testing it the correct way because I am able to test English text on English models following the same methodology successfully. </p>

<p>Are there any properties I need to set in the prop file specifically for Arabic language? Has anyone tried training an arabic ner on Stanford NLP before? I know this has been done using LingPipe but I would prefer to stick with SNLP.</p>
",Named Entity Recognition (NER),named entity recognition arabic corpus using stanford ner wish find use stanford ner train corpus arabic wa hoping use freely available corpus anercorp available used following prop file trained model using training ran successfully wa serialized model test model always get blank dataset e entity recognised know testing correct way able test english text english model following methodology successfully property need set prop file specifically arabic language ha anyone tried training arabic ner stanford nlp know ha done using lingpipe would prefer stick snlp
how to implement fast spellchecker in Python with Pandas?,"<p>I work on text mining problem and need to extract all mentioned of certain keywords. For example, given the list:</p>

<pre><code>list_of_keywords = ['citalopram', 'trazodone', 'aspirin']
</code></pre>

<p>I need to find all occurrences of the keywords in a text. That could be easily done with Pandas (assuming my text is read in from a csv file):</p>

<p>import pandas as pd</p>

<pre><code>df_text = pd.read_csv('text.csv')
df_text['matches'] = df_text.str.findall('|'.join(list_of_keywords))
</code></pre>

<p>However, there are spelling mistakes in the text and some times my keywords will be written as: </p>

<pre><code>'citalopram' as 'cetalopram'
</code></pre>

<p>or </p>

<pre><code>'trazodone' as 'trazadon'
</code></pre>

<p>Searching on the web, I found some suggestions how to implement the <a href=""http://norvig.com/spell-correct.html"" rel=""nofollow noreferrer"">spell checker</a>, but I am not sure where to insert the spell checker and I reckon that it may slow down the search in the case a very large text.</p>

<p>As another option, it has been suggested to use a wild card with regex and insert in the potential locations of confusion (<strong>conceptually</strong> written)</p>

<pre><code>.findall('c*t*l*pr*m')
</code></pre>

<p>However I am not convinced that I can capture all possible problematic cases. I tried some out-of-the-box spell checkers, but my texts are some-what specific and I need a spell checker that 'knows' my domain (medical domain).</p>

<p><strong>QUESTION</strong></p>

<p>Is there any efficient way to extract keywords from a text including spelling mistakes?</p>
",Named Entity Recognition (NER),implement fast spellchecker python panda work text mining problem need extract mentioned certain keywords example given list need find occurrence keywords text could easily done panda assuming text read csv file import panda pd however spelling mistake text time keywords written searching web found suggestion implement spell checker sure insert spell checker reckon may slow search case large text another option ha suggested use wild card regex insert potential location confusion conceptually written however convinced capture possible problematic case tried box spell checker text specific need spell checker know domain medical domain question efficient way extract keywords text including spelling mistake
How to use entitymentions annotator in stanford CoreNLP?,"<p>I am trying the newest version of Stanford CoreNLP. When I extract location or organisation names, I see that every single word is tagged with the annotation. So, if the entity is ""NEW YORK TIMES"", then it is getting recorded as three different entities : ""NEW"", ""YORK"" and ""TIMES"". I find that the newest CoreNLP have ""entitymentions"" annotator. I think this annotator may help me to solve this problem. However, there is no usage instruction or example for this annotator. Could anyone give me more info about this new feature?</p>
",Named Entity Recognition (NER),use entitymentions annotator stanford corenlp trying newest version stanford corenlp extract location organisation name see every single word tagged annotation entity new york time getting recorded three different entity new york time find newest corenlp entitymentions annotator think annotator may help solve problem however usage instruction example annotator could anyone give info new feature
Extract Repeated Substrings from a List or Body of Text,"<p>Yes - This question has been asked in variations over the years but it seems a number of variables impact the best approach based on use case. </p>

<p>We have a database of webpage titles - many of which contain repeated strings - such as the website's name or website's section or both.  We are trying to extract the most repeated phrases to create a dictionary that will allow us to remove the substrings in a separate process. </p>

<p>The analysis will typically be against 10,000 lines of text with a max length of 256 characters. We also expect the substrings to be delimited by special characters such as ""-"", ""|"" or "":""</p>

<p>Solutions we have seen include regex, suffix arrays and suffix trees but we are unsure which would be the most efficient given our data structure. This calculation needs to be done thousands of times per day against unique lists. </p>

<p>Here is an example of a list:</p>

<pre><code>Sports lottery sales soar 70% in June on FIFA World Cup | Society | FOCUS TAIWAN - CNA ENGLISH NEWS
Scorching heat forecast to continue Tuesday | Society | FOCUS TAIWAN - CNA ENGLISH NEWS
Tech startups eye Taiwan's market | video | FOCUS TAIWAN - CNA ENGLISH NEWS
Taiwan headline news | Society | FOCUS TAIWAN - CNA ENGLISH NEWS
About 30% of working fathers feel alienated from children: poll | Society | FocusTaiwan Mobile - CNA English News
Taiwan wins 2 championships in Pony League baseball world series | Entertainment &amp; Sports | FocusTaiwan Mobile - CNA English News
A smart way to escape in a fire | video | FOCUS TAIWAN - CNA ENGLISH NEWS
Taiwan headline news | What the Papers Say | FOCUS TAIWAN - CNA ENGLISH NEWS
Taiwanese co-authors article nominated by European publisher | Society | FOCUS TAIWAN - CNA ENGLISH NEWS
Taiwan to help Indonesia with post-earthquake relief: MOFA | Politics | FOCUS TAIWAN - CNA ENGLISH NEWS
Taiwan shares close down 0.37% | Economics | FocusTaiwan Mobile - CNA English News
Taiwan shares open higher | Economics | FocusTaiwan Mobile - CNA English News
U.S. dollar closes lower on Taipei forex market | Economics | FocusTaiwan Mobile - CNA English News
</code></pre>

<p>From this list we would want to receive back the data including any punctuation or delimiters:</p>

<pre><code>| FOCUS TAIWAN - CNA ENGLISH NEWS = 8 Occurrences
| FocusTaiwan Mobile - CNA English News = 5 Occurrences 
| Society | FOCUS TAIWAN - CNA ENGLISH NEWS = 4 Occurrences 
| Economics | FocusTaiwan Mobile - CNA English News = 3 Occurrences
</code></pre>

<p>and so on...Any suggestions on the most appropriate approach to investigate would be very welcome.</p>

<p>*sample analysis data extracted from <a href=""https://www.online-utility.org/text/analyzer.jsp"" rel=""nofollow noreferrer"">https://www.online-utility.org/text/analyzer.jsp</a></p>
",Named Entity Recognition (NER),extract repeated substring list body text yes question ha asked variation year seems number variable impact best approach based use case database webpage title many contain repeated string website name website section trying extract repeated phrase create dictionary allow u remove substring separate process analysis typically line text max length character also expect substring delimited special character solution seen include regex suffix array suffix tree unsure would efficient given data structure calculation need done thousand time per day unique list example list list would want receive back data including punctuation delimiters suggestion appropriate approach investigate would welcome sample analysis data extracted
What is the maximum train dataset limit for training Stanford NER?,"<p>I am trying to train a Stanford NER model for 2 new named entities and I have a huge training set available (~300,000 files each around 2k). Is it possible to train this at once? What might be the memory requirements?</p>
",Named Entity Recognition (NER),maximum train dataset limit training stanford ner trying train stanford ner model new named entity huge training set available file around k possible train might memory requirement
Train NER model to recognize Custom entity,"<p>Currently, I am able to train the NER model to recognize the Custom value for Location,Person,Organization, but How can I train the NER model to recognize other  entities like Skill,Proofs,Vehicle..etc.</p>
",Named Entity Recognition (NER),train ner model recognize custom entity currently able train ner model recognize custom value location person organization train ner model recognize entity like skill proof vehicle etc
Train NER model in Stanford NLP,"<p>Exception while executing Train the model. Please find the steps that I have followed to train NER model,</p>

<p><strong>Step1</strong> : Create the training file like,</p>

<pre><code>the 0
Greenland   LOC
whale   0
is  0
deposed 0
,   0
-   0
the 0
great   0
sperm   0
whale   0
now 0
reigneth    0
!   0
</code></pre>

<p>and save as ""TrainingFile.tsv"" file.</p>

<p><strong>Step2:</strong> Created a .prop file we could use to train the first classifier.</p>

<p>Custom-ner.prop:</p>

<pre><code>trainFile = TrainingFile.tsv
serializeTo = ner-model.ser.gz
map = word=0,answer=1
maxLeft=1
useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useDisjunctive=true
useSequences=true
usePrevSequences=true
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
</code></pre>

<p>Step3: Build the classifier by executing the below code,</p>

<pre><code>java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier \
-prop propforclassifierone.prop
</code></pre>

<p><strong>While perform the Step3 throws an exception like,</strong></p>

<p>Exception :</p>

<pre><code>useSequences=true
wordShape=chris2useLC
useTypeySequences=true
useDisjunctive=true
noMidNGrams=true
serializeTo=ner-model.ser.gz
maxNGramLeng=6
useNGrams=true
usePrev=true
useNext=true
maxLeft=1
trainFile=directories2-10combined.tsv
map=word=0,answer=1
useWord=true
useTypeSeqs=true
=\
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: java.io.FileN
otFoundException: directories2-10combined.tsv (The system cannot find the file s
pecified)
        at edu.stanford.nlp.io.IOUtils.inputStreamFromFile(IOUtils.java:509)
        at edu.stanford.nlp.io.IOUtils.readerFromFile(IOUtils.java:550)
        at edu.stanford.nlp.objectbank.ReaderIteratorFactory$ReaderIterator.setN
extObject(ReaderIteratorFactory.java:189)
        at edu.stanford.nlp.objectbank.ReaderIteratorFactory$ReaderIterator.&lt;ini
t&gt;(ReaderIteratorFactory.java:161)
        at edu.stanford.nlp.objectbank.ResettableReaderIteratorFactory.iterator(
ResettableReaderIteratorFactory.java:98)
        at edu.stanford.nlp.objectbank.ObjectBank$OBIterator.&lt;init&gt;(ObjectBank.j
ava:414)
        at edu.stanford.nlp.objectbank.ObjectBank.iterator(ObjectBank.java:253)
        at edu.stanford.nlp.sequences.ObjectBankWrapper.iterator(ObjectBankWrapp
er.java:45)
        at edu.stanford.nlp.ie.crf.CRFClassifier.train(CRFClassifier.java:1585)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequence
Classifier.java:758)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequence
Classifier.java:746)
        at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3034)
Caused by: java.io.FileNotFoundException: directories2-10combined.tsv (The syste
m cannot find the file specified)
        at java.io.FileInputStream.open0(Native Method)
        at java.io.FileInputStream.open(Unknown Source)
        at java.io.FileInputStream.&lt;init&gt;(Unknown Source)
        at edu.stanford.nlp.io.IOUtils.inputStreamFromFile(IOUtils.java:503)
        ... 11 more
</code></pre>
",Named Entity Recognition (NER),train ner model stanford nlp exception executing train model please find step followed train ner model step create training file like save trainingfile tsv file step created prop file could use train first classifier custom ner prop step build classifier executing code perform step throw exception like exception
How to train NER to recognize that a word is not an entity?,"<p>I may have worded my question poorly, but basically I have been training new models using spaCy for NER.  I have trained some custom entities and it's doing a really great job when I test it.  However, when I send it something that shouldn't be recognized as an entity, it seems to guess one of the entities anyways.  I am guessing it's because I never trained it what would = O(I think that's how stanford does it).</p>

<p>Here is a sample of my training data, does this look right?  Do I need to just add trash values and set the entity as O?</p>

<pre><code>[ ""644663"" , {""entities"": [[0,6, ""CARDINAL""]]}],
[ ""871448"" , {""entities"": [[0,6, ""CARDINAL""]]}],
[ ""6/26/1967"" , {""entities"": [[0,9, ""DATE""]]}],
[ ""1/21/1969"" , {""entities"": [[0,9, ""DATE""]]}],
[ ""GORDON GARDIN"" , {""entities"": [[0,13, ""PERSON""]]}],
[ ""CANDRA CARDINAL"" , {""entities"": [[0,15, ""PERSON""]]}],
[ ""FIAT"" , {""entities"": [[0,4, ""CARMAKE""]]}],
[ ""FORD"" , {""entities"": [[0,4, ""CARMAKE""]]}]
</code></pre>
",Named Entity Recognition (NER),train ner recognize word entity may worded question poorly basically training new model using spacy ner trained custom entity really great job test however send something recognized entity seems guess one entity anyways guessing never trained would think stanford doe sample training data doe look right need add trash value set entity
Creating relations in sentence using chunk tags (not NER) with NLTK | NLP,"<p>I am trying to create custom chunk tags and to extract relations from them. Following is the code that takes me to the cascaded chunk tree.</p>

<pre><code>grammar = r""""""
  NPH: {&lt;DT|JJ|NN.*&gt;+}          # Chunk sequences of DT, JJ, NN
  PPH: {&lt;IN&gt;&lt;NP&gt;}               # Chunk prepositions followed by NP
  VPH: {&lt;VB.*&gt;&lt;NP|PP|CLAUSE&gt;+$} # Chunk verbs and their arguments
  CLAUSE: {&lt;NP&gt;&lt;VP&gt;}           # Chunk NP, VP
  """"""
cp = nltk.RegexpParser(grammar)
sentence = [(""Mary"", ""NN""), (""saw"", ""VBD""), (""the"", ""DT""), (""cat"", ""NN""),
    (""sit"", ""VB""), (""on"", ""IN""), (""the"", ""DT""), (""mat"", ""NN"")]


chunked = cp.parse(sentence)
</code></pre>

<p><strong>Output -</strong> </p>

<p>(S
  (NPH Mary/NN)
  saw/VBD
  (NPH the/DT cat/NN)
  sit/VB
  on/IN
  (NPH the/DT mat/NN))</p>

<p>Now I am trying to extract relations between the NPH tag values with the text in between using the nltk.sem.extract_rels function, BUT it seems to work ONLY on named entities generated with the ne_chunk function. </p>

<pre><code>IN = re.compile(r'.*\bon\b')
for rel in nltk.sem.extract_rels('NPH', 'NPH', chunked,corpus='ieer',pattern = IN):
        print(nltk.sem.rtuple(rel))
</code></pre>

<p>This gives the following error - </p>

<p><strong>ValueError: your value for the subject type has not been recognized: NPH</strong></p>

<p>Is there an easy way to use only chunk tags to create relations as I don't really want to retrain the NER model to detect my chunk tags as respective named entities</p>

<p>Thank you!</p>
",Named Entity Recognition (NER),creating relation sentence using chunk tag ner nltk nlp trying create custom chunk tag extract relation following code take cascaded chunk tree output nph mary nn saw vbd nph dt cat nn sit vb nph dt mat nn trying extract relation nph tag value text using nltk sem extract rels function seems work named entity generated ne chunk function give following error valueerror value subject type ha recognized nph easy way use chunk tag create relation really want retrain ner model detect chunk tag respective named entity thank
What is the better way to tag entities for NER using CRF,"<p>I am currently working on a custom named-entitie recognizer so as to recognize 4 types of entitiy: car, equipment, date, issue.</p>

<p>To do so, I use rasa_nlu with NER_crf from sklearn-crfsuite. However, before tagging hundreds of sentences, I asked myself two questions and I haven't found the answers:</p>

<ol>
<li>If you have for example ""On 31st Jan., the wheels of AA-075-ZP exhibited an increase in friction"". <strong>Is it better to tag ""On 31st Jan."" or ""31st Jan."" as a date ?</strong> Same question for ""the wheels"" or ""wheels"" as an equipment.</li>
</ol>

<p>I took a look at how does CRF work. From what I understood, the probability for a word w to be classified as an entity e1 depends on the fact that this word has already been tagged e1 in other documents but also on the fact that it follows a word w2 tagged e2 and that we often see words tagged e1 following words tagged e2.</p>

<p>Then, the question is: <strong>is it better to prefer entity tagging sequences or entity tagging content ?</strong>
Is it more interesting to say that a date comes after ""on"" or that it is composed of ""on"" so as to detect this date ?</p>

<ol start=""2"">
<li>My samples are often a description of the issue such as: ""On 31st Jan., the wheels of AA-075-ZP exhibited an increase in friction. This was caused by ... and .... on ... No more impact on the car, the four rubbers have been replaced""
<strong>Is it interesting to tag ""rubbers"" as an equipment considering that it comes at the end of a long description and that I most of the time just want to get the first entities in the text ?</strong> Is it worth to increase the number of occurences for rubber (so that rubber has more chance to be tagged as an equipment) but to give at the same time importance to the pattern ""an equipment coming after a lot of words"" ?</li>
</ol>

<p>Thank you in advance</p>
",Named Entity Recognition (NER),better way tag entity ner using crf currently working custom named entitie recognizer recognize type entitiy car equipment date issue use rasa nlu ner crf sklearn crfsuite however tagging hundred sentence asked two question found answer example st jan wheel aa zp exhibited increase friction better tag st jan st jan date question wheel wheel equipment took look doe crf work understood probability word w classified entity e depends fact word ha already tagged e document also fact follows word w tagged e often see word tagged e following word tagged e question better prefer entity tagging sequence entity tagging content interesting say date come composed detect date sample often description issue st jan wheel aa zp exhibited increase friction wa caused impact car four rubber replaced interesting tag rubber equipment considering come end long description time want get first entity text worth increase number occurences rubber rubber ha chance tagged equipment give time importance pattern equipment coming lot word thank advance
How to speed up spaCy lemmatization?,"<p>I'm using spaCy (version 2.0.11) for lemmatization in the first step of my NLP pipeline but unfortunately it's taking a verrry long time. It is clearly the slowest part of my processing pipeline and I want to know if there are improvements I could be making. I am using a pipeline as:</p>

<pre><code>nlp.pipe(docs_generator, batch_size=200, n_threads=6, disable=['ner'])
</code></pre>

<p>on a 8 core machine, and I have verified that the machine is using all the cores.</p>

<p>On a corpus of about 3 million short texts totaling almost 2gb it takes close to 24hrs to lemmatize and write to disk. Reasonable?</p>

<p>I have tried disabling a couple parts of the processing pipeline and found that it broke the lemmatization (parser, tagger). </p>

<p><strong>Are there any parts of the default processing pipeline that are not required for lemmatization besides named entity recognition?</strong></p>

<p><strong>Are there other ways of speeding up the spaCy lemmatization process?</strong></p>

<p>Aside:</p>

<p>It also appears that documentation doesn't list all the operations in the parsing pipeline. At the top of the spacy Language class we have:</p>

<pre><code>factories = {
    'tokenizer': lambda nlp: nlp.Defaults.create_tokenizer(nlp),
    'tensorizer': lambda nlp, **cfg: Tensorizer(nlp.vocab, **cfg),
    'tagger': lambda nlp, **cfg: Tagger(nlp.vocab, **cfg),
    'parser': lambda nlp, **cfg: DependencyParser(nlp.vocab, **cfg),
    'ner': lambda nlp, **cfg: EntityRecognizer(nlp.vocab, **cfg),
    'similarity': lambda nlp, **cfg: SimilarityHook(nlp.vocab, **cfg),
    'textcat': lambda nlp, **cfg: TextCategorizer(nlp.vocab, **cfg),
    'sbd': lambda nlp, **cfg: SentenceSegmenter(nlp.vocab, **cfg),
    'sentencizer': lambda nlp, **cfg: SentenceSegmenter(nlp.vocab, **cfg),
    'merge_noun_chunks': lambda nlp, **cfg: merge_noun_chunks,
    'merge_entities': lambda nlp, **cfg: merge_entities
}
</code></pre>

<p>which includes some items not covered in the documentation here: 
<a href=""https://spacy.io/usage/processing-pipelines"" rel=""noreferrer"">https://spacy.io/usage/processing-pipelines</a></p>

<p>Since they are not covered I don't really know which may be disabled, nor what their dependencies are.</p>
",Named Entity Recognition (NER),speed spacy lemmatization using spacy version lemmatization first step nlp pipeline unfortunately taking verrry long time clearly slowest part processing pipeline want know improvement could making using pipeline core machine verified machine using core corpus million short text totaling almost gb take close hr lemmatize write disk reasonable tried disabling couple part processing pipeline found broke lemmatization parser tagger part default processing pipeline required lemmatization besides named entity recognition way speeding spacy lemmatization process aside also appears documentation list operation parsing pipeline top spacy language class includes item covered documentation since covered really know may disabled dependency
Building NER ensembles,"<p>I am trying to build NER ensembles with models trained on Spacy NER(python), OpenNLP NER (java), StanfordNLP (java) and NLTK(python). I went through this guide for stacking multiple models <a href=""http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/"" rel=""nofollow noreferrer"">here</a>. </p>

<p>I am not sure how to go build a pipeline for this and choosing a meta-classifier.</p>

<p>Since the target is categorical, should I go ahead with choosing the mode for target.</p>
",Named Entity Recognition (NER),building ner ensemble trying build ner ensemble model trained spacy ner python opennlp ner java stanfordnlp java nltk python went guide stacking multiple model sure go build pipeline choosing meta classifier since target categorical go ahead choosing mode target
Azure LUIS - Prebuilt Geography Entity Alternative,"<p>I am creating an Azure LUIS app that needs to recognize locations in utterances, for example: ""I need a list of all employees currently in Seattle."" I do not have an exhaustive list of locations as they change often due to the nature of the business.</p>

<p>Using the prebuilt entity builtin.geography would be ideal, but it has been deprecated and I cannot add it to my app. The  closest thing I found was Calendar.Location (a prebuilt domain) but I am not sure if I can use it for this purpose. Any advice? What is everybody using as an alternative to the Geography entity? </p>

<p>Thanks in advance!</p>
",Named Entity Recognition (NER),azure luis prebuilt geography entity alternative creating azure luis app need recognize location utterance example need list employee currently seattle exhaustive list location change often due nature business using prebuilt entity builtin geography would ideal ha deprecated add app closest thing found wa calendar location prebuilt domain sure use purpose advice everybody using alternative geography entity thanks advance
convert xml (into iob2 format for NER/ABSA),"<p>I have an xml like this:<br>
&lt; sentences ><br> 
&lt; sentence id=""1""><br>
&lt; text > Pizza is great&lt; / text ><br>
&lt; aspectTerms ><br>
&lt; aspectTerm to=""5"" from=""1"" polarity=""positive"" term=""pizza""/>
(SemEval ABSA dataset)<br></p>

<p>and I would like to convert it like this:<br>
pizza B<br>
is O<br> great O<br></p>

<p>i.e. extract the sentences from the xml and place each word in a new line. 
Achieving this would already help me. </p>

<p>Ideally, there would also be a blank line between the last word of a sentence and the first of the next sentence. 
And in the best case, there would be a second column next to the words with an IOB2-tag (i.e. O for all words other than the xml-indicated aspectTerms, B for the aspect [or I for composed aspect terms]) </p>

<p>the idea is to use this converted file as training data for a sequence labeling task (to extract aspect terms from sentences, somewhat similar to NER)</p>

<p>I tried to achieve it with ElementTree but wasn't successful. </p>

<p>Sorry for this newbie question - any tips would help :) </p>
",Named Entity Recognition (NER),convert xml iob format ner absa xml like sentence sentence id text pizza great text aspectterms aspectterm polarity positive term pizza semeval absa dataset would like convert like pizza b great e extract sentence xml place word new line achieving would already help ideally would also blank line last word sentence first next sentence best case would second column next word iob tag e word xml indicated aspectterms b aspect composed aspect term idea use converted file training data sequence labeling task extract aspect term sentence somewhat similar ner tried achieve elementtree successful sorry newbie question tip would help
Named entity recognition - tagging tools,"<p>Does someone have a recommendation of tagging tool for NER types in raw text?</p>

<p>The input for the tool should be a library of text files(.txt simple format) , there should be a convenient UI for selecting words and set the tag/annotation fit to selection, the output should be structural representations of the tags(e.gs tart index , last index, tag in a JSON format)  </p>
",Named Entity Recognition (NER),named entity recognition tagging tool doe someone recommendation tagging tool ner type raw text input tool library text file txt simple format convenient ui selecting word set tag annotation fit selection output structural representation tag e g tart index last index tag json format
Identify which company a news article is about,"<p>We have a task of identifying which company a news article is about. The input is a (business) news article, the goal is the company name. </p>

<p>Could recommend a solution, please?</p>

<p>At the moment, we start with finding the N most mentioned company names in the article (by a Named Entity Recognition algorithm). When N >= 2, NER results can give us >75% accuracy. But when N = 1, this only give us about 50% accuracy.</p>
",Named Entity Recognition (NER),identify company news article task identifying company news article input business news article goal company name could recommend solution please moment start finding n mentioned company name article named entity recognition algorithm n ner result give u accuracy n give u accuracy
Combining relative time expressions in NLP in python?,"<p>I am working on extracting dates from natural language texts, and am wondering what might be the most efficient way of doing the following:</p>

<p>I have journal entries with dates as titles extracted as named entities with StanfordCoreNLP wrapper for python such as ""2018-07-13"", and from in text dates such as ""last monday"", which refers to the monday before the title date. I would like to be able to standardize ""last monday"" to ""2018-07-09"" in my data. </p>

<p>I looked into SUtime, but it appears there is no way to set the relative title date as reference to the in text date in python, or at least I cannot figure out how to do it with the wrapper. What would be the best course of action?</p>
",Named Entity Recognition (NER),combining relative time expression nlp python working extracting date natural language text wondering might efficient way following journal entry date title extracted named entity stanfordcorenlp wrapper python text date last monday refers monday title date would like able standardize last monday data looked sutime appears way set relative title date reference text date python least figure wrapper would best course action
Assign categories to words based on similarities,"<p>I have corpus of about 1 million documents of sentences. Let's say:-</p>

<blockquote>
  <p>sentence 1: ""Thrilling contest between manchester city and manchester united ends in draw""</p>
</blockquote>

<p>I want to assign sentences based on categories like above message belongs to category of ""sports"" and again assigned it to further categories like ""football"" categories in sports. 
I want to categorize texts into four categories ""entertainment"",""sports"",""politics"",""technology"". 
I have used an approach of Word2vec but it can only tells the relationship between two sentences. How exactly I can do this? I don't have any predefined labelled data just only million of records.
what I really want to do something like <a href=""https://www.paralleldots.com/text-classification"" rel=""nofollow noreferrer"">this</a></p>
",Named Entity Recognition (NER),assign category word based similarity corpus million document sentence let say sentence thrilling contest manchester city manchester united end draw want assign sentence based category like message belongs category sport assigned category like football category sport want categorize text four category entertainment sport politics technology used approach word vec tell relationship two sentence exactly predefined labelled data million record really want something like
What does NER model to find person names inside a resume/CV?,"<p>i just have started with Stanford CoreNLP, I would like to build a custom NER model to find <strong>persons</strong>.</p>

<p>Unfortunately, I did not find a good ner model for italian. I need to find these entities inside a resume/CV document.</p>

<p>The problem here is that document like those can have different structure, for example i can have:</p>

<p><strong>CASE 1</strong></p>

<pre><code>- Name: John

- Surname: Travolta

- Last name: Travolta

- Full name: John Travolta

(so many labels that can represent the entity of the person i need to extract)
</code></pre>

<p><strong>CASE 2</strong></p>

<pre><code>My name is John Travolta and I was born ...
</code></pre>

<p>Basically, i can have structured data (with different labels) or a context where i should find these entities.</p>

<p>What is the best approach for this kind of documents? Can a maxent model work in this case?</p>

<hr>

<h2><strong>EDIT @vihari-piratla</strong></h2>

<p>At the moment, i adopt the strategy to find a pattern that has something on the left and something on the right, following this method i have 80/85% to find the entity.</p>

<p>Example:</p>

<pre><code>Name: John
Birthdate: 2000-01-01
</code></pre>

<p>It means that i have ""Name:"" on the left of the pattern and a <strong>\n</strong> on the right (until it finds the <strong>\n</strong>).
I can create a very long list of patterns like those. I thought about patterns because i do not need names inside ""other"" context. </p>

<p>For example, if the user writes other names inside a <strong>job experience</strong> i do not need them. Because i am looking for the personal name, not others. With this method i can reduce false positives because i will look at specific patterns not ""general names"".</p>

<p>A problem with this method is that i have a big list of patterns (1 pattern = 1 regex), so it does not scale so well if i add others.</p>

<p>If i can train a NER model with all those patterns it will be awesome, but i should use tons of documents to train it well.</p>
",Named Entity Recognition (NER),doe ner model find person name inside resume cv started stanford corenlp would like build custom ner model find person unfortunately find good ner model italian need find entity inside resume cv document problem document like different structure example case case basically structured data different label context find entity best approach kind document maxent model work case edit vihari piratla moment adopt strategy find pattern ha something left something right following method find entity example mean name left pattern n right find n create long list pattern like thought pattern need name inside context example user writes name inside job experience need looking personal name others method reduce false positive look specific pattern general name problem method big list pattern pattern regex doe scale well add others train ner model pattern awesome use ton document train well
Name Extraction - CV/Resume - Stanford NER/OpenNLP,"<p>I'm currently on a learning project to extract an individuals name from their CV/Resume.</p>

<p>Currently I'm working with Stanford-NER and OpenNLP which both perform with a degree of success out of the box on, tending to struggle on ""non-western"" type names (no offence intended towards anybody).</p>

<p>My question is - given the general lack of sentence structure or context in relation to an individuals name in a CV/Resume, am I likely to gain any significant improvement in name identification by creating something akin to a CV corpus?</p>

<p>My initial thoughts are that I'd probably have a more success by sentence splitting, removing obvious text and applying a bit of logic to make a best guess on the individual's name. </p>

<p>I can see how training would work if the a name appears in within a structured sentence, however as a standalone entity without context (Akbar Agho for example) I suspect it will struggle regardless of the training. </p>

<p>Is there a level of AI that if given enough data would begin to formulate a pattern for finding a name or should I maybe just go for applying a level of logic based string extraction? </p>

<p>I'd appreciate people's thoughts, opinions and suggestions.</p>

<p>Side note: I having been using PHP with Appache Tika to do the initial text extraction from Doc/Pdf and am experimenting with Stanford and OpenNLP via PHP/Commandline.</p>

<p>Chris</p>
",Named Entity Recognition (NER),name extraction cv resume stanford ner opennlp currently learning project extract individual name cv resume currently working stanford ner opennlp perform degree success box tending struggle non western type name offence intended towards anybody question given general lack sentence structure context relation individual name cv resume likely gain significant improvement name identification creating something akin cv corpus initial thought probably success sentence splitting removing obvious text applying bit logic make best guess individual name see training would work name appears within structured sentence however standalone entity without context akbar agho example suspect struggle regardless training level ai given enough data would begin formulate pattern finding name maybe go applying level logic based string extraction appreciate people thought opinion suggestion side note using php appache tika initial text extraction doc pdf experimenting stanford opennlp via php commandline chris
Adding entities to Stanford NLP NER Classifier,"<p>I have a very simple method to extract Names, Organisations and Locations from a string. I am using the .NET Nuget Libraries for Stanford NLP. It looks like this. </p>

<pre><code>CRFClassifier Classifier = CRFClassifier.getClassifier(StanfordNLPConfig.NER.ClassifierModel);

        List&lt;IndexViewModel&gt; ivms = new List&lt;IndexViewModel&gt;();

        try
        {
            foreach (List sentence in Classifier.classify(content).toArray())
            {
                NLPTranslator translator = new NLPTranslator();
                ivms.AddRange(translator.NERTranslate(sentence));
            }
        }
        catch (Exception ex)
        {
            throw ex;
            // Error silently
        }

        return ivms;
</code></pre>

<p>The model is the 3class jar file it came with - english.all.3class.distsim.crf.ser.gz.</p>

<p>This is working really well for me, but would I'd like to do is interface with the model to be able to add in my own entities should I need to, this seems very American Centric and I'd like to be able to put my own UK companies, locations etc. </p>

<p>Is there any way I can just add in these entities as I have been reading about training it but that you possibly can't extend the model, if this is the case can I combine Classifiers and run it through a UK one/US one etc. If that's possible, how can I actually make my own Classifier as I would like to make these in .NET if possible. </p>
",Named Entity Recognition (NER),adding entity stanford nlp ner classifier simple method extract name organisation location string using net nuget library stanford nlp look like model class jar file came english class distsim crf ser gz working really well would like interface model able add entity need seems american centric like able put uk company location etc way add entity reading training possibly extend model case combine classifier run uk one u one etc possible actually make classifier would like make net possible
Person titles co-referencing in one text,"<p>If I have in the same txt ""president of the united states"" and ""Park Obama"", how could I extract the title ""President of the United States"" as a name entity referring to the ""Obama"" name entity?</p>

<p>What I have done is:
I add ""president of united state"" to the person names list in the gazetteer and it annotated it as a person but without co-referencing to Barak Obama.</p>

<p>Example of the text:</p>

<blockquote>
  <p>Barack Obama is an American politician serving as the 44th President
  of the United States. Born in Honolulu, Hawaii, Obama is a graduate of
  Columbia University and Harvard Law School.</p>
</blockquote>

<p>Another example: How could I refer to Prince of Wales as  Prince Charles that they are the same person in the following text?</p>

<blockquote>
  <p>The Prince of Wales is the Queen's eldest son and first in line to the
  throne. On 29 July 1981 he married Lady Diana Spencer, who became the
  Princess of Wales. The couple had two sons, William and Harry. They
  later separated and their marriage was dissolved in 1996. On 31 August
  1997, the Princess was killed in a car crash in Paris. Prince Charles
  married Camilla Parker Bowles on 9 April 2005. As heir to the throne,
  his main duties are to support the Queen in her royal commitments.</p>
</blockquote>

<p>In general, if I need to resolve any title to its original name entities.</p>
",Named Entity Recognition (NER),person title co referencing one text txt president united state park obama could extract title president united state name entity referring obama name entity done add president united state person name list gazetteer annotated person without co referencing barak obama example text barack obama american politician serving th president united state born honolulu hawaii obama graduate university harvard law school another example could refer prince wale prince charles person following text prince wale queen eldest son first line throne july married lady diana spencer became princess wale couple two son william harry later separated marriage wa dissolved august princess wa killed car crash paris prince charles married camilla parker bowles april heir throne main duty support queen royal commitment general need resolve title original name entity
fetching name and age from a text file,"<p>I have a .txt file from which I have to fetch name and age.
The .txt file has data in the format like:</p>

<pre><code>Age: 71 . John is 47 years old. Sam; Born: 05/04/1989(29).
Kenner is a patient Age: 36 yrs    Height: 5 feet 1 inch; weight is 56 kgs. 
This medical record is 10 years old. 

Output 1: John, Sam, Kenner
Output_2: 47, 29, 36  
</code></pre>

<p>I am using the regular expression to extract data. For example, for age, I am using the below regular expressions:</p>

<pre><code>re.compile(r'age:\s*\d{1,3}',re.I)

re.compile(r'(age:|is|age|a|) \s*\d{1,3}(\s|y)',re.I)

re.compile(r'.* Age\s*:*\s*[0-9]+.*',re.I)

re.compile(r'.* [0-9]+ (?:year|years|yrs|yr) \s*',re.I)
</code></pre>

<p>I will apply another regular expression to the output of these regular expressions to extract the numbers. The problem is with these regular expressions, I am also getting the data which I do not want. For example</p>

<pre><code>This medical record is 10 years old.
</code></pre>

<p>I am getting '10' from the above sentence which I do not want.
I only want to extract the names of people and their age. I want to know what should be the approach? I would appreciate any kind of help.</p>
",Named Entity Recognition (NER),fetching name age text file txt file fetch name age txt file ha data format like using regular expression extract data example age using regular expression apply another regular expression output regular expression extract number problem regular expression also getting data want example getting sentence want want extract name people age want know approach would appreciate kind help
Extract textbook names and journal articles from various syllabi,"<p>I am trying to extract textbook names, and other journal articles in syllabi collected from various courses using R. My basic assumption is that most of these will be in some kind of a citation format (e.g. APA, MLA, etc). While I can try to create regex-s to extract this information, I was wondering if anyone has tried to do this before, or if an R package exists that I may be able to use to extract this information from differently formatted text. 
Below are two examples of the syllabi that I am working with. In Sample 1, the book name is not in a citation format, but in sample 2, it is in a citation format. Both samples have been truncated to meet stackoverflow character limits.</p>

<blockquote>
  <p>SAMPLE 1:
  ""ABC State University  ARTS 3366 Intermediate Digital Photography  Fall 2015 JCM 4127 T/TH 2­4:30 pm   Lecturer: John Smith Office Hours: T/TH prior to and after class Email: ​johnsmith@abcstate.edu Alternate email: johnsmith@gmail.com Prerequisites​: ARTS 3364 ­ Introduction to Digital Photography  Course Description &amp; Objectives:  This course is designed to expand and build on the skills and knowledge acquired in Introduction to Digital Photography.  This course builds on the skills and knowledge acquired in Introduction to Digital Photography. Specifically, we will use the history, critical analysis, and production of photography books to: (1) explore the complexities of the medium in social, political, and aesthetic contexts; (2) develop more advanced and conceptually driven photography work; (3) work toward a greater understanding of how photography books function as self­contained art, cultural, and political objects; (4) learn how to choose subject matter and continually explore, experiment, and refine our work. The final outcomes ofthe class will be the creation of an on­demand book and an accompanying folio of fine prints.  We will use digital cameras, inkjet printers, Adobe Photoshop, Lightroom, and Macintosh computers in this course. Through lectures, discussions and readings, we will explore and discuss historical trends in traditional (analog) photography, as well as emerging practices in contemporary digital imaging. This will serve as a foundation to help determine the approach, subject matter, and style of the work created for class. In addition to refining these skills, students will also address the practical and theoretical roles of digital imagery.   The course objective will be to focus on technical, aesthetic, and conceptual growth of a student’s endeavors in the digital medium. This course requires the completion of:  all assignments (on time), participation in all group critiques and completion of a Twelve to Fifteen image final portfolio of prints or equivalent, and three projects throughout the semester.   Requirements:  Coursework: This course requires the completion of:  all assignments (on time), participation in all group critiques and completion of a 12­15 image final portfolio of prints or equivalent, the creation of a book printed with an on demand printing service, as well as making new photographs consistently throughout the entire semester.  Suggested (not required)Books:  Adobe Photoshop Lightroom 5 Book, The: The Complete Guide for Photographers   By Martin Evening Published Jun 30, 2013 by Adobe Press  The Photographer’s Playbook 307 Assignments and Ideas Edited by Jason Fulford and Gregory Halpern  Published by Aperture   On Being a Photographer: A Practical Guide ​ ​by David Hurn and Bill Jay Local Stores:""</p>
  
  <p>SAMPLE 2:""Physical Education Activity ProgramHealth &amp; Fitness Strength TrainingKINE 198-837Instructor: JANE DOE Office: PEAP 230Office Hours: By appointmentPhone: (000) 000-0000E-Mail: jdoe@xyz.edu A. Activity Instructor: Jane DoeOffice: PEAP 250Office Hours: By appointmentClass Time: Thursday 2:20 pmPhone: (000) 000-0000Email: jdoe@xyz1.edu Class Meeting Site: PEAP 117B. Activity Instructor: Jane Doe Phone:Office: PEAP 239Email: jdoe@xyz1.eduOffice Hours: Thursday 10:00 am – 12:00 pmClass Time: Thursday 2:20 pmClass Meeting Site: PEAP 118C. Activity Instructor: John doe Office: PEAP 250/Doe 213KOffice Hours: Tuesday 1:00-2:00 pmClass Time: Thursday 2:20 pmPhone:Email: johndoe@xyz.eduClass Meeting Site: PEAP 120Attire: Proper clothes and shoes designed specifically for strength training on activitydays.Required Materials:Bounds, L., Agnor, D., Darnell,G., &amp; Brekken Shea, K. (2012). Health &amp; Fitness: AGuide to a Healthy Lifestyle (5th edition). Dubuque, IA: Kendall/Hunt Publishing Co.ISBN 978-1-4652-0712-8Cissik, J. (2001). The Basics of Strength Training (3rd Edition). McGraw-Hill,Primus Custom PublishingCourse Description:Health and Fitness is intended for the student who is seeking knowledge and practicalapplication of wellness choices to their life. The course consists of two components,lecture and activity. Students will meet face-to-face one day per week for the activityportion of the class and work approximately the equivalent of one day per week onlinewith lecture materials. The lecture portion will cover current health issues includingmental and physical health, nutrition, human sexuality, communicable and noncommunicable diseases, use and abuse of drugs, and safety. The activity portion willconsist of 14 class days and cover basic knowledge and techniques of strength trainingand improving the individual’s fitness through the utilization of this knowledge.Course Rationale:Research indicates that daily health/fitness related behaviors enhance learning anddetermine the quality and longevity of our life.""</p>
</blockquote>
",Named Entity Recognition (NER),extract textbook name journal article various syllabus trying extract textbook name journal article syllabus collected various course using r basic assumption kind citation format e g apa mla etc try create regex extract information wa wondering anyone ha tried r package exists may able use extract information differently formatted text two example syllabus working sample book name citation format sample citation format sample truncated meet stackoverflow character limit sample abc state university art intermediate digital photography fall jcm th pm lecturer john smith office hour th prior class email johnsmith abcstate edu alternate email johnsmith gmail com prerequisite art introduction digital photography course description objective course designed expand build skill knowledge acquired introduction digital photography course build skill knowledge acquired introduction digital photography specifically use history critical analysis production photography book explore complexity medium social political aesthetic context develop advanced conceptually driven photography work work toward greater understanding photography book function self contained art political object learn choose subject matter continually explore experiment refine work final outcome ofthe class creation demand book folio fine print use digital camera inkjet printer adobe photoshop lightroom macintosh computer course lecture discussion reading explore discus historical trend traditional analog photography well emerging practice contemporary digital imaging serve foundation help determine approach subject matter style work created class addition refining skill student also address practical theoretical role digital imagery course objective focus technical aesthetic conceptual growth student endeavor digital medium course requires completion assignment time participation group critique completion twelve fifteen image final print equivalent three project throughout semester requirement coursework course requires completion assignment time participation group critique completion image final print equivalent creation book printed demand printing service well making new photograph consistently throughout entire semester suggested required book adobe photoshop lightroom book complete guide photographer martin evening published jun adobe press photographer playbook assignment idea edited jason fulford gregory halpern published aperture photographer practical guide david hurn bill jay local store sample physical education activity programhealth fitness strength trainingkine instructor jane doe office peap office hour appointmentphone e mail jdoe xyz edu activity instructor jane doeoffice peap office hour appointmentclass time thursday pmphone email jdoe xyz edu class meeting site peap b activity instructor jane doe phone office peap email jdoe xyz eduoffice hour thursday pmclass time thursday pmclass meeting site peap c activity instructor john doe office peap doe koffice hour tuesday pmclass time thursday pmphone email johndoe xyz educlass meeting site peap attire proper clothes shoe designed specifically strength training activitydays required material bound l agnor darnell g brekken shea k health fitness aguide healthy lifestyle th edition dubuque ia kendall hunt publishing co isbn cissik j basic strength training rd edition mcgraw hill primus custom publishingcourse description health fitness intended student seeking knowledge practicalapplication wellness choice life course consists two component lecture activity student meet face face one day per week activityportion class work approximately equivalent one day per week onlinewith lecture material lecture portion cover current health issue includingmental physical health nutrition human sexuality communicable noncommunicable disease use abuse drug safety activity portion willconsist class day cover basic knowledge technique strength trainingand improving individual fitness utilization knowledge course rationale research indicates daily health fitness related behavior enhance learning anddetermine quality longevity life
USE (NLP) GATE TOOL FOR NAMED-ENTITY,"<p>Can I use GATE <a href=""http://gate.ac.uk/"" rel=""noreferrer"">http://gate.ac.uk/</a> within my java program to extract named-entity. If yes, could you give any examples or guide me to some sources. Thank you </p>
",Named Entity Recognition (NER),use nlp gate tool named entity use gate within java program extract named entity yes could give example guide source thank
"I did Name Entity Recognition ( NER) in open nlp, I want to export that data into excel","<p>I did Name Entity Recognition ( NER) in open nlp, I want to export that data into excel.</p>
",Named Entity Recognition (NER),name entity recognition ner open nlp want export data excel name entity recognition ner open nlp want export data excel
Which technique is the best for extracting location from a Resume in Python 2.7?,"<p>I am working on creating a parser for job description. Here I have to extract every project details separately.  I have used NLTK and Stanford but the results are not accurate. Can anyone suggest a module to use which provides best results</p>
",Named Entity Recognition (NER),technique best extracting location resume python working creating parser job description extract every project detail separately used nltk stanford result accurate anyone suggest module use provides best result
How to robustly extract author names from pdf papers?,"<p>I'd like to extract author names from pdf papers. Does anybody know a robust way to do so?</p>

<p>For example, I'd like to extract the name <code>Archana Shukla</code> from this pdf <a href=""https://arxiv.org/pdf/1111.1648"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1111.1648</a></p>
",Named Entity Recognition (NER),robustly extract author name pdf paper like extract author name pdf paper doe anybody know robust way example like extract name pdf
Customize Spacy NER using IOB tagging scheme for Movie Review dataset,"<p>I have previously used CRF++ model to identify NER in movie review dataset. However, I have found spacy very effective in usage and visualization. However, the question here is that how I can incorporate CRF++ model in spacy? If this is not possible how can I train Spacy NER model for my requirement using IOB tagging?</p>
",Named Entity Recognition (NER),customize spacy ner using iob tagging scheme movie review dataset previously used crf model identify ner movie review dataset however found spacy effective usage visualization however question incorporate crf model spacy possible train spacy ner model requirement using iob tagging
Processing noisy unlabelled textual data for specific Named Entity Recognition,"<p>I am trying to extract some very specific info. i.e Organization names from very noisy data. an example is as follows: </p>

<p><code>AAC: 1233 BOB'S ELECTRONICS 12\323 ENTRY-123</code></p>

<p>The only important part to me is the BOB'S ELECTRONICS. Is there a good parser that handles noisy data like this and gives chunked results? Like so:</p>

<p><code>(Bob's Electronics, Organization)</code></p>

<p>P.S. -  Stanford Core NLP gives me <code>(Bob, person)</code></p>
",Named Entity Recognition (NER),processing noisy unlabelled textual data specific named entity recognition trying extract specific info e organization name noisy data example follows important part bob electronics good parser handle noisy data like give chunked result like p stanford core nlp give
Tokenizer Training with StanfordNLP,"<p>So my requirement is verbally simple. I need StanfordCoreNLP default models along with my custom trained model, based on custom entities. In a final run, I need to be able to isolate specific phrases from a given sentence (RegexNER will be used)</p>

<p>Following are my efforts :-</p>

<p><strong>EFFORT I :-</strong>
So I wanted to use the StanfordCoreNLP CRF files, tagger files and ner model files, along with my custom trained ner models.
I tried to find if there is any official way of doing this, but didnt get anything. There is a property ""ner.model"" for StanfordCoreNLP pipeline, but it will skip the default ones if used.</p>

<p><strong>EFFORT II :-</strong>
Next (might not be the smartest thing ever. Sorry! Just a guy trying to make ends meet!) , I extracted the model <pre><code>stanford-corenlp-models-3.7.0.jar</code></pre> , and copied all :-</p>

<pre><code>
*.ser.gz (Parser Models)
*.tagger (POS Tagger)
*.crf.ser.gz (NER CRF Files)
</code></pre>

<p>and tried to put Comma Separated Values with properties ""parser.model"", ""pos.model"" and ""ner.model"" respectively, as follows :-</p>

<pre><code>
parser.model=models/ner/default/anaphoricity_model.ser.gz,models/ner/default/anaphoricity_model_conll.ser.gz,models/ner/default/classification_model.ser.gz,models/ner/default/classification_model_conll.ser.gz,models/ner/default/clauseSearcherModel.ser.gz,models/ner/default/clustering_model.ser.gz,models/ner/default/clustering_model_conll.ser.gz,models/ner/default/english-embeddings.ser.gz,models/ner/default/english-model-conll.ser.gz,models/ner/default/english-model-default.ser.gz,models/ner/default/englishFactored.ser.gz,models/ner/default/englishPCFG.caseless.ser.gz,models/ner/default/englishPCFG.ser.gz,models/ner/default/englishRNN.ser.gz,models/ner/default/englishSR.beam.ser.gz,models/ner/default/englishSR.ser.gz,models/ner/default/gender.map.ser.gz,models/ner/default/md-model-dep.ser.gz,models/ner/default/ranking_model.ser.gz,models/ner/default/ranking_model_conll.ser.gz,models/ner/default/sentiment.binary.ser.gz,models/ner/default/sentiment.ser.gz,models/ner/default/truecasing.fast.caseless.qn.ser.gz,models/ner/default/truecasing.fast.qn.ser.gz,models/ner/default/word_counts.ser.gz,models/ner/default/wsjFactored.ser.gz,models/ner/default/wsjPCFG.ser.gz,models/ner/default/wsjRNN.ser.gz
ner.model=models/ner/default/english.all.3class.caseless.distsim.crf.ser.gz,models/ner/default/english.all.3class.distsim.crf.ser.gz,models/ner/default/english.all.3class.nodistsim.crf.ser.gz,models/ner/default/english.conll.4class.caseless.distsim.crf.ser.gz,models/ner/default/english.conll.4class.distsim.crf.ser.gz,models/ner/default/english.conll.4class.nodistsim.crf.ser.gz,models/ner/default/english.muc.7class.caseless.distsim.crf.ser.gz,models/ner/default/english.muc.7class.distsim.crf.ser.gz,models/ner/default/english.muc.7class.nodistsim.crf.ser.gz,models/ner/default/english.nowiki.3class.caseless.distsim.crf.ser.gz,models/ner/default/english.nowiki.3class.nodistsim.crf.ser.gz
pos.model=models/tagger/default/english-left3words-distsim.tagger
</code></pre>

<p>But, I get the following exception :-</p>

<pre><code>
Caused by: edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)
Caused by: java.io.StreamCorruptedException: invalid stream header: EFBFBDEF
</code></pre>

<p><strong>EFFORT III :-</strong>
I thought I will be able to handle with RegexNER, and I was successful to some extent. Just that the entities that it learns through RegexNER, it doesn't apply to forthcoming expressions. Eg: It will find the entity ""CUSTOM_ENTITY"" inside a text, but if i put a RegexNER like <pre><code> ( [ {ner:CUSTOM_ENTITY} ] /with/ [ {ner:CUSTOM_ENTITY} ] ) </code></pre> it never succeeds in finding the right phrase.</p>

<p>Really need help here!!! I don't wanna train the complete model again, Stanford guys got over a GB of model information which is useful to me. Just that I want to add custom entities too.</p>
",Named Entity Recognition (NER),tokenizer training stanfordnlp requirement verbally simple need stanfordcorenlp default model along custom trained model based custom entity final run need able isolate specific phrase given sentence regexner used following effort effort wanted use stanfordcorenlp crf file tagger file ner model file along custom trained ner model tried find official way didnt get anything property ner model stanfordcorenlp pipeline skip default one used effort ii next might smartest thing ever sorry guy trying make end meet extracted model copied tried put comma separated value property parser model po model ner model respectively follows get following exception effort iii thought able handle regexner wa successful extent entity learns regexner apply forthcoming expression eg find entity custom entity inside text put regexner like never succeeds finding right phrase really need help wan na train complete model stanford guy got gb model information useful want add custom entity
Extracting information from web-pages using NER,"<p>My task is to extract information from a various web-pages of a particular site. Now, the information to be extracted can be of the form as product name, product id, price, etc. The information is given in text using natural language. Also, I have been asked to extract that information using some Machine Learning algorithm. I thought of using NER (Named Entity Recognition) and training it on custom training data (which I can prepare using the scraped data and manually labeling the integers/data as required). I wanted to know if the model can even work this way?</p>

<p>Also, let me know if I can improve this question further.</p>
",Named Entity Recognition (NER),extracting information web page using ner task extract information various web page particular site information extracted form product name product id price etc information given text using natural language also asked extract information using machine learning algorithm thought using ner named entity recognition training custom training data prepare using scraped data manually labeling integer data required wanted know model even work way also let know improve question
Stanford Core NLP pipeline,"<p>I'm trying to create pipeline with NER tagging.</p>

<p>How to get the NER tagging in this way?</p>

<p>Line triggering the error:
<code>String nerrr = token.ner();</code></p>

<p>Code:</p>

<pre><code>public class NLPpipeline {

public AnnotationPipeline buildPipeline() {

    Properties props = new Properties();
    AnnotationPipeline pl = new AnnotationPipeline();

    pl.addAnnotator( new TokenizerAnnotator( false ) );
    pl.addAnnotator( new WordsToSentencesAnnotator( false ) );
    pl.addAnnotator( new POSTaggerAnnotator( false ) );
    pl.addAnnotator( new MorphaAnnotator( false ) );
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));


    return pl;
}


public static void main(String[] args) {


    NLPpipeline nlp = new NLPpipeline();
    AnnotationPipeline pipeline = nlp.buildPipeline();
    Annotation annotation = new Annotation( ""Last summer, Sali and Nadav met every Tuesday afternoon, from 1:00 pm to 3:00 pm."" );
    pipeline.annotate( annotation );


    for (CoreMap sentence : sentences) {
        for (CoreLabel token : sentence.get( CoreAnnotations.TokensAnnotation.class )) {

            String word = token.word();
            String pos = token.tag();
            String nerrr = token.ner();
            String role = token.lemma();


            System.out.println( ""=====\n"" + word );
            System.out.println( pos );
            System.out.println( nerrr );
            System.out.println( role );
        }
    }
}
</code></pre>

<p>thank you very much for your answer. I tried to create a pipe like you described, but it's very slow because I have a long text and I have to divide it into sentences, and each time it loads the NER files and it takes about 45 seconds for each sentence. My project is converting user stories into test cases, and I need to identify entities in user stories.
I realized I had the opportunity to create the department once:
         SentimentAnalyzer sentimentAnalyzer = new SentimentAnalyzer ();          sentimentAnalyzer.initializeCoreNLP (); // run this only once
And send at a time, but I do not understand how I should do it</p>
",Named Entity Recognition (NER),stanford core nlp pipeline trying create pipeline ner tagging get ner tagging way line triggering error code thank much answer tried create pipe like described slow long text divide sentence time load ner file take second sentence project converting user story test case need identify entity user story realized opportunity create department sentimentanalyzer sentimentanalyzer new sentimentanalyzer sentimentanalyzer initializecorenlp run send time understand
NTLK - Classifying Noun as a Location,"<p>I'm trying to use the NTLK to classify a noun as being a location. I realize this might be done with Named Entity Recognition, but that seems to apply to proper nouns in the context of a block of text. Is there a dataset I can check a noun against to see if it refers to a person, place, or thing? For example, I would like similar to 'facility', 'building',  and 'town' to be classified as a location (alternatively as a thing). </p>
",Named Entity Recognition (NER),ntlk classifying noun location trying use ntlk classify noun location realize might done named entity recognition seems apply proper noun context block text dataset check noun see refers person place thing example would like similar facility building town classified location alternatively thing
How to detect source and destination from message using neural model,"<p>I would like to extract the origin and destination from the given text.</p>

<p>For example,</p>

<pre><code>I am travelling from London to New York.
I am flying to Sydney from Singapore.
</code></pre>

<p>Origin -- > London, Singapore.
Destination --> Sydney, New York.</p>

<p>NER would give only the Location names, but couldn't fetch the Origin and destination.</p>

<p>Is it possible to train a neural model to detect the same ?</p>

<p>I have tried training the neural networks to classify the text like,</p>

<pre><code>{""tag"": ""Origin"",
     ""patterns"": [""Flying from "", ""Travelling from "", ""My source is"", ]
</code></pre>

<p>This way we could classify the text as origin, but I need to get the values as well (London , Singapore in this case).</p>

<p>Is there anyway we can achieve this?</p>
",Named Entity Recognition (NER),detect source destination message using neural model would like extract origin destination given text example origin london singapore destination sydney new york ner would give location name fetch origin destination possible train neural model detect tried training neural network classify text like way could classify text origin need get value well london singapore case anyway achieve
How to distinguish between two Different Named Entities of same name?,"<p>I have few articles, in which I am taking out name using NER Model (Named Entity Recognition). Since NER is classifying into four categories ( PERSON, LOCATION, ORGANISATION, MISCELLANEOUS ). Now I having two people of same name. How will I go about distinguishing between them? <br>
Kindly direct me towards some research available on this problem, if possible.</p>
",Named Entity Recognition (NER),distinguish two different named entity name article taking name using ner model named entity recognition since ner classifying four category person location organisation miscellaneous two people name go distinguishing kindly direct towards research available problem possible
Is Spacy language independent when training NER?,"<p>If I am training a NER model completely from scratch, does the language matter? In the API I set the language, but I also give the API the spans of the named entities. The command-line format goes one step further and I give the NER labels for each token for each sentence. For example, could I tokenize Japanese using ICU, label the tokens, then feed that to Spacy?</p>
",Named Entity Recognition (NER),spacy language independent training ner training ner model completely scratch doe language matter api set language also give api span named entity command line format go one step give ner label token sentence example could tokenize japanese using icu label token feed spacy
How to re-train an existing spacy NER model for currency,"<p>I am trying to update an existing spacy model ""en_core_web_sm"" with some different country currency such as ""euro"", ""rupees"", ""eu"", ""Rs."", ""INR"" etc. How can I achieve that ? The spacy tutorial didn't quite help me as training a fixed string such as ""horses"" as ""ANIMAL"" seems different than my requirements. The reason is I can have currency value indifferent formats : ""1 million euros"", ""Rs. 10,000"", ""INR 1 thousand"" etc. My sample dataset contains around  1000 samples with the following format :</p>

<pre><code>TRAIN_DATA = [      
 ("" You have activated International transaction limit for Debit Card ending XXXX1137 on 2017-07-05 12:48:20.0 via NetBanking. The new limit is Rs. 250,000.00"", {'entities' : [(140, 154, 'MONEY')] }),...
]
</code></pre>

<p>Can anyone please help me out with this with the data format, training size or any other relevant information ?</p>
",Named Entity Recognition (NER),train existing spacy ner model currency trying update existing spacy model en core web sm different country currency euro rupee eu r inr etc achieve spacy tutorial quite help training fixed string horse animal seems different requirement reason currency value indifferent format million euro r inr thousand etc sample dataset contains around sample following format anyone please help data format training size relevant information
How to add another text feature to current bag of words classification? In Scikit-learn,"<p>this is my input matrix <a href=""https://i.sstatic.net/Hdtfx.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>my sample Code: </p>

<pre><code>from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(data['Extract'], 
data['Expense Account code Description'], random_state = 0)

from sklearn.pipeline import Pipeline , FeatureUnion
text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1))),
              ('tfidf', TfidfTransformer(use_idf = False)),
              ('clf', RandomForestClassifier(n_estimators =100, 
 max_features='log2',criterion = 'entropy')),
 ])
 text_clf = text_clf.fit(X_train, y_train)
</code></pre>

<p>here I am applying Bag of word model for 'Extract' column classifying 'Expense Account code Description' , Here i am getting an accuracy of around 92% , but if i want to include 'Vendor name' as the set of another input feature how can i do that.  Is there any way of doing it along with the bag of words ? , </p>
",Named Entity Recognition (NER),add another text feature current bag word classification scikit learn input matrix enter image description sample code applying bag word model extract column classifying expense account code description getting accuracy around want include vendor name set another input feature way along bag word
Are titles included when performing named entity recognition?,"<p>Using NER software like Stanford NLP or Apache OpenNLP, when training the model to recognize person names, do I have to include the title along with the name?</p>

<p>For example using Apache</p>

<pre><code>&lt;START:person&gt; Robert M. Haugh, MD &lt;END&gt;
signed by &lt;START:person&gt; Dr. Holt B. Zolt &lt;END&gt;
</code></pre>

<p>VS</p>

<pre><code>&lt;START:person&gt; Robert M. Haugh, &lt;END&gt; MD 
signed by Dr. &lt;START:person&gt; Holt B. Zolt &lt;END&gt;
</code></pre>

<p>Or using Stanford</p>

<pre><code>At  O
the O
request O
of  O
Dr. PERS
Kelly   PERS
Schmeick    PERS
on  O
</code></pre>

<p>VS</p>

<pre><code>At  O
the O
request O
of  O
Dr. O
Kelly   PERS
Schmeick    PERS
on  O
</code></pre>
",Named Entity Recognition (NER),title included performing named entity recognition using ner software like stanford nlp apache opennlp training model recognize person name include title along name example using apache v using stanford v
Customizing my Own model in Stanford NER,"<p>Could I ask about Stanford NER?? Actually, I'm trying to train my own model, to use it later for learning. According to the documentation, I have to add my own features in SeqClassifierFlags and add code for each Feature in NERFeatureFactory. </p>

<p>My questions is that, I have my tokens with all features extracted and Last column represents the label. So, is there any way in Stanford NER to give it my Tab-Delimeted file which contains 30 columns (1 is word , 28 are featurs, and 1 is label) to train my own model without spending time for extracting features??? 
Of course, in Testing phase, I will give it a file like the the aforementioned file without label to predict the label.</p>

<p>Is this possible or Not??</p>

<p>Many thanks in Advance</p>
",Named Entity Recognition (NER),customizing model stanford ner could ask stanford ner actually trying train model use later learning according documentation add feature seqclassifierflags add code feature nerfeaturefactory question token feature extracted last column represents label way stanford ner give tab delimeted file contains column word featurs label train model without spending time extracting feature course testing phase give file like aforementioned file without label predict label possible many thanks advance
Is there any word shape feature library for NER in python?,"<p>As a beginner in python, I am trying to build my own named entity recognizer and it is known that word shape features are particularly important in NER. Are there any known libraries where these features are defined? For example, one version of these features denotes lowers-case letters by x and upper-case letters by X, numbers by d and retaining punctuation, maps <b>DC10-30</b> to <b>XX-dd-dd</b> and <b>I.M.F</b> to <b>X.X.X</b>. <br><br>
So I look for a library which will improve my recognizer by applying these popularly known features. If there is no such library, how can I extract word shape features of a word like </p>

<pre><code>wordshape(""D-Day"") = X-Xxx
</code></pre>

<p>Thanks in advance.</p>
",Named Entity Recognition (NER),word shape feature library ner python beginner python trying build named entity recognizer known word shape feature particularly important ner known library feature defined example one version feature denotes lower case letter x upper case letter x number retaining punctuation map dc xx dd dd f x x x look library improve recognizer applying popularly known feature library extract word shape feature word like thanks advance
UK License Number RUTA entity extraction,"<p>I have been developing a proof of concept of free text analytics. The RUTA scripts which I have developed for account number, date, salutations, addresses, pin codes, name seem to work properly. </p>

<p>But I am stuck on one rule where I want to extract the license number in UK format from a textual paragraph. The rule I developed seems to work properly when it is alone passed as input but for some reason it fails in a text. </p>

<p>Any help would be highly appreciated as I have been with this issue for quite sometime. </p>

<pre><code>       PACKAGE uima.ruta.example;
       DECLARE VarA;
       DECLARE VarB;
       DECLARE VarC;

       W{REGEXP(""^(?i)(a-z){2}"") -&gt; MARK(VarA)}
       NUM{REGEXP("".."") -&gt; MARK(VarB)}

       W{REGEXP(""(?i)(a-z){3}$"") -&gt; MARK(VarC),  MARK(EntityType,1,3), UNMARK(VarA), UNMARK(VarB), UNMARK(VarC)};
</code></pre>

<p>The format which I am expecting is 
C - Character 
N - Number</p>

<ol>
<li>CCNNCCC</li>
<li>CCNN CCC</li>
</ol>
",Named Entity Recognition (NER),uk license number ruta entity extraction developing proof concept free text analytics ruta script developed account number date salutation address pin code name seem work properly stuck one rule want extract license number uk format textual paragraph rule developed seems work properly alone passed input reason fails text help would highly appreciated issue quite sometime format expecting c character n number ccnnccc ccnn ccc
Gensim: how to load precomputed word vectors from text file,"<p>I have a text file with my precomputed word vectors in the following format (example):</p>

<p><code>word -0.0762464299711 0.0128308048976 ... 0.0712385589283\n’</code></p>

<p>on each line for every word (with 297 extra floats in place of the <code>...</code>). I am trying to load these with Gensim as KeyedVectors, because I ultimately would like to compute the cosine similarity, find most similar words, etc. Unfortunately I have not worked with Gensim before and from the documentation it's not quite clear to me how to do this. I have tried the following which I found <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""noreferrer"">here</a>:</p>

<p><code>word_vectors = KeyedVectors.load_word2vec_format('/embeddings/word.vectors', binary=False)</code></p>

<p>However this gives the following error:</p>

<p><code>ValueError: invalid literal for int() with base 10: 'the'</code></p>

<p>'the' is the first word in the text file, so I suspect that the loading function is expecting something to be there that is not. But I can't find any information on what should be there. I would highly appreciate a pointer to such information or any other solution to my problem. Thanks!</p>
",Named Entity Recognition (NER),gensim load precomputed word vector text file text file precomputed word vector following format example line every word extra float place trying load gensim keyedvectors ultimately would like compute cosine similarity find similar word etc unfortunately worked gensim documentation quite clear tried following found however give following error first word text file suspect loading function expecting something find information would highly appreciate pointer information solution problem thanks
Speed up Spacy Named Entity Recognition,"<p>I'm using spacy to recognize street addresses on web pages.  </p>

<p>My model is initialized basically using spacy's new entity type sample code found here:
<a href=""https://github.com/explosion/spaCy/blob/master/examples/training/train_new_entity_type.py"" rel=""noreferrer"">https://github.com/explosion/spaCy/blob/master/examples/training/train_new_entity_type.py</a></p>

<p>My training data consists of plain text webpages with their corresponding Street Address entities and character positions.</p>

<p>I was able to quickly build a model in spacy to start making predictions, but I found its prediction speed to be very slow.</p>

<p>My code works by iterating through serveral raw HTML pages and then feeding each page's plain text version into spacy as it's iterating.  For reasons I can't get into, I need to make predictions with Spacy page by page, inside of the iteration loop.</p>

<p>After the model is loaded, I'm using the standard way of making predictions, which I'm referring to as the prediction/evaluation phase:</p>

<pre><code>  doc = nlp(plain_text_webpage)

  if len(doc.ents) &gt; 0:

         print (""found entity"")
</code></pre>

<p>Questions:</p>

<ol>
<li><p>How can I speed up the entity prediction / recognition phase?  I'm using a c4.8xlarge instance on AWS and all 36 cores are constantly maxed out when spacy is evaluating the data.  Spacy is turning processing a few million webpages from a 1 minute job to a 1 hour+ job.</p></li>
<li><p>Will the speed of entity recognition improve as my model becomes more accurate?</p></li>
<li><p>Is there a way to remove pipelines like tagger during this phase, can ER be decoupled like that and still be accurate?  Will removing other pipelines affect the model itself or is it just a temporary thing?</p></li>
<li><p>I saw that you can use GPU during the ER training phase, can it also be used in this evaluating phase in my code for faster predictions?</p></li>
</ol>

<hr>

<p><strong>Update:</strong></p>

<p>I managed to significantly cut down the processing time by:</p>

<ol>
<li><p>Using a custom tokenizer (used the one in the docs)</p></li>
<li><p>Disabling other pipelines that aren't for Named Entity Recognition</p></li>
<li><p>Instead of feeding the whole body of text from each webpage into spacy, I'm only sending over a maximum of 5,000 characters</p></li>
</ol>

<p>My updated code to load the model:</p>

<pre><code>nlp = spacy.load('test_model/', disable=['parser', 'tagger', 'textcat'])
nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)
doc = nlp(text)
</code></pre>

<p>However, it is <strong><em>still</em></strong> too slow (20X slower than I need it)</p>

<p><strong>Questions:</strong></p>

<ol>
<li><p>Are there any other improvements I can make to speed up the Named Entity Recognition?  Any fat I can cut from spacy?</p></li>
<li><p>I'm still looking to see if a GPU based solution would help - I saw that GPU use is supported during the Named Entity Recognition training phase, can it also be used in this evaluation phase in my code for faster predictions?</p></li>
</ol>
",Named Entity Recognition (NER),speed spacy named entity recognition using spacy recognize street address web page model initialized basically using spacy new entity type sample code found training data consists plain text webpage corresponding street address entity character position wa able quickly build model spacy start making prediction found prediction speed slow code work iterating serveral raw html page feeding page plain text version spacy iterating reason get need make prediction spacy page page inside iteration loop model loaded using standard way making prediction referring prediction evaluation phase question speed entity prediction recognition phase using c xlarge instance aws core constantly maxed spacy evaluating data spacy turning processing million webpage minute job hour job speed entity recognition improve model becomes accurate way remove pipeline like tagger phase er decoupled like still accurate removing pipeline affect model temporary thing saw use gpu er training phase also used evaluating phase code faster prediction update managed significantly cut processing time using custom tokenizer used one doc disabling pipeline named entity recognition instead feeding whole body text webpage spacy sending maximum character updated code load model however still slow x slower need question improvement make speed named entity recognition fat cut spacy still looking see gpu based solution would help saw gpu use supported named entity recognition training phase also used evaluation phase code faster prediction
How to process Scrapy output for NLP?,"<p>I'm trying to extract text data from companies' website using python Scrapy.</p>

<p>The code below scrapes texts with no errors, but the output seems some further processes are required for NLP.</p>

<p><strong>Spider code:</strong></p>

<pre><code># -*- coding: utf-8 -*-
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from ..items import ScrapingtestItem
from scrapy_splash import SplashRequest
from scrapy.utils.log import configure_logging

import re

class TestscraperSpider(scrapy.Spider):
    name = 'Testscraper'
    global search_pages
    search_pages = ['https://www.impossiblefoods.com/', 'http://www.ycombinator.com/']

    list_allow = search_pages
    list_allow_parse = search_pages

    custom_settings = {""DOWNLOAD_DELAY"": 1,}  # interval seconds 
    rules = (
        Rule(LinkExtractor(allow=list_allow,), follow=True),
        Rule(LinkExtractor(allow=list_allow_parse, unique=True), callback='start_requests'),
        )

    def start_requests(self):
        for url in search_pages:
            yield SplashRequest(url=url, callback=self.parse)

    def parse(self, response):
        item = ScrapingtestItem() 
        try:
            item['extracted_ptag'] = response.xpath('//p/text()').extract()
        except:
            item['extracted_ptag'] = None
        try:
            item['extracted_atag'] = response.xpath('//a/text()').extract()
        except:
            item['extracted_atag'] = None
        try:
            item['extracted_pretag'] = response.xpath('//pre/text()').extract()
        except:
            item['extracted_pretag'] = None
        try:
            item['extracted_strongtag'] = response.xpath('//strong/text()').extract()
        except:
            item['extracted_strongtag'] = None
        yield item
</code></pre>

<p><strong>Item code</strong></p>

<pre><code># -*- coding: utf-8 -*-

import scrapy

class ScrapingtestItem(scrapy.Item):
    extracted_ptag= scrapy.Field()
    extracted_atag= scrapy.Field()
    extracted_pretag= scrapy.Field()
    extracted_strongtag= scrapy.Field()
</code></pre>

<p><strong>The output json file</strong>:</p>

<pre><code>[
{""extracted_text_ptag"": [""It\u2019s here. A delicious burger made entirely from plants for people who love meat. No more compromises. Ready for an introduction?"", ""We're committed to creating a better planet and better meat, from plants. Meet heme, the magic molecule that makes it all possible."", ""Turn On Sound "", ""The world loves meat. But relying on cows to make meat is land-hungry, water-thirsty, and pollution-heavy. That\u2019s why we set out to do the impossible: make delicious meats that are good for people and the planet."", ""It all starts with the Impossible Burger. But our world-renowned team of scientists are hard at work inventing more ways to make the foods we love most."", ""We spent the past five years researching what makes meat unique: the sizzle, the smell, the juicy first bite. Then we set out to find precisely the right ingredients from the plant kingdom to recreate the experience meat lovers crave. You\u2019ve never tasted plants like this."", ""Every time you choose a quarter-pound Impossible Burger instead of a burger made from a cow, you can make a huge difference without compromising."", ""Welcome to the era of plant-based meat"", ""The patty sizzles like beef in the pan, which gets my appetite going."", ""You\u2019re trying to do in meat, what Tesla did in electric cars.""], ""extracted_text_atag"": [""Our Burger"", ""Locations"", ""About Us"", ""FAQs"", ""Press"", ""Blog"", ""Home"", ""Our Burger"", ""Locations"", ""About Us"", ""FAQs"", ""Press"", ""Blog"", ""Meet Heme"", ""About Us"", ""Learn More"", ""Read More "", ""Read More "", ""Read More "", ""View More Articles"", ""twitter"", ""facebook"", ""youtube"", ""Our Burger"", ""About Us"", ""FAQs"", ""Careers"", ""Press"", ""Locations"", ""Facebook"", ""Twitter"", ""Instagram"", ""YouTube"", ""+1 855 877 6365"", ""Privacy Policy"", ""Terms of Service""], ""extracted_text_pretag"": [], ""extracted_text_strongtag"": []},
{""extracted_text_ptag"": [""Twice a year we invest a small amount of money ("", "") in a large number of startups."", ""The startups move to Silicon Valley for 3 months, during which we work intensively with them to get the company into the best possible shape and refine their pitch to investors. Each cycle culminates in Demo Day, when the startups present their companies to a carefully selected, invite-only audience."", ""But YC doesn't end on Demo Day. We and the YC alumni network continue to help founders for the life of their company, and beyond."", "" "", "" "", ""\""Y Combinator is the best program for creating top-end entrepreneurs that has ever existed.\"""", ""\""I\u00a0doubt that Stripe would have worked\u00a0without YC. It's that simple. Acquiring early customers, figuring out who to hire, closing deals with banks, raising money --\u00a0YC's partners were closely involved and crucially helpful.\"""", ""\""I've been fortunate to engage with the YC community at past events over the last few years, and always walk away impressed with the passion and caliber of talent that YC brings together.\""""], ""extracted_text_atag"": [""About"", ""Companies"", ""People"", ""YC Continuity"", ""Startup School"", ""Blog"", ""Resources"", ""Apply"", ""$120k"", ""Learn More"", ""Application FAQs"", ""Female Founder Stories"", ""More Quotes"", "" "", "" "", "" "", "" "", "" "", "" "", "" "", "" "", "" "", "" "", "" "", "" "", ""About"", ""Contact"", ""Press"", ""Legal"", ""Security"", ""Apply""], ""extracted_text_pretag"": [], ""extracted_text_strongtag"": []}
]
</code></pre>

<p>For the list of dicts output, I want delete blank content such as <em>"" "",</em> or <em>""\n""</em> from the list, and then aggregate them as the one sentence in order for NLP.
So after all I want to make the dict, of which key is company's name and the value is the aggregated sentence like:</p>

<pre><code>{""company_name1"": *aggregated_sentence1*, ""company_name2"": *aggregated_sentence2*}
</code></pre>

<p>How can I process the Scrapy output?
Any answers / advice will be greatly appreciated. </p>

<p>Thanks in advance.</p>
",Named Entity Recognition (NER),process scrapy output nlp trying extract text data company website using python scrapy code scrape text error output seems process required nlp spider code item code output json file list dicts output want delete blank content n list aggregate one sentence order nlp want make dict key company name value aggregated sentence like process scrapy output answer advice greatly appreciated thanks advance
Keras saved model predicting different values on different session,"<p>I haved trained a named entity recognition model, after saving it  and loading it back it is giving  correct prediction on the same IPython session, but whenever I close the session and open it again, the loaded model prediction randomly. Can you help me with that?</p>

<p>I have saved the model in hdf5 format using:</p>

<pre><code>Model.save(""filename"")
</code></pre>

<p>And I am loading it using:</p>

<pre><code>Model.load_model(""filename"")
</code></pre>

<p>here is  my full code </p>

<pre><code>import pandas as pd
import numpy as np
import os
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from keras.models import Model, Input,load_model
from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout,  
Bidirectional
from nltk import pos_tag, word_tokenize,sent_tokenize



data = pd.read_csv(""E:\ml tut\entity recognition\exdataset.csv"", 
encoding=""latin1"")
data = data.fillna(method=""ffill"")
words = list(set(data[""Word""].values))
words.append(""ENDPAD"")
n_words = len(words); n_words

tags = list(set(data[""Tag""].values))
n_tags = len(tags); n_tags

class SentenceGetter(object):

    def __init__(self, data):
        self.n_sent = 1
        self.data = data
        self.empty = False
        agg_func = lambda s: [((w, p), t) for w, p, t in 
                 zip(s[""Word""].values.tolist(),s[""POS""].values.tolist(),     
                                             s[""Tag""].values.tolist())]
        self.grouped = self.data.groupby(""Sentence #"").apply(agg_func)
        self.sentences = [s for s in self.grouped]

    def get_next(self):
        try:
            s = self.grouped[""Sentence: {}"".format(self.n_sent)]
            self.n_sent += 1
            return s
        except:
            return None

getter = SentenceGetter(data)

sent = getter.get_next()
print(sent)

sentences = getter.sentences

max_len = 50
word2idx = {w: i for i, w in enumerate(words)}
tag2idx = {t: i for i, t in enumerate(tags)}





input = Input(shape=(max_len,))
model = Embedding(input_dim=n_words, output_dim=50, input_length=max_len) 
       (input)

model = Dropout(0.1)(model)

model = Bidirectional(LSTM(units=100, return_sequences=True, 
recurrent_dropout=0.1))(model)

out = TimeDistributed(Dense(n_tags, activation=""softmax""))(model)  

if os.path.exists('my_model.h5'):
    print(""loading model"")
    model = load_model('my_model.h5')
else:
    print(""training model"")
    X = [[word2idx[w[0][0]] for w in s] for s in sentences]
    X = pad_sequences(maxlen=max_len, sequences=X, padding=""post"", 
    value=n_words - 1)
    y = [[tag2idx[w[1]] for w in s] for s in sentences]
    y = pad_sequences(maxlen=max_len, sequences=y, padding=""post"", 
    value=tag2idx[""O""])
    y = [to_categorical(i, num_classes=n_tags) for i in y]
    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)
    model = Model(input, out)
    model.compile(optimizer=""rmsprop"", loss=""categorical_crossentropy"", 
    metrics=[""accuracy""])
    model.fit(X_tr, np.array(y_tr), batch_size=32, epochs=5, 
    validation_split=0.1, verbose=1)
    model.save('my_model.h5')



    my_input=""Albert Einstein is a great guy,he lives in berlin, Germany.""
    print(""--------------"") 

    test_sentence = word_tokenize(my_input)
    x_test_sent = pad_sequences(sequences=[[word2idx.get(w, 0) for w in 
    test_sentence]],padding=""post"", value=0, maxlen=max_len)
    i = 0
    p = model.predict(np.array([x_test_sent[i]]))
    p = np.argmax(p, axis=-1)
    print(""{:15}||{}"".format(""Word"", ""Prediction""))
    print(30 * ""="")
    for w, pred in zip(test_sentence, p[0]):
    if w != 0:
         print(""{:15}: {}"".format(w, tags[pred]))
</code></pre>
",Named Entity Recognition (NER),kera saved model predicting different value different session haved trained named entity recognition model saving loading back giving correct prediction ipython session whenever close session open loaded model prediction randomly help saved model hdf format using loading using full code
How to add custom slangs into spaCy&#39;s norm_exceptions.py module?,"<p>SpaCy's documentation has some information on adding new slangs <a href=""https://spacy.io/usage/adding-languages#norm-exceptions"" rel=""nofollow noreferrer"">here</a>.</p>

<p>However, I'd like to know:</p>

<p><strong>(1) When should I call the following function?</strong></p>

<pre><code>lex_attr_getters[NORM] = add_lookups(Language.Defaults.lex_attr_getters[NORM], NORM_EXCEPTIONS, BASE_NORMS)
</code></pre>

<p>The typical usage of spaCy, according to the introduction guide <a href=""https://spacy.io/usage/spacy-101"" rel=""nofollow noreferrer"">here</a>, is something as follows:</p>

<pre><code>import spacy
nlp = spacy.load('en')
# Should I call the function add_lookups(...) here?
doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')
</code></pre>

<p><strong>(2) When in the processing pipeline are norm exceptions handled?</strong></p>

<p>I'm assuming a typical pipeline as such: tokenizer -> tagger -> parser -> ner.</p>

<p>Are norm exceptions handled right before the tokenizer? And also, how is the norm exceptions component organized with respect to the other pre-processing components such as stop words, lemmatizer (see full list of components <a href=""https://spacy.io/usage/spacy-101#language-data"" rel=""nofollow noreferrer"">here</a>)? What comes before what?</p>

<hr>

<p>Am new to spaCy and much help would be appreciated. Thanks!</p>
",Named Entity Recognition (NER),add custom slang spacy norm exception py module spacy documentation ha information adding new slang however like know call following function typical usage spacy according introduction guide something follows processing pipeline norm exception handled assuming typical pipeline tokenizer tagger parser ner norm exception handled right tokenizer also norm exception component organized respect pre processing component stop word lemmatizer see full list component come new spacy much help would appreciated thanks
Spacy training multithread CPU usage,"<p>I'm training some models with my own NER pipe. I need to run spacy in lxc container so I can run it with python3.6 (which allow multi thread on training).<br>
But.. on my 7 core authorized to run on my container only 1 run at 100% others run at 40-60% (actually they start at 100% but decrease after fews minutes). I would really like to improve this % core usage. Any idea to where to look ? Could it be a problem of Producer / Consumer ?</p>

<p><strong>Env:</strong><br>
<strong>- spaCy version 2.0.8<br>
- Location /root/.env/lib/python3.6/site-packages/spacy<br>
- Platform Linux-3.14.32-xxxx-grs-ipv6-64-x86_64-with-debian-buster-sid<br>
- Python version 3.6.4</strong>  </p>
",Named Entity Recognition (NER),spacy training multithread cpu usage training model ner pipe need run spacy lxc container run python allow multi thread training core authorized run container run others run actually start decrease minute would really like improve core usage idea look could problem producer consumer env spacy version location root env lib python site package spacy platform linux xxxx grs ipv x debian buster sid python version
"Named Entity Recognition on Spacy for product category like phones , vehicles not recognized","<p>The description of NER on spcay implies it has a classified entity as Product ( vehicles, food etc) but they are not recognized at all. Any idea if I am missing anything. I have tried giving simple sentences like.
sample_String = "" I use a car"", "" I use mobile phone"", "" I eat rice for lunch"" etc</p>

<pre><code>import spacy
import en_core_web_md
nlp = en_core_web_md.load()
a = nlp("" I eat rice for lunch"")
for ent in a.ents:
    print(a.text, a.start_char, a.end_char, a.label_)
</code></pre>

<p>Output was blank ( no print executed )</p>
",Named Entity Recognition (NER),named entity recognition spacy product category like phone vehicle recognized description ner spcay implies ha classified entity product vehicle food etc recognized idea missing anything tried giving simple sentence like sample string use car use mobile phone eat rice lunch etc output wa blank print executed
Storing unstructured data for sentiment analysis,"<p>I am doing an NLP term project and am analyzing over 100,000 news articles from this corpus. <a href=""https://github.com/philipperemy/financial-news-dataset"" rel=""nofollow noreferrer"">https://github.com/philipperemy/financial-news-dataset</a></p>

<p>I am looking to perform sentiment analysis on this dataset using NLTK. However, I am a bit confused about how this pipeline should look for storing and accessing all of these articles.</p>

<p>The articles are text files that I read and perform some preprocessing on in order to extract some metadata and extract the main article text. Currently, I am storing the data from each article in a python object such as this:</p>

<pre><code>{
   'title' : title,
   'author' : author,
   'date' : date,
   'text' : text,
}
</code></pre>

<p>I would like to store these objects in a database so I don't have to read all of these files every time I want to do analysis. My problem is, I'm not really sure which database to use. I want to be able to use regexes on certain fields such as date and title so I can isolate documents by date and company names. I was thinking of going the NoSql route and using a DB like MongoDb or CouchDB or maybe even a search engine such as ElasticSearch. </p>

<p>After I query for the documents I want to use for analysis, I will tokenize the text, POS tag it, and perform NER using NLTK. I have already implemented this part of the pipeline. Is it smart to do this after the database is already indexed in the database? Or should I look at storing the processed data in the database well? </p>

<p>Finally, I will use this processed data to classify each article, using a trained model I've already developed. I already have a gold standard, so I will compare the classification against the gold standard.</p>

<p>Does this pipeline generally look correct? I don't have much experience with using large datasets like this.</p>
",Named Entity Recognition (NER),storing unstructured data sentiment analysis nlp term project analyzing news article corpus looking perform sentiment analysis dataset using nltk however bit confused pipeline look storing accessing article article text file read perform preprocessing order extract metadata extract main article text currently storing data article python object would like store object database read file every time want analysis problem really sure database use want able use regexes certain field date title isolate document date company name wa thinking going nosql route using db like mongodb couchdb maybe even search engine elasticsearch query document want use analysis tokenize text po tag perform ner using nltk already implemented part pipeline smart database already indexed database look storing processed data database well finally use processed data classify article using trained model already developed already gold standard compare classification gold standard doe pipeline generally look correct much experience using large datasets like
Is it possible to train Stanford NER system to recognize more named entities types?,"<p>I'm using some NLP libraries now, (stanford and nltk) 
Stanford I saw the demo part but just want to ask if it possible to use it to identify more entity types.</p>

<p>So currently stanford NER system (as the demo shows) can recognize entities as person(name), organization or location. But the organizations recognized are limited to universities or some, big organizations. I'm wondering if I can use its API to write program for more entity types, like if my input is ""Apple"" or  ""Square"" it can recognize it as a company.</p>

<p>Do I have to make my own training dataset?</p>

<p>Further more, if I ever want to extract entities and their relationships between each other, I feel I should use the stanford dependency parser.
I mean, extract first the named entities and other parts tagged as ""noun"" and find relations between them.</p>

<p>Am I correct.</p>

<p>Thanks.</p>
",Named Entity Recognition (NER),possible train stanford ner system recognize named entity type using nlp library stanford nltk stanford saw demo part want ask possible use identify entity type currently stanford ner system demo show recognize entity person name organization location organization recognized limited university big organization wondering use api write program entity type like input apple square recognize company make training dataset ever want extract entity relationship feel use stanford dependency parser mean extract first named entity part tagged noun find relation correct thanks
Using spacy&#39;s displacy REST microservices on RHEL,"<p>I need to use Spacy services (<a href=""https://github.com/explosion/spacy-services"" rel=""nofollow noreferrer"">https://github.com/explosion/spacy-services</a>) for named entity recognition making use of its ent POST request. </p>

<p>Need to know how can I get the service cloned from github started on my localhost on RHEL box so as to make use of something like <a href=""http://localhost:8000/ent"" rel=""nofollow noreferrer"">http://localhost:8000/ent</a> to post my requests.</p>
",Named Entity Recognition (NER),using spacy displacy rest microservices rhel need use spacy service named entity recognition making use ent post request need know get service cloned github started localhost rhel box make use something like post request
Watson Knowledge Studio annotating addresses,"<p>I am trying to create a machine learning model in WKS and am currently in the process of annotating documents. I want the model to extract address entities. My broader goal is to understand an author's intent to switch their mailing address from an old address to a new one. The challenge is that there will be two or more mentions of an address in the text and the model needs to distinguish between the two.
I have seen examples where each piece of the address is treated as a discrete entity </p>

<p>I.E. </p>

<ul>
<li>[735] [Airport Rd], [Bismarck], [ND] [58504]
entities: street number, street name, city, state, zip</li>
</ul>

<p>-VS-</p>

<ul>
<li>treating the entire address as one entity
[735 Airport Rd, Bismarck, ND 58504]
entity: address</li>
</ul>

<p>the reason I would want to treat the entire address as one entity is because I need the model to distinguish between the <strong>old address</strong> and the <strong>new address</strong> I believe if I treat an address as one entity then I can use the relationship between the identifying clause such as: </p>

<ul>
<li>new address: [new_address] or, the new address is [new_address]</li>
</ul>

<p>Has anyone tried to do something similar in WKS or with another NLP tool?
Is it possible to treat each piece of the address as an entity and define a relationship between each piece of the address and old_address/new_address respectively?</p>
",Named Entity Recognition (NER),watson knowledge studio annotating address trying create machine learning model wks currently process annotating document want model extract address entity broader goal understand author intent switch mailing address old address new one challenge two mention address text model need distinguish two seen example piece address treated discrete entity e airport rd bismarck nd entity street number street name city state zip v treating entire address one entity airport rd bismarck nd entity address reason would want treat entire address one entity need model distinguish old address new address believe treat address one entity use relationship identifying clause new address new address new address new address ha anyone tried something similar wks another nlp tool possible treat piece address entity define relationship piece address old address new address respectively
"Difference between NER, NERC and NEL","<p>What is the difference between named entity recognition,  named entity recognition and classification, named entity linking? Would appreciate a practical example.</p>
",Named Entity Recognition (NER),difference ner nerc nel difference named entity recognition named entity recognition classification named entity linking would appreciate practical example
Extract information from text,"<p>I have the following text:</p>

<pre><code>Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.              

Name                                 Group                       12345678        
ALEX A ALEX                                                                   
ID#                                  PUBLIC NETWORK                  
XYZ123456789                                                                  


Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.
</code></pre>

<p>I want to extract the ID value which is located under the ID# keyword in the text.</p>

<p>The issue is that in different text files <code>ID</code> can be located at the different location, for example in the middle of another text, like this:</p>

<pre><code>Lorem Ipsum is simply dummy text of                                          ID#             the printing and typesetting industry. Lorem Ipsum has been the industry's          
standard dummy text ever since the 1500s, when an unknown printer took a     XYZ123456789    galley of type and scrambled it to make a type specimen book.       
</code></pre>

<p>Also, can have extra lines between <code>ID#</code> and value:</p>

<pre><code>Lorem Ipsum is simply dummy text of                                          ID#             the printing and typesetting industry. Lorem Ipsum has been the industry's      
printing and typesetting industry. Lorem Ipsum has been the                                  printing and typesetting industry. Lorem Ipsum has been the 
standard dummy text ever since the 1500s, when an unknown printer took a     XYZ123456789    galley of type and scrambled it to make a type specimen book.
</code></pre>

<p>Could you please show an approach how the mentioned <code>ID#</code> value can be extracted? Is there any standard technic that can be applied here in order to extract this information? For example RegEx or some approach on the top of RegEx . Is it possible to apply NLP here?</p>
",Named Entity Recognition (NER),extract information text following text want extract id value located id keyword text issue different text file located different location example middle another text like also extra line value could please show approach mentioned value extracted standard technic applied order extract information example regex approach top regex possible apply nlp
Annotated Training data for NER corpus,"<p>It is mentioned in the documentation of opennlp that we've to train our model with 15000 line for a good performance. 
now, I've to extract different entities from the document which means I've to add different tags for many tokens in the training data(15000 lines) which will take a lot of time. Is there any other way to do this? which will reduce the time or any other method which I can proceed.</p>

<p>Thanks.</p>
",Named Entity Recognition (NER),annotated training data ner corpus mentioned documentation opennlp train model line good performance extract different entity document mean add different tag many token training data line take lot time way reduce time method proceed thanks
What text processing tool is recommended for parsing screenplays?,"<p>I have some plain-text kinda-structured screenplays, formatted like the example at the end of this post. I would like to parse each into some format where: </p>

<ul>
<li>It will be easy to pull up just stage directions that deal with a specific place.</li>
<li>It will be easy to pull up just dialogue belonging to a particular character.</li>
</ul>

<p>The most obvious approach I can think of is using <code>sed</code> or <code>perl</code> or <code>php</code> to put div tags around each block, with classes representing character, location, and whether it's stage directions or dialogue. Then, open it up as a web-page and use jQuery to pull out whatever I'm interested in. But this sounds like a roundabout way to do it and maybe it only seems like a good idea because these are tools I'm accustomed to. But I'm sure this is a recurring problem that's been solved before, so can anybody recommend a more efficient workflow that can be used on a Linux box? Thanks.</p>

<p>Here is some sample input:</p>

<pre><code>      SOMEWHERE CORPORATION - OPTIONAL COMMENT
      A guy named BOB is sitting at his computer.

                             BOB
                Mmmm. Stackoverflow. I like.

      Footsteps are heard approaching.

                             ALICE
                Where's that report you said you'd have for me?

      Closeup of clock ticking.

                             BOB (looking up)
                Huh? What?

                             ALICE
                Some more dialogue.

      Some more stage directions.
</code></pre>

<p>Here is what sample output might look like:</p>

<pre><code>      &lt;div class='scene somewhere_corporation'&gt;
       &lt;div class='comment'&gt;OPTIONAL COMMENT&lt;/div&gt;
       &lt;div class='direction'&gt;A guy named BOB is sitting at his computer.&lt;/div&gt;
       &lt;div class='dialogue bob'&gt;Mmmm. Stackoverflow. I like.&lt;/div&gt;
       &lt;div class='direction'&gt;Footsteps are heard approaching.&lt;/div&gt;
       &lt;div class='dialogue alice'&gt;Where's that report you said you'd have for me?&lt;/div&gt;
       &lt;div class='direction'&gt;Closeup of clock ticking.&lt;/div&gt;
       &lt;div class='comment bob'&gt;looking up&lt;/div&gt;
       &lt;div class='dialogue bob'&gt;Huh? What?&lt;/div&gt;
       &lt;div class='dialogue alice'&gt;Some more dialogue.&lt;/div&gt;
       &lt;div class='direction'&gt;Some more stage directions.&lt;/div&gt;
      &lt;/div&gt;
</code></pre>

<p>I'm using DOM as an example, but again, only because that's something I understand. I'm open to whatever is considered a best practice for this type of text-processing task if, as I suspect, roll-your-own regexps and jQuery is not the best practice. Thanks.</p>
",Named Entity Recognition (NER),text processing tool recommended parsing screenplay plain text kinda structured screenplay formatted like example end post would like parse format easy pull stage direction deal specific place easy pull dialogue belonging particular character obvious approach think using put div tag around block class representing character location whether stage direction dialogue open web page use jquery pull whatever interested sound like roundabout way maybe seems like good idea tool accustomed sure recurring problem solved anybody recommend efficient workflow used linux box thanks sample input sample output might look like using dom example something understand open whatever considered best practice type text processing task suspect roll regexps jquery best practice thanks
Difference between IOB Accuracy and Precision,"<p>I'm doing some works on NLTK with named entity recognition and chunkers. I retrained a classifier using <code>nltk/chunk/named_entity.py</code> for that and I got the following mesures:</p>

<pre><code>ChunkParse score:
    IOB Accuracy:  96.5%
    Precision:     78.0%
    Recall:        91.9%
    F-Measure:     84.4%
</code></pre>

<p>But I don't understand what is the exact difference between IOB Accuracy and Precision in this case. Actually, I found on the docs (<a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html"" rel=""nofollow"">here</a>) the following for an specific example:</p>

<blockquote>
  <p>The IOB tag accuracy indicates that more than a third of the words are
  tagged with O, i.e. not in an NP chunk. However, since our tagger did
  not find any chunks, its precision, recall, and f-measure are all
  zero.</p>
</blockquote>

<p>So, if IOB accuracy is just the number of O labels, how come we don't have chunks and IOB accuracy is not 100% at the same time, in that example?</p>

<p>Thank you in advance</p>
",Named Entity Recognition (NER),difference iob accuracy precision work nltk named entity recognition chunkers retrained classifier using got following mesures understand exact difference iob accuracy precision case actually found doc following specific example iob tag accuracy indicates third word tagged e np chunk however since tagger find chunk precision recall f measure zero iob accuracy number label come chunk iob accuracy time example thank advance
Getting tag association in KNIME,"<p>KNIME comes with several native nodes for performing different tagging tasks, like POS tagging or named entity recognition. In order to use the identified tags or terms, you can use the Bag of Words node, which produces <em>terms</em> (not words) and associated tags. However, this approach does not detail which tag is associated to each <em>word</em>, and neither the order of the tags (or words).</p>

<p>Therefore, if you want to extract features like 'POS tags +/- N words with respect to the actual word' (eg. a words window), how can you?</p>

<p>For example, for 'That city was New York', I would like KNIME to produce an ordered list like:
      
(where the last NN would be a named entity).</p>
",Named Entity Recognition (NER),getting tag association knime knime come several native node performing different tagging task like po tagging named entity recognition order use identified tag term use bag word node produce term word associated tag however approach doe detail tag associated word neither order tag word therefore want extract feature like po tag n word respect actual word eg word window example city wa new york would like knime produce ordered list like last nn would named entity
extracting n-grams from tweets in python,"<p>Say that I have 100 tweets.<br/>
In those tweets, I need to extract: 1) food names, and 2) beverage names.<br/>
<br/></p>

<p><strong>Example of tweet:</strong><br/></p>

<blockquote>
  <p>""Yesterday I had a coca cola, and a hot dog for lunch, and some bana split for desert. I liked the coke, but the banana in the banana split dessert was ripe""</p>
</blockquote>

<p>I have to my disposal two lexicons. One with food names, and one with beverage names.<br/></p>

<p>Example in food names lexicon:<br/>
""hot dog""<br/>
""banana""<br/>
""banana split""<br/></p>

<p>Example in beverage names lexicon:<br/>
""coke""<br/>
""cola""<br/>
""coca cola""<br/>
<br/></p>

<p><strong>What I should be able to extract:</strong><br/></p>

<blockquote>
  <p>[[[""coca cola"", ""beverage""], [""hot dog"", ""food""], [""banana split"", ""food""]],<br>
  [[""coke"", ""beverage""], [""banana"", ""food""], [""banana split"", ""food""]]]</p>
</blockquote>

<p>The names in the lexicons can be 1-5 word(s) long. How do I go about extracting n-grams from the tweets, using my lexicons?</p>
",Named Entity Recognition (NER),extracting n gram tweet python say tweet tweet need extract food name beverage name example tweet yesterday coca cola hot dog lunch bana split desert liked coke banana banana split dessert wa ripe disposal two lexicon one food name one beverage name example food name lexicon hot dog banana banana split example beverage name lexicon coke cola coca cola able extract coca cola beverage hot dog food banana split food coke beverage banana food banana split food name lexicon word long go extracting n gram tweet using lexicon
Extract word from a list of synsets in NLTK for Python,"<p>Using this <code>[x for x in wn.all_synsets('n')]</code> I am able to get a list <code>allnouns</code> with all nouns from Wordnet with help from NLTK.</p>

<p>The list <code>allnouns</code> looks like this <code>Synset('pile.n.01'), Synset('compost_heap.n.01'), Synset('mass.n.03')</code> and so on. Now I am able to get any element by using <code>allnouns[2]</code> and this should be <code>Synset('mass.n.03')</code>. </p>

<p>I would like to extract only the word <em>mass</em> but for some reason I cannot treat it like a string and everything I try shows a <code>AttributeError: 'Synset' object has no attribute</code> or <code>TypeError: 'Synset' object is not subscriptable</code> or <code>&lt;bound method Synset.name of Synset('mass.n.03')&gt;</code> if I try to use .name or .pos</p>
",Named Entity Recognition (NER),extract word list synset nltk python using able get list noun wordnet help nltk list look like able get element using would like extract word mass reason treat like string everything try show try use name po
How to resume the NER training?,"<p>I am using Spacy 2.0.6 with an italian NER model.
I would like to add samples to that model to improve accuracy. What is the correct way?</p>

<p>At the beginning i have trained the model with this code:</p>

<pre><code>    with nlp.disable_pipes(*other_pipes):  # only train NER
        optimizer = nlp.begin_training()
        for itn in range(epochs):
            random.shuffle(train)
            losses = {}

            for batch in minibatch(train, size=32):
                docs, golds = zip(*batch)
                nlp.update(docs, golds, drop=.3, sgd=optimizer, losses=losses)
</code></pre>

<p>Unfortunately, It does not work for new samples.
I get an error on the <code>optimizer = nlp.begin_training()</code> line.</p>

<pre><code>optimizer = nlp.begin_training()
  File ""/home/damiano/lavoro/python/parser/.env/lib/python3.5/site-packages/spacy/language.py"", line 456, in begin_training
    sgd=self._optimizer)
  File ""nn_parser.pyx"", line 843, in spacy.syntax.nn_parser.Parser.begin_training
KeyError: 'token_vector_width'
</code></pre>

<p>How can i ""resume"" the training of a NER model?</p>
",Named Entity Recognition (NER),resume ner training using spacy italian ner model would like add sample model improve accuracy correct way beginning trained model code unfortunately doe work new sample get error line resume training ner model
Save spacy`s NER model after every iteration,"<p>I am trying to save to Spacy custom NER model after every iteration. Do we have any API similar to the ones in tensorflow to save model weights after every/certain no. of iterations. And then I can reload that saved model and continue training from there.</p>

<p>Also how can I utilise all the cores on my system in linux. I find that only two cores out of four are being utilised. They use Multi-task CNN for NER, which I know would take more time to re-train on CPU. Also other ways to speed up the NER model training.</p>

<pre><code>@plac.annotations(
    model=(""Model name. Defaults to blank 'en' model."", ""option"", ""m"", str),
    output_dir=(""Optional output directory"", ""option"", ""o"", Path),
    n_iter=(""Number of training iterations"", ""option"", ""n"", int))
def main(model=None, output_dir=None, n_iter=100):
    """"""Load the model, set up the pipeline and train the entity recognizer.""""""
    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank('en')  # create blank Language class
        print(""Created blank 'en' model"")

    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)
    # otherwise, get it so we can add labels
    else:
        ner = nlp.get_pipe('ner')

    # add labels
    for _, annotations in TRAIN_DATA:
        for ent in annotations.get('entities'):
            ner.add_label(ent[2])

    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        optimizer = nlp.begin_training()
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            losses = {}
            for text, annotations in TRAIN_DATA:
                nlp.update(
                    [text],  # batch of texts
                    [annotations],  # batch of annotations
                    drop=0.5,  # dropout - make it harder to memorise data
                    sgd=optimizer,  # callable to update weights
                    losses=losses)
            print(losses)

    # save model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.to_disk(output_dir)
        print(""Saved model to"", output_dir)

if __name__ == '__main__':
    plac.call(main)
</code></pre>
",Named Entity Recognition (NER),save spacy ner model every iteration trying save spacy custom ner model every iteration api similar one tensorflow save model weight every certain iteration reload saved model continue training also utilise core system linux find two core four utilised use multi task cnn ner know would take time train cpu also way speed ner model training
spaCy 2.0: Save and Load a Custom NER model,"<p>I've trained a custom NER model in spaCy with a custom tokenizer. I'd like to save the NER model without the tokenizer. I tried the following code with I found in the spaCy support forum:</p>

<pre><code>import spacy

nlp = spacy.load(""en"")
nlp.tokenizer = some_custom_tokenizer
# Train the NER model...
nlp.tokenizer = None
nlp.to_disk('/tmp/my_model', disable=['tokenizer'])
</code></pre>

<p>When I try to load it, the pipeline is empty, and surprisingly, is has the default spaCy tokenizer.</p>

<pre><code>nlp = spacy.blank('en').from_disk('/tmp/model', disable=['tokenizer'])
</code></pre>

<p>Any idea how can I load the model without the tokenizer, but get the full pipeline? thanks</p>
",Named Entity Recognition (NER),spacy save load custom ner model trained custom ner model spacy custom tokenizer like save ner model without tokenizer tried following code found spacy support forum try load pipeline empty surprisingly ha default spacy tokenizer idea load model without tokenizer get full pipeline thanks
Extracting information from sentence : NER or other ways?,"<p>What I'm trying to do now is extracting 'customers'names' from a firm's disclosure text.</p>
<p>What I have done up to now stated as below:</p>
<ol>
<li>Classify every sentences in disclosure data whether it contains information about its customers or not by machine learning(1 if it contains customer data, 0 if not)</li>
</ol>
<p>So, I got sentences that marked as 1 and below are the examples of those sentences.</p>
<blockquote>
<p>*Sentence no.1&gt;
For the fiscal year ended on December 31, 2008, FAW Jiefang Automotive
Co., Ltd. Dongfeng Axle Co., Ltd. ShiYan Automobile Works and FAW
Qingdao Automobile Works accounted for approximately 6.5%, 6.0% and
5.3% of total sales revenue, respectively.</p>
<p>*Sentence No 2.&gt;
;;;;;PRINCIPAL CUSTOMERS In fiscal 2004, the Company
derived approximately 38% ($14,706,748) of its consolidated revenues
from continuing operations from direct transactions with The Home
Depot, Inc.</p>
<p>*sentence No 3.&gt;
;;;;;Delphi Corporation is the Rubber Groups largest customer.</p>
</blockquote>
<p>I want to extract &quot;<strong>FAW Jiefang Automotive, Co., Ltd. Dongfeng Axle Co., Ltd. ShiYan Automobile Works and FAW Qingdao Automobile Works</strong>&quot; from sentence 1, 'T<strong>he Home Depot</strong>' from sentence 2, and <strong>'Delphi'</strong> from sentence 3. (But not the words like Rubber Groups from sentence 3 since it's not a customer's name)</p>
<p>Basically, I thought that this is similar to the <em><strong>NER problem (Named Entity Recognition)</strong></em>. So I tagged every words in those sentences.</p>
<p>For example,</p>
<blockquote>
<p>sentence 1&gt; [('For', 'IN', 'O'), ('the', 'DT', 'O'), ('fiscal', 'JJ',
'O'), ('year', 'NN', 'O'), ('ended', 'VBN', 'O'), ('on', 'IN', 'O'),
('December', 'NNP', 'O'), ('31', 'CD', 'O'), (',', ',', 'O'), ('2008',
'CD', 'O'), (',', ',', 'O'), <strong>('FAW', 'NNP', 'B-ORGANIZATION'),
('Jiefang', 'NNP', 'I-ORGANIZATION'), ('Automotive', 'NNP',
'I-ORGANIZATION'), ('Co.', 'NNP', 'O'), (',', ',', 'O'), ('Ltd.',
'NNP', 'O'), ('Dongfeng', 'NNP', 'B-ORGANIZATION'), ('Axle', 'NNP',
'I-ORGANIZATION'), ('Co.', 'NNP', 'O'), (',', ',', 'O'), ('Ltd.',
'NNP', 'O'), ('ShiYan', 'NNP', 'B-ORGANIZATION'), ('Automobile',
'NNP', 'I-ORGANIZATION'), ('Works', 'NNP', 'I-ORGANIZATION'), ('and',
'CC', 'O'), ('FAW', 'NNP', 'B-ORGANIZATION'), ('Qingdao', 'NNP',
'I-ORGANIZATION'), ('Automobile', 'NNP', 'I-ORGANIZATION'), ('Works',
'NNP', 'I-ORGANIZATION')</strong>, ('accounted', 'VBD', 'O'), ('for', 'IN',
'O'), ('approximately', 'RB', 'O'), ('6.5', 'CD', 'O'), ('%', 'NN',
'O'), (',', ',', 'O'), ('6.0', 'CD', 'O'), ('%', 'NN', 'O'), ('and',
'CC', 'O'), ('5.3', 'CD', 'O'), ('%', 'NN', 'O'), ('of', 'IN', 'O'),
('total', 'JJ', 'O'), ('sales', 'NNS', 'O'), ('revenue', 'NN', 'O'),
(',', ',', 'O'), ('respectively', 'RB', 'O'), ('.', '.', 'O')]</p>
<p>Sentence no2&gt; [(';', ':', 'O'), (';', ':', 'O'), (';', ':', 'O'),
(';', ':', 'O'), (';', ':', 'O'), ('PRINCIPAL', 'NNP', 'O'),
('CUSTOMERS', 'NNPS', 'O'), ('In', 'IN', 'O'), ('fiscal', 'JJ', 'O'),
('2004', 'CD', 'O'), (',', ',', 'O'), ('the', 'DT', 'O'), ('Company',
'NNP', 'O'), ('derived', 'VBD', 'O'), ('approximately', 'RB', 'O'),
('38', 'CD', 'O'), ('%', 'NN', 'O'), ('(', '(', 'O'), ('$', '$', 'O'),
('14,706,748', 'CD', 'O'), (')', ')', 'O'), ('of', 'IN', 'O'), ('its',
'PRP$', 'O'), ('consolidated', 'JJ', 'O'), ('revenues', 'NNS', 'O'),
('from', 'IN', 'O'), ('continuing', 'VBG', 'O'), ('operations', 'NNS',
'O'), ('from', 'IN', 'O'), ('direct', 'JJ', 'O'), ('transactions',
'NNS', 'O'), ('with', 'IN', 'O'), ('The', 'DT', 'O'), <strong>('Home', 'NNP',
'B-ORGANIZATION'), ('Depot', 'NNP', 'I-ORGANIZATION')</strong>, (',', ',',
'O'), ('Inc', 'NNP', 'O'), ('.', '.', 'O')]</p>
<p>Sentence no3
[(';', ':', 'O'), (';', ':', 'O'), (';', ':', 'O'), (';', ':', 'O'),
(';', ':', 'O'), <strong>('Delphi', 'NNP', 'B-ORGANIZATION')</strong>, ('Corporation',
'NNP', 'O'), ('is', 'VBZ', 'O'), ('the', 'DT', 'O'), ('Rubber', 'NNP',
'O'), ('Groups', 'NNP', 'O'), ('largest', 'JJS', 'O'), ('customer',
'NN', 'O'), ('.', '.', 'O')]</p>
</blockquote>
<p>And I trained based on ;
those tagged data (about 3700 setences) + basic features used on NER classification (<strong>previous and next 5 words, the shape of words (upper, title, contains digit etc,), previous words IOB tag</strong>)</p>
<p>I tried almost every machine learning techniques including MLP, however, the classification is far from accurate.</p>
<p>I think this is because, the word classified as customer according to my tag rule, not only affected by some previous and next n words but by the type and the structure of whole sentence. For example, from sentence 1, for FAW Jiefang Automotive Co to be tagged as 'ORGANIZATION' the machine (computer) should notice that it belongs to the 'Subject chunk' which is followed by 'accounted for ~ of revenue' that is a very important feature for the classification decision.</p>
<p><strong>To sum up, I am looking for a NER machine learning techniques that can deal with the problem (NER classification that can also deal with the feature of whole sentence) stated above.</strong></p>
<p>Or would it be wiser to use other ways instead of NER?</p>
",Named Entity Recognition (NER),extracting information sentence ner way trying extracting customer name firm disclosure text done stated classify every sentence disclosure data whether contains information customer machine learning contains customer data got sentence marked example sentence sentence fiscal year ended december faw jiefang automotive co ltd dongfeng axle co ltd shiyan automobile work faw qingdao automobile work accounted approximately total sale revenue respectively sentence principal customer fiscal company derived approximately consolidated revenue continuing operation direct transaction home depot inc sentence delphi corporation rubber group largest customer want extract faw jiefang automotive co ltd dongfeng axle co ltd shiyan automobile work faw qingdao automobile work sentence home depot sentence delphi sentence word like rubber group sentence since customer name basically thought similar ner problem named entity recognition tagged every word sentence example sentence dt fiscal jj year nn ended vbn december nnp cd cd faw nnp b organization jiefang nnp organization automotive nnp organization co nnp ltd nnp dongfeng nnp b organization axle nnp organization co nnp ltd nnp shiyan nnp b organization automobile nnp organization work nnp organization cc faw nnp b organization qingdao nnp organization automobile nnp organization work nnp organization accounted vbd approximately rb cd nn cd nn cc cd nn total jj sale nns revenue nn respectively rb sentence principal nnp customer nnps fiscal jj cd dt company nnp derived vbd approximately rb cd nn cd prp consolidated jj revenue nns continuing vbg operation nns direct jj transaction nns dt home nnp b organization depot nnp organization inc nnp sentence delphi nnp b organization corporation nnp vbz dt rubber nnp group nnp largest jjs customer nn trained based tagged data setences basic feature used ner classification previous next word shape word upper title contains digit etc previous word iob tag tried almost every machine learning technique including mlp however classification far accurate think word classified customer according tag rule affected previous next n word type structure whole sentence example sentence faw jiefang automotive co tagged organization machine computer notice belongs subject chunk followed accounted revenue important feature classification decision sum looking ner machine learning technique deal problem ner classification also deal feature whole sentence stated would wiser use way instead ner
Multi-term named entities in Stanford Named Entity Recognizer,"<p>I'm using the Stanford Named Entity Recognizer <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"">http://nlp.stanford.edu/software/CRF-NER.shtml</a> and it's working fine. This is</p>

<pre><code>    List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(text);
    for (List&lt;CoreLabel&gt; sentence : out) {
        for (CoreLabel word : sentence) {
            if (!StringUtils.equals(word.get(AnswerAnnotation.class), ""O"")) {
                namedEntities.add(word.word().trim());           
            }
        }
    }
</code></pre>

<p>However the problem I'm finding is identifying names and surnames. If the recognizer encounters ""Joe Smith"", it is returning ""Joe"" and ""Smith"" separately. I'd really like it to return ""Joe Smith"" as one term. </p>

<p>Could this be achieved through the recognizer maybe through a configuration? I didn't find anything in the javadoc till now. </p>

<p>Thanks!</p>
",Named Entity Recognition (NER),multi term named entity stanford named entity recognizer using stanford named entity recognizer could achieved recognizer maybe configuration find anything javadoc till thanks
Resume Parsing using Solr and TIKA,"<p>I was going through this <a href=""https://www.slideshare.net/lucenerevolution/kang-laura-using-solr-to-find-the-right-person-for-the-right-job"" rel=""nofollow noreferrer"">slide</a>. I'm getting little difficulty in understanding the approach. </p>

<p>My two queries are:</p>

<ol>
<li>How does <code>Solr</code> maintain schema of <code>semi-structured document</code> like
resumes (such as Name, Skills, Education etc)</li>
<li>Can <code>Apache TIKA</code> extract the section wise information from PDFs? Since every resume would have dissimilar sections, how do I define a
common schema of entities?</li>
</ol>
",Named Entity Recognition (NER),resume parsing using solr tika wa going slide getting little difficulty understanding approach two query doe maintain schema like resume name skill education etc extract section wise information pdfs since every resume would dissimilar section define common schema entity
Tools/methods to extract math expressions from plain unstructured text,"<p>I need to identify math expressions embedded in text. For example ""Write down the value of A + B."" Here 'A + B' is the part I need to extract and it is not expressed using any structured text like Latex, but in plain unstructured text.</p>

<p>Are there any tools/methods to extract such expressions in text, like a POS tagger or a Name Entity recognition tool for math expressions? </p>

<p>First I tried to do this using regular expressions, but that is not successful for complex expressions. I found many research papers to extract math expressions from scanned documents ( from an image) or from structured text, but I found nothing for plain text. </p>
",Named Entity Recognition (NER),tool method extract math expression plain unstructured text need identify math expression embedded text example write value b b part need extract expressed using structured text like latex plain unstructured text tool method extract expression text like po tagger name entity recognition tool math expression first tried using regular expression successful complex expression found many research paper extract math expression scanned document image structured text found nothing plain text
How to detect statements made about entities using NLP,"<p>I want to research using NLP to detect negative/non-constructive comments, i.e. those that frequently arise when discussing politics online. I am curious to know that if given a sentence like this:</p>

<blockquote>
  <p>You're a liberal dweeb. Clinton is ruining the US with her inappropriate behavior as president.</p>
</blockquote>

<p>Whether it's possible to not only deduce the entities (you, Clinton) using NER but also get a tree of the statements made about each entity:</p>

<pre><code>+-----------------+                             +------------------------+
|                 |                             |                        |
|                 |                             |                        |
|       you       |                             |          Clinton       |
|                 +------+                      |                        +------+
|                 |      |                      |                        |      |
+--+--------------+      |                      |                        |      |
   |                     |                      +--+---------------------+      |
   |                     |                         |                            |
   |                     |                         |                            |
 +-+-------+        +----+-----+                   |                  +---------+----------+
 |         |        |          |              +----+---------+        |                    |
 |         |        |   dweeb  |              |              |        |                    |
 |  liberal|        |          |              |  ruining US  |        | has inappropriate  |
 |         |        +----------+              |              |        | behavior as pres.  |
 +---------+                                  |              |        |                    |
                                              +--------------+        +--------------------+
</code></pre>

<p>Is something like this possible with NLP?</p>
",Named Entity Recognition (NER),detect statement made entity using nlp want research using nlp detect negative non constructive comment e frequently arise discussing politics online curious know given sentence like liberal dweeb clinton ruining u inappropriate behavior president whether possible deduce entity clinton using ner also get tree statement made entity something like possible nlp
How to train a machine to label individual words in a text,"<p>For a text (say):</p>

<p>""I am leaving India today. I am headed to USA for a week.""
""I am travelling from India to USA""</p>

<p>I need to train the machine to label USA as ""Destination"" and India as ""Source""</p>

<p>I am using SpaCy's NER to extract the locations.</p>

<p>How should I proceed to create a training set and train it. What would be my feature vector and label vector?</p>
",Named Entity Recognition (NER),train machine label individual word text text say leaving india today headed usa week travelling india usa need train machine label usa destination india source using spacy ner extract location proceed create training set train would feature vector label vector
ignore certain word from gate nlp NER,"<p>I want to add ignore.lst in gate annie gazetteer so that the word in that list does not show while using NE. I see stop.lst inside annie gazetteer. What is the use of stop.lst for? I created ignore.lst and added it to list.def. How to make gate nlp not show names contain in ignore.lst?</p>
",Named Entity Recognition (NER),ignore certain word gate nlp ner want add ignore lst gate annie gazetteer word list doe show using ne see stop lst inside annie gazetteer use stop lst created ignore lst added list def make gate nlp show name contain ignore lst
Predefined Multilable Text Classification,"<p>Friends,
We are trying work on a problem where we have a dump of only reviews but there is no rating in a .csv file. Each row in .csv is one review given by customer of a particular product, lets a TV.</p>

<p>Here, I wanted to do classification of that text into below pre-defined category given by the domain expert of that products:</p>

<ul>
<li>Quality</li>
<li>Customer</li>
<li>Support</li>
<li>Positive Feedback</li>
<li>Price</li>
<li>Technology</li>
</ul>

<p>Some reviews are as below:</p>

<ol>
<li>Bought this product recently, feeling a great product in the market.</li>
<li>Was waiting for this product since long, but disappointed</li>
<li>The built quality is not that great</li>
<li>LED screen is picture perfect. Love this product</li>
<li>Damm! bought this TV 2 months ago, guess what, screen showing a straight line, poor quality LED screen</li>
<li>This has very complicated options, documentation of this TV is not so user-friendly</li>
<li>I cannot use my smart device to connect to this TV. Simply does not work</li>
<li>Customer support is very poor. I don't recommend this </li>
<li>Works great. Great product</li>
</ol>

<p>Now, with above 10 reviews by 10 different customers, how do I categorize them into the given buckets (you can call multilabel classification or Named Entity Recognition or Information extraction with sentiment analysis or be it anything)</p>

<p>I tried all NLP word frequency counting related stuff (in R) and referred StanfordNLP (<a href=""https://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/CRF-NER.shtml</a>) and many more. But could not get a concrete solution.</p>

<p>Can anybody please guide me how can we tackle this problem? Thanks !!!</p>
",Named Entity Recognition (NER),predefined multilable text classification friend trying work problem dump review rating csv file row csv one review given customer particular product let tv wanted classification text pre defined category given domain expert product quality customer support positive feedback price technology review bought product recently feeling great product market wa waiting product since long disappointed built quality great led screen picture perfect love product damm bought tv month ago guess screen showing straight line poor quality led screen ha complicated option documentation tv user friendly use smart device connect tv simply doe work customer support poor recommend work great great product review different customer categorize given bucket call multilabel classification named entity recognition information extraction sentiment analysis anything tried nlp word frequency counting related stuff r referred stanfordnlp many could get concrete solution anybody please guide tackle problem thanks
How to group up NER tags in order to get data from sentence as a whole?,"<p>Through the CoreNLP library, upon calling <code>ner()</code> on a <code>CoreLabel</code> I receive a string indicating its named entity tag (such as <code>PERSON</code> or <code>DATE</code>).</p>

<p>However, I know of no way of comparing tokens in a sentence against each other. For example: (text of tokens surrounded in backticks)</p>

<pre><code>`Ellen` PERSON
`Wexler `PERSON
`,` O
`February` DATE
`9` DATE
`,` DATE
`2016` DATE
</code></pre>

<p><strong>Through CoreNLP, How do I group up the person tags in order to get the name <code>Ellen Wexler</code>? Or the date tags in order to get <code>February 9, 2016</code>, or another representation that I could eventually turn into a Date/Calendar object in Java?</strong> I have looked at the example given <a href=""https://stanfordnlp.github.io/CoreNLP/api.html"" rel=""nofollow noreferrer"">here</a>, however that only finds the ner tags for each individual core label. It does not provide me a way to group consecutive, identical ner tags together.</p>

<p><strong>What I have tried:</strong>
I have written a for loop that iterates over the sentence and finds X number of consecutive, identical ner tags (so if X is 2 and the ner tag is <code>PERSON</code>, it will find 2 consecutive PERSONs). In this scenario, that is <code>Ellen Wexler</code>. However, this breaks down when punctuation comes into play, as punctuation, depending on context, is given the ner tag of its adjacent tokens. In addition, there must be some way to do this through CoreNLP.</p>

<p><strong>My Resarch</strong>:
<a href=""https://stackoverflow.com/questions/45349957/merge-consecutive-tokens-with-same-ner-tag-in-corenlp-conll-output"">This</a> similar question has not been answered. The CoreNLP home page provides no answer, as it only provides an example regarding analysis of individual core labels/tokens. </p>
",Named Entity Recognition (NER),group ner tag order get data sentence whole corenlp library upon calling receive string indicating named entity tag however know way comparing token sentence example text token surrounded backticks corenlp group person tag order get name date tag order get another representation could eventually turn date calendar object java looked example given however find ner tag individual core label doe provide way group consecutive identical ner tag together tried written loop iterates sentence find x number consecutive identical ner tag x ner tag find consecutive person scenario however break punctuation come play punctuation depending context given ner tag adjacent token addition must way corenlp resarch href similar question ha answered corenlp home page provides answer provides example regarding analysis individual core label token p
How to add feature to the training data in Stanford NER CRF,"<p>How do I make a 3 column training data file using stanford ner crf? Their default training data is just 2 column, which is the tab separated (I've already trained and created a model with that).</p>

<p>What I want to is to make an experiment, what will be the result if I add one feature in the annotation?</p>

<p>For example like this:</p>

<p><code>John Capital NAME</code></p>

<p>John = Word, Capital = additionalFeature, NAME = Tag/Answer</p>
",Named Entity Recognition (NER),add feature training data stanford ner crf make column training data file using stanford ner crf default training data column tab separated already trained created model want make experiment result add one feature annotation example like john word capital additionalfeature name tag answer
Named Entity Recognition with Regular Expression: NLTK,"<p>I have been playing with NLTK toolkit. I come across this problem a lot and searched for solution online but nowhere I got a satisfying answer. So I am putting my query here. </p>

<p>Many times NER doesn't tag consecutive NNPs as one NE. I think editing the NER to use RegexpTagger also can improve the NER.</p>

<p>Example:</p>

<p>Input: </p>

<blockquote>
  <p>Barack Obama is a great person.</p>
</blockquote>

<p>Output:  </p>

<blockquote>
  <p>Tree('S', [Tree('PERSON', [('Barack', 'NNP')]), Tree('ORGANIZATION', [('Obama', 'NNP')]), ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('person', 'NN'), ('.', '.')])</p>
</blockquote>

<p>where as </p>

<p>input: </p>

<blockquote>
  <p>Former Vice President Dick Cheney told conservative radio host Laura Ingraham that he ""was honored"" to be compared to Darth Vader while in office.</p>
</blockquote>

<p>Output: </p>

<blockquote>
  <p>Tree('S', [('Former', 'JJ'), ('Vice', 'NNP'), ('President', 'NNP'), Tree('NE', [('Dick', 'NNP'), ('Cheney', 'NNP')]), ('told', 'VBD'), ('conservative', 'JJ'), ('radio', 'NN'), ('host', 'NN'), Tree('NE', [('Laura', 'NNP'), ('Ingraham', 'NNP')]), ('that', 'IN'), ('he', 'PRP'), ('<code>', '</code>'), ('was', 'VBD'), ('honored', 'VBN'), (""''"", ""''""), ('to', 'TO'), ('be', 'VB'), ('compared', 'VBN'), ('to', 'TO'), Tree('NE', [('Darth', 'NNP'), ('Vader', 'NNP')]), ('while', 'IN'), ('in', 'IN'), ('office', 'NN'), ('.', '.')])</p>
</blockquote>

<p>Here Vice/NNP, President/NNP, (Dick/NNP, Cheney/NNP) , is correctly extracted. </p>

<p>So I think if nltk.ne_chunk is used first and then if two consecutive trees are NNP there are high chances that both refers to one entity. </p>

<p>Any suggestion will be really appreciated. I am looking for flaws in my approach.</p>

<p>Thanks.</p>
",Named Entity Recognition (NER),named entity recognition regular expression nltk playing nltk toolkit come across problem lot searched solution online nowhere got satisfying answer putting query many time ner tag consecutive nnps one ne think editing ner use regexptagger also improve ner example input barack obama great person output tree tree person barack nnp tree organization obama nnp vbz dt great jj person nn input former vice president dick cheney told conservative radio host laura ingraham wa honored compared darth vader office output tree former jj vice nnp president nnp tree ne dick nnp cheney nnp told vbd conservative jj radio nn host nn tree ne laura nnp ingraham nnp prp wa vbd honored vbn vb compared vbn tree ne darth nnp vader nnp office nn vice nnp president nnp dick nnp cheney nnp correctly extracted think nltk ne chunk used first two consecutive tree nnp high chance refers one entity suggestion really appreciated looking flaw approach thanks
Multi-threading training for spacy in python,"<p>I trying to find a way to use multi-thread on spacy for training a NER model. It's look like multithread is used by default on my working computer (Ubuntu 16.04 Python3.5) but not on my server.</p>

<p>Any idea why?</p>

<p><strong>Info about spaCy &amp; env on server</strong></p>

<pre><code>Platform           Linux-3.14.32-xxxx-grs-ipv6-64-x86_64-with-Debian-8
Python version     3.4.2          
Location           /home/nlp/.env/lib/python3.4/site-packages/spacy
Models             fr, fr_core_news_md
spaCy version      2.0.5
</code></pre>

<p><strong>Process for try:</strong></p>

<p><strong>Installation</strong></p>

<pre><code>python3 -m venv .env
source .env/bin/activate
pip install -U spacy
pip3 install pip --upgrade
python -m spacy download fr
python -m spacy validate
</code></pre>

<p><strong>Script python3</strong></p>

<pre><code>import spacy
import random

ITERATION_NBR = 100
DROP_RATE = 0.5

TRAIN_DATA = [
    ('Who is Shaka Khan?', {
        'entities': [(7, 17, 'PERSON')]
    }),
    ('I like London and Berlin.', {
        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]
    })
]

def main():
    try:
        nlp = spacy.load(""fr"")
    except:
        nlp = spacy.load(""fr_core_news_sm"")
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)
    else:
        ner = nlp.get_pipe('ner')
    for _, annotations in TRAIN_DATA:
        for ent in annotations.get('entities'):
            ner.add_label(ent[2])
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):
        optimizer = nlp.begin_training()
        for itn in range(ITERATION_NBR):
            random.shuffle(TRAIN_DATA)
            losses = {}
            for text, annotations in TRAIN_DATA:
                nlp.update(
                    [text],
                    [annotations],
                    drop=DROP_RATE,
                    sgd=optimizer,
                    losses=losses)
</code></pre>

<p><strong>Execution</strong></p>

<pre><code>python3 &lt;scriptName&gt;.py
</code></pre>
",Named Entity Recognition (NER),multi threading training spacy python trying find way use multi thread spacy training ner model look like multithread used default working computer ubuntu python server idea info spacy env server process try installation script python execution
How to classify new documents with tf-idf?,"<p>If I use the <code>TfidfVectorizer</code> from <code>sklearn</code> to generate feature vectors as:</p>

<p><code>features = TfidfVectorizer(min_df=0.2, ngram_range=(1,3)).fit_transform(myDocuments)</code></p>

<p>How would I then generate feature vectors to classify a new document? Since you cant calculate the tf-idf for a single document. </p>

<p>Would it be a correct approach, to extract the feature names with:</p>

<p><code>feature_names = TfidfVectorizer.get_feature_names()</code></p>

<p>and then count the term frequency for the new document according to the <code>feature_names</code>?</p>

<p>But then I won't get the weights that have the information of a words importance.</p>
",Named Entity Recognition (NER),classify new document tf idf use generate feature vector would generate feature vector classify new document since cant calculate tf idf single document would correct approach extract feature name count term frequency new document according get weight information word importance
How does spacy use word embeddings for Named Entity Recognition (NER)?,"<p>I'm trying to train an NER model using <code>spaCy</code> to identify locations, (person) names, and organisations. I'm trying to understand how <code>spaCy</code> recognises entities in text and I've not been able to find an answer. From <a href=""https://github.com/explosion/spaCy/issues/491"" rel=""noreferrer"">this issue</a> on Github and <a href=""https://github.com/explosion/spaCy/blob/master/examples/training/train_ner_standalone.py"" rel=""noreferrer"">this example</a>, it appears that spaCy uses a number of features present in the text such as POS tags, prefixes, suffixes, and other character and word-based features in the text to train an Averaged Perceptron.</p>

<p>However, nowhere in the code does it appear that <code>spaCy</code> uses the GLoVe embeddings (although each word in the sentence/document appears to have them, if present in the GLoVe corpus).</p>

<p>My questions are - </p>

<ol>
<li>Are these used in the NER system now? </li>
<li>If I were to switch out the word vectors to a different set, should I expect performance to change in a meaningful way?</li>
<li>Where in the code can I find out how (if it all) <code>spaCy</code> is using the word vectors?</li>
</ol>

<p>I've tried looking through the Cython code, but I'm not able to understand whether the labelling system uses word embeddings.</p>
",Named Entity Recognition (NER),doe spacy use word embeddings named entity recognition ner trying train ner model using identify location person name organisation trying understand recognises entity text able find answer issue github example appears spacy us number feature present text po tag prefix suffix character word based feature text train averaged perceptron however nowhere code doe appear us glove embeddings although word sentence document appears present glove corpus question used ner system switch word vector different set expect performance change meaningful way code find using word vector tried looking cython code able understand whether labelling system us word embeddings
openNLP - Name Finder Training for Addresses,"<p>I am trying to isolate postal addresses from CVs (curriculum vitae).  The CVs are from many different countries so have no standard layout, formats, rhyme or reason to the addresses.</p>

<p>I have my raw data which has been segmented into sentences and tokens and is ready for markup.</p>

<p>Questions:</p>

<p>Whist City/town is of primary interest to me, should I mark up the entire address for best results?</p>

<pre><code>eg blah blah blah &lt;START:location&gt;1 Stack Avenue, London, SE1 KTB&lt;END&gt; blah blah
eg blah blah blah &lt;START:location&gt;Hoch Strasse 21, Berlin 17009, Germany&lt;END&gt; blah blah
</code></pre>

<p>Given that the address I seek mainly appear in the top quarter of a CV, should I trim the training data to that 25% and do the same with the live data or will I get better results by keeping the documents as a whole and just tagging the bit I need?</p>

<p>Finally any ideas on the level of success I'm likely to have finding addresses from none structured documents?</p>

<p>Advice, help and alternative methods greatly appreciated.  </p>
",Named Entity Recognition (NER),opennlp name finder training address trying isolate postal address cv curriculum vitae cv many different country standard layout format rhyme reason address raw data ha segmented sentence token ready markup question whist city town primary interest mark entire address best result given address seek mainly appear top quarter cv trim training data live data get better result keeping document whole tagging bit need finally idea level success likely finding address none structured document advice help alternative method greatly appreciated
How to add proper nouns as vocab to Spacy models?,"<p>I am using Spacy 1.8.0 with Python and I would like to use Spacy to do analysis on Medical Documents. There is a way off adding new entity types to spacy's named entity recognizer. However, is it possible to add the names of medicines/drugs as proper nouns to spcay's vocab? Or do they need to be added by training the spacy NER? 
Thanks</p>
",Named Entity Recognition (NER),add proper noun vocab spacy model using spacy python would like use spacy analysis medical document way adding new entity type spacy named entity recognizer however possible add name medicine drug proper noun spcay vocab need added training spacy ner thanks
Python Named Entities Recognition finding a specific entity,"<p>I currently have a project about NLP, I try to use NLTK to recognize a PERSON name. But, the problem is more challenging than just finding part-of-speech.</p>

<pre><code>""input = ""Hello world, the case is complex. John Due, the plaintiff in the case has hired attorney John Smith for the case.""
</code></pre>

<p>So, the challenge is: I just want to get the attorney's name as the return from the whole document, not the other person, so ""John Smith"", part-of-speech: PERSON, occupation: attorney. The return could look like this, or just ""John Smith"".</p>

<pre><code>{ 
 ""name"": ""John Smith"",
 ""type"": ""PERSON"",
 ""occupation"": ""attorney""
}
</code></pre>

<p>I have tried NLTK part-of-speech, also the Google Cloud Natural Language API, but it just helped me to detect the PERSON name. How can I detect if it is an attorney? Please guide me to the right approach. Do I have to train my own data or corpus to detect ""attorney"". I have thousands of court document txt files.</p>
",Named Entity Recognition (NER),python named entity recognition finding specific entity currently project nlp try use nltk recognize person name problem challenging finding part speech challenge want get attorney name return whole document person john smith part speech person occupation attorney return could look like john smith tried nltk part speech also google cloud natural language api helped detect person name detect attorney please guide right approach train data corpus detect attorney thousand court document txt file
Spacy 2.0 NER Training,"<p>In SpacyV1 it was possible to train the NER model by providing a document and a list of entity annotations in BILOU format.</p>

<p>However it seems as if in V2 training is only possible by providing entity annotation like this (7, 13, 'LOC'), so with enity offsets and entity tag.</p>

<p>Is the old way of providing the list of tokens and another list of entity tags in BILOU format still valid? </p>

<p>From what I gather from the documentation it looks like the nlp.update method accepts a list of GoldParse objects so I could create a GoldParse Object for each doc and pass the BILOU tags to its entities attribute. However would I loose important information by ignoring the other attributes of the GoldParse class (e.g. heads or tags <a href=""https://spacy.io/api/goldparse"" rel=""nofollow noreferrer"">https://spacy.io/api/goldparse</a> ) or are the other attributes not needed for training the NER?</p>

<p>Thanks!</p>
",Named Entity Recognition (NER),spacy ner training spacyv wa possible train ner model providing document list entity annotation bilou format however seems v training possible providing entity annotation like loc enity offset entity tag old way providing list token another list entity tag bilou format still valid gather documentation look like nlp update method accepts list goldparse object could create goldparse object doc pas bilou tag entity attribute however would loose important information ignoring attribute goldparse class e g head tag attribute needed training ner thanks
Named Entity Recognition with less time: NLTK,"<p>I‘m playing with NLTK. I need NER but it's not fast with many sentences. Now my code is below:</p>

<pre><code>from nltk.tag import StanfordNERTagger   
st = StanfordNERTagger(...)
for s in sents:
    w_tokens = word_tokenize(s.strip())
    ner_tags =st_ner.tag(w_tokens)
</code></pre>

<p>One sentence is pretty.</p>

<p>Input:</p>

<blockquote>
  <p>Barack H. Obama is the 44th President of the United States.</p>
</blockquote>

<p>output:</p>

<blockquote>
  <p>[('Barack', 'PERSON'), ('H.', 'PERSON'), ('Obama', 'PERSON'), ('is', 'O'), ('the', 'O'), ('44th', 'O'), ('President', 'O'), ('of', 'O'), ('the', 'O'), ('United', 'LOCATION'), ('States', 'LOCATION')</p>
</blockquote>

<p>But, I need handle many sentences. Do I have any method like <code>chunk</code> to make me finish the job faster?</p>
",Named Entity Recognition (NER),named entity recognition le time nltk playing nltk need ner fast many sentence code one sentence pretty input barack h obama th president united state output barack person h person obama person th president united location state location need handle many sentence method like make finish job faster
&#39;negative examples&#39; for spacy NER transfer learning?,"<p>I want to train the spacy v2 NER model on my own labels, for which I crawled some text from different webpages. Coming with the crawling, there's of course lots of text that is just garbage and don't contain any information, but fortunately in most cases it's the exact same text because it's crawled from some news feed that is integrated in the webpages.</p>

<p>So my question is, can I use the 'garbage part' of the crawled text as some sort of negative example for the model to learn? That is, does it make sense not to annotate these parts of the crawled texts and feed it to the model so that the model learns not to annotate these kind of examples? Otherwise I would have to filter out these examples manually for my train/test set which I obviously cannot do, when the model is in production and has to work fully automated</p>
",Named Entity Recognition (NER),negative example spacy ner transfer learning want train spacy v ner model label crawled text different webpage coming crawling course lot text garbage contain information fortunately case exact text crawled news feed integrated webpage question use garbage part crawled text sort negative example model learn doe make sense annotate part crawled text feed model model learns annotate kind example otherwise would filter example manually train test set obviously model production ha work fully automated
Extract Numbers associated with particular phrase,"<p>I want to extract group names and associated numbers with that in Python. </p>

<p>Sample Input:</p>

<pre><code>34 patients have admitted in hospital and distributed in Pune group with 20 patients, Mumbai group with 10 patients and Nagpur group with 4 patients.
</code></pre>

<p>Sample Output:</p>

<pre><code>'Pune group, 20'
'Mumbai group, 10'
'Nagpur group, 4'
</code></pre>
",Named Entity Recognition (NER),extract number associated particular phrase want extract group name associated number python sample input sample output
Named Entity Recognition Cosine Similarity,"<p>I'm trying to use google's word2vec model (<a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">https://code.google.com/archive/p/word2vec/</a> - <a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing</a>) based on google news in Python 3.6.
I load the model using gensim package and specially <code>KeyedVectors</code> class (<a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html</a>). After, I use <code>similarity()</code> method, but I have some problems with named entities.</p>

<p>For example, I would compare and measure ""Roger Federer"" with ""Michael Jordan"" (and not ""federer"" with ""jordan"", because I wouldn't want to tokenize the single named entity), and with many named entities I got an error <code>word 'named_entities' is not in vocabulary</code>.</p>

<p>Is there any other pre-trained word2vec model which can be used to measure distance between named entities.</p>
",Named Entity Recognition (NER),named entity recognition cosine similarity trying use google word vec model based google news python load model using gensim package specially class use method problem named entity example would compare measure roger federer michael jordan federer jordan want tokenize single named entity many named entity got error pre trained word vec model used measure distance named entity
How to extract scene locations from a given text?,"<p>I am working on a task where I am supposed to find/extract details about scene locations. Consider following sentences:</p>

<p>1 - <em>Open on a small school classroom in south Italy</em>.</p>

<p>2 - <em>Cut to dawn breaking over a wheat field.</em></p>

<p>In the first sentence, it should extract ""school classroom"" or ""classroom"". For the later one, it should extract ""wheat field"".</p>

<p>I was unable to find any resources related to this task. I would like to know:</p>

<ul>
<li>whether this is possible or not</li>
<li>if yes, then how? Links to any paper/code will be very helpful.</li>
</ul>

<p>Thanks</p>
",Named Entity Recognition (NER),extract scene location given text working task supposed find extract detail scene location consider following sentence open small school classroom south italy cut dawn breaking wheat field first sentence extract school classroom classroom later one extract wheat field wa unable find resource related task would like know whether possible yes link paper code helpful thanks
"Training Stanford-NER-CRF, control number of iterations and regularisation (L1,L2) parameters","<p>I was looking through StanfordNER documentation/FAQ but I can't find anything related to specifying the maximum number of iterations in training and also the value of the regularisation parameters L1 and L2.</p>

<p>I saw an answer on which is suggested to set, for instance: </p>

<p><code>maxIterations=10</code> </p>

<p>in the properties file, but that did not gave any results.</p>

<p>Is it possible to set these parameters?</p>
",Named Entity Recognition (NER),training stanford ner crf control number iteration regularisation l l parameter wa looking stanfordner documentation faq find anything related specifying maximum number iteration training also value regularisation parameter l l saw answer suggested set instance property file gave result possible set parameter
R unnest_tokens and calculate positions (start and end location) of each token,"<p>How to get the position of all the tokens after using unnest_tokens?
Here is a simple example - </p>

<pre><code>df&lt;-data.frame(id=1,
               doc=c(""Patient:   [** Name **], [** Name **] Acct.#:         
[** Medical_Record_Number **]        MR #:     [** Medical_Record_Number **]
Location: [** Location **] ""))
</code></pre>

<p>Tokenize by white space using tidytext - </p>

<pre><code>library(tidytext)
tokens_df&lt;-df %&gt;% 
unnest_tokens(tokens,doc,token = stringr::str_split, 
pattern = ""\\s"",
to_lower = F, drop = F)
</code></pre>

<p>How to get the position of all the tokens?</p>

<pre><code>id  tokens  start  end
 1  Patient: 1      8
 1           9      9
 1  [**      12     14
 1  Name     16     19  
</code></pre>
",Named Entity Recognition (NER),r unnest token calculate position start end location token get position token using unnest token simple example tokenize white space using tidytext get position token
What can I do to speed up Stanford CoreNLP (dcoref/ner)?,"<p>I'm processing a large amount of documents using Stanford's CoreNLP library alongside the <a href=""https://github.com/brendano/stanford_corenlp_pywrapper"" rel=""noreferrer"">Stanford CoreNLP Python Wrapper</a>. I'm using the following annotators: </p>

<pre><code>tokenize, ssplit, pos, lemma, ner, entitymentions, parse, dcoref
</code></pre>

<p>along with the shift-reduce parser model <code>englishSR.ser.gz</code>. I'm mainly using CoreNLP for its co-reference resolution / named entity recognition, and as far as I'm aware I'm using the minimal set of annotators for this purpose.</p>

<p><strong>What methods can I take to speed up the annotation of documents?</strong></p>

<p>The other SO answers all suggest not loading the models for every document, but I'm already doing that (since the wrapper starts the server once and then passes documents/results back and forth).</p>

<p>The documents I am processing have an average length of 20 sentences, with some as long as 400 sentences and some as short as 1. The average parse time per sentence is 1 second. I can parse ~2500 documents per day with one single-threaded process running on one machine, but I'd like to double that (if not more).</p>
",Named Entity Recognition (NER),speed stanford corenlp dcoref ner processing large amount document using stanford corenlp library alongside stanford corenlp python wrapper using following annotator along shift reduce parser model mainly using corenlp co reference resolution named entity recognition far aware using minimal set annotator purpose method take speed annotation document answer suggest loading model every document already since wrapper start server pass document result back forth document processing average length sentence long sentence short average parse time per sentence second parse document per day one single threaded process running one machine like double
Stanford CoreNLP NER .net gives different output to the java version and the online demo ones,"<p>I am doing NLP NER task and I'm using the Stanford CoreNLP, while trying the .net version I have noticed that the output of .net version is different to the online demo and the java versions(and those 2 are identical).</p>

<p>Let's take an example of ""Obama was born on August 4, 1961, at Kapiolani Medical Center for Women and Children in Honolulu, Hawaii, USA."" sentence.</p>

<p>Both online demo and java give same output:</p>

<p>The online demo:</p>

<p><a href=""https://i.sstatic.net/2M7Ec.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2M7Ec.png"" alt=""the online demo""></a> </p>

<p>The java version:</p>

<p><a href=""https://i.sstatic.net/FfNV0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FfNV0.png"" alt=""the java version""></a></p>

<p>while the C# version gives:</p>

<p><a href=""https://i.sstatic.net/lP48l.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lP48l.png"" alt=""the C# version""></a></p>

<p>In the java version I import all the NLP package while in the C# version I import only one file and that's here:</p>

<pre><code>string path = @""some_path\stanford-ner-2017-06-09\classifiers\english.muc.7class.distsim.crf.ser.gz"";
CRFClassifier classifier = CRFClassifier.getClassifierNoExceptions(path);
</code></pre>

<p>Is that the really reason? if yes, then how can I resolve that issue?</p>

<p>I have noticed that too, the C# version gives better prediction in some words and vice versa.</p>
",Named Entity Recognition (NER),stanford corenlp ner net give different output java version online demo one nlp ner task using stanford corenlp trying net version noticed output net version different online demo java version identical let take example obama wa born august kapiolani medical center woman child honolulu hawaii usa sentence online demo java give output online demo java version c version give java version import nlp package c version import one file really reason yes resolve issue noticed c version give better prediction word vice versa
Spacy NER splitting the entity into two separate entities,"<p>I am doing NER on the following text</p>

<pre><code>print([(i.text, i.label_) for i in doc.ents])
</code></pre>

<p>My text looks like </p>

<pre><code>ZS L-1 Cocoa &amp; Burgers Ltd
-
2013 to 2017
</code></pre>

<p>I am getting the output </p>

<pre><code>('ZS L-1', 'ORG'), ('Cocoa &amp; Burgers Ltd', 'ORG'), ('2017', 'DATE')
</code></pre>

<p>How can i get the ORG name correctly as </p>

<pre><code>('ZS L-1 Cocoa &amp; Burgers Ltd', 'ORG')
</code></pre>

<p>Also it fails to recognise the 2013 as DATE entity. My spacy version is 2.0 and I am using this model - en_core_web_md-2.0.0. This generally happens when I am doing NER on the whole text. When I do NER on just the company name, it works fine. </p>
",Named Entity Recognition (NER),spacy ner splitting entity two separate entity ner following text text look like getting output get org name correctly also fails recognise date entity spacy version using model en core web md generally happens ner whole text ner company name work fine
Nullpointer Exception with OpenNLP in NameFinderME class,"<p>I am using <strong><a href=""https://opennlp.apache.org/"" rel=""nofollow noreferrer"">OpenNLP</a></strong> to extract named entities from a given text.
It gives me the following error while running the code on large data. When I run it on small data it works fine.</p>

<pre><code>java.lang.NullPointerException
    at opennlp.tools.util.Cache.put(Cache.java:134)
    at opennlp.tools.util.featuregen.CachedFeatureGenerator.createFeatures(CachedFeatureGenerator.java:71)
    at opennlp.tools.namefind.DefaultNameContextGenerator.getContext(DefaultNameContextGenerator.java:116)
    at opennlp.tools.namefind.DefaultNameContextGenerator.getContext(DefaultNameContextGenerator.java:39)
    at opennlp.tools.util.BeamSearch.bestSequences(BeamSearch.java:125)
    at opennlp.tools.util.BeamSearch.bestSequence(BeamSearch.java:198)
    at opennlp.tools.namefind.NameFinderME.find(NameFinderME.java:214)
    at opennlp.tools.namefind.NameFinderME.find(NameFinderME.java:198)
</code></pre>

<p>Please help me out with this.</p>
",Named Entity Recognition (NER),nullpointer exception opennlp namefinderme class using opennlp extract named entity given text give following error running code large data run small data work fine please help
Entity Extraction/Recognition with free tools while feeding Lucene Index,"<p>I'm currently investigating the options to extract person names, locations, tech words and categories from text (a lot articles from the web) which will then feeded into a Lucene/ElasticSearch index. The additional information is then added as metadata and should increase precision of the search. </p>

<p>E.g. when someone queries 'wicket' he should be able to decide whether he means the cricket sport or the Apache project. I tried to implement this on my own with minor success so far. Now I found a lot tools, but I'm not sure if they are suited for this task and which of them integrates good with Lucene or if precision of entity extraction is high enough.</p>

<ul>
<li><a href=""http://dbpedia.org/spotlight"" rel=""noreferrer"">Dbpedia Spotlight</a>, the <a href=""http://spotlight.dbpedia.org/demo/index.xhtml"" rel=""noreferrer"">demo</a> looks very promising</li>
<li><a href=""http://incubator.apache.org/opennlp/index.html"" rel=""noreferrer"">OpenNLP</a> requires <a href=""https://stackoverflow.com/questions/6952512/how-i-train-an-named-entity-reconigzer-identifier-in-opennlp"">training</a>. Which training data to use?</li>
<li><a href=""http://opennlp.sourceforge.net/projects.html"" rel=""noreferrer"">OpenNLP tools</a></li>
<li><a href=""http://incubator.apache.org/stanbol/"" rel=""noreferrer"">Stanbol</a></li>
<li><a href=""http://www.nltk.org/download"" rel=""noreferrer"">NLTK</a></li>
<li><a href=""http://balie.sourceforge.net/"" rel=""noreferrer"">balie</a></li>
<li><a href=""http://uima.apache.org/"" rel=""noreferrer"">UIMA</a></li>
<li><a href=""http://gate.ac.uk/"" rel=""noreferrer"">GATE</a> -> <a href=""http://gate.ac.uk/wiki/code-repository/"" rel=""noreferrer"">example code</a></li>
<li><a href=""http://mahout.apache.org/"" rel=""noreferrer"">Apache Mahout</a></li>
<li><a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""noreferrer"">Stanford CRF-NER</a></li>
<li><a href=""http://code.google.com/p/maui-indexer"" rel=""noreferrer"">maui-indexer</a></li>
<li><a href=""http://mallet.cs.umass.edu/"" rel=""noreferrer"">Mallet</a></li>
<li><a href=""http://cogcomp.cs.illinois.edu/page/software_view/4"" rel=""noreferrer"">Illinois Named Entity Tagger</a> Not open source but free</li>
<li><a href=""http://code.google.com/p/wikipedianerdata/source/browse/trunk/src/main/entityExtractor/EntityExtractionManager.java?r=2"" rel=""noreferrer"">wikipedianer data</a></li>
</ul>

<p><strong>My questions:</strong></p>

<ul>
<li>Does anyone have experience with some of the listed tools above and its precision/recall? Or if there is training data required + available.</li>
<li>Are there articles or tutorials where I can get started with entity extraction(NER) for each and every tool?</li>
<li>How can they be integrated with Lucene?</li>
</ul>

<p>Here are some questions related to that subject:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/5544475/does-an-algorithm-exist-to-help-detect-the-primary-topic-of-an-english-sentence"">Does an algorithm exist to help detect the &quot;primary topic&quot; of an English sentence?</a></li>
<li><a href=""https://stackoverflow.com/questions/188176/named-entity-recognition-libraries-for-java"">Named Entity Recognition Libraries for Java</a></li>
<li><a href=""https://stackoverflow.com/questions/5571519/named-entity-recognition-with-java"">Named entity recognition with Java</a></li>
</ul>
",Named Entity Recognition (NER),entity extraction recognition free tool feeding lucene index currently investigating option extract person name location tech word category text lot article web feeded lucene elasticsearch index additional information added metadata increase precision search e g someone query wicket able decide whether mean cricket sport apache project tried implement minor success far found lot tool sure suited task integrates good lucene precision entity extraction high enough dbpedia spotlight demo look promising opennlp requires opennlp tool stanbol nltk balie uima gate example code apache mahout stanford crf ner maui indexer mallet illinois named entity tagger open source free wikipedianer data question doe anyone experience listed tool precision recall training data required available article tutorial get started entity extraction ner every tool integrated lucene question related subject href entity recognition java
How good is GATE for NLP?,"<p>I am trying to build a NLP app which essentially has to do Named Entity Recognition (NER). I came across <a href=""https://gate.ac.uk/"" rel=""noreferrer"">GATE</a>. From what i understand it is a framework to build NLP apps. I tested ANNIE, the IE system distributed with GATE but the NER results for my domain is not up-to the expectation. As a matter of fact any NER, like Stanford CoreNLP or NLTK, is not giving me required results. So i decide to tweak the existing systems to get desired result.</p>

<p>Regarding GATE i liked few things:<br>
1. The modularity of components: For example in ANNIE, components like Tokenizer, Gaztteer, Sentence splitter, POS tagger etc can be used independently of each other.<br>
2. Its rule language called JAPE which has a very nice way of writing rules or patterns.</p>

<p>But few things i want to know about GATE are:<br>
1. What are the other major advantages of GATE particularly for NER?<br>
2. How flexible is GATE for adding new components? For example some day if i want to use NLTK's POS tagger inside GATE?<br>
3. If i want to use custom machine learning models with GATE?<br>
4. I am aware that NLP group at University of Sheffield is involved in GATE, but i want to know how active is GATE's community and how active is the support for GATE?<br>
5. Can GATE be used for commercial software?</p>

<p>Keen to here suggestions from people who have actually used GATE </p>
",Named Entity Recognition (NER),good gate nlp trying build nlp app essentially ha named entity recognition ner came across gate understand framework build nlp apps tested annie ie system distributed gate ner result domain expectation matter fact ner like stanford corenlp nltk giving required result decide tweak existing system get desired result regarding gate liked thing modularity component example annie component like tokenizer gaztteer sentence splitter po tagger etc used independently rule language called jape ha nice way writing rule pattern thing want know gate major advantage gate particularly ner flexible gate adding new component example day want use nltk po tagger inside gate want use custom machine learning model gate aware nlp group university sheffield involved gate want know active gate community active support gate gate used commercial software keen suggestion people actually used gate
Find multi-word terms in a tokenized text in Python,"<p>I have a text that I have tokenized, or in general a list of words is ok as well. For example:</p>

<pre><code>   &gt;&gt;&gt; from nltk.tokenize import word_tokenize
    &gt;&gt;&gt; s = '''Good muffins cost $3.88\nin New York.  Please buy me
    ... two of them.\n\nThanks.'''
    &gt;&gt;&gt; word_tokenize(s)
        ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.',
        'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
</code></pre>

<p>If I have a Python dict that contains single word as well as multi-word keys, how can I efficiently and correctly check for their presence in the text? The ideal output would be key:location_in_text pairs, or something as convenient.
Thanks in advance!</p>

<p>P.S. To explain ""correctly"" - If I have ""lease"" in my dict, I do not wish Please marked. Also, recognizing plurals is required. I am wondering if this can be elegantly solved without many if-else clauses.</p>
",Named Entity Recognition (NER),find multi word term tokenized text python text tokenized general list word ok well example python dict contains single word well multi word key efficiently correctly check presence text ideal output would key location text pair something convenient thanks advance p explain correctly lease dict wish please marked also recognizing plural required wondering elegantly solved without many else clause
How does precision and recall work in this situation?,"<p>Consider the following text, which has been manually annotated using IO-style annotation for persons (PER), locations (LOC) and organizations (ORG).</p>

<blockquote>
  <p>Chicago/LOC Mayor/O Rahm/PER Emanuel/PER, a/O former/O White/ORG
  House/ORG aide/O to/O US/LOC Presidents/O Barack/PER Obama/PER and/O
  Bill/PER Clinton/PER, on/O Friday/O joined/O the/O Ready/O For/O
  Hillary/PER group/O that/O is/O urging/O former/O US/LOC Secretary/O
  of/O State/O Hillary/PER Clinton/PER to/O run/O for/O president/O in/O
  2016/O.</p>
</blockquote>

<p>Consider the following feature assignments f(FEATURES,LABEL), which indicate assigning LABEL upon observing FEATURE, where w is the current token, and w-1 is the previous token.</p>

<pre><code>f1(isCapitalized(w), PER)

f2(label(w-1) = PER, PER)

f3(isCapitalized(w), LOC)

f4(lemma(w-1) = ""president"" OR ""mayor"", PER)
</code></pre>

<p>Based on the observed data, compute precision and recall for each of the features above, assuming that each of them is used in isolation in order to assign labels, and each starts with an unlabelled text.</p>

<p>How should I calculate the precision/ recall, in this situation?
Should I consider, for example, Rahm Emanuel as one True Positive feature? Or each token is a true positive feature? Or each token is a false positive as Rahm Emanuel is actually a single true positive feature?</p>
",Named Entity Recognition (NER),doe precision recall work situation consider following text ha manually annotated using io style annotation person per location loc organization org chicago loc mayor rahm per emanuel per former white org house org aide u loc president barack per obama per bill per clinton per friday joined ready hillary per group urging former u loc secretary state hillary per clinton per run president consider following feature assignment f feature label indicate assigning label upon observing feature w current token w previous token based observed data compute precision recall feature assuming used isolation order assign label start unlabelled text calculate precision recall situation consider example rahm emanuel one true positive feature token true positive feature token false positive rahm emanuel actually single true positive feature
Detecting text relevant to an entity in nlp,"<p>I am trying to solve a problem where I'm identifying entities in articles (ex: names of cars), and trying to predict sentiment about each car within the article. For that, I need to extract the text relevant to each entity from within the article.</p>

<p>Currently, the approach I am using is as follows:</p>

<ul>
<li>If a sentence contains only 1 entity, tag the sentence as text for that entity </li>
<li>If sentence has more than 1 entity, ignore it</li>
<li>If sentence contains no entity, tag as a sentence for previously identified entity</li>
</ul>

<p>However, this approach is not yielding accurate results, even if we assume that our sentiment classification is working.
Is there any method that the community may have come across that can solve this problem?</p>

<p>The approach fails for many cases and gives wrong results. For example if I am saying - 'Lets talk about the Honda Civic. The car was great, but failed in comparison to the Ford focus. The car also has good economy.'
Here, the program would pick up Ford Focus as the entity in last 2 sentences and tag those sentences for it.</p>

<p>I am using nltk for descriptive words tagging, and scikit-learn for classification (linear svm model).</p>

<p>If anyone could point me in the right direction, it would be greatly appreciated. Is there some classifier I could build with custom features that can detect this type of text if I were to manually tag say - 50 articles and the text in them?
Thanks in advance!</p>
",Named Entity Recognition (NER),detecting text relevant entity nlp trying solve problem identifying entity article ex name car trying predict sentiment car within article need extract text relevant entity within article currently approach using follows sentence contains entity tag sentence text entity sentence ha entity ignore sentence contains entity tag sentence previously identified entity however approach yielding accurate result even assume sentiment classification working method community may come across solve problem approach fails many case give wrong result example saying let talk honda civic car wa great failed comparison ford focus car also ha good economy program would pick ford focus entity last sentence tag sentence using nltk descriptive word tagging scikit learn classification linear svm model anyone could point right direction would greatly appreciated classifier could build custom feature detect type text manually tag say article text thanks advance
how to use pos tag as feature in Stanford NER training?,"<p>I am trying to use the useTags and related features in training Stanford NER CRF model. However, although I have specified in the .prop file that I will use this feature, CoreAnnotations.PartOfSpeechAnnotation.class does not seem to return anything and hence the training does not use this feature at all. Is there something I did wrong that it wasn't using this feature? Thanks!</p>
",Named Entity Recognition (NER),use po tag feature stanford ner training trying use usetags related feature training stanford ner crf model however although specified prop file use feature coreannotations partofspeechannotation class doe seem return anything hence training doe use feature something wrong using feature thanks
NLP - How to Identify whether 2 texts refer to similar object,"<p>I am looking for some general guidance here. </p>

<blockquote>
  <p>The high-level use case is such that I receive some product documents
  from which I need to extract some information and process it. Before
  doing that, I need to verify that the document is actually referring
  to the correct product. For that I need to validate the product
  heading/description from document against what I know to be correct.</p>
</blockquote>

<p>So I have 2 texts</p>

<ol>
<li>Text 1 - this refers to the product information extracted from some document </li>
<li>Text 2 - this is the actual product heading/description available with me, which can be considered as correct.</li>
</ol>

<p>I need to validate that both texts refer to same product or object.</p>

<p>Example:</p>

<pre><code>Text 1 (to be validated) - Optimus Prime Costume, Blue, with good packaging and warranty
Text 2 (correct info) - Optimus Prime Blue Costume, Medium Size`
</code></pre>

<p>You see, I need to validate that both text refer to <code>Optimus Prime Costume</code>.</p>

<p>I tried following methods - </p>

<ul>
<li>Cosine Similarity</li>
<li>TF-IDF similarity</li>
<li>Overlapping words between strings</li>
</ul>

<p>But the problem with them is that they depends on the entire text rather than the primary object being referred in the text.</p>

<p>I was thinking of processing as follows:-</p>

<ul>
<li>Remove colors, size info etc. from the text 2. The text 2, is very concise and does not contain random data. It contains product name and size, colour info.</li>
<li>Validate that the remaining elements from Text 2 are present in Text 1, or atleast a majority of them are.</li>
</ul>

<p>I am not quite sure what different NLP techniques might be there, which would be better than this approach, so any suggestions would be appreciated.</p>
",Named Entity Recognition (NER),nlp identify whether text refer similar object looking general guidance high level use case receive product document need extract information process need verify document actually referring correct product need validate product heading description document know correct text text refers product information extracted document text actual product heading description available considered correct need validate text refer product object example see need validate text refer tried following method cosine similarity tf idf similarity overlapping word string problem depends entire text rather primary object referred text wa thinking processing follows remove color size info etc text text concise doe contain random data contains product name size colour info validate remaining element text present text atleast majority quite sure different nlp technique might would better approach suggestion would appreciated
Extract relationships from a sentence in NLTK,"<p>I am using NLTK to extract the relationship between a PERSON and an ORGANIZATION.</p>

<p>Also, I want to extract the relationship between ORGANIZATION and LOCATION.
The NLTK version is 3.2.1.</p>

<p>I've made use of Part-Of-Speech tagging and Named Entity Recognition (NER). Also the Parse Tree is drawn for the NER results.<br>
But I am not able to extract the mentioned relationships from that sentence.</p>

<p>Here is the code:</p>

<pre><code>import nltk, re
from nltk import word_tokenize

sentence = ""Mark works at JPMC in London every day""
pos_tags = nltk.pos_tag(word_tokenize(sentence))            # POS tagging of the sentence
ne = nltk.ne_chunk(pos_tags)                                # Named Entity Recognition
ne.draw()                                                   # Draw the Parse Tree

IN = re.compile(r'.*\bin\b(?!\b.+ing)')
for rel1 in nltk.sem.extract_rels('PER', 'ORG', pos_tags, pattern = IN):
    print(nltk.sem.rtuple(rel1))
for rel2 in nltk.sem.extract_rels('ORG', 'LOC', pos_tags, pattern = IN):
    print(nltk.sem.rtuple(rel2))
</code></pre>

<p><br>
<strong>How to extract <em>'Person - Organization'</em> relationship and <em>'Organization - Location'</em> relationship?</strong></p>
",Named Entity Recognition (NER),extract relationship sentence nltk using nltk extract relationship person organization also want extract relationship organization location nltk version made use part speech tagging named entity recognition ner also parse tree drawn ner result able extract mentioned relationship sentence code extract person organization relationship organization location relationship
Three part related entities not specifically identified by a sentence,"<p>How do I train a Watson Knowledge Studio machine learning annotator to identify education info that is not a part of a proper sentence. For example, two bullet points. How do I form a type system that will identify entities without breaking them all apart? I've considered using relation annotations, but according to the official documentation relation types should only be annotated if the sentence specifically mentions the relation. Such as ""Mary works for IBM"" is an example of the employedBy relation type. (Mary employedBy IBM) However, their own videos show them annotating ""Ford F-150"" with a manufacturedBy relation even though the sentence doesn't specifically state the relation. For example, ""The Ford F-150 struck a light pole."" (F-150 manufacturedBy Ford)</p>

<p>This is the kind of text I'm working with:</p>

<ul>
<li>B.A., City University of New York, 1995</li>
<li>M.A., New York University, 1997</li>
<li>Ph.D, Columbia University, 1999</li>
</ul>

<p>I could annotate these with degree, school, and graduationYear entities, but I'll end up getting back ""1995"", ""1997"", ""1999"" ""B.A."", ""City University of New York"", ""Columbia University"", ""M.A."", ""New York University"", ""Ph.D""; a jumble that I can't work with because I can't tell anymore what degree belongs with what school belongs with what graduation year.</p>
",Named Entity Recognition (NER),three part related entity specifically identified sentence train watson knowledge studio machine learning annotator identify education info part proper sentence example two bullet point form type system identify entity without breaking apart considered using relation annotation according official documentation relation type annotated sentence specifically mention relation mary work ibm example employedby relation type mary employedby ibm however video show annotating ford f manufacturedby relation even though sentence specifically state relation example ford f struck light pole f manufacturedby ford kind text working b city university new york new york university ph university could annotate degree school graduationyear entity end getting back b city university new york university new york university ph jumble work tell anymore degree belongs school belongs graduation year
Extract human name from his CV in Python,"<p>As you all know names of persons normally on the top of their resume, so i did NER(name entity recognition) tagging using spaCy library on CV's and then i extract the first tag of <strong>PERSON</strong> (hoping it should be Human Name). Some time it works for me fine but some time it gives me Other things which are not names(because spaCy don't even recognize some names with any <strong>NER</strong> tag), so it is giving me some other things which it recognize as a <strong>PERSON</strong> it may be like 'Curriculam vitae' obviously this i don't want.
Following is a Code for which i was talking above...</p>

<pre><code>import spacy
import docx2txt

nlp = spacy.load('en_default')
my_text = docx2txt.process(""/home/waqar/CV data/Adnan.docx"")
doc_2 = nlp(my_text)
for ent in doc_2.ents:
    if ent.label_ == ""PERSON"":
        print('{}'.format(ent))
        break
</code></pre>

<p>Is there any way through which i can add some name to NER for 'PERSON' tag in spaCy because then it will be able to recognize human names written in CV's
i think my logic is fine but something i am missing....
I would be very thankful if u peoples help me as i am Student and a beginer in python hope u peoples will definitely suggest some thing</p>

<pre><code>OutPut

Abdul Ahad Ghous
</code></pre>

<p>but some time it giving me OutPuts like following as NER recognize it as a PERSON and don't even give any tag to the human name in this CV. </p>

<pre><code>Curriculum Vitae
</code></pre>
",Named Entity Recognition (NER),extract human name cv python know name person normally top resume ner name entity recognition tagging using spacy library cv extract first tag person hoping human name time work fine time give thing name spacy even recognize name ner tag giving thing recognize person may like curriculam vitae obviously want following code wa talking way add name ner person tag spacy able recognize human name written cv think logic fine something missing would thankful u people help student beginer python hope u people definitely suggest thing time giving output like following ner recognize person even give tag human name cv
Syntaxnet POS tagger use of capitalization,"<p>I want to use Syntaxnet for getting the POS tags of tweets (and more specifically, extracting named entities from the text). However, Parsey McParseface is case-sensitive by default. Since tweets often not use capitalization, I was thinking of using a case-less tagger. I found something about capitalization in the code, but I was not sure if and how to use it:</p>

<p><a href=""https://github.com/dsindex/syntaxnet/blob/15831789a706cbc482efeeec635a8f0315d0b3fb/English/context.pbtxt"" rel=""nofollow noreferrer"">https://github.com/dsindex/syntaxnet/blob/15831789a706cbc482efeeec635a8f0315d0b3fb/English/context.pbtxt</a></p>

<p>Let me give an example to be more clear. Consider the example sentences <code>John gave the money to Maria</code> and <code>john gave the money to maria</code> (with case and without case):</p>

<p>With caps:</p>

<pre><code>gave VBD ROOT
 +-- John NNP nsubj
 +-- money NN dobj
 |   +-- the DT det
 +-- to IN prep
     +-- Maria NNP pobj
</code></pre>

<p>Without caps:</p>

<pre><code>gave VBD ROOT
 +-- john NNP nsubj
 +-- money NN dobj
 |   +-- the DT det
 +-- to TO prep
     +-- maria NN pobj
</code></pre>

<p>As you can see, Maria is a NNP, whereas maria (without caps) is NN. When extracting named entities, it makes a difference if a word is tagged as NN or as NNP.</p>

<p>Is there a way to improve this? Is there a case-less Parsey McParseface for Syntaxnet?</p>
",Named Entity Recognition (NER),syntaxnet po tagger use capitalization want use syntaxnet getting po tag tweet specifically extracting named entity text however parsey mcparseface case sensitive default since tweet often use capitalization wa thinking using case le tagger found something capitalization code wa sure use let give example clear consider example sentence case without case cap without cap see maria nnp whereas maria without cap nn extracting named entity make difference word tagged nn nnp way improve case le parsey mcparseface syntaxnet
Extracting personal attributes from text,"<p>I'd like to extract personal attributes from a text written by a person. e.g.,</p>

<blockquote>
  <p>I have always been interested in professional cycling. Being a single mother, it was never easy to find enough time to pursue a sport professionally. The best I could do was to go for short rides along Melbourne's beautiful beaches...</p>
</blockquote>

<p>Ideally, I'd want to extract something like <em>cycling: interest, female: gender, sports: interest, Melbourne: location</em>. I think this is called named entity extraction, but I'm not sure. I tried <a href=""http://nlp.stanford.edu:8080/ner/process"" rel=""nofollow noreferrer"">Stanford Named Entity Recognizer</a> and it didn't give me quite what I wanted. The most important things are personal attributes, such as gender, age, interests etc. and it missed most of these on different samples.</p>

<p>Is there any tool/library (preferably in Python) that can help me do this? I know about NLTK, but I don't know how/if I can utilize it here.</p>
",Named Entity Recognition (NER),extracting personal attribute text like extract personal attribute text written person e g always interested professional cycling single mother wa never easy find enough time pursue sport professionally best could wa go short ride along melbourne beautiful beach ideally want extract something like cycling interest female gender sport interest melbourne location think called named entity extraction sure tried stanford named entity recognizer give quite wanted important thing personal attribute gender age interest etc missed different sample tool library preferably python help know nltk know utilize
NE tags in NLTK ConllCorpusReader,"<p>I'm trying to use CoNLLCorpusReader for CoNLL2003 dataset. This dataset contains 4 columns (example):</p>

<pre class=""lang-html prettyprint-override""><code>WORD      POS   CHUNK NE
U.N.      NNP   I-NP  I-ORG
official  NN    I-NP  O
Ekeus     NNP   I-NP  I-PER
heads     VBZ   I-VP  O
for       IN    I-PP  O
Baghdad   NNP   I-NP  I-LOC
.         .     O     O
</code></pre>



<p>I create corpus and it works - I can get words, sents and tuples with pos tags and chunk tags.</p>

<p>The question is, how can i get Named Entity tags from my corpus? I know there is corpus.raw() method, but is there really no way to get it with something like corpus.iob_words()? I found this issue: <a href=""https://github.com/nltk/nltk/issues/63"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/issues/63</a>, but in the latest version of this corpusReader there are no additional arguments in iob_words method that I can use to change the list of columns I want to get.</p>
",Named Entity Recognition (NER),ne tag nltk conllcorpusreader trying use conllcorpusreader conll dataset dataset contains column example create corpus work get word sent tuples po tag chunk tag question get named entity tag corpus know corpus raw method really way get something like corpus iob word found issue latest version corpusreader additional argument iob word method use change list column want get
separate from and to places from text using nlp python,"<p>I have a sample text like </p>

<p>'I'm travelling from Spain to India i.e on 23/09/2017 to 27/09/2017</p>

<p>From this type of text i want to separate from and to countries and dates.</p>

<p>How can i approach?</p>
",Named Entity Recognition (NER),separate place text using nlp python sample text like travelling spain india e type text want separate country date approach
A template based Text Summarization using Python,"<p>I want to extract key takeaways from a meeting transcript such as Project name , Challenges faced , Deadline , Solutions . I have a template for all these takeaways . 
So when after a meeting i need my model to extract text related to these takeaways.
Please give me some pointers regarding the problem and also I have a language constraint of using python.
Thanks </p>
",Named Entity Recognition (NER),template based text summarization using python want extract key takeaway meeting transcript project name challenge faced deadline solution template takeaway meeting need model extract text related takeaway please give pointer regarding problem also language constraint using python thanks
What methodology is best suited for Regression from text data?,"<p>I have a bunch of sentences each associated with a numerical performance value.</p>

<p>I would like to do two things:</p>

<ul>
<li>Based on a new sentence predict performance</li>
<li>Find out which words have the highest correlation with a high score</li>
</ul>

<p>What's the best way to extract features from sentences? Can someone advise me one which model is best suited for the above and is available in Sklearn?</p>

<p>Thanks!
Charles</p>

<p>ps: Down the line the text input will be combined with other numerical features to predict performance.</p>

<p>EDIT: Text Samples:</p>

<blockquote>
  <p>Barry is a Wonderful Host! His place is very nice and Barry is a very
  kind &amp; helping person. I will recommend him to all of my friends
  travelling to Amsterdam for a short or long trip! The location of his
  apartment is very convenient and can be easily accessed by tram. Just
  a short  tram ride or a few minutes of walking or biking then we were
  home. Thanks Barry for such a wonderful time! Wish u the best of luck
  and thanks again!</p>
</blockquote>
",Named Entity Recognition (NER),methodology best suited regression text data bunch sentence associated numerical performance value would like two thing based new sentence predict performance find word highest correlation high score best way extract feature sentence someone advise one model best suited available sklearn thanks charles p line text input combined numerical feature predict performance edit text sample barry wonderful host place nice barry kind helping person recommend friend travelling amsterdam short long trip location apartment convenient easily accessed tram short tram ride minute walking biking home thanks barry wonderful time wish u best luck thanks
SpaCy model training data: WikiNER,"<p>For the model <code>xx_ent_wiki_sm</code> of 2.0 version of SpaCy there is mention of ""WikiNER"" dataset, which leads to article 'Learning multilingual named entity recognition from Wikipedia'. </p>

<p>Is there any resource for downloading of such dataset for retraining that model? Or script for Wikipedia dump processing?</p>
",Named Entity Recognition (NER),spacy model training data wikiner model version spacy mention wikiner dataset lead article learning multilingual named entity recognition wikipedia resource downloading dataset retraining model script wikipedia dump processing
Train NER model in NLTK with custom corpus,"<p>I have an annotated corpus in the conll2002 format, namely a tab separated file with a token, pos-tag, and IOB tag followed by entity tag. Example:</p>

<blockquote>
  <p>John NNP B-PERSON</p>
</blockquote>

<p>I want to train a <strong>portuguese</strong> NER model in NLTK, preferably the MaxEnt model. I do <strong>not</strong> want to use the ""built-in"" Stanford NER in NLTK since I was already able to use the stand-alone Stanford NER. I want to use the MaxEnt model to use as comparison to the Stanford NER.</p>

<p>I found <a href=""http://nltk-trainer.readthedocs.io/en/latest/train_chunker.html"" rel=""noreferrer"">NLTK-trainer</a> but I wasn't able to use it.</p>

<p>How can I achieve this?</p>
",Named Entity Recognition (NER),train ner model nltk custom corpus annotated corpus conll format namely tab separated file token po tag iob tag followed entity tag example john nnp b person want train portuguese ner model nltk preferably maxent model want use built stanford ner nltk since wa already able use stand alone stanford ner want use maxent model use comparison stanford ner found nltk trainer able use achieve
feature extraction in python for nlp,"<p>I am trying to extract features like ""delhi police"" , ""newyork police"" using python regular expression. in short city and police name separated by space. 
 so city name will be differed and ""police"" will be constant.</p>

<p>Can I do using named entity recognition for location + ""police"" as a constant.</p>

<p>if yes HOW.?  any sort of regular expression can you provide.</p>
",Named Entity Recognition (NER),feature extraction python nlp trying extract feature like delhi police newyork police using python regular expression short city police name separated space city name differed police constant using named entity recognition location police constant yes sort regular expression provide
"Stanford NER won&#39;t use my training file, instead uses it&#39;s default","<p>I'm trying to train the Stanford NER classifier to identify specific things in text data bases.I have made a new .prop file and a training file, and I get results, but they are the default results that I would get if I just ran the classifier without training. Anything I can do to fit this?</p>

<p><strong>This is my code:</strong></p>

<pre><code>import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;
import edu.stanford.nlp.util.StringUtils;
import java.io.File;
import java.io.IOException;
import java.util.List;
import java.util.Properties;public class NLP_train {


   public static void main(String[] args) throws IOException {

       Properties props = StringUtils.propFileToProperties(""C:/Users/Admin/Desktop/trainingfile.prop"");

       StanfordCoreNLP pipeline = new StanfordCoreNLP(props);


       // read some text in the text variable
       File inputFile = new File(""C:/Users/Admin/Desktop/target.txt"");
       // create an empty Annotation just with the given text
       Annotation document = new Annotation(IOUtils.slurpFileNoExceptions(inputFile));

       // run all Annotators on this text
       pipeline.annotate(document);

       List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);

       for (CoreMap sentence : sentences) {
           // traversing the words in the current sentence
           // a CoreLabel is a CoreMap with additional token-specific methods
           for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
               // this is the text of the token
               String word = token.get(CoreAnnotations.TextAnnotation.class);
               // this is the POS tag of the token
               String pos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);
               // this is the NER label of the token
               String ne = token.get(CoreAnnotations.NamedEntityTagAnnotation.class);






               System.out.println(String.format(""Print: word: [%s] pos: [%s] ne: [%s]"", word, pos, ne));
           }
       }
   }
}
</code></pre>

<p><strong>Here is my .prop file:</strong></p>

<blockquote>
  <p>trainFile = C:/Users/Admin/Desktop/trainingfile.tsv</p>
  
  <p>serializeTo = C:/Users/Admin/Desktop/ner-model.ser.gz</p>
  
  <p>map = word=0,answer=1</p>
  
  <p>useClassFeature=true</p>
  
  <p>useWord=true</p>
  
  <p>useNGrams=true</p>
  
  <p>noMidNGrams=true</p>
  
  <p>useDisjunctive=true</p>
  
  <p>maxNGramLeng=6</p>
  
  <p>usePrev=true</p>
  
  <p>useNext=true</p>
  
  <p>useSequences=true</p>
  
  <p>usePrevSequences=true</p>
  
  <p>maxLeft=1</p>
  
  <p>the next 4 deal with word shape features</p>
  
  <p>useTypeSeqs=true</p>
  
  <p>useTypeSeqs2=true</p>
  
  <p>useTypeySequences=true</p>
  
  <p>wordShape=chris2useLC</p>
</blockquote>

<p><strong>And an excerpt of my training file:</strong></p>

<blockquote>
  <p>The      0</p>
  
  <p>Type     Radar</p>
  
  <p>347G     Radar</p>
  
  <p>``       0</p>
  
  <p>Rice     0</p>
  
  <p>Bowl     0</p>
  
  <p>''       0</p>
</blockquote>
",Named Entity Recognition (NER),stanford ner use training file instead us default trying train stanford ner classifier identify specific thing text data base made new prop file training file get result default result would get ran classifier without training anything fit code prop file trainfile c user admin desktop trainingfile tsv serializeto c user admin desktop ner model ser gz map word answer useclassfeature true useword true usengrams true nomidngrams true usedisjunctive true maxngramleng useprev true usenext true usesequences true useprevsequences true maxleft next deal word shape feature usetypeseqs true usetypeseqs true usetypeysequences true wordshape chris uselc excerpt training file type radar g radar rice bowl
Should I stem domain words for named entity recognition?,"<p>My question is perhaps not entirely programming, but I know many talented programmers are doing NLP and might be able to answer my question yet.</p>

<p>I have compiled a document with domain words that I perform fuzzy matching on to extract named entities in text. The format is as follows:</p>

<pre><code>  ""ferry names"": [
    {
      ""stena danica"": [
        ""stena danica"",
        ""danica""
      ]
    },
</code></pre>

<p>The outer object is the category, the inner is the entity. An innermost list is a list of synonyms that the entity may be called by.
Now, my named entity recognition, simple as it is, works quite well. To make it easier on it though, I decided to stem all the words on the text passed in. </p>

<pre><code>{
  ""category"": ""ferry names"",
  ""distance"": 1,
  ""entity"": ""stena danica"",
  ""interpreted"": ""stena danica"",
  ""raw"": ""stena danica"",
  ""stemmed"": ""stena danic""
}
</code></pre>

<p>The stemmer (nltk snowball stemmer, SwedishStemmer) works brilliantly, but it also stems domain words, in this case, <code>Stena Danica</code>. </p>

<p><strong>Question:</strong> I'm not sure how to approach this, should I simply <em>not</em> stem domain words, or put the stemmed version in with the synonyms? 
As it is, it will still be picked up by the fuzzy matcher, but it might introduce problems. Thank you.</p>
",Named Entity Recognition (NER),stem domain word named entity recognition question perhaps entirely programming know many talented programmer nlp might able answer question yet compiled document domain word perform fuzzy matching extract named entity text format follows outer object category inner entity innermost list list synonym entity may called named entity recognition simple work quite well make easier though decided stem word text passed stemmer nltk snowball stemmer swedishstemmer work brilliantly also stem domain word case question sure approach simply stem domain word put stemmed version synonym still picked fuzzy matcher might introduce problem thank
Extract information from sentence,"<p>I'm creating a simple chatbot. I want to obtain the information from the user response. An example scenario:</p>

<pre><code>Bot : Hi, what is your name?
User: My name is Edwin.
</code></pre>

<p>I wish to extract the name Edwin from the sentence. However, the user can response in different ways such as </p>

<pre><code>User: Edwin is my name.
User: I am Edwin.
User: Edwin. 
</code></pre>

<p>I'm tried to rely on the dependency relations between words but the result does not do well.</p>

<p>Any idea on what technique I could use to tackle this problem?</p>
",Named Entity Recognition (NER),extract information sentence creating simple chatbot want obtain information user response example scenario wish extract name edwin sentence however user response different way tried rely dependency relation word result doe well idea technique could use tackle problem
Can (google/microsoft/facebook/watson) NLUs be used to index 100&#39;s of help documents (web pages) and then query it?,"<p>I have the following</p>

<p>a) Document base (100's of web pages) where we have captured all the help topics that users need. </p>

<p>b) Forums Web Page, which is active. </p>

<p>Question: 
What is the best way to do a search and pull the relevant pages when a user types his/her query?</p>

<p>I am specifically trying to see if I can use any of the API.ai, Wit.a, Microsoft LUIS or IBM Watson framework.
Does anyone have any experience with the above framworks/apis for indexing and querying HELP documents.</p>

<p>My understanding is that the above api's will just take the query and figure out the intent, entities and slots/utterances. After that, it is up to the application developer to query the help docs with intent, entities and slots. But in case of HELP documents, the intent is always the same, ""to know"" or ""how to"" and the entities are too many, unlike the few entities in Airline/Restaurant Booking domains. 
How can one leverage such huge document base using the Natural Language Understanding (NLU) from API.ai/Wit.ai/LUIS/Watson.</p>

<p>Constraint: Having the content in the same web location (URL) where it is hosted would be ideal. It is not possible to extract and upload all the help pages and the forums to some cloud.</p>
",Named Entity Recognition (NER),google microsoft facebook watson nlus used index help document web page query following document base web page captured help topic user need b forum web page active question best way search pull relevant page user type query specifically trying see use api ai wit microsoft luis ibm watson framework doe anyone experience framworks apis indexing querying help document understanding api take query figure intent entity slot utterance application developer query help doc intent entity slot case help document intent always know entity many unlike entity airline restaurant booking domain one leverage huge document base using natural language understanding nlu api ai wit ai luis watson constraint content web location url hosted would ideal possible extract upload help page forum cloud
"how to merge entities of same type if spaCy shows multiple entities due to &#39;,&#39; &#39;\n&#39; or &#39;any other reason&#39;","<p>I have to extract organization name from company letters. When extracting entities, due to ',' or '\n' or 'sometimes for other reason' it splits the organization name. </p>

<pre><code>spacy_data = nlp(text)
spacy_data.ents if ent.label_ in =='ORG' 

expected output: capital international partners vi
actual output:   capital 
                   international partners vi 
</code></pre>

<p>It showing as two different organizations. I want my final output to be <code>capital_international_partners_vi</code> so that I can use it further for creating one-word vector</p>
",Named Entity Recognition (NER),merge entity type spacy show multiple entity due n reason extract organization name company letter extracting entity due n sometimes reason split organization name showing two different organization want final output use creating one word vector
Relationship extraction between person and city/state,"<p>I'm trying to take a sentence and extract the relationship between Person(PER) and Place(GPE).</p>

<p><strong>Sentence</strong>: ""John is from Ohio, Michael is from Florida and Rebecca is from Nashville which is in Tennessee.""</p>

<p>For the final person, she has both a city and a state that could get extracted as her place.  So far, I've tried using nltk to do this, but have only been able to extract her city and not her state.</p>

<p>What I've tried:</p>

<pre><code>import re
from nltk import ne_chunk, pos_tag, word_tokenize
from nltk.sem.relextract import extract_rels, rtuple

sentence = ""John is from Ohio, Michael is from Florida and Rebecca is from Nashville which is in Tennessee.""
chunked = ne_chunk(pos_tag(word_tokenize(sentence)))
ISFROM = re.compile(r'.*\bfrom\b.*')
rels = extract_rels('PER', 'GPE', chunked, corpus = 'ace', pattern = ISFROM)
for rel in rels:
    print(rtuple(rel))
</code></pre>

<p>My output is:</p>

<pre><code>[PER: 'John/NNP'] 'is/VBZ from/IN' [GPE: 'Ohio/NNP']
[PER: 'Michael/NNP'] 'is/VBZ from/IN' [GPE: 'Florida/NNP']
[PER: 'Rebecca/NNP'] 'is/VBZ from/IN' [GPE: 'Nashville/NNP']
</code></pre>

<p>The problem is Rebecca.  How can I extract that both Nashville and Tennesee are part of her location?  Or even just Tennessee alone?</p>
",Named Entity Recognition (NER),relationship extraction person city state trying take sentence extract relationship person per place gpe sentence john ohio michael florida rebecca nashville tennessee final person ha city state could get extracted place far tried using nltk able extract city state tried output problem rebecca extract nashville tennesee part location even tennessee alone
How do I enable multi-core option for training Stanford NER model?,"<p>Does Stanford crf classifier for NER support multi-core operations?</p>

<p>If yes, how do I enable it?</p>

<p>I have tried <code>nthreads</code> parameter, but it shows this:</p>

<blockquote>
  <p>Unknown property: |nthreads|</p>
</blockquote>
",Named Entity Recognition (NER),enable multi core option training stanford ner model doe stanford crf classifier ner support multi core operation yes enable tried parameter show unknown property nthreads
Stanford NER Tagger super slow,"<p>I am trying to process a text file of 1M lines. Each string is one line, and I need to obtain location information from each string. The easiest way I thought of doing so was using Stanford's NER tagger in PYTHON. </p>

<p>However, the tagger is performing extremely slow, like 2 seconds per line. By this speed it will take 20 days to complete my file. Also its somewhat inaccurate, missing obvious locations like ""nyc"".</p>

<p>Anyone know how I can speed things up a little and make it more accurate?</p>

<p>Thanks</p>
",Named Entity Recognition (NER),stanford ner tagger super slow trying process text file line string one line need obtain location information string easiest way thought wa using stanford ner tagger python however tagger performing extremely slow like second per line speed take day complete file also somewhat inaccurate missing obvious location like nyc anyone know speed thing little make accurate thanks
Identifying the subject of a sententce,"<p>I have been exploring NLP techniques with the goal of identifying the subject of survey comments (which I then use in conjunction with sentiment analysis). I want to make high level statements such as ""10% of survey respondents made a positive comment (+ sentiment) about Account Managers"".</p>

<p>My approach has used <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Named Entity Recognition (NER)</a>. Now that I am working with real data, I am getting visibility of some of the complexities &amp; nuances associated with identifying the subject of a sentence.  Here are 5 examples of sentences where the subject is the Account Manager. I have put the named entity in bold for demonstration purposes. </p>

<ol>
<li>Our <strong>account manager</strong> is great, he always goes the extra mile!</li>
<li><strong>Steve</strong> our <strong>account manager</strong> is great, he always goes the extra mile!</li>
<li><strong>Steve</strong> our <strong>relationship manager</strong> is great, he always goes the extra 
mile!</li>
<li><strong>Steven</strong> is great, he always goes the extra mile!</li>
<li><strong>Steve Smith</strong> is great, he always goes the extra mile!</li>
<li>Our <strong>business mgr</strong>. is great,he always goes the extra mile!</li>
</ol>

<p>I see three challenges that add complexity to my task</p>

<ol>
<li>Synonyms: Account manager vs relationship manager vs business mgr. This is somewhat domain specific and tends to vary with the survey target audience.</li>
<li>Abbreviations:  Mgr. vs manager</li>
<li>Ambiguity -  Whether “Steven” is “Steve Smith” &amp; therefore an
“account manager”.</li>
</ol>

<p>Of these the synonym problem is the most frequent issue, followed by the ambiguity issues. Based on what I have seen, the abbreviation issue isn’t that frequent in my data. </p>

<p>Are there any NLP techniques that can help deal with any of these issues to a relatively high degree of confidence?   </p>
",Named Entity Recognition (NER),identifying subject sententce exploring nlp technique goal identifying subject survey comment use conjunction sentiment analysis want make high level statement survey respondent made positive comment sentiment account manager approach ha used named entity recognition ner working real data getting visibility complexity nuance associated identifying subject sentence example sentence subject account manager put named entity bold demonstration purpose account manager great always go extra mile steve account manager great always go extra mile steve relationship manager great always go extra mile great always go extra mile steve smith great always go extra mile business mgr great always go extra mile see three challenge add complexity task synonym account manager v relationship manager v business mgr somewhat domain specific tends vary survey target audience abbreviation mgr v manager ambiguity whether steve smith therefore account manager synonym problem frequent issue followed ambiguity issue based seen abbreviation issue frequent data nlp technique help deal issue relatively high degree confidence
Extract region name from user query,"<p>I am using weather js npm module (<a href=""https://www.npmjs.com/package/weather-js"" rel=""nofollow noreferrer"">weather-js</a>) to find weather of a region.</p>

<p>Everything works fine, I was trying to modify it based on user query.</p>

<p>As per the module, it only accepts <strong>region name</strong> in search parameter.How do I Modify it so that I can process it based on user input? Where user input can be in any element in query array.</p>

<p><strong>How do I extract region from user query and pass it to <code>weather.find</code></strong></p>

<pre><code>var weather = require('weather-js');
var query   = [""Weather of US"" ,""Tell weather Of US"", ""US weather today"" ,""What is US weather""], 
weather(query);
function weather(query){
  weather.find({search: query, degreeType: 'F'}, function(err, result) {
  if(err){
    err = 'temperature could not be fetched';
    return err;
  }
  var temp   = result[0].current.temperature;
  var fToCel = (temp - 32) * 5 / 9;
  return fToCel;
});
}
</code></pre>
",Named Entity Recognition (NER),extract region name user query using weather j npm module weather j find weather region everything work fine wa trying modify based user query per module accepts region name search parameter modify process based user input user input element query array extract region user query pas
Which NN models does spaCy actually implement? What determines their size in memory?,"<p>I have seen that there is a <a href=""http://arxiv.org/abs/1511.06388"" rel=""nofollow noreferrer"">paper</a> supplying the idea behind <a href=""https://explosion.ai/blog/sense2vec-with-spacy"" rel=""nofollow noreferrer"">Sense2Vec</a>, but how are/were  the standard spaCy models created in the first place? When I download something like the standard ""en_core_web_md"" model from <a href=""https://spacy.io/docs/usage/models"" rel=""nofollow noreferrer"">the selection of models</a>, how was that actually created? Are there any papers I can read or spaCy blog posts?</p>

<p><strong>Bonus question:</strong></p>

<p>How are the new models in the upcoming <code>spaCy 2.0</code> so much smaller in size? </p>

<p>From the <a href=""https://alpha.spacy.io/docs/usage/v2#features-models"" rel=""nofollow noreferrer"">version 2 Release summary</a>:</p>

<blockquote>
  <p>This release features entirely new deep learning-powered models for spaCy's tagger, parser and entity recognizer. The new models are 20x smaller than the linear models that have powered spaCy until now: from 300 MB to only 15 MB.</p>
</blockquote>

<p>the only real reference that goes in this direction is <a href=""https://alpha.spacy.io/docs/usage/v2#features-models"" rel=""nofollow noreferrer"">here</a> on the release summary.
The summary of all model memory footprints can be found <a href=""https://github.com/explosion/spacy-models"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Are the model weights provided and every call to get relevant attributes actually being <em>computed</em> on the fly? That would explain the slower throughput shown in the benchmarks on <a href=""https://alpha.spacy.io/docs/usage/training-ner"" rel=""nofollow noreferrer"">this page</a></p>
",Named Entity Recognition (NER),nn model doe spacy actually implement determines size memory seen paper supplying idea behind sense vec standard spacy model created first place download something like standard en core web md model selection model wa actually created paper read spacy blog post bonus question new model upcoming much smaller size version release summary release feature entirely new deep learning powered model spacy tagger parser entity recognizer new model x smaller linear model powered spacy mb mb real reference go direction release summary summary model memory footprint found model weight provided every call get relevant attribute actually computed fly would explain slower throughput shown benchmark page
Data set for named entity recognition,"<p>I have to create training data set for named-entity recognition project. </p>

<p>For example, I have text </p>

<pre><code>""Last year, I was in London where I saw Tom""
</code></pre>

<p>Training data should be</p>

<pre><code>""Last year, I was in &lt;ENAMEX TYPE=""LOCATION""&gt;London&lt;/ENAMEX&gt; where I saw  
&lt;ENAMEX TYPE=""NAME""&gt;Tom&lt;/ENAMEX&gt;""
</code></pre>

<p>It is easy to do it by hand but it takes time when there are a large number of data.  I can not use an open set. I have small training data set but I should extend it. </p>

<p>How can I create a larger training data set by extending small training data set? Are there some ready packages or open projects for it? Or do you suggest different methods?</p>
",Named Entity Recognition (NER),data set named entity recognition create training data set named entity recognition project example text training data easy hand take time large number data use open set small training data set extend create larger training data set extending small training data set ready package open project suggest different method
Extract list in api.ai from user input,"<p>I have a query of the following nature in API.ai
""btc, ltc, xrp to usd, inr"" How can I extract the query as 
source = [btc, ltc, xrp]
destination = [usd, inr]</p>

<p>The number of elements in the source can be variable and the number of elements in the destination can also be variable. I am aware of the list entity and I tried it with the query. It picks up only btc in one list and puts the rest in another rest. Any suggestions</p>
",Named Entity Recognition (NER),extract list api ai user input query following nature api ai btc ltc xrp usd inr extract query source btc ltc xrp destination usd inr number element source variable number element destination also variable aware list entity tried query pick btc one list put rest another rest suggestion
How to use OwlExporter in GATE embedded,"<p>I have <a href=""https://gate.ac.uk/family/embedded.html"" rel=""nofollow noreferrer"">GATE</a> pipeline having PR (Processing Resources) and LR (Language Resources), but don't know how to add <a href=""http://www.semanticsoftware.info/owlexporter"" rel=""nofollow noreferrer"">OwlExporter</a> plugin. I have OwlExporter folder downloaded but no idea how to add it. I have done entity extraction for Person, Place, Organisation.</p>

<p>Thanks in advance.</p>
",Named Entity Recognition (NER),use owlexporter gate embedded gate pipeline pr processing resource lr language resource know add owlexporter plugin owlexporter folder downloaded idea add done entity extraction person place organisation thanks advance
OpenNLP creating Annotator,"<p>I used to use Stanford CoreNLP, and I would like to investigate OpenNLP this time.
Is it possible to create your own Annotator?
For example, I would like to analyze a text and pick out only colors, or aircraft names.</p>

<p>Stanford NER lets me to create my own NER model to do so.</p>

<p>Any suggestion??</p>

<p>Thank you</p>
",Named Entity Recognition (NER),opennlp creating annotator used use stanford corenlp would like investigate opennlp time possible create annotator example would like analyze text pick color aircraft name stanford ner let create ner model suggestion thank
How to add NER tags to features,"<p>I have a set of training sentences for which I computed some float features. In each sentence, <strong>two</strong> entities are identified. They are either of type 'PERSON', 'ORGANIZATION', 'LOCATION', or 'OTHER'. I would like to add these types to my feature matrix (which stores float variables). </p>

<p>My question is: is there a recommended way to add these entity types ? </p>

<p>I could think of two ways for now: </p>

<ul>
<li>either adding TWO columns, one for each <strong>entity</strong>, that will be filled with entity types ids (e.g 0 to 3 or 1 to 4)</li>
<li>adding EIGHT columns, one for each <strong>entity type</strong> and each <strong>entity</strong>, and filling them with 0's and 1's</li>
</ul>

<p>Best!</p>
",Named Entity Recognition (NER),add ner tag feature set training sentence computed float feature sentence two entity identified either type person organization location would like add type feature matrix store float variable question recommended way add entity type could think two way either adding two column one entity filled entity type id e g adding eight column one entity type entity filling best
Gender detection by full name,"<p>I want to create a model that detects the gender based on a full name.
I have two dictionaries with male &amp; female names. I want to develop a model to classify previously unseen names.</p>

<p>I need to determine the gender after the NER (name entity recognition) process. This delivers a PERSON entity with any one of these characteristics:</p>

<ul>
<li>FULL NAME (John Travolta) </li>
<li>NAME only (John) </li>
<li>SURNAME only (Travolta)</li>
</ul>

<p>I can do male vs female determination on (given) name only.  The model needs to handle SURNAME only, classifying it as <strong>NO_GENDER</strong>.</p>

<p>I know that surnames can be noisy, but I must deal with them, because they could be a part of the input.</p>
",Named Entity Recognition (NER),gender detection full name want create model detects gender based full name two dictionary male female name want develop model classify previously unseen name need determine gender ner name entity recognition process delivers person entity one characteristic full name john travolta name john surname travolta male v female determination given name model need handle surname classifying gender know surname noisy must deal could part input
Creating new annotation sets in GATE,"<p>I have started learning GATE application and I would like to use it to extract information from an unstructured document. The information I am interested in are date, location, event information and person’s names. I would like to get information about events that happened at a specific location on a specific date and the person/s name. I have been reading the GATE manual and thats how I got the glimpse on how to build your pipeline. However, I am not figuring out how I can create my new annotation types and make sure that they are annotated to a new annotation set which should appear under the annotation sets on the right. I found similar questions like <a href=""https://stackoverflow.com/questions/29258842/gate-how-to-create-a-new-annotation-set"">GATE - How to create a new annotation SET?</a> but it didn help me either.</p>

<p>Let me explain what I did so far:</p>

<ol>
<li>Created .lst file for my new NE and put them under ANNIE resources/gazetteer directory</li>
<li>I added the .lst file description in the list.def file</li>
<li><p>I identified my patterns in the document e.g for Date formats like ddmm, dd.mm.yyyy</p></li>
<li><p>I wrote JAPE rule for each pattern in a separate .jape file</p></li>
<li>Added the JAPE file names into the main.jape file</li>
<li>Loaded the PR and my document into GATE</li>
<li>Run the application</li>
</ol>

<p>This is how my JAPE Rule looks like for one date format:</p>

<pre><code>    Phase: datesearching
    Input: Token Lookup SpaceToken
    Options: control = appelt

    ////////////////////////////////////Macros
    //Initialization of regular expressions
    Macro: DAY_ONE
    ({Token.kind == number,Token.category==CD, Token.length == ""1""})

    Macro: C
    ({Token.kind == number,Token.category==CD, Token.length == ""2""})

    Macro: YEAR
    ({Token.kind == number,Token.category==CD, Token.length == ""4""})

    Macro: MONTH
    ({Lookup.majorType==""Month""})

    Rule: ddmmyyydash
    (
        (DAY_ONE|DAY_TWO)
        ({Token.string == "",""}|{Token.string == "".""} |{Token.string == ""-""})
        (MONTH)
        ({Token.string == "",""}|{Token.string == "".""} |{Token.string == ""-""})
        (YEAR)
    )
    :ddmmyyyydash
    --&gt;
        :ddmmyyyydash.DateMonthYearDash= {rule = ""ddmmyyyydash""}
</code></pre>

<p>Can someone please help me with what I should do to make sure that DateMonthYearDash is created as a new annotation set? How do I do it? Thanks a lot.</p>

<p>When I change the outputAsName of the Jape Transducer the new set is not appearing like the rest. This is how it looks:</p>

<p><a href=""https://i.sstatic.net/9H64R.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9H64R.png"" alt=""annotation set list""></a></p>
",Named Entity Recognition (NER),creating new annotation set gate started learning gate application would like use extract information unstructured document information interested date location event information person name would like get information event happened specific location specific date person name reading gate manual thats got glimpse build pipeline however figuring create new annotation type make sure annotated new annotation set appear annotation set right found similar question like
How to add new cities in nlp-compromise?,"<p>I am using compromise module to extract places from a paragraph, but how can I add new cities to this module?</p>

<p><code>var r = nlp(""I live in San Francisco"").places().out(""text"")</code> => gives ""San Francisco""</p>

<p>But, <code>var r = nlp(""I live in Dallas"").places().out(""text"")</code> => gives blank response.</p>

<p>So, I want to add new cities to compromise module, could you please guide me where and how can I do that.</p>
",Named Entity Recognition (NER),add new city nlp compromise using compromise module extract place paragraph add new city module give san francisco give blank response want add new city compromise module could please guide
Learnig NER using category list,"<p>In the template for training CRF++, how can I include a custom <code>dictionary.txt</code> file for listed companies, another for popular European foods, for eg, or just about any category.</p>

<p>Then provide a sample training data for each category whereby it learns how those specific named entites are used within a context for that category.<br>
In this way, I as well as the system, can be sure it correctly understood how certain named entites are structured in a text, whether a tweet or a Pulitzer prize winning news article, instead of providing hundred megabytes of data.</p>

<p>This would be rather cool. Model would have a definite dictionary of known entites <em>(which does not need to be expanded)</em> and a statistical approach on how those known entites are structured in human text.</p>

<p><em>PS - Just for clarity, not yearning for a regex ner. These are only cool if you got lots in the dictionary, lots of rule and lots of dulltime.</em></p>
",Named Entity Recognition (NER),learnig ner using category list template training crf include custom file listed company another popular european food eg category provide sample training data category whereby learns specific named entites used within context category way well system sure correctly understood certain named entites structured text whether tweet pulitzer prize winning news article instead providing hundred megabyte data would rather cool model would definite dictionary known entites doe need expanded statistical approach known entites structured human text p clarity yearning regex ner cool got lot dictionary lot rule lot dulltime
Improving parsing of unstructured text,"<p>I am parsing contract announcements into columns to capture the company, the amount awarded, the description of the project awarded, etc. <a href=""https://www.defense.gov/News/Contracts/Contract-View/Article/1243908/"" rel=""nofollow noreferrer"">A raw example can be found here.</a></p>

<p>I wrote a script using regular expressions to do this but over time contingencies arise that I have to account for which bar the regexp method from being a long term solution. I have been reading up on NLTK and it seems there are two ways to go about using NLTK to solve my problem:</p>

<ol>
<li>chunk the announcements using RegexpParser expressions - this might be a weak solution if two different fields I want to capture have the same sentence structure.</li>
<li>take n announcements, tokenize and run the n announcements through the pos tagger, manually tag the parts of the announcements I want to capture using the IOB format and then use those tagged announcements to train an NER model. <a href=""https://stackoverflow.com/questions/33717131/training-tagger-with-custom-tags-in-nltk"">A method discussed here</a></li>
</ol>

<p>Before I go about manually tagging announcements I want to gauge </p>

<ol>
<li>that 2 is a reasonable solution</li>
<li>if there are existing tagged corpus that might be useful to train my model</li>
<li>knowing that accuracy improves with training data size, how many manually tagged announcements I should start with. </li>
</ol>

<p>Here's an example of how I am building the training set. If there are any apparent flaws please let me know.</p>

<p><a href=""https://i.sstatic.net/aPhcB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/aPhcB.png"" alt=""IOB_tagged_text""></a></p>
",Named Entity Recognition (NER),improving parsing unstructured text parsing contract announcement column capture company amount awarded description project awarded etc raw example found wrote script using regular expression time contingency arise account bar regexp method long term solution reading nltk seems two way go using nltk solve problem chunk announcement using regexpparser expression might weak solution two different field want capture sentence structure take n announcement tokenize run n announcement po tagger manually tag part announcement want capture using iob format use tagged announcement train ner model
Get a token index from stanford corenlp,"<p>I want to get the index of a token using CoreNLP. I can get all the token annotations like POS, NER, but however the index returns <code>null</code>.
Here is the code:</p>

<pre><code>import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentenceIndexAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.IndexAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.ling.IndexedWord;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import java.util.Properties;
import edu.stanford.nlp.util.CoreMap;    

protected StanfordCoreNLP pipeline;
private void getPipeline(){
    // creates a StanfordCoreNLP object, with POS tagging, lemmatization, parsing
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, parse"");
    pipeline = new StanfordCoreNLP(props);
}

private Annotation annotatedocument(String text) {
    Annotation document = new Annotation(text);

    // run all Annotators on this text
    pipeline.annotate(document);
    return document;    
}

private  void  getAnnotation(String sentence){  
    Annotation annotation = annotatedocument(sentence);
    List&lt;CoreMap&gt; sentences = annotation.get(SentencesAnnotation.class);
    int numSent=sentences.size();
    assert (numSent == 1):""Number of sentences in annotation  is &gt; 1: "" + numSent;

    for(CoreMap sentence: sentences) {  
        List&lt;CoreLabel&gt; tokens = sentence.get(TokensAnnotation.class);
        for (CoreLabel token: tokens) { 
            String word = token.get(TextAnnotation.class);
            Integer index= token.get(IndexAnnotation.class);
            String pos=token.getString(PartOfSpeechAnnotation.class);
        }
    }
}
</code></pre>

<p>I get word and POS correct but index is <code>null</code>.             </p>
",Named Entity Recognition (NER),get token index stanford corenlp want get index token using corenlp get token annotation like po ner however index return code get word po correct index
Stanford NLP named entity recognition libraries for biomedical entities,"<p>Where can I find Stanford NLP named entity recognition libraries for genes, proteins, hormones, transcription factors, neurotransmitters, cytokines, biomarkers,  epigenetic markers, enzymes, RNAs, organs, cells, drugs, and diseases?</p>
",Named Entity Recognition (NER),stanford nlp named entity recognition library biomedical entity find stanford nlp named entity recognition library gene protein hormone transcription factor neurotransmitter cytokine biomarkers epigenetic marker enzyme rna organ cell drug disease
Linear Chain Conditional Random Field Sequence Model - NER,"<p>I am confused with what a linear chain CRF implementation exactly is. While some people say that ""The Linear Chain CRF restricts the features to depend on only the current(i) and previous label(i-1), rather than arbitrary labels throughout the sentence"" , some people say that it restricts the features to depend on the current(i) and future label(i+1). </p>

<p>I am trying to understand the implementation that goes behind the Stanford NER Model. Can someone please explain what exactly the linear chain CRF Model is?</p>
",Named Entity Recognition (NER),linear chain conditional random field sequence model ner confused linear chain crf implementation exactly people say linear chain crf restricts feature depend current previous label rather arbitrary label throughout sentence people say restricts feature depend current future label trying understand implementation go behind stanford ner model someone please explain exactly linear chain crf model
Extract entities from Simple passive voice sentence by Python Spacy,"<p>Using Python Spacy, how to extract entity from simple passive voice sentence? In the follow sentence, my intention is to extract both ""John”  from the sentence as <em>nsubjpass</em> and <em>.ent_</em>.</p>

<p>sentence =  ""John was accused of crimes by David""</p>
",Named Entity Recognition (NER),extract entity simple passive voice sentence python spacy using python spacy extract entity simple passive voice sentence follow sentence intention extract john sentence nsubjpass ent sentence john wa accused crime david
Custom NER model extracts substring of keyword used for training,"<p>I trained a custom NER model with more than a million sentences using the training API of OpenNlp for identifying the skill that I taught. During the testing I have found that the model identifying the skill and a substring of skill. For example, I have taught a skill 'Core Java', if the sentence have  word 'Core' then my model will identify it as a skill. I want to avoid such situation. I want only identify the whole word 'Core Java' if the whole word is there in the test sentences.</p>

<p>How can improve my custom model for above mentioned result?</p>
",Named Entity Recognition (NER),custom ner model extract substring keyword used training trained custom ner model million sentence using training api opennlp identifying skill taught testing found model identifying skill substring skill example taught skill core java sentence word core model identify skill want avoid situation want identify whole word core java whole word test sentence improve custom model mentioned result
pandas merge multiple Dataframes and the do text analysis?,"<h1>Problem statement:</h1>

<p>I have this crummy file from a government department that lists the operation schedules of 500+ bus routes across multiple sheets in a single excel. There is really no structure here and the author seems to have a single objective - pack everything tight in a single file !</p>

<p>Now, what am I trying to do:</p>

<p>Do extensive text analysis to extract the starting time of each run on the route. Please note there are multiple routes on a single sheet and then there are around 12 sheets in all.</p>

<p>I am cutting my teeth with the pandas library and stuck at this point:</p>

<p>Have a dictionary where 
Key:   sheet name (random str to identify the route sequence)
Value: DataFrame created with all cell data on that sheet.</p>

<h1>What do I would like to know:</h1>

<ol>
<li><p>Create one gigantic DataFrame that has all the rows from across the 12 sheets. Start with my text analysis post this step.</p></li>
<li><p>Is that above the right way forward?</p></li>
</ol>

<p>Thanks in advance.
AT</p>
",Named Entity Recognition (NER),panda merge multiple dataframes text analysis problem statement crummy file government department list operation schedule bus route across multiple sheet single excel really structure author seems single objective pack everything tight single file trying extensive text analysis extract starting time run route please note multiple route single sheet around sheet cutting teeth panda library stuck point dictionary key sheet name random str identify route sequence value dataframe created cell data sheet would like know create one gigantic dataframe ha row across sheet start text analysis post step right way forward thanks advance
Training an NER classifier to recognise Author names,"<p>I want to use NER(CRF classifier) to identify Author names in a query. I trained NER following the method  given in  <strong>nlp.stanford.edu</strong> site using the training file:<a href=""https://i.sstatic.net/c78ay.png"" rel=""nofollow noreferrer"">training-data.col</a>. And tested using the file:<a href=""https://i.sstatic.net/e5wGI.png"" rel=""nofollow noreferrer"">testing-data.tsv</a>.</p>

<p>The NER is tagging every input as Author, even the data that is tagged as non-Author in the training data. Can anyone tell me why NER is tagging the non-Authors in training data as Authors and how to train NER to identify Authors(I have the list of Author names to train).</p>

<p>Any suggestions for reference material on NER other than <strong>nlp.stanford.edu</strong> site will be helpful.</p>
",Named Entity Recognition (NER),training ner classifier recognise author name want use ner crf classifier identify author name query trained ner following method given nlp stanford edu site using training file training data col tested using file testing data tsv ner tagging every input author even data tagged non author training data anyone tell ner tagging non author training data author train ner identify author list author name train suggestion reference material ner nlp stanford edu site helpful
How to insert training data into spacy standalone NER trainer?,"<p>My aim to train standalone NER with the help of spacy.
Spacy provides <a href=""https://github.com/explosion/spaCy/blob/master/examples/train_ner_standalone.py"" rel=""nofollow noreferrer"">standalone ner trainer</a></p>

<p>which can help to save model and load it later into the nlp pipeline.</p>

<p>I have prepared training data which in the following give format.</p>

<pre><code>MUST,      O
have,       O
experience, U-technology
as,         O
a,          O
translator, O
, ,         O
editor,     U-technology
,,          O
interpreter,U-technology
,,          O
or,         O
writer,     U-technology
</code></pre>

<p>but i am not quite getting  way into to insert it as they give in their <a href=""https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py"" rel=""nofollow noreferrer"">integrated ner</a> with spacy</p>
",Named Entity Recognition (NER),insert training data spacy standalone ner trainer aim train standalone ner help spacy spacy provides standalone ner trainer help save model load later nlp pipeline prepared training data following give format quite getting way insert give integrated ner spacy
How to extract only entities not intents out of string using NLP?,"<p>I am buildingg a Messenger bot with <a href=""https://www.botkit.ai/"" rel=""nofollow noreferrer"">botkit</a> and language processing is done via <a href=""https://api.ai/"" rel=""nofollow noreferrer"">api.ai</a>.</p>
<p>Apiai's intents are a great way to guide user through the bot's experience. I use predefiened intents to understand what user want to do and the start a conversation with him using botkit's <a href=""https://github.com/howdyai/botkit/blob/master/docs/readme.md#start-a-conversation"" rel=""nofollow noreferrer"">convo</a> object. At this point I have predefined questions that user need to answer.</p>
<p>For example one of the questions is &quot;When did that happen?&quot; and user can answer in plain text. I do not need for apiai to tell me the intent (as well as I dont want to spend time training for that) because I already know what to expect.</p>
<p>So I am looking for a way to simply extract system and developer-defined entities out of string. So If user provides answer &quot;I happened yesterday&quot; I could validate that I have entities of date but the time entity is empty so I will promt to give time as well.</p>
<p>I have read apiai and other competitor docs and have not found a way to do that. It is always about defining intents.</p>
<p>So basically - find and extract entities from string. Is it possible and if is - how to? Currently apiai is my tool but I am willing to change it if neccesary.</p>
",Named Entity Recognition (NER),extract entity intent string using nlp buildingg messenger bot botkit language processing done via api ai apiai intent great way guide user bot experience use predefiened intent understand user want start conversation using botkit convo object point predefined question user need answer example one question happen user answer plain text need apiai tell intent well dont want spend time training already know expect looking way simply extract system developer defined entity string user provides answer happened yesterday could validate entity date time entity empty promt give time well read apiai doc found way always defining intent basically find extract entity string possible currently apiai tool willing change neccesary
Extract Entities and Relationships,"<p>Given text documents (student essays with about 100 words per essay) I want to extract entities and relationships important to the context of the sentence (maybe by considering <strong>Noun Phrase</strong> and <strong>Verb Phrase</strong>) to automatically score the answer. </p>

<p>Are there any popular algorithms/tools that I can use to perform this task?</p>
",Named Entity Recognition (NER),extract entity relationship given text document student essay word per essay want extract entity relationship important context sentence maybe considering noun phrase verb phrase automatically score answer popular algorithm tool use perform task
Using scikit-learn to training an NLP log linear model for NER,"<p>I wonder how to use <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"" rel=""noreferrer""><code>sklearn.linear_model.LogisticRegression</code></a> to train an NLP log linear model for named-entity recognition (NER).</p>

<p>A typical log-linear model for defines a conditional probability as follows:</p>

<p><a href=""https://i.sstatic.net/4Yp7D.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/4Yp7D.png"" alt=""enter image description here""></a></p>

<p>with:</p>

<ul>
<li>x: the current word</li>
<li>y: the class of a word being considered</li>
<li>f: the feature vector function, which maps a word x and a class y to a vector of scalars.</li>
<li>v: the feature weight vector</li>
</ul>

<p>Can <code>sklearn.linear_model.LogisticRegression</code> train such a model?</p>

<p>The issue is that features depend on the class.</p>
",Named Entity Recognition (NER),using scikit learn training nlp log linear model ner wonder use train nlp log linear model named entity recognition ner typical log linear model defines conditional probability follows x current word class word considered f feature vector function map word x class vector scalar v feature weight vector train model issue feature depend class
Named Entities in NLP,"<p>I am trying to get the named entities in a small test using stanford NER tagger. Here is the code:</p>

<pre><code>from nltk.tag import StanfordNERTagger
st = StanfordNERTagger('english.all.3class.distsim.crf.ser')
print st.tag('Ram offers computer science course'.split())
</code></pre>

<p>Output I get is:</p>

<pre><code>[(u'Ram', u'O'), (u'offers', u'O'), (u'computer', u'O'), (u'science', u'O'), (u'course', u'O')]
</code></pre>

<p>I am not getting why all the tags are 'O'. While Ram is a person, Computer is an object,science and course are also classifiable objects.</p>

<p>Is there any way I can get named entities for my sentence Like below:</p>

<pre><code>Ram -&gt; Person
Coputer Science -&gt; Course
</code></pre>
",Named Entity Recognition (NER),named entity nlp trying get named entity small test using stanford ner tagger code output get getting tag ram person computer object science course also classifiable object way get named entity sentence like
Can we add conditions to the output of a deep neural network?,"<p>I am currently working on a clinical NER tagging application (CliNER - Clinical Named Entity Recognition system) and I am using a Bi-Directional LSTM built with Keras / Tensorflow backend for this task. </p>

<p>I want to know whether there is any method for adding a condition to the output.</p>

<pre><code>For E.X. if the previous output was B-Medicine my next should definitely
I-Medicine or Other and not some random tag like B-Body-Part. 
</code></pre>

<p>Is there a way to accomplish this? </p>
",Named Entity Recognition (NER),add condition output deep neural network currently working clinical ner tagging application cliner clinical named entity recognition system using bi directional lstm built kera tensorflow backend task want know whether method adding condition output way accomplish
Best way to pre-tag a dataset of words to be used to train a MITIE entity extractor on?,"<p>I want to use the MITIE NER trainer to build an entity extractor. However is there a more efficient way to tag the training data rather than hard coding the location of each one?</p>

<p>Thanks in advance :) </p>
",Named Entity Recognition (NER),best way pre tag dataset word used train mitie entity extractor want use mitie ner trainer build entity extractor however efficient way tag training data rather hard coding location one thanks advance
Named Entity Tagger,"<p>I have made a function that extracts Gpe but it doesn't work. 
The error generated is ""invalid syntax"" in the line that i will underline with * * in the code. </p>

<pre><code>def EstraiLuoghi(frasi): 
TokensTOT = []
TokensPOStot = []
NamedGPE = []
for frase in frasi:
    tokens=nltk.word_tokenize(frase)
    tokensPOS=nltk.pos_tag(tokens)
    analisi=nltk.ne_chunk(tokensPOS)
    for nodo in analisi:    
        NE=''
        if hasattr(nodo, 'label'):  
            if nodo.label() in [""GPE""]:
                for partNE in nodo.leaves():   
                    NE=NE+' '+partNE[0]
                NamedGPE.append(NE)    
    TokensTOT=TokensTOT+tokens
TokensPOStot=TokensPOStot+tokensPOS
return TokensTOT, TokensPOStot, NamedGPE


TokensTOT1, TokensPOStot1, NamedGPEC1 = EstraiLuoghi(frasi1)********   
freqGPEC1 = nltk.FreqDist(NamedGPEC1)                       
luoghiOrdinatiC1 = freqGPEC1.most_common(20) 

TokensTOT2, TokensPOStot2, NamedGPEC2 = EstraiLuoghi(frasi2)    
freqGPEC2 = nltk.FreqDist(NamedGPEC2)   
luoghiOrdinatiC2 = freqGPEC2.most_common(20)
</code></pre>
",Named Entity Recognition (NER),named entity tagger made function extract gpe work error generated invalid syntax line underline code
How to create Custom model using OpenNLP?,"<p>I am trying to <strong>extract entities</strong> like <strong>Names, Skills</strong> from document using <strong>OpenNLP Java API</strong>. but <strong>it is not extracting proper Names</strong>. I am using model available on <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow"">opennlp sourceforge link</a></p>

<p>Here is a piece of java code-</p>

<pre><code>public class tikaOpenIntro {

    public static void main(String[] args) throws IOException, SAXException,
            TikaException {

        tikaOpenIntro toi = new tikaOpenIntro();
        toi.filest("""");
        String cnt = toi.contentEx();
        toi.sentenceD(cnt);
        toi.tokenization(cnt);

        String names = toi.namefind(toi.Tokens);
        toi.files(names);

    }

    public String Tokens[];

    public String contentEx() throws IOException, SAXException, TikaException {
        InputStream is = new BufferedInputStream(new FileInputStream(new File(
                ""/home/rahul/Downloads/rahul.pdf"")));
        // URL url=new URL(""http://in.linkedin.com/in/rahulkulhari"");
        // InputStream is=url.openStream();
        Parser ps = new AutoDetectParser(); // for detect parser related to

        BodyContentHandler bch = new BodyContentHandler();

        ps.parse(is, bch, new Metadata(), new ParseContext());

        return bch.toString();

    }

    public void files(String st) throws IOException {
        FileWriter fw = new FileWriter(""/home/rahul/Documents/extrdata.txt"",
                true);
        BufferedWriter bufferWritter = new BufferedWriter(fw);
        bufferWritter.write(st + ""\n"");
        bufferWritter.close();
    }

    public void filest(String st) throws IOException {
        FileWriter fw = new FileWriter(""/home/rahul/Documents/extrdata.txt"",
                false);

        BufferedWriter bufferWritter = new BufferedWriter(fw);
        bufferWritter.write(st);
        bufferWritter.close();
    }

    public String namefind(String cnt[]) {
        InputStream is;
        TokenNameFinderModel tnf;
        NameFinderME nf;
        String sd = """";
        try {
            is = new FileInputStream(
                    ""/home/rahul/opennlp/model/en-ner-person.bin"");
            tnf = new TokenNameFinderModel(is);
            nf = new NameFinderME(tnf);

            Span sp[] = nf.find(cnt);

            String a[] = Span.spansToStrings(sp, cnt);
            StringBuilder fd = new StringBuilder();
            int l = a.length;

            for (int j = 0; j &lt; l; j++) {
                fd = fd.append(a[j] + ""\n"");

            }
            sd = fd.toString();

        } catch (FileNotFoundException e) {

            e.printStackTrace();
        } catch (InvalidFormatException e) {

            e.printStackTrace();
        } catch (IOException e) {

            e.printStackTrace();
        }
        return sd;
    }


    public void sentenceD(String content) {
        String cnt[] = null;
        InputStream om;
        SentenceModel sm;
        SentenceDetectorME sdm;
        try {
            om = new FileInputStream(""/home/rahul/opennlp/model/en-sent.bin"");
            sm = new SentenceModel(om);
            sdm = new SentenceDetectorME(sm);
            cnt = sdm.sentDetect(content);

        } catch (IOException e) {
            e.printStackTrace();
        }

    }

    public void tokenization(String tokens) {

        InputStream is;
        TokenizerModel tm;

        try {
            is = new FileInputStream(""/home/rahul/opennlp/model/en-token.bin"");
            tm = new TokenizerModel(is);
            Tokenizer tz = new TokenizerME(tm);
            Tokens = tz.tokenize(tokens);
            // System.out.println(Tokens[1]);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

}
</code></pre>

<p>what am i trying to do is : </p>

<ul>
<li>i am using <strong>Apache Tika</strong>  to convert PDF document into plain text document.</li>
<li>I am passing plain text document for <strong>sentence boundary detection.</strong></li>
<li>After this <strong>tokenization</strong></li>
<li>after this <strong>Name entity extraction</strong></li>
</ul>

<p>But it is extracting names and other words.  <strong>It is not extract proper names.</strong> and <strong>how to create Custom model to extract Skills from document like Swimming, Programming etc?</strong> </p>

<p><strong>Give me some idea!</strong> </p>

<p><strong>Any help will be greatly appreciated!?</strong></p>
",Named Entity Recognition (NER),create custom model using opennlp trying extract entity like name skill document using opennlp java api extracting proper name using model available opennlp sourceforge link piece java code trying using apache tika convert pdf document plain text document passing plain text document sentence boundary detection tokenization name entity extraction extracting name word extract proper name create custom model extract skill document like swimming programming etc give idea help greatly appreciated
Machine Learning/NLP vs Keyword Searches to convert unstructured data into structured data,"<p>I oversee a research project where we aggregate newspaper articles on political violence in Africa, and then identify and code incidents. We keep track of where and when the incident took place, the actors involved, the number of people killed, etc. You can see the dataset here: </p>

<p><a href=""https://docs.google.com/spreadsheets/d/1_QYl4xhMu5nZVluprOgRs6rUzgkkBemapdsg5lFzKU/pubhtml"" rel=""nofollow noreferrer"">https://docs.google.com/spreadsheets/d/1_QYl4xhMu5nZVluprOgRs6rUzgkkBemapdsg5lFzKU/pubhtml</a></p>

<p>This is a labor intensive process and I think machine learning could be helpful. I'm trying to figure out the best approach. </p>

<p>My question: Am I better of using a set of keywords to decide how to code each article? I.e. </p>

<pre><code>if ""boko haram"" in article:
     code Boko Haram

or 

if [list of locations] in article:
    code location
</code></pre>

<p>Or can I use my existing dataset and the text from the articles and apply machine learning to do the feature extraction?</p>

<p>Some features are straightforward: if the article describes a violent event and Boko Haram is mentioned, we code Boko Haram. Or if a bomb is mentioned, we code bomb.</p>

<p>Some are more complicated. To determine if the event is ""sectarian"", we look for violent events where conflict between ethnic groups is referenced ('Fulani', 'Igbo', etc)</p>

<p>We code location based on a list of 774 districts. The challenge here is that there are often multiple spellings for the same place. Time is also complicated because the event is usually described as ""last Tuesday,"" or ""Wednesday night."" </p>

<p>I did experiment with this a bit awhile ago using TextBlob's Naive Bayes Classifier to try to figure out location. I bumped into two problems. My program would either never finish. I'm assuming performing nlp on two thousand 500 word articles requires more juice than my Macbook Air can handle. The other was encoding issues with the article text. I'm hoping that switching to python 3 will help resolve this.</p>

<p>If I'm going to sink some time into this, I love some recommendations on the best path to take. If it is indeed machine learning, maybe i should be using something other than naive bayes? Maybe I should be running this in the cloud so I have more power? A different package from TextBlob? </p>

<p>Guidance is much appreciated!</p>
",Named Entity Recognition (NER),machine learning nlp v keyword search convert unstructured data structured data oversee research project aggregate newspaper article political violence africa identify code incident keep track incident took place actor involved number people killed etc see dataset labor intensive process think machine learning could helpful trying figure best approach question better using set keywords decide code article e use existing dataset text article apply machine learning feature extraction feature straightforward article describes violent event boko haram mentioned code boko haram bomb mentioned code bomb complicated determine event sectarian look violent event conflict ethnic group referenced fulani igbo etc code location based list district challenge often multiple spelling place time also complicated event usually described last tuesday wednesday night experiment bit awhile ago using textblob naive bayes classifier try figure location bumped two problem program would either never finish assuming performing nlp two thousand word article requires juice macbook air handle wa encoding issue article text hoping switching python help resolve going sink time love recommendation best path take indeed machine learning maybe using something naive bayes maybe running cloud power different package textblob guidance much appreciated
Train data of synonyms word English with opennlp,"<p>I'm sorry, I'm newbie in NLP.
I'm using opennlp to create AI for find synonyms word into raw text and show them to website.
Example : If I press : I go to school, AI will show me ""go to"" synonyms with ""get to""...., and ""school"" synonyms with ""University"" or ""High School""
I try use named entity recognition for this purpose but It unpossible..
Can anyone help me for this ??</p>
",Named Entity Recognition (NER),train data synonym word english opennlp sorry newbie nlp using opennlp create ai find synonym word raw text show website example press go school ai show go synonym get school synonym university high school try use named entity recognition purpose unpossible anyone help
Need a good relation extractor,"<p>I'm doing a NLP project.</p>

<p>The purpose of the project is to extract possible relationship between two things. For example, for a pair ""location"" and ""person"" the extracted results would be ""near"", ""lives in"", ""works in"", etc.</p>

<p>Is there any existing NLP tool capable of doing this?</p>
",Named Entity Recognition (NER),need good relation extractor nlp project purpose project extract possible relationship two thing example pair location person extracted result would near life work etc existing nlp tool capable
New named entity class in Spacy,"<p>I need to train Spacy NER to be able to recognize 2 new classes for named entity recognition, all I have are files with list of items that are supposed to be in new classes.</p>

<p>For example: Rolling Stones, Muse, Arctic Monkeys - artists
Any idea how this can be done?</p>
",Named Entity Recognition (NER),new named entity class spacy need train spacy ner able recognize new class named entity recognition file list item supposed new class example rolling stone muse arctic monkey artist idea done
How to extract &quot;buyer&quot; and &quot;seller&quot; from a contract,"<p>I'm learning about natural language processing and I'm wondering if someone could point me in the right direction.
Say I have a bunch of contracts, and they have something like:</p>

<pre><code>Joe's Farm, hereafter known as the seller, 
and Bob's supermarket, hereafter known as the buyer, blah blah..
</code></pre>

<p>I'd like to be able to identify which party is the buyer and seller in this sentence. From what I have read, it should be theoretically possible to:</p>

<pre><code>1. Give the AI a lot of sample sentences and tell it ""this is the buyer/seller"".
2. After training, it should be able to analyze a new sentence.
</code></pre>

<p>I have tried some entity extraction (tokenizing the sentence and identifying the party names) but I don't know how to tell it ""this party is the buyer"".
One workaround is to identify segments of the sentence and search if that has the word ""buyer"" in it... which probably works in most cases, but I want to try to do this in an ""AI"" way.</p>

<p>Can anyone point me to the right direction on what to research?</p>
",Named Entity Recognition (NER),extract buyer seller contract learning natural language processing wondering someone could point right direction say bunch contract something like like able identify party buyer seller sentence read theoretically possible tried entity extraction tokenizing sentence identifying party name know tell party buyer one workaround identify segment sentence search ha word buyer probably work case want try ai way anyone point right direction research
Pattern Recognition OR Named Entity Recognition for Information Extraction in NLP,"<p>There are some event description texts.
I want to extract the entrance fee of the events.
Sometimes the entrance fee is conditional.</p>

<p>What I want to achieve is to extract the entrance fee and it's conditions(if available). It's fine to retrieve the whole phrase or sentence which tells the entrance fee + it's conditions.</p>

<p>Note I: The texts are in German language.
Note II: Often the sentences are not complete, as they are mainly event flyers or advertisements.</p>

<p>What would be the category of this problem in NLP? Is it Named Entity Recognition and could be solved by training an own model with Apache openNLP?
Or I thought maybe easier would be to detect the pattern via the usual keywords in the use-case(entrance, $, but, only till, [number]am/pm, ...).</p>

<p>Please shed some light on me.</p>

<p>Input Examples: 
- ""If you enter the club before 10pm, the entrance is for free. Afterwards it is 6$.""
- ""Join our party tonight at 11pm till 5am. The entrance fee is 8$. But for girls and students it's half price.""</p>
",Named Entity Recognition (NER),pattern recognition named entity recognition information extraction nlp event description text want extract entrance fee event sometimes entrance fee conditional want achieve extract entrance fee condition available fine retrieve whole phrase sentence tell entrance fee condition note text german language note ii often sentence complete mainly event flyer advertisement would category problem nlp named entity recognition could solved training model apache opennlp thought maybe easier would detect pattern via usual keywords use case entrance till number pm please shed light input example enter club pm entrance free afterwards join party tonight pm till entrance fee girl student half price
Why does Stanford CoreNLP server split named entities into single tokens?,"<p>I'm using this command to post the data (a bit of copy pasta from the stanford site):</p>

<pre><code>wget --post-data 'Barack Obama was President of the United States of America in 2016' 'localhost:9000/?properties={""annotators"": ""ner"", ""outputFormat"": ""json""}' -O out.json
</code></pre>

<p>The response looks like this:</p>

<pre><code>{
    ""sentences"": [{
        ""index"": 0,
        ""tokens"": [{
            ""index"": 1,
            ""word"": ""Barack"",
            ""originalText"": ""Barack"",
            ""lemma"": ""Barack"",
            ""characterOffsetBegin"": 0,
            ""characterOffsetEnd"": 6,
            ""pos"": ""NNP"",
            ""ner"": ""PERSON"",
            ""before"": """",
            ""after"": "" ""
        }, {
            ""index"": 2,
            ""word"": ""Obama"",
            ""originalText"": ""Obama"",
            ""lemma"": ""Obama"",
            ""characterOffsetBegin"": 7,
            ""characterOffsetEnd"": 12,
            ""pos"": ""NNP"",
            ""ner"": ""PERSON"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 3,
            ""word"": ""was"",
            ""originalText"": ""was"",
            ""lemma"": ""be"",
            ""characterOffsetBegin"": 13,
            ""characterOffsetEnd"": 16,
            ""pos"": ""VBD"",
            ""ner"": ""O"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 4,
            ""word"": ""President"",
            ""originalText"": ""President"",
            ""lemma"": ""President"",
            ""characterOffsetBegin"": 17,
            ""characterOffsetEnd"": 26,
            ""pos"": ""NNP"",
            ""ner"": ""O"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 5,
            ""word"": ""of"",
            ""originalText"": ""of"",
            ""lemma"": ""of"",
            ""characterOffsetBegin"": 27,
            ""characterOffsetEnd"": 29,
            ""pos"": ""IN"",
            ""ner"": ""O"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 6,
            ""word"": ""the"",
            ""originalText"": ""the"",
            ""lemma"": ""the"",
            ""characterOffsetBegin"": 30,
            ""characterOffsetEnd"": 33,
            ""pos"": ""DT"",
            ""ner"": ""O"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 7,
            ""word"": ""United"",
            ""originalText"": ""United"",
            ""lemma"": ""United"",
            ""characterOffsetBegin"": 34,
            ""characterOffsetEnd"": 40,
            ""pos"": ""NNP"",
            ""ner"": ""LOCATION"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 8,
            ""word"": ""States"",
            ""originalText"": ""States"",
            ""lemma"": ""States"",
            ""characterOffsetBegin"": 41,
            ""characterOffsetEnd"": 47,
            ""pos"": ""NNPS"",
            ""ner"": ""LOCATION"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 9,
            ""word"": ""of"",
            ""originalText"": ""of"",
            ""lemma"": ""of"",
            ""characterOffsetBegin"": 48,
            ""characterOffsetEnd"": 50,
            ""pos"": ""IN"",
            ""ner"": ""LOCATION"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 10,
            ""word"": ""America"",
            ""originalText"": ""America"",
            ""lemma"": ""America"",
            ""characterOffsetBegin"": 51,
            ""characterOffsetEnd"": 58,
            ""pos"": ""NNP"",
            ""ner"": ""LOCATION"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 11,
            ""word"": ""in"",
            ""originalText"": ""in"",
            ""lemma"": ""in"",
            ""characterOffsetBegin"": 59,
            ""characterOffsetEnd"": 61,
            ""pos"": ""IN"",
            ""ner"": ""O"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 12,
            ""word"": ""2016"",
            ""originalText"": ""2016"",
            ""lemma"": ""2016"",
            ""characterOffsetBegin"": 62,
            ""characterOffsetEnd"": 66,
            ""pos"": ""CD"",
            ""ner"": ""DATE"",
            ""normalizedNER"": ""2016"",
            ""before"": "" "",
            ""after"": """",
            ""timex"": {
                ""tid"": ""t1"",
                ""type"": ""DATE"",
                ""value"": ""2016""
            }
        }]
    }]
}
</code></pre>

<p>Am I doing something wrong? I have Java client code that would at least recognize <code>Barack Obama</code> and <code>United States of America</code> as full NERs, but using the service it seems to treat each token separately. Any ideas why?</p>
",Named Entity Recognition (NER),doe stanford corenlp server split named entity single token using command post data bit copy pasta stanford site response look like something wrong java client code would least recognize full ners using service seems treat token separately idea
When training NER with BIO chunks what would be the most suitable approach in following case?,"<p>In my case I want to tag following under DISEASE and SYMPTOM tags </p>

<p><em>Eg : Osgood-Schlatter disease is a painful inflammation of the upper portion of the tibia.</em></p>

<p>where I have successfully trained as <code>Osgood-Schlatter/b-disease disease/i-disease</code> (I have used BIO NER tag approach)</p>

<p>However I am not sure whether is it correct to tag remaining part <strong>""painful inflammation of the upper portion of the tibia""</strong> as Symptom itself.</p>

<p>as <code>painful/b-symptom  inflammation/i-symptom of/i-symptom the/i-symptom upper/i-symptom portion/i-symptom of/i-symptom the/i-symptom tibia/i-symptom</code></p>

<p>I would like any opinion whether to use my current approach or should I follow POS tag based parsing approach or any other suitable approach?</p>
",Named Entity Recognition (NER),training ner bio chunk would suitable approach following case case want tag following disease symptom tag eg osgood schlatter disease painful inflammation upper portion tibia successfully trained used bio ner tag approach however sure whether correct tag remaining part painful inflammation upper portion tibia symptom would like opinion whether use current approach follow po tag based parsing approach suitable approach
Best method to extract information from unstructured text,"<p>My aim is to extract information from old scanned reports and store in the structured database. I have already extracted text from these reports using Solr.</p>

<p>All of these are scientific reports and have a different structure in terms of the content of the report, but all of these has similar information. I wanted to a create a structured database using these reports such as name of the company involved in the report, name of the software involved in the report, name of the location, date of the experiment etc. For each of these fields, I have some keywords which shall be used for extraction, For example for the Location information: Location, Place of experiment, Place, Facility etc. What will be the best way to proceed in the direction?</p>

<p>Also, in some of these files, there are no sentences to process. Information is given in Form like structure, for example:</p>

<p>Location: Canada</p>

<p>Date of the experiment: 1985-05-01. </p>

<p>Which techniques will be best to extract the information? also which software, libraries should I use?</p>
",Named Entity Recognition (NER),best method extract information unstructured text aim extract information old scanned report store structured database already extracted text report using solr scientific report different structure term content report ha similar information wanted create structured database using report name company involved report name software involved report name location date experiment etc field keywords shall used extraction example location information location place experiment place facility etc best way proceed direction also file sentence process information given form like structure example location canada date experiment technique best extract information also software library use
The failure in using CRF+0.58 train NE Model,"<h2>when i use CRF++0.58 to model a NE and progarm have a problem:</h2>

<h3>""reading training data:tagger.cpp(399) [feature_index_->buildFeatures(this)] 0.00s""</h3>

<ol>
<li>the develop environment:

<ul>
<li>red hat linux 6.5,gcc 5.0,CRF++0.58</li>
</ul></li>
<li>written feature template:

<ul>
<li>template</li>
</ul></li>
<li>dataset:

<ul>
<li>Boson_train.txt</li>
<li>Boson_test.txt</li>
<li>the first column is words ,the second column is pos,the third column is NER tagger</li>
</ul></li>
<li>the problem:

<ul>
<li>when i want to train the NER model, i type this sentences ""crf_learn -f 3 -c 4.0 template Boson_train crf_model"", and i got
this notification, ""reading training data:tagger.cpp(399) [feature_index_->buildFeatures(this)] 0.00s"". I can't understand
the C++ language, so i can't fix the problem.</li>
</ul></li>
<li>the method i tryed:

<ul>
<li>1.change the encode type of dataset. I use notepad++ to change ""utf-8 with no BOM"" to ""utf-8"". It didn't work.</li>
<li>2.change the delimiter from '\t' to ' '(space). It didn't work.</li>
<li>3.And i think maybe the template was wrong.So i use the crf++0.58/example/seg/template for test. It worked. But this template
  is simple, so I use /example/JapaneseNE/template which is more similar with my feature template. It didn't  work. Then， i check
  the JapaneseNE example It works well. So i got confused. Is there someone can help me.</li>
</ul></li>
<li><p>template</p>

<ul>
<li>U00:%x[-2,0]</li>
<li>U01:%x[-1,0]</li>
<li>U02:%x[0,0]</li>
<li>U03:%x[1,0]</li>
<li>U04:%x[2,0]</li>
<li>U05:%x[-2,0]/%x[-1,0]/%x[0,0]</li>
<li>U06:%x[-1,0]/%x[0,0]/%x[1,0]</li>
<li>U07:%x[0,0]/%x[1,0]/%x[2,0]</li>
<li>U08:%x[-1,0]/%x[0,0]</li>
<li><p>U09:%x[0,0]/%x[1,0]</p></li>
<li><p>U10:%x[-2,1]/%x[0,1]</p></li>
<li>U11:%x[-2,1]/%x[1,1]</li>
<li>U11:%x[-1,1]/%x[0,1]</li>
<li>U12:%x[0,0]/%x[0,1]</li>
<li>U13:%x[0,1]/%x[1,1]</li>
<li>U14:%x[0,1]/%x[2,1]</li>
<li>U15:%x[-1,0]/%x[0,1]</li>
<li>U16:%x[-1,0]/%x[-1,1]</li>
<li>U17:%x[1,0]/%x[1,1]</li>
<li>U18:%x[1,0]/%x[1,1]</li>
<li><p>U19:%x[2,0]/%x[2,1]</p></li>
<li><p>U20:%x[-1,2]</p></li>
<li>U21:%x[-2,2]</li>
<li>U22:%x[0,1]/%x[-1,2]</li>
<li>U23:%x[0,1]/%x[-2,2]</li>
<li>U24:%x[0,0]/%x[-1,2]</li>
<li>U25:%x[0,0]/%x[-2,2]</li>
<li>U26:%x[-1,2]/%x[-2,2]/%x[0,1]</li>
<li>U27:%x[-2,2]/%x[0,1]/%x[1,1]</li>
<li>U28:%x[-1,1]/%x[-1,2]/%x[0,1]</li>
<li>U29:%x[-1,2]/%x[0,0]/%x[0,1]</li>
</ul></li>
<li>Boson_train

<ul>
<li>浙江    ns  B_product_name</li>
<li>在线    b   I_product_name</li>
<li>杭州    ns  I_product_name</li>
<li>4 m   B_time</li>
<li>月 m   I_time</li>
<li>25    m   I_time</li>
<li>日 m   I_time</li>
<li>讯 ng  Out</li>
<li>（ x   Out</li>
<li>记者    n   Out</li>
<li>x   Out</li>
<li>x   B_person_name</li>
<li>施宇翔   nr  I_person_name</li>
<li>x   Out</li>
<li>通讯员   n   B_person_name</li>
<li>x   Out</li>
<li>方英    nr  B_person_name</li>
<li>） x   Out</li>
<li>毒贩    n   Out</li>
<li>很 zg  Out</li>
<li>“ x   Out</li>
<li>时髦    nr  Out</li>
<li>” x   Out</li>
<li>， x   Out</li>
<li>用 p   Out</li>
<li>微信    vn  B_product_name</li>
<li>交易    n   Out</li>
<li>毒品    n   Out</li>
<li>。 x   Out</li>
<li>没 v   Out</li>
<li>料想    v   Out</li>
<li>警方    n   B_person_name</li>
<li>也 d   Out</li>
</ul></li>
</ol>
",Named Entity Recognition (NER),failure using crf train ne model use crf model ne progarm problem reading training data tagger cpp feature index buildfeatures develop environment red hat linux gcc crf written feature template template dataset boson train txt boson test txt first column word second column po third column ner tagger problem want train ner model type sentence crf learn f c template boson train crf model got notification reading training data tagger cpp feature index buildfeatures understand c language fix problem method tryed change encode type dataset use notepad change utf bom utf work change delimiter space work think maybe template wa wrong use crf example seg template test worked template simple use example japanesene template similar feature template work check japanesene example work well got confused someone help template u x u x u x u x u x u x x x u x x x u x x x u x x u x x u x x u x x u x x u x x u x x u x x u x x u x x u x x u x x u x x u x u x u x x u x x u x x u x x u x x x u x x x u x x x u x x x boson train n b product name b product name n product name b time time time time ng x n x x b person name nr person name x n b person name x nr b person name x n zg x nr x x p vn b product name n n x v v n b person name
R package that implements maxent for NLP tasks,"<p>I am searching for an R package that implements MaxEnt for NLP tasks (i.e. for situations with large no. of parameters).</p>

<p>The requirement is to perform Named Entity Recognition on medical text. I got used to relying on CRF++ for sequence labelling jobs but for this particular entity recognition task at hand, I dont want the transition/label input.</p>

<p>I have checked Maxent Package - it creates a tf-idf representation which I dont want. I need to provide my features as combinations of previous and next words, pos tags, etc. Very similar to how one would use CRF++.</p>
",Named Entity Recognition (NER),r package implement maxent nlp task searching r package implement maxent nlp task e situation large parameter requirement perform named entity recognition medical text got used relying crf sequence labelling job particular entity recognition task hand dont want transition label input checked maxent package creates tf idf representation dont want need provide feature combination previous next word po tag etc similar one would use crf
Stanford NLP: Keeping punctuation tokens?,"<p>I am looking for sentences such as </p>

<blockquote>
  <p>Bachelors Degree in early childhood teaching, psychology</p>
</blockquote>

<ul>
<li>I annotate the text using the Stanford Parser. </li>
<li>I then iterate each sentence and identify ""Bachelor's Degree"" using NER (named entity recognition).</li>
<li>By processing triples, I can see that the object follows ""BE IN"" and is likely to be a college major.</li>
<li>So I send the object phrase for further analysis. My trouble is that I don't know how to separate </li>
</ul>

<blockquote>
  <p>early childhood teaching</p>
</blockquote>

<p>from</p>

<blockquote>
  <p>psychology</p>
</blockquote>

<p>My code for this procedure loops through the object triple and keeps it if certain POS requirements are met.</p>

<pre><code>private void processTripleObject(List&lt;CoreLabel&gt; objectPhrase )
{
    try
    {
        StringBuilder sb = new StringBuilder();
        for(CoreLabel token: objectPhrase)
        {
            String pos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);

            TALog.getLogger().debug(""pos: ""+pos+""  word ""+token.word());
            if(!matchDegreeNameByPos(pos))
            {
                return;
            }

            sb.append(token.word());
            sb.append(SPACE);
        }

        IdentifiedToken itoken = new IdentifiedToken(IdentifiedToken.SKILL, sb.toString());

    }
    catch(Exception e)
    {
        TALog.getLogger().error(e.getMessage(),e);
    }
</code></pre>

<p>Since the comma between teaching and psychology is not in the tokens, I don't know how to recognize the divide.</p>

<p>Can anyone advise?</p>
",Named Entity Recognition (NER),stanford nlp keeping punctuation token looking sentence bachelor degree early childhood teaching psychology annotate text using stanford parser iterate sentence identify bachelor degree using ner named entity recognition processing triple see object follows likely college major send object phrase analysis trouble know separate early childhood teaching psychology code procedure loop object triple keep certain po requirement met since comma teaching psychology token know recognize divide anyone advise
Detecting country names with nltk not working on forms,"<p>Im parsing a form which has this text</p>

<p>'1a. Country United States'</p>

<p>Which is not getting detected as GPE</p>

<pre><code>from nltk import pos_tag, ne_chunk
from nltk.tokenize import SpaceTokenizer

tokenizer = SpaceTokenizer()
toks = tokenizer.tokenize(cioms_)
pos = pos_tag(toks)
chunked_nes = ne_chunk(pos) 

nes = [' '.join(map(lambda x: x[0], ne.leaves())) for ne in chunked_nes if isinstance(ne, nltk.tree.Tree)]
chunked_nes

Out[83]: Tree('S', [(u'1a.', 'CD'), Tree('ORGANIZATION', [(u'Country', 'NNP'), (u'United', 'NNP'), (u'States', 'NNPS')])])
</code></pre>

<p>But when I trim this to 'Country United States', its getting detected</p>

<pre><code>Out[81]: Tree('S', [Tree('PERSON', [(u'Country', 'NNP')]), Tree('GPE', [(u'United', 'NNP'), (u'States', 'NNPS')])])
</code></pre>

<p>Why is it so ?</p>
",Named Entity Recognition (NER),detecting country name nltk working form im parsing form ha text country united state getting detected gpe trim country united state getting detected
Possible to train NLTK to detect &quot;made up&quot; names in a sentence?,"<p>I've recently starting looking at data extraction using NLTK.  While there are several examples and techniques for detecting ""real"" names, locations, etc.. I haven't found an efficient way to detect ""made up"" or ""imaginary"" names.  An example string would be:</p>

<p><strong><em>His name is wuzzywugg and he has a dog named fizzbuzz</em></strong></p>

<p>I would like to train NLTK to be able to detect that ""wuzzywugg"" and ""fizzbuzz"" are names of characters.  Seen some solutions that rely on the word starting with a CAPITAL letter, but this feels very ""hacky""
and prone to errors and false positives. </p>

<p>Any help on how to solve this issue would be greatly appreciated.  Thanks in advance.</p>
",Named Entity Recognition (NER),possible train nltk detect made name sentence recently starting looking data extraction using nltk several example technique detecting real name location etc found efficient way detect made imaginary name example string would name wuzzywugg ha dog named fizzbuzz would like train nltk able detect wuzzywugg fizzbuzz name character seen solution rely word starting capital letter feel hacky prone error false positive help solve issue would greatly appreciated thanks advance
StanfordNLP : ArrayIndexOutOfBoundsException for Named Entity Recognition,"<p>I am trying to learn NER using <a href=""https://humphreysheil.blogspot.in/2014/10/named-entity-recognition-short-tutorial.html"" rel=""nofollow noreferrer"">this</a> short Named Entity Recognition tutorial. But I am unable to run the code successfully. I provided one entry in location.txt file as mentioned there.</p>

<p>I am getting <code>ArrayIndexOutOfBoundsException</code> error.</p>

<pre><code>09:32:09.431 [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator regexner

java.lang.ArrayIndexOutOfBoundsException: 1

at  edu.stanford.nlp.pipeline.TokensRegexNERAnnotator.readEntries(TokensRegexNERAnnotator.java:696)
at edu.stanford.nlp.pipeline.TokensRegexNERAnnotator.readEntries(TokensRegexNERAnnotator.java:593)
at edu.stanford.nlp.pipeline.TokensRegexNERAnnotator.&lt;init&gt;(TokensRegexNERAnnotator.java:294)
at edu.stanford.nlp.pipeline.AnnotatorImplementations.tokensRegexNER(AnnotatorImplementations.java:135)
at edu.stanford.nlp.pipeline.AnnotatorFactories$7.create(AnnotatorFactories.java:305)
at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:152)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:451)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:154)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:150)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:137)
</code></pre>

<p>Kindly help me.
Thanks in advance.</p>
",Named Entity Recognition (NER),stanfordnlp arrayindexoutofboundsexception named entity recognition trying learn ner using short named entity recognition tutorial unable run code successfully provided one entry location txt file mentioned getting error kindly help thanks advance
How to count [any name from a list of names] + [specific last name] in a block of text?,"<p>first time post here. I’m hoping I can find a little help on something I’m trying to accomplish in terms of text analysis. </p>

<p>First, I’m doing this in python and would like to remain in python as this function would be part of a larger, otherwise healthy tool I’m happy with. I have NLKT and Anaconda all set up as well, so drawing on those resources is also possible.</p>

<p>I’ve been working on a tool that tracks and adds up references to city names in large blocks of text. For instance, the tool can count how many times “Chicago,” “New York” or “Los Angeles,” “San Francisco” etc… are detected in a text chunk and can rank them. </p>

<p>The current problem I am having is figuring out how to remove false positives from city names that are also last names. So, for instance, I would want to count, say Jackson Mississippi, but not count “Frank Jackson” “Jane Jackson” etc…</p>

<p>What I would like to do however is figure out a way to account for any false positive that might be [any name from a long list of first names] + [Select last name]. </p>

<p>I have assembled a list of ~5000 first names from the census data that I can also bring into python as a list. I can also check true/false to find if a name is on that list, so I know I’m getting closer.</p>

<p>However, what I can’t figure out is how to express what I want, which is something like (I’ll use Jackson as an example again):</p>

<pre><code>totalfirstnamejacksoncount = count (“[any name from census list] + Jackson”)
</code></pre>

<p>More or less. Is there some way I can phrase it as a wildcard from census list so ? Set a variable that would read as “any item in this list” so I could go “anynamevariable + Jackson,”?  Or is there any other way to denote something like “any word in census list + Jackson”? </p>

<p>Ideally, my aim is to get a total count of “[Any first name] + [Specified last name]” so I can a) subtract them from the total of [Last name that is also a city name] count and maybe use that count for some other refinements. </p>

<p>In a worst case scenario I can see a way I could directly modify the census list and add  Jackson (or whatever last name I need) to each name and have the lines manually add, but I feel like that would make a complete mess of my code when you look at ~5000 names for each name I’d like to do. </p>

<p>Sorry for the long-winded post. I appreciate your help with all this. If you have other suggestions you think might be better ways to approach it I’m happy to hear those out as well. </p>
",Named Entity Recognition (NER),count name list name specific last name block text first time post hoping find little help something trying accomplish term text analysis first python would like remain python function would part larger otherwise healthy tool happy nlkt anaconda set well drawing resource also possible working tool track add reference city name large block text instance tool count many time chicago new york los angeles san francisco etc detected text chunk rank current problem figuring remove false positive city name also last name instance would want count say jackson mississippi count frank jackson jane jackson etc would like however figure way account false positive might name long list first name select last name assembled list first name census data also bring python list also check true false find name list know getting closer however figure express want something like use jackson example le way phrase wildcard census list set variable would read item list could go anynamevariable jackson way denote something like word census list jackson ideally aim get total count first name specified last name subtract total last name also city name count maybe use count refinement worst case scenario see way could directly modify census list add jackson whatever last name need name line manually add feel like would make complete mess code look name name like sorry long winded post appreciate help suggestion think might better way approach happy hear well
"Get output from SyntaxNet as python object, not text","<p>After executing some of an example syntaxnet scripts(like <code>parse.sh</code>) I receive output in <em>text-conll</em> format. My goal is to take some features and proceed them to next network. One possible choice is to parse text output with something like <code>nltk.corpus.reader.ConllCorpusReader</code> to a python object. But for me interesting 
is: </p>

<p>It is possible with some code modification to get from SyntaxNet not text, but Python object related to parsed results?</p>

<p>I've found that in <code>parser_eval.py</code> on lines 133-138 syntaxnet fetched already text version of results.</p>

<pre><code>while True:
    tf_eval_epochs, tf_eval_metrics, tf_documents = sess.run([
        parser.evaluation['epochs'],
        parser.evaluation['eval_metrics'],
        parser.evaluation['documents'],
    ])
</code></pre>

<p>But I cannot locate the place from what object this text was generated and how.</p>
",Named Entity Recognition (NER),get output syntaxnet python object text executing example syntaxnet script like receive output text conll format goal take feature proceed next network one possible choice parse text output something like python object interesting possible code modification get syntaxnet text python object related parsed result found line syntaxnet fetched already text version result locate place object text wa generated
Bigram analysis and Term document Matrix,"<p>I am.doing a bigram analyis on my text corpus. My feature vector is a predefined set of bigram and unigram tokens.</p>

<p><strong>Feature vector</strong> = ( good location, bad experience, clean, unfriendly, tidy, excellent, beautiful place) </p>

<p><strong>my text</strong> :  location is good but unfriendly staff. </p>

<p><strong>Cleaned text :</strong> location good unfriendly staff.</p>

<p>I created a tdf using the above dictionary and cleaned text but the ""location good"" bigram is not giving a ""1"". 
But when I changed the cleaned text to  ""good location unfriendly staff"".
In a bigram analysis do the order of the words matter and why ? or am i messing up with the code ? Kindly clarify </p>

<p>""bad experience""    ""tidy""  ""clean"" ""good location"" ""excellent"" ""beautiful"" ""place"" ""unfriendly""</p>

<p>0   0   0   0   0   0   1 -- location good but  unfriendly staff.   </p>

<p>0   0   0   1   0   0   1  --  good location but  unfriendly staff.</p>
",Named Entity Recognition (NER),bigram analysis term document matrix bigram analyis text corpus feature vector predefined set bigram unigram token feature vector good location bad experience clean unfriendly tidy excellent beautiful place text location good unfriendly staff cleaned text location good unfriendly staff created tdf using dictionary cleaned text location good bigram giving changed cleaned text good location unfriendly staff bigram analysis order word matter messing code kindly clarify bad experience tidy clean good location excellent beautiful place unfriendly location good unfriendly staff good location unfriendly staff
What is pseudo code of maxent classifier for NER?,"<p>I am building a Named Entity Recognizer with a Maximum Entropy and am looking for two things:</p>

<p>1) what is meant by features?</p>

<p>2) how features will extract from training data set?</p>
",Named Entity Recognition (NER),pseudo code maxent classifier ner building named entity recognizer maximum entropy looking two thing meant feature feature extract training data set
Accuracy: ANNIE vs Stanford NLP vs OpenNLP with UIMA,"<p>My work is planning on using a UIMA cluster to run documents through to extract named entities and what not.  As I understand it, UIMA have very few NLP components packaged with it.  I've been testing GATE for awhile now and am fairly comfortable with it.  It does ok on normal text, but when we run it through some representative test data, the accuracy drops way down.  The text data we have internally is sometimes all caps, sometimes all lowercase, or a mix of the two in the same document.  Even using ANNIE's all caps rules, the accuracy still leaves much to be desired.  I've recently heard of Stanford NLP and OpenNLP but haven't had time to extensively train and test them.  How do those two compare in terms of accuracy with ANNIE?  Do they work with UIMA like GATE does?</p>

<p>Thanks in advance.</p>
",Named Entity Recognition (NER),accuracy annie v stanford nlp v opennlp uima work planning using uima cluster run document extract named entity understand uima nlp component packaged testing gate awhile fairly comfortable doe ok normal text run representative test data accuracy drop way text data internally sometimes cap sometimes lowercase mix two document even using annie cap rule accuracy still leaf much desired recently heard stanford nlp opennlp time extensively train test two compare term accuracy annie work uima like gate doe thanks advance
Detecting employee designation from text using ner/nlp,"<p>I am very new to the field of NLP and i am interested in detecting the position/designation/role along with their name, email,phone number etc. I tried using stanford NLP to detect names from text. Email and phone number parsing seems pretty straightforward. I am unable to however detect the designation from a given text.</p>

<p>For instance, here are some sample examples of text</p>

<p>1)Medical Superintendent,Dr. A.B. Ahmad,example1@example.com<br/>
Name:Dr. A.B. Ahmad, Email: example1@example.com</p>

<p>2)Sub-Dean Academics    Prof. S. Antony  example2@example.com<br/>
Name:Prof. S. Antony, Email: example2@example.com</p>

<p>3)Sub-Dean Academics &amp; PG-Cell &amp; Surg. Discipline Resident Trg. Programe,Mr. Sandeep<br/>
Name: Mr. Sandeep, Email: none</p>

<p>4)Director, Networking, Robert Adams, example3@example.com,9900131213<br/>
Name: Robert Adams, Email: example3@example.com, Phone: 9900131213</p>

<p>I am not interested in any regex matching algorithms since the nature of the text is non deterministic. What i am interested in knowing is how do i go about extracting the above designtations from the text. Any solution even beyond stanford NLP like using nltk, lingpipe etc is fine. If i am using stanford NLP, how do i build a training model for the same with a different entity type like ""POSITION"" or ""DESIGNATION"" and how do i include this model along with my other models(i am running stanford NLP in server mode).</p>
",Named Entity Recognition (NER),detecting employee designation text using ner nlp new field nlp interested detecting position designation role along name email phone number etc tried using stanford nlp detect name text email phone number parsing seems pretty straightforward unable however detect designation given text instance sample example text medical superintendent dr b ahmad example example com name dr b ahmad email example example com sub dean academic prof antony example example com name prof antony email example example com sub dean academic pg cell surg discipline resident trg programe mr sandeep name mr sandeep email none director networking robert adam example example com name robert adam email example example com phone interested regex matching algorithm since nature text non deterministic interested knowing go extracting designtations text solution even beyond stanford nlp like using nltk lingpipe etc fine using stanford nlp build training model different entity type like position designation include model along model running stanford nlp server mode
Tagging and Training NER dataset,"<p>I have a data set and I want to tag it for Named Entity Recognition. My dataset is in Persian.
I want to know how should I tag expressions like :</p>

<p>*** آقای مهدی کاظمی  = Mr Mehdi Kazemi / Mr will Smith. >>> (names with titles) should I tag all as a person or just the first name and last name should be tagged? (I mean should i also tag ""Mr"")</p>

<p>Mr  >>  b_per     ||     Mr >> O</p>

<p>Mehdi  >>   i_per || Mehdi >> b_per</p>

<p>Kazemi  >> i_per || Kazemi >> i_per</p>

<p>*** بیمارستان نور = Noor hospital >>> Should I tag the name only or the name and hospital both as Named Entity?</p>

<p>*** Eiffel tower / The Ministry of Defense (I mean the us DOD for example) >>> in Persian it is called :
وزارت دفاع    (vezarate defa)
should I only tag Defense ? or all together?</p>

<p>There are many more examples for schools, movies, cities, countries and.... since we use the entity class before the named entity.</p>

<p>I would appreciate if you can help me with tagging this dataset.</p>
",Named Entity Recognition (NER),tagging training ner dataset data set want tag named entity recognition dataset persian want know tag expression like mr mehdi kazemi mr smith name title tag person first name last name tagged mean also tag mr mr b per mr mehdi per mehdi b per kazemi per kazemi per noor hospital tag name name hospital named entity eiffel tower defense mean u dod example persian called vezarate defa tag defense together many example school movie city country since use entity class named entity would appreciate help tagging dataset
Running CRFSuite examples,"<p>I'm trying to use CRFSuite but I can't figure out how to use the example/ner.py and pos.py</p>

<p>Precisely, how do I make an input of the form:</p>

<pre><code># Ner.py
fields = 'y w pos chk'
</code></pre>

<p>or</p>

<pre><code># Pos.py
fields = 'w num cap sym p1 p2 p3 p4 s1 s2 s3 s4 y'
</code></pre>

<p>The ""y w pos"" I can get from a CoNNL model, for example, but the ""chk"" part and all those fields in pos.py I don't really get.</p>

<p>Also, is there a way to process a raw text (without all those tags) with CRFSuite given that I have a trained model?</p>
",Named Entity Recognition (NER),running crfsuite example trying use crfsuite figure use example ner py po py precisely make input form w po get connl model example chk part field po py really get also way process raw text without tag crfsuite given trained model
How to extract meaning from sentences after running named entity recognition?,"<p>First: Any recs on how to modify the title?</p>

<p>I am using my own named entity recognition algorithm to parse data from plain text.  Specifically, I am trying to extract lawyer practice areas.  A common sentence structure that I see is:</p>

<p>1) Neil focuses his practice on employment, tax, and copyright litigation.</p>

<p>or</p>

<p>2) Neil focuses his practice on general corporate matters including securities, business organizations, contract preparation, and intellectual property protection.</p>

<p>My entity extraction is doing a good job of finding the key words, for example, my output from sentence one might look like this:</p>

<p>Neil focuses his practice on (employment), (tax), and (copyright litigation).</p>

<p>However, that doesn't really help me.  What would be more helpful is if i got an output that looked more like this:</p>

<p>Neil focuses his practice on (employment - litigation), (tax - litigation), and (copyright litigation).</p>

<p>Is there a way to accomplish this goal using an existing python framework such as nltk (after my algo extracts the practice areas) can I use ntlk to extract the other words that my ""practice areas"" modify in order to get a more complete picture?</p>
",Named Entity Recognition (NER),extract meaning sentence running named entity recognition first recs modify title using named entity recognition algorithm parse data plain text specifically trying extract lawyer practice area common sentence structure see neil focus practice employment tax copyright litigation neil focus practice general corporate matter including security business organization contract preparation intellectual property protection entity extraction good job finding key word example output sentence one might look like neil focus practice employment tax copyright litigation however really help would helpful got output looked like neil focus practice employment litigation tax litigation copyright litigation way accomplish goal using existing python framework nltk algo extract practice area use ntlk extract word practice area modify order get complete picture
Determine document novelty/similarity with the aid of Latent Dirichlet allocation (LDA) or Named Entities,"<p>Given an index or database with a lot of (short) documents (~ 1 million), I am trying to do some kind of novelty detection for each newly incoming document.</p>

<p>I know that I have to compute the similarity of the new document with each document in the index. If the similarity is below a certain threshold, one can consider this document as novel. One common approach - which I want to do - is to use a Vector Space Model and compute the cosine similarity (e.g. by using Apache Lucene).</p>

<p>But this approach has two shortcomings: <strong>1)</strong> it is computationally expensive and <strong>2)</strong> it does not incorporate the semantics of documents and words respectively.</p>

<p>In order to overcome these shortcomings, my idea was to either use an LDA topic distribution or named entities to augment the Lucene index and the query (i.e. the document collection and each new document) with semantics.</p>

<p>Now, I am completely lost regarding the concrete implementation. I have already trained an LDA topic model using Mallet and I am also able to do Named Entity Recognition on the corpus. But I do not know <strong>how</strong> to use these topics and named entities in order to realise novelty detection. More specifically, I do not know how to use these features for index and query creation.</p>

<p>For example, is it already sufficient to store all named entities of one document as a separate field in the index, add certain weights (i.e. boost them) and use a MultiFieldQuery? I do not think that this already adds some kind of semantics to the similarity detection. The same applies to LDA topics: is it sufficient to add the topic probability of each term as a Payload and implement a new similarity score?</p>

<p>I would be very happy if you could provide some hints or even code snippets on how to incorporate LDA topics or named entities in Lucene for some kind of novelty detection or semantic similarity measure.</p>

<p>Thank you in advance.</p>
",Named Entity Recognition (NER),determine document novelty similarity aid latent dirichlet allocation lda named entity given index database lot short document million trying kind novelty detection newly incoming document know compute similarity new document document index similarity certain threshold one consider document novel one common approach want use vector space model compute cosine similarity e g using apache lucene approach ha two shortcoming computationally expensive doe incorporate semantics document word respectively order overcome shortcoming idea wa either use lda topic distribution named entity augment lucene index query e document collection new document semantics completely lost regarding concrete implementation already trained lda topic model using mallet also able named entity recognition corpus know use topic named entity order realise novelty detection specifically know use feature index query creation example already sufficient store named entity one document separate field index add certain weight e boost use multifieldquery think already add kind semantics similarity detection applies lda topic sufficient add topic probability term payload implement new similarity score would happy could provide hint even code snippet incorporate lda topic named entity lucene kind novelty detection semantic similarity measure thank advance
"NLP - Extract the action verb, noun from a list of instructions","<p>I am looking for some directions on how to perform my ideas.
I will have a recipe with the ingredients' name, equipment list and the cooking method. What I want to do is simplifed the cooking method into just the action verb, ingredients' name and equipment's name.</p>

<ul>
<li>For example: 

<ol>
<li>Ingredient: Eggs , pepper , salt</li>
<li>Equipment: Jug</li>
<li>Method: 1. Crack the eggs into a jug.

<ol start=""2"">
<li>Add pepper and salt </li>
</ol></li>
</ol></li>
</ul>

<p>The output will be store in a list.
Output: 1. Crack , eggs , jug
        2. Add, pepper, jug
        3. Add, salt , jug</p>
",Named Entity Recognition (NER),nlp extract action verb noun list instruction looking direction perform idea recipe ingredient name equipment list cooking method want simplifed cooking method action verb ingredient name equipment name example ingredient egg pepper salt equipment jug method crack egg jug add pepper salt output store list output crack egg jug add pepper jug add salt jug
"Title (Mr., Mrs., etc.) Inconsistencies with Stanford NER Tagger","<p>I have been working with Stanford's Named Entity Recognition (NER) tagger (<a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/CRF-NER.shtml</a>) in Java and Python, and I've stumbled on an inconsistency that I cannot solve.</p>

<p>Here is the sentence I'm using as an example:</p>

<pre><code>""I said hello to Mr. Jones, and then I went on my way.""
</code></pre>

<p>In the online demo (<a href=""http://nlp.stanford.edu:8080/ner/process"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/ner/process</a>) of the NER tagger, this returns <code>""Jones""</code> as the named entity for the 3-class model and <code>""Mr. Jones""</code> for the 4-class and 7-class models. I <em>want</em> it to return <code>""Mr. Jones""</code>, something the NE_chunker in Python's NLTK has no problem doing.</p>

<p>But, when I try this on my machine (either using the Java GUI or through Python), I only ever get <code>""Jones""</code>, without the <code>""Mr.""</code>. Interestingly, if I remove the period after ""Mr"" in this sentence:</p>

<pre><code>""I said hello to Mr Jones, and then I went on my way.""
</code></pre>

<p>Then I do get <code>""Mr Jones""</code> as my named entity. And even more bizarrely, if I remove all punctuation:</p>

<pre><code>""I said hello to Mr Jones and then I went on my way""
</code></pre>

<p>I get only <code>""Jones""</code> again. I have no idea why there is this inconsistency. Especially because the online demo version correctly returns <code>""Mr Jones""</code> in all forms in all three of those sentences for the 7-class model (the one I prefer to use for my project).</p>

<p>Any ideas why this is happening?</p>

<p>Tools/version: Windows 7; Java JDK 1.8.0_121; Stanford CoreNLP 3.7.0 (2016-10-31); Stanford NER 3.7.0 (2016-10-31); Python 3.5; NLTK 3.2.1</p>

<p>Python code to reproduce results:</p>

<pre><code>import nltk
import os
stanford_dir = '/my_path_to/stanford_files/'
jarfile = stanford_dir + 'stanford_ner.jar'
model_7class = stanford_dir + 'classifiers/English.muc.7class.distsim.crf.ser.gz'
postagger_jar = stanford_dir + 'stanford_postagger.jar'
java_path = '/my_path_to/Java/jdk1.8.0_121/bin/java.exe'
os.environ['JAVAHOME'] = java_path

st = nltk.tag.StanfordNERTagger(model_filename=model_7class,path_to_jar=jarfile, encoding='utf-8')
st_tokenize = nltk.tokenize.StanfordTokenizer(path_to_jar=postagger_jar).tokenize

my_sent = 'I said hello to Mr. Jones, and then I went on my way.'
tokens = st_tokenize(my_sent)
tags = st.tag(tokens)
named_entities = [word for word,tag in tags if tag != 'O']
print(named_entities)
</code></pre>
",Named Entity Recognition (NER),title mr mr etc inconsistency stanford ner tagger working stanford named entity recognition ner tagger java python stumbled inconsistency solve sentence using example online demo ner tagger return named entity class model class class model want return something ne chunker python nltk ha problem try machine either using java gui python ever get without interestingly remove period mr sentence get named entity even bizarrely remove punctuation get idea inconsistency especially online demo version correctly return form three sentence class model one prefer use project idea happening tool version window java jdk stanford corenlp stanford ner python nltk python code reproduce result
Regex - Lookbehind with not fixed-length,"<p>I am trying to match a name in a sentence using Java RegEx. A name should be matched only if it is surrounded by normal text and not other names. For example, I would like to match the word <strong>Obama</strong> in the following sentence:</p>

<pre><code>Americans said that Obama is ...
</code></pre>

<p>But not in the following one:</p>

<pre><code>Americans said that Barack Obama is ...
</code></pre>

<p>To check that a token is a name I need to use something simple (an easy regex), without relying on more complex tools (i.e. NER). The regex uses a not-fixed width quantifier (*):</p>

<pre><code>[A-Z][a-z]*
</code></pre>

<p>I can easily find a way to avoid matching names followed by other names with a negative look-ahead, by I can not use the same regex in a negative look-behind, because of the presence of the not-fixed width quantifier.</p>

<p>In other words, I can not use the following regex:</p>

<pre><code>(?&lt;![A-Z][a-z]*\s)Obama(?!\s[A-Z][a-z]*)
</code></pre>

<p>Do you have any other simple but effective ideas to solve this problem?</p>
",Named Entity Recognition (NER),regex lookbehind fixed length trying match name sentence using java regex name matched surrounded normal text name example would like match word obama following sentence following one check token name need use something simple easy regex without relying complex tool e ner regex us fixed width quantifier easily find way avoid matching name followed name negative look ahead use regex negative look behind presence fixed width quantifier word use following regex simple effective idea solve problem
Best method to confirm an entity,"<p>I would like to understand the best approach to the following problem.</p>

<p>I have documents really similar to resume/cv and I have to extract entities (Name, Surname, Birthday, Cities, zipcode etc).</p>

<p>To extract those entities I am combining different finders (Regex, Dictionary etc)</p>

<p>There are no problems with those finders, but, I am looking for a method / algorithm or something like that to confirm the entities.</p>

<p>With ""confirm"" I mean that I have to find specific term (or entities) in proximities (closer to the entities I have found).</p>

<p>Example:</p>

<pre><code>My name is &lt;name&gt;
Name: &lt;name&gt;
Name and Surname: &lt;name&gt;
</code></pre>

<p>I can confirm the entity <code>&lt;name&gt;</code> because it is closer to specific term that let me understand the ""context"". If i have ""name"" or ""surname"" words near the entity  so i can say that i have found the <code>&lt;name&gt;</code> with a good probability.</p>

<p>So the goal is write those kind of rules to confirm entities. Another example should be:</p>

<blockquote>
  <p>My address is ......, 00143 Rome</p>
</blockquote>

<p>Italian zipcodes are 5 digits long (numeric only), it is easy to find a 5 digits number inside my document (i use regex as i wrote above), and i also check it by querying a database to understand if the number exists. The problem here is that i need one more check to confirm (definitely) it.</p>

<p>I must see if that number is near the entity <code>&lt;city&gt;</code>, if yes, ok... I have good probabilities.</p>

<p>I also tried to train a model but i do not really have a ""context"" (sentences).
Training the model with:</p>

<pre><code>My name is: &lt;name&gt;John&lt;/name&gt;
Name: &lt;name&gt;John&lt;/name&gt;
Name/Surname: &lt;name&gt;John&lt;/name&gt;
&lt;name&gt;John&lt;/name&gt; is my name
</code></pre>

<p>does not sound good to me because:</p>

<ol>
<li>I have read we need many sentences to train a good model</li>
<li>Those are not ""sentences"" i do not have a ""context"" (remember where I said the document is similar to resume/cv)</li>
<li>Maybe those phrases are too short</li>
</ol>

<p>I do not know how many different ways i could find to say the exact thing, but surely I can not find 15000 ways :)</p>

<p>What method should I use to try to confirm my entities?</p>

<p>Thank you so much!</p>
",Named Entity Recognition (NER),best method confirm entity would like understand best approach following problem document really similar resume cv extract entity name surname birthday city zipcode etc extract entity combining different finder regex dictionary etc problem finder looking method algorithm something like confirm entity confirm mean find specific term entity proximity closer entity found example confirm entity closer specific term let understand context name surname word near entity say found good probability goal write kind rule confirm entity another example address rome italian zipcodes digit long numeric easy find digit number inside document use regex wrote also check querying database understand number exists problem need one check confirm definitely must see number near entity yes ok good probability also tried train model really context sentence training model doe sound good read need many sentence train good model sentence context remember said document similar resume cv maybe phrase short know many different way could find say exact thing surely find way method use try confirm entity thank much
NLP for reliable text classification on raspberry pi,"<p>Trying to get up and running my very own smart room.</p>

<p>As of now the system is on raspi 3, Google STT, naive bayes for text classification, PoS/NER by nlp-compromise, bunch of APIs, and then eSpeak. <em>(sure there are lot of other stages, but generally speaking)</em></p>

<p>One thing which is <strong>problematic</strong> though is the <strong>text classification</strong>. Though, NB is doing a fair job but yeah there are issues.</p>

<p>Various text classification heavily rely on the fact that there would be large corpora to train with. And this makes sense, particularly if the application is news categorisation, for example.</p>

<p>But here we are talking about spoken language. If the sentence is <code>Tell me the weather</code>, there's only so much corpus you can generate for the variation in that simple statement. And still, find some other way to ask for the weather.</p>

<p>I don't think for each category there can be a large datasets of statements which would help to make the device clearly distinguish between commands.</p>

<p>Question</p>

<ol>
<li><p>What can I do here, since more categories (or skillsets) would mean more similar statements.   </p></li>
<li><p>Since it is a classification problem, even using SVM or RNN or any other trick should not make any such difference, even if I have to rig an external GPU for it. The corpus is about spoken sentences for various categories and the dataset can't be expected to be diversely educative for the system. </p></li>
</ol>

<p>But honestly I am not clear of what could be a reliable method for classification, only for such specific purposes.</p>

<p><em>PS - I have seen how Jasper works, but even that does not resolve to better ""understanding"" of categories, many times</em></p>
",Named Entity Recognition (NER),nlp reliable text classification raspberry pi trying get running smart room system raspi google stt naive bayes text classification po ner nlp compromise bunch apis espeak sure lot stage generally speaking one thing problematic though text classification though nb fair job yeah issue various text classification heavily rely fact would large corpus train make sense particularly application news categorisation example talking spoken language sentence much corpus generate variation simple statement still find way ask weather think category large datasets statement would help make device clearly distinguish command question since category skillsets would mean similar statement since classification problem even using svm rnn trick make difference even rig external gpu corpus spoken sentence various category dataset expected diversely educative system honestly clear could reliable method classification specific purpose p seen jasper work even doe resolve better understanding category many time
NER model training with IOB encoding fails (Stanford CoreNLP),"<p>I am trying to train a NER model for Stanford CoreNLP. But as soon as the 8th or 9th iteration of the training process is reached, it stops and nothing else is happening.</p>

<p>The corpus is annotated with IOB/BIO encoding like this:</p>

<pre><code>How O
to  O
play    O
a   O
video   O
in  O
Java    B-Fram
Swing   I-Fram
?   O
</code></pre>

<p>My properties file:</p>

<pre><code>trainFile = C:\\Data\\corpora\\train\\train.tsv
serializeTo = C:\\Data\\ner-model.ser.gz

map = word=0,answer=1

useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
useDisjunctive=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=2
maxRight=2
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
useGazettes=true
sloppyGazette=true
gazette=C:\\Data\\gazetteers\\gaz1.txt,C:\\Data\\gazetteers\\gaz2.txt
entitySubclassification=bio
</code></pre>

<p>The content of my Gazetteers:</p>

<pre><code>Fram LiteDB
Fram RavenDB
Fram MongoDB
Fram Cassandra
Fram Couchbase
...
</code></pre>

<p>The command for the training process:</p>

<p><code>java -mx8g -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop C:\\Data\\ner.prop -readerAndWriter edu.stanford.nlp.sequences.CoNLLDocumentReaderAndWriter</code></p>

<p>Why is the training process suddenly stopping? Has this something to do with incorrect properties? Or does the gazetteers have to have the same labels as the annotated corpus?</p>

<p>At the end I want the entities to be tagged with just ""Fram"" instead of ""B-Fram"" or ""I-Fram"". How is that possible?</p>

<p>Thank you in advance.</p>
",Named Entity Recognition (NER),ner model training iob encoding fails stanford corenlp trying train ner model stanford corenlp soon th th iteration training process reached stop nothing else happening corpus annotated iob bio encoding like property file content gazetteer command training process training process suddenly stopping ha something incorrect property doe gazetteer label annotated corpus end want entity tagged fram instead b fram fram possible thank advance
Extract entities from Multiple Subject passive sentence by Spacy,"<p>Using Python Spacy, I am trying to extract entities from multiple subject passive voice sentence.</p>
<p>Sentence = &quot;John and Jenny were accused of crimes by David&quot;</p>
<p>My intention is to extract both &quot;John and Jenny”  from the sentence as <em>nsubjpass</em> and <em>.ent_</em>.</p>
<p>However, I am only able to extract  “John” as  nsubjpass.</p>
<p>How to extract both them?</p>
<p>Notice that while John is found as an entity in .ents, Jenny is considered as conj instead of nsubjpass.
How to improve it?</p>
<h2>code</h2>
<pre><code>each_sentence3 =  &quot;John and Jenny were accused of crimes by David&quot;
doc=nlp(each_sentence3)

passive_toks=[tok for tok in doc if (tok.dep_ == &quot;nsubjpass&quot;) ]
if passive_toks != []:
    print(passive_toks)
</code></pre>
<h2>Result:</h2>
<pre><code>[John]
</code></pre>
<p>The entity List shows:</p>
<h2>code</h2>
<p>`</p>
<pre><code>print(list(doc.ents)
</code></pre>
<h1>Result</h1>
<pre><code>[John, Jenny, David]
</code></pre>
<p>Now if we examine the whole sentence, we see as follows:</p>
<h2>Code:</h2>
<pre><code>for tok in doc:   
        print(tok, tok.dep_)
</code></pre>
<h2>Result</h2>
<pre><code>John nsubjpass
and cc
Jenny conj
were auxpass
accused ROOT
of prep
crimes pobj
by agent
David pobj
</code></pre>
<p>Notice that the second passive subject Jenny is identified as conj in Spacy instead of nsubjpass.</p>
",Named Entity Recognition (NER),extract entity multiple subject passive sentence spacy using python spacy trying extract entity multiple subject passive voice sentence sentence john jenny accused crime david intention extract john jenny sentence nsubjpass ent however able extract john nsubjpass extract notice john found entity ents jenny considered conj instead nsubjpass improve code result entity list show code result examine whole sentence see follows code result notice second passive subject jenny identified conj spacy instead nsubjpass
Finding the Head Word in Python,"<p>I need to extract the head words of sentences (more specifically, the head words of the highest noun phrase in a sentence). I am using the Stanford CoreNLP server through py-corenlp to annotate my sentences. The suite has a modification of Michael Collin's head word finding algorithm, but I have not found any method to use it through the server. I would like to avoid reinventing the wheel, so is there any way I can achieve this with existing tools in Python?</p>

<p>Example: </p>

<blockquote>
  <p><em>The <strong>number</strong> of elementary entities in 1 mole of a substance</em> is known as what? </p>
</blockquote>

<pre><code>(ROOT
  (S
    (NP
      (NP (DT The) (NN number))
      (PP (IN of)
        (NP
          (NP (JJ elementary) (NNS entities))
          (PP (IN in)
            (NP
              (NP (CD 1) (NN mole))
              (PP (IN of)
                (NP (DT a) (NN substance))))))))
    (VP (VBZ is)
      (VP (VBN known)
        (PP (IN as)
          (NP (WP what)))))
    (. ?)))
</code></pre>

<p>""The number of elementary entities in 1 mole of a substance"" is the highest noun phrase.</p>

<p>""number"" is the head word of the phrase, which I want to extract.</p>

<hr>

<p>EDIT: Added example.</p>
",Named Entity Recognition (NER),finding head word python need extract head word sentence specifically head word highest noun phrase sentence using stanford corenlp server py corenlp annotate sentence suite ha modification michael collin head word finding algorithm found method use server would like avoid reinventing wheel way achieve existing tool python example number elementary entity mole substance known number elementary entity mole substance highest noun phrase number head word phrase want extract edit added example
Comparing two LinkedHashMaps with values as a list,"<p>I've asked this question in different ways a couple of times already. Each time I get a breakthrough I encounter another issue. This is also due to the fact that I am not proficient in Java yet and have difficulty with collections like Maps. So please bear with me.</p>

<p>I have two maps like this:</p>

<pre><code>Map1 -{ORGANIZATION=[Fulton Tax Commissioner 's Office, Grady Hospital, Fulton Health Department], LOCATION=[Bellwood, Alpharetta]}

Map2 - {ORGANIZATION=[Atlanta Police Department, Fulton Tax Commissioner, Fulton Health Department], LOCATION=[Alpharetta], PERSON=[Bellwood, Grady Hospital]}
</code></pre>

<p>The maps are defined as : <code>LinkedHashMap&lt;String, List&lt;String&gt;&gt; sampleMap = new LinkedHashMap&lt;String, List&lt;String&gt;&gt;();</code></p>

<p>I am comparing these two maps based on the values and there are only 3 keys i.e ORGANIZATION, PERSON and LOCATION. Map1 is my goldset that I am comparing Map2 against. Now the problem that I am facing is when I iterate over the values of ORGANIZATION key in Map1 and check for matches in Map2, even though my first entry does have a partial match in Map2 (Fulton Tax Commissioner) but because the first entry of Map2 (Atlanta Police Department) is not a match I get an incorrect result(I am looking for exact and partial matches both). The result here being increment the true positive, false positive and false negative counters which enable me to ultimately calculate precision and recall for this i.e. Named Entity Recognition.   </p>

<p><strong>EDIT</strong></p>

<p>The result I am expecting for this is </p>

<pre><code>Organization: 
True Positive Count = 2
False Negative Count = 1
False Positive Count = 1

Person:
False Positive Count = 2

Location:
True Positive Count = 1
False Negative Count = 1
</code></pre>

<p>The output I am currently getting is :</p>

<pre><code>Organization: 
    True Positive Count = 1
    False Negative Count = 2
    False Positive Count = 0

    Person:
    True Positive Count = 0
    False Negative Count = 0
    False Positive Count = 2

    Location:
    True Positive Count = 0
    False Negative Count = 1
    False Positive Count = 0
</code></pre>

<p><strong>CODE</strong></p>

<pre><code>private static List&lt;Integer&gt; compareMaps(LinkedHashMap&lt;String, List&lt;String&gt;&gt; annotationMap, LinkedHashMap&lt;String, List&lt;String&gt;&gt; rageMap) 
    {
        List&lt;Integer&gt; compareResults = new ArrayList&lt;Integer&gt;();  

         if (!annotationMap.entrySet().containsAll(rageMap.entrySet())){
               for (Entry&lt;String, List&lt;String&gt;&gt; rageEntry : rageMap.entrySet()){
                   if (rageEntry.getKey().equals(""ORGANIZATION"") &amp;&amp; !(annotationMap.containsKey(rageEntry.getKey()))){
                       for (int j = 0; j&lt; rageEntry.getValue().size(); j++) {
                           orgFalsePositiveCount++;
                       }
               }
                   if (rageEntry.getKey().equals(""PERSON"") &amp;&amp; !(annotationMap.containsKey(rageEntry.getKey()))){
                      // System.out.println(rageEntry.getKey());
                      // System.out.println(annotationMap.entrySet());
                       for (int j = 0; j&lt; rageEntry.getValue().size(); j++) {
                           perFalsePositiveCount++;
                       }
               }
                   if (rageEntry.getKey().equals(""LOCATION"") &amp;&amp; !(annotationMap.containsKey(rageEntry.getKey()))){
                       for (int j = 0; j&lt; rageEntry.getValue().size(); j++) {
                           locFalsePositiveCount++;
                     }
                 }
              }
           }



               for (Entry&lt;String, List&lt;String&gt;&gt; entry : annotationMap.entrySet()){

                   int i_index = 0;
                   if (rageMap.entrySet().isEmpty()){
                       orgFalseNegativeCount++;
                       continue;
                   }

                  // for (Entry&lt;String, List&lt;String&gt;&gt; rageEntry : rageMap.entrySet()){

                   if (entry.getKey().equals(""ORGANIZATION"")){
                       for(String val : entry.getValue()) {
                           if (rageMap.get(entry.getKey()) == null){
                               orgFalseNegativeCount++;
                               continue;
                       }
            recusion:      for (int i = i_index; i&lt; rageMap.get(entry.getKey()).size();){
                                String rageVal = rageMap.get(entry.getKey()).get(i);
                               if(val.equals(rageVal)){
                                   orgTruePositiveCount++;
                                   i_index++;
                                   break recusion;
                       }

                           else if((val.length() &gt; rageVal.length()) &amp;&amp; val.contains(rageVal)){  //|| dataB.get(entryA.getKey()).contains(entryA.getValue())){
                               orgTruePositiveCount++;
                               i_index++;
                               break recusion;
                       }
                           else if((val.length() &lt; rageVal.length()) &amp;&amp; rageVal.contains(val)){
                               orgTruePositiveCount++;
                                i_index++;
                                break recusion;
                           }

                           else if(!val.contains(rageVal)){
                               orgFalseNegativeCount++;
                               i_index++;
                               break recusion;
                           }
                           else if(!rageVal.contains(val)){
                                 orgFalsePositiveCount++;
                                 i_index++;
                                 break recusion;
                             }


                      }
                    }
                   }

                  ......................... //(Same for person and location)


                    compareResults.add(orgTruePositiveCount); 
                    compareResults.add(orgFalseNegativeCount); 
                    compareResults.add(orgFalsePositiveCount);  
                    compareResults.add(perTruePositiveCount); 
                    compareResults.add(perFalseNegativeCount);  
                    compareResults.add(perFalsePositiveCount); 
                    compareResults.add(locTruePositiveCount); 
                    compareResults.add(locFalseNegativeCount);  
                    compareResults.add(locFalsePositiveCount); 

                    System.out.println(compareResults);
                    return compareResults;

            }  
</code></pre>
",Named Entity Recognition (NER),comparing two linkedhashmaps value list asked question different way couple time already time get breakthrough encounter another issue also due fact proficient java yet difficulty collection like map please bear two map like map defined comparing two map based value key e organization person location map goldset comparing map problem facing iterate value organization key map check match map even though first entry doe partial match map fulton tax commissioner first entry map atlanta police department match get incorrect result looking exact partial match result increment true positive false positive false negative counter enable ultimately calculate precision recall e named entity recognition edit result expecting output currently getting code
Match with Phrase for Google Speech API,"<p>I capture an audio from a speaker where they say - <code>""I want to meet John Disilva""</code>. I pass this to Google Speech API with Phrase as <code>{ 'John Disilva', 'Ashish Mundra'}</code>. However, Google Speech API returns me full phrase i.e. - <code>'I want to meet John Disilva'</code>. </p>

<p>Is there a way I can only get my phrase as return value as I am only interested to extract the name part? </p>

<p>The reason is that I cannot control what someone is saying to my mic. They can say <code>'I would like to see John Disilva'</code> or <code>'Do you know John Disilva'</code>, but I am sure that my user will always have that name somewhere in this sentence which I want to extract. </p>

<p>If Google Speech API can give me the exact phrase via which it was able to detect <code>John Disilva</code> in that sentence then I can use that Phrase for further processing in my code.</p>
",Named Entity Recognition (NER),match phrase google speech api capture audio speaker say pas google speech api phrase however google speech api return full phrase e way get phrase return value interested extract name part reason control someone saying mic say sure user always name somewhere sentence want extract google speech api give exact phrase via wa able detect sentence use phrase processing code
"Why is &quot;MacBook&quot; an Entity, but not &quot;laptop&quot;?","<p>I'm building a Twitterbot to reply to tweets about broken products.</p>

<p>I was trying to use the IBM Watson AlchemyLanguage Entities API to extract products and product types from plain text. Sadly, it doesn't seem to extract any product types, e.g. ""laptop"", only specific product names, e.g. ""MacBook"". How can I get it to return ""laptop""?</p>

<p>Also, I looked at the XLS sheet of Types and SubTypes linked from the API documentation. The Types are ideal for my purposes, but I can only see the SubTypes in the output. How can I get it to return Types?</p>
",Named Entity Recognition (NER),macbook entity laptop building twitterbot reply tweet broken product wa trying use ibm watson alchemylanguage entity api extract product product type plain text sadly seem extract product type e g laptop specific product name e g macbook get return laptop also looked xl sheet type subtypes linked api documentation type ideal purpose see subtypes output get return type
Comparing two maps to calculate precision and recall for NER,"<p>I am trying to calculate precision and recall for our Named Entity Recognizer by comparing our output to a gold set output. annotationMap is the gold set map and myMap is the output of my NER.To give you a sense, the maps contain data like:</p>

<pre><code>{ORGANIZATION=[Pearl Williams Hartsfield, Fulton Superior Court],
DATE=[Friday], PERSON=[William B. Hartsfield]}
</code></pre>

<p>According to an answer I had read here at stack overflow, the way we calculate precision and recall is (copy pasting from there): </p>

<pre><code>[Microsoft Corp.] CEO [Steve Ballmer] announced the release of [Windows 7] today
</code></pre>

<p>This has 3 entities.</p>

<p>Supposing your actual extraction has the following</p>

<pre><code>[Microsoft Corp.] [CEO] [Steve] Ballmer announced the release of Windows 7 [today]
</code></pre>

<p><strong>Exact match:</strong> True Positives = 1 (Microsoft Corp., the only exact match), False Positives =3 (CEO, today, and Steve, which isn't an exact match), False Negatives = 2 (Steve Ballmer and Windows 7)</p>

<pre><code>Precision = True Positives / (True Positives + False Positives) = 1/(1+3) = 0.25
Recall = True Positives / (True Positives + False Negatives) = 1/(1+2) = 0.33
</code></pre>

<p><strong>Any Overlap OK:</strong> True Positives = 2 (Microsoft Corp., and Steve which overlaps Steve Ballmer), False Positives =2 (CEO, and today), False Negatives = 1 (Windows 7)</p>

<pre><code>Precision = True Positives / (True Positives + False Positives) = 2/(2+2) = 0.55
Recall = True Positives / (True Positives + False Negatives) = 2/(2+1) = 0.66
</code></pre>

<p>I have modeled my code on the same logic but the value of false positives for me is always zero as a result my precision and recall values are 1 and 0 respectively.</p>

<p>I think the way I am calculating false positives is incorrect. But I am following the same logic and checking to see if I have any entities in myMap that are not contained in the annotatioMap and calling them false positive (like CEO and today in the example above). As a result I am confused as to where the problem exactly is!</p>

<pre><code>private static List&lt;Integer&gt; compareMaps(LinkedHashMap&lt;String, Vector&lt;String&gt;&gt; annotationMap, LinkedHashMap&lt;String, Vector&lt;String&gt;&gt; myMap) 
        {
            List&lt;Integer&gt; compareResults = new ArrayList&lt;Integer&gt;();  
                if (annotationMap != null &amp;&amp; myMap != null){
                   for (String key: annotationMap.keySet()){
                       if (key.equals(""ORGANIZATION"")){
                           if (annotationMap.get(key).equals(myMap.get(key))){
                               orgTruePositiveCount++;
                               continue;
                           }
                           if (annotationMap.get(key).contains(myMap.get(key))){
                               orgFalseNegativeCount++;
                               continue;
                           }
                           if (!annotationMap.get(key).contains(myMap.get(key))){
                               orgFalseNegativeCount++;
                               continue;
                           }
                           if (!myMap.get(key).contains(annotationMap.get(key))){
                               orgFalsePositiveCount++;
                               continue;
                           }
                       }
</code></pre>
",Named Entity Recognition (NER),comparing two map calculate precision recall ner trying calculate precision recall named entity recognizer comparing output gold set output annotationmap gold set map mymap output ner give sense map contain data like according answer read stack overflow way calculate precision recall copy pasting ha entity supposing actual extraction ha following exact match true positive microsoft corp exact match false positive ceo today steve exact match false negative steve ballmer window overlap ok true positive microsoft corp steve overlap steve ballmer false positive ceo today false negative window modeled code logic value false positive always zero result precision recall value respectively think way calculating false positive incorrect following logic checking see entity mymap contained annotatiomap calling false positive like ceo today example result confused problem exactly
Natural Language Processing Algorithms,"<p>I am planning on developing a Natural Language Question System using NLP.
I have performed literature study regarding the possible algorithms which are applicable for a NLQ System.</p>

<p>The end-user should, after finishing the tool, be able to ask a question to the system, which on its turn gives an answer in the form of a table of will visualize the answer in the form of a graph.</p>

<p>Furthermore, the answering part is already finished. Programming will happen in Python, using the PyNLPl library.</p>

<p>The main tool can already perform mathematical operations and summarizes the outcome of these operations. The user should be able to ask questions as:</p>

<ul>
<li>""How were the sales on a rainy day in the month january?""</li>
<li>""What is the amount of ... of the whole of Europe""</li>
</ul>

<p>This question is not meant to be subjective, as I mentioned before, I did literature study.
I made a proper selection of the list of algorithms which I found. And am left with a decision of:</p>

<ul>
<li>POST, Chunking, Named Entity Extraction</li>
<li>Parsing</li>
<li>Topic Modeling and keyword extraction.</li>
</ul>

<p>Algorithms per bullet point would be:</p>

<ul>
<li>Conditional Random Fields - Hidden Markov Model</li>
<li>CKY Algorithm  - Earley Algorithm</li>
<li>Latend Dirichlet Allocation</li>
</ul>

<p>Furthermore, the variables which should be mentioned in the questions, are not independent. Is Naive Bayes in that case also applicable?
The chosen algorithm, should outperform the rest of the algorithms and make the best fit.</p>
",Named Entity Recognition (NER),natural language processing algorithm planning developing natural language question system using nlp performed literature study regarding possible algorithm applicable nlq system end user finishing tool able ask question system turn give answer form table visualize answer form graph furthermore answering part already finished programming happen python using pynlpl library main tool already perform mathematical operation summarizes outcome operation user able ask question sale rainy day month january amount whole europe question meant subjective mentioned literature study made proper selection list algorithm found left decision post chunking named entity extraction parsing topic modeling keyword extraction algorithm per bullet point would conditional random field hidden markov model cky algorithm earley algorithm latend dirichlet allocation furthermore variable mentioned question independent naive bayes case also applicable chosen algorithm outperform rest algorithm make best fit
"How to extract named entities like PER, ORG, GPE from the tree structure when binary = False?","<p>I'm new to nltk and trying to extract PERSON, ORGANIZATION, GPE from the the following code:</p>

<pre><code>for i in tokcomp:
words = nltk.word_tokenize(i)
tagged = nltk.pos_tag(words)
namedEnt = nltk.ne_chunk(tagged, binary=False)
print(namedEnt)
</code></pre>

<p>The output i got is :</p>

<pre><code>(S
  Our/PRP$
  direct/JJ
  competitors/NNS
  include/VBP
  ,/,
  among/IN
  others/NNS
  ,/,
  (PERSON Accenture/NNP)
  ,/,
  (GPE Capgemini/NNP)
  ,/,
  (ORGANIZATION Computer/NNP Sciences/NNPS Corporation/NNP)
  ,/,
  (GPE Genpact/NNP)
  ,/,
  (ORGANIZATION HCL/NNP Technologies/NNPS)
  ,/,
  (ORGANIZATION HP/NNP Enterprise/NNP)
  ,/,
  (ORGANIZATION IBM/NNP Global/NNP Services/NNPS)
  ,/,
  (ORGANIZATION Infosys/NNP Technologies/NNPS)
  ,/,
  (PERSON Tata/NNP Consultancy/NNP Services/NNPS)
  and/CC
  (PERSON Wipro/NNP)
  ./.)
(S
  These/DT
  markets/NNS
  also/RB
  include/VBP
  numerous/JJ
  smaller/JJR
  local/JJ
  competitors/NNS
  in/IN
  the/DT
  various/JJ
  geographic/JJ
  markets/NNS
  in/IN
  which/WDT
  we/PRP
  operate/VBP
  which/WDT
  may/MD
  be/VB
  able/JJ
  to/TO
  provide/VB
  services/NNS
  and/CC
  solutions/NNS
  at/IN
  lower/JJR
  costs/NNS
  or/CC
  on/IN
  terms/NNS
  more/RBR
  attractive/JJ
  to/TO
  clients/NNS
  than/IN
  we/PRP
  can/MD
  ./.)
(S
  Our/PRP$
  direct/JJ
  competitors/NNS
  include/VBP
  ,/,
  among/IN
  others/NNS
  ,/,
  (PERSON Accenture/NNP)
  ,/,
  (GPE Capgemini/NNP)
  ,/,
  (ORGANIZATION Computer/NNP Sciences/NNPS Corporation/NNP)
  ,/,
  (GPE Genpact/NNP)
  ,/,
  (ORGANIZATION HCL/NNP Technologies/NNPS)
  ,/,
  (ORGANIZATION HP/NNP Enterprise/NNP)
  ,/,
  (ORGANIZATION IBM/NNP Global/NNP Services/NNPS)
  ,/,
  (ORGANIZATION Infosys/NNP Technologies/NNPS)
  ,/,
  (PERSON Tata/NNP Consultancy/NNP Services/NNPS)
  and/CC
  (PERSON Wipro/NNP)
  ./.)
(S
  The/DT
  rates/NNS
  we/PRP
  are/VBP
  able/JJ
  to/TO
  recover/VB
  for/IN
  our/PRP$
  services/NNS
  are/VBP
  affected/VBN
  by/IN
  a/DT
  number/NN
  of/IN
  factors/NNS
  ,/,
  including/VBG
  :/:
  •/VB
  our/PRP$
  clients’/JJ
  perceptions/NNS
  of/IN
  our/PRP$
  ability/NN
  to/TO
  add/VB
  value/NN
  through/IN
  our/PRP$
  services/NNS
  ;/:
  •/NNP
  introduction/NN
  of/IN
  new/JJ
  services/NNS
  or/CC
  products/NNS
  by/IN
  us/PRP
  or/CC
  our/PRP$
  competitors/NNS
  ;/:
  •/VB
  our/PRP$
  competitors’/NN
  pricing/NN
  policies/NNS
  ;/:
  •/VB
  our/PRP$
  ability/NN
  to/TO
  accurately/RB
  estimate/VB
  ,/,
  attain/NN
  and/CC
  sustain/NN
  contract/NN
  revenues/NNS
  ,/,
  margins/NNS
  and/CC
  cash/NN
  flows/NNS
  over/IN
  increasingly/RB
  longer/JJR
  contract/NN
  periods/NNS
  ;/:
  •/NNP
  bid/NN
  practices/NNS
  of/IN
  clients/NNS
  and/CC
  their/PRP$
  use/NN
  of/IN
  third-party/JJ
  advisors/NNS
  ;/:
  •/VB
  the/DT
  use/NN
  by/IN
  our/PRP$
  competitors/NNS
  and/CC
  our/PRP$
  clients/NNS
  of/IN
  offshore/JJ
  resources/NNS
  to/TO
  provide/VB
  lower-cost/JJ
  service/NN
  delivery/NN
  capabilities/NNS
  ;/:
  •/VB
  our/PRP$
  ability/NN
  to/TO
  charge/VB
  premium/NN
  prices/NNS
  when/WRB
  justified/VBN
  by/IN
  market/NN
  demand/NN
  or/CC
  the/DT
  type/NN
  of/IN
  service/NN
  ;/:
  and/CC
  •/VB
  general/JJ
  economic/JJ
  and/CC
  political/JJ
  conditions/NNS
  ./.)
(S
  For/IN
  our/PRP$
  internal/JJ
  management/NN
  reporting/NN
  and/CC
  budgeting/NN
  purposes/NNS
  ,/,
  we/PRP
  use/VBP
  non-GAAP/JJ
  financial/JJ
  information/NN
  that/WDT
  does/VBZ
  not/RB
  include/VB
  stock-based/JJ
  compensation/NN
  expense/NN
  ,/,
  acquisition-related/JJ
  charges/NNS
  and/CC
  net/JJ
  non-operating/JJ
  foreign/JJ
  currency/NN
  exchange/NN
  gains/NNS
  or/CC
  losses/NNS
  for/IN
  financial/JJ
  and/CC
  operational/JJ
  decision/NN
  making/NN
  ,/,
  to/TO
  evaluate/VB
  period-to-period/JJ
  comparisons/NNS
  and/CC
  for/IN
  making/VBG
  comparisons/NNS
  of/IN
  our/PRP$
  operating/NN
  results/NNS
  to/TO
  those/DT
  of/IN
  our/PRP$
  competitors/NNS
  ./.)
</code></pre>

<p>I went through many links but didn't find a way which fits my purpose to extract the companies which are tagged as Person, Organization and GPE.</p>

<p>Will be very thankful if any links to learn more about extracting named entities other than nltk website are provided. </p>
",Named Entity Recognition (NER),extract named entity like per org gpe tree structure binary false new nltk trying extract person organization gpe following code output got went many link find way fit purpose extract company tagged person organization gpe thankful link learn extracting named entity nltk website provided
NLP: Position feature of a word in an OCR of a document,"<p>I'm trying to apply NLP to an OCR document. To extract named entities, how can I use features like position of the word in the document?</p>

<p>For example, I have a health report I need to extract the chemical terms in the report in a particular area and avoid their occurrence elsewhere. Can I define a position feature for this in terms of <code>{top:x , left:y}</code> values?</p>

<p>Are there any sklearn libraries?   </p>
",Named Entity Recognition (NER),nlp position feature word ocr document trying apply nlp ocr document extract named entity use feature like position word document example health report need extract chemical term report particular area avoid occurrence elsewhere define position feature term value sklearn library
how to Get The Code of part in stanford-nlp?,"<p>hey all how can i get a code for a Stanford-nlp for example i need to see code for <strong>named-entity recognition</strong> or <strong>relation extraction</strong> is this Available or not ?
thanks in advance..</p>
",Named Entity Recognition (NER),get code part stanford nlp hey get code stanford nlp example need see code named entity recognition relation extraction available thanks advance
How does the trait entity extraction in wit.ai work?,"<p>Wit.ai provides an entity of type ""trait"". Wit.ai defines trait entities as </p>

<blockquote>
  <p>When the entity value is not inferred from a keyword or specific
  phrase in the sentence. There is no obvious association between
  certain words in the sentence and the value of the entity, but rather
  you need the sentence as a whole to determine the value.
  (<a href=""https://wit.ai/docs/recipes#which-entity-should-i-use"" rel=""nofollow noreferrer"">https://wit.ai/docs/recipes#which-entity-should-i-use</a>)</p>
</blockquote>

<p>I'm curious to know how does this work? For example,</p>

<pre><code>I want to know the status
Check the status
How is it going?
</code></pre>

<p>I want to extract the entity <code>entities[""cmd"":""status""]</code> from all of these statements. How do I go on about starting to implement this?</p>
",Named Entity Recognition (NER),doe trait entity extraction wit ai work wit ai provides entity type trait wit ai defines trait entity entity value inferred keyword specific phrase sentence obvious association certain word sentence value entity rather need sentence whole determine value curious know doe work example want extract entity statement go starting implement
Can I use punctuation in Stanford CoreNLP Named Entities?,"<p>I'm trying to get Stanford Core NLP to recognise an identification code. The problem is the code has punctuation in it. e.g. <code>01.A01.01</code> which causes the input to be separated into three sentences.</p>
<p>The matching expression for this code would be <code>[0-9][0-9][.][a-z,A-Z][0-9][0-9][.][0-9][0-9]</code>. I've tried adding this into my <code>regexner.txt</code> file but it doesn't identify it (presumably because the tokens are across separate sentences?)</p>
<p>I've also tried to match it using a TokenRegex similar to the following (also without any success).</p>
<p><code>/tell/ /me/ /about/ (?$refCode /[0-9][0-9]/ /./ /[a-z,A-Z][0-9][0-9]/ /./ /[0-9][0-9]/ )</code></p>
<p>Some example uses...</p>
<blockquote>
<p>The user has resource 02.G36.63 reserved.</p>
<p>Is 21.J83.02 available?</p>
</blockquote>
<p>Does anyone have any ideas or suggestions?</p>
",Named Entity Recognition (NER),use punctuation stanford corenlp named entity trying get stanford core nlp recognise identification code problem code ha punctuation e g cause input separated three sentence matching expression code would tried adding file identify presumably token across separate sentence also tried match using tokenregex similar following also without success example us user ha resource g reserved j available doe anyone idea suggestion
Stanford Parser: How to extract dependencies,"<p>I would like to extract all of the dependencies in a sentence using Stanford parser. Consider the followoing code</p>

<pre><code>        LexicalizedParser lp = LexicalizedParser.loadModel();
        lp.setOptionFlags(new String[] { ""-maxLength"", ""80"", ""-retainTmpSubcategories"" });
        String[] sent = ""There is a football on the grass pitch which is 50% white with a pressure flow of 1"".split("" "");
        List&lt;CoreLabel&gt; rawWords = Sentence.toCoreLabelList(sent);
        Tree parse = lp.apply(rawWords);
        parse.pennPrint();
        System.out.println();

        TreebankLanguagePack tlp = new PennTreebankLanguagePack();
        GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
        GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
        List&lt;TypedDependency&gt; tdl = gs.typedDependenciesCCprocessed();
        System.out.println(tdl);
        TreePrint tp = new TreePrint(""penn,typedDependenciesCollapsed"");
        tp.printTree(parse);

        Collection&lt;TypedDependency&gt; td = gs.typedDependenciesCollapsed();
        //System.out.println(td);

        Object[] list = td.toArray();
        System.out.println(list.length);
        TypedDependency typedDependency;
        for (Object object : list) {
            typedDependency = (TypedDependency) object;
            System.out.println(
                    ""Depdency Name "" + typedDependency.dep().toString() + "" :: "" + typedDependency.reln());
            if (typedDependency.reln().getShortName().equals(""something"")) {
                // your code
            }
        }
</code></pre>

<p>so this allows me to see the dependencies as</p>

<pre><code>Depdency Name There/EX :: expl
Depdency Name is/VBZ :: root
Depdency Name a/DT :: det
Depdency Name football/NN :: nsubj
Depdency Name on/IN :: case
Depdency Name the/DT :: det
Depdency Name grass/NN :: compound
Depdency Name pitch/NN :: nmod:on
Depdency Name which/WDT :: nsubj
Depdency Name is/VBZ :: cop
Depdency Name 50%/CD :: nummod
Depdency Name white/JJ :: amod
Depdency Name with/IN :: case
Depdency Name a/DT :: det
Depdency Name pressure/NN :: nmod:with
Depdency Name flow/NN :: acl:relcl
Depdency Name of/IN :: case
Depdency Name 1/CD :: nmod:of
</code></pre>

<p>now, when i visualize the same at <a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/corenlp/process</a></p>

<p>i see</p>

<p><a href=""https://i.sstatic.net/NqyZ1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NqyZ1.png"" alt=""enter image description here""></a></p>

<p>How can i extract everything that is dependent on the <code>nsubj</code>? For instance in this case,</p>

<ol>
<li><p>I want to know what the sunbject is. This is easy as i just look for the node name <code>nsubj</code></p></li>
<li><p>Now i also want the following information</p>

<ul>
<li>Where is the football ? <code>on the grass pitch</code> </li>
<li>Any other attributes of the football? <code>which is 50% white</code> and <code>pressure flow of 1</code></li>
</ul></li>
</ol>

<p>How should i navigate the tree in order to extract these dependencies? Is there a way for me to start at the <code>nsubj</code> node and go to all the dependencies?</p>
",Named Entity Recognition (NER),stanford parser extract dependency would like extract dependency sentence using stanford parser consider followoing code allows see dependency visualize see extract everything dependent instance case want know sunbject easy look node name also want following information football attribute football navigate tree order extract dependency way start node go dependency
nlp- Difference between Sentences and a Document in Stanford OpenNLP?,"<p>Let us say we have an article that we want to annotate. If we input the text as one really long Sentence as opposed to a Document, does Stanford do anything differently between annotating that one long Sentence as opposed to looping through every Sentence in the Document and culminating all of its results together? </p>

<p>EDIT: I ran a test and it seems like the two approaches return two different NER sets. I might be just doing it wrong, but it's certainly super interesting and I'm curious as to why this happens.</p>
",Named Entity Recognition (NER),nlp difference sentence document stanford opennlp let u say article want annotate input text one really long sentence opposed document doe stanford anything differently annotating one long sentence opposed looping every sentence document culminating result together edit ran test seems like two approach return two different ner set might wrong certainly super interesting curious happens
How to customize Stanford NER in python?,"<p>I learned how to customize Stanford NER (Named Entity Recognizer) in Java from here:</p>

<p><a href=""http://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/crf-faq.shtml#a</a></p>

<p>But I am developing my project with Python and here I need to train my classier with some custom entities.</p>

<p>I searched a lot for a solution but could not find any. Any idea? If it is not possible, is there any other way to train my classifier with custom entities, i.e, with nltk or others in python?</p>

<p><strong>EDIT: Code addition</strong> 
This is what I did to set up and test Stanford NER which worked nicely:</p>

<pre><code>from nltk.tag.stanford import StanfordNERTagger
path_to_model = ""C:\..\stanford-ner-2016-10-31\classifiers\english.all.3class.distsim.crf.ser""
path_to_jar = ""C:\..\stanford-ner-2016-10-31\stanford-ner.jar""
nertagger=StanfordNERTagger(path_to_model, path_to_jar)
query=""Show  me the best eye doctor in Munich""
print(nertagger.tag(query.split()))
</code></pre>

<p>This code worked successfully. Then, I downloaded the sample austen.prop file and both jane-austen-emma-ch1.tsv and jane-austen-emma-ch2.tsv file and put it in a custom folder in NerTragger library folder. I modified the jane-austen-emma-ch1.tsv file with my custom entity tags. The code of austen.prop file has link to jane-austen-emma-ch1.tsv file. Now, I modified the above code as follow but it is not working:</p>

<pre><code>from nltk.tag.stanford import StanfordNERTagger
path_to_model = ""C:\..\stanford-ner-2016-10-31\custom/austen.prop""
path_to_jar = ""C:\..\stanford-ner-2016-10-31\stanford-ner.jar""
nertagger=StanfordNERTagger(path_to_model, path_to_jar)
query=""Show  me the best eye doctor in Munich""
print(nertagger.tag(query.split()))
</code></pre>

<p>But this code is producing the following error:</p>

<pre><code>Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: java.io.StreamCorruptedException: invalid stream header: 236C6F63
    raise OSError('Java command failed : ' + str(cmd))
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1507)
    at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3017)
Caused by: java.io.StreamCorruptedException: invalid stream header: 236C6F63
OSError: Java command failed : ['C:\\Program Files\\Java\\jdk1.8.0_111\\bin\\java.exe', '-mx1000m', '-cp', 'C:/Users/HP/Desktop/Downloads1/Compressed/stanford-ner-2016-10-31/stanford-ner-2016-10-31\\stanford-ner-3.7.0-javadoc.jar;C:/Users/HP/Desktop/Downloads1/Compressed/stanford-ner-2016-10-31/stanford-ner-2016-10-31\\stanford-ner-3.7.0-sources.jar;C:/Users/HP/Desktop/Downloads1/Compressed/stanford-ner-2016-10-31/stanford-ner-2016-10-31\\stanford-ner-3.7.0.jar;C:/Users/HP/Desktop/Downloads1/Compressed/stanford-ner-2016-10-31/stanford-ner-2016-10-31\\stanford-ner.jar;C:/Users/HP/Desktop/Downloads1/Compressed/stanford-ner-2016-10-31/stanford-ner-2016-10-31\\lib\\joda-time.jar;C:/Users/HP/Desktop/Downloads1/Compressed/stanford-ner-2016-10-31/stanford-ner-2016-10-31\\lib\\jollyday-0.4.9.jar;C:/Users/HP/Desktop/Downloads1/Compressed/stanford-ner-2016-10-31/stanford-ner-2016-10-31\\lib\\stanford-ner-resources.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', 'C:/Users/HP/Desktop/Downloads1/Compressed/stanford-ner-2016-10-31/stanford-ner-2016-10-31/custom/austen.prop', '-textFile', 'C:\\Users\\HP\\AppData\\Local\\Temp\\tmppk8_741f', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '""tokenizeNLs=false""', '-encoding', 'utf8']
    at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:808)
    at java.io.ObjectInputStream.&lt;init&gt;(ObjectInputStream.java:301)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1462)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1494)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1505)
    ... 1 more
</code></pre>
",Named Entity Recognition (NER),customize stanford ner python learned customize stanford ner named entity recognizer java developing project python need train classier custom entity searched lot solution could find idea possible way train classifier custom entity e nltk others python edit code addition set test stanford ner worked nicely code worked successfully downloaded sample austen prop file jane austen emma ch tsv jane austen emma ch tsv file put custom folder nertragger library folder modified jane austen emma ch tsv file custom entity tag code austen prop file ha link jane austen emma ch tsv file modified code follow working code producing following error
Issues in Training Custom Named Entity Recogniser in Apache OpenNLP,"<p>I am trying to build a custom NER model to extract Indian names. However I lack training data (I only have the list of names).</p>

<p>So I thought of creating my own corpus, using each name to be replaced in the following lines</p>

<pre><code>Hi How are you &lt;START:person&gt; Hardik &lt;END&gt; 
&lt;START:person&gt; Hardik &lt;END&gt;  is a great personality
Contributions from &lt;START:person&gt; Hardik &lt;END&gt;  are very important
&lt;START:person&gt; Hardik &lt;END&gt;  believes in honesty
Meek and tidy soul, &lt;START:person&gt; Hardik &lt;END&gt; derives happiness from all opportunities.
</code></pre>

<p>I do this for each of 3000 names that I have and hence end up with a corpus of around 15K.</p>

<p>However now when I try to run my code, using this as my test data</p>

<pre><code>This employment is for Hardik
</code></pre>

<p>I end up getting an output like this</p>

<pre><code>[0..1) person  This
[1..2) person  employment
[2..3) person  is
[3..4) person  for
[4..5) person  Hardik
</code></pre>

<p>Each word in the document is thrown as <code>person</code>. Is this an example of <code>overfitting/noise</code>. I am really confused here as to why is it behaving like this even when the words are not annotated like that</p>
",Named Entity Recognition (NER),issue training custom named entity recogniser apache opennlp trying build custom ner model extract indian name however lack training data list name thought creating corpus using name replaced following line name hence end corpus around k however try run code using test data end getting output like word document thrown example really confused behaving like even word annotated like
how to identify a end of a sentence,"<pre><code>String x="" i am going to the party at 6.00 in the evening. are you coming with me?"";
</code></pre>

<p>if i have the above string, i need that to be broken to sentences by using sentence boundry punctuations(like . and ?)</p>

<p>but it should not split the sentence at 6 because of having an pointer there. is there a way to identify what is the correct sentence boundry place in java? i have tried using stringTokenizer in java.util pakage but it always break the sentence whenever it finds a pointer. Can someone suggest me a method to do this correctly?</p>

<p>This is the method which i have tried in tokenizing a text into sentences. </p>

<pre><code>public static ArrayList&lt;String&gt; sentence_segmenter(String text) {
    ArrayList&lt;String&gt; Sentences = new ArrayList&lt;String&gt;();

    StringTokenizer st = new StringTokenizer(text, "".?!"");
    while (st.hasMoreTokens()) {

        Sentences.add(st.nextToken());
    }
    return Sentences;
}
</code></pre>

<p>also i have a method to segement sentences into phrases, but here also when the program found comma(,) it splits the text. but i dont need to split it when there is a number like 60,000 with a comma in the middle.  following is the method i am using to segment the phrases.</p>

<pre><code>   public static ArrayList&lt;String&gt; phrasesSegmenter(String text) {
    ArrayList&lt;String&gt; phrases = new ArrayList&lt;String&gt;();
    StringTokenizer st = new StringTokenizer(text, "","");
    while (st.hasMoreTokens()) {
        phrases.add(st.nextToken());
    }
    return phrases;
}
</code></pre>
",Named Entity Recognition (NER),identify end sentence string need broken sentence using sentence boundry punctuation like split sentence pointer way identify correct sentence boundry place java tried using stringtokenizer java util pakage always break sentence whenever find pointer someone suggest method correctly method tried tokenizing text sentence also method segement sentence phrase also program found comma split text dont need split number like comma middle following method using segment phrase
How to use Mallet for NER,"<p>I'm new to the subject of NLP and requested to perform -named entity recognition- (NER) using Mallet.
I have a text, and I give feature vector for each word in it. I would like to train a model which later on I can test on fresh text file.
My question is how do I create such model, what is the input for the model. I could use some code examples :) 
Thanks !</p>
",Named Entity Recognition (NER),use mallet ner new subject nlp requested perform named entity recognition ner using mallet text give feature vector word would like train model later test fresh text file question create model input model could use code example thanks
Seven class classifier not giving desired results in StanfordNLP python,"<p>I am trying to use Stanford's named Entity Recognizer. I want to use the 7 class classifier because I even want to detect time(or date) and other things in a sentence. When entering the sentence:</p>

<pre><code>""He was born on October 15, 1931 at Dhanushkothi in the temple town Rameshwaram in Tamil Nadu.""
</code></pre>

<p>in the online demo at Stanford NLP site (<a href=""http://nlp.stanford.edu:8080/ner/process"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/ner/process</a>) it is classifying correctly as can be seen in this image (the demo in the Stanford site for the above line):</p>

<p><img src=""https://i.sstatic.net/EE4g5.png"" alt=""The demo in the stanford site for the above line""></p>

<p>But, when I'm trying the code to run on my system using NLTL and StanfordTagger, I am getting wrong result. I am getting the output as:</p>

<pre><code>[(u'He', u'O'), (u'was', u'O'), (u'born', u'O'), (u'on', u'O'), (u'1931-10-15', u'O'), 
(u'at', u'O'), (u'Dhanushkothi', u'O'), (u'in', u'O'), (u'the', u'O'), 
(u'temple', u'O'), (u'town', u'O'), (u'Rameshwaram', u'O'), (u'in', u'O'), 
(u'Tamil', u'ORGANIZATION'), (u'Nadu', u'ORGANIZATION'), (u'.', u'O')]
</code></pre>

<p>It is identifying the date incorrectly here as 'other' and even Tamil Nadu as an organization instead of a location. The code I've used is here below:</p>

<pre><code>from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.tag import StanfordNERTagger

st = StanfordNERTagger('english.muc.7class.distsim.crf.ser.gz','stanford-ner.jar')

i= ""He was born on October 15, 1931 at Dhanushkothi in the temple town Rameshwaram in Tamil Nadu.""

words = nltk.word_tokenize(i)
namedEnt = st.tag(words)

print namedEnt 
</code></pre>

<p>Can anyone please tell the mistake I'm doing (if any) or any other way to identify location and time in a sentence? I'm a beginner to NLP and any help regarding this would be appreciated.</p>
",Named Entity Recognition (NER),seven class classifier giving desired result stanfordnlp python trying use stanford named entity recognizer want use class classifier even want detect time date thing sentence entering sentence online demo stanford nlp site classifying correctly seen image demo stanford site line trying code run system using nltl stanfordtagger getting wrong result getting output identifying date incorrectly even tamil nadu organization instead location code used anyone please tell mistake way identify location time sentence beginner nlp help regarding would appreciated
Joining current token with previous in a vector array (or any collection),"<p>I am working on building rule based Named Entity Recognizer and the problem I am facing is this:</p>

<p>I am able to identify all the proper nouns in a text but the way they get identified is what I am having issues with.</p>

<p>E.g ""Rhode Island based Atrion Corporation"" gets outputted as [[Rhode],[Island],[Atriaon],[Corporation]]</p>

<p>I am building rules to combine proper nouns occuring together and identify them as one entity but I am iterating over every word in a chunk/clause and adding the identified proper noun into a vector. Is there a way I could join elements in a vector or any collection for that matter to get an output like:</p>

<p>[[Rhode Island], [Atrion Corporation]]</p>

<p>Any suggestion would be greatly appreciated.</p>

<p><strong>EDIT</strong></p>

<p>So I've added the rules and am using a string builder to build the string and then add the proper nouns, however, my results still aren't ideal. Now I get something like:</p>

<p>[ Rhode Island Atrion]</p>

<p>Here is my code:</p>

<pre><code>Vector&lt;String&gt; nnpTokens = new Vector&lt;String&gt;();
    Set&lt;String&gt; NNPs = new HashSet&lt;String&gt;();
    StringBuilder builder = new StringBuilder();
    Vector &lt;Integer&gt; nnpIndexes = new Vector&lt;Integer&gt;();

            for( int i = 0; i &lt;= chunk.getSize(); i++)
            {

                NLWord word = chunk.getTokens().elementAt(i);

                //NNP followed by NNP
                if(i &lt; chunk.getSize() &amp;&amp; (word.getPostag().equals(""NNP"") || word.getPostag().equals(""NNPS"")) &amp;&amp; (chunk.getTokens().elementAt(i + 1).getPostag().equals(""NNP"") 
                        || (chunk.getTokens().elementAt(i + 1).getPostag().equals(""NNPS""))))
                {
                    builder.append("" "");
                    builder.append(word.getToken());
                    nnpIndexes.add(word.getIndex());
                    //nnpTokens.add(word.getToken());
                    continue;

                }
                //NNP preceded by NNP
                if( i &gt; 0 &amp;&amp; (word.getPostag().equals(""NNP"") || word.getPostag().equals(""NNPS"")) &amp;&amp; (chunk.getTokens().elementAt(i - 1).getPostag().equals(""NNP"") 
                        || chunk.getTokens().elementAt(i - 1).getPostag().equals(""NNPS"")))
                {
                    builder.append("" "");
                    builder.append(word.getToken());
                    //nnpTokens.add(word.getToken());
                    nnpIndexes.add(word.getIndex());
                    continue;

                }

                //NNP followed by a prep/cc relation 
                if( i &lt; chunk.getSize() &amp;&amp; (word.getPostag().equals(""NNP"") || word.getPostag().equals(""NNPS""))
                        &amp;&amp;  (chunk.getTokens().elementAt(i + 1).getToken().equalsIgnoreCase(""for"") 
                        ||   chunk.getTokens().elementAt(i + 1).getToken().equalsIgnoreCase(""of"") 
                        ||   chunk.getTokens().elementAt(i + 1).getToken().equalsIgnoreCase(""and""))){

                    builder.append("" "");
                    builder.append(word.getToken());
                    //nnpTokens.add(word.getToken());
                    nnpIndexes.add(word.getIndex());
                    continue;

                }

                //NNP preceded by a prep/cc relation 
                if( i &gt; 0 &amp;&amp; (word.getPostag().equals(""NNP"") || word.getPostag().equals(""NNPS""))
                        &amp;&amp; (chunk.getTokens().elementAt(i - 1).getToken().equalsIgnoreCase(""for"") 
                        ||  chunk.getTokens().elementAt(i - 1).getToken().equalsIgnoreCase(""of"") 
                        ||  chunk.getTokens().elementAt(i - 1).getToken().equalsIgnoreCase(""and""))){

                    builder.append("" "");
                    builder.append(word.getToken());
                    //nnpTokens.add(word.getToken());
                    nnpIndexes.add(word.getIndex());
                    continue;

                }



                if(word.getPostag().equals(""NNP"") || word.getPostag().equals(""NNPS"") &amp;&amp; (chunk.getTokens().elementAt(i - 1).getPostag().equals(""NNP"") 
                        || (chunk.getTokens().elementAt(i - 1).getPostag().equals(""NNPS""))))
                {

                    //nnpTokens.add(word.getToken());
                    builder.append("" "");
                    builder.append(word.getToken());
                    nnpIndexes.add(word.getIndex());
                    continue;
                }


                if (i &gt; 0 &amp;&amp; (word.getToken().equalsIgnoreCase(""for"") || word.getToken().equalsIgnoreCase(""of"") 
                            || word.getToken().equalsIgnoreCase(""and"")) &amp;&amp; (chunk.getTokens().elementAt(i - 1).getPostag().equals(""NNP"") 
                            || chunk.getTokens().elementAt(i - 1).getPostag().equals(""NNPS""))){

                        //nnpTokens.add(word.getToken());
                        builder.append("" "");
                        builder.append(word.getToken());
                        nnpIndexes.add(word.getIndex());
                        continue;
                }

                if( i &lt; chunk.getSize() &amp;&amp; i &gt; 0 &amp;&amp; (word.getPostag().equals(""NNP"") || word.getPostag().equals(""NNPS"")) 
                        &amp;&amp; !(chunk.getTokens().elementAt(i - 1).getPostag().equals(""NNP"") 
                        ||   chunk.getTokens().elementAt(i - 1).getPostag().equals(""NNPS"")) 
                        &amp;&amp; !(chunk.getTokens().elementAt(i + 1).getPostag().equals(""NNP"") 
                        ||   chunk.getTokens().elementAt(i + 1).getPostag().equals(""NNPS"")) 
                        &amp;&amp; !(chunk.getTokens().elementAt(i + 1).getPostag().equals(""PRP"") 
                        ||   chunk.getTokens().elementAt(i + 1).getPostag().equals(""CC""))
                        &amp;&amp; !(chunk.getTokens().elementAt(i - 1).getPostag().equals(""PRP"") 
                        ||   chunk.getTokens().elementAt(i - 1).getPostag().equals(""CC""))){
                    nnpTokens.add(word.getToken());
                    nnpIndexes.add(word.getIndex());
                    continue;

                }
                else 
                    continue;
            }

            //String newNNPTokens = builder.toString();
            NNPs.add(builder.toString());
            NNPs.addAll(nnpTokens);
            //System.out.println(""NNPs:"" + NNPs);
            nnpTokens.clear();


       //System.out.println(NNPs);
       return NNPs;
    }
</code></pre>
",Named Entity Recognition (NER),joining current token previous vector array collection working building rule based named entity recognizer problem facing able identify proper noun text way get identified issue e g rhode island based atrion corporation get outputted rhode island atriaon corporation building rule combine proper noun occuring together identify one entity iterating every word chunk clause adding identified proper noun vector way could join element vector collection matter get output like rhode island atrion corporation suggestion would greatly appreciated edit added rule using string builder build string add proper noun however result still ideal get something like rhode island atrion code
NLTK Named Entity recognition for a column in a dataset,"<p>Thanks to ""alvas"" code from here , <a href=""https://stackoverflow.com/questions/24398536/named-entity-recognition-with-regular-expression-nltk"">Named Entity Recognition with Regular Expression: NLTK</a> and as an example:</p>

<pre><code>from nltk import ne_chunk, pos_tag
from nltk.tokenize import word_tokenize
from nltk.tree import Tree

def get_continuous_chunks(text):
    chunked = ne_chunk(pos_tag(word_tokenize(text)))
    prev = None
    continuous_chunk = []
    current_chunk = []

    for i in chunked:
        if type(i) == Tree:
            current_chunk.append("" "".join([token for token, pos in i.leaves()]))
        elif current_chunk:
            named_entity = "" "".join(current_chunk)
            if named_entity not in continuous_chunk:
                continuous_chunk.append(named_entity)
                current_chunk = []
        else:
            continue

    return continuous_chunk

txt = 'The new GOP era in Washington got off to a messy start Tuesday as House Republicans,under pressure from President-elect Donald Trump.'
print (get_continuous_chunks(txt))
</code></pre>

<p>the output is :<br/></p>

<blockquote>
  <p>['GOP', 'Washington', 'House Republicans', 'Donald Trump']</p>
</blockquote>

<p>I replaced this text with this : <code>txt = df['content'][38]</code> from my dataset and I get this result : </p>

<blockquote>
  <p>['Ina', 'Tori K.', 'Martin Cuilla', 'Phillip K', 'John J Lavorato']</p>
</blockquote>

<p>This dataset has many rows and one column named 'content'.My question is how can I use this code to extract names from this column for each row and store that names in another column and corresponding rows?</p>

<pre><code>import os
from nltk.tag import StanfordNERTagger
from nltk.tokenize import word_tokenize
from nltk.tree import Tree
st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')
text = df['content']
tokenized_text = word_tokenize(text)
classified_text = st.tag(tokenized_text)
print (classified_text)
</code></pre>
",Named Entity Recognition (NER),nltk named entity recognition column dataset thanks alvas code href entity recognition regular expression nltk example output gop washington house republican donald trump replaced text dataset get result ina torus k martin cuilla phillip k john j lavorato dataset ha many row one column named content question use code extract name column row store name another column corresponding row
How to find the locations from where the word was selected as keyword by Microsoft text analytics APIi?,"<p>I am using Microsoft cognitive APIs for finding relevant keywords in my paragraph. I need to know from which sentences were those keywords were selected. Is there any way to do that because demo by Microsoft highlights the places from where that keyword was selected. </p>
",Named Entity Recognition (NER),find location word wa selected keyword microsoft text analytics apii using microsoft cognitive apis finding relevant keywords paragraph need know sentence keywords selected way demo microsoft highlight place keyword wa selected
"CRF model trained on plural, not working on singular","<p>I have made a CRF model. My data set has 24 classes and at this time I am in beginning so my training data has just 1200 tokens/corpus. I have train the model. In my training data I have used the plural of tokens like addresses, photos, states, countries etc.</p>

<p>Now at the time of testing if I give plural of tokens in sentence form to this model then it work good but if I enter my sentence in singular like photo, state etc then it does not assign any tag to it.</p>

<p>This behavior of crf is looking very strange. I have explore the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ie/NERFeatureFactory.html"" rel=""nofollow noreferrer"">NER Feature Factory</a> and used some lemma features but it also did not work. Sharing my <code>austen.prop</code> for the model formation.</p>

<pre><code># location of the training file
trainFile = training_data_for_ner.txt
# location where you would like to save (serialize) your
# classifier; adding .gz at the end automatically gzips the file,
# making it smaller, and faster to load
serializeTo = ner-model.ser.gz

# structure of your training file; this tells the classifier that
# the word is in column 0 and the correct answer is in column 1
map = word=0,answer=1,pos=2,lemma=3

# This specifies the order of the CRF: order 1 means that features
# apply at most to a class pair of previous class and current class
# or current class and next class.
maxLeft=1

# these are the features we'd like to train with
# some are discussed below, the rest can be
# understood by looking at NERFeatureFactory
useClassFeature=true
useWord=true
# word character ngrams will be included up to length 6 as prefixes
# and suffixes only 
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useDisjunctive=true
useSequences=true
usePrevSequences=true
# the last 4 properties deal with word shape features
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
# newly added features.
useLemmas=true
usePrevNextLemmas=true
useLemmaAsWord=true
useTags=true
</code></pre>

<p>Last four features were added by reading that <code>NER Feature Factory</code>. If anyone can help me to solve this problem then I will be thankful to you.</p>
",Named Entity Recognition (NER),crf model trained plural working singular made crf model data set ha class time beginning training data ha token corpus train model training data used plural token like address photo state country etc time testing give plural token sentence form model work good enter sentence singular like photo state etc doe assign tag behavior crf looking strange explore ner feature factory used lemma feature also work sharing model formation last four feature added reading anyone help solve problem thankful
How the classifier on Stanford NER works?,"<p>I try to use Stanford NER to classify some text contain Call for Papers information.
I have some question on it.</p>

<ol>
<li>how Stanford NER works on using classifier? how the label can assign to an entity and how the tool identify an entity?</li>
<li>I have an example like there ""San Francisco, USA"".. Why San Francisco labeled as LOCATION, but USA labeled as ORGANIZATION? is there specific reason?</li>
<li>And last but not least, the classifier is it include the dictionary of words or how?</li>
</ol>

<p>Thank you very much.</p>
",Named Entity Recognition (NER),classifier stanford ner work try use stanford ner classify text contain call paper information question stanford ner work using classifier label assign entity tool identify entity example like san francisco usa san francisco labeled location usa labeled organization specific reason last least classifier include dictionary word thank much
Extract entities from folksonomies,"<p>I am a newbie to NLP and related technologies. I have been researching on decomposing folksonomies such as, hashtags into individual terms (ex:- #harrypotterworld as harry potter world) in order to carry out Named-Entity Recognition.</p>

<p>But I did not come across any available library or previous work I could use for this. Is this achievable or am I following a wrong procedure? If so, are there any available libraries or algorithmic techniques I could use?</p>
",Named Entity Recognition (NER),extract entity folksonomies newbie nlp related technology researching decomposing folksonomies hashtags individual term ex harrypotterworld harry potter world order carry named entity recognition come across available library previous work could use achievable following wrong procedure available library algorithmic technique could use
Stanford NER custom model not working,"<p>I am working on an aspect level sentiment analysis project.
I'm now in the implementation stage of aspect term extraction module, and use Stanford NER to train a custom model of my own, using an annotated dataset that I have which comprises 1000 TripAdvisor travel reviews.</p>

<p>I have managed to train a custom NER. The code for that is as follows;</p>

<pre><code>import java.util.Properties;

import edu.stanford.nlp.ie.crf.CRFClassifier;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.sequences.SeqClassifierFlags;
import edu.stanford.nlp.util.StringUtils;

public class NERTrainer {

    public static void main(String[] args) {
        // TODO Auto-generated method stub
        String prop = ""c:\\Users\\User\\Downloads\\properties.prop"";
        Properties props = StringUtils.propFileToProperties(prop);
        String to = props.getProperty(""serializeTo"");
        props.setProperty(""serializeTo"", ""c:\\Users\\User\\Desktop\\ner-travel-planner-model.ser.gz"");
        SeqClassifierFlags flags = new SeqClassifierFlags(props);
        CRFClassifier&lt;CoreLabel&gt; crf = new CRFClassifier&lt;CoreLabel&gt;(flags);
        crf.train();
        crf.serializeClassifier(""c:\\Users\\User\\Desktop\\ner-travel-planner-model.ser.gz"");
    }
</code></pre>

<p>My properties file: (Using the default file given in Stanford website)</p>

<pre><code> trainFile = IOB.tsv
#serializeTo = ner-model.ser.gz
map = word=0,answer=1

useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
useDisjunctive=true
</code></pre>

<p>The logs showed that it was trained successfully.</p>

<pre><code>usePrevSequences=true
useClassFeature=true
useTypeSeqs2=true
useSequences=true
wordShape=chris2useLC
useTypeySequences=true
useDisjunctive=true
noMidNGrams=true
serializeTo=c:\Users\User\Desktop\ner-travel-planner-model.ser.gz
maxNGramLeng=6
useNGrams=true
usePrev=true
useNext=true
maxLeft=1
trainFile=IOB.tsv
map=word=0,answer=1
useWord=true
useTypeSeqs=true
numFeatures = 114317
Time to convert docs to feature indices: 2.0 seconds
numClasses: 3 [0=O,1=I-TERM,2=B-TERM]
numDocuments: 2
numDatums: 56513
numFeatures: 114317
Time to convert docs to data/labels: 1.1 seconds
numWeights: 596487
QNMinimizer called on double function of 596487 variables, using M = 25.
               An explanation of the output:
Iter           The number of iterations
evals          The number of function evaluations
SCALING        &lt;D&gt; Diagonal scaling was used; &lt;I&gt; Scaled Identity
LINESEARCH     [## M steplength]  Minpack linesearch
                   1-Function value was too high
                   2-Value ok, gradient positive, positive curvature
                   3-Value ok, gradient negative, positive curvature
                   4-Value ok, gradient negative, negative curvature
               [.. B]  Backtracking
VALUE          The current function value
TIME           Total elapsed time
|GNORM|        The current norm of the gradient
{RELNORM}      The ratio of the current to initial gradient norms
AVEIMPROVE     The average improvement / current value
EVALSCORE      The last available eval score

Iter ## evals ## &lt;SCALING&gt; [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE

Iter 1 evals 1 &lt;D&gt; [11M 8.212E-5] 1.714E5 1.06s |1.080E4| {1.082E-1} 0.000E0 - 
Iter 2 evals 4 &lt;D&gt; [33131M 6.201E0] 1.204E5 2.78s |8.770E3| {8.784E-2} 2.120E-1 - 
Iter 3 evals 10 &lt;D&gt; [1M 2.210E-2] 1.158E5 3.36s |4.819E3| {4.826E-2} 1.603E-1 - 
.
.
.
Iter 175 evals 207 &lt;D&gt; [M 1.000E0] 2.132E3 74.42s
QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| &lt; TOL 
Total time spent in optimization: 74.43s
</code></pre>

<p>The classifier file can be found <a href=""http://rghost.net/7MrBHCdTL"" rel=""nofollow"">here</a>. </p>

<p>Training data is in IOB notation;</p>

<pre><code>B-TERM - begining of aspect term label
I-TERM - continuation of aspect term label
O - Default 'not a keyword' label
</code></pre>

<p>Sample training data;</p>

<pre><code>so  O
peaceful    B-TERM
interesting B-TERM
and I-TERM
informative I-TERM
it  O
had O
been    O
raining B-TERM
so  O
we  O
had O
it's    O
still   O
a   O
place   B-TERM
of  I-TERM
worship I-TERM
after   O
that    O
just    O
walk    B-TERM
down    O
to  O
jungle  O
beach   O
and O
grab    O
yourself    O
a   O
cold    B-TERM
beer    I-TERM
or  O
two O
and O
a   O
cool    O
off O
in  O
the O
surf    B-TERM
</code></pre>

<p>When I tried to test it however, it didn't seem to work. All the tokens were tagged with O only.</p>

<pre><code>import edu.stanford.nlp.ie.NERClassifierCombiner;
import edu.stanford.nlp.ie.AbstractSequenceClassifier;
import edu.stanford.nlp.ie.crf.*;
import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.CoreLabel;
import java.io.IOException;
import java.util.List;

public class NERDemo {

  public static void main(String[] args) throws IOException {
    String serializedClassifier = ""c:\\Users\\User\\Desktop\\ner-travel-planner-model.ser.gz"";
   // String serializedClassifier2 = ""/local/stanford-ner-2015-01-30/classifiers/english.muc.7class.distsim.crf.ser.gz"";

    if (args.length &gt; 0) {
      serializedClassifier = args[0];
    }

    NERClassifierCombiner classifier = new NERClassifierCombiner(false, false, 
            serializedClassifier);

    String fileContents = IOUtils.slurpFile(""c:\\Users\\User\\Desktop\\test-ner.txt"");
    List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(fileContents);

    int i = 0;
    for (List&lt;CoreLabel&gt; lcl : out) {
      i++;
      int j = 0;
      for (CoreLabel cl : lcl) {
        j++;
        System.out.printf(""%d:%d: %s%n"", i, j,
                cl.toShorterString(""Text"", ""CharacterOffsetBegin"", ""CharacterOffsetEnd"", ""NamedEntityTag""));
      }
    }
  }
</code></pre>

<p>output:</p>

<pre><code>Loading classifier from c:\Users\User\Desktop\ner-travel-planner-model.ser.gz ... done [0.4 sec].
1:1: [Text=If CharacterOffsetBegin=0 CharacterOffsetEnd=2 NamedEntityTag=O]
1:2: [Text=you CharacterOffsetBegin=3 CharacterOffsetEnd=6 NamedEntityTag=O]
1:3: [Text=happen CharacterOffsetBegin=7 CharacterOffsetEnd=13 NamedEntityTag=O]
1:4: [Text=to CharacterOffsetBegin=14 CharacterOffsetEnd=16 NamedEntityTag=O]
1:5: [Text=visit CharacterOffsetBegin=17 CharacterOffsetEnd=22 NamedEntityTag=O]
1:6: [Text=Kandy CharacterOffsetBegin=23 CharacterOffsetEnd=28 NamedEntityTag=O]
1:7: [Text=the CharacterOffsetBegin=30 CharacterOffsetEnd=33 NamedEntityTag=O]
1:8: [Text=Tea CharacterOffsetBegin=34 CharacterOffsetEnd=37 NamedEntityTag=O]
1:9: [Text=Museum CharacterOffsetBegin=38 CharacterOffsetEnd=44 NamedEntityTag=O]
1:10: [Text=is CharacterOffsetBegin=45 CharacterOffsetEnd=47 NamedEntityTag=O]
1:11: [Text=a CharacterOffsetBegin=48 CharacterOffsetEnd=49 NamedEntityTag=O]
1:12: [Text=must CharacterOffsetBegin=50 CharacterOffsetEnd=54 NamedEntityTag=O]
1:13: [Text=visit CharacterOffsetBegin=55 CharacterOffsetEnd=60 NamedEntityTag=O]
1:14: [Text=place CharacterOffsetBegin=61 CharacterOffsetEnd=66 NamedEntityTag=O]
1:15: [Text=it CharacterOffsetBegin=68 CharacterOffsetEnd=70 NamedEntityTag=O]
1:16: [Text=is CharacterOffsetBegin=71 CharacterOffsetEnd=73 NamedEntityTag=O]
1:17: [Text=located CharacterOffsetBegin=74 CharacterOffsetEnd=81 NamedEntityTag=O]
1:18: [Text=in CharacterOffsetBegin=82 CharacterOffsetEnd=84 NamedEntityTag=O]
1:19: [Text=a CharacterOffsetBegin=85 CharacterOffsetEnd=86 NamedEntityTag=O]
1:20: [Text=lovely CharacterOffsetBegin=87 CharacterOffsetEnd=93 NamedEntityTag=O]
1:21: [Text=place CharacterOffsetBegin=94 CharacterOffsetEnd=99 NamedEntityTag=O]
1:22: [Text=with CharacterOffsetBegin=100 CharacterOffsetEnd=104 NamedEntityTag=O]
1:23: [Text=a CharacterOffsetBegin=105 CharacterOffsetEnd=106 NamedEntityTag=O]
1:24: [Text=breathtaking CharacterOffsetBegin=107 CharacterOffsetEnd=119 NamedEntityTag=O]
1:25: [Text=view CharacterOffsetBegin=120 CharacterOffsetEnd=124 NamedEntityTag=O]
1:26: [Text=. CharacterOffsetBegin=124 CharacterOffsetEnd=125 NamedEntityTag=O]
2:1: [Text=This CharacterOffsetBegin=126 CharacterOffsetEnd=130 NamedEntityTag=O]
2:2: [Text=place CharacterOffsetBegin=131 CharacterOffsetEnd=136 NamedEntityTag=O]
2:3: [Text=will CharacterOffsetBegin=137 CharacterOffsetEnd=141 NamedEntityTag=O]
2:4: [Text=tell CharacterOffsetBegin=142 CharacterOffsetEnd=146 NamedEntityTag=O]
2:5: [Text=you CharacterOffsetBegin=147 CharacterOffsetEnd=150 NamedEntityTag=O]
2:6: [Text=everything CharacterOffsetBegin=151 CharacterOffsetEnd=161 NamedEntityTag=O]
2:7: [Text=you CharacterOffsetBegin=162 CharacterOffsetEnd=165 NamedEntityTag=O]
2:8: [Text=should CharacterOffsetBegin=166 CharacterOffsetEnd=172 NamedEntityTag=O]
2:9: [Text=know CharacterOffsetBegin=173 CharacterOffsetEnd=177 NamedEntityTag=O]
2:10: [Text=about CharacterOffsetBegin=178 CharacterOffsetEnd=183 NamedEntityTag=O]
2:11: [Text=the CharacterOffsetBegin=184 CharacterOffsetEnd=187 NamedEntityTag=O]
2:12: [Text=history CharacterOffsetBegin=188 CharacterOffsetEnd=195 NamedEntityTag=O]
2:13: [Text=of CharacterOffsetBegin=196 CharacterOffsetEnd=198 NamedEntityTag=O]
2:14: [Text=Tea CharacterOffsetBegin=199 CharacterOffsetEnd=202 NamedEntityTag=O]
2:15: [Text=in CharacterOffsetBegin=203 CharacterOffsetEnd=205 NamedEntityTag=O]
2:16: [Text=Sri CharacterOffsetBegin=206 CharacterOffsetEnd=209 NamedEntityTag=O]
2:17: [Text=Lanka CharacterOffsetBegin=210 CharacterOffsetEnd=215 NamedEntityTag=O]
2:18: [Text=. CharacterOffsetBegin=215 CharacterOffsetEnd=216 NamedEntityTag=O]
3:1: [Text=There CharacterOffsetBegin=217 CharacterOffsetEnd=222 NamedEntityTag=O]
3:2: [Text=are CharacterOffsetBegin=223 CharacterOffsetEnd=226 NamedEntityTag=O]
3:3: [Text=guides CharacterOffsetBegin=227 CharacterOffsetEnd=233 NamedEntityTag=O]
3:4: [Text=in CharacterOffsetBegin=234 CharacterOffsetEnd=236 NamedEntityTag=O]
3:5: [Text=the CharacterOffsetBegin=237 CharacterOffsetEnd=240 NamedEntityTag=O]
3:6: [Text=building CharacterOffsetBegin=241 CharacterOffsetEnd=249 NamedEntityTag=O]
3:7: [Text=who CharacterOffsetBegin=250 CharacterOffsetEnd=253 NamedEntityTag=O]
3:8: [Text=will CharacterOffsetBegin=254 CharacterOffsetEnd=258 NamedEntityTag=O]
3:9: [Text=take CharacterOffsetBegin=259 CharacterOffsetEnd=263 NamedEntityTag=O]
3:10: [Text=you CharacterOffsetBegin=264 CharacterOffsetEnd=267 NamedEntityTag=O]
3:11: [Text=around CharacterOffsetBegin=268 CharacterOffsetEnd=274 NamedEntityTag=O]
3:12: [Text=explaining CharacterOffsetBegin=275 CharacterOffsetEnd=285 NamedEntityTag=O]
3:13: [Text=what CharacterOffsetBegin=286 CharacterOffsetEnd=290 NamedEntityTag=O]
3:14: [Text=they CharacterOffsetBegin=291 CharacterOffsetEnd=295 NamedEntityTag=O]
3:15: [Text=have CharacterOffsetBegin=296 CharacterOffsetEnd=300 NamedEntityTag=O]
3:16: [Text=in CharacterOffsetBegin=301 CharacterOffsetEnd=303 NamedEntityTag=O]
3:17: [Text=each CharacterOffsetBegin=304 CharacterOffsetEnd=308 NamedEntityTag=O]
3:18: [Text=floor CharacterOffsetBegin=309 CharacterOffsetEnd=314 NamedEntityTag=O]
3:19: [Text=. CharacterOffsetBegin=314 CharacterOffsetEnd=315 NamedEntityTag=O]
4:1: [Text=You CharacterOffsetBegin=316 CharacterOffsetEnd=319 NamedEntityTag=O]
4:2: [Text=could CharacterOffsetBegin=320 CharacterOffsetEnd=325 NamedEntityTag=O]
4:3: [Text=enjoy CharacterOffsetBegin=326 CharacterOffsetEnd=331 NamedEntityTag=O]
4:4: [Text=a CharacterOffsetBegin=332 CharacterOffsetEnd=333 NamedEntityTag=O]
4:5: [Text=cup CharacterOffsetBegin=334 CharacterOffsetEnd=337 NamedEntityTag=O]
4:6: [Text=of CharacterOffsetBegin=338 CharacterOffsetEnd=340 NamedEntityTag=O]
4:7: [Text=good CharacterOffsetBegin=341 CharacterOffsetEnd=345 NamedEntityTag=O]
4:8: [Text=tea CharacterOffsetBegin=346 CharacterOffsetEnd=349 NamedEntityTag=O]
4:9: [Text=in CharacterOffsetBegin=350 CharacterOffsetEnd=352 NamedEntityTag=O]
4:10: [Text=the CharacterOffsetBegin=353 CharacterOffsetEnd=356 NamedEntityTag=O]
4:11: [Text=restaurant CharacterOffsetBegin=357 CharacterOffsetEnd=367 NamedEntityTag=O]
4:12: [Text=upstairs CharacterOffsetBegin=368 CharacterOffsetEnd=376 NamedEntityTag=O]
4:13: [Text=but CharacterOffsetBegin=378 CharacterOffsetEnd=381 NamedEntityTag=O]
4:14: [Text=they CharacterOffsetBegin=382 CharacterOffsetEnd=386 NamedEntityTag=O]
4:15: [Text=cant CharacterOffsetBegin=387 CharacterOffsetEnd=391 NamedEntityTag=O]
4:16: [Text=make CharacterOffsetBegin=392 CharacterOffsetEnd=396 NamedEntityTag=O]
4:17: [Text=a CharacterOffsetBegin=397 CharacterOffsetEnd=398 NamedEntityTag=O]
4:18: [Text=proper CharacterOffsetBegin=399 CharacterOffsetEnd=405 NamedEntityTag=O]
4:19: [Text=tea CharacterOffsetBegin=406 CharacterOffsetEnd=409 NamedEntityTag=O]
4:20: [Text=even CharacterOffsetBegin=411 CharacterOffsetEnd=415 NamedEntityTag=O]
4:21: [Text=if CharacterOffsetBegin=416 CharacterOffsetEnd=418 NamedEntityTag=O]
4:22: [Text=it CharacterOffsetBegin=419 CharacterOffsetEnd=421 NamedEntityTag=O]
4:23: [Text=saves CharacterOffsetBegin=422 CharacterOffsetEnd=427 NamedEntityTag=O]
4:24: [Text=their CharacterOffsetBegin=428 CharacterOffsetEnd=433 NamedEntityTag=O]
4:25: [Text=life CharacterOffsetBegin=434 CharacterOffsetEnd=438 NamedEntityTag=O]
4:26: [Text=. CharacterOffsetBegin=438 CharacterOffsetEnd=439 NamedEntityTag=O]
</code></pre>

<p>I can't seem to figure out what I'm doing wrong. Please help.</p>
",Named Entity Recognition (NER),stanford ner custom model working working aspect level sentiment analysis project implementation stage aspect term extraction module use stanford ner train custom model using annotated dataset comprises tripadvisor travel review managed train custom ner code follows property file using default file given stanford website log showed wa trained successfully classifier file found training data iob notation sample training data tried test however seem work token tagged output seem figure wrong please help
How to identify n-gram before tokenization in stanford core-nlp?,"<p>I am trying to use the core-nlp annotation pipeline with default settings all through from tokenizing until ner_tags. I did observe that the ""tokenizer"" module is identifying , say ""Vice President"" as two individual tokens {vice,President} resulting in ner_tags identification as {o,TITLE} instead of {Vice President} and  {TITLE}. How can I get the tokenizer to identify ""Vice president"" as one single token , that help Ner_Tags to identify titles appropriately.</p>
",Named Entity Recognition (NER),identify n gram tokenization stanford core nlp trying use core nlp annotation pipeline default setting tokenizing ner tag observe tokenizer module identifying say vice president two individual token vice president resulting ner tag identification title instead vice president title get tokenizer identify vice president one single token help ner tag identify title appropriately
Distant Supervision Code,"<p>I have to implement Distant Supervision python code. I have constructed a feature vector for the unlabeled pool of data. The feature vector contains the lexical features of the sentences. I am not able to understand how to apply the classifier on this vector.</p>

<p><strong>EDIT:</strong><br>
1) I have an unlabeled pool of data from which I want to extract new relations.<br>
2) I have a file which contains typeOf relations. So on the basis of the relations in this file I want to get new relations in my data.<br>
3) I have extracted the lexical features from the sentences in the following format:</p>

<p>[[sequence of words between the entities"",""POS tags of these words"",""Flag indicating which entity came first"",""Window of k words to the left of entity 1"",""POS tags of these words"",""Window of k words to the right of entity 2"",""POS of these tags""],....]</p>

<p>This is an example of the data existing in the typeOf.txt file(relation file):</p>

<p>Tupi ----------------- 2D animation software<br>
Pencil2D -----------2D animation software<br>
SWFTools ---- -----2D animation software<br>
Synfig -------------- 2D animation software<br>
Flipnote Studio --- 2D animation software  </p>

<p>I hope this makes my query a little bit more clearer.</p>
",Named Entity Recognition (NER),distant supervision code implement distant supervision python code constructed feature vector unlabeled pool data feature vector contains lexical feature sentence able understand apply classifier vector edit unlabeled pool data want extract new relation file contains typeof relation basis relation file want get new relation data extracted lexical feature sentence following format sequence word entity po tag word flag indicating entity came first window k word left entity po tag word window k word right entity po tag example data existing typeof txt file relation file tupi animation software pencil animation software swftools animation software synfig animation software flipnote studio animation software hope make query little bit clearer
Customised tokens annotation in R,"<p>Currently I'm working on an NLP project. It's totally new for me that's why i'm really struggling with implementation of NLP techniques in R. 
Generally speaking, I need to extract machines entities from descriptions. I have a dictionary of machines which contains 2 columns: Manufacturer and Model. </p>

<p>To train the extraction model, I have to have an annotated corpus. That's where I'm stuck. How to annotate machines in text? Here is an example of the text:</p>

<p><em>The <strong>Skyjack 3219E</strong> electric scissor lift is a self-propelled device powered by 4 x 6 V batteries. The machine is easy to charge, just plug it into the mains. This unit can be used in construction, manufacturing and maintenance operations as a working installation on any flat paved surface. You can use it both indoors and outdoors. Thanks to its non-marking tyres, the machine does not leave any visible tracks on floors. The machine can be driven at full height and is very easy to operate. The S3219E has a 250 kg platform payload capacity. It can handle two people when operating indoors and one outdoors. Discover our trainings via Heli Safety Academy.</em></p>

<p><strong>Skyjack 3219E</strong> - this is a machine which has to be identified and tagged.
I wanna have results similar to POS tagging but instead of nouns and verbs - manufacturer and model. All the other words might be tagged as irrelevant.</p>

<p>Manual annotation is very expensive and not an option as usually descriptions are really long and messy.</p>

<p>Is there a way to adapt POS tagger and use a customised dictionary for tagging? Any help is appreciated!</p>
",Named Entity Recognition (NER),customised token annotation r currently working nlp project totally new really struggling implementation nlp technique r generally speaking need extract machine entity description dictionary machine contains column manufacturer model train extraction model annotated corpus stuck annotate machine text example text skyjack e electric scissor lift self propelled device powered x v battery machine easy charge plug main unit used construction manufacturing maintenance operation working installation flat paved surface use indoors outdoors thanks non marking tyre machine doe leave visible track floor machine driven full height easy operate e ha kg platform payload capacity handle two people operating indoors one outdoors discover training via heli safety academy skyjack e machine ha identified tagged wan na result similar po tagging instead noun verb manufacturer model word might tagged irrelevant manual annotation expensive option usually description really long messy way adapt po tagger use customised dictionary tagging help appreciated
NER: Relate extracted entity to single real world concept,"<p>I am processing plain text documents and identifying entities like college/university names present in the document. Some times these names are written in different formats but they refer to a single college/university name.
<br>Example:
<br>Jawaharlal Nehru Technological University Hyderabad
<br>J.N.T.U Hyderabad
<br>JNTU Hyderabad
<br>JNTU-H
<br>Jawaharlal Nehru Technological University (JNTU) Hyderabad</p>

<p>All the above names refer to same college name.</p>

<p>How can we relate all these names to a single college/university names?<br>
(I am looking for some kind of web service or something like Google search because if i search for any of those names it returns same college link.)</p>
",Named Entity Recognition (NER),ner relate extracted entity single real world concept processing plain text document identifying entity like college university name present document time name written different format refer single college university name example jawaharlal nehru technological university hyderabad j n u hyderabad jntu hyderabad jntu h jawaharlal nehru technological university jntu hyderabad name refer college name relate name single college university name looking kind web service something like google search search name return college link
Can anyone give a brief overview of how to proceed with Named Entity Recognition in Tamil Language?,"<p>I have looked at Stanford NER and Polyglot. Both does not support Tamil Language.
I would like to use ML along with some rule based NLP processing to do the entity recognition</p>
",Named Entity Recognition (NER),anyone give brief overview proceed named entity recognition tamil language looked stanford ner polyglot doe support tamil language would like use ml along rule based nlp processing entity recognition
How to retrieve list of all possible ner_tags classes that stanford nlp module classifies?,"<p>I want to see all possible ner_tags that stanford nlp classifies the text into, such as PERSON,LOCATION, ORGANIZATION, TITLE etc, where can I find those, any pointers are much appreciated.</p>
",Named Entity Recognition (NER),retrieve list possible ner tag class stanford nlp module classifies want see possible ner tag stanford nlp classifies text person location organization title etc find pointer much appreciated
extracting relations from text,"<p>I want to extract relations from unstructured text in the form of (SUBJECT,OBJECT,ACTION) relations,</p>

<p>for instance,</p>

<p>""The boy is sitting on the table eating the chicken""</p>

<p>would give me, <br/> <br/>
(boy,chicken,eat)  <br/>
(boy,table,LOCATION)</p>

<p>etc..</p>

<p>although a python program + NLTK could process such a simple sentence as above.</p>

<p>I'd like to know if any of you have used tools or libraries preferably opensource to extract relations from a much wider domain such as a large collection of text documents or the web.</p>
",Named Entity Recognition (NER),extracting relation text want extract relation unstructured text form subject object action relation instance boy sitting table eating chicken would give boy chicken eat boy table location etc although python program nltk could process simple sentence like know used tool library preferably opensource extract relation much wider domain large collection text document web
How to get the text used for entity extraction in wit.ai?,"<p>If the text is ""What's the weather in chicago today?"", wit.ai returns the following entities</p>

<pre><code>""entities"": {
""location"": [
  {
    ""confidence"": 0.9483763321862972,
    ""type"": ""value"",
    ""value"": ""Chicago"",
    ""suggested"": true
  }
],
""datetime"": [
  {
    ""confidence"": 0.9499492133072054,
    ""type"": ""value"",
    ""value"": ""2016-12-13T00:00:00.000-08:00"",
    ""grain"": ""day"",
    ""values"": [
      {
        ""type"": ""value"",
        ""value"": ""2016-12-13T00:00:00.000-08:00"",
        ""grain"": ""day""
      }
    ]
  }
]
</code></pre>

<p>}</p>

<p>How can I get wit.ai to also return the text corresponding to each entity extraction, in this case ""chicago"" for wit/location and ""today"" for wit/datetime</p>

<p>For a user defined entity, I know I can do it by defining the search strategy as free-text &amp; keyword. But there is no option to change that for wit/datetime. </p>
",Named Entity Recognition (NER),get text used entity extraction wit ai text weather chicago today wit ai return following entity get wit ai also return text corresponding entity extraction case chicago wit location today wit datetime user defined entity know defining search strategy free text keyword option change wit datetime
merge two models in machine learning,"<p>I'm currently training a ner model using stanford ner. Is it possible to merge two models together? or is there a way to expend current model?</p>

<p>Suppose I originally have a large corpus called A, and I get a model MA. (Which works good)
But the problem is, it takes long time to train it and in the future I want to add entries into corpus A, say, B. I have to retrain (A + B) again which takes longer time.</p>

<p>Is there a way to just train B and merge it with A? Or I can.. expend the model MA to make it work for new entities.</p>

<p>Thanks!</p>
",Named Entity Recognition (NER),merge two model machine learning currently training ner model using stanford ner possible merge two model together way expend current model suppose originally large corpus called get model work good problem take long time train future want add entry corpus say b retrain b take longer time way train b merge expend model make work new entity thanks
Sample size for Named Entity Recognition gold standard corpus,"<p>I have a corpus of 170 Dutch literary novels on which I will apply Named Entity Recognition. For an evaluation of existing NER taggers for Dutch I want to manually annotate Named Entities in a random sample of this corpus – I use <a href=""http://brat.nlplab.org/"" rel=""nofollow noreferrer"">brat</a> for this purpose. The manually annotated random sample will function as the 'gold standard' in my evaluation of the NER taggers. I wrote a Python script that outputs a random sample of my corpus on the sentence level. </p>

<p>My question is: what is the ideal size of the random sample in terms of the amount of sentences per novel? For now, I used a random 100 sentences per novel, but this leads to a pretty big random sample containing almost 21626 lines (which is a lot to manually annotate, and which leads to a slow working environment in brat). </p>
",Named Entity Recognition (NER),sample size named entity recognition gold standard corpus corpus dutch literary novel apply named entity recognition evaluation existing ner tagger dutch want manually annotate named entity random sample corpus use brat purpose manually annotated random sample function gold standard evaluation ner tagger wrote python script output random sample corpus sentence level question ideal size random sample term amount sentence per novel used random sentence per novel lead pretty big random sample containing almost line lot manually annotate lead slow working environment brat
How to give RegexNER mapping file as input in the stanford server,"<p>I found the info about running RegexNER from <a href=""http://nlp.stanford.edu/software/regexner.html"" rel=""nofollow"">http://nlp.stanford.edu/software/regexner.html</a></p>

<p>Also I worked through the sample and following cmd worked fine for me.</p>

<p>java -mx1g -cp '*' edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators 'tokenize,ssplit,pos,lemma,ner,regexner' -file JuliaGillard.txt -regexner.mapping jg-regexner.txt</p>

<p>I looked at setting up Stanford NLP server from <a href=""http://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/corenlp-server.html</a>.</p>

<p>and following cmd </p>

<p>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer </p>

<p>could start a nlp server at 9000 port.</p>

<p>But when I tried to run the server with -regexner.mapping jg-regexner.txt it dosnt work. Can someone help me with this.</p>
",Named Entity Recognition (NER),give regexner mapping file input stanford server found info running regexner also worked sample following cmd worked fine java mx g cp edu stanford nlp pipeline stanfordcorenlp annotator tokenize ssplit po lemma ner regexner file juliagillard txt regexner mapping jg regexner txt looked setting stanford nlp server following cmd java mx g cp edu stanford nlp pipeline stanfordcorenlpserver could start nlp server port tried run server regexner mapping jg regexner txt dosnt work someone help
How to align two GloVe models in text2vec?,"<p>Let's say I have trained two separate GloVe vector space models (using <code>text2vec</code> in <code>R</code>) based on two different corpora. There could be different reasons for doing so: the two base corpora may come from two different time periods, or two very different genres, for example. I would be interested in comparing the usage/meaning of words between these two corpora. If I simply concatenated the two corpora and their vocabularies, that would not work (the location in the vector space for word pairs with different usages would just be somewhere in the ""middle""). </p>

<p>My initial idea was to train just one model, but when preparing the texts, append a suffix (_x, _y) to each word (where x and y stand for the usage of word A in corpus x/y), as well as keep a separate copy of each corpus without the suffixes, so that the vocabulary of the final concatenated training corpus would consist of: A, A_x, A_y, B, B_x, B_y ... etc, e.g.:</p>

<pre><code>this is an example of corpus X
this be corpus Y yo
this_x is_x an_x example_x of_x corpus_x X_x
this_y be_y corpus_y Y_y yo_y
</code></pre>

<p>I figured the ""mean"" usages of A and B would serve as sort of ""coordinates"" of the space, and I could measure the distance between A_x and A_y in the same space. But then I realized since A_x and A_y never occur in the same context (due to the suffixation of all words, including the ones around them), this would probably distort the space and not work. I also know there is something called an orthogonal procrustes problem, which relates to aligning matrices, but I wouldn't know how to implement it for my case.</p>

<p><strong>What would be a reasonable way to fit two GloVe models (preferably in <code>R</code> and so that they work with <code>text2vec</code>) into a common vector space, if my final goal is to measure the cosine similarity of word pairs, which are orthographically identical, but occur in two different corpora?</strong></p>
",Named Entity Recognition (NER),align two glove model text vec let say trained two separate glove vector space model using based two different corpus could different reason two base corpus may come two different time period two different genre example would interested comparing usage meaning word two corpus simply concatenated two corpus vocabulary would work location vector space word pair different usage would somewhere middle initial idea wa train one model preparing text append suffix x word x stand usage word corpus x well keep separate copy corpus without suffix vocabulary final concatenated training corpus would consist x b b x b etc e g figured mean usage b would serve sort coordinate space could measure distance x space realized since x never occur context due suffixation word including one around would probably distort space work also know something called procrustes problem relates aligning matrix know implement case would reasonable way fit two glove model preferably work common vector space final goal measure cosine similarity word pair orthographically identical occur two different corpus
How to extract Named Entity + Verb from text,"<p>Well, my aim is to extract NE (Person) and a verb connected to it from a text. For example, I have this text:</p>

<blockquote>
  <p>Dumbledore turned and walked back down the street. Harry Potter rolled over inside his blankets without waking up. </p>
</blockquote>

<p>As an ideal result i should get </p>

<blockquote>
  <p>Dumbledore turned walked; Harry Potter rolled</p>
</blockquote>

<p>I use Stanford NER to find and mark persons, then I delete all sentences that don't contain NE. So, in the end I have a 'pure' text that consists only of sentences with names of characters.
After that I use Stanford Dependencies. As the result I get smth like this (CONLLU output-format):</p>

<pre><code>1   Dumbledore  _   _   NN  _   2   nsubj   _   _
2   turned  _   _   VBD _   0   root    _   _
3   and _   _   CC  _   2   cc  _   _
4   walked  _   _   VBD _   2   conj    _   _
5   back    _   _   RB  _   4   advmod  _   _
6   down    _   _   IN  _   8   case    _   _
7   the _   _   DT  _   8   det _   _
8   street  _   _   NN  _   4   nmod    _   _
9   .   _   _   .   _   2   punct   _   _

1   Harry   _   _   NNP _   2   compound    _   _
2   Potter  _   _   NNP _   3   nsubj   _   _
3   rolled  _   _   VBD _   0   root    _   _
4   over    _   _   IN  _   3   compound:prt    _   _
5   inside  _   _   IN  _   7   case    _   _
6   his _   _   PRP$    _   7   nmod:poss   _   _
7   blankets    _   _   NNS _   3   nmod    _   _
8   without _   _   IN  _   9   mark    _   _
9   waking  _   _   VBG _   3   advcl   _   _
10  up  _   _   RP  _   9   compound:prt    _   _
11  .   _   _   .   _   3   punct   _   _
</code></pre>

<p>And that's where all my problems start. I know the person and the verb, but how to extract it from this format I have no idea.
I guess, i can do it this way: find NN/NNP in the table, find its 'parent' and then extract all its 'child'-words. Theoretically it should work. Theoretically.</p>

<p>The question is if anyone can come up with any other idea how to get a person and its action from the text? Or if there any more rational way to do it?</p>

<p>I'll be very grateful for any help!</p>
",Named Entity Recognition (NER),extract named entity verb text well aim extract ne person verb connected text example text dumbledore turned walked back street harry potter rolled inside blanket without waking ideal result get dumbledore turned walked harry potter rolled use stanford ner find mark person delete sentence contain ne end pure text consists sentence name character use stanford dependency result get smth like conllu output format problem start know person verb extract format idea guess way find nn nnp table find parent extract child word theoretically work theoretically question anyone come idea get person action text rational way grateful help
How to use an external corpus in nltk,"<p>I need to make a named entity recognition program for college. I'm using nltk and python 2.7. The corpus I am using is Spanish that nltk has. There is a specific method tokenize this corpus in nltk and I did the whole program with this form of tokenization, but I want to use an external corpus now and wanted tokenize it in the same way, but I do not know how.</p>

<p>How nltk's method was tokenizing data:</p>

<blockquote>
  <p>[(u'Melbourne', u'NP', u'B-LOC'), (u'(', u'Fpa', u'O'), (u'Australia',
  u'NP', u'B-LOC'), (u')', u'Fpt', u'O'), (u',', u'Fc', u'O'), (u'25',
  u'Z', u'O'), (u'may', u'NC', u'O'), (u'(', u'Fpa', u'O'), (u'EFE',
  u'NC', u'B-ORG'), (u')', u'Fpt', u'O'), (u'.', u'Fp', u'O')]</p>
</blockquote>

<p>A list for each sentence and each row is a tuple</p>

<p>And how i do it:</p>

<blockquote>
  <p>[u'Melbourne', u'NP', u'B-LOC', u'(', u'Fpa', u'O', u'Australia',
  u'NP', u'B-LOC', u')', u'Fpt', u'O', u',', u'Fc', u'O', u'25', u'Z',
  u'O', u'may', u'NC', u'O', u'(', u'Fpa', u'O', u'EFE', u'NC',
  u'B-ORG', u')', u'Fpt', u'O', u'.']</p>
</blockquote>

<p>There is no tuple</p>

<p>The code I used:</p>

<pre><code>f = io.open(train_corpus, 'r', encoding='utf8')
raw = f.read()
#my method
tokens = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(raw)]
train_sents = nltk.Text(tokens)
#nltk's method
train = list(nltk.corpus.conll2002.iob_sents('esp.train'))
#print results of two forms
print train_sents[0]
print 
print train[0]
</code></pre>

<p>Then I would like to know how tokenizer the program to stay the same as the tokenization method done by nltk for that corpus in another corpus.</p>
",Named Entity Recognition (NER),use external corpus nltk need make named entity recognition program college using nltk python corpus using spanish nltk ha specific method tokenize corpus nltk whole program form tokenization want use external corpus wanted tokenize way know nltk method wa tokenizing data u melbourne u np u b loc u u fpa u u australia u np u b loc u u fpt u u u fc u u u z u u may u nc u u u fpa u u efe u nc u b org u u fpt u u u fp u list sentence row tuple u melbourne u np u b loc u u fpa u u australia u np u b loc u u fpt u u u fc u u u z u u may u nc u u u fpa u u efe u nc u b org u u fpt u u tuple code used would like know tokenizer program stay tokenization method done nltk corpus another corpus
Coding Matrix with overlap counts in R,"<p>I am proficient in Python but a complete novice in R. I can't find an answer to this question elsewhere online, and whilst it's going to be a bit lengthy, I am hoping it will be useful to other users of the R library <a href=""http://rqda.r-forge.r-project.org"" rel=""nofollow noreferrer"">RQDA</a>.</p>

<p>Essentially, RQDA is a qualitative research tool, that is primarily used for assigning codes (themes) to text files. It's a bit like a highlighter pen that counts where it has highlighted.</p>

<p>If you put in a lot of files, you can code the text in different places with themes (e.g. a project about interviewing people working in cloth manufacturing might be ""equipment"", ""sewing"", ""linen"", ""silk"", ""lighting"", ""lunch breaks"", etc). This enables you to count how many times different codes were used, and in RQDA it gives a table output as follows:</p>

<pre><code>rowid   cid fid codenamefilename    index1  index2  CodingLength
1   1   12  1   silk    2010-01-28  409     939     530
2   2   21  1   cotton  2010-01-28  1008    1172    164
3   3   12  1   silk    2010-01-28  1173    1924    751
4   4   39  1   sewing  2010-01-28  1008    1250    751
5   5   38  1   weaving 2010-01-28  1173    1924    751
6   6   78  1   costs   2010-01-28  727     939     212
7   7   23  1   lunch   2010-01-28  1553    1788    235
8   9   7   2   lunch   2010-01-29  1001    1230    371
9   10  4   2   weaving 2010-01-29  1547    1724    135
10  11  6   2   social  2010-01-29  1001    1290    350
11  12  7   2   silk    2010-01-29  1926    2276    350
12  14  17  2   supply  2010-01-29  1926    2276    350
13  15  78  2   costs   2010-01-29  1926    2276    350
14  17  78  2   weaving 2010-01-29  1890    2106    212
</code></pre>

<p>codename = code the text was given (theme)</p>

<p>filename = filename of text (in this case, date of diary entry)</p>

<p>index1 = character position in file where code starts (highlighted text)</p>

<p>index2 = character position in file where code ends (highlighted text)</p>

<p>CodingLength = overall length of coded/highlighted text</p>

<p>What I'd like to do is to iterate over the entire table (around 1,500 rows) with the total list of codes (codename in the table above, around 100 unique codes) in order to output a 2-way matrix of overlap between codes, for example (indicative only, with 5 codes):</p>

<pre><code>    silk    cotton  sewing  weaving lunch breaks    socialising
silk    *     0      0       3       2              0
cotton  0     *      5       0       0              0
sewing  0     5      *       0       0              0
weaving 3     0      0       *       0              0
lunchs  2     0      0       0       *              5
socialg 0     0      0       0       5              *
</code></pre>

<p>(Code messed up a bit on this output but hopefully you get the idea)</p>

<p>Therefore, in R I need a bit of code that will iterate over the code list and count the number of instances where A) filename is the same and B) there is overlap in the range between index1 and index2 (CodingLength probably not important).</p>

<p>Apart from the following vague hunches I am lost as to exactly how to make this work:</p>

<ol>
<li><p>I probably need to asign the table as a variable e.g:</p>

<p>coding_table &lt;- getCodingTable()</p></li>
<li><p>I probably need to make a list of the unique variables e.g:</p>

<p>x = c(""silk"",""cotton"",""weaving"",""sewing"",""lunch"" ... etc. )</p></li>
<li><p>I need a function that does the checks</p></li>
<li>I need a for-loop for the rows</li>
<li>I need a boolean test where the range and file name is checked e.g. any(409:939 %in% 727:939) &amp;&amp; filename == filename</li>
</ol>

<p>Based on this, can anyone see a way to produce a very short solution to this? I feel like the equivalent in python would be 10 lines maximum, but given the extra bits required in R I am completely lost as to how to do this.</p>
",Named Entity Recognition (NER),coding matrix overlap count r proficient python complete novice r find answer question elsewhere online whilst going bit lengthy hoping useful user r library rqda essentially rqda qualitative research tool primarily used assigning code theme text file bit like highlighter pen count ha highlighted put lot file code text different place theme e g project interviewing people working cloth manufacturing might equipment sewing linen silk lighting lunch break etc enables count many time different code used rqda give table output follows codename code text wa given theme filename filename text case date diary entry index character position file code start highlighted text index character position file code end highlighted text codinglength overall length coded highlighted text like iterate entire table around row total list code codename table around unique code order output way matrix overlap code example indicative code code messed bit output hopefully get idea therefore r need bit code iterate code list count number instance filename b overlap range index index codinglength probably important apart following vague hunch lost exactly make work probably need asign table variable e g coding table getcodingtable probably need make list unique variable e g x c silk cotton weaving sewing lunch etc need function doe check need loop row need boolean test range file name checked e g filename filename based anyone see way produce short solution feel like equivalent python would line maximum given extra bit required r completely lost
Extracting location from text string,"<p>I have a problem that may seem trivial on the surface, but I am not sure how to go about solving this problem.</p>

<p>I have a file of 1million lines, each line is a description. In each description, I want to extract the location(City and state if exist). Some locations may be misspelled or incomplete. My goal is to be as accurate as possible.</p>

<p>Here is an example string from my file.</p>

<pre><code>wendys-alexandria q25,ALEXANDRIA,,US
wendys-alexandria q25   alexandria   ky,ALEXANDRIA,KY,US
wendys-alexandria q25 alexandria    ky,ALEXANDRIA,KY,US
wendys-alexandria q25 alexandria   ky,ALEXANDRIA,KY,US
wendys-alexandria q25 alexandria ky,ALEXANDRIA,KY,US
wendys-altoona#,ALTOONA,,US
wendys-altoona#       altoona      pa,ALTOONA,PA,US
wendys-altoona#     altoona      pa,ALTOONA,PA,US
wendys-altoona# 000 altoona,ALTOONA,,US
wendys-altoona# 0altoona,ALTOONA,,US
wendys-altoona# altoona      pa,ALTOONA,PA,US
wendys-altoona# altoona pa see all tags,ALTOONA,,US
</code></pre>

<p>I understand that this problem is Name entity recognization, and I should use ""NLP techniques"" to solve it. I'm thinking of using Python+Pandas+NLP library.</p>

<p>But if anyone has done similar task before, and can point out a direction for me like which NLP libraries to use, I would greatly appreciate it.</p>

<p>Thanks</p>
",Named Entity Recognition (NER),extracting location text string problem may seem trivial surface sure go solving problem file million line line description description want extract location city state exist location may misspelled incomplete goal accurate possible example string file understand problem name entity recognization use nlp technique solve thinking using python panda nlp library anyone ha done similar task point direction like nlp library use would greatly appreciate thanks
Recognize and get city from an unstructured text,"<p>I have sample texts as below numbered :</p>

<pre><code>1)';Roy\'\'s Chalet, sears road,Green woods;Street avenue;Arlington;Texas;United States;'
2)';PLOT NO. A-10, Seras -1, Green woods woods;PARK,;Arlington;Texas;United States;'
3)';Seras tampon woods avenue park green AS IN;Wallet Hall;St Ann ROAD Arlington Texas;United States;'.
</code></pre>

<p>I need a way to recognize city ""Arlington"" from each text.
There are million of records with such texts and the cities embedded in text . What is the best way to achieve this in java.</p>

<p>Would nlp like technique help here ?.</p>
",Named Entity Recognition (NER),recognize get city unstructured text sample text numbered need way recognize city arlington text million record text city embedded text best way achieve java would nlp like technique help
How to extract specific information from sentences using NLTK,"<p>I am new using Python and NLTK for NLP operations. Starting with different sentences I was wondering how I can extract certain dependent relations within a sentence. </p>

<p>For example:
<em>Edward has a black jacket and white shoes with red laces</em></p>

<p>Using POS tagging I can extract certain parts of speech, but I want to specifically extract that he has, for example, a black jacket to ultimately list the information like:</p>

<p><strong>Name:</strong> Edward</p>

<p><strong>Clothing:</strong> Black jacket</p>

<p><strong>Shoes:</strong> White shoes with red laces</p>
",Named Entity Recognition (NER),extract specific information sentence using nltk new using python nltk nlp operation starting different sentence wa wondering extract certain dependent relation within sentence example edward ha black jacket white shoe red lace using po tagging extract certain part speech want specifically extract ha example black jacket ultimately list information like name edward clothing black jacket shoe white shoe red lace
Problems while trying to generate pandas dataframe columns from regulars expressions?,"<p>I am working with a number of <code>.txt</code> files allocated in a directory. From all of these files, how should I extract specific words or chunks of text (i.e. sentences, paragraphs, and tokens defined by a regex) and place them into a pandas dataframe (i.e. tabular format), preserving a column with the name of each file? So far, I created this function that does this task (I know... it ain't perfect):</p>

<p>In:</p>

<pre><code>import glob, os, re
import pandas as pd
regex = r'\&lt;the regex&gt;\b'
ind = 'path/dir'
out = 'path/dir'
f ='path/redirected/output/'


def foo(ind, reg, out):
    for filename in glob.glob(os.path.join(in_directory, '*.txt')):
        with open(filename, 'r') as file:
            stuff = re.findall(a_regex, file.read(), re.M)
            #my_list = [str([j.split()[0] for j in i]) for i in stuff]

            lis = [t[::2] for t in stuff]
            cont = ' '.join(map(str, lis))
            print(cont)
            with open(out, 'a') as f:
                print(filename.split('/')[-1] + '\t' + cont, file = f)


foo(directory, regex, out)
</code></pre>

<p>Then the output is redirected to third file:</p>

<p>Out:</p>

<pre><code>fileName1.txt       
fileName2.txt       stringOrChunk stringOrChunk stringOrChunk stringOrChunk stringOrChunk stringOrChunk stringOrChunk stringOrChunk stringOrChunk stringOrChunk stringOrChunk
fileName3.txt       stringOrChunk stringOrChunk stringOrChunk stringOrChunk stringOrChunk stringOrChunk stringOrChunk stringOrChunk
....
fileNameN.txt       stringOrChunk
</code></pre>

<p>Then this is how I create the dataframe from the previous file (yeah I know its awful):</p>

<pre><code>import pandas as pd
df = pd.read_csv(/path/of/f/, sep='\t', names = ['file_names','col1'])
df.to_csv('/pathOfNewCSV.csv', index=False, sep='\t')
</code></pre>

<p>And Finally:</p>

<pre><code>    file_names  col1
0   fileName1.txt   NaN
1   fileName2.txt   stringOrChunk stringOrChunk stringOrChunk...
2   fileName3.txt   stringOrChunk stringOrChunk stringOrChunk...
3   fileName4.txt   stringOrChunk
.....
N   fileNameN.txt   stringOrChunk
</code></pre>

<p>So, any idea of how to do this in a more pythonic and efficient way?</p>

<p><strong>Update</strong></p>

<p>I uploaded a .zip with some docs as <a href=""https://ufile.io/22e6"" rel=""nofollow"">data</a>, so if we want to extract all the adverbs from the documents we should do:</p>

<pre><code>a_regex = r""\w+ly""
directory = '/Users/user/Desktop/Docs/'
output_dir = '/Users/user/Desktop/'

foo(ind, reg, out)
</code></pre>

<p>Then, it should create a table with all the adverbs of the documents:</p>

<pre><code>Files            words
doc1.txt    
doc2.txt    
doc3.txt     DIRECTLY PROBABLY EARLY 
doc4.txt    
</code></pre>

<p>Any idea of how to enhance the above function? Additionally, I don't know if this is the best way to do this <a href=""https://en.wikipedia.org/wiki/Information_extraction"" rel=""nofollow"">information extraction task</a> (i.e. just using regex). What about using an string indexer like <a href=""https://whoosh.readthedocs.io/en/latest/quickstart.html#a-quick-introduction"" rel=""nofollow"">woosh</a> project or what about nltk?</p>

<p><strong>UPDATE</strong></p>

<p>For example, consider creating a <a href=""http://pastebin.com/RTQ7BAKT"" rel=""nofollow"">dataframe</a> that extracts all the sentences that contain the word: <code>JESUITS</code>:</p>

<pre><code>    Files   words1  words2  words3  words4
0   doc1.txt    A GOVERNMENT SPOKESMAN HAS ANNOUNCED THAT WITH...   NaN     NaN     NaN
1   doc2.txt    11/12/98 ""THERE WAS NO TORTURE OR MISTREATMENT...   NaN     NaN     NaN
2   doc3.txt    WHAT WE HAD PREDICTED HAS OCCURRED. CRISTIANI ...   SO, THE QUESTION IS: WHO GAVE THE ORDER TO KIL...   THE MASSACRE OF THE JESUITS WAS NOT A PERSONAL...   LET US REMEMBER THAT AFTER THE MASSSACRE OF TH...
3   doc4.txt    IN 11/12/98 OUR VIEW, THE ASSASSINS OF THE JES...   THE ASSASSINATION OF THE JESUITS AGAIN CONFIRM...   NaN     NaN
</code></pre>
",Named Entity Recognition (NER),problem trying generate panda dataframe column regular expression working number file allocated directory file extract specific word chunk text e sentence paragraph token defined regex place panda dataframe e tabular format preserving column name file far created function doe task know perfect output redirected third file create dataframe previous file yeah know awful finally idea pythonic efficient way update uploaded zip doc data want extract adverb document create table adverb document idea enhance function additionally know best way information extraction task e using regex using string indexer like woosh project nltk update example consider creating dataframe extract sentence contain word
Get dbpedia link of entity using stanford NER,"<p>I am trying to find entities from text using <code>stanford NER</code>. It is working fine so far. Now I want to find the <code>dbpedia</code> link of the entities. 
I have seen it is available in <code>alchemy API</code>. 
Is it possible to find the <code>dbpedia</code> links of entities using stanford NER? </p>
",Named Entity Recognition (NER),get dbpedia link entity using stanford ner trying find entity text using working fine far want find link entity seen available possible find link entity using stanford ner
Named Entity Recognition with Syntaxnet,"<p>I am trying to understand and learn SyntaxNet. I am trying to figure out whether is there any way to use SyntaxNet for Name Entity Recognition of a corpus. Any sample code or helpful links would be appreciated. </p>
",Named Entity Recognition (NER),named entity recognition syntaxnet trying understand learn syntaxnet trying figure whether way use syntaxnet name entity recognition corpus sample code helpful link would appreciated
Training n-gram NER with Stanford NLP,"<p>Recently I have been trying to train n-gram entities with Stanford Core NLP. I have followed the following tutorials - <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#b"" rel=""noreferrer"">http://nlp.stanford.edu/software/crf-faq.shtml#b</a></p>

<p>With this, I am able to specify only unigram tokens and the class it belongs to. Can any one guide me through so that I can extend it to n-grams. I am trying to extract known entities like movie names from chat data set.  </p>

<p>Please guide me through in case I have mis-interpretted the Stanford Tutorials and the same can be used for the n-gram training. </p>

<p>What I am stuck with is the following property</p>

<pre><code>#structure of your training file; this tells the classifier
#that the word is in column 0 and the correct answer is in
#column 1
map = word=0,answer=1
</code></pre>

<p>Here the first column is the word (unigram) and the second column is the entity, for example </p>

<pre><code>CHAPTER O
I   O
Emma    PERS
Woodhouse   PERS
</code></pre>

<p>Now that I need to train known entities (say movie names) like <strong>Hulk</strong>, <strong>Titanic</strong> etc as movies, it would be easy with this approach. But in case I need to train <strong>I know what you did last summer</strong> or <strong>Baby's day out</strong>, what is the best approach ? </p>
",Named Entity Recognition (NER),training n gram ner stanford nlp recently trying train n gram entity stanford core nlp followed following tutorial able specify unigram token class belongs one guide extend n gram trying extract known entity like movie name chat data set please guide case mi interpretted stanford tutorial used n gram training stuck following property first column word unigram second column entity example need train known entity say movie name like hulk titanic etc movie would easy approach case need train know last summer baby day best approach
Hit Location in SOLR When Indexing a Large Text,"<p>We are creating a SOLR index of a corpus of digital books that are marked up in XML. The index is to be used for NLP analysis of the corpus as well as locating passages within the corpus. I am processing each text into a JSON solr doc for indexing and posting that to the index. </p>

<p>The XML of each book includes markers for all the page breaks in the text, each marked with a ""milestone"" tag. I have created the initial version of the index by stripping out all the milestones from the body of the text and dumping the text of whole book into a single field so that strings or passages that go over a page break can be found. </p>

<p>My question is: How can I determine the location of hits in a text (i.e. what page they are on) when a search is made? </p>
",Named Entity Recognition (NER),hit location solr indexing large text creating solr index corpus digital book marked xml index used nlp analysis corpus well locating passage within corpus processing text json solr doc indexing posting index xml book includes marker page break text marked milestone tag created initial version index stripping milestone body text dumping text whole book single field string passage go page break found question determine location hit text e page search made
Custom NER model - FAIL,"<p>I am new to the NLP scene and am using <code>OpenNLP 1.5</code> for getting started.</p>

<p>I went through some the commands given in the documentation here:
<a href=""https://opennlp.apache.org/documentation/manual/opennlp.html"" rel=""nofollow"">https://opennlp.apache.org/documentation/manual/opennlp.html</a> <br>
(I am using the command line interface to get started)</p>

<p>I used the already available sample models for experimenting with the different tools and finally decided to create a <i>custom NER model</i>.</p>

<p>I followed the instruction given in the aforementioned link.</p>

<p>Copied the sample sentences given into a <code>.train</code> file (I simply created a new file with that extension and pasted the contents into it):</p>

<pre><code>&lt;START:person&gt; Pierre Vinken &lt;END&gt; , 61 years old , will join the board as a nonexecutive director Nov. 29 .
Mr . &lt;START:person&gt; Vinken &lt;END&gt; is chairman of Elsevier N.V. , the Dutch publishing group .
</code></pre>

<p>I used the following command to make the model:</p>

<pre><code>bin/opennlp TokenNameFinderTrainer -model en-ner-person2.bin -lang en -data en-ner-person2.train -encoding UTF-8
</code></pre>

<p>The problem is that even though the model is getting created, it seems to be not working properly. Tested this by using the newly created model:
<code>bin/opennlp TokenNameFinder en-ner-person2.bin</code></p>

<p>But when I input <code>Pierre Vinken</code>, it's not getting recognised as a person. I also tried creating the model from a <code>.txt</code> file with the exact same content, but that too failed.</p>

<p>What am I doing wrong?</p>

<p>TIA.</p>
",Named Entity Recognition (NER),custom ner model fail new nlp scene using getting started went command given documentation using command line interface get started used already available sample model experimenting different tool finally decided create custom ner model followed instruction given aforementioned link copied sample sentence given file simply created new file extension pasted content used following command make model problem even though model getting created seems working properly tested using newly created model input getting recognised person also tried creating model file exact content failed wrong tia
custom named entity extraction,"<p>I'm trying to implement NER(Named Entity Extraction) using stanford NLP. 
final goal is to convert free text to query format.
I created a custom dictionary and am able to extract entities and build query</p>

<pre><code>people who are from newyork
</code></pre>

<p>I'll build query</p>

<pre><code>     select * from people where region = 'newyork'
</code></pre>

<p>but the issue comes when the statement is negated </p>

<p>people who are not from newyork</p>

<p>How to extract negative scenario from this statement, Is there any way possible even outside of stanford NLP</p>

<p>Any help is appreciated</p>
",Named Entity Recognition (NER),custom named entity extraction trying implement ner named entity extraction using stanford nlp final goal convert free text query format created custom dictionary able extract entity build query build query issue come statement negated people newyork extract negative scenario statement way possible even outside stanford nlp help appreciated
Training NER model to parse temporal and location expressionsions,"<p>Folks, does anybody has thoughts on building NER models for labeling text sequences like addresses or temporal expressions?</p>

<p>There is a parser for temporal expressions like ""last five days"" called SUTime: <a href=""http://nlp.stanford.edu/software/sutime.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/sutime.shtml</a>. Unfortunately, it's buggy and built as massive mash of rules. </p>

<p>Parsing addresses is even more difficult and error prone. CoreNLP parser fails to parse even simple things like Mountain View, CA.</p>

<p>I feel that there should be a way to train RNN to recognize these patterns without maintaining a giant list of rules or a giant lookup table.</p>
",Named Entity Recognition (NER),training ner model parse temporal location expressionsions folk doe anybody ha thought building ner model labeling text sequence like address temporal expression parser temporal expression like last five day called sutime unfortunately buggy built massive mash rule parsing address even difficult error prone corenlp parser fails parse even simple thing like mountain view ca feel way train rnn recognize pattern without maintaining giant list rule giant lookup table
Imbalance corpus using label propagation for classification,"<p>I am trying to do user location classification with <strong>semi-supervised</strong> learning.</p>

<p>The dataset is used to do user location classification in tweet, the mixture of all messages posted by a user is considered as one sample of this user. </p>

<p>The number of samples vary 8 categories (locations) is <code>24</code>, <code>233</code>, <code>780</code>, <code>804</code>, <code>144</code>, <code>91</code>, <code>503</code>, <code>243</code> as shown in <strong>confusion matrix</strong>. </p>

<p>I want to <strong>predict</strong> the user's location via these messages. </p>

<p>I assume that some labeled data are unlabeled, which labels are set as <code>-1</code>. Using Label propagation in <code>scikit-learning</code> leads to that all unlabeled data are predicted as the label <code>3</code> that the amount of the samples is the most.</p>

<p><strong>Question:</strong> Which method should I use to handle this imbalance semi-supervised texts classification?</p>

<p>Here is the full snippet of my code:</p>

<pre><code>from document import readfile
trainDocs,trainLabels,nu1 = readfile(r'train', 700)
valDocs,valLabels,nu2 = readfile(r'valid', 100)
testDocs,testLabels,nu3 = readfile(r'test', 200)
#Valids,nu4,userIds = readfile(r'classifiedValid',1200)

from sklearn.feature_extraction.text import CountVectorizer
#count_vect = CountVectorizer(stop_words=""english"",decode_error='ignore')
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(valDocs+trainDocs+testDocs)
#print (X_train_counts.shape)
#print (X_train_counts)

from sklearn.feature_extraction.text import TfidfTransformer

#tf_transformer = TfidfTransformer(use_idf = False).fit(X_train_counts)
#X_train_tf = tf_transformer.transform(X_train_counts)
#print (X_train_tf.shape)
#print (X_train_tf)
#
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
X_train_tfidf = X_train_tfidf.toarray()
#X_train_tfidf.shape

print (X_train_tfidf.shape)
#print (X_train_tfidf)

#X_test_counts = count_vect.transform(Valids)
#X_test_tfidf = tfidf_transformer.transform(X_test_counts)
#X_test_tfidf = X_test_tfidf.toarray()

#from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.semi_supervised import label_propagation

labels = np.array(valLabels+trainLabels+testLabels)
unlabeled_set = np.arange(len(trainLabels+testLabels))
#testLabels2[unlabeled_set] = -1
unlabeled_set2 = np.arange(len(trainLabels+testLabels))
unlabeled_set2[unlabeled_set] += len(valLabels)
labels[unlabeled_set2] = -1

print 'start'
#clf = SVC(C=1.0, gamma=0.10000000000000001).fit(X_train_tfidf,np.array(trainLabels+valLabels))
clf = label_propagation.LabelSpreading(gamma = 0.25, max_iter=100).fit(X_train_tfidf,labels)
predicted_labels = clf.transduction_[unlabeled_set2]
print (np.mean(predicted_labels == np.array(trainLabels+testLabels)))
from sklearn import metrics
print(metrics.classification_report(np.array(trainLabels+testLabels),predicted_labels))

cm = metrics.confusion_matrix(np.array(trainLabels+testLabels), predicted_labels,
                          labels=clf.classes_)
print(""Confusion matrix"")
print(cm)

precision    recall  f1-score   support

      0       0.00      0.00      0.00        24
      1       0.00      0.00      0.00       233
      2       0.00      0.00      0.00       780
      3       0.28      1.00      0.44       804
      4       0.00      0.00      0.00       144
      5       0.00      0.00      0.00        91
      6       0.00      0.00      0.00       503
      7       0.00      0.00      0.00       243
avg / total 0.08 0.28 0.13 2822

Confusion matrix
[[  0   0   0  24   0   0   0   0]
 [  0   0   0 233   0   0   0   0]
 [  0   0   0 780   0   0   0   0]
 [  0   0   0 804   0   0   0   0]
 [  0   0   0 144   0   0   0   0]
 [  0   0   0  91   0   0   0   0]
 [  0   0   0 503   0   0   0   0]
 [  0   0   0 243   0   0   0   0]]
</code></pre>
",Named Entity Recognition (NER),imbalance corpus using label propagation classification trying user location classification semi supervised learning dataset used user location classification tweet mixture message posted user considered one sample user number sample vary category location shown confusion matrix want predict user location via message assume labeled data unlabeled label set using label propagation lead unlabeled data predicted label amount sample question method use handle imbalance semi supervised text classification full snippet code
Customizing the Named Entity Recogntition model in Azure ML,"<p>Can we customize the Named Entity Recognition (NER) model in Azure ML Studio with a separate training dataset? What I want to do is to find out non-English names from a text. (Training dataset includes the set of names that going to use for training)</p>
",Named Entity Recognition (NER),customizing named entity recogntition model azure ml customize named entity recognition ner model azure ml studio separate training dataset want find non english name text training dataset includes set name going use training
Deep learning and text analysis / extraction,"<p>i am trying to  build a model based on deep learning to extract specific text from long sentences.</p>

<p>Let's suppose a text of 200 words, and a table where i have my client name and surname. I am trying to build a model to extract from these 200 words the specific client name/surname using deep learning.</p>

<p>I've read about CNN and RNTN models, semantic parsing and word2vec models, but clearly i am not a pro in that field.</p>

<p>My thoughts are :</p>

<ul>
<li><strong>step 1</strong> : make a 1st model where input = client surname , output =
class surname</li>
<li><strong>step 2</strong> : make a 2nd model where input = client name ,
   output = class name</li>
<li><strong>step 3</strong> : make a 3rd model where input = client
   name + surname and surname + name, output = class client</li>
<li><strong>step 4</strong> :
   make a 4th model where i send bag of words in input and find a way
   to find the client class in output.</li>
</ul>

<p>The same way we can find noun/adverbs/verbs/ ... we should be able to create a sort of new ""semantic sort"" as client, address, ....</p>

<p>Can anyone give me some advices about my way of thinking ? or tell me what part i should change / improve ?</p>

<p>Thanks a lot.</p>
",Named Entity Recognition (NER),deep learning text analysis extraction trying build model based deep learning extract specific text long sentence let suppose text word table client name surname trying build model extract word specific client name surname using deep learning read cnn rntn model semantic parsing word vec model clearly pro field thought step make st model input client surname output class surname step make nd model input client name output class name step make rd model input client name surname surname name output class client step make th model send bag word input find way find client class output way find noun adverb verb able create sort new semantic sort client address anyone give advice way thinking tell part change improve thanks lot
Detecting content based on position in sentence with OpenNLP,"<p>I've successfully used OpenNLP for document categorization and also was able to extract names from trained samples and using regular expressions.</p>

<p>I was wondering if it is also possible to extract names (or more generally speaking, subjects) based on their <em>position</em> in a sentence?</p>

<p>E.g. instead of training with concrete names that are know a priori, like <code>Travel to &lt;START:location&gt; New York &lt;/START&gt;</code>, I would prefer not to provide concrete examples but let OpenNLP decide that anything appearing at the specified position could be an entity. That way, I wouldn't have to provide each and every possible option (which is impossible in my case anyway) but only provide one for the possible surrounding sentence.</p>
",Named Entity Recognition (NER),detecting content based position sentence opennlp successfully used opennlp document categorization also wa able extract name trained sample using regular expression wa wondering also possible extract name generally speaking subject based position sentence e g instead training concrete name know priori like would prefer provide concrete example let opennlp decide anything appearing specified position could entity way provide every possible option impossible case anyway provide one possible surrounding sentence
Stanford NER precedence,"<p>I'm using Stanford NER for training, I have 2 questions!</p>

<p>1)How do i give precedence to a particular class label.
for example if a ""word"" is trained as <strong>Class A</strong> and the same ""word"" is <strong>Class B</strong>, and when ""word"" is found in the text I want it to overcome the conflict and be labelled as <strong>Class B</strong> only, irrespective of how many occurrence ""word"" has when trained for <strong>Class A</strong>. </p>

<p>2)How to train a token with two words, for example ""Mysore"" is a <strong>City</strong> and ""Mysore Road"" is a <strong>Road</strong>. But in Stanford NER i need to train both ""Mysore"" and ""Road"" separately as <strong>Road</strong>, so there is a conflict for ""Mysore"" to be <strong>City</strong> or <strong>Road</strong>. </p>
",Named Entity Recognition (NER),stanford ner precedence using stanford ner training question give precedence particular class label example word trained class word class b word found text want overcome conflict labelled class b irrespective many occurrence word ha trained class train token two word example mysore city mysore road road stanford ner need train mysore road separately road conflict mysore city road
Free Chinese Named Entity Datasets or Free Chinese NER System,"<p>I want to use Stanford NER system as a NLP tool in my work. However it lacks a classifier model for Chinese Named Entity. I must train the model by myself. I did not find any free datasets that can be used to train classifier through Google.</p>

<p>Who could tell me some free Chinese Named Entity Labeled datasets or free Chinese NER system? Any help will be appreciated:)</p>
",Named Entity Recognition (NER),free chinese named entity datasets free chinese ner system want use stanford ner system nlp tool work however lack classifier model chinese named entity must train model find free datasets used train classifier google could tell free chinese named entity labeled datasets free chinese ner system help appreciated
How to set whitespace tokenizer on NER Model?,"<p>i am creating a custom NER model using CoreNLP 3.6.0</p>

<p>My props are:</p>

<pre><code># location of the training file 
trainFile = /home/damiano/stanford-ner.tsv 
# location where you would like to save (serialize) your 
# classifier; adding .gz at the end automatically gzips the file, 
# making it smaller, and faster to load 
serializeTo = ner-model.ser.gz

# structure of your training file; this tells the classifier that 
# the word is in column 0 and the correct answer is in column 1 
map = word=0,answer=1

# This specifies the order of the CRF: order 1 means that features 
# apply at most to a class pair of previous class and current class 
# or current class and next class. 
maxLeft=1

# these are the features we'd like to train with 
# some are discussed below, the rest can be 
# understood by looking at NERFeatureFactory 
useClassFeature=true 
useWord=true 
# word character ngrams will be included up to length 6 as prefixes 
# and suffixes only  
useNGrams=true 
noMidNGrams=true 
maxNGramLeng=6 
usePrev=true 
useNext=true 
useDisjunctive=true 
useSequences=true 
usePrevSequences=true 
# the last 4 properties deal with word shape features 
useTypeSeqs=true 
useTypeSeqs2=true 
useTypeySequences=true 
wordShape=chris2useLC
</code></pre>

<p>I build with this command:</p>

<pre><code>java -classpath ""stanford-ner.jar:lib/*"" edu.stanford.nlp.ie.crf.CRFClassifier  -prop /home/damiano/stanford-ner.prop
</code></pre>

<p>The problem is when i use this model to retrieve the entities inside a textfile. The command is:</p>

<pre><code>java -classpath ""stanford-ner.jar:lib/*"" edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier ner-model.ser.gz -textFile file.txt
</code></pre>

<p>Where <strong>file.txt</strong> is:</p>

<pre><code>Hello!
my
name
is
John.
</code></pre>

<p>The output is:</p>

<p>Hello/O !/O 
my/O name/O is/O John/PERSON ./O </p>

<p>As you can see it split ""Hello!"" into two tokens. Same thing for ""John.""</p>

<p>I must use whitespace tokenizer. </p>

<p>How can i set it? </p>

<p>why does CoreNlp is splitting those words in two tokens?</p>
",Named Entity Recognition (NER),set whitespace tokenizer ner model creating custom ner model using corenlp prop build command problem use model retrieve entity inside textfile command file txt output hello name john person see split hello two token thing john must use whitespace tokenizer set doe corenlp splitting word two token
Extract name entities from web domain address,"<p>I am working on an NLP problem (in Python 2.7) to extract the location of a news report from the text inside the report. For this task I am using the Clavin API which works well enough. </p>

<p>However I've noticed that the name of the location area is often mentioned in the URL of the report itself and I'd like to find a way to extract this entity from a domain name, to increase the level of accuracy from Clavin by providing an additional named entity in the request.</p>

<p>In an ideal world I'd like to be able to give this input:
<code>
www.britainnews.net
</code></p>

<p>and return this, or a similar, output:
<code>
[www,britain,news,net]
</code></p>

<p>Of course I can use .split() feature to separate the <code>www</code> and <code>net</code> tokens which are unimportant, however I'm stumped as to how to split the middle phrase without an intensive dictionary lookup.</p>

<p>I'm not asking for someone to solve this problem or write any code for me - but this is an open call for suggestions as to the ideal NLP library (if one exists) or any ideas as to how to solve this problem. </p>
",Named Entity Recognition (NER),extract name entity web domain address working nlp problem python extract location news report text inside report task using clavin api work well enough however noticed name location area often mentioned url report like find way extract entity domain name increase level accuracy clavin providing additional named entity request ideal world like able give input return similar output course use split feature separate token unimportant however stumped split middle phrase without intensive dictionary lookup asking someone solve problem write code open call suggestion ideal nlp library one exists idea solve problem
Case insensitive POS (Part of Speech) Tagger for SyntaxNet,"<p>I have tried, <code>Parsey McParseface</code>, the pre-trained POS tagger that comes with Syntax Net and it does a good job at tagging sentences that have proper capitalization.</p>

<p>I would like to tag sentences that are all <strong>lower case</strong>, like: <code>i grew up in toronto</code> and then parse it to identify <strong>named entities</strong> such as cities, in this case, <code>toronto</code>.</p>

<p>I have a couple of questions:</p>

<ul>
<li>Is there a pre-trained <strong>case insensitive</strong> POS tagger for SyntaxNet that I can use? </li>
<li>How should I go about training my own <strong>case insensitive</strong> POS tagger for SyntaxNet? </li>
<li>Does training the SyntaxNet POS tagger require substantial amount of CPU/GPU power or it can be done on regular servers I could rent on Amazon or similar services? </li>
<li>Is the data-set that google used to train <code>Parsey McParseface</code> available for public use?</li>
</ul>
",Named Entity Recognition (NER),case insensitive po part speech tagger syntaxnet tried pre trained po tagger come syntax net doe good job tagging sentence proper capitalization would like tag sentence lower case like parse identify named entity city case couple question pre trained case insensitive po tagger syntaxnet use go training case insensitive po tagger syntaxnet doe training syntaxnet po tagger require substantial amount cpu gpu power done regular server could rent amazon similar service data set google used train available public use
How to Find the Main Topic of a Body of Text,"<p>I know that in NLP it is a challenge to determine the topic of a sentence or possibly a paragraph. However, I am trying to determine what the title may be for something like a Wikipedia article (of course without using other methods). My only though is finding the most frequent words. For the article on New York City these were the top results:</p>

<pre><code>[('new', 429), ('city', 380), ('york', 361), (""'s"", 177), ('manhattan', 90), ('world', 84), ('united', 78), ('states', 74), ('===', 70), ('island', 68), ('largest', 66), ('park', 64), ('also', 56), ('area', 52), ('american', 49)]
</code></pre>

<p>From this I can see some sort of statistical significance is the sharp drop from 361 to 177. Regardless, I am neither a statistics or NLP expert (in fact I'm a complete noob at both) so <strong>is this a viable way of determining the topic of a longer body of text. If so, what math am I looking for to calculate this? If not is there some other way in NLP to determine the topic or title for a larger body of text?</strong> For reference, I am using nltk and Python 3. </p>
",Named Entity Recognition (NER),find main topic body text know nlp challenge determine topic sentence possibly paragraph however trying determine title may something like wikipedia article course without using method though finding frequent word article new york city top result see sort statistical significance sharp drop regardless neither statistic nlp expert fact complete noob viable way determining topic longer body text math looking calculate way nlp determine topic title larger body text reference using nltk python
Does NLTK have a tool for tagging for animate versus inanimate?,"<p>I am trying to tag a list of words for <a href=""https://en.wikipedia.org/wiki/Animacy"" rel=""nofollow"">animacy</a>. I've tried using SpaCy and Stanford Core NLP, but these seem only to tag for named entities. </p>

<p>I know that Stanford has an animacy dictionary, but I don't know how to use it and I can't find any good instructions. </p>

<p>Can anyone point me in the right direction?</p>
",Named Entity Recognition (NER),doe nltk tool tagging animate versus inanimate trying tag list word animacy tried using spacy stanford core nlp seem tag named entity know stanford ha animacy dictionary know use find good instruction anyone point right direction
"NER CRF, Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory","<p>I have downloaded the latest version for NER from this <a href=""http://nlp.stanford.edu/software/stanford-ner-2015-12-09.zip"" rel=""nofollow"">link</a>. Then after extracting it, I have run this command.</p>

<pre><code>java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop
</code></pre>

<p>This is not working and getting following exception.</p>

<pre><code>CRFClassifier invoked on Mon Jul 25 06:56:22 EDT 2016 with arguments:
   -prop austen.prop
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory
    at edu.stanford.nlp.io.IOUtils.&lt;clinit&gt;(IOUtils.java:42)
    at edu.stanford.nlp.util.StringUtils.argsToProperties(StringUtils.java:942)
    at edu.stanford.nlp.util.StringUtils.argsToProperties(StringUtils.java:891)
    at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:2994)
Caused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 4 more
</code></pre>

<p>In the folder, <strong>stanford-ner-2015-12-09</strong> there is another folder <strong>lib</strong>, <strong>it already contains slf4j libraries but still it is not executing above command.</strong> I just downloaded and then extracted files and run that command to make a model but this exception is coming. I will be thankful to you if you can help me.</p>
",Named Entity Recognition (NER),ner crf exception thread main java lang noclassdeffounderror org slf j loggerfactory downloaded latest version ner link extracting run command working getting following exception folder stanford ner another folder lib already contains slf j library still executing command downloaded extracted file run command make model exception coming thankful help
how to Create Name Entity Recognition and evaluate its performance in terms of precision and recall?,"<p>I am working on aspects identification (explicit and implicit aspects) from a movie review dataset. Here, aspects could be actors, directors, production companies, music, plot, movie type etc. Despite searching a lot, I have come to know I need a NER (name entity recognition) for movie review dataset. Unfortunately, there is no NER available for my dataset. </p>

<p>My Questions are:</p>

<ol>
<li>how can I evaluate my ""Movie NER"" in terms of <em>precision</em>, <em>recall</em> and F1 <em>measure</em>? </li>
<li>what tool should I use for this purpose?</li>
</ol>
",Named Entity Recognition (NER),create name entity recognition evaluate performance term precision recall working aspect identification explicit implicit aspect movie review dataset aspect could actor director production company music plot movie type etc despite searching lot come know need ner name entity recognition movie review dataset unfortunately ner available dataset question evaluate movie ner term precision recall f measure tool use purpose
Should I use LingPipe or NLTK for extracting names and places?,"<p>I'm looking to extract names and places from very short bursts of text example</p>

<pre>
 ""cardinals vs jays in toronto""
 "" Daniel Nestor and Nenad Zimonjic play Jonas Bjorkman w/ Kevin Ullyett, paris time to be announced""
""jenson button - pole position, brawn-mercedes - monaco"".
</pre>

<p>This data is currently in a MySQL database, and I (pretty much) have a separate record for each athlete, though names are sometimes spelled wrong, etc.</p>

<p>I would like to extract the athletes and locations.
I usually work in PHP, but haven't been able to find a library for entity extraction (and I may want to get deeper into some <a href=""http://en.wikipedia.org/wiki/Natural_language_processing"" rel=""nofollow noreferrer"">NLP</a> and <a href=""http://en.wikipedia.org/wiki/Machine_learning"" rel=""nofollow noreferrer"">ML</a> in the future). </p>

<p>From what I've found, <a href=""http://alias-i.com/lingpipe/"" rel=""nofollow noreferrer"">LingPipe</a> and <a href=""http://en.wikipedia.org/wiki/Natural_Language_Toolkit"" rel=""nofollow noreferrer"">NLTK</a> seem to be the most recommended, but I can't figure out if either will really suit my purpose, or if something else would be better.  </p>

<p>I haven't programmed in either Java or Python, so before I start learning new languages, I'm hoping to get some advice on what route I should follow, or other recommendations.</p>
",Named Entity Recognition (NER),use lingpipe nltk extracting name place looking extract name place short burst text example cardinal v jay toronto daniel nestor nenad zimonjic play jonas bjorkman w kevin ullyett paris time jenson button pole position brawn mercedes monaco data currently mysql database pretty much separate record athlete though name sometimes spelled wrong etc would like extract athlete location usually work php able find library entity extraction may want get deeper nlp ml future found lingpipe nltk seem recommended figure either really suit purpose something else would better programmed either java python start learning new language hoping get advice route follow recommendation
Different result in StanfordNERTagger in python3.5 - Stanford-ner-2015-12-09,"<p>I tried to run a sample sentence:</p>

<pre><code>from nltk.tag import StanfordNERTagger
_model_filename = r'D:/standford/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz'

_path_to_jar = r'D:/standford/stanford-ner-2015-12-09/stanford-ner.jar'

st = StanfordNERTagger(model_filename=_model_filename, path_to_jar=_path_to_jar)

st.tag('Rami Eid is studying at Stony Brook University in NY'.split()) 
</code></pre>

<p>My output was as below in python:</p>

<blockquote>
  <p>[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying',
  'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook',
  'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY',
  'O')]</p>
</blockquote>

<p>while I was expected NY also selected as location based on this <a href=""https://stackoverflow.com/questions/23861355/how-to-install-and-invoke-stanford-nertagger"">reference</a>.</p>

<p>I tried another example as below:</p>

<pre><code>st.tag('Ali is living in London.'.split())
</code></pre>

<p>the result was as below which was correct.</p>

<blockquote>
  <p>[('Ali', 'PERSON'), ('is', 'O'), ('living', 'O'), ('in', 'O'),
  ('London.', 'LOCATION')]</p>
</blockquote>

<p>Do you have any idea why it didn't recognize NY as location in first sentence?</p>

<p>I am using visual studio 2015, Python 3.5, Stanford-ner-2015-12-09</p>
",Named Entity Recognition (NER),different result stanfordnertagger python stanford ner tried run sample sentence output wa python ramus person eid person studying stony organization brook organization university organization ny wa expected ny also selected location based href p tried another example result wa wa correct ali person living london location idea recognize ny location first sentence using visual studio python stanford ner
The natural language sentences generation,"<p>I'm looking for a way how to generate or synthesize natural language sentences. I need to write about 1000 sentences like following: </p>

<p>""Hi there! I would like to go for a walk around the city in the evening. Do not be shy, text me message""</p>

<p>or</p>

<p>""Whats up guys! I'm looking for a company for visiting museum of science. It is more enjoyable to visit it with somebody""</p>

<p>It is users' events. For example a user wants to visit some place, event, seek a company for outdoor or trip together.</p>

<p>I found just one way: write list of first, second and third part of a sentence and then randomly assemble whole sentence from parts. 
But I do not know, maybe there already is similar lists? Maybe there are another way or special techniques for solve my task?</p>
",Named Entity Recognition (NER),natural language sentence generation looking way generate synthesize natural language sentence need write sentence like following hi would like go walk around city evening shy text message whats guy looking company visiting museum science enjoyable visit somebody user event example user want visit place event seek company outdoor trip together found one way write list first second third part sentence randomly assemble whole sentence part know maybe already similar list maybe another way special technique solve task
Identifying geographical locations in text,"<p>What kind of work has been done to determine whether a specific string pertains to a geographical location?  For example:</p>

<pre><code>'troy, ny'
'austin, texas'
'hotels in las vegas, nv'
</code></pre>

<p>I guess what I'm sort of expecting is a statistical approach that gives a degree of confidence that the first two are locations.  The last one would probably require a heuristic which grabs ""%s, %s"" and then uses the same technique.  I'm specifically looking for approaches that don't rely too heavily on the proposition 'in', seeing as it's not an entirely unambiguous or consistently available indicator of location.</p>

<p>Can anyone point me to approaches, papers, or existing utilities?  Thanks!</p>
",Named Entity Recognition (NER),identifying geographical location text kind work ha done determine whether specific string pertains geographical location example guess sort expecting statistical approach give degree confidence first two location last one would probably require heuristic grab us technique specifically looking approach rely heavily proposition seeing entirely unambiguous consistently available indicator location anyone point approach paper existing utility thanks
Stanford NER Combined classifiers run on the port,"<p>I can run on a port single classifier with using the command given below.</p>

<pre><code>java -mx400m -cp ""*:lib/*"" edu.stanford.nlp.ie.NERServer -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -port 1234 -outputFormat inlineXML
</code></pre>

<p>And i can use the result in Python <code>ner</code> package like this.</p>

<pre><code>tagger = ner.SocketNER(host='localhost', port=1234)
print tagger.get_entities('Rahul working in uniserved.')
#Output  {u'PERSON': [u'Rahul']}
</code></pre>

<p>I combined the classifiers with using the command.</p>

<pre><code>java -mx1g -cp ""*:lib/*"" edu.stanford.nlp.ie.NERClassifierCombiner -textFile sample.txt -ner.model classifiers/english.all.3class.distsim.crf.ser.gz,classifiers/english.conll.4class.distsim.crf.ser.gz,classifiers/english.muc.7class.distsim.crf.ser.gz
</code></pre>

<p>But i need need to run on port. I tried different possibilities like given argument <code>-port 8080 -outputFormat inlineXML</code>. But it shows invalid argument.  </p>

<p>How can i archive it.</p>

<p>Thank you.</p>
",Named Entity Recognition (NER),stanford ner combined classifier run port run port single classifier using command given use result python package like combined classifier using command need need run port tried different possibility like given argument show invalid argument archive thank
Language Understanding: How to check whether two tasks or todos are same or not?,"<p>Are there any standard algos which given two queries/tasks/todos and tell me whether they are same or not? The way I'm thinking is that if somehow I can extract the intent of the task and entities involved in it and check them with that of the other task. But I'm not sure whether this approach will work in all scenarios or not.</p>
",Named Entity Recognition (NER),language understanding check whether two task todos standard algos given two query task todos tell whether way thinking somehow extract intent task entity involved check task sure whether approach work scenario
Where can I find a text list or library that contains a list of common foods?,"<p>I'm writing a Python script that parses emails which involves searching the text of the email for any words that are common food items. I need some way to determine whether words are indeed food items.</p>

<p>I've looked at several natural language processing APIs (such as AlchemyAPI and NLTK 2.0) and they appear to have Named Entity Extraction (which is what I want), but I can't find an entity type for food in particular.</p>

<p>It's quite possible that natural language processing is overkill for what I need-- I just want a list of foods that I can match to. Where can I find such a word list? Do I need to write my own scraper to parse it off some online source, or is there an easier way?</p>
",Named Entity Recognition (NER),find text list library contains list common food writing python script par email involves searching text email word common food item need way determine whether word indeed food item looked several natural language processing apis alchemyapi nltk appear named entity extraction want find entity type food particular quite possible natural language processing overkill need want list food match find word list need write scraper parse online source easier way
How to identify more than one label for an entity using Stanford NER,"<p>I want to identify a word or entity with two labels.</p>

<p>For example: <code>John works in India.</code></p>

<p>Output should be:</p>

<blockquote>
  <p>John PER NNP<br>
  works O O<br>
  in O O<br>
  India LOC NNP</p>
</blockquote>

<p>So it should identify the <strong>named entity</strong> as well as the <strong>POS tags</strong>.</p>

<p>I have created my own set of training data.</p>

<p>I am using the below property file</p>

<blockquote>
  <p>trainFile = training.tsv<br>
  serializeTo = model.ser.gz<br>
  map = word=0,answer=1,tag=2<br>
  useClassFeature=true 
  useWord=true<br>
  useNGrams=true<br>
  noMidNGrams=true<br>
  maxNGramLeng=6<br>
  usePrev=true<br>
  useNext=true<br>
  useSequences=true<br>
  usePrevSequences=true<br>
  useTags = true<br>
  useWordTag = true<br>
  useGenericFeatures = true<br>
  mergeTags = true<br>
  maxLeft=1<br>
  useTypeSeqs=true<br>
  useTypeSeqs2=true<br>
  useTypeySequences=true<br>
  wordShape=chris2useLC<br>
  useDisjunctive=true</p>
</blockquote>

<p>The training file looks like this:</p>

<blockquote>
  <p>John PER NNP<br>
  works O O<br>
  in O O<br>
  India LOC NNP</p>
</blockquote>

<p>I am using the below code to run NER in python:</p>

<pre><code>tagging_NER = StanfordNERTagger('.../model.ser.gz','.../stanford-ner.jar',encoding='utf-8')
token = StanfordTokenizer('.../stanford-ner.jar')
tok = token.tokenize(text)
tags = tagging_NER.tag(tok)
</code></pre>

<p>Now this code gives me just the <code>named entities</code>. It does not return me the <code>POS tags</code>.</p>

<p>Is there any method by which I could get both these things?</p>
",Named Entity Recognition (NER),identify one label entity using stanford ner want identify word entity two label example output john per nnp work india loc nnp identify named entity well po tag created set training data using property file trainfile training tsv serializeto model ser gz map word answer tag useclassfeature true useword true usengrams true nomidngrams true maxngramleng useprev true usenext true usesequences true useprevsequences true usetags true usewordtag true usegenericfeatures true mergetags true maxleft usetypeseqs true usetypeseqs true usetypeysequences true wordshape chris uselc usedisjunctive true training file look like john per nnp work india loc nnp using code run ner python code give doe return method could get thing
Does Stanford NER detect duplicate Entity mentions?,"<p>Let’s take an example of a text: “<strong><em>Barack Obama</em></strong> is an American politician serving as the 44th President of the United States. Born in Honolulu, Hawaii, <strong><em>Obama</em></strong> is a graduate of Columbia University and Harvard Law School.” </p>

<p>Below screenshot is how the 4 class NER model tags the entities:
<a href=""https://i.sstatic.net/1GyTB.jpg"" rel=""nofollow"">Result from Stanford NER Online Demo</a></p>

<p>Does Stanford NER detect that <strong><em>Barack Obama</em></strong> in line 1 is the same entity as <strong><em>Obama</em></strong> in line 2 ?</p>
",Named Entity Recognition (NER),doe stanford ner detect duplicate entity mention let take example text barack obama american politician serving th president united state born honolulu hawaii obama graduate university harvard law school screenshot class ner model tag entity result stanford ner online demo doe stanford ner detect barack obama line entity obama line
Classpath Error when training a model with Stanford NLP,"<p>I get a classpath error when I want to train my NER model :</p>

<blockquote>
  <p>Loading JAR-internal classifier
  /edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ...
  Exception in thread ""main"" java.lang.RuntimeException: Error loading
  classifier from jar file (most likely you are not running this code
  from a jar file or the named classifier is not stored in the jar file)</p>
</blockquote>

<p>I'm using this command line : </p>

<p>java -cp ""stanford-ner.jar:lib/*""  edu.stanford.nlp.ie.crf.CRFClassifier /Users/Desktop/austen.prop</p>

<p>I'm following the instructions from the Stanford NER FAQ <a href=""http://nlp.stanford.edu/software/crf-faq.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/crf-faq.shtml</a></p>

<p>Someone know how to run correctly this command ?</p>
",Named Entity Recognition (NER),classpath error training model stanford nlp get classpath error want train ner model loading jar internal classifier edu stanford nlp model ner english class distsim crf ser gz exception thread main java lang runtimeexception error loading classifier jar file likely running code jar file named classifier stored jar file using command line java cp stanford ner jar lib edu stanford nlp ie crf crfclassifier user desktop austen prop following instruction stanford ner faq someone know run correctly command
Datasets in Biodomain like Word similarity datasets used in word2vec and Glove,"<p>I am training word2vec on biomedical texts. In order to perform word similarity and word analogy tests I want to have pairs of biomedical terms having same relationships(could be any), just like we have a comprehensive list of City-State data in word2vec. I tried searching the web but since I am new to the domain I am finding it confusing.  </p>

<p>So, where can I find the list relevant to Drug-gene or Protein-action, etc? Or how can I mine this data. Please suggest publicly available such datasets. Also, please suggest any additional interesting relationships which I can also query. </p>

<p>Another way would be to use available ontologies as they include relations between concepts such as has-part, is-a-way-of-doing, is-a-cause-of, is-a-symptom-of etc. Can I use ontologies to extract such pairs? If yes, then what ontologies and how? </p>

<p>Are there any gold standard datasets already available that can serve my purpose? </p>
",Named Entity Recognition (NER),datasets biodomain like word similarity datasets used word vec glove training word vec biomedical text order perform word similarity word analogy test want pair biomedical term relationship could like comprehensive list city state data word vec tried searching web since new domain finding confusing find list relevant drug gene protein action etc mine data please suggest publicly available datasets also please suggest additional interesting relationship also query another way would use available ontology include relation concept ha part way cause symptom etc use ontology extract pair yes ontology gold standard datasets already available serve purpose
Python frameworks for NLP?,"<p>I am working on a project wherein I have to extract the following information from a set of articles (the articles could be on anything):</p>

<ul>
<li><p><strong>People</strong> Find the names of any people present, like <em>""Barack Obama""</em></p></li>
<li><p><strong>Topic</strong> or related tags of the article, like <em>""Parliament""</em>, <em>""World Energy""</em></p></li>
<li><p><strong>Company/Organisation</strong> I should be able to obtain the names of the any companies or organisations mentioned, like <em>""Apple""</em> or <em>""Google""</em></p></li>
</ul>

<p>Is there an NLP framework/library of this sort available in Python which would help me accomplish this task?</p>
",Named Entity Recognition (NER),python framework nlp working project wherein extract following information set article article could anything people find name people present like barack obama topic related tag article like parliament world energy company organisation able obtain name company organisation mentioned like apple google nlp framework library sort available python would help accomplish task
R: Natural Language Processing on Support Vector Machine,"<p>I have started working on a project which requires Natural Language Processing and building a model on Support Vector Machine (SVM) in R (I was requested to do it in R, though I know Python is more developed on this). 
I found an article <a href=""https://rpubs.com/lmullen/nlp-chapter"" rel=""nofollow"">here</a> (Packages: <code>NLP</code>, <code>OpenNLP</code>, <code>rJava</code>, <code>RWeka</code>). However, the article focuses on how to extract key words (ex. Place, names…). </p>

<p>But since I want to build a SVM model, I’d like to generate a Term Document Matrix with all the tokens. I couldn’t get it work since the class of the annotation does not apply in <code>tm</code> package. </p>

<p>Example:</p>

<pre><code>testset &lt;- c(""From month 2 the AST and total bilirubine were not measured."", ""16:OTHER - COMMENT REQUIRED IN COMMENT COLUMN;07/02/2004/GENOTYPING;SF- genotyping consent not offered until T4."",  ""M6 is 13 days out of the visit window"")
word_ann &lt;- Maxent_Word_Token_Annotator()
sent_ann &lt;- Maxent_Sent_Token_Annotator()
test_annotations &lt;- annotate(testset, list(sent_ann, word_ann))
test_doc &lt;- AnnotatedPlainTextDocument(testset, test_annotations)
sents(test_doc)

[[1]]
 [1] ""From""       ""month""      ""2""          ""the""        ""AST""        ""and""        ""total""     
 [8] ""bilirubine"" ""were""       ""not""        ""measured""   "".""         

[[2]]
 [1] ""16:OTHER""                         ""-""                               
 [3] ""COMMENT""                          ""REQUIRED""                        
 [5] ""IN""                               ""COMMENT""                         
 [7] ""COLUMN;07/02/2004/GENOTYPING;SF-"" ""genotyping""                      
 [9] ""consent""                          ""not""                             
[11] ""offered""                          ""until""                           
[13] ""T4""                               "".""                               

[[3]]
[1] ""M6""     ""is""     ""13""     ""days""   ""out""    ""of""     ""the""    ""visit""  ""window"" 
sessionInfo()
R version 3.3.0 (2016-05-03)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows &gt;= 8 x64 (build 9200)

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] tm_0.6-2       openxlsx_3.0.0 magrittr_1.5   RWeka_0.4-28   openNLP_0.2-6  NLP_0.1-9     
[7] rJava_0.9-8   

loaded via a namespace (and not attached):
[1] openNLPdata_1.5.3-2 parallel_3.3.0      tools_3.3.0         Rcpp_0.12.5         slam_0.1-34        
[6] grid_3.3.0          knitr_1.13          RWekajars_3.9.0-1  
</code></pre>

<p>And now I don't know how to generate a TDM from here...
Could anyone please give me some advice on this?</p>
",Named Entity Recognition (NER),r natural language processing support vector machine started working project requires natural language processing building model support vector machine svm r wa requested r though know python developed found article package however article focus extract key word ex place name since want build svm model like generate term document matrix token get work since class annotation doe apply package example know generate tdm could anyone please give advice
How to train Tokenizer in OpenNLP?,"<p>I'm currently using the whitespace tokenizer in OpenNLP which tokenizes the sentence wherever it finds a whitespace.
so, if I have a sentence like:</p>

<pre><code>My hobbies are reading books, magazines, Roller skating and playing football.
</code></pre>

<p>Now, if I want to get the hobbies from that sentence I want to tokenize the sentence, not on the basis of whitespace but a comma based tokenizing.</p>

<p>How can I achieve this?</p>

<p>EDIT 1:
 The main problem is after training an NER model, I'm applying it on the tokens. As a result, it won't be recognizing 'roller skating' because in tokens they're present as 'roller' and 'skating' and the NER model cannot recognize that hobby now.</p>
",Named Entity Recognition (NER),train tokenizer opennlp currently using whitespace tokenizer opennlp tokenizes sentence wherever find whitespace sentence like want get hobby sentence want tokenize sentence basis whitespace comma based tokenizing achieve edit main problem training ner model applying token result recognizing roller skating token present roller skating ner model recognize hobby
Number of Tags of NER,"<p>As far as I know, Stanford NER has 3,4 and 7 class models (or tags).
I need a standard and a Java implementation of a Named Entity Recognizer which has more than 7 tags (for example 13 tags). I don’t want to develop it by myself and I need it to be accurate. Is there any NER which satisfies these conditions?</p>

<p>Thanks</p>
",Named Entity Recognition (NER),number tag ner far know stanford ner ha class model tag need standard java implementation named entity recognizer ha tag example tag want develop need accurate ner satisfies condition thanks
Stanford NER: AbstractSequenceClassifier vs NamedEntityTagAnnotation,"<p><strong>QUESTIONS</strong></p>

<ol>
<li><p>How do I load a custom properties file using AbstractSequenceClassifier?
e.g.,</p>

<p>Master's Degree\tDEGREE</p>

<p>MBA\tDEGREE</p></li>
<li><p>What are the benefits/drawbacks of each approach?(AbstractSequenceClassifier vs NamedEntityTagAnnotation)</p></li>
<li><p>Is there any accessible documentation/tutorial on the internet. I can play with demo code and read javadocs, but a good tutorial would save me and many others a lot of time.</p></li>
</ol>

<p>During my perusal of the  Stanford NER documentation, I have encountered two java examples.</p>

<p><strong>NamedEntityTagAnnotation</strong></p>

<p>The first uses NamedEntityTagAnnotation. This allows me to add my own properties file for training data (using regexner.mapping).</p>

<p>The key code is as follows:
Initialize Pipeline:</p>

<pre><code>   Properties props = new Properties();
   props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, regexner, depparse,  natlog,  openie"");
   props.put(""regexner.mapping"",  ""mypath/mytraineddatacodes.properties"");

   pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>Initialize document:</p>

<pre><code>   Annotation document = new Annotation(pass4);
   pipeline.annotate(document);
</code></pre>

<p>Then access the NER tokens and any other data needed:</p>

<pre><code>List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);


for (CoreMap sentence : sentences)
{

  for (CoreLabel token : sentence.get(TokensAnnotation.class))
  {
     currNeToken = token.get(NamedEntityTagAnnotation.class);

     String word = token.get(TextAnnotation.class);
  }
}
</code></pre>

<p><strong>AbstractSequenceClassifier</strong></p>

<p>This is the method demonstrated in the Stanford NERDemo.java example. IT seems to provide much deeper access to the API, but I don't know how to load my customized properties file of trained data.</p>

<p>Initialize Classifier (which bi-passes the pipeline)</p>

<pre><code>   String serializedClassifier = ""classifiers/english.all.3class.distsim.crf.ser.gz"";

  AbstractSequenceClassifier classifier =    CRFClassifier.getClassifierNoExceptions(serializedClassifier);
</code></pre>

<p>Load the file to analyze:</p>

<pre><code>      byte[] encoded = Files.readAllBytes(p);
  String s = new String(encoded);
     String fileContents = s;
     List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(fileContents);
     for (List&lt;CoreLabel&gt; sentence : out)
     {
        for (CoreLabel word : sentence)
        {
           Log.getLogger().debug(word.word() + '/' + word.get(AnswerAnnotation.class) + ' ');
        }
        System.out.println();
     }
</code></pre>

<p>And your off to the races, except it hasn't loaded my custom properties file for trained data.</p>

<p><strong>QUESTIONS</strong></p>

<ol>
<li><p>How do I load a custom properties file using AbstractSequenceClassifier?
e.g.,</p>

<p>Master's Degree\tDEGREE</p>

<p>MBA\tDEGREE</p></li>
<li><p>What are the benefits/drawbacks of each method?</p></li>
<li><p>Is there any accessible documentation/tutorial on the internet. I can play with demo code and read javadocs, but a good tutorial would save me and many others a lot of time.</p></li>
</ol>
",Named Entity Recognition (NER),stanford ner abstractsequenceclassifier v namedentitytagannotation question load custom property file using abstractsequenceclassifier e g master degree tdegree mba tdegree benefit drawback approach abstractsequenceclassifier v namedentitytagannotation accessible documentation tutorial internet play demo code read javadocs good tutorial would save many others lot time perusal stanford ner documentation encountered two java example namedentitytagannotation first us namedentitytagannotation allows add property file training data using regexner mapping key code follows initialize pipeline initialize document access ner token data needed abstractsequenceclassifier method demonstrated stanford nerdemo java example seems provide much deeper access api know load customized property file trained data initialize classifier bi pass pipeline load file analyze race except loaded custom property file trained data question load custom property file using abstractsequenceclassifier e g master degree tdegree mba tdegree benefit drawback method accessible documentation tutorial internet play demo code read javadocs good tutorial would save many others lot time
Chunking Stanford Named Entity Recognizer (NER) outputs from NLTK format,"<p>I am using NER in NLTK to find persons, locations, and organizations in sentences. I am able to produce the results like this:</p>

<pre><code>[(u'Remaking', u'O'), (u'The', u'O'), (u'Republican', u'ORGANIZATION'), (u'Party', u'ORGANIZATION')]
</code></pre>

<p>Is that possible to chunk things together by using it?
What I want is like this:</p>

<pre><code>u'Remaking'/ u'O', u'The'/u'O', (u'Republican', u'Party')/u'ORGANIZATION'
</code></pre>

<p>Thanks!</p>
",Named Entity Recognition (NER),chunking stanford named entity recognizer ner output nltk format using ner nltk find person location organization sentence able produce result like possible chunk thing together using want like thanks
StanfordNLP Openie fails,"<p>I have StanfordNLP up and running.</p>

<p>My maven dependency structure is as follows:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.6.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.6.0&lt;/version&gt;
    &lt;classifier&gt;models&lt;/classifier&gt;
&lt;/dependency&gt;</code></pre>
</div>
</div>
</p>

<p>My code runs just fine as follows:</p>

<pre><code>@Test
public void testTA() throws Exception
{

    Path p = Paths.get(""s.txt"");

    byte[] encoded = Files.readAllBytes(p);
    String s = new String(encoded);

    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, parse, ner, dcoref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // read some text in the text variable
    String text = s;

    StringBuffer sb = new StringBuffer();

    sb.append(text);
    sb.append(
            ""\n\n\n\n\n\n\n===================================================================\n\n\n\n\n\n\n\n\n\n\n"");

    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);

    // run all Annotators on this text
    pipeline.annotate(document);

    // these are all the sentences in this document
    // a CoreMap is essentially a Map that uses class objects as keys and
    // has values with custom types
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

    sb.append(
            ""\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n+++++++++++++++++++++++SENTENCES++++++++++++++++++++++++++++\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"");
    for (CoreMap sentence : sentences)
    {
        // traversing the words in the current sentence
        // a CoreLabel is a CoreMap with additional token-specific methods
        sb.append(""\n\n\n==============SENTENCE==============\n\n\n"");
        sb.append(sentence.toString());
        sb.append(""\n"");
        for (CoreLabel token : sentence.get(TokensAnnotation.class))
        {
            // this is the text of the token
            sb.append(""\n==============TOKEN==============\n"");
            String word = token.get(TextAnnotation.class);
            sb.append(word);
            sb.append("" : "");
            // this is the POS tag of the token
            String pos = token.get(PartOfSpeechAnnotation.class);
            // this is the NER label of the token
            sb.append(pos);
            sb.append("" : "");
            String lemma = token.get(LemmaAnnotation.class);
            sb.append(lemma);
            sb.append("" : "");
            String ne = token.get(NamedEntityTagAnnotation.class);
            sb.append(ne);
            sb.append(""\n"");

        }

        // this is the parse tree of the current sentence
        Tree tree = sentence.get(TreeAnnotation.class);
        sb.append(""\n\n\n=====================TREE==================\n\n\n"");
        sb.append(tree.toString());

        // this is the Stanford dependency graph of the current sentence
        SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
        sb.append(""\n\n\n"");
        sb.append(dependencies.toString());
    }
</code></pre>

<p>However, when I add openie to the pipeline, the code fails.</p>

<p>props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, parse, ner, dcoref, openie"");</p>

<p>The error I get is as follows:</p>

<p>annotator ""openie"" requires annotator ""natlog""</p>

<p>Can anyone advise me on this?</p>
",Named Entity Recognition (NER),stanfordnlp openie fails stanfordnlp running maven dependency structure follows code run fine follows however add openie pipeline code fails prop setproperty annotator tokenize ssplit po lemma parse ner dcoref openie error get follows annotator openie requires annotator natlog anyone advise
Custom NER and POS tagging,"<p>I was checking out Stanford CoreNLP in order to understand NER and POS tagging. But what if I want to create custom tags for entities like<code>&lt;title&gt;Nights&lt;/title&gt;, &lt;genre&gt;Jazz&lt;/genre&gt;, &lt;year&gt;1992&lt;/year&gt;</code> How can I do it? is CoreNLP useful in this case?</p>
",Named Entity Recognition (NER),custom ner po tagging wa checking stanford corenlp order understand ner po tagging want create custom tag entity like corenlp useful case
Sentence correction using NLP,"<p>I'm trying to build a chat assistant in my website and it should answer queries like ""Can you track my order?"", ""How's performance of XXX"". The majority of the work lies in understanding the user's query.</p>

<p>I'm using 'Named Entity Recognizers' and ""Text Parsers"" for processing the queries. Before this, I'm passing the query through 'Spell checker' to reduce the errors like,</p>

<pre><code>Can you track my ordr?
</code></pre>

<p>to</p>

<pre><code>Can you track my order?
</code></pre>

<p>It's working in most of the cases but failing in cases like,</p>

<pre><code>Can you track my water?
</code></pre>

<p>In this case, the spelling corrector doesn't correct the word 'water' and NER is not able identify the entity as 'order'. </p>

<p>The problem is 'Can you track my water?' may be a correct sentence in some other context but it's definitely a mistake in my context (domain). So I should be able to correct this sentence.</p>

<p>I'm stuck here.</p>

<p>Is there anyway I can correct these sentences using predefined queries and/or statistical data of user entered queries? </p>
",Named Entity Recognition (NER),sentence correction using nlp trying build chat assistant website answer query like track order performance xxx majority work lie understanding user query using named entity recognizers text parser processing query passing query spell checker reduce error like working case failing case like case spelling corrector correct word water ner able identify entity order problem track water may correct sentence context definitely mistake context domain able correct sentence stuck anyway correct sentence using predefined query statistical data user entered query
Extracting parts of JSON from json.loads list in python,"<p>I have ~100k JSON files, each containing JSON which I am looping through to create a bag of words model - very simple. Each JSON file looks like this:</p>

<pre><code>[{""tokens"":[{""word"":""Voices"",""lemma"":""voice"",""pos"":""NNS"",""ner"":""O""},{""word"":""from"",""lemma"":""from"",""pos"":""IN"",""ner"":""O""},{""word"":""Russia"",""lemma"":""Russia"",""pos"":""NNP"",""ner"":""LOCATION""}],""dependencies"":[{""head"":0,""dep"":2,""label"":""prep_from""}]},{""tokens"":[{""word"":""Wednesday"",""lemma"":""Wednesday"",""pos"":""NNP"",""ner"":""DATE""},{""word"":"","",""lemma"":"","",""pos"":"","",""ner"":""DATE""},{""word"":""11"",""lemma"":""11"",""pos"":""CD"",""ner"":""DATE""},
....
</code></pre>

<p>What I need is to extract only the values of the <code>""word""</code>  keys for each file, and store this array in a new file called so each file has an array like:</p>

<p><code>[""Voices"", ""from"", ""Wednesday"",""Russia"", "","" ,""11""...]</code></p>

<p>And also I have a similiar array for all the files put together, stored in <code>../../data/train_jsons/all_words.json</code></p>

<p>However <code>json.loads</code> creates a list for each item not a dict. How can I achieve what I want just from looping through the list for each file, and store these individual arrays of words in new files that maintain the name of the filepath of the json e.g. new files called <code>../../data/train_jsons/words_for_.........json</code>? </p>

<p>Trying to transform to a dict and using the key ""word"" doesn't seem to work per:</p>

<pre><code>for subdir, dirs, files in os.walk('../../data/train_jsons'):
    for file in files:
        filepath = subdir + os.sep + file
        if filepath.endswith("".json""):
            with open(filepath) as data_file:
                data = json.load(data_file)
                dict = dict(itertools.izip_longest(*[iter(data)] * 2, fillvalue=""""))
</code></pre>

<p>Speed is a key factor in my solution.</p>
",Named Entity Recognition (NER),extracting part json json load list python k json file containing json looping create bag word model simple json file look like need extract value key file store array new file called file ha array like also similiar array file put together stored however creates list item dict achieve want looping list file store individual array word new file maintain name filepath json e g new file called trying transform dict using key word seem work per speed key factor solution
How to clean sentences for StanfordNER,"<p>I want to use <code>StanfordNER</code> in python to detect named entities. How should i clean up the sentences?</p>

<p>for example, consider</p>

<p><code>qry=""In the UK, the class is relatively crowded with Zacc competing with Abc's Popol (market leader) and  Xyz's Abcvd.""</code></p>

<p>if i do</p>

<pre><code>st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') 
print st.tag(qry.split())
</code></pre>

<p>i get</p>

<pre><code>[
    (u'In', u'O'), (u'the', u'O'), (u'UK,', u'O'), (u'the', u'O'), 
    (u'class', u'O'), (u'is', u'O'), (u'relatively', u'O'), (u'crowded', u'O'), 
    (u'with', u'O'), (u'Zacc', u'PERSON'), (u'competing', u'O'), (u'with', u'O'), 
    (u""Abc's"", u'O'), (u'Popol', u'O'), (u'(market', u'O'), (u'leader)', u'O'), 
    (u'and', u'O'), (u""Xyz's"", u'O'), (u'Abcvd.', u'O')
]
</code></pre>

<p>`</p>

<p>so only 1 named entities was detected. However, if i do some cleanup by replacing all special characters with spaces</p>

<p><code>qry=""In the UK the class is relatively crowded with Zacc competing with Abc s Popol  market leader and  Xyz s Abcvd""</code></p>

<p>i get</p>

<pre><code>[
    (u'In', u'O'), (u'the', u'O'), (u'UK', u'LOCATION'), (u'the', u'O'), 
    (u'class', u'O'), (u'is', u'O'), (u'relatively', u'O'), (u'crowded', u'O'), 
    (u'with', u'O'), (u'Zacc', u'PERSON'), (u'competing', u'O'), (u'with', u'O'), 
    (u'Abc', u'ORGANIZATION'), (u's', u'O'), (u'Popol', u'PERSON'), (u'market', u'O'), 
    (u'leader', u'O'), (u'and', u'O'), (u'Xyz', u'ORGANIZATION'), (u's', u'O'), (u'Abcvd', u'PERSON')]
</code></pre>

<p>`</p>

<p>so clearly, this is more appropriate. Are there any general rules on how to clean up sentences for <code>StanfordNER</code>? Initially i thought that there is no cleanup required at all!</p>
",Named Entity Recognition (NER),clean sentence stanfordner want use python detect named entity clean sentence example consider get clearly appropriate general rule clean sentence initially thought cleanup required
Entity extraction from bank wire transactions ( like not-so-natural-text),"<p>I am trying to extract entities ( name , address , organization) from not so natural text, like comment in bank wire transactions.
Obviously not getting good results , used NLTK , OpenNLP and CoreNLP.</p>

<p>Any idea how to improve the results?</p>

<p>the text can look like,</p>

<ol>
<li>EVERITT 620122T NAT ABC INDIA LTD </li>
<li>REF ROBERT FINEMANN  - REASON SHOP RENTAL </li>
<li>REF BY92 00 112233999 - REASON SPEEDING FINE </li>
<li>GEM SS HEUTIGEM SCHIENDLER </li>
<li>PENSION CH1234  CAB28</li>
</ol>

<p>...</p>

<p>Reference to research work or existing products will also help</p>
",Named Entity Recognition (NER),entity extraction bank wire transaction like natural text trying extract entity name address organization natural text like comment bank wire transaction obviously getting good result used nltk opennlp corenlp idea improve result text look like everitt nat abc india ltd ref robert finemann reason shop rental ref reason speeding fine gem heutigem schiendler pension ch cab reference research work existing product also help
NLP extract category from text using java,"<p>How to classify the words extracted from a text (using NLP/NLTK or Textblob) in entities and categories (from lists or taxonomy) ?</p>

<p>I will have keyword database and I would like to extract line by line from a text the matching keyword.</p>

<p>For example below a text file</p>

<blockquote>
  <p>Date, Description,    Money in (€),   Money out (€)<br>
  1) 3-Mar-16,    CNC CNCWORD1 CNCWORD2 P 01/03 3,    ,   2.95<br>
  2) 3-Mar-16 POS POSWORD1 CNCWORD2 01/03 0,  ,   20<br>
  3) 2-Mar-16 CNC CNCWORD3 28/02, ,   1.60 </p>
  
  <p>4) 2-Mar-16    POS POSWORD3 POSWORD4 29/02 17,     ,102.3</p>
</blockquote>

<p>And keywords database</p>

<blockquote>
  <p>{CNC CNCWORD1, CNCWORD3, POS POSWORD1 CNCWORD2,  POS POSWORD3}</p>
</blockquote>

<p>Using NLP from each line get the likely matching keyword</p>

<p>For above example, we shall have:</p>

<blockquote>
  <p>1) CNC CNCWORD1
  2) POS POSWORD1 CNCWORD2
  3) CNCWORD3
  4) POS POSWORD3</p>
</blockquote>
",Named Entity Recognition (NER),nlp extract category text using java classify word extracted text using nlp nltk textblob entity category list taxonomy keyword database would like extract line line text matching keyword example text file date description money money mar cnc cncword cncword p mar po posword cncword mar cnc cncword mar po posword posword keywords database cnc cncword cncword po posword cncword po posword using nlp line get likely matching keyword example shall cnc cncword po posword cncword cncword po posword
find job role from text data,"<p>I have a text file from which i have to extract on what role the people are working. ""Mechanical engineer"",""software developer"" etc.
I have used NLTK to extract this using grammer like,</p>

<pre><code>grammer= r""""""
          NP: {&lt;NN.*|JJ&gt;*&lt;NN.*&gt;}  """"""
</code></pre>

<p>the result i am getting is good, but still for few documnets junk is coming. for those lines i want to apply Regular expressions.</p>

<p>my sample texts are like this.</p>

<ul>
<li>""I am software developement engineer in microsoft""</li>
<li>""I am mechanical engineer with 10 years experience""</li>
</ul>

<p>what i want is, I will extract two or three words before ""Engineer"".
I m using regular expression like,</p>

<pre><code>regex=re.compile('|'.join([r'(?:\S+\s)?\S*[eE]ngineer']))
</code></pre>

<p>but, it extracts only one word before the specific word. How to make it to extract two or more words.?</p>

<p>i tried putting {2-3} in place of ""?"" in expression. but i am not getting desired result.</p>

<p>Is my approach correct ?
or any other approach to extract this specific phrase in better way ?</p>
",Named Entity Recognition (NER),find job role text data text file extract role people working mechanical engineer software developer etc used nltk extract using grammer like result getting good still documnets junk coming line want apply regular expression sample text like software developement engineer microsoft mechanical engineer year experience want extract two three word engineer using regular expression like extract one word specific word make extract two word tried putting place expression getting desired result approach correct approach extract specific phrase better way
How to conduct the training of NER models for &#39;Urdu&#39; in OpenNLP?,"<p>I would like to train a NER models using Apache OpenNLP for my native language Urdu. I have training data in <code>train.txt</code> ready. 
What are the next steps to make a trained model(.bin), like we find on <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow"">OpenNLP site</a> in the model download section.</p>
",Named Entity Recognition (NER),conduct training ner model urdu opennlp would like train ner model using apache opennlp native language urdu training data ready next step make trained model bin like find opennlp site model download section
How to handle &#39;ExtensionNotLoadedException&#39; when using custom NER model in OpenNLP?,"<p>I'm trying to execute the OpenNLP model which I have trained for my customized names. I executed the below command to make customized model :</p>

<blockquote>
  <p>opennlp TokenNameFinderTrainer -encoding UTF-8 -lang en -data c:\Users\nshah\Desktop\en-ner-person.train -model en-ner-personName.bin</p>
</blockquote>

<p>I am able to successfully create model named <strong>en-ner-personName.bin</strong>.</p>

<p>Now i am trying to execute my program as below :</p>

<pre><code>import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;

import opennlp.tools.namefind.NameFinderME;
import opennlp.tools.namefind.TokenNameFinderModel;
import opennlp.tools.util.InvalidFormatException;
import opennlp.tools.util.Span;

public class NameFinder {

    public String nameFind(String inputText){

        String finalNames = """";
        try {

            TokenNameFinderModel tokenNameFinderModel = new TokenNameFinderModel(new FileInputStream(""D://NISUM_OFFICIAL_WORKSPACE//opennlp//src//models//en-ner-personName.bin""));
            NameFinderME nameFinderME = new NameFinderME(tokenNameFinderModel);
            Tokenization tokenize = new Tokenization();
            String[] tokens = tokenize.tokenization(inputText);
            Span drugSp[] = nameFinderME.find(tokens);
            String [] sp = Span.spansToStrings(drugSp, tokens);
            StringBuilder string = new StringBuilder();
            if(sp.length!=0){
                for(int i=0;i&lt;=sp.length;i++){
                    string = string.append(sp[i]+""\n"");
                }
                finalNames = string.toString();
            }
        } catch (InvalidFormatException e) {
            e.printStackTrace();
        } catch (FileNotFoundException e) {
            e.printStackTrace();
        } catch (IOException e) {
            e.printStackTrace();
        }
        return finalNames;
    }

}
</code></pre>

<p>but after executing above program i am getting the below exception on console as mentioned :</p>

<pre><code>Could not instantiate the opennlp.tools.namefind.TokenNameFinderFactory. The initialization throw an exception.
opennlp.tools.util.ext.ExtensionNotLoadedException: Unable to find implementation for opennlp.tools.util.BaseToolFactory, the class or service opennlp.tools.namefind.TokenNameFinderFactory could not be located!
    at opennlp.tools.util.ext.ExtensionLoader.instantiateExtension(ExtensionLoader.java:97)
    at opennlp.tools.util.BaseToolFactory.create(BaseToolFactory.java:106)
    at opennlp.tools.util.model.BaseModel.initializeFactory(BaseModel.java:254)
    at opennlp.tools.util.model.BaseModel.loadModel(BaseModel.java:237)
    at opennlp.tools.util.model.BaseModel.&lt;init&gt;(BaseModel.java:181)
    at opennlp.tools.namefind.TokenNameFinderModel.&lt;init&gt;(TokenNameFinderModel.java:110)
    at com.naimesh.opennlp.DrugNameFinder.drugNameFind(DrugNameFinder.java:19)
    at com.naimesh.opennlp.TextFinder.main(TextFinder.java:18)
Exception in thread ""main"" java.lang.IllegalArgumentException: opennlp.tools.util.InvalidFormatException: Could not instantiate the opennlp.tools.namefind.TokenNameFinderFactory. The initialization throw an exception.
    at opennlp.tools.util.model.BaseModel.initializeFactory(BaseModel.java:256)
    at opennlp.tools.util.model.BaseModel.loadModel(BaseModel.java:237)
    at opennlp.tools.util.model.BaseModel.&lt;init&gt;(BaseModel.java:181)
    at opennlp.tools.namefind.TokenNameFinderModel.&lt;init&gt;(TokenNameFinderModel.java:110)
    at com.naimesh.opennlp.DrugNameFinder.drugNameFind(DrugNameFinder.java:19)
    at com.naimesh.opennlp.TextFinder.main(TextFinder.java:18)
Caused by: opennlp.tools.util.InvalidFormatException: Could not instantiate the opennlp.tools.namefind.TokenNameFinderFactory. The initialization throw an exception.
    at opennlp.tools.util.BaseToolFactory.create(BaseToolFactory.java:117)
    at opennlp.tools.util.model.BaseModel.initializeFactory(BaseModel.java:254)
    ... 5 more
Caused by: opennlp.tools.util.ext.ExtensionNotLoadedException: Unable to find implementation for opennlp.tools.util.BaseToolFactory, the class or service opennlp.tools.namefind.TokenNameFinderFactory could not be located!
    at opennlp.tools.util.ext.ExtensionLoader.instantiateExtension(ExtensionLoader.java:97)
    at opennlp.tools.util.BaseToolFactory.create(BaseToolFactory.java:106)
    ... 6 more
</code></pre>

<p>Please can someone help me on this issue, i tried lot many things and read blogs, but couldn't find something useful. That will be really appreciated if someone can give useful solution.</p>
",Named Entity Recognition (NER),handle extensionnotloadedexception using custom ner model opennlp trying execute opennlp model trained customized name executed command make customized model opennlp tokennamefindertrainer encoding utf lang en data c user nshah desktop en ner person train model en ner personname bin able successfully create model named en ner personname bin trying execute program executing program getting exception console mentioned please someone help issue tried lot many thing read blog find something useful really appreciated someone give useful solution
Error while training opennlp with indian names,"<p>I am getting the following response while I am trying to train the opennlp for Indian names.</p>

<blockquote>
  <p>$ opennlp TokenNameFinderTrainer -model en-ner-person.bin -lang en
  -data en-ner-person.train -encoding UTF-8</p>
</blockquote>

<pre><code>Indexing events using cutoff of 5

Computing event counts...  java.io.IOException: Found unexpected annotation: &lt;start:PERSON&gt; suresh ###&lt;END&gt;### 61 years
Incorporating indexed data for training...  
Exception in thread ""main"" java.lang.NullPointerException
    at opennlp.tools.ml.maxent.GISTrainer.trainModel(GISTrainer.java:264)
    at opennlp.tools.ml.maxent.GIS.trainModel(GIS.java:298)
    at opennlp.tools.ml.maxent.GIS.doTrain(GIS.java:83)
    at opennlp.tools.ml.maxent.GIS.doTrain(GIS.java:36)
    at opennlp.tools.ml.AbstractEventTrainer.train(AbstractEventTrainer.java:93)
    at opennlp.tools.namefind.NameFinderME.train(NameFinderME.java:337)
    at opennlp.tools.cmdline.namefind.TokenNameFinderTrainerTool.run(TokenNameFinderTrainerTool.java:229)
    at opennlp.tools.cmdline.CLI.main(CLI.java:224)
</code></pre>

<p>My training data is in following format,</p>

<pre><code>I am &lt;start:PERSON&gt; suresh &lt;END&gt; 61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Kavitha &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; MohannarayanNew &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Single Raj &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; surendhiran.builder &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Veera &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; prem &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; lashwin &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; suresh &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; babu &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; raja &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; swamy &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Mahendiran &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; prabakar &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; sanjeeeth &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; venkatesh &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; antony &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; suresh &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; venkatesh &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; selvaraju &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Mahendiran &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; sathiya &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; sucithra &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; karthik.s &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Dilip Kumar Moharana &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; prem &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Jaya &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Sathish &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; jack &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; kumar &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; raja &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Veera &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; potter &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Karthi &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Azarudeen &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Abhinav &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; sam &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; surendhiranjayaraman &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; venky &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; karthick &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Gopikrishnan &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Gopi &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Raja Saravanan D &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; karuppu &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Karthikeyan &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Vinod &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; selladurai &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; dharmendra &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; jino &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Vinod &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Mike &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; jagdish &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Shailesh &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Srikanta Kumar Swain &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Sathish &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; babu &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; soundaryaindivi &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; melmaruvathur &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; raja &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; jayanthy &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; karan &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Mohan &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; surendhiran &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Gopinath J &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Dilip &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; mogan &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; venkatesh &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Karthik &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; prem &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; sathiyamoorthi &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 I am &lt;start:PERSON&gt; brintha &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; mahi &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Dilip &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; prabu &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Vijay &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; JAYAN A &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; pappu &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; karthik &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Gopinath J &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; karuppu &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Rakesh &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Karthik D &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Raja Saravanan &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 I am &lt;start:PERSON&gt; Raja Saravanan &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 
I am &lt;start:PERSON&gt; Methu Karthik &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Vijayakumar S &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Raja Saravanan &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 
I am &lt;start:PERSON&gt; sathiyamoorthi &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 Nov. 29 .
I am &lt;start:PERSON&gt; Shakti Kapoor &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; venkat shanmugam &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Ramasamy S &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Abhineet Sinha &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 I am &lt;start:PERSON&gt; moganarangan &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Raja Kullayapp &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29
I am &lt;start:PERSON&gt; tamil vanan &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Karthik D &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Murugan Murugan &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Jegadeesh Kumar &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Babu &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; muurgan murugan &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Ramesh &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; soundarya devi &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 I am &lt;start:PERSON&gt; kalai &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Karthik &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Raja &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; shankar shri &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; vignesh iyer &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; Gopi &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
I am &lt;start:PERSON&gt; vignesh iyer &lt;END&gt;   61 years old , will join the board as a nonexecutive director Nov. 29 .
</code></pre>

<p>Regards,
Jino.</p>
",Named Entity Recognition (NER),error training opennlp indian name getting following response trying train opennlp indian name opennlp tokennamefindertrainer model en ner person bin lang en data en ner person train encoding utf training data following format regard jino
Named Entity Recognition on Upper Case Text,"<p>How to extract entities from upper case text.</p>

<p>I use - <a href=""http://corenlp.run/"" rel=""nofollow"">http://corenlp.run/</a> </p>

<p>with test data - I KNOW TOM LIVES IN LONDON.</p>

<p>LONDON - Location
TOM - not identified
relation not identified</p>

<p>with test data - I know Tom lives in London.
Tom - Person
London - Location
relation correctly identified.</p>

<p>How to improve this? </p>
",Named Entity Recognition (NER),named entity recognition upper case text extract entity upper case text use test data know tom life london london location tom identified relation identified test data know tom life london tom person london location relation correctly identified improve
CRF model could be loaded as NER Annotator?,"<p>I have made my own CRF model. Now I want to do testing of that model, I need to load that model. A <a href=""https://www.dropbox.com/s/0kfv309bbcqd1o9/NERDemo.java?dl=0"" rel=""nofollow"">demo</a> is given in documentation. But my question is can I use this model with <strong>NER annotator</strong> of Stanford Corenlp? Can I do following things.</p>

<pre><code>Properties          props;
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, parse,ner"");
props.put(""ner.model"", ""myCrf-model.ser.gz"");
</code></pre>

<p>I need to know <strong>pros and cons</strong> of using these lines of code instead of those which are given in <a href=""https://www.dropbox.com/s/0kfv309bbcqd1o9/NERDemo.java?dl=0"" rel=""nofollow"">demo</a>. What would be effect in results?</p>

<p>I will be thankful to you if anyone can help me.</p>
",Named Entity Recognition (NER),crf model could loaded ner annotator made crf model want testing model need load model demo given documentation question use model ner annotator stanford corenlp following thing need know pro con using line code instead given demo would effect result thankful anyone help
JAVA: How use Gazettes with Stanford NLP?,"<p>I read this <a href=""http://www-nlp.stanford.edu/software/crf-faq.shtml#gazette"" rel=""nofollow"">faq</a> but i not understand. I try with this code:</p>

<pre><code>   Properties pp=new Properties();  
   pp.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse"");
   pp.put(""ner.useSUTime"",""false"");

   pp.put(""useGazettes"",""true"");
   pp.put(""gazette"",""C:\\gaz.txt"");

   StanfordCoreNLP s=new StanfordCoreNLP(pp);
</code></pre>

<p>This is String: ""Dan became a member of the Music friends association in 2008""</p>

<p>the gazette file is:</p>

<pre><code>  CLASS Music friends association 
</code></pre>

<p>But ""Music friends association"" is not recognized by NER.</p>

<p>Where am I wrong?</p>
",Named Entity Recognition (NER),java use gazette stanford nlp read faq understand try code string dan became member music friend association gazette file music friend association recognized ner wrong
Can not get an Output file in Stanford NER,"<p>I'm new to Stanford NER and have some problems.
I have downloaded Stanford Named Entity Recognizer version 3.6.0. It works, no problem. But I can't get a tagged text as an output file. Read about extracting data on this site: <a href=""http://www.themacroscope.org/2.0/using-the-stanford-named-entity-recognizer-to-extract-data-from-texts/"" rel=""nofollow noreferrer"">http://www.themacroscope.org/2.0/using-the-stanford-named-entity-recognizer-to-extract-data-from-texts</a> (Windows user).
Tried to do the same, but got a few errors in command line:</p>
<pre><code>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/slf4j/LoggerFacto
ry
        at edu.stanford.nlp.io.IOUtils.&lt;clinit&gt;(IOUtils.java:42)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(Abstrac
tSequenceClassifier.java:1484)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExcept
ions(AbstractSequenceClassifier.java:1497)
        at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3015)
Caused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory
        at java.net.URLClassLoader.findClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        ... 4 more
</code></pre>
<p>Can somebody explain what's wrong and how should I correct it?
Thank you!</p>
",Named Entity Recognition (NER),get output file stanford ner new stanford ner problem downloaded stanford named entity recognizer version work problem get tagged text output file read extracting data site window user tried got error command line somebody explain wrong correct thank
"rJava -- NLP, java heap error","<p>I'm trying to run the NLP and openNLP packages in R with rJava, and I keep getting a similar error: </p>

<pre><code>java.lang.OutOfMemoryError: Java heap space
</code></pre>

<p>I've already tried: </p>

<pre><code>options( java.parameters = ""-Xmx4g"" )
library( ""RWeka"" )
</code></pre>

<p>I've also tried increasing to 8g. Still running out of space and still crashing R. </p>

<p>I'm working on annotating a .txt file (a large one, yes, but just one file). I am trying to extract location names. Any ideas on how I can fix this issue?</p>

<p>Thanks! </p>
",Named Entity Recognition (NER),rjava nlp java heap error trying run nlp opennlp package r rjava keep getting similar error already tried also tried increasing g still running space still crashing r working annotating txt file large one yes one file trying extract location name idea fix issue thanks
Why does my NamedEntityAnnotator for date mentions differ from CoreNLP demo&#39;s output?,"<p>The date detected from my following program gets split into two separate mentions whereas the detected date in the NER output of <a href=""http://corenlp.run/"" rel=""nofollow noreferrer"">CoreNLP demo</a> is single as it should be. What should I edit in my program to correct this.</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, entitymentions"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

String text =  ""This software was released on Februrary 5, 2015."";
Annotation document = new Annotation(text);
pipeline.annotate(document);
List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

for(CoreMap sentence: sentences) {
      List&lt;CoreMap&gt; mentions = sentence.get(MentionsAnnotation.class);
      if (mentions != null) {
              for (CoreMap mention : mentions) {
                     System.out.println(""== Token="" + mention.get(TextAnnotation.class));
                     System.out.println(""NER="" + mention.get(NamedEntityTagAnnotation.class));
                     System.out.println(""Normalized NER="" + mention.get(NormalizedNamedEntityTagAnnotation.class));
              }
       }
}
</code></pre>

<p>Output from this program:</p>

<pre><code>== Token=Februrary 5,
NER=DATE
Normalized NER=****0205
== Token=2015
NER=DATE
Normalized NER=2015  
</code></pre>

<p>Output from CoreNLP online demo:
<a href=""https://i.sstatic.net/50oef.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/50oef.png"" alt=""enter image description here""></a></p>
",Named Entity Recognition (NER),doe namedentityannotator date mention differ corenlp demo output date detected following program get split two separate mention whereas detected date ner output corenlp demo single edit program correct output program output corenlp online demo
NLTK - Chunk grammar doesn&#39;t read commas,"<pre><code>from nltk.chunk.util import tagstr2tree
from nltk import word_tokenize, pos_tag
text = ""John Rose Center is very beautiful place and i want to go there with Barbara Palvin. Also there are stores like Adidas ,Nike ,Reebok Center.""
tagged_text = pos_tag(text.split())

grammar = ""NP:{&lt;NNP&gt;+}""

cp = nltk.RegexpParser(grammar)
result = cp.parse(tagged_text)

print(result)
</code></pre>

<p>Output:</p>

<pre><code>(S
  (NP John/NNP Rose/NNP Center/NNP)
  is/VBZ
  very/RB
  beautiful/JJ
  place/NN
  and/CC
  i/NN
  want/VBP
  to/TO
  go/VB
  there/RB
  with/IN
  (NP Barbara/NNP Palvin./NNP)
  Also/RB
  there/EX
  are/VBP
  stores/NNS
  like/IN
  (NP Adidas/NNP ,Nike/NNP ,Reebok/NNP Center./NNP))
</code></pre>

<p>The grammar i use for chunking only works on nnp tags but if words are sequential with commas they will still on the same line.I want my chunk like this:</p>

<pre><code>(S
  (NP John/NNP Rose/NNP Center/NNP)
  is/VBZ
  very/RB
  beautiful/JJ
  place/NN
  and/CC
  i/NN
  want/VBP
  to/TO
  go/VB
  there/RB
  with/IN
  (NP Barbara/NNP Palvin./NNP)
  Also/RB
  there/EX
  are/VBP
  stores/NNS
  like/IN
  (NP Adidas,/NNP)
  (NP Nike,/NNP)
  (NP Reebok/NNP Center./NNP))
</code></pre>

<p>What should i write in the ""grammar="" or can i edit the output like i wrote above?As you can see i only parse proper nouns for my named entity project pls help me out.</p>
",Named Entity Recognition (NER),nltk chunk grammar read comma output grammar use chunking work nnp tag word sequential comma still line want chunk like write grammar edit output like wrote see parse proper noun named entity project pls help
"Tag, extract phrases from free text using a custom vocabulary (python)?","<p>I have a custom vocabulary with approx. 1M rows in a SQL table. Each row has a UID and a corresponding phrase that can be many words in length. This table rarely changes.</p>

<p>I need tag, extract, chunk or recognize (NER ?) entity phrases in a free-text document against the above mentioned custom vocabulary. So for a phrase found in the free text, I can pull its UID. </p>

<p>It would be nice if partial matches and also phrase tokens appearing in a different order would be tagged / extracted according to some threshold / algorithm settings.</p>

<ul>
<li>Which NLP tool, preferably Python based, can make use of a custom vocabulary in its tagging, extraction, chunking or NER from free text ?</li>
<li>Knowing the goal is to extract phrases from free text - which format is best suited for this custom vocabulary to work with the NLP tool ? XML, JSON, trees, IOB chunks, other ?</li>
<li>Any tool to help transform the SQL table (original custom vocabulary)  into the format of the vocabulary the NLP algorithm requires to work with ?</li>
<li>Do I need integrate with other (non-pythonic) tools such as GATE, KEA, Lingpipe, Apache Stanbol or OpenNLP ?</li>
<li>Is there an API for both tagging / extracting and for creating a custom vocabulary ?</li>
<li>Any experience with RapidMiner or TextRazor ? Can these tools help with the above ?</li>
</ul>

<p>Thanks!</p>
",Named Entity Recognition (NER),tag extract phrase free text using custom vocabulary python custom vocabulary approx row sql table row ha uid corresponding phrase many word length table rarely change need tag extract chunk recognize ner entity phrase free text document mentioned custom vocabulary phrase found free text pull uid would nice partial match also phrase token appearing different order would tagged extracted according threshold algorithm setting nlp tool preferably python based make use custom vocabulary tagging extraction chunking ner free text knowing goal extract phrase free text format best suited custom vocabulary work nlp tool xml json tree iob chunk tool help transform sql table original custom vocabulary format vocabulary nlp algorithm requires work need integrate non pythonic tool gate kea lingpipe apache stanbol opennlp api tagging extracting creating custom vocabulary experience rapidminer textrazor tool help thanks
Name Entity Resolution Algorithm,"<p>I was trying to build an entity resolution system, where my entities are,</p>

<pre><code>(i) General named entities, that is organization, person, location,date, time, money, and percent.
(ii) Some other entities like, product, title of person like president,ceo, etc. 
(iii) Corefererred entities like, pronoun, determiner phrase,synonym, string match, demonstrative noun phrase, alias, apposition. 
</code></pre>

<p>From various literature and other references, I have defined its scope as I would not consider the ambiguity of each of the entity beyond its entity category. That is, I am taking Oxford of Oxford University
as different from Oxford as place, as the previous one is the first word of an organization entity and second one is the entity of location. </p>

<p>My task is to construct one resolution algorithm, where I would extract 
and resolve the entities. </p>

<p>So, I am working out an entity extractor in the first place. 
In the second place, if I try to relate the coreferences as I found from 
various literatures like this <a href=""http://anthology.aclweb.org/J/J01/J01-4004.pdf"" rel=""noreferrer"">seminal work</a>, they are trying to work out 
a decision tree based algorithm, with some features like, distance,
i-pronoun, j-pronoun, string match, definite noun
phrase, demonstrative noun phrase, number agreement feature,
semantic class agreement, gender agreement, both proper names, alias, apposition
etc. </p>

<p>The algorithm seems a nice one where enities are extracted with Hidden Markov Model(HMM).</p>

<p>I could work out one entity recognition system with HMM.
Now I am trying to work out a coreference as well as an entity
resolution system. I was trying to feel instead of using so many
features if I use an annotated corpus and train it directly with 
HMM based tagger, with a view to solve a relationship extraction like,</p>

<pre><code>*""Obama/PERS is/NA delivering/NA a/NA lecture/NA in/NA Washington/LOC, he/PPERS knew/NA it/NA was/NA going/NA to/NA be/NA
small/NA as/NA it/NA may/NA not/NA be/NA his/PoPERS speech/NA as/NA Mr. President/APPERS""

where, PERS-&gt; PERSON
       PPERS-&gt;PERSONAL PRONOUN TO PERSON
       PoPERS-&gt; POSSESSIVE PRONOUN TO PERSON
       APPERS-&gt; APPOSITIVE TO PERSON
       LOC-&gt; LOCATION
       NA-&gt; NOT AVAILABLE*
</code></pre>

<p>would I be wrong? I made an experiment with around 10,000 words. Early results seem
encouraging. With a support from one of my colleague I am trying to insert some
semantic information like,
PERSUSPOL, LOCCITUS, PoPERSM, etc. for PERSON OF US IN POLITICS, LOCATION CITY US, POSSESSIVE PERSON MALE, in the tagset to incorporate entity disambiguation at one go. My feeling relationship extraction would be much better now. 
Please see this new thought too. 
I got some good results with Naive Bayes classifier also where sentences
having predominately one set of keywords are marked as one class. </p>

<p>If any one may suggest any different approach, please feel free to suggest so.</p>

<p>I use Python2.x on MS-Windows and try to use libraries like NLTK, Scikit-learn, Gensim,
pandas, Numpy, Scipy etc. </p>

<p>Thanks in Advance.  </p>
",Named Entity Recognition (NER),name entity resolution algorithm wa trying build entity resolution system entity various literature reference defined scope would consider ambiguity entity beyond entity category taking oxford oxford university different oxford place previous one first word organization entity second one entity location task construct one resolution algorithm would extract resolve entity working entity extractor first place second place try relate coreference found various literature like seminal work trying work decision tree based algorithm feature like distance pronoun j pronoun string match definite noun phrase demonstrative noun phrase number agreement feature semantic class agreement gender agreement proper name alias apposition etc algorithm seems nice one enities extracted hidden markov model hmm could work one entity recognition system hmm trying work coreference well entity resolution system wa trying feel instead using many feature use annotated corpus train directly hmm based tagger view solve relationship extraction like would wrong made experiment around word early result seem encouraging support one colleague trying insert semantic information like persuspol loccitus popersm etc person u politics location city u possessive person male tagset incorporate entity disambiguation one go feeling relationship extraction would much better please see new thought got good result naive bayes classifier also sentence predominately one set keywords marked one class one may suggest different approach please feel free suggest use python x window try use library like nltk scikit learn gensim panda numpy scipy etc thanks advance
Named Entity Recognition (Ner) - Organization Name Database,"<p>I'm Working on my current graduating project which is Named Entity Recognition For Turkish. The recognizer should catch Turkish words when i work with Person Names and Locations (Sometimes locations can be in different Languages, for example Hilton Hotels in Taksim/Istanbul) all i need add ""Hotel"" in my dataset which is Full of specific location tags like Hotel , Restaurant or Mall. But when its come to Organization Name Tag. I need to find a good dataset of bands , products , company names, But cant figure out how to find or collect this dataset</p>

<p>In stanford nlp tool : <a href=""http://nlp.stanford.edu:8080/ner/process"" rel=""nofollow"">http://nlp.stanford.edu:8080/ner/process</a></p>

<p>When i type Facebook , Nike , Adidas etc it can find it's organization. So is there any way to have that organization name Dataset ? </p>
",Named Entity Recognition (NER),named entity recognition ner organization name database working current graduating project named entity recognition turkish recognizer catch turkish word work person name location sometimes location different language example hilton hotel taksim istanbul need add hotel dataset full specific location tag like hotel restaurant mall come organization name tag need find good dataset band product company name cant figure find collect dataset stanford nlp tool type facebook nike adidas etc find organization way organization name dataset
Spacy Natural Language Processing Pickle file issue,"<p>For the Spacy package, model files for deps, ner, and pos throw an invalid load key or EOF error when I try to load them using pickle.</p>

<p>I have executed the code on windows and linux systems. I don't think it is a binary mode transfer issue. I have checked it in detail. I am not able to figure out the issue. Most likely the file is corrupt but I am not sure. Is there a way it can be fixed using the hex editor?</p>

<p>Any help is highly appreciated. It will be great if someone can explain pickling in a bit detail.</p>

<p>Appreciate your help.</p>
",Named Entity Recognition (NER),spacy natural language processing pickle file issue spacy package model file deps ner po throw invalid load key eof error try load using pickle executed code window linux system think binary mode transfer issue checked detail able figure issue likely file corrupt sure way fixed using hex editor help highly appreciated great someone explain pickling bit detail appreciate help
MaxEnt classifier implementation in java for linguistic features?,"<p>I want to train a MaxEnt classifier over a training corpus. My features are syntactical and semantical like POS tags or NER tags and so on. Is there any MaxEnt classifier implementation in Java that support defining such features?</p>
",Named Entity Recognition (NER),maxent classifier implementation java linguistic feature want train maxent classifier training corpus feature syntactical semantical like po tag ner tag maxent classifier implementation java support defining feature
information extraction of NAMED ENTITIES python 2.7,"<p>I have a text that looks like:</p>

<pre><code>""&lt;ENAMEX TYPE=""PERSON""&gt;Edward R. Kimmel&lt;/ENAMEX&gt;, one of Admiral &lt;ENAMEX TYPE=""PERSON""&gt;Jack&lt;/ENAMEX&gt;'s two surviving sons and...""
</code></pre>

<p>I want an output as the following:</p>

<p>PERSON Edward R. Kimmel</p>

<p>PERSON Jack</p>

<p>Any idea using RegEX?</p>

<p>Thanks a lot</p>
",Named Entity Recognition (NER),information extraction named entity python text look like want output following person edward r kimmel person jack idea using regex thanks lot
Extract the names from raw text,"<p>I am working on a problem where i have a raw text in which there's a pattern like <code>Some Name (vs|v.) Some other name</code> and i want to extract those names i.e. </p>

<p>I TRIED </p>

<p><code>(first group) (vs|v.) (second group)</code> </p>

<p>using regex to catch the words on both sides of vs but that catches only a word and not the complete name and i don't know where to stop while extracting names on both the sides of <code>vs</code> as the names are pretty much the same as other text</p>

<p>Any help would be appreciated  </p>

<p>text is something like</p>

<pre><code>person concerned applies, the Assessing Officer has to issue an appropriate certificate [Cf. 
&lt;em&gt;L. Hirday Narain vs Income Tax Officer, 
&lt;/em&gt;(1970) 78 ITR 26(SC) ]. 
</code></pre>

<p>so here the regex should catch 
<code>L. Hirday Narain</code> and <code>Income Tax Officer</code></p>
",Named Entity Recognition (NER),extract name raw text working problem raw text pattern like want extract name e tried using regex catch word side v catch word complete name know stop extracting name side name pretty much text help would appreciated text something like regex catch
what is distant supervision?,"<p>According to my understanding, Distant Supervision is the process of specifying the concept which the individual words of a passage, usually a sentence, are trying to convey. </p>

<p>For example, a database maintains the structured relationship <code>concerns( NLP, this sentence).</code></p>

<p>Our distant supervision system would take as input the sentence: <code>""This is a sentence about NLP.""</code></p>

<p>Based on this sentence it would recognize the entities, since as a pre-processing step the sentence would have been passed through a named-entity recognizer, <code>NLP</code> &amp; <code>this sentence</code>. </p>

<p>Since our database has it that <code>NLP</code> and <code>this sentence</code> are related by the bond of <code>concern(s)</code> it would identify the input sentence as expressing the relationship <code>Concerns(NLP, this sentence)</code>. </p>

<p>My questions is two fold: </p>

<p>1) What is the use of that? Is it that later our system might see a sentence in ""the wild"" such as <code>That sentence is about OPP</code> and realize that it's seen something similar to that before and thereby realize the novel relationship such that <code>concerns(OPP, that sentence).</code>, based only on the words/ individual tokens?</p>

<p>2) Does it take into account the actual words of the sentence? The verb 'is' and the adverb 'about' for instance, realizing (through WordNet or some other hyponymy system) that this is somehow similar to the higher-order concept ""concerns""?</p>

<p>Does anyone have some code used to generate a distant supervision system that I could look at, i.e. a system that cross references a KB, such as Freebase, and a corpus, such as the NYTimes, and produces a distant supervision database? I think that would go a long way in clarifying my conception of distant supervision. </p>
",Named Entity Recognition (NER),distant supervision according understanding distant supervision process specifying concept individual word passage usually sentence trying convey example database maintains structured relationship distant supervision system would take input sentence based sentence would recognize entity since pre processing step sentence would passed named entity recognizer since database ha related bond would identify input sentence expressing relationship question two fold use later system might see sentence wild realize seen something similar thereby realize novel relationship based word individual token doe take account actual word sentence verb adverb instance realizing wordnet hyponymy system somehow similar higher order concept concern doe anyone code used generate distant supervision system could look e system cross reference kb freebase corpus nytimes produce distant supervision database think would go long way clarifying conception distant supervision
How to extract data from HTML-formatted EMail files via OpenNLP?,"<p>I am working on a project where I have emailed receipts from various courier agents. The emails are of HTML format. </p>

<p>But, they do not all form a specific structure. Each email is of different format. I tried jsoup to extract data, but its difficult to write the extraction for each specific type of html. I need to extract Name, from location, to location, organization and few other details from the mail. I tried openNLP, but it does not recognize all locations and names. It catches some of the locations if it is in a sentence form. </p>

<p>Can I create my own training data with html content in it, annotate them and train it to detect locations and names based on the html structure i have in the training data? </p>
",Named Entity Recognition (NER),extract data html formatted email file via opennlp working project emailed receipt various courier agent email html format form specific structure email different format tried jsoup extract data difficult write extraction specific type html need extract name location location organization detail mail tried opennlp doe recognize location name catch location sentence form create training data html content annotate train detect location name based html structure training data
TensorFlow with a NER-Tagger,"<p>I was wondering if there is any possibility to use Named-Entity-Recognition with a self trained model in tensorflow.</p>

<p>There is a word2vec implementation, but I could not find the 'classic' POS or NER tagger.</p>

<p>Thanks for your help!</p>
",Named Entity Recognition (NER),tensorflow ner tagger wa wondering possibility use named entity recognition self trained model tensorflow word vec implementation could find classic po ner tagger thanks help
Stanford NER in NLTK not tagging multiple sentences correctly - Python,"<p>I have a function which returns the named entities in a given body of text, using the Stanford NER. </p>

<pre><code>def get_named_entities(text):
    load_ner_files()

    print text[:100] # to show that the text is fine
    text_split = text.split()        
    print text_split # to show the split is working fine
    result = ""named entities = "", st.tag(text_split)
    return result
</code></pre>

<p>I am loading in the text from a url using the newspaper Python package. </p>

<pre><code>def get_page_text():
    url = ""https://aeon.co/essays/elon-musk-puts-his-case-for-a-multi-planet-civilisation""
    page = Article(url)
    page.download()
    page.parse() 
    return unicodedata.normalize('NFKD', page.text).encode('ascii', 'ignore')
</code></pre>

<p>However, when I run the function I get the following output: </p>

<pre><code>['Fuck', 'Earth!', 'Elon', 'Musk', 'said', 'to', 'me,', 'laughing.', 'Who', 'cares', 'about', 'Earth?'......... (continued)
named entities = [('Fuck', 'O'), ('Earth', 'O'), ('!', 'O')]
</code></pre>

<p>So my question is, why are only the first three words being tagged?</p>
",Named Entity Recognition (NER),stanford ner nltk tagging multiple sentence correctly python function return named entity given body text using stanford ner loading text url using newspaper python package however run function get following output question first three word tagged
Review of Name Entity Recognition NLTK,"<p>I am trying to create one entity recognition(NER) application, where I am trying to take Parts of Speech Tagging(PoS) approach.
I am trying to exploit Python's NLTK library, and using it as <code>hmm_tagger=nltk.HiddenMarkovModelTagger.train(train_set)</code>.
In train set, I am trying to give data in the format of Brown corpus's tagged_sents(). 
It is as below for PoS Tagging</p>

<pre><code>brown_a = nltk.corpus.brown.tagged_sents()[:2]
&gt;&gt;&gt; brown_a
[[(u'The', u'AT'), (u'Fulton', u'NP-TL'), (u'County', u'NN-TL'), (u'Grand', u'JJ-TL'), (u'Jury', u'NN-TL'), (u'said', u'VBD'), (u'Friday', u'NR'), (u'an', u'AT'), (u'investigation', u'NN'), (u'of', u'IN'), (u""Atlanta's"", u'NP$'), (u'recent', u'JJ'), (u'primary', u'NN'), (u'election', u'NN'), (u'produced', u'VBD'), (u'``', u'``'), (u'no', u'AT'), (u'evidence', u'NN'), (u""''"", u""''""), (u'that', u'CS'), (u'any', u'DTI'), (u'irregularities', u'NNS'), (u'took', u'VBD'), (u'place', u'NN'), (u'.', u'.')], [(u'The', u'AT'), (u'jury', u'NN'), (u'further', u'RBR'), (u'said', u'VBD'), (u'in', u'IN'), (u'term-end', u'NN'), (u'presentments', u'NNS'), (u'that', u'CS'), (u'the', u'AT'), (u'City', u'NN-TL'), (u'Executive', u'JJ-TL'), (u'Committee', u'NN-TL'), (u',', u','), (u'which', u'WDT'), (u'had', u'HVD'), (u'over-all', u'JJ'), (u'charge', u'NN'), (u'of', u'IN'), (u'the', u'AT'), (u'election', u'NN'), (u',', u','), (u'``', u'``'), (u'deserves', u'VBZ'), (u'the', u'AT'), (u'praise', u'NN'), (u'and', u'CC'), (u'thanks', u'NNS'), (u'of', u'IN'), (u'the', u'AT'), (u'City', u'NN-TL'), (u'of', u'IN-TL'), (u'Atlanta', u'NP-TL'), (u""''"", u""''""), (u'for', u'IN'), (u'the', u'AT'), (u'manner', u'NN'), (u'in', u'IN'), (u'which', u'WDT'), (u'the', u'AT'), (u'election', u'NN'), (u'was', u'BEDZ'), (u'conducted', u'VBN'), (u'.', u'.')]]
</code></pre>

<p>{Here size of brown_a we may increase. It is given only as an example.}</p>

<p>I am now trying to build an NER, where, I am changing above data as, </p>

<pre><code>[[(u'The', u'NameP'), (u'Fulton', u'Name'), (u'County', u'NameC'), (u'Grand', u'NameCC'), (u'Jury', u'NameCCC'), (u'said', u'VBD'), (u'Friday', u'NR'), (u'an', u'AT'), (u'investigation', u'NA'), (u'of', u'NA'), (u""Atlanta's"", u'Name'), (u'recent', u'NA'), (u'primary', u'NA'), (u'election', u'NA'), (u'produced', u'NA'), (u'``', u'NA'), (u'no', u'NA'), (u'evidence', u'NA'), (u""''"", u""NA""), (u'that', u'NA'), ...]
</code></pre>

<p>Here, I am keeping data format but changing the tagset to my definition as, NA for Not Available(anything which is not NE),
                                                                            NameP for Previous to Name,
                                                                            Name for Name,..etc.</p>

<p>I am now making this new data as training set and training.</p>

<p>Is my approach fine or do I need to change anything major?</p>

<p>Please suggest. </p>
",Named Entity Recognition (NER),review name entity recognition nltk trying create one entity recognition ner application trying take part speech tagging po approach trying exploit python nltk library using train set trying give data format brown corpus tagged sent po tagging size brown may increase given example trying build ner changing data keeping data format changing tagset definition na available anything ne namep previous name name name etc making new data training set training approach fine need change anything major please suggest
Need help in Web scraping webpages and its links by automatic funciton in R,"<p>I am interested to extract the data of paranormal activity reported in news, so that i can analyze the 
data of space and time of appearance for any correlations. This project is just for fun, to learn and use web scraping, text extraction and spatial and time correlation analysis. So please forgive me for deciding on this topic, I wanted to do something interesting and challenging work.
First I found this website has some collection of the reported paranormal incidences, they have collection for 2009,2010,2011 and 2012.
The structure of the website goes like this in every year they have 1..10 pages...and links goes like this
for year2009
link <a href=""http://paranormal.about.com/od/paranormalgeneralinfo/tp/2009-paranormal-activity.htm"" rel=""nofollow"">http://paranormal.about.com/od/paranormalgeneralinfo/tp/2009-paranormal-activity.htm</a></p>

<p>In each page they have collected the stories under the heading like this
Internal structure
Paranormal Activity, Posted 03-14-09
each of these head lines has two pages inside it..goes like this
link <a href=""http://paranormal.about.com/od/paranormalgeneralinfo/a/news_090314n.htm"" rel=""nofollow"">http://paranormal.about.com/od/paranormalgeneralinfo/a/news_090314n.htm</a></p>

<p>On each of these pages they have actual reported stories collected on various headlines..and the actual websites link for those stories. I am interested in collecting those reported text and extract information regarding the kind of paranormal activity like ghost, demon or UFOs and  the time, date and place of incidents. I wish to analyze this data for any spatial and time correlations. If UFO or Ghosts are real they must have some behavior and correlations in space or time in their movements. This is long shot of the story...</p>

<p><em><strong>I need help in web scraping the text form the above said pages. Here i have wrote down the code to follow one page and its link down to last final text i want. Can anyone let me know is there any better and efficient way to get the clean text from the final page. Also automation of the collecting text by following all 10 pages for whole 2009.</em></strong></p>

<pre><code>library(XML)
#source of paranormal news from about.com
#first page to start
#2009 -  http://paranormal.about.com/od/paranormalgeneralinfo/tp/2009-paranormal-activity.htm
pn.url&lt;-""http://paranormal.about.com/od/paranormalgeneralinfo/tp/2009-paranormal-activity.htm""
pn.html&lt;-htmlTreeParse(pn.url,useInternalNodes=T)
pn.h3=xpathSApply(pn.html,""//h3"",xmlValue)
#extracting the links of the headlines to follow to the story
pn.h3.links=xpathSApply(pn.html,""//h3/a/@href"")
#Extracted the links of the Internal structure to follow ...
#Paranormal Activity, Posted 01-03-09 (following this head line)
#http://paranormal.about.com/od/paranormalgeneralinfo/a/news_090314n.htm
pn.l1.url&lt;-pn.h3.links[1]
pn.l1.html&lt;-htmlTreeParse(pn.l1.url,useInternalNodes=T)
pn.l1.links=xpathSApply(pn.l1.html,""//p/a/@href"")
#Extracted the links of the Internal structure to follow ...
#British couple has 'black-and-white-twins' twice (following this head line)
#http://www.msnbc.msn.com/id/28471626/
pn.l1.f1.url=pn.l1.links[7]
pn.l1.f1.html=htmlTreeParse(pn.l1.f1.url,useInternalNodes=T)
pn.l1.f1.text=xpathSApply(pn.l1.f1.html,""//text()[not(ancestor::script)][not(ancestor::style)]"",xmlValue)
</code></pre>

<p>I sincerely thanks in advance for reading my post and your time for helping me.
I will be great full for any expert who would like to mentor me in this whole project.</p>

<p>Regards
Sathish</p>
",Named Entity Recognition (NER),need help web scraping webpage link automatic funciton r interested extract data paranormal activity reported news analyze data space time appearance correlation project fun learn use web scraping text extraction spatial time correlation analysis please forgive deciding topic wanted something interesting challenging work first found website ha collection reported paranormal incidence collection structure website go like every year page link go like year link page collected story heading like internal structure paranormal activity posted head line ha two page inside go like link page actual reported story collected various headline actual website link story interested collecting reported text extract information regarding kind paranormal activity like ghost demon ufo time date place incident wish analyze data spatial time correlation ufo ghost real must behavior correlation space time movement long shot story need help web scraping text form said page wrote code follow one page link last final text want anyone let know better efficient way get clean text final page also automation collecting text following page whole sincerely thanks advance reading post time helping great full expert would like whole project regard sathish
How can I extract tracking ID and Courier name from text message received from that company using java?,"<p>I have text messages in my inbox,i want to select the messages which are from any courier company and extract tracking Id and Courier company name from that message using java code for Android application</p>
",Named Entity Recognition (NER),extract tracking id courier name text message received company using java text message inbox want select message courier company extract tracking id courier company name message using java code android application
Named Entity recognition in Gate using LingPipe,"<p>I'm using <code>GATE NLP</code> to process my document, and I want to Use entity names to use as tag candidates 
In Gate there are <code>OpenNLP</code> and <code>LingPipe</code> 
as I read an answer form <a href=""https://stackoverflow.com/questions/1875765/how-to-define-persons-names-in-text-java?rq=1"">here</a> @Shashikant Kore answer he said </p>

<blockquote>
  <p>if you have the sentence ""My friend Joe Smith went to the Walmart
  store"", OpenNLP identifies two named entities - ""Joe Smith"" and
  ""Walmart"". I couldn't get it tag ""Joe Smith"" as Person and ""Walmart""
  as Organization.</p>
</blockquote>

<p>and suggests to use <code>LingPipe</code> so I used LingPipe that provided in Gate NLP 
like here</p>

<pre><code>SerialAnalyserController pipeline = (SerialAnalyserController) Factory.createResource(""gate.creole.SerialAnalyserController"");
        pipeline.add((ProcessingResource) Factory.createResource(""gate.lingpipe.TokenizerPR""));
        pipeline.add((ProcessingResource) Factory.createResource(""gate.lingpipe.NamedEntityRecognizerPR""));
        pipeline.add((ProcessingResource) Factory.createResource(""gate.lingpipe.POSTaggerPR""));
        pipeline.add((ProcessingResource) Factory.createResource(""gate.lingpipe.SentenceSplitterPR""));
        Corpus corpus = Factory.newCorpus(""SegmenterCorpus"");
        Document document = Factory.newDocument(handler.toString());
        corpus.add(document); 
        pipeline.setCorpus(corpus); 
        pipeline.execute();
</code></pre>

<p>However, when I run my program I have this Exception</p>

<blockquote>
  <p>Exception in thread ""main"" gate.creole.ResourceInstantiationException:
  No model file provided!   at
  gate.lingpipe.NamedEntityRecognizerPR.init(NamedEntityRecognizerPR.java:55)   at
  gate.lingpipe.NamedEntityRecognizerPR.init(NamedEntityRecognizerPR.java:55)</p>
</blockquote>

<p>whats the meaning by No model file provided ?? 
sorry because I'm asking this question but I'm totally new to this field 
and I just learn about ANNIE and it didn't need any file when I used it to extract the POS tagging 
any help?? </p>
",Named Entity Recognition (NER),named entity recognition gate using lingpipe using process document want use entity name use tag candidate gate read answer form suggests use used lingpipe provided gate nlp like however run program exception exception thread main gate creole resourceinstantiationexception model file provided gate lingpipe namedentityrecognizerpr init namedentityrecognizerpr java gate lingpipe namedentityrecognizerpr init namedentityrecognizerpr java whats meaning model file provided sorry asking question totally new field learn annie need file used extract po tagging help
"Train Stanford NER with big gazette, memory issue","<p>I have previously trained a german classifier using the Stanford NER and a training-file with 450.000 tokens. Because I had almost 20 classes, this took about 8 hours and I had to cut a lot of features short in the prop file.</p>

<p>I now have a gazette-file with 16.000.000 unique tagged tokens. I want to retrain my classifier under use of those tokens, but I keep running into memory issues. The gazette-txt is 386mb and mostly contains two-token objects (first + second name), all unique.</p>

<p>I have reduced the amount of classes to 5, reduced the amount of tokens in the gazette by 4 million and I've removed all the features listed on the Stanford NER FAQ-site from the prop-file but I still run into the out of memory: java heap space error. I have 16gb of ram and start the jvm with -mx15g -Xmx14g.</p>

<p>The error occurs about 5 hours into the process.</p>

<p>My problem is that I don't know how to further reduce the memory usage without arbitrarily deleting entries from the gazette. Does someone have further suggestions on how I could reduce my memory-usage?</p>

<p>My prop-file looks like this:</p>

<pre><code>trainFile = ....tsv
serializeTo = ...ser.gz
map = word=0,answer=1

useWordPairs=false
useNGrams=false
useClassFeature=true
useWord=true
noMidNGrams=true
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
useDisjunctive=true
saveFeatureIndexToDisk=true
qnSize=2
printFeatures=true
useObservedSequencesOnly=true

cleanGazette=true
gazette=....txt
</code></pre>

<p>Hopefully this isnt too troublesome. Thank you in advance!</p>
",Named Entity Recognition (NER),train stanford ner big gazette memory issue previously trained german classifier using stanford ner training file token almost class took hour cut lot feature short prop file gazette file unique tagged token want retrain classifier use token keep running memory issue gazette txt mb mostly contains two token object first second name unique reduced amount class reduced amount token gazette million removed feature listed stanford ner faq site prop file still run memory java heap space error gb ram start jvm mx g xmx g error occurs hour process problem know reduce memory usage without arbitrarily deleting entry gazette doe someone suggestion could reduce memory usage prop file look like hopefully isnt troublesome thank advance
How can I get a dataset annotated with Jobtitles?,"<p>I need it to do some entity extraction. How do I get an annotated dataset with <code>JobTitles</code>?</p>
",Named Entity Recognition (NER),get dataset annotated jobtitles need entity extraction get annotated dataset
How could I identify a sentence disclosing some specific information in a paragraph?,"<p>For example, I have such a paragraph as below:
The first sentence (bold and italic) is what I hope to identify out. 
The identification goal includes: 
1. whether this paragraph contain such disclosure. 
2. what this disclosure is.</p>

<p>The possible problems are :
1. this sentence may not be in the begin of the text string. it could be in any place of the given paragraph.
2. this sentence may vary with words but with same meaning. For example, it could also be expressed as: ""Sample provided for review"" or ""They sent to me an item for evaluation"" or something like this.</p>

<p>So how could I identify such disclosures ? Anyone's idea would be greatly appreciated. Thanks.</p>

<p>The paragraph:</p>

<p><strong><em>I was sent this Earbuds Audiophile headphones to review.</em></strong> I am just going to copy here the information from the site: ""High Definition Stereo Earphones with microphone Equipped with two 9mm high fidelity drivers, unique sound performance, well-balanced bass, mids and trebble. Designed specially for those who enjoy classic music, rock music, pop music, or gaming with superb quality sound. Let COR3 be your in ear sports earbuds. Replaceable Back Caps, inline controller and mic
Extreme flexible tangle free flat TPE cable including inline controller with universal microphone. Play/Pause your music or Answer/Hang up a call with a touch of a button right next to your hands, feature available depending on your device capability. COR3 should be your best gaming earbuds.
Extremely Comfortable</p>

<p>Methods I have tried: 
Up to now, my processing is very naive: 1) humanly labeled 1000 pieces of reviews as a binary variable (1 represents including the disclosure text, 0 otherwise). 2) Collect all the disclosure texts as a corpus denoted by DisclosureCor; 3) Based on these DisclosureCor, I discovered some basic regular regression rules, like "" review.* evaluation|test|opinion"". 4) Using these summarized rules to label new data. 5) The problem is that rules may not be complete, since they are just my own subject summarizations. Besides,  theses rules may not only occur in the disclosure text, but also some other parts in the review paragraphs, thus generating lots of noises (i.e. low precision); 6) I tried to use classification based association rules to train some rules from the labeled data. As keywords number is huge, long long time is needed to train the rule, crashed often. 7) I also tried to compare the similarity the review paragraph with the DisclosureCorp, but it's difficult to find a threshold to cut whether a review paragraph contains disclosure.  These are all the efforts I have tried, could you please give me some hints ? Thanks.</p>
",Named Entity Recognition (NER),could identify sentence disclosing specific information paragraph example paragraph first sentence bold italic hope identify identification goal includes whether paragraph contain disclosure disclosure possible problem sentence may begin text string could place given paragraph sentence may vary word meaning example could also expressed sample provided review sent item evaluation something like could identify disclosure anyone idea would greatly appreciated thanks paragraph wa sent earbuds audiophile headphone review going copy information site high definition stereo earphone microphone equipped two mm high fidelity driver unique sound performance well balanced bass mids trebble designed specially enjoy classic music rock music pop music gaming superb quality sound let cor ear sport earbuds replaceable back cap inline controller mic extreme flexible tangle free flat tpe cable including inline controller universal microphone play pause music answer hang call touch button right next hand feature available depending device capability cor best gaming earbuds extremely comfortable method tried processing naive humanly labeled piece review binary variable represents including disclosure text otherwise collect disclosure text corpus denoted disclosurecor based disclosurecor discovered basic regular regression rule like review evaluation test opinion using summarized rule label new data problem rule may complete since subject summarization besides thesis rule may occur disclosure text also part review paragraph thus generating lot noise e low precision tried use classification based association rule train rule labeled data keywords number huge long long time needed train rule crashed often also tried compare similarity review paragraph disclosurecorp difficult find threshold cut whether review paragraph contains disclosure effort tried could please give hint thanks
NLP : What are some common verbs surrounding organization names in text,"<p>I am trying to come up with some rules to detect named entities, specifically company or organization names in text. I think it makes sense to focus on verbs. There are a lot of <code>POS Taggers</code> that can easily detect proper nouns. I personally like <code>StanfordPOSTagger</code>. Now, once i have the proper noun, i know that it is a named entity. However, to be certain that it is the name of a company, i need to come up with rules and possibly <code>Gazetteers</code></p>

<p>I was thinking of focusing on verbs. Is there a set of common verbs that occur frequently around company names?</p>

<p>I could create an annotated corpus and explicitly train a Machine Learning classifier to predict such verbs, but that is a LOT of work. It would be great if someone has already done some research on this.</p>

<p>Additionally, can some other <code>POS</code> tags give clues? Not just verbs.</p>
",Named Entity Recognition (NER),nlp common verb surrounding organization name text trying come rule detect named entity specifically company organization name text think make sense focus verb lot easily detect proper noun personally like proper noun know named entity however certain name company need come rule possibly wa thinking focusing verb set common verb occur frequently around company name could create annotated corpus explicitly train machine learning classifier predict verb lot work would great someone ha already done research additionally tag give clue verb
"Decrypting SENNA Chunk, SRL and Parser Output","<p><a href=""http://ml.nec-labs.com/senna/"" rel=""nofollow noreferrer"">Senna</a> is a NLP tool built using neural nets and it's able to do:</p>

<ul>
<li>POS tagging</li>
<li>NER tagging</li>
<li>Chunk tagging</li>
<li>Semantic Role Label tagging and</li>
<li>Parsing </li>
</ul>

<p>After downloading the pre-compiled package from <a href=""http://ml.nec-labs.com/senna/download.html"" rel=""nofollow noreferrer"">http://ml.nec-labs.com/senna/download.html</a></p>

<p>I ran the <code>--help</code> menu and see what are the options:</p>

<pre><code>alvas@ubi:~/senna$ ./senna-linux64 --help
invalid argument: --help

SENNA Tagger (POS - CHK - NER - SRL)
(c) Ronan Collobert 2009

Usage: ./senna-linux64 [options]

 Takes sentence (one line per sentence) on stdin
 Outputs tags on stdout
 Typical usage: ./senna-linux64 [options] &lt; inputfile.txt &gt; outputfile.txt

Display options:
  -h             Display this help
  -verbose       Display model informations on stderr
  -notokentags   Do not output tokens
  -offsettags    Output start/end offset of each token
  -iobtags       Output IOB tags instead of IOBES
  -brackettags   Output 'bracket' tags instead of IOBES

Data options:
  -path &lt;path&gt;   Path to the SENNA data/ and hash/ directories [default: ./]

Input options:
  -usrtokens     Use user's tokens (space separated) instead of SENNA tokenizer

SRL options:
  -posvbs        Use POS verbs instead of SRL style verbs for SRL task
  -usrvbs &lt;file&gt; Use user's verbs (given in &lt;file&gt;) instead of SENNA verbs for SRL task

Tagging options:
  -pos           Output POS
  -chk           Output CHK
  -ner           Output NER
  -srl           Output SRL
  -psg           Output PSG
</code></pre>

<p>The command-line interface is straight forward and the outputs for POS and NER tags are also easy to interpret.</p>

<p>Given this input:</p>

<pre><code>alvas@ubi:~/senna$ cat test.in
Foo went to eat bar at the Foobar.
</code></pre>

<p>This is out standard Penn Treebank tagset:</p>

<pre><code>alvas@ubi:~/senna$ ./senna-linux64 -pos &lt; test.in
            Foo        NNP
           went        VBD
             to         TO
            eat         VB
            bar         NN
             at         IN
            the         DT
         Foobar        NNP
              .          .
</code></pre>

<p>And this is the <a href=""https://stackoverflow.com/questions/17116446/what-do-the-bilou-tags-mean-in-named-entity-recognition"">BIO tagset</a>:</p>

<pre><code>alvas@ubi:~/senna$ ./senna-linux64 -ner &lt; test.in
            Foo      S-PER
           went          O
             to          O
            eat          O
            bar          O
             at          O
            the          O
         Foobar      S-LOC
              .          O
</code></pre>

<p>And for the chunking it's also some sort of the <a href=""https://stackoverflow.com/questions/32333312/how-to-extract-chunks-from-bio-chunked-sentences-python"">BIOE tagset</a> we're used to:</p>

<pre><code>alvas@ubi:~/senna$ ./senna-linux64 -chk &lt; test.in
            Foo       S-NP
           went       B-VP
             to       I-VP
            eat       E-VP
            bar       S-NP
             at       S-PP
            the       B-NP
         Foobar       E-NP
              .          O
</code></pre>

<p><strong>But what does the <code>S-</code> tags mean?</strong> It seems like it's only attached to tokens that are single token chunks, is that true?</p>

<p>The SRL tags are a little weird, they are multiple-annotations per token:</p>

<pre><code>alvas@ubi:~/senna$ ./senna-linux64 -srl &lt; test.in
            Foo               -       S-A1        S-A0
           went            went        S-V           O
             to               -   B-AM-PNC           O
            eat             eat   I-AM-PNC         S-V
            bar               -   I-AM-PNC        S-A1
             at               -   I-AM-PNC    B-AM-LOC
            the               -   I-AM-PNC    I-AM-LOC
         Foobar               -   E-AM-PNC    E-AM-LOC
              .               -          O           O
</code></pre>

<p>The look like the ""tuple-like"" outputs we get from semantic frames but I don't understand the conventions, e.g. what is <code>-AM-</code>? what is <code>-PNC</code>?</p>

<p><strong>What does the output mean and how should we interpret it?</strong></p>

<p>And for the Parser output:</p>

<pre><code>alvas@ubi:~/senna$ ./senna-linux64 -psg &lt; test.in
            Foo (S1(S(NP*)
           went (VP*
             to (S(VP*
            eat (VP*
            bar (ADVP*)
             at (PP*
            the (NP*
         Foobar *))))))
              . *))
</code></pre>

<p>It looks like the <a href=""https://stackoverflow.com/questions/28704060/how-to-flatten-the-parse-tree-and-store-in-a-string-for-further-string-operation"">bracketed parse output we see in parsing</a> but <strong>what does the <code>*</code> mean?</strong></p>
",Named Entity Recognition (NER),decrypting senna chunk srl parser output senna nlp tool built using neural net able po tagging ner tagging chunk tagging semantic role label tagging parsing downloading pre compiled package ran menu see option command line interface straight forward output po ner tag also easy interpret given input standard penn treebank tagset used doe tag mean seems like attached token single token chunk true srl tag little weird multiple annotation per token look like tuple like output get semantic frame understand convention e g doe output mean interpret parser output look like href parse output see parsing doe mean
NLP : Is Gazetteer a cheat,"<p>In NLP there is a concept of <code>Gazetteer</code> which can be quite useful for creating annotations. As far as i understand, </p>

<p><code>A gazetteer consists of a set of lists containing names of entities such as cities, organisations, days of the week, etc. These lists are used to ﬁnd occurrences of these names in text, e.g. for the task of named entity recognition.</code></p>

<p>So it is essentially a lookup. Isn't this kind of a cheat? If we use a <code>Gazetteer</code> for detecting named entities, then there is not much <code>Natural Language Processing</code> going on. Ideally, i would want to detect named entities using <code>NLP</code> techniques. Otherwise how is it any better than a regex pattern matcher.</p>

<p>Does that make sense?</p>
",Named Entity Recognition (NER),nlp gazetteer cheat nlp concept quite useful creating annotation far understand essentially lookup kind cheat use detecting named entity much going ideally would want detect named entity using technique otherwise better regex pattern matcher doe make sense
How are StanfordNER Classifiers built,"<p>I am working with StanfordNER classifiers. There are 4 classifiers as</p>

<pre><code>english.all.3class.distsim.crf.ser.gz
english.muc.7class.distsim.crf.ser.gz
english.conll.4class.distsim.crf.ser.gz
example.serialized.ncc.ncc.ser.gz
</code></pre>

<p>How are these classifiers built? Since each of them is based on a different corpus, here is my guess</p>

<ol>
<li><p>Train a machine learning classifier like <code>SVM</code> coupled with <code>OVR</code> (for multi label case) on the corpus to detect entities like <code>ORGANIZATION</code>,<code>PERSON</code>,<code>LOCATION</code> etc. This means that the training data would be the entire text of a document in the corpus. For that piece of text we explicitly indicate the <code>ORGANIZATION</code>s,<code>PERSON</code>s and <code>LOCATION</code>s. Thus the classifiers would be able to predict those entities.</p></li>
<li><p>Train a machine learning classifier to link POS tags with entities like <code>ORGANIZATION</code>,<code>PERSON</code>,<code>LOCATION</code>. For example, a classifier can be trained to predict which proper nouns should be <code>ORGANIZATION</code></p></li>
</ol>

<p>Is this the correct big picture? I am just trying to work out how to build my own NER.</p>
",Named Entity Recognition (NER),stanfordner classifier built working stanfordner classifier classifier classifier built since based different corpus guess train machine learning classifier like coupled multi label case corpus detect entity like etc mean training data would entire text document corpus piece text explicitly indicate thus classifier would able predict entity train machine learning classifier link po tag entity like example classifier trained predict proper noun correct big picture trying work build ner
extract columns according to a list,"<p>I have a table df, which lists the frequency of 2000 words in 1000 document</p>

<pre><code>id   happy so  today    cut  song dad  may
 1    2      4     3     2    1    0    2 
 2    1      2     1     4    0    2    2
 3    0      2     1     1    2    0    3
</code></pre>

<p>I want to extract some columns (words) from the table according to a list like this:</p>

<pre><code>     Topic 1   Topic 2   
[1,] ""cut""     ""one""     
[2,] ""may""     ""day""     
[3,] ""song""    ""job""     
[4,] ""act""     ""start""   
[5,] ""control"" ""check""
</code></pre>

<p>extracting one column from a table is df$col, here the column names in df are the names in the list. The outcome would be like this:</p>

<pre><code>id    cut  may song 
 1     2     2   1    
 2     4     2   0
 3     1     3   2
</code></pre>
",Named Entity Recognition (NER),extract column according list table df list frequency word document want extract column word table according list like extracting one column table df col column name df name list outcome would like
How to avoid extracing non-proper nouns from headings in text with capitalization?,"<p>I am trying to extract keywords from a piece of text using <code>nltk</code> and <code>Stanford NLP</code> tools. After I run my code, I can get a list like this</p>

<pre><code>companyA
companyB
companyC
Trend Analysis For companyA
</code></pre>

<p>This is all good but notice the last item. That is actually a heading that appears in the text. Since all the words for a heading are capitalized, my program thinks that those are all proper nouns and thus clubs them together as if they were a big company name.</p>

<p>The good thing is that as long as a company has been mentioned somewhere in the text, my program will pick it up, hence I get individual items like <code>companyA</code> as well. These are coming from the actual piece of text that talks about that company.</p>

<p>Here is what I want to do.</p>

<p>In the list that I get above, is there a way to look at an item and determine if any previous items are a substring of the current one? For example, in this case when I come across</p>

<p><code>Trend Analysis For companyA</code></p>

<p>I can check whether I have seen any part of this before. So I can determine that I already have <code>companyA</code> and thus I will ignore <code>Trend Analysis For companyA</code>. <strong>I am confident that the text will mention any companies enough times for StanfordNER to pick it up</strong>. Thus I do not have to rely on headings to get what I need.</p>

<p>Does that make sense? Is this the correct approach? I am afraid that this will not be very efficient but i can't think of anything else.</p>

<p><strong>Edit</strong></p>

<p>Here is the code that i use</p>

<pre><code>sentences = nltk.sent_tokenize(document) 
sentences = [nltk.word_tokenize(sent) for sent in sentences] 
sentences = [nltk.pos_tag(sent) for sent in sentences] 
</code></pre>

<p>after that i simply use the <code>StanfordNERTagger</code> on each sentence</p>

<pre><code>result = []
stn = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') 
for s in sentences:
    taggedwords = stn.tag(s)
    for tag, chunk in groupby(taggedWords, lambda x:x[1]):

        if tag == ""ORGANIZATION"":

            result.append((tag, "" "".join(w for w, t in chunk)))

return result
</code></pre>

<p>in this way i can get all the <code>ORGANIZATION</code>s.</p>

<p>To @Alvas's point about Truecasing, dont you think its a bit of an overkill here? When i studied the algorithm, it appears to me that they are trying to come up with the most likely spelling for each word. The likelihood will be based on a corpus. I dont think that i will need to build a corpus as i can use a dictionary like <code>wordnet</code> or something like <code>pyenchant</code> to figure out the appropriate spelling. Also, here i already have all the information i need i.e. i am picking up all the companies mentioned.</p>

<p>There is another problem. Consider the company name</p>

<p><code>American Eagle Outfitters</code></p>

<p>note that <code>American</code> and <code>american</code> are both proper spellings. Simlarl for <code>Eagle</code> and <code>eagle</code>. I am afraid that even if i employ Truecasing into my algorithm, it will end up lowercasing terms that should not be lowercased.</p>

<p>Again, my problem right now is that i have all the company names extracted, but i am also extracting the headings. The brute force way wold be to perform a substring check on the list of results. I was just wondering whether there is a more efficient way of doing this. Moreover, i dont think that any tweaking that i do will improve the tagging. I dont think i will be able to outperform <code>StanfordNERTagger</code></p>
",Named Entity Recognition (NER),avoid extracing non proper noun heading text capitalization trying extract keywords piece text using tool run code get list like good notice last item actually heading appears text since word heading capitalized program think proper noun thus club together big company name good thing long company ha mentioned somewhere text program pick hence get individual item like well coming actual piece text talk company want list get way look item determine previous item substring current one example case come across check whether seen part determine already thus ignore confident text mention company enough time stanfordner pick thus rely heading get need doe make sense correct approach afraid efficient think anything else edit code use simply use sentence way get alvas point truecasing dont think bit overkill studied algorithm appears trying come likely spelling word likelihood based corpus dont think need build corpus use dictionary like something like figure appropriate spelling also already information need e picking company mentioned another problem consider company name note proper spelling simlarl afraid even employ truecasing algorithm end lowercasing term lowercased problem right company name extracted also extracting heading brute force way wold perform substring check list result wa wondering whether efficient way moreover dont think tweaking improve tagging dont think able outperform
Named entity recognition with NLTK or Stanford NER using custom corpus,"<p>I am trying to train a NER model in <strong>Indian</strong> with custom NE (named entity) dictionary for chunking. I refer to NLTK and Stanford NER repectively:</p>

<ol>
<li>NLTK</li>
</ol>

<p>I found the <code>nltk.chunk.named_entity.NEChunkParser</code> <a href=""http://www.nltk.org/api/nltk.chunk.html#module-nltk.chunk.named_entity"" rel=""nofollow noreferrer"">nechunkparser</a> able to train on a custom corpus. However, the format of training corpus was not specified in the documentation or the comment of the source code.</p>

<p>Where could I find some guide to the custom corpus for NER in NLTK?</p>

<ol start=""2"">
<li>Stanford NER</li>
</ol>

<p>According to the <a href=""https://stackoverflow.com/questions/11333903/nltk-named-entity-recognition-with-custom-data"">question</a>, the FAQ of Stanford NER gives direction of how to train a custom NER model. </p>

<p>One of the major concern is that default Stanford NER does not support Indian. So is it viable to feed an Indian NER corpus to the model?</p>
",Named Entity Recognition (NER),named entity recognition nltk stanford ner using custom corpus trying train ner model indian custom ne named entity dictionary chunking refer nltk stanford ner repectively nltk found nechunkparser able train custom corpus however format training corpus wa specified documentation comment source code could find guide custom corpus ner nltk stanford ner according href faq stanford ner give direction train custom ner model p one major concern default stanford ner doe support indian viable feed indian ner corpus model
Retrieve semantic predicate and named entity tag from nltk boxer,"<p>How to get the semantic predicate and named entity tag after using the nltk boxer?</p>

<pre><code>import nltk
x = nltk.sem.boxer.Boxer()
x.interpret(""The quick brown fox jumped."")
</code></pre>
",Named Entity Recognition (NER),retrieve semantic predicate named entity tag nltk boxer get semantic predicate named entity tag using nltk boxer
How do I form a feature vector for a classifier targeted at Named Entity Recognition?,"<p>I have a set of tags (different from the conventional Name, Place, Object etc.). In my case, they are domain-specific and I call them: Entity, Action, Incident. I want to use these as a seed for extracting more named-entities. </p>

<p>I came across this paper: ""<em>Efﬁcient Support Vector Classiﬁers for Named Entity Recognition</em>"" by Isozaki et al. While I like the idea of using Support Vector Machines for doing named-entity recognition, I am stuck on how to encode the feature vector. For their paper, this is what they say:</p>

<blockquote>
  <p>For instance, the words in “President George Herbert Bush said Clinton
  is . . . ” are classiﬁed as follows: “President” = OTHER,  “George” =
  PERSON-BEGIN, “Herbert” = PERSON-MIDDLE, “Bush” = PERSON-END, “said” =
  OTHER, “Clinton” = PERSON-SINGLE, “is”
  = OTHER. In this way, the ﬁrst word of a person’s name is labeled as PERSON-BEGIN. The last word is labeled as PERSON-END. Other words in
  the name are PERSON-MIDDLE. If a person’s name is expressed by a
  single word,  it is labeled as PERSON-SINGLE. If a word does not
  belong to any named entities, it is labeled as OTHER. Since IREX de-
  ﬁnes eight NE classes, words are classiﬁed into 33 categories. </p>
  
  <p>Each sample is represented by 15 features because  each word has three
  features (part-of-speech tag, character type, and the word itself),
  and two preceding words and two succeeding words are also used for
  context dependence. Although infrequent features are usually removed
  to prevent overﬁtting, we use all features because SVMs are robust.
  Each sample is represented by a long binary vector, i.e., a sequence
  of 0 (false) and 1 (true). For instance, “Bush” in the above example
  is represented by a vector x = x[1] ... x[D] described below. Only
  15 elements are 1.</p>
</blockquote>

<pre><code>x[1] = 0 // Current word is not ‘Alice’ 
x[2] = 1 // Current word is ‘Bush’ 
x[3] = 0 // Current word is not ‘Charlie’

x[15029] = 1 // Current POS is a proper noun 
x[15030] = 0 // Current POS is not a verb

x[39181] = 0 // Previous word is not ‘Henry’ 
x[39182] = 1 // Previous word is ‘Herbert
</code></pre>

<p>I don't really understand how the binary vector here is being constructed. I know I am missing a subtle point but can someone help me understand this?</p>
",Named Entity Recognition (NER),form feature vector classifier targeted named entity recognition set tag different conventional name place object etc case domain specific call entity action incident want use seed extracting named entity came across paper ef cient support vector classi er named entity recognition isozaki et al like idea using support vector machine named entity recognition stuck encode feature vector paper say instance word president george herbert bush said clinton classi ed follows president george person begin herbert person middle bush person end said clinton person single way rst word person name labeled person begin last word labeled person end word name person middle person name expressed single word labeled person single word doe belong named entity labeled since irex de ne eight ne class word classi ed category sample represented feature word ha three feature part speech tag character type word two preceding word two succeeding word also used context dependence although infrequent feature usually removed prevent tting use feature svms robust sample represented long binary vector e sequence false true instance bush example represented vector x x x described element really understand binary vector constructed know missing subtle point someone help understand
"How can Stanford CoreNLP Named Entity Recognition capture measurements like 5 inches, 5&quot;, 5 in., 5 in","<p>I'm looking to capture measurements using <a href=""http://stanfordnlp.github.io/CoreNLP/ner.html"" rel=""noreferrer"">Stanford CoreNLP</a>. (If you can suggest a different extractor, that is fine too.)</p>

<p>For example, I want to find <strong>15kg</strong>, <strong>15 kg</strong>, <strong>15.0 kg</strong>, <strong>15 kilogram</strong>, <strong>15 lbs</strong>, <strong>15 pounds</strong>, etc. But among CoreNLPs extraction rules, I don't see one for measurements.</p>

<p>Of course, I can do this with pure regexes, but toolkits can run more quickly, and they offer the opportunity to chunk at a higher level, e.g.  to treat <strong>gb</strong> and <strong>gigabytes</strong> together, and <strong>RAM</strong> and <strong>memory</strong> as building blocks--even without full syntactic parsing--as they build  bigger units  like <strong>128 gb RAM</strong> and <strong>8 gigabytes memory</strong>.</p>

<p>I want an  extractor for this that is  rule-based, not machine-learning-based), but don't see one as part of <a href=""http://nlp.stanford.edu/software/regexner/"" rel=""noreferrer"">RegexNer</a> or elsewhere. How do I go about this?</p>

<p><a href=""https://www-01.ibm.com/support/knowledgecenter/SSPT3X_2.1.1/com.ibm.swg.im.infosphere.biginsights.text.doc/doc/ana_txtan_NamedEntities.html"" rel=""noreferrer"">IBM Named Entity Extraction</a> can do this. The regexes are run in an efficient way rather than passing the text through each one. And the regexes are bundled to express meaningful entities, as for example one that unites all the measurement units into a single concept.</p>
",Named Entity Recognition (NER),stanford corenlp named entity recognition capture measurement like inch looking capture measurement using stanford corenlp suggest different extractor fine example want find kg kg kg kilogram lb pound etc among corenlps extraction rule see one measurement course pure regexes toolkits run quickly offer opportunity chunk higher level e g treat gb gigabyte together ram memory building block even without full syntactic parsing build bigger unit like gb ram gigabyte memory want extractor rule based machine learning based see one part regexner elsewhere go ibm named entity extraction regexes run efficient way rather passing text one regexes bundled express meaningful entity example one unites measurement unit single concept
Building NER using Sequence Alignment algorithms,"<p><strong>Background:</strong>
Wikipedia page on <a href=""https://en.wikipedia.org/wiki/Sequence_alignment#Non-biological_uses"" rel=""nofollow"">Sequence Alignment</a> says that DNA Sequence Alignment algorithms can also be used for Natural Language Processing.</p>

<p><strong>Question:</strong>
Because Named Entity Recognizer and DNA Sequence Libraries both do Approximate String Matching - is it <strong>practical</strong> to use a DNA Sequencing library (like <a href=""http://bowtie-bio.sourceforge.net/index.shtml"" rel=""nofollow"">Bowtie</a>) and build your NER?</p>

<p>One reason to NOT use existing NER open sources but rather use a DNA Sequencing library to build NER is to hopefully get 'misspelling correction' automatically in my NER.</p>

<p>If my supposition above makes sense - is there some online DNA Sequencing tool where I can input my database of celebrity names rather than DNA sequences, and try to search a misspelling 'Michale Jacksun' in DNA Sequencing tool in a hope that it matches that with 'Michael Jackson' from input database</p>
",Named Entity Recognition (NER),building ner using sequence alignment algorithm background wikipedia page sequence alignment say dna sequence alignment algorithm also used natural language processing question named entity recognizer dna sequence library approximate string matching practical use dna sequencing library like bowtie build ner one reason use existing ner open source rather use dna sequencing library build ner hopefully get misspelling correction automatically ner supposition make sense online dna sequencing tool input database celebrity name rather dna sequence try search misspelling michale jacksun dna sequencing tool hope match michael jackson input database
integrating Elasticsearch &amp; Stanford NLP without re-indexing,"<p>We've been using Elasticsearch in the system. Although i used its analyzers and queries. I didn't do deep into its indexing. as of now, i don't know how far ES lets us work the Lucene (inverted-)indexes it has in its shards. </p>

<p>We're now looking at a range of NLP features-- NER for one thing
and Stanford NLP appealed. </p>

<p>There's no plug-in to work these 2 packages together(?)</p>

<p>I haven't had a deep look into Stanford NLP. however - as far as i saw, 
it's working 
it all on its own indexes. whichever object or type passed to it, 
Stanford NLP is indexing it itself and going from there. </p>

<p>This would make the system work 2 different indexes for the same set of documents-- 
those of ES &amp; StanfordNLP, and this would be costly. </p>

<p>Is there a way to get around this? </p>

<p>One scenario i have is: make StanfordNLP work on Lucene segments-- the inverted indexes that ES already built. 
In this case: </p>

<p>1.) does StanfordNLP use Lucene indexes without re-indexing anything for itself? i don't know StanfordNLP's indexing structure-- or even how far it uses/doesn't use Lucene. </p>

<p>2.) are there any restrictions on using the Lucene indexes in ES shards? would we hit a rock bottom in using these Lucene segments directly as is, bypassing ES in between? </p>

<p>I'm trying to put things together-- all in the air for now. sorry for the naive Q. </p>

<p>I'm aware of OpenNLP and its plug-in. i haven't checked - i'm guessing it wouldn't be ""double-indexing"" and using ES's indexes(?)
However, it's StanfordNLP we're after. </p>

<p>TIA.  </p>
",Named Entity Recognition (NER),integrating elasticsearch stanford nlp without indexing using elasticsearch system although used analyzer query deep indexing know far e let u work lucene inverted index ha shard looking range nlp feature ner one thing stanford nlp appealed plug work package together deep look stanford nlp however far saw working index whichever object type passed stanford nlp indexing going would make system work different index set document e stanfordnlp would costly way get around one scenario make stanfordnlp work lucene segment inverted index e already built case doe stanfordnlp use lucene index without indexing anything know stanfordnlp indexing structure even far us use lucene restriction using lucene index e shard would hit rock bottom using lucene segment directly bypassing e trying put thing together air sorry naive q aware opennlp plug checked guessing double indexing using e index however stanfordnlp tia
Named Entity Recognition Python,"<p>What I want to do: Extract all the occurrences of n consecutive words that all begin with a capital letter.</p>

<pre><code>Input: (""Does John Doe eat pizza in New York?"", 2)
Output: [(""Does"", ""John""),(""John"", ""Doe"")(""New"",""York"")]

Input: (""Does John Doe eat pizza in New York?"", 3)
Output: [(""Does"", ""John"",""Doe"")]
</code></pre>

<p>Here is what I have come up with so far:  </p>

<pre><code># create text file
fw = open(""ngram.txt"", ""w"")
fw.write (""Does John Doe eat pizza in New York?"")
fw.close()

def UpperCaseNGrams (file,n):
    fr = open (file, ""r"")
    text = fr.read().split()

    ngramlist = [text[word:word+n] for word in range(len(text)-(n-1)) if word[0].isupper() if word+n[0].isupper()]  
    return ngramlist

print (UpperCaseNGrams(""ngram.txt"",2))
</code></pre>

<p>I get the following error:<br>
TypeError: 'int' object is not subscriptable</p>

<p>What do I have to change in order for it to work?  </p>
",Named Entity Recognition (NER),named entity recognition python want extract occurrence n consecutive word begin capital letter come far get following error typeerror int object subscriptable change order work
"Directions on how to index words and annotate with their type (entity, etc) and then Elasticsearch/w.e. returns these words with the annotations?","<p>I'm trying to build a very simple NLP chat (I could even say pseudo-NLP?), where I want to identify a fixed subset of intentions (verbs, sentiments) and entities (products, etc)</p>

<p>It's a kind of entity identification or named-entity recognition, but I'm not sure I need a full fledged NER solution for what I want to achieve. <strong>I don't care if the person types cars instead of car. HE HAS to type the EXACT word.</strong> So no need to deal with language stuff here.</p>

<p><strong>It doesn't need to identity and classify the words, I'm just looking for a way that when I search a phrase, it returns all results that contains each word of if.</strong></p>

<p>I want to index something like:</p>

<pre><code>want [type: intent]
buy [type: intent]
computer [type: entity]
car [type: entity]
</code></pre>

<p>Then the user will type:</p>

<blockquote>
  <p>I want to buy a car.</p>
</blockquote>

<p>Then I send this phrase to ElasticSearch/Solr/w.e. and it should return me something like below (it doesn't have to be structured like that, but each word should come with its type):</p>

<pre><code>[
    {""word"":""want"", ""type:""intent""},
    {""word"":""buy"", ""type"":""intent""},
    {""word"":""car"",""type"":""car""}
]
</code></pre>

<hr>

<p>The approach I came with was Indexing each word as:</p>

<pre><code>{
    ""word"": ""car"",
    ""type"": ""entity""
}
{
    ""word"": ""buy"",
    ""type"": ""intent""
}
</code></pre>

<p>And then I provide the whole phrase, searching by ""word"". But I had no success so far, because Elastic Search doesn't return any of the words, even although phrases contains words that are indexed.</p>

<p><strong>Any insights/ideas/tips to keep this using one of the main search engines?</strong></p>

<p><strong>If I do need to use a dedicated NER solution</strong>, what would be the approach to annotate words like this, <strong>without the need to worry about fixing typos, multi-languages, etc?</strong> I want to return results only if the person types the intents and entities exactly as they are, so not an advanced NLP solution.</p>

<p>Curiously I didn't find much about this on google.</p>
",Named Entity Recognition (NER),direction index word annotate type entity etc elasticsearch w e return word annotation trying build simple nlp chat could even say pseudo nlp want identify fixed subset intention verb sentiment entity product etc kind entity identification named entity recognition sure need full fledged ner solution want achieve care person type car instead car ha type exact word need deal language stuff need identity classify word looking way search phrase return result contains word want index something like user type want buy car send phrase elasticsearch solr w e return something like structured like word come type approach came wa indexing word provide whole phrase searching word success far elastic search return word even although phrase contains word indexed insight idea tip keep using one main search engine need use dedicated ner solution would approach annotate word like without need worry fixing typo multi language etc want return result person type intent entity exactly advanced nlp solution curiously find much google
Named-entity recognition: How to tag the training set and chose the algorithm?,"<p>For text that contains company names I want to train a model that automatically tags contractors (company executing the task) and principals (company hiring the contractor).</p>

<p>An example sentence would be:</p>

<blockquote>
  <p>Blossom Inc. hires the consultants of Big Think to develop an outsourcing strategy.</p>
</blockquote>

<p>with <em>Blossom Inc</em> as the principal and <em>Big Think</em> as the contractor.</p>

<p>My first question: <strong>Is it enough to tag only the principals and contractors in my training set or is it better to additionally use POS-tagging?</strong></p>

<p>In other words, either</p>

<blockquote>
  <p>Blossom/PRINCIPAL Inc./PRINCIPAL hires/NN the/NN consultants/NN of/NN Big/CONTRACTOR Think/CONTRACTOR to/NN develop/NN an/NN outsourcing/NN strategy/NN ./.</p>
</blockquote>

<p>or</p>

<blockquote>
  <p>Blossom/PRINCIPAL Inc./PRINCIPAL hires/VBZ the/DT consultants/NNS of/IN Big/CONTRACTOR Think/CONTRACTOR to/TO develop/VB an/DT outsourcing/NN strategy/NN ./.</p>
</blockquote>

<p>Second question: <strong>Once I have my training set, which algorithm(s) of the nltk-package is/are most promising?</strong> N-Gram Tagger, Brill Tagger, TnT Tagger, Maxent Classifier, Naive Bayes, ...? Or am I completely on the wrong track here?</p>

<p>I am new to NLP and I just wanted to ask for advice before I invest a lot of time in tagging my training set. And my text is in German, which might add some difficulties... Thanks for any advice!</p>
",Named Entity Recognition (NER),named entity recognition tag training set chose algorithm text contains company name want train model automatically tag contractor company executing task principal company hiring contractor example sentence would blossom inc hire consultant big think develop outsourcing strategy blossom inc principal big think contractor first question enough tag principal contractor training set better additionally use po tagging word either blossom principal inc principal hire nn nn consultant nn nn big contractor think contractor nn develop nn nn outsourcing nn strategy nn blossom principal inc principal hire vbz dt consultant nns big contractor think contractor develop vb dt outsourcing nn strategy nn second question training set algorithm nltk package promising n gram tagger brill tagger tnt tagger maxent classifier naive bayes completely wrong track new nlp wanted ask advice invest lot time tagging training set text german might add difficulty thanks advice
NLTK : combining stanford tagger and personal tagger,"<p>The goal of my project is to answer queries such as, for example:
""I am looking for American women between 20 and 30 years old who work in Google""
I then have to process the query and to look into a DB to find the answer.</p>

<p>For this, I would need to combine the Stanford 3-class NERTagger and my own tagger. Indeed, my NER tagger can tag ages, nationalities and gender. But I need the Stanford tagger to tag organizations as I don't have any training file for this.</p>

<p>Right now, I have a code like this:</p>

<pre><code>def __init__(self, q):
    self.userQuery = q
def get_tagged_tokens(self):
    st = NERTagger('C:\stanford-ner-2015-01-30\my-ner-model.ser.gz','C:\stanford-ner-2015-01-30\stanford-ner.jar')
    result = st.tag(self.userQuery.split())[0]
    return result
</code></pre>

<p>And I would like to have something like this:</p>

<pre><code>def get_tagged_tokens(self):
    st = NERTagger('C:\stanford-ner-2015-01-30\my-ner-model.ser.gz','C:\stanford-ner-2015-01-30\stanford-ner.jar')
    st_def = NERTagger('C:\stanford-ner-2015-01-30\classifiers\english.all.3class.distsim.crf.ser.gz','C:\stanford-ner-2015-01-30\stanford-ner.jar')
    tagger = BackoffTagger([st, st_def])
    result = st.tag(self.userQuery.split())[0]
    return result
</code></pre>

<p>This would mean that the tagger first uses my tagger and then the stanford one to tag untagged words. </p>

<p>Is it possible to combine my model with the Stanford model just to tag organizations? If yes, what is the best way to perform this?</p>

<p>Thank you!</p>
",Named Entity Recognition (NER),nltk combining stanford tagger personal tagger goal project answer query example looking american woman year old work google process query look db find answer would need combine stanford class nertagger tagger indeed ner tagger tag age nationality gender need stanford tagger tag organization training file right code like would like something like would mean tagger first us tagger stanford one tag untagged word possible combine model stanford model tag organization yes best way perform thank
How can I get only heading names.from the text file,"<p>I have a Text file as below:</p>

<pre><code>Education: 

askdjbnakjfbuisbrkjsbvxcnbvfiuregifuksbkvjb.iasgiufdsegiyvskjdfbsldfgd

Technical skills : 
 java,j2ee etc.,

work done: 

oaugafiuadgkfjwgeuyrfvskjdfviysdvfhsdf,aviysdvwuyevfahjvshgcsvdfs,bvisdhvfhjsvjdfvshjdvhfjvxjhfvhjsdbvfkjsbdkfg
</code></pre>

<p>I would like to extract only the heading names such as Education,Technical Skills etc.</p>

<p>the code is  : </p>

<pre><code>with open(""aks.txt"") as infile, open(""fffm"",'w') as outfile:
    copy = False
    for line in infile:
        if line.strip() == ""Technical Skills"":
            copy =True
        elif line.strip() == ""Workdone"":
            copy = True


        elif line.strip() ==  ""Education"":
            copy = False
        elif copy:
            outfile.write(line)
        fh = open(""fffm.txt"", 'r')
        contents = fh.read()
        len(contents)
</code></pre>
",Named Entity Recognition (NER),get heading name text file text file would like extract heading name education technical skill etc code
Ignore text inside XML elements when parsing text with Stanford CoreNLP,"<p>I'd like to use Stanford CoreNLP to analyze the text content of XML files.</p>

<p>Here's an example of the kind of XML element I'm analyzing:</p>

<p><code>&lt;cmd&gt;In the new plug-in directory, add a &lt;filepath&gt;cfg/catalog.xml&lt;/filepath&gt; file that specifies the custom XSLT style sheets.&lt;/cmd&gt;
</code></p>

<p>One thing I'd like to check is whether a <code>&lt;cmd&gt;</code> element contains more than one sentence. Now, if I extract the text content of the <code>&lt;cmd&gt;</code> element above, the result is this:</p>

<blockquote>
  <p>In the new plug-in directory, add a cfg/catalog.xml file that specifies the custom XSLT style sheets.</p>
</blockquote>

<p>If I give that piece of text to Stanford CoreNLP, it thinks the text has two sentences because of the dot in <code>cfg/catalog.xml</code>, even though it's really just one sentence.</p>

<p>In this particular example, I could probably just omit the content of the <code>&lt;filepath&gt;</code> element when analyzing the text and it'd work well enough, but that's not necessarily always the case.</p>

<p>Any suggestions on how to best approach this problem on a general level? I guess I'm looking for a way to either ignore the content of <code>&lt;filepath&gt;</code> and similar elements for certain purposes or somehow force them to be recognized as named entities, if that makes any sense.</p>
",Named Entity Recognition (NER),ignore text inside xml element parsing text stanford corenlp like use stanford corenlp analyze text content xml file example kind xml element analyzing one thing like check whether element contains one sentence extract text content element result new plug directory add cfg catalog xml file specifies custom xslt style sheet give piece text stanford corenlp think text ha two sentence dot even though really one sentence particular example could probably omit content element analyzing text work well enough necessarily always case suggestion best approach problem general level guess looking way either ignore content similar element certain purpose somehow force recognized named entity make sense
Automatic whois data parsing,"<p>I need to parse WHOIS raw data records into fields.
There is no one consistent format for the raw data, and I need to support all the possible formats (there are ~ 40 unique formats that I know of).
For examples, here are excerpts from 3 different WHOIS raw data records:</p>

<pre><code>Created on: 2007-01-04
Updated on: 2014-01-29
Expires on: 2015-01-04
Registrant Name: 0,75 DI VALENTINO ROSSI
Contact: 0,75 Di Valentino Rossi
Registrant Address: Via Garibaldi 22
Registrant City: Pradalunga
Registrant Postal Code: 24020
Registrant Country: IT
Administrative Contact Organization: Giorgio Valoti
Administrative Contact Name: Giorgio Valoti
Administrative Contact Address: Via S. Lucia 2
Administrative Contact City: Pradalunga
Administrative Contact Postal Code: 24020
Administrative Contact Country: IT
Administrative Contact Email: giorgio_v@mac.com
Administrative Contact Tel: +39 340 4050596
---------------------------------------------------------------
Registrant :
onse telecom corporation
Gangdong-gu Sangil-dong, Seoul

Administrative Contact :
onse telecom corporation    ruhisashi@onsetel.co.kr
Gangdong-gu Sangil-dong, Seoul, 
07079976571

Record created on 19-Jul-2004 EDT.
Record expires on 19-Jul-2015 EDT.
Record last updated on 15-Jul-2014 EDT.
---------------------------------------------------------------
Registrant:

Name:                markaviva comunica??o Ltda
Organization:        markaviva comunica??o Ltda
E-mail:              helissonmaia@markaviva.com.br
Address:             RUA FERNANDES LIMA 360 sala 03
Address:             57300070
Address:             ARAPIRACA - AL
Phone:               55 11 40039011
Country:             BRASIL
Created:             20130405
Updated:             20130405

Administrative Contact:

Name:                markaviva comunica??o Ltda
Organization:        markaviva comunica??o Ltda
E-mail:              helissonmaia@markaviva.com.br
Address:             RUA FERNANDES LIMA 360 sala 03
Address:             57300070
Address:             ARAPIRACA - AL
Phone:               55 11 40039011
Country:             BRASIL
Created:             20130405
Updated:             20130405
</code></pre>

<p>As you can see, there's no repeating pattern.
I need to extract fields such as 'Registrant Name', 'Registrant Address', 'Admin Name', 'Admin City', etc...</p>

<p>I first tried a basic method of field extraction, based on splitting the line on the first colon found, but it only works when the row prefixes are distinct, injective (no 2 rows with the same prefix exists) and, well, separated by a colon... (which is not always the case)</p>

<p>Now, I could go over the formats one by one and try to come up with a regex for each one of them, but that would require a lot of time, which I don't have.
I wonder if there's any way to automatically mine and treat blocks of text as a context-based ""chunk"" (with regards to their spacing and common repeating words such as 'registrant' or 'admin') and analyze them accordingly. NLP Maybe?</p>

<p>I'll be glad to hear any ideas, as I'm kind of stumped here.
Thanks</p>
",Named Entity Recognition (NER),automatic whois data parsing need parse whois raw data record field one consistent format raw data need support possible format unique format know example excerpt different whois raw data record see repeating pattern need extract field registrant name registrant address admin name admin city etc first tried basic method field extraction based splitting line first colon found work row prefix distinct injective row prefix exists well separated colon always case could go format one one try come regex one would require lot time wonder way automatically mine treat block text context based chunk regard spacing common repeating word registrant admin analyze accordingly nlp maybe glad hear idea kind stumped thanks
How to get the stanford NER plugin working with GATE?,"<p>I am new to GATE (version 8.0) and trying to get the Stanford NER working as part of the processing pipeline. </p>

<p>If I just do a search for Stanford in the plugins I don't see it:</p>

<p><img src=""https://i.sstatic.net/6WlSj.png"" alt=""enter image description here""></p>

<p>I'm finding conflicting information online about how to add it in. </p>

<p>I know that there is <a href=""http://sourceforge.net/p/gate/code/HEAD/tree/gate/trunk/plugins/Stanford_CoreNLP/src/gate/stanford/NER.java"" rel=""nofollow noreferrer"">this</a> plugin file. I also see that there is a StanfordNER plugin <a href=""https://gate.ac.uk/sale/tao/splitch23.html#sec:misc:creole:stanford"" rel=""nofollow noreferrer"">listed</a> in the GATE docs. I saw this on a stanford <a href=""https://mailman.stanford.edu/pipermail/java-nlp-user/2012-March/001918.html"" rel=""nofollow noreferrer"">mailing list</a>. Plus I saw this old post from stackoverflow: <a href=""https://stackoverflow.com/questions/27423778/stanford-corenlp-plugin-for-gate"">Stanford_CoreNLP plugin for gate</a></p>

<p>How do I get Stanford NER installed and working with GATE? </p>
",Named Entity Recognition (NER),get stanford ner plugin working gate new gate version trying get stanford ner working part processing pipeline search stanford plugins see finding conflicting information online add know plugin file also see stanfordner plugin listed gate doc saw stanford mailing list plus saw old post stackoverflow href plugin gate get stanford ner installed working gate
Get begin poisitions and/or NER from words after parsing,"<p>I am using the new Stanford CoreNLP NN parser. Here's a simplified version of the code:</p>

<pre><code>// Sentence to be parsed
String sentence = ""This is an example sentence."";

// This is where we store the result from the parser. Initially set to ""null"".
GrammaticalStructure gs = null;

// Parse the sentence
DocumentPreprocessor tokenizer = new DocumentPreprocessor(new StringReader(sentence));
List&lt;TaggedWord&gt; tagged = null;
for (List&lt;HasWord&gt; sent : tokenizer) {
    tagged = tagger.tagSentence(sent);
    gs = parser.predict(tagged);
}

// Convert the GrammaticalStructure object (the parsing result) into a semantic graph
SemanticGraph semanticGraph = SemanticGraphFactory.generateUncollapsedDependencies(gs);
</code></pre>

<p>Now, when I iterate over the vertices of <code>semanticGraph</code>, I can get the POS tag, but I can't get the NER of the word nor the begin position. So, when I do this:</p>

<pre><code>for (IndexedWord vertex : new ArrayList&lt;&gt;(semanticGraph.vertexSet())){
    String tag = vertex.tag();
    String ner = vertex.ner();
    int beginPosition = vertex.beginPosition();
}
</code></pre>

<p>for <code>tag</code> I get the POS tag correctly, for <code>ner</code> I get <code>null</code> and for <code>beginPostion</code> I always get -1. </p>

<p>How can I do the parsing with correctly preserving the begin position of the word in the original string? And if possible, how do I get the NER? (<code>beginPosition</code> is actually more important in my case)</p>
",Named Entity Recognition (NER),get begin poisitions ner word parsing using new stanford corenlp nn parser simplified version code iterate vertex get po tag get ner word begin position get po tag correctly get always get parsing correctly preserving begin position word original string possible get ner actually important case
Using multiple classifiers in a Java program,"<p>I am using Stanford named entity recognition system  to identify named entities in my queries.
I discover that  one of the classifier (english.all.3class.distsim.crf.ser.gz) identify Person named entity more than the other (english.muc.7class.distsim.crf.ser.gz). While the second classifier identify Organization named entity more than the first classifier. 
The question is how do i modify my code to combine both the performance of 3class and 7class classifiers. I mean how to combine line 2 and 3.  Below is my program </p>

<pre><code>public void main () {
    //String serializedClassifier = ""classifiers/english.all.3class.distsim.crf.ser.gz"";
    String serializedClassifier = ""classifiers/english.muc.7class.distsim.crf.ser.gz"";
    AbstractSequenceClassifier&lt;CoreLabel&gt; classifier = CRFClassifier.getClassifierNoExceptions                                  (serializedClassifier);
    //String s5 = ""Access Team Microsoft"";
    String s5 = "" Victor Vianu"";
    String ans4 = classifier.classifyToString(s5);
    System.out.println(ans4);
}
</code></pre>
",Named Entity Recognition (NER),using multiple classifier java program using stanford named entity recognition system identify named entity query discover one classifier english class distsim crf ser gz identify person named entity english muc class distsim crf ser gz second classifier identify organization named entity first classifier question modify code combine performance class class classifier mean combine line program
how to represent gazetteers or dictionaries as features in crf++?,"<p>how to use gazetteers or dictionaries as features in <a href=""https://taku910.github.io/crfpp/"" rel=""nofollow"">CRF++</a>?</p>

<p>To elaborate: suppose I want to do NER on person names, and I am having a gazetteer (or dictionary) containing commonly seen person names, I want to use this gazetteer as an input to crf++, how can I do that? </p>

<p>I am using the conditional random field package crf++ to perform named entity recognition tasks.
I know how to represent some commonly used features in crf++. For example, if we want to use Capitalization as a feature, we can add one separate column in the feature template of crf indicating if a word is capitalized or not.   </p>
",Named Entity Recognition (NER),represent gazetteer dictionary feature crf use gazetteer dictionary feature crf elaborate suppose want ner person name gazetteer dictionary containing commonly seen person name want use gazetteer input crf using conditional random field package crf perform named entity recognition task know represent commonly used feature crf example want use capitalization feature add one separate column feature template crf indicating word capitalized
Could I define entity type automatically?,"<p>I am trying to develop software to get suitable attributes for entities names depending on entity type.</p>

<p>For example if I have entities such doctor, nurse, employee , customer, patient , lecturer , donor, user, developer, designer, driver, passenger and technician, they all will have attributes such as name, sex, date of birth , email address, home address and telephone number because all of them are people.</p>

<p>Second example word such as university, college, hospital, hotel and supermarket can share attributes such as name, address and telephone number because all of them could be organization.</p>

<p>Are there any Natural Language Processing tools and software could help me to achieve my goal. 
I need to identify entity type as person or origination then I attached  suitable attributes according to the entity type?</p>

<p>I have looked at Name Entity Recognition (NER) tool such as Stanford Name Entity recognizer which can extract Entity such as Person, Location, Organization, Money, time, Date and Percent But it was not really useful.</p>

<p>I can do it by building my own gazetteer however I do not prefer to go to this option unless I failed to do it automatically. </p>

<p>Any helps, suggestions and ideas will be appreciated.   </p>
",Named Entity Recognition (NER),could define entity type automatically trying develop software get suitable attribute entity name depending entity type example entity doctor nurse employee customer patient lecturer donor user developer designer driver passenger technician attribute name sex date birth email address home address telephone number people second example word university college hospital hotel supermarket share attribute name address telephone number could organization natural language processing tool software could help achieve goal need identify entity type person origination attached suitable attribute according entity type looked name entity recognition ner tool stanford name entity recognizer extract entity person location organization money time date percent wa really useful building gazetteer however prefer go option unless failed automatically help suggestion idea appreciated
"From of list of strings, identify which are human names and which are not","<p>I have a vector like the one below and would like to determine which elements in the list are human names and which are not. I found the humaniformat package, which formats names but unfortunately does not determine if a string is in fact a name. I also found a few packages for entity extraction, but they seem to require actual text for part-of-speech tagging, rather than a single name. </p>

<p><strong>Example</strong></p>

<pre><code>pkd.names.quotes &lt;- c(""Mr. Rick Deckard"", # Name
                      ""Do Androids Dream of Electric Sheep"", # Not a name
                      ""Roy Batty"", # Name 
                      ""How much is an electric ostrich?"", # Not a name
                      ""My schedule for today lists a six-hour self-accusatory depression."", # Not a name
                      ""Upon him the contempt of three planets descended."", # Not a name
                      ""J.F. Sebastian"", # Name
                      ""Harry Bryant"", # Name
                      ""goat class"", # Not a name
                      ""Holden, Dave"", # Name
                      ""Leon Kowalski"", # Name
                      ""Dr. Eldon Tyrell"") # Name
</code></pre>
",Named Entity Recognition (NER),list string identify human name vector like one would like determine element list human name found humaniformat package format name unfortunately doe determine string fact name also found package entity extraction seem require actual text part speech tagging rather single name example
Is Word2Vec and Glove vectors are suited for Entity Recognition?,"<p>I am working on Named Entity Recognition. I evaluated libraries, such as MITIE, Stanford NER , NLTK NER etc., which are built upon conventional nlp techniques. I also looked at deep learning models such as word2vec and Glove vectors for representing words in vector space, they are interesting since they provide the information about the context of a word, but specifically for the task of NER, I think its not well suited. Since all these vector models create a vocab and corresponding vector representation. If any word failed to be in the vocabulary it will not be recognised. Assuming that it is highly likely that a named entity is not present since they are not bound by the language. It can be anything. So if any deep learning technique have to be useful in such cases are the ones which are more dependent on the structure of the sentence by using standard english vocab i.e. ignoring named fields. Is there any such model or method available? Will CNN or RNN may be the answer for it ?</p>
",Named Entity Recognition (NER),word vec glove vector suited entity recognition working named entity recognition evaluated library mitie stanford ner nltk ner etc built upon conventional nlp technique also looked deep learning model word vec glove vector representing word vector space interesting since provide information context word specifically task ner think well suited since vector model create vocab corresponding vector representation word failed vocabulary recognised assuming highly likely named entity present since bound language anything deep learning technique useful case one dependent structure sentence using standard english vocab e ignoring named field model method available cnn rnn may answer
How to extract key phrases from a given text with OpenNLP?,"<p>I'm using Apache OpenNLP and i'd like to extract the Keyphrases of a given text. I'm already gathering entities - but i would like to have Keyphrases.</p>

<p>The problem i have is that i can't use TF-IDF cause i don't have models for that and i only have a single text (not multiple documents)</p>

<p>Here is some code (prototyped - not so clean)</p>

<pre><code> public List&lt;KeywordsModel&gt; extractKeywords(String text, NLPProvider pipeline) {

        SentenceDetectorME sentenceDetector = new SentenceDetectorME(pipeline.getSentencedetecto(""en""));
        TokenizerME tokenizer = new TokenizerME(pipeline.getTokenizer(""en""));
        POSTaggerME posTagger = new POSTaggerME(pipeline.getPosmodel(""en""));
        ChunkerME chunker = new ChunkerME(pipeline.getChunker(""en""));

        ArrayList&lt;String&gt; stopwords = pipeline.getStopwords(""en"");

        Span[] sentSpans = sentenceDetector.sentPosDetect(text);
        Map&lt;String, Float&gt; results = new LinkedHashMap&lt;&gt;();
        SortedMap&lt;String, Float&gt; sortedData = new TreeMap(new MapSort.FloatValueComparer(results));

        float sentenceCounter = sentSpans.length;
        float prominenceVal = 0;
        int sentences = sentSpans.length;
        for (Span sentSpan : sentSpans) {
            prominenceVal = sentenceCounter / sentences;
            sentenceCounter--;
            String sentence = sentSpan.getCoveredText(text).toString();
            int start = sentSpan.getStart();
            Span[] tokSpans = tokenizer.tokenizePos(sentence);
            String[] tokens = new String[tokSpans.length];
            for (int i = 0; i &lt; tokens.length; i++) {
                tokens[i] = tokSpans[i].getCoveredText(sentence).toString();
            }
            String[] tags = posTagger.tag(tokens);
            Span[] chunks = chunker.chunkAsSpans(tokens, tags);
            for (Span chunk : chunks) {
                if (""NP"".equals(chunk.getType())) {
                    int npstart = start + tokSpans[chunk.getStart()].getStart();
                    int npend = start + tokSpans[chunk.getEnd() - 1].getEnd();
                    String potentialKey = text.substring(npstart, npend);
                    if (!results.containsKey(potentialKey)) {
                        boolean hasStopWord = false;
                        String[] pKeys = potentialKey.split(""\\s+"");
                        if (pKeys.length &lt; 3) {
                            for (String pKey : pKeys) {
                                for (String stopword : stopwords) {
                                    if (pKey.toLowerCase().matches(stopword)) {
                                        hasStopWord = true;
                                        break;
                                    }
                                }
                                if (hasStopWord == true) {
                                    break;
                                }
                            }
                        }else{
                            hasStopWord=true;
                        }
                        if (hasStopWord == false) {
                            int count = StringUtils.countMatches(text, potentialKey);
                            results.put(potentialKey, (float) (Math.log(count) / 100) + (float)(prominenceVal/5));
                        }
                    }
                }
            }
        }
        sortedData.putAll(results);
        System.out.println(sortedData);
        return null;
    }
</code></pre>

<p>What it basically does is giving me the Nouns back and sorting them by prominence value (where is it in the text?) and counts. </p>

<p>But honestly - this doesn't work soo good. </p>

<p>I also tried it with lucene analyzer but the results were also not so good.</p>

<p>So - how can i achieve what i want to do? I already know of KEA/Maui-indexer etc (but i'm afraid i can't use them because of GPL :( )</p>

<hr>

<p>Also interesting? Which other algorithms can i use instead of TF-IDF?</p>

<p>Example:</p>

<p>This text: <a href=""http://techcrunch.com/2015/09/04/etsys-pulling-the-plug-on-grand-st-at-the-end-of-this-month/"" rel=""nofollow"">http://techcrunch.com/2015/09/04/etsys-pulling-the-plug-on-grand-st-at-the-end-of-this-month/</a></p>

<p>Good output in my opinion: Etsy, Grand St., solar chargers, maker marketplace, tech hardware</p>
",Named Entity Recognition (NER),extract key phrase given text opennlp using apache opennlp like extract keyphrases given text already gathering entity would like keyphrases problem use tf idf cause model single text multiple document code prototyped clean basically doe giving noun back sorting prominence value text count honestly work soo good also tried lucene analyzer result also good achieve want already know kea maui indexer etc afraid use gpl also interesting algorithm use instead tf idf example text good output opinion etsy grand st solar charger maker marketplace tech hardware
"Extracting City, State and Country from Raw address string","<p>Given a raw string input </p>

<pre><code>1600 Divisadero St
San Francisco, CA 94115
b/t Post St &amp; Sutter St 
Lower Pacific Heights
</code></pre>

<p>I want to extract</p>

<p>City:<code>San Francisco</code><br/>
state:<code>California</code> or <code>CA</code><br/>
Country:<code>USA</code></p>

<p>I'll be parsing millions of addresses and using a Paid API is not feasible</p>

<p>I'm planning to use a <strong>Named Entity Recognizer</strong> but i'm unable to find a vast quantity of training data to ideally cover any location</p>

<p>Is there an opensource project out there which i may use?</p>
",Named Entity Recognition (NER),extracting city state country raw address string given raw string input want extract city state country parsing million address using paid api feasible planning use named entity recognizer unable find vast quantity training data ideally cover location opensource project may use
Adding custom features to Stanford NER without touching the source code,"<p>I have added custom features to my Stanford NER model as suggested in the following link:</p>

<p><a href=""https://stackoverflow.com/questions/22950995/stanford-ner-customization-to-classify-software-programming-keywords"">Stanford-NER customization to classify software programming keywords</a></p>

<p>I was wondering, Is there any better approach at this movement to add custom features without modifying source code?</p>

<p>Thanks in advance.</p>
",Named Entity Recognition (NER),adding custom feature stanford ner without touching source code added custom feature stanford ner model suggested following link href customization classify software programming keywords wa wondering better approach movement add custom feature without modifying source code thanks advance
"Information Extraction from a document, with not much training set","<ul>
<li><strong>What I want to do:</strong>
Extract basic biographic information from a text document. (Relation Extraction to be specific)</li>
<li><strong>Explanation:</strong>
I have n text documents containing biographies of n different people. I want to extract information corresponding to their names, age, qualifications, affiliations and interests.</li>
<li><strong>What I was able to do:</strong>
I used Stanford NER to extract name, age and organization in some cases. However, there were many False Positives as well as False Negatives-specially for ""organization"" tag.</li>
<li><strong>Why is it difficult:</strong>
As it is a biographic document, it contains text associated with the concerned person. I can't use the other documents for training my classifier as things will be totally different for another person.
Yes, I surely can write some rules. However, that is restricting my domain considerably. For example, I wrote rules to extract qualification..simple ones being: if any of the degrees (in my pre-specified dictionary) is present in a sentence, I can extract entities from that sentence and try to find a relation. </li>
<li><strong>My question:</strong>
Is there any way of making this task automatic? Since we are analyzing just one document each time, please don't suggest me to use bootstrapping based approaches. I tried learning patterns from collecting specific sentences from each document and then applying bootstrap-based algorithms like Snowball, but failed miserably.
I am aware that parsing might help me over here, I am trying to learn some patterns from the dependency parse of specific sentences..however I am not really sure how to proceed with it.
I thought of applying distant supervision learning, however that too requires a large dataset.</li>
<li><strong>Personal Take (till now):</strong>
Such problem could be solved by rule-based approaches augmented by parsing-based methods. However, I am not yet able to incorporate probabilistic or statistical model to generalize it for different types of biographies.</li>
</ul>

<p>PS: I want to change the latter sentence of my ""Personal Take"". Hence, seeking help.</p>

<p>An example:<br>
Document containing following text:<br>
Tim obtained his PhD from Stanford University in 2010. He did his bachelor (hons) from Massachusetts Institute of Technology in 2004. Currently, he is working in ABC company.  </p>

<p>Should extract facts in the form: [Entity1, Relation, Entity2]<br>
Ex: [Tim, affiliation-PhD, Stanford University],<br>
[He(Resolved to Tim), affiliation-bachelor(hons), Massachusetts Institute of Technology] and<br>
[He(Resolved t Tim), affiliation-works, ABC]</p>
",Named Entity Recognition (NER),information extraction document much training set want extract basic biographic information text document relation extraction specific explanation n text document containing biography n different people want extract information corresponding name age qualification affiliation interest wa able used stanford ner extract name age organization case however many false positive well false negative specially organization tag difficult biographic document contains text associated concerned person use document training classifier thing totally different another person yes surely write rule however restricting domain considerably example wrote rule extract qualification simple one degree pre specified dictionary present sentence extract entity sentence try find relation question way making task automatic since analyzing one document time please suggest use bootstrapping based approach tried learning pattern collecting specific sentence document applying bootstrap based algorithm like snowball failed miserably aware parsing might help trying learn pattern dependency parse specific sentence however really sure proceed thought applying distant supervision learning however requires large dataset personal take till problem could solved rule based approach augmented parsing based method however yet able incorporate probabilistic statistical model generalize different type biography p want change latter sentence personal take hence seeking help example document containing following text tim obtained phd stanford university bachelor hons massachusetts institute technology currently working abc company extract fact form entity relation entity ex tim affiliation phd stanford university resolved tim affiliation bachelor hons massachusetts institute technology resolved tim affiliation work abc
Issue training own model using Stanford NLP Regexner feature,"<p>We have a requirement to extract proper nouns like person, location and organization from unstructured document types (e.g.invoice).</p>

<p>We tried below things:</p>

<ol>
<li><p>Use Stanford NER classifiers to extract data. The 7 class classifier is giving better results compared to other classifiers, however the accuracy achieved is too less.</p></li>
<li><p>As recommended, we tried to use caseless classifier for unstructured document type, the accuracy is little higher than earlier, but is still not good enough.</p></li>
<li><p>We tried to train the model using Stanford NER feature, however that meant training the non noun entities, as others.</p></li>
<li><p>Therefore, to avoid training non noun entities as others, we decided to train our own data model using Stanford core NLP Regexner feature. However,  some trained entities are tagged as Others. Eg. Italy trained as location is tagged as Others in some cases. Please help.</p></li>
</ol>

<p>Sample Input:</p>

<pre><code>Sham Ventures
Unit no. 1833, Burj Tower
Plot No. 12 A
Sheikh Zayed Street
P.O.Box 486095
Dubai - U.A.E.
Tel. : +971-4-XXXXXX
Fax : +971-4-XXXXXXX
APPLICATION FOR COLLECTION OF BILLS
DATE: MARCH 14, 2015
TO: TEST BANK 
    DUBAI, UAE
    ATTN: BILLS DEPARTMENT
COLLECTING BANK
DUMMY BANK LIMITED
PLOT 455 Park Avenue Street
P.O.Box 1234
Lagos, Nigeria
TENOR OF DRAFT
60 DAYS FROM B/L DATE MARCH 14, 2015
AMOUNT OF DRAFT
US$90,450.00(US DOLLARS NINETY THOUSAND FOUR HUNDRED FIFTY ONLY)
NAME &amp; ADDRESS OF DRAWER
Sham Ventures
Unit no. 1833, Burj Tower
Plot No. 12 A
Sheikh Zayed Street
P.O.Box 486095
Dubai - U.A.E.
NAME &amp; ADDRESS OF DRAWEE(Buyer)
SUNSHINE MERCANTILE CO LIMITED
PLOT NO. 456 Park Avenue Street
Lagos, Nigeria
PLEASE FOLLOW INSTRUCTIONS MARKED X
DELIVER DOCUMENTS AGAINST PAYMENT ACCEPTANCE
NON-PAYMENT PROTEST NO PRTEST ADVISE BY MAIL ADVISE BY CABLE
INTEREST AT 6% PER ANNUM FROM B/L DATE MARCH 15,2015 to JULY 16, 2015
DO NOT SEND CHASERS FOR NON-ACCEPTANCE OR NON PAYMENT
PLEASE CONTACT AKBAR AT TELEPHONE NO. +971XXXXXXXX FOR INSTRUCTIONS, STAMP &amp; AUTHORISED SIGNATORY(S)
ENQUIRY REGARDING THESE DRAFTS AND OR DOCUMENTS
</code></pre>

<p>We are expecting following proper nouns to be extracted:
Organization- Sham Ventures,  TEST BANK, DUMMY BANK LIMITED,  SUNSHINE MERCANTILE CO LIMITED</p>

<p>Location- Dubai,  U.A.E, UAE, Lagos, Nigeria</p>

<p>Person- AKBAR</p>

<p>Output extracted using Stanford NER 7 class caseless model is following:</p>

<p>Location- Dubai,  Lagos, Nigeria
Organization- U.A.E, UAE</p>

<p>So, we trained both false positives and false negatives ( following data) to improve the accuracy of entities extraction:</p>

<p>False negatives- </p>

<p>Organization-Sham Ventures,  TEST BANK, DUMMY BANK LIMITED, SUNSHINE MERCANTILE CO LIMITED</p>

<p>Person- AKBAR</p>

<p>False positives-</p>

<p>Location- U.A.E, UAE. </p>

<p>Output after training data-</p>

<p>Person-AKBAR</p>

<p>Organization- DUMMY BANK LIMITED, Sham Ventures,  SUNSHINE MERCANTILE CO LIMITED, TEST BANK</p>

<p>Location-U.A.E, UAE, Lagos, Nigeria, US</p>

<p>Here, Dubai is missing from the results.</p>

<p>Due to such instances, the resultant accuracy for documents with large number of proper nouns  is low</p>

<p>Also we are seeing lot of false positives, for example,</p>

<p>For input,</p>

<p>Yours sincerely
Bosch Packaging Services AG</p>

<p>Output is </p>

<p>Organization- Yours sincerely Bosch Packaging Services AG</p>

<p>Also almost all abbreviations are tagged as Organization using Stanford. </p>

<p>Is there a way out to handle such false positives so that we only get the desired output?</p>

<p>Please suggest.</p>

<p>Thanks in advance.</p>

<p>Arti</p>
",Named Entity Recognition (NER),issue training model using stanford nlp regexner feature requirement extract proper noun like person location organization unstructured document type e g invoice tried thing use stanford ner classifier extract data class classifier giving better result compared classifier however accuracy achieved le recommended tried use caseless classifier unstructured document type accuracy little higher earlier still good enough tried train model using stanford ner feature however meant training non noun entity others therefore avoid training non noun entity others decided train data model using stanford core nlp regexner feature however trained entity tagged others eg italy trained location tagged others case please help sample input expecting following proper noun extracted organization sham test bank dummy bank limited sunshine mercantile co limited location dubai u e uae lagos nigeria person akbar output extracted using stanford ner class caseless model following location dubai lagos nigeria organization u e uae trained false positive false negative following data improve accuracy entity extraction false negative organization sham test bank dummy bank limited sunshine mercantile co limited person akbar false positive location u e uae output training data person akbar organization dummy bank limited sham sunshine mercantile co limited test bank location u e uae lagos nigeria u dubai missing result due instance resultant accuracy document large number proper noun low also seeing lot false positive example input sincerely bosch packaging service ag output organization sincerely bosch packaging service ag also almost abbreviation tagged organization using stanford way handle false positive get desired output please suggest thanks advance arti
How to train Name model in OpenNLP?,"<p>Iam trying to train the name finder model to detect the Names but it is not giving proper result.
Here is  the <strong>Code</strong>.</p>

<pre><code>protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);
        InputStream is=null;
        Resources resources=this.getResources();
        assetManager=resources.getAssets();

        String trainingDataFile = ""en-ner-person.train"";
        String outputModelFile = ""en-ner-person.bin"";
        String sentence[] = {""Sunil"", ""61 years old , will join the board as a nonexecutive director Nov. 29"" };

        train(trainingDataFile, outputModelFile, ""person"");
        try {
            predict(sentence, outputModelFile);
        }
        catch(Exception e)
        {
            System.out.println(""Errror Preditct"" + e.getMessage());
        }
    }

    private static void train(String trainingDataFile, String outputModelFile, String tagToFind) {

        NameSampleDataStream nss = null;
        try {
            nss = new NameSampleDataStream(new PlainTextByLineStream(new java.io.FileReader(trainingDataFile)));
        } catch (Exception e) {}

        TokenNameFinderModel model = null;

        try {

            model = NameFinderME.train(""en"", tagToFind, nss, Collections.&lt;String, Object&gt;emptyMap());
        } catch(Exception e) {}

        try {
            File outFile = new File(outputModelFile);
            FileOutputStream outFileStream = new FileOutputStream(outFile);
            model.serialize(outFileStream);
        }
        catch (Exception ex) {}
    }

    private  void predict(String sentence[], String modelFile) throws Exception {
        InputStream is1 ;
        is1 = assetManager.open(""en-ner-person.bin"",MODE_PRIVATE);

        TokenNameFinderModel model1 = new TokenNameFinderModel(is1);

        String sd;
        NameFinderME nameFinder = new NameFinderME(model1);
        Span sp[] = nameFinder.find(sentence);

        String a[] = Span.spansToStrings(sp, sentence);
        StringBuilder fd = new StringBuilder();
        int l = a.length;

        for (int j = 0; j &lt; l; j++) {
            fd = fd.append(a[j] + ""\n"");

        }
        sd = fd.toString();
        Log.d(""Name Detected:"", sd);

    }



}
</code></pre>

<p>Here is <strong>Output</strong> iam getting:</p>

<p>D:\Name Detected: [ 07-20 19:35:47.516  8799: 8799 I/Adreno-EGL ]</p>

<p>Content Of <strong>en-ner-person.train</strong> is:</p>

<pre><code>&lt;START:person&gt; Sunil &lt;END&gt; , 61 years old , will join the board as a nonexecutive director Nov. 29 .
</code></pre>

<p>Kindly help.</p>
",Named Entity Recognition (NER),train name model opennlp iam trying train name finder model detect name giving proper result code output iam getting name detected adreno egl content en ner person train kindly help
How to realize Named entity recognition with OpenNLP for the Albanian language?,"<p>I am trying out OpenNLP for Albanian language. For this I am using OPenNLP and trying to build models for person, location and organisation entity recognition in Albanian language. 
I am building my self the corpus, but I need an Open NLP expert to confirm me the below doubts:
1- Should I build a separated corpus for each model, e.g. for the ner-person build a corpus where only   tags are present?
2- Is it possible to label person, location and organization in teh same corpus and use it to train a single model able to extract all of teh three entity types?
3- is there a resource where I can find more on the algorithm used from OpenNLP Name finder module?</p>

<p>Thanks for a reply, I really need your support for my thesis </p>
",Named Entity Recognition (NER),realize named entity recognition opennlp albanian language trying opennlp albanian language using opennlp trying build model person location organisation entity recognition albanian language building self corpus need open nlp expert confirm doubt build separated corpus model e g ner person build corpus tag present possible label person location organization teh corpus use train single model able extract teh three entity type resource find algorithm used opennlp name finder module thanks reply really need support thesis
How to perform information extraction from news paper articles with a NLP library?,"<p>I am doing university final year project, My task is to extract causalities from news containing crime. I need to extract also location of crime. I used OPENNLP name entity recognizer to extract location. I trained a model for location names and its working fine for my scenario till now. Now I am searching some thing(a way OR a library) to extract causality. Here is a snap shot of a news which i am using and the Bold and Italic text is that which I want to extract. Need help for a library for this purpose same like OPENNLP, or any thing which can be useful or me in order to perform this task.</p>

<blockquote>
  <p>News 1:</p>
  
  <p>KARACHI: At least <strong><em>12 people were gunned down</em></strong> in the city on Monday, two of them
      apparently killed in sectarian attacks and one of the other victims a Muttahida Qaumi 
      Movement activist. </p>
  
  <p>News 2:</p>
  
  <p>KARACHI: Police on Tuesday <strong><em>arrested three accused</em></strong> in different raids at Gulistan-e-Jauhar and Brigade areas, Geo News reported.</p>
  
  <p>News 3: </p>
  
  <p>KARACHI: <strong><em>Five members of a family were found dead</em></strong> inside their house in Baldia Town here on Monday, Geo News reported.</p>
  
  <p>News 4:</p>
  
  <p>KARACHI: Sindh Rangers in their continued targeted operation in the city last night <strong><em>rounded up eight professional criminals</em></strong> and recovered weapons from them, Geo News reported.</p>
</blockquote>
",Named Entity Recognition (NER),perform information extraction news paper article nlp library university final year project task extract causality news containing crime need extract also location crime used opennlp name entity recognizer extract location trained model location name working fine scenario till searching thing way library extract causality snap shot news using bold italic text want extract need help library purpose like opennlp thing useful order perform task news karachi least people gunned city monday two apparently killed sectarian attack one victim muttahida qaumi movement activist news karachi police tuesday arrested three accused different raid gulistan e jauhar brigade area geo news reported news karachi five member family found dead inside house baldia town monday geo news reported news karachi sindh ranger targeted operation city last night rounded eight professional criminal recovered weapon geo news reported
Named Entity Recognition IOB annotation transformation,"<p>How can an IOB (Intermediate, Other, Begin) annotation format like ""John/B-PERSON Doe/I_PERSON..."" be transformed into some other formats that can be digested in Java?</p>

<p>Could not figure it out from the documentation of Stanford NLP related classes: IOBUtils and  CoNLLDocumentReaderAndWriter</p>
",Named Entity Recognition (NER),named entity recognition iob annotation transformation iob intermediate begin annotation format like john b person doe person transformed format digested java could figure documentation stanford nlp related class iobutils conlldocumentreaderandwriter
Word count statistics on a web page,"<p>I am looking for a way to extract basic stats (total count, density, count in links, hrefs) for words on an arbitrary website, ideally a Python based solution. </p>

<p>While it is easy to parse a specific website using, say BautifulSoup and determine where the bulk of the content is, it requires you to define the location of the content in the DOM tree ahead of processing. This is easy for, say, hrefs or any arbitraty tag but gets more complicated when determining where the rest of the data (not enclosed in well defined markers) is.</p>

<p>If I understand correctly, robots used by the likes of Google (GoogleBot?) are able to extract data from any website to determine the keyword density. My scenario is similar, obtain the info related to the words that define what the website is about (i.e. after removing js, links and fillers).</p>

<p>My question is, are there any libraries or web APIs that would allow me to get statistics of meaningful words from any given page?</p>
",Named Entity Recognition (NER),word count statistic web page looking way extract basic stats total count density count link hrefs word arbitrary website ideally python based solution easy parse specific website using say bautifulsoup determine bulk content requires define location content dom tree ahead processing easy say hrefs arbitraty tag get complicated determining rest data enclosed well defined marker understand correctly robot used like google googlebot able extract data website determine keyword density scenario similar obtain info related word define website e removing j link filler question library web apis would allow get statistic meaningful word given page
How to detect if a sentence is pointing to particular defined concepts,"<p>How can I detect if a particular sentence is pointing to particular defined concepts such as: </p>

<p><code>Start</code>:</p>

<p>E.g. “Can we begin” or “Let’s start”</p>

<p><code>Definition</code>:</p>

<p>E.g. “Define (a word)” or “What does (a word) mean” or “Meaning of (a word)”</p>

<p>In the first case the sentences point to the concept “to start,” and in the second example, the sentences point to the concept of a <code>definition</code>. </p>

<p>So how do I identify these concepts in a sentence. A similar question to mine is 
<a href=""https://stackoverflow.com/questions/31355225/nlp-how-to-detect-if-a-word-in-a-sentence-is-pointing-to-a-color-body-part-ve"">nlp - How to detect if a word in a sentence is pointing to a color/body part /vehicle</a></p>

<p>I used Stanford’s NLP API to make a Name Entity Recognizer class, but it can only detect </p>

<pre><code>LOCATION
ORGANIZATION
DATE
MONEY
PERSON
PERCENT
TIME
</code></pre>

<p>Here is the code:</p>

<pre><code>public void NER() throws ClassCastException, ClassNotFoundException, IOException
{
    String serializedClassifier = ""english.muc.7class.distsim.crf.ser.gz"";
    AbstractSequenceClassifier&lt;CoreLabel&gt; classifier = CRFClassifier.getClassifier(serializedClassifier);
    int i=0;
      String PER = """";
      String LOC = """";
      String TIME = """";
      String DATE = """";
      String ORG = """";
      for (String str : toComputeOn) {
        for (List&lt;CoreLabel&gt; lcl : classifier.classify(str)) {
          for (CoreLabel cl : lcl) {
            System.out.print(i++ + "": "");
            System.out.println(cl.toShorterString());

            if(cl.get(CoreAnnotations.AnswerAnnotation.class).equals(""TIME""))
                TIME += ""\n""+ cl.toString();
            if(cl.get(CoreAnnotations.AnswerAnnotation.class).equals(""PERSON""))
                PER += ""\n""+ cl.toString();
            if(cl.get(CoreAnnotations.AnswerAnnotation.class).equals(""ORGANIZATION""))
                ORG += ""\n""+ cl.toString();
            if(cl.get(CoreAnnotations.AnswerAnnotation.class).equals(""DATE""))
                DATE += ""\n""+ cl.toString();
            if(cl.get(CoreAnnotations.AnswerAnnotation.class).equals(""LOCATION""))
                LOC += ""\n""+ cl.toString();
          }
        }
      }

      if(!PER.equals("""")) {
          System.out.println(""PERSON: ""+ PER);
          System.out.println(""---"");
      }
      if(!ORG.equals("""")) {
          System.out.println(""ORGANIZATION: ""+ ORG);
          System.out.println(""---"");
      }
      if(!LOC.equals("""")) {
          System.out.println(""LOCATION: ""+ LOC);
          System.out.println(""---"");
      }
      if(!TIME.equals("""")) {
          System.out.println(""TIME: ""+ TIME);
          System.out.println(""---"");
      }
      if(!DATE.equals(""""))
          System.out.println(""DATE: ""+ DATE);
}
</code></pre>

<p>I need something that can detect other concepts. I was looking through Stanford’s API and found the BasicRelationExtractor class. I am not 100% sure what it does, but would using this class help me solve my problem, OR would it be better for me to train my own NER Classifier, OR would it be better to use MIT’s Java Wordnet Interface?</p>

<p>Thanks for any help</p>
",Named Entity Recognition (NER),detect sentence pointing particular defined concept detect particular sentence pointing particular defined concept e g begin let start e g define word doe word mean meaning word first case sentence point concept start second example sentence point concept identify concept sentence similar question mine href detect word sentence pointing color body part vehicle used stanford nlp api make name entity recognizer class detect code need something detect concept wa looking stanford api found basicrelationextractor class sure doe would using class help solve problem would better train ner classifier would better use mit java wordnet interface thanks help
Extract text between two strings,"<p>How can I extract text between two strings. </p>

<p>For e.g.:</p>

<pre><code>x &lt;- ""ABCDName:Mr.Praveen KumarDOB""
</code></pre>

<p>I want to extract <code>Mr. Praveen Kumar</code>.</p>

<p>Also, I want to extract string from starting till it encounters Name:.</p>
",Named Entity Recognition (NER),extract text two string extract text two string e g want extract also want extract string starting till encounter name
How to NER and POS tag a pre-tokenized text with Stanford CoreNLP?,"<p>I'm using the Stanford's CoreNLP Named Entity Recognizer (NER) and Part-of-Speech (POS) tagger in my application. The problem is that my code tokenizes the text beforehand and then I need to NER and POS tag each token. However I was only able to find out how to do that using the command line options but not programmatically.</p>

<p>Can someone please tell me how programmatically can I NER and POS tag pretokenized text using Stanford's CoreNLP?</p>

<p>Edit: </p>

<p>I'm actually using the individual NER and POS instructions. So my code was written as instructed in the tutorials given in the Stanford's NER and POS packages. But I have CoreNLP in my classpath. So I have the CoreNLP in my classpath but using the tutorials in the NER and POS packages. </p>

<p>Edit:</p>

<p>I just found that there are instructions as how one can set the properties for CoreNLP here <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""noreferrer"">http://nlp.stanford.edu/software/corenlp.shtml</a> but I wish if there was a quick way to do what I want with Stanford NER and POS taggers so I don't have to recode everything!</p>
",Named Entity Recognition (NER),ner po tag pre tokenized text stanford corenlp using stanford corenlp named entity recognizer ner part speech po tagger application problem code tokenizes text beforehand need ner po tag token however wa able find using command line option programmatically someone please tell programmatically ner po tag pretokenized text using stanford corenlp edit actually using individual ner po instruction code wa written instructed tutorial given stanford ner po package corenlp classpath corenlp classpath using tutorial ner po package edit found instruction one set property corenlp wish wa quick way want stanford ner po tagger recode everything
Approach to extract quotations from text and their speakers,"<p>I want to be able to extract quotations and their speakers from given text. For this, I am following the rules mentioned in section 5.2 of the paper <a href=""http://langtech.jrc.ec.europa.eu/Documents/0709_RANLP_Quotation-detection_BP-RS-CB_final.pdf"" rel=""nofollow"">http://langtech.jrc.ec.europa.eu/Documents/0709_RANLP_Quotation-detection_BP-RS-CB_final.pdf</a></p>

<p>The rules are like: </p>

<pre><code>(1) quote-mark QUOTE quote-mark [,] verb [modifier]
[determiner] [title] name
e.g. ""blah blah"", said again the journalist John Smith.

(2) name [, up to 60 characters ,] verb [:|that] quote-mark
QUOTE quote-mark
e.g. John Smith, supporting AFG, said: ""blah blah"".

(3) quote-mark QUOTE quote-mark [; or ,] [title] name
[modifier] verb
e.g. ""blah blah"", Mr John Smith said.
</code></pre>

<p>The ""quote-mark"" are general opening and closing quotation marks. A ""QUOTE"" is actual quotation text, a ""modifier"" is an adverb, a ""verb"" is a reporting/communication verb which needs to be present in a lexicon of verbs, ""title"" and ""name"" are title and name for persons. The names will be considered as the speaker of quotation.</p>

<p>For each given text, I have a set of NLP annotations indicating which words from text are verbs, adverbs, names and titles.</p>

<p>I am looking for an approach to match the given text and NLP annotations against the rule mentioned above and come up with set of quotations and their speakers from it. I understand that I can write the hardcoded logic to represent these rules but I want to make these rules configurable. </p>

<p>Is there any way to deal with this problem using regex, grammar parsers or some other approach?</p>
",Named Entity Recognition (NER),approach extract quotation text speaker want able extract quotation speaker given text following rule mentioned section paper rule like quote mark general opening closing quotation mark quote actual quotation text modifier adverb verb reporting communication verb need present lexicon verb title name title name person name considered speaker quotation given text set nlp annotation indicating word text verb adverb name title looking approach match given text nlp annotation rule mentioned come set quotation speaker understand write hardcoded logic represent rule want make rule configurable way deal problem using regex grammar parser approach
Text mining - extract name of band from unstructured text,"<p>I'm aware that this is kind of a general, open-ended question. I'm essentially looking for help in deciding a way forward, and perhaps for some reading material. </p>

<p>I'm working on an algorithm that does unstructured text mining, and trying to extract something specific - the names of bands (single artists, bands, etc) from that text. The text itself has no predictable structure, but it is relatively small (1, 2 rows of text). </p>

<p>Some examples may be (not real events): </p>

<pre><code>Concert Green Day At Wembley Stadium
Extraordinary representation - Norah Jones in Poland - at the Polish Opera
</code></pre>

<p>Now, I'm thinking of trying out a classifier but the text seems to small to provide any real training information for it. 
There probably are several other text mining techniques, heuristics or algorithms that may yield good results for this kind of problem (or perhaps no algorithm will). </p>
",Named Entity Recognition (NER),text mining extract name band unstructured text aware kind general open ended question essentially looking help deciding way forward perhaps reading material working algorithm doe unstructured text mining trying extract something specific name band single artist band etc text text ha predictable structure relatively small row text example may real event thinking trying classifier text seems small provide real training information probably several text mining technique heuristic algorithm may yield good result kind problem perhaps algorithm
How extracting meaning of sentences for sentiment analysis using NLP,"<p>""I had safe journey"" ,assume this is a feedback for a driver ,provided by a  passenger. I need to extract theses information from this sentence..</p>

<pre><code>""I had safe journey"" -&gt; 
 SUBJECT= ""driving""
 SENTIMENT= ""positive""
</code></pre>

<p>I tried with NLP Extracting Information from Text method. But I don't know how recognized Entities from these kind of sentences.How am I supposed to do that ?</p>
",Named Entity Recognition (NER),extracting meaning sentence sentiment analysis using nlp safe journey assume feedback driver provided passenger need extract thesis information sentence tried nlp extracting information text method know recognized entity kind sentence supposed
Extracting words next to a location or Duration in python,"<p>How can i extract words next to a location or Duration? What is the best possible regex in python to do this action?</p>

<p>Example:-</p>

<p>Kathick Kumar, Bangalore who was a great person and lived from 29th March 1980 - 21 Dec 2014.</p>

<p>In the above example i want to extract the words before location and the words before duration. Here the location and duration is not fixed, what will be the best possible regex for this in python? Or can we do this using nltk?</p>

<p>Desired output:-</p>

<p>Output-1: Karthick Kumar (Keyword here is Location)</p>

<p>Output-2: who was a great person and lived from (Keyword here is duration)</p>
",Named Entity Recognition (NER),extracting word next location duration python extract word next location duration best possible regex python action example kathick kumar bangalore wa great person lived th march dec example want extract word location word duration location duration fixed best possible regex python using nltk desired output output karthick kumar keyword location output wa great person lived keyword duration
Named Entity Recognition - Do we need an external list to match results?,"<p>I am not an expert in Machine Learning, so I will try to be as accurate as possible...</p>

<p>I am currently analyzing financial documents that are giving information on a specific fund. What I would like to do is to be able to extract the fund name.</p>

<p>For this, I am using Named Entity Recognition (NER) in Azure Machine Learning platform. After analyzing approx. 100 documents, I get results classified as Organizations. In most cases, they are really organizations. This is great, but my problem is that the fund name is also categorized as an organization. I am not able to distinguish between a company name and a fund name.</p>

<p>From some readings on Internet, I could discover that Gazette system could help so that we can match the recognized organizations against a list of funds, and therefore make sure that we have a fund name.</p>

<p>Do you think this would be a good approach? Or is there any other algorithm that I should try to improve the results?</p>

<p>Thanks for any suggestion!</p>
",Named Entity Recognition (NER),named entity recognition need external list match result expert machine learning try accurate possible currently analyzing financial document giving information specific fund would like able extract fund name using named entity recognition ner azure machine learning platform analyzing approx document get result classified organization case really organization great problem fund name also categorized organization able distinguish company name fund name reading internet could discover gazette system could help match recognized organization list fund therefore make sure fund name think would good approach algorithm try improve result thanks suggestion
Training a Custom Model using Java Code - Stanford NER,"<p>Could someone help me to convert these lines to Java code, instead of using terminal? </p>

<p>I'm trying to train my own model using Stanford Ner:</p>

<pre><code>java -cp stanford-ner.jar edu.stanford.nlp.process.PTBTokenizer jane-austen-emma-ch1.txt &gt; jane-austen-emma-ch1.tok

perl -ne 'chomp; print ""$_\tO\n""' jane-austen-emma-ch1.tok &gt; jane-austen-emma-ch1.tsv

java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop
</code></pre>

<p>And must the training file be in .tsv format???</p>
",Named Entity Recognition (NER),training custom model using java code stanford ner could someone help convert line java code instead using terminal trying train model using stanford ner must training file tsv format
Converting textual datasets to numerical datasets,"<p>Could I ask you please about converting datasets. I have 2 datasets, one for training and the other for testing. Both contains mixed features of texts and numbers about words in sentence. e.g. (indexes, tags, pos, ..etc).</p>

<p>this is an example of an instance in one of the datasets (.csv format):</p>

<p>1,point,6,1279,1284,point,NN,confluence,NN,would,MD,maps::NNS the::DT confluence::NN,NNS_DT DT_NN NNS_DT_NN,would::MD have::VB to::TO,MD_VB VB_TO MD_VB_TO,FALSE,FALSE,FALSE,NPe,PLACE</p>

<p>What I want to do is to train these datasets using neural networks in Matlab and extract deep features from it. The problem is that Matlab is dealing only with numerical datasets. Also, I'm using some classifiers in java and they are working only with numbers not texts.</p>

<p>Any one have a suggestion please how to train such datasets? or how to convert it to numerical format with regards to numerical features in it??</p>

<p>Thanks in Advance,</p>
",Named Entity Recognition (NER),converting textual datasets numerical datasets could ask please converting datasets datasets one training testing contains mixed feature text number word sentence e g index tag po etc example instance one datasets csv format point point nn confluence nn would md map nns dt confluence nn nns dt dt nn nns dt nn would md vb md vb vb md vb false false false npe place want train datasets using neural network matlab extract deep feature problem matlab dealing numerical datasets also using classifier java working number text one suggestion please train datasets convert numerical format regard numerical feature thanks advance
How to use serialized CRFClassifier with StanfordCoreNLP prop &#39;ner&#39;,"<p>I'm using the StanfordCoreNLP API interface to programmatically do some basic NLP. I need to train a model on my own corpus, but I'd like to use the <code>StanfordCoreNLP</code> interface to do it, because it handles a lot of the dry mechanics behind the scenes and I don't need much specialization there. </p>

<p>I've trained a CRFClassifier that I'd like to use for NER, serialized to a file. Based on the documentation, I'd think the following would work, but it doesn't seem to find my model and instead barfs on not being able to find the standard models (I'm not sure why I don't have those model files, but I'm not concerned about it since I don't want to use them anyway):</p>

<pre><code>    // String constants
    final String serializedClassifierFilename = ""/absolute/path/to/model.ser.gz"";

    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, ner"");
    props.setProperty(""ner.models"", serializedClassifierFilename);

    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    String fileContents = IOUtils.slurpFileNoExceptions(""test.txt"");
    Annotation document = new Annotation(fileContents);
</code></pre>

<p>Results in:</p>

<pre><code>Adding annotator tokenize
TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
Adding annotator ssplit
Adding annotator ner
Loading classifier from /path/build/edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... java.io.FileNotFoundException: edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz (No such file or directory)
    at java.io.FileInputStream.open0(Native Method)
    at java.io.FileInputStream.open(FileInputStream.java:195)
    at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1554)
</code></pre>

<p>etc., etc.</p>

<p>I know that I don't have their built-in model (again, not sure why.. I just cloned their git repo and compiled with <code>ant compile</code>. Regardless, I don't want to use their model anyway, I want to use the one I trained).</p>

<p>How can I get the StanfordCoreNLP interface to use my model in the <code>ner</code> step? Is possible? Is not possible?</p>
",Named Entity Recognition (NER),use serialized crfclassifier stanfordcorenlp prop ner using stanfordcorenlp api interface programmatically basic nlp need train model corpus like use interface handle lot dry mechanic behind scene need much specialization trained crfclassifier like use ner serialized file based documentation think following would work seem find model instead barf able find standard model sure model file concerned since want use anyway result etc etc know built model sure cloned git repo compiled regardless want use model anyway want use one trained get stanfordcorenlp interface use model step possible possible
Train model using Named entity,"<p>I am looking on standford corenlp using the Named Entity REcognizer.I have different kinds of input text and i need to tag it into my own Entity.So i  started training my own model and it doesnt seems to be working.</p>

<p>For eg: my input text string is ""Book of 49 Magazine Articles on Toyota Land Cruiser 1956-1987 Gold Portfolio <a href=""http://t.co/EqxmY1VmLg"">http://t.co/EqxmY1VmLg</a> <a href=""http://t.co/F0Vefuoj9Q"">http://t.co/F0Vefuoj9Q</a>""</p>

<p>I go through the examples to train my own models and and look for only some words that I am interested in.</p>

<p>My jane-austen-emma-ch1.tsv looks like this</p>

<pre><code>Toyota  PERS
Land Cruiser    PERS
</code></pre>

<p>From the above input text i am only interested in those two words. The one is 
Toyota and the other word is Land Cruiser.</p>

<p>The austin.prop look like this</p>

<pre><code>trainFile = jane-austen-emma-ch1.tsv
serializeTo = ner-model.ser.gz
map = word=0,answer=1
useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
useDisjunctive=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
</code></pre>

<p>Run the following command to generate the ner-model.ser.gz file </p>

<p>java -cp stanford-corenlp-3.4.1.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop</p>

<pre><code>public static void main(String[] args) {
        String serializedClassifier = ""edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz"";
        String serializedClassifier2 = ""C:/standford-ner/ner-model.ser.gz"";
        try {
            NERClassifierCombiner classifier = new NERClassifierCombiner(false, false, 
                    serializedClassifier2,serializedClassifier);
            String ss = ""Book of 49 Magazine Articles on Toyota Land Cruiser 1956-1987 Gold Portfolio http://t.co/EqxmY1VmLg http://t.co/F0Vefuoj9Q"";
            System.out.println(""---"");
            List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(ss);
            for (List&lt;CoreLabel&gt; sentence : out) {
              for (CoreLabel word : sentence) {
                System.out.print(word.word() + '/' + word.get(AnswerAnnotation.class) + ' ');
              }
              System.out.println();
            }

        } catch (ClassCastException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }  catch (Exception e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }

    }
</code></pre>

<p>Here is the output I am getting</p>

<pre><code>Book/PERS of/PERS 49/O Magazine/PERS Articles/PERS on/O Toyota/PERS Land/PERS Cruiser/PERS 1956-1987/PERS Gold/O Portfolio/PERS http://t.co/EqxmY1VmLg/PERS http://t.co/F0Vefuoj9Q/PERS
</code></pre>

<p>which i think its wrong.I am looking for Toyota/PERS and Land Cruiser/PERS(Which is a multi valued fied.</p>

<p>Thanks for the Help.Any help is really appreciated.</p>
",Named Entity Recognition (NER),train model using named entity looking standford corenlp using named entity recognizer different kind input text need tag entity started training model doesnt seems working eg input text string book magazine article toyota land cruiser gold go example train model look word interested jane austen emma ch tsv look like input text interested two word one toyota word land cruiser austin prop look like run following command generate ner model ser gz file java cp stanford corenlp jar edu stanford nlp ie crf crfclassifier prop austen prop output getting think wrong looking toyota pers land cruiser pers multi valued fied thanks help help really appreciated
How can I get popular tags/keywords from a collection of unstructured text chunks?,"<p>I am storing small chunks of texts - say of around 100 - 200 words - in a NoSQL database, and need to display the trending keywords/tags among all of these chunks. </p>

<p>I know of text analysis APIs like alchemy which extract entities from a single chunk of text, but I want top keywords/tags among all the chunks. </p>

<p>Should I store keywords against each text-chunk and then do an exhaustive counting of the top keywords? In which case, each keyword may differ slightly and may lead to fragmentation of similar keywords. </p>
",Named Entity Recognition (NER),get popular tag keywords collection unstructured text chunk storing small chunk text say around word nosql database need display trending keywords tag among chunk know text analysis apis like alchemy extract entity single chunk text want top keywords tag among chunk store keywords text chunk exhaustive counting top keywords case keyword may differ slightly may lead similar keywords
Annotator for Relationship Extraction,"<p>I have a set of urls in a text file. For each url in that text file, I want to tag the entities and relationships in the text contained in that url. </p>

<p>I am aware of the entity taggers like Stanford NER, NLTK and GATE which can perform the entity tagging. However, I am more interested in relationship extraction. </p>

<p>In order to extract relationships, I am thinking of annotating the text contained in those urls for training purpose. For this, I do not want to do manual annotation. I can write few regex to extract the relationship which I want, however it would be difficult to scale up. </p>

<p>Is there a tool where in I can specify what I want to annotate? </p>

<p>For example: </p>

<blockquote>
  <p>"" Rob is working as the Director of ABC organization. He graduated from
  XYZ University ""</p>
</blockquote>

<p>Here, I want to extract the <em>affiliations</em> relationship, so intuitively I would like to annotate words which describe the affiliations like <em>working</em>, <em>graduated</em>.</p>

<p>Edit:
By ""a set of URLs in the text file"", I mean I have about 200 links to certain webpages in that text file, each of the webpage contains some text. I want to analyse (annotate) that text.</p>
",Named Entity Recognition (NER),annotator relationship extraction set url text file url text file want tag entity relationship text contained url aware entity tagger like stanford ner nltk gate perform entity tagging however interested relationship extraction order extract relationship thinking annotating text contained url training purpose want manual annotation write regex extract relationship want however would difficult scale tool specify want annotate example rob working director abc organization graduated xyz university want extract affiliation relationship intuitively would like annotate word describe affiliation like working graduated edit set url text file mean link certain webpage text file webpage contains text want analyse annotate text
How to suppress unmatched words in Stanford NER classifiers?,"<p>I am new to Stanford NLP and NER and trying to train a custom classifier with a data sets of currencies and countries.</p>

<p>My training data in training-data-currency.tsv looks like -</p>

<pre><code>USD CURRENCY
GBP CURRENCY
</code></pre>

<p>And, training data in training-data-countries.tsv looks like -</p>

<pre><code>USA COUNTRY
UK  COUNTRY
</code></pre>

<p>And, classifiers properties look like -</p>

<pre><code>trainFileList = classifiers/training-data-currency.tsv,classifiers/training-data-countries.tsv
ner.model=classifiers/english.conll.4class.distsim.crf.ser.gz,classifiers/english.muc.7class.distsim.crf.ser.gz,classifiers/english.all.3class.distsim.crf.ser.gz
serializeTo = classifiers/my-classification-model.ser.gz
map = word=0,answer=1

useClassFeature=true
useWord=true
useNGrams=true
#no ngrams will be included that do not contain either the
#beginning or end of the word
noMidNGrams=true
useDisjunctive=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
#the next 4 deal with word shape features
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
</code></pre>

<p>Java code to find the categories is -</p>

<pre><code>LinkedHashMap&lt;String, LinkedHashSet&lt;String&gt;&gt; map = new&lt;String, LinkedHashSet&lt;String&gt;&gt; LinkedHashMap();
NERClassifierCombiner classifier = null;
try {
    classifier = new NERClassifierCombiner(true, true, 
            ""C:\\Users\\perso\\Downloads\\stanford-ner-2015-04-20\\stanford-ner-2015-04-20\\classifiers\\my-classification-model.ser.gz""
            );
} catch (IOException e) {
    // TODO Auto-generated catch block
    e.printStackTrace();
}
List&lt;List&lt;CoreLabel&gt;&gt; classify = classifier.classify(""Zambia"");
for (List&lt;CoreLabel&gt; coreLabels : classify) {
    for (CoreLabel coreLabel : coreLabels) {

        String word = coreLabel.word();
        String category = coreLabel
                .get(CoreAnnotations.AnswerAnnotation.class);
        if (!""O"".equals(category)) {
            if (map.containsKey(category)) {
                map.get(category).add(word);
            } else {
                LinkedHashSet&lt;String&gt; temp = new LinkedHashSet&lt;String&gt;();
                temp.add(word);
                map.put(category, temp);
            }
            System.out.println(word + "":"" + category);
        }

    }

}
</code></pre>

<p>When I run the above code with input as ""USD"" or ""UK"", I get expected result as ""CURRENCY"" or ""COUNTRY"". But, when I input something like ""Russia"", return value is ""CURRENCY"" which is from the first train file in the properties. I am expecting 'O' would be returned for these values which is not present in my training dat.</p>

<p>How can I achieve this behavior? Any pointers where I am going wrong would be really helpful.</p>
",Named Entity Recognition (NER),suppress unmatched word stanford ner classifier new stanford nlp ner trying train custom classifier data set currency country training data training data currency tsv look like training data training data country tsv look like classifier property look like java code find category run code input usd uk get expected result currency country input something like russia return value currency first train file property expecting would returned value present training dat achieve behavior pointer going wrong would really helpful
How to extract brand from product name,"<p>I have two website and i have datas in my hands now i want to do analysis with that data</p>

<p>I have two product name(Brand + Product name) i want to extract only brand name</p>

<pre><code>http://www.thehut.com/jeans-clothing/men/clothing/brave-soul-men-s-cardiff-jeans-denim/10741907.html
</code></pre>

<p>In the above website the product name is </p>

<pre><code>Brave Soul Men's Swansea Jeans - Denim
</code></pre>

<p>Brand name is </p>

<pre><code>Brave Soul
</code></pre>

<p>So i want only </p>

<pre><code>Brave Soul
</code></pre>

<p>Amazon weblink </p>

<pre><code>http://www.amazon.in/gp/product/B00L8WT2UI
</code></pre>

<p>Similarly In the above website the product name is </p>

<pre><code>Apple iPhone 5c (White, 8GB)
</code></pre>

<p>Brand name is </p>

<pre><code>Apple
</code></pre>

<p>So i want output like </p>

<pre><code>Brave Soul
Apple
</code></pre>
",Named Entity Recognition (NER),extract brand product name two website data hand want analysis data two product name brand product name want extract brand name website product name brand name want amazon weblink similarly website product name brand name want output like
How to train Stanford CRF NER from a tsv file,"<p>I am looking to train my own model, e.g. this string I need to run through my trained model: ""Book of 49 Magazine Articles on Toyota Land Cruiser 1956-1987 Gold Portfolio <a href=""http://t.co/EqxmY1VmLg"">http://t.co/EqxmY1VmLg</a> <a href=""http://t.co/F0Vefuoj9Q"">http://t.co/F0Vefuoj9Q</a>""</p>

<p>The tsv file looks like this:</p>

<pre><code>Toyota  PERS
Land    PERS
</code></pre>

<p>When I run it through the programme:</p>

<pre><code>public static void main(String[] args) {
        String serializedClassifier2 = ""C:/standford-ner/ner-model.ser.gz"";
      try {
            NERClassifierCombiner classifier = new NERClassifierCombiner(false, false, 
                     serializedClassifier2);
            String ss = ""Book of 49 Magazine Articles on Toyota Land Cruiser 1956-1987 Gold Portfolio http://t.co/EqxmY1VmLg http://t.co/F0Vefuoj9Q"";
            System.out.println(""---"");
            List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(ss);
            for (List&lt;CoreLabel&gt; sentence : out) {
              for (CoreLabel word : sentence) {
                System.out.print(word.word() + '/' + word.get(AnswerAnnotation.class) + ' ');
              }
              System.out.println();
            }


        } catch (ClassCastException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }  catch (Exception e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }

    }
</code></pre>

<p>Here is the output I am getting:</p>

<pre><code>Book/PERS of/PERS 49/O Magazine/PERS Articles/PERS on/O Toyota/PERS Land/PERS Cruiser/O 1956-1987/PERS Gold/PERS Portfolio/PERS http://t.co/EqxmY1VmLg/PERS http://t.co/F0Vefuoj9Q/PERS 
</code></pre>

<p>For me, the output is wrong. I need to get <code>Book/O of/O</code>.  I am not sure how it's getting this value, e.g. ""book"" is not mentioned in my tsv file. The words I have not mentioned in the tsv file should come as <code>O</code>. This tsv file is just the beginning; I have many more words to add.</p>
",Named Entity Recognition (NER),train stanford crf ner tsv file looking train model e g string need run trained model book magazine article toyota land cruiser gold tsv file look like run programme output getting output wrong need get sure getting value e g book mentioned tsv file word mentioned tsv file come tsv file beginning many word add
Text processing tool for tweets,"<p>I am collecting millions of sports related tweets daily. I want to process the text in those tweets. I want to recognize the entities, find the sentiment of the sentence and find the events in those tweets.</p>

<p>Entity recognizing :</p>

<p>For example :
""Rooney will play for England in their next match"".</p>

<p>From this tweet i want to recognize person entity ""Rooney"" and place entity ""England""</p>

<p>sentiment analysis:</p>

<p>I want to find the sentiment of a sentence. For example </p>

<ol>
<li>Chelsea played their worst game ever</li>
<li>Ronaldo scored a beautiful goal</li>
</ol>

<p>The first one should marked as ""negative"" sentence and the later one should marked as ""positive"".</p>

<p>Event recognizing :</p>

<p>I want to find ""goal scoring event"" from tweets. Sentences like  ""messi scored goal in first half"" and ""that was a fantastic goal from gerrald"" should marked as ""goal scoring event"".</p>

<p>I know entity recognizing and sentiment analysis tools are available and i need to write the rules for event recognizing. I have seen so many tools like <code>Stanford NER</code>, <code>alchemy api</code>, <code>open calais</code>, <code>meaning cloud api</code>, <code>ling pipe</code>, <code>illinois</code> etc..
I'm really confused about which tool I should select? Is there any free tools available without daily rate limits? I want to process millions of tweets daily and <code>java</code> is my preferable language. </p>

<p>Thanks. </p>
",Named Entity Recognition (NER),text processing tool tweet collecting million sport related tweet daily want process text tweet want recognize entity find sentiment sentence find event tweet entity recognizing example rooney play england next match tweet want recognize person entity rooney place entity england sentiment analysis want find sentiment sentence example chelsea played worst game ever ronaldo scored beautiful goal first one marked negative sentence later one marked positive event recognizing want find goal scoring event tweet sentence like messi scored goal first half wa fantastic goal gerrald marked goal scoring event know entity recognizing sentiment analysis tool available need write rule event recognizing seen many tool like etc really confused tool select free tool available without daily rate limit want process million tweet daily preferable language thanks
Stanford NER CharacterOffsetBegin,"<p>I am using Stanford CoreNLP for NER for a list of short documents. </p>

<pre><code>java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP
-annotators tokenize,ssplit,pos,lemma,ner -ssplit.eolonly -pos.model edu/stanford/nlp/models/pos-tagger/english-caseless-left3words-distsim.tagger
-ner.model edu/stanford/nlp/models/ner/english.all.3class.caseless.distsim.crf.ser.gz
-file .../input -outputDirectory .../stanford_ner
</code></pre>

<p>The problem is the <code>CharacterOffsetBegin</code> and <code>CharacterOffsetEnd</code> I get from each token are continuous number from the previous documents. Therefore for example the very first token of document_2 has a <code>CharacterOffsetBegin</code> of 240 rather than 0. Is there any option I can use in the command line? Any help would be greatly appreciated, thanks!</p>
",Named Entity Recognition (NER),stanford ner characteroffsetbegin using stanford corenlp ner list short document problem get token continuous number previous document therefore example first token document ha rather option use command line help would greatly appreciated thanks
How can I use machine learning to extract larger chunks of text from a document?,"<p>I am currently learning about machine learning, as I think it might be helpful to solve a problem I have. However, I am unsure about what techniques I should apply to solve my problem. I apologise in advance for probably not knowing enough about this field to even ask a proper question. </p>

<p>What I want to to is extract the significant parts of a knitting pattern (the actual pattern, not all the intro and stuff like that). For instance, I would like to feed <a href=""http://whiteflowerneedle.hubpages.com/hub/Free-Knitting-Pattern-Boot-Style-Red-and-White-Baby-Booties"" rel=""nofollow"">this web page</a> into my program and get out something like this:</p>

<pre><code>{
    title: ""Boot Style Red and White Baby Booties for Cold Weather""
    directions: ""
    Right Bootie.
    Cast on (31, 43) with white color.
    Rows (1, 3, 5, 7, 9, 10, 11): K.
    Row 2: K1, M1, (K14, K20), M1, K1, M1, (K14, K20), M1, K1. (35, 47 sts)
    Row 4: K2, M1, (K14, K20), M1, K3, M1, (K14, K20), M1, K2. (39, 51 sts)
    Row 6: K3, M1, (K14, K20), M1, K5, M1, (K14, K20), M1, K3. (43, 55 sts)
    ...""
}
</code></pre>

<p>I've been reading about extracting smaller parts, like sentences and words, and also about stuff like Named Entity Recognition, but they all seem to be focused on very small parts of the text. </p>

<p>My current thoughts are to use supervised learning, but I'm also very unsure about how to extract features out of the text. Naive methods like using letters, words or even sentences as features seems like they wouldn't be relevant enough to yield any kind of satisfactory results (and also, there would be tons of features, unless I use some kind of sampling), but what are really the significant features for finding out which parts are what in a knitting pattern? </p>

<p>Can someone point me in the right direction of algorithms and methods to do extraction of larger portions of the text?</p>
",Named Entity Recognition (NER),use machine learning extract larger chunk text document currently learning machine learning think might helpful solve problem however unsure technique apply solve problem apologise advance probably knowing enough field even ask proper question want extract significant part knitting pattern actual pattern intro stuff like instance would like feed web page program get something like reading extracting smaller part like sentence word also stuff like named entity recognition seem focused small part text current thought use supervised learning also unsure extract feature text naive method like using letter word even sentence feature seems like relevant enough yield kind satisfactory result also would ton feature unless use kind sampling really significant feature finding part knitting pattern someone point right direction algorithm method extraction larger portion text
How can I use machine learning to extract larger chunks of text from a document?,"<p>I am currently learning about machine learning, as I think it might be helpful to solve a problem I have. However, I am unsure about what techniques I should apply to solve my problem. I apologise in advance for probably not knowing enough about this field to even ask a proper question. </p>

<p>What I want to to is extract the significant parts of a knitting pattern (the actual pattern, not all the intro and stuff like that). For instance, I would like to feed <a href=""http://whiteflowerneedle.hubpages.com/hub/Free-Knitting-Pattern-Boot-Style-Red-and-White-Baby-Booties"" rel=""nofollow"">this web page</a> into my program and get out something like this:</p>

<pre><code>{
    title: ""Boot Style Red and White Baby Booties for Cold Weather""
    directions: ""
    Right Bootie.
    Cast on (31, 43) with white color.
    Rows (1, 3, 5, 7, 9, 10, 11): K.
    Row 2: K1, M1, (K14, K20), M1, K1, M1, (K14, K20), M1, K1. (35, 47 sts)
    Row 4: K2, M1, (K14, K20), M1, K3, M1, (K14, K20), M1, K2. (39, 51 sts)
    Row 6: K3, M1, (K14, K20), M1, K5, M1, (K14, K20), M1, K3. (43, 55 sts)
    ...""
}
</code></pre>

<p>I've been reading about extracting smaller parts, like sentences and words, and also about stuff like Named Entity Recognition, but they all seem to be focused on very small parts of the text. </p>

<p>My current thoughts are to use supervised learning, but I'm also very unsure about how to extract features out of the text. Naive methods like using letters, words or even sentences as features seems like they wouldn't be relevant enough to yield any kind of satisfactory results (and also, there would be tons of features, unless I use some kind of sampling), but what are really the significant features for finding out which parts are what in a knitting pattern? </p>

<p>Can someone point me in the right direction of algorithms and methods to do extraction of larger portions of the text?</p>
",Named Entity Recognition (NER),use machine learning extract larger chunk text document currently learning machine learning think might helpful solve problem however unsure technique apply solve problem apologise advance probably knowing enough field even ask proper question want extract significant part knitting pattern actual pattern intro stuff like instance would like feed web page program get something like reading extracting smaller part like sentence word also stuff like named entity recognition seem focused small part text current thought use supervised learning also unsure extract feature text naive method like using letter word even sentence feature seems like relevant enough yield kind satisfactory result also would ton feature unless use kind sampling really significant feature finding part knitting pattern someone point right direction algorithm method extraction larger portion text
How are features generated and used in Stanford NER,"<p>Take as an example two features from <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ie/NERFeatureFactory.html"" rel=""nofollow"">NERFeatureFactory</a>:</p>

<pre><code>pw, w, c
</code></pre>

<p>and</p>

<pre><code>pc, nc, c
</code></pre>

<p>Questions:</p>

<ol>
<li><p>Using the first feature function we want something like pw = 'in' w = 'Berlin' c = 'LOCATION' which would presumably get a high weight whereas changing c = 'PERSON' would get a low/negative weight. The question is, how are <strong>w</strong> and <strong>pw</strong> picked? Are they hand selected, are they taken from the vicinities of labeled words in the training set, or from the set of all possible words? <em>Is every combination pw, w then considered?</em></p></li>
<li><p>When the second feature function is used in the training phase do the matrices in the forward-backward algorithm become N^3 where N is the number of classes. Or am I missing something?</p></li>
</ol>

<p>Thank you in advance :) ! </p>
",Named Entity Recognition (NER),feature generated used stanford ner take example two feature nerfeaturefactory question using first feature function want something like pw w berlin c location would presumably get high weight whereas changing c person would get low negative weight question w pw picked hand selected taken vicinity labeled word training set set possible word every combination pw w considered second feature function used training phase matrix forward backward algorithm become n n number class missing something thank advance
How can I print probability score for a named entity using stanford NER?,"<p>inherently the classifier must be using a cut-off to classify an Entity in a certain class say person or organization.
how do I get that probability score?</p>

<p>for example can I get something like.
Hiranandani : location(0.8),builder(0.7),name(0.3)
where location,builder,name are different classes of named entity</p>
",Named Entity Recognition (NER),print probability score named entity using stanford ner inherently classifier must using cut classify entity certain class say person organization get probability score example get something like hiranandani location builder name location builder name different class named entity
Transform XML to use as training set for Named Entity Recognition (NER),"<p>I would like to leverage structured information I have in the form of XML to train a CRF model for the Stanford NLP package. The XML looks something like:</p>

<pre><code>&lt;dates&gt;
   &lt;date&gt;Advance Access publication on 
      &lt;month&gt;July&lt;/month&gt;
      &lt;day&gt;11&lt;/day&gt;, 
      &lt;year&gt;2007&lt;/year&gt;
   &lt;/date&gt;
&lt;/dates&gt;
</code></pre>

<p>According to <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""nofollow"">http://nlp.stanford.edu/software/crf-faq.shtml#a</a> I could use </p>

<pre><code>java -cp stanford-ner.jar edu.stanford.nlp.process.PTBTokenizer sample.xml &gt; date.tok
</code></pre>

<p>to get my tokens. But how could I leverage the XML encapsulation to automatically tag my tokens with the appropriate class? </p>

<p>Is there such a support/process in the Stanford NLP package, or should I rather go about writing my token file by hand (using XSLT for example)? </p>
",Named Entity Recognition (NER),transform xml use training set named entity recognition ner would like leverage structured information form xml train crf model stanford nlp package xml look something like according could use get token could leverage xml encapsulation automatically tag token appropriate class support process stanford nlp package rather go writing token file hand using xslt example
Difference between tag and class in Stanford NER,"<p>I'm not sure what the difference between tag and class is? </p>

<p><a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ie/NERFeatureFactory.html"" rel=""nofollow"">NERFeatureFactory</a> mentions:</p>

<pre><code>t - tag
c - class
</code></pre>

<p>The NER <a href=""http://nlp.stanford.edu/software/crf-faq.shtml"" rel=""nofollow"">FAQ</a> seems to use the two terms interchangeably as well? </p>

<p>For example what does the following feature do?</p>

<pre><code>t,c   useTags
</code></pre>

<p>Many thanks in advance!</p>
",Named Entity Recognition (NER),difference tag class stanford ner sure difference tag class nerfeaturefactory mention ner faq seems use two term interchangeably well example doe following feature many thanks advance
LingPipe Named Entity Recognizer outputs a lot of mismatches,"<p>I'm trying to extract named entities (people, persons and organizations) using LingPipe and following <a href=""http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html"" rel=""nofollow"">this tutorial</a>. Here is the <a href=""http://pastebin.com/YAUCWBMi"" rel=""nofollow"">full text</a> that I am trying to extract names from and here is the code (exception handling omitted for brevity):</p>

<pre><code>Chunker chunker = readChunker(""/path-to-chunker""); // custom method for
                                                     reading the model
String article = ""Some long news article spanning multiple lines..."";

Chunking chunking = chunker.chunk(article);
Set&lt;Chunk&gt; chunkingSet = chunking.chunkSet();
for (Chunk chunk : chunkingSet) {
   String name = article.substring(chunk.start(), chunk.end()));
   System.out.println(name);
}
</code></pre>

<p>And this is (part of) the output I get:</p>

<pre><code>Tony Abbott
Indonesia
Joko Widodo
Sukumaran
Andrew Chan
Bali.
pair
the Bali
Nusa Kambangan
Indonesian
Indonesian
I’
Widodo. I
” Abbott
Julie Bishop
Widodo
al-Jazeera
Sukumaran
Chan
Bishop
”
</code></pre>

<p>As you can see, there are a lot of mismatches/partial matches like <code>Bali.</code>, <code>pair</code>, <code>the Bali</code>, <code>I'</code>, <code>Widodo. I</code>, <code>"" Abbott</code>, <code>""</code>. I assume library's NER is working just fine and the problem is that the above code is somehow misusing the classes/methods from this library. But I just can't seem to find what is wrong about the code? </p>

<p>Any ideas?</p>
",Named Entity Recognition (NER),lingpipe named entity recognizer output lot mismatch trying extract named entity people person organization using lingpipe following tutorial full text trying extract name code exception handling omitted brevity part output get see lot mismatch partial match like assume library ner working fine problem code somehow misusing class method library seem find wrong code idea
Entity extraction API for Android,"<p>I have extracted text successfully from image but now I have no idea how to extract the name, location, phone, and cell number from extracted text.</p>

<p>Here is some example text that was extracted.</p>

<pre><code>Comsats Institute of Information technology,Abbottabad.
Dr Usama Ijaz bajwa 
Assistant Professor 
Phone:+92 321 6647911
</code></pre>
",Named Entity Recognition (NER),entity extraction api android extracted text successfully image idea extract name location phone cell number extracted text example text wa extracted
Stanford NER Features,"<p>I am currently trying to use the Stanford NER system and I am trying to see what features can be extracted through setting of the flags in a properties file. It seems that the features documented at <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ie/NERFeatureFactory.html"" rel=""nofollow"">http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ie/NERFeatureFactory.html</a> are not comprehensive. For example, all the feature flags related to dist similarity and clustering are not included (e.g. useDistSim, etc.). Is there a more complete list of all the features and corresponding flags that is available somewhere? </p>

<p>Thanks for the help!</p>
",Named Entity Recognition (NER),stanford ner feature currently trying use stanford ner system trying see feature extracted setting flag property file seems feature documented comprehensive example feature flag related dist similarity clustering included e g usedistsim etc complete list feature corresponding flag available somewhere thanks help
Extracting Important words from a sentence using Node,"<p>I admit that I havent searched extensively in the SO database. I tried reading the natural npm package but doesnt seem to provide the feature. I would like to know if the below requirement is somewhat possible ? </p>

<p>I have a database that has list of all cities of a country. I also have rating of these cities (best place to live, worst place to live, best rated city, worsrt rated city etc..). Now from the User interface, I would like to enable the user to enter free text and from there I should be able to search my database.</p>

<p>For e.g Best place to live in California 
or places near California 
or places in California </p>

<p>From the above sentence, I want to extract the nouns only (may be ) as this will be name of the city or country that I can search for. </p>

<p>Then extract 'best' means I can sort is a particular order etc...</p>

<p>Any suggestions or directions to look for? </p>

<p>I risk a chance that the question will be marked as 'debatable'. But the reason I posted is to get some direction to proceed. </p>
",Named Entity Recognition (NER),extracting important word sentence using node admit havent searched extensively database tried reading natural npm package doesnt seem provide feature would like know requirement somewhat possible database ha list city country also rating city best place live worst place live best rated city worsrt rated city etc user interface would like enable user enter free text able search database e g best place live california place near california place california sentence want extract noun may name city country search extract best mean sort particular order etc suggestion direction look risk chance question marked debatable reason posted get direction proceed
Ground Truth datasets for Evaluating Open Source NLP tools for Named Entity Recognition,"<p>I am working on building a document similarity graph for a collection. I already do all the basic things like tokenization, stemming, stop-word removal, and bag-of-word representation to represent the documents and computing similarity using Jaccard coefficient. I am now trying to extract Named Entities and evaluate if these would be helpful in improving the quality of the document similarity graph.  I have been spending much of time on finding ground-truth datasets for my analysis. I have been very disappointed with Message Understanding Conference (MUC) datasets. They are cryptic to understand and requires sufficient data cleaning/massaging before it can be used on a different platform (like Scala)</p>

<p>My questions are here more specifically</p>

<ol>
<li>Are there tutorials on getting started with MUC datasets that would make it easier for analyzing the results using open source NLP tools like openNLP</li>
<li>there other datasets available?</li>
<li>Tools like OpenNLP and Stanford Core NLP employ approaches that are essentially supervised. Correct?</li>
<li>GATE is a great tool for hand-annotating your own text corpus Correct?</li>
<li>For a new test dataset (that I hand-create) how can I compute the baseline (Vocabulary Transfer) or what kind of metrics can I compute? </li>
</ol>
",Named Entity Recognition (NER),ground truth datasets evaluating open source nlp tool named entity recognition working building document similarity graph collection already basic thing like tokenization stemming stop word removal bag word representation represent document computing similarity using jaccard coefficient trying extract named entity evaluate would helpful improving quality document similarity graph spending much time finding ground truth datasets analysis disappointed message understanding conference muc datasets cryptic understand requires sufficient data cleaning massaging used different platform like scala question specifically tutorial getting started muc datasets would make easier analyzing result using open source nlp tool like opennlp datasets available tool like opennlp stanford core nlp employ approach essentially supervised correct gate great tool hand annotating text corpus correct new test dataset hand create compute baseline vocabulary transfer kind metric compute
Parsing People Name from large text using NER,"<p>I want to parse out all person name from the input text data. I already tried it by using <a href=""http://nlp.stanford.edu/downloads/CRF-NER.shtml"" rel=""nofollow""><strong>Stanford Java  NER library</strong></a> but it does not give accurate answers for indian Names. Is it possible to create customised classifiers? or is there any other method to parse names using other NLP techniques with maximum accuracy.</p>

<p><strong>EDIT:</strong> I gave random text as input containing many person names to the GUI application of <a href=""http://nlp.stanford.edu/downloads/CRF-NER.shtml"" rel=""nofollow""><strong>Stanford Named Entity Recognizer version 3.5.1</strong></a> But it is very less accurate. It does not recognize most of the names. </p>
",Named Entity Recognition (NER),parsing people name large text using ner want parse person name input text data already tried using stanford java ner library doe give accurate answer indian name possible create customised classifier method parse name using nlp technique maximum accuracy edit gave random text input containing many person name gui application stanford named entity recognizer version le accurate doe recognize name
Extract Person Name from unstructure text,"<p>I have a collection of bills and Invoices, so there is no context in the text (i mean they don't tell a story).
I want to extract people names from those bills.
I tried OpenNLP but the quality of trained model is not good because i don't have context.
so the first question is: can I train model contains only people names without context? and if that possible can you give me good article for how i build that new model (most of the article that i read didn't explain the steps that i should made to build new model).</p>

<p>I have database name with more than 100,000 person name (first name, last name), so if the NER systems don't work in my case (because there is no context), what is the best way to search for those candidates (I mean searching for every first name with all other last names?)</p>

<p>thanks.</p>
",Named Entity Recognition (NER),extract person name unstructure text collection bill invoice context text mean tell story want extract people name bill tried opennlp quality trained model good context first question train model contains people name without context possible give good article build new model article read explain step made build new model database name person name first name last name ner system work case context best way search candidate mean searching every first name last name thanks
Extracting relationship from NER parse,"<p>I'm working on a problem that at the very least seems to require named entity recognition, but I'm not sure how to go farther than the NER parse. What I'm trying to do is parse information (likely from tweets) regarding scheduling of events. So, for example, I'd like to be able to automatically resolve the yes/no answer to the question of ""Are The Beatles playing tomorrow?"" from short messages like:</p>

<p>""The Beatles cancelled their show tomorrow"" or
""The Beatles' show is still on tomorrow""</p>

<p>I know NER will get me close as it will identify the band of interest and the time (if it's indicated), but there are many ways to express the concepts I'm interested in, for example:</p>

<p>""The Beatles are on for tomorrow"" or
""The Beatles won't be playing tomorrow.""</p>

<p>How can I go from an NER parsed representation to extracting the information of interest? Any suggestions would be much appreciated.</p>
",Named Entity Recognition (NER),extracting relationship ner parse working problem least seems require named entity recognition sure go farther ner parse trying parse information likely tweet regarding scheduling event example like able automatically resolve yes answer question beatles playing tomorrow short message like beatles cancelled show tomorrow beatles show still tomorrow know ner get close identify band interest time indicated many way express concept interested example beatles tomorrow beatles playing tomorrow go ner parsed representation extracting information interest suggestion would much appreciated
NLTK NER: Continuous Learning,"<p>I have been trying to use NER feature of NLTK. I want to extract such entities from the articles. I know that it can not be perfect in doing so but I wonder if there is human intervention in between to manually tag NEs, will it improve?</p>

<p>If yes, is it possible with present model in NLTK to continually train the model. (Semi-Supervised Training)</p>
",Named Entity Recognition (NER),nltk ner continuous learning trying use ner feature nltk want extract entity article know perfect wonder human intervention manually tag ne improve yes possible present model nltk continually train model semi supervised training
OpenNLP Name entity recognition model for time and date,"<p>I am using OpenNLP models for Name-entity recognition. </p>

<p>I am passing sentences, in which I want to identify words.
Open NLP requires a String [] variable, hence I split my String into words separated by space.</p>

<p>I am facing the problem to recognize the Date. If for example the string contains the date: 7 Jan 2012 and I split the string into words, ""7"", ""Jan"" and ""2012"" get separated as 3 different words. Although they are recognized as dates but the 3 different tokens don't make sense for me for further processing.
How can I possibly split my string, so that ""2 Jan 2012"" can be taken as one string...
7 Jan 2012 is one format... Sometimes it is also Jan 7,2012. Date also recognizes the time format I input: like 12:18pm </p>

<p>The NER time model is does not recognize the time in 12:18pm or 09:52:52 .. What kind of time format does it accept?</p>
",Named Entity Recognition (NER),opennlp name entity recognition model time date using opennlp model name entity recognition passing sentence want identify word open nlp requires string variable hence split string word separated space facing problem recognize date example string contains date jan split string word jan get separated different word although recognized date different token make sense processing possibly split string jan taken one string jan one format sometimes also jan date also recognizes time format input like pm ner time model doe recognize time pm kind time format doe accept
Detect relation between two persons in text,"<p>Goal is to find all the pairs of persons between which there is <em>any</em> kind of relation in a piece of text. Particularly, if we have this piece of text:</p>

<blockquote>
  <p>Alice Wilson, doctor with more than 30 years of experience in suppressing virus epidemics, has met with the president of Neverland
  country, John Doe, to discuss ways of tackling a new virus.</p>
  
  <p>John Doe will meet next week with the state official Jack Sparrow of some other country to discuss something
  totally-unrelated-to-the-first-part-of-text.</p>
</blockquote>

<p>There is a relation between <code>Alice Wilson</code> and <code>John Doe</code>, as well as <code>John Doe</code> and <code>Jack Sparrow</code>. However, there is no real relation between <code>Alice Wilson</code> and <code>Jack Sparrow</code>, apart from that they both appear in the same text.</p>

<p>Therefore, the resulting pairs would be:</p>

<pre><code>Alice Wilson, John Doe
John Doe, Jack Sparrow
</code></pre>

<p>I have found a way to extract people's names from text using Stanford CoreNLP's Named Entity Recognizer, thus having <code>Alice Wilson</code>, <code>John Doe</code> and <code>Jack Sparrow</code>, but I am not sure how to find relations between them. CoreNLP can perform Part-of-Speech tagging, which can tag words with <code>Subject</code>, <code>Verb</code>, <code>Object</code> etc. Nonetheless, I still don't see a way of tackling all the possible variations where a person's name can be found, e.g.:</p>

<pre><code>John Doe said that... &lt;- 'John Doe' is a Subject
Jack Sparrow introduced John Doe to the senior officials... &lt;- 'John Doe' is an Object
Jack Sparrow, John Doe's cousin, told the press... &lt;- Not even sure what 'John Doe' here is.
</code></pre>

<p>Any ideas (code is welcome, but not necessary) on how to approach this?</p>
",Named Entity Recognition (NER),detect relation two person text goal find pair person kind relation piece text particularly piece text alice wilson doctor year experience suppressing virus epidemic ha met president neverland country john doe discus way tackling new virus john doe meet next week state official jack sparrow country discus something totally unrelated first part text relation well however real relation apart appear text therefore resulting pair would found way extract people name text using stanford corenlp named entity recognizer thus sure find relation corenlp perform part speech tagging tag word etc nonetheless still see way tackling possible variation person name found e g idea code welcome necessary approach
What is a place hold word?,"<p><img src=""https://i.sstatic.net/RN39L.png"" alt=""enter image description here""></p>

<p>Hi i am trying to extract head words from a set of questions. I am implementing an algorithm mentioned in Huang et al., 2008 paper. In line 10 it says placehold-word. I cannot understand what a placehold word is. Can anyone help please.</p>
",Named Entity Recognition (NER),place hold word hi trying extract head word set question implementing algorithm mentioned huang et al paper line say placehold word understand placehold word anyone help please
How do I use python interface of Stanford NER(named entity recogniser)?,"<p>I want to use Stanford NER in python using pyner library. Here is one basic code snippet.</p>

<pre><code>import ner 
tagger = ner.HttpNER(host='localhost', port=80)
tagger.get_entities(""University of California is located in California, United States"")
</code></pre>

<p>When I run this on my local python console(IDLE). It should have given me an output like this</p>

<pre><code>  {'LOCATION': ['California', 'United States'],
 'ORGANIZATION': ['University of California']}
</code></pre>

<p>but when I execut this, it showed empty brackets. I am actually new to all this.</p>
",Named Entity Recognition (NER),use python interface stanford ner named entity recogniser want use stanford ner python using pyner library one basic code snippet run local python console idle given output like execut showed empty bracket actually new
Extracting Entity-Verb relations from open knowledge bases like Freebase and DBPedia,"<p>Is there any way that we can extract entity-verb relations from already existing online KBs like Freebase, DBPedia, Wikidata or Wordnet, I checked and only found that these sources concentrate on entities.</p>

<p>My aim is to derive relations like ""A Person can Eat"", ""A Car can move"", ""A Man can play football"".</p>
",Named Entity Recognition (NER),extracting entity verb relation open knowledge base like freebase dbpedia way extract entity verb relation already existing online kb like freebase dbpedia wikidata wordnet checked found source concentrate entity aim derive relation like person eat car move man play football
train caseless model for NER in openNLP,"<p>I want to train model for extracting person name (part of NER system) but I want to make this model caseless (I mean the model will not take letter case in consideration, no difference between uppercase and lowercase letters), because i have noisy text.</p>

<p>so is there any parameter in training step to do that, or any other way?</p>
",Named Entity Recognition (NER),train caseless model ner opennlp want train model extracting person name part ner system want make model caseless mean model take letter case consideration difference uppercase lowercase letter noisy text parameter training step way
How to create modular and extendable code for arbitrary feature extraction for machine learning?,"<p>I have been working to create a Python module that performs feature extraction for feature to eventually be used by a machine learning algorithm down the line.  </p>

<p>My approach has been to augment the initial gold standard dataset with (handcrafted) features, and thereby create a new dataset so training doesn't involve any feature creation, which can be expensive.  I believe this is the norm of most datasets--core features are always included (e.g. part-of-speech tags, named entity tags, semantic labels, etc.).  </p>

<p>The dataset I am using only includes all of the sentences tokenized, formatted as XML tags.  For example:</p>

<pre><code>&lt;s&gt;
  &lt;lex begin='351' end='354'&gt;The&lt;/lex&gt;
  &lt;lex begin='355' end='361'&gt;people&lt;/lex&gt;
  &lt;lex begin='362' end='366'&gt;here&lt;/lex&gt;
  &lt;lex begin='367' end='370'&gt;are&lt;/lex&gt;
  &lt;lex begin='371' end='374'&gt;far&lt;/lex&gt;
  &lt;lex begin='375' end='384'&gt;wealthier&lt;/lex&gt;
  &lt;lex begin='384' end='385'&gt;.&lt;/lex&gt;
&lt;/s&gt;
</code></pre>

<p>I would like to add additional information to each token, e.g. part-of-speech, NER, semantic labels, etc.  </p>

<p>I have been using <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">Stanford NLP POS tagger</a> and <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow"">Stanford NLP NER tagger</a>.  These are incredibly slow, but (hopefully) the speed provides more accurate POS and NER labels.  I also throw in another parser to get semantic labels.  Below is the new sentence, augmented with features.  </p>

<pre><code>&lt;s&gt;
    &lt;lex ner='O' begin='351' end='354' pos='DT' label='None'&gt;The&lt;/lex&gt;
    &lt;lex CATEGORY='#ref-category PERSON' begin='355' end='361'
        FORM='#ref-category COMMON-NOUN/PLURAL' ENDS-AT='#edges ending at 3'
        CONSTITUENTS='NIL' USED-IN='NIL' Type='SPARSER::EDGE' LEFT-DAUGHTER='#word ""people""'
        pos='NNS' RULE='#PSR577  person -  ""people""' label='SPATIAL_ENTITY'
        REFERENT='#people 1' POSITION-IN-RESOURCE-ARRAY='1' SPANNED-WORDS='NIL'
        RIGHT-DAUGHTER=':SINGLE-TERM' ner='O' Class='#STRUCTURE-CLASS SPARSER::EDGE'
        STARTS-AT='#edges starting at 2'&gt;people&lt;/lex&gt;
    &lt;lex CATEGORY='#ref-category DEICTIC-LOCATION' begin='362' end='366'
        FORM='#ref-category PROPER-NOUN' ENDS-AT='#edges ending at 4'
        CONSTITUENTS='NIL' USED-IN='NIL' Type='SPARSER::EDGE' LEFT-DAUGHTER='#word ""here""'
        pos='RB' RULE='#PSR271  deictic-location -  ""here""' label='PLACE'
        REFERENT='#deictic-location ""here"" 3' POSITION-IN-RESOURCE-ARRAY='3'
        SPANNED-WORDS='NIL' RIGHT-DAUGHTER=':SINGLE-TERM' ner='O'
        Class='#STRUCTURE-CLASS SPARSER::EDGE' STARTS-AT='#edges starting at 3'&gt;here&lt;/lex&gt;
    &lt;lex CATEGORY='#ref-category BE' begin='367' end='370'
        FORM='#ref-category VERB' ENDS-AT='#edges ending at 5' CONSTITUENTS='NIL'
        USED-IN='NIL' Type='SPARSER::EDGE' LEFT-DAUGHTER='#word ""are""' pos='VBP'
        RULE='#PSR145  be -  ""are""' label='None' REFERENT='#be 1'
        POSITION-IN-RESOURCE-ARRAY='4' SPANNED-WORDS='NIL' RIGHT-DAUGHTER=':SINGLE-TERM'
        ner='O' Class='#STRUCTURE-CLASS SPARSER::EDGE' STARTS-AT='#edges starting at 4'&gt;are&lt;/lex&gt;
    &lt;lex CATEGORY='#word ""far""' begin='371' end='374'
        FORM='#ref-category SPATIAL-PREPOSITION' ENDS-AT='#edges ending at 6'
        CONSTITUENTS='NIL' USED-IN='NIL' Type='SPARSER::EDGE' LEFT-DAUGHTER='#word ""far""'
        pos='RB' RULE='(5)' label='None' REFERENT='#word ""far""'
        POSITION-IN-RESOURCE-ARRAY='5' SPANNED-WORDS='NIL' RIGHT-DAUGHTER=':LITERAL-IN-A-RULE'
        ner='O' Class='#STRUCTURE-CLASS SPARSER::EDGE' STARTS-AT='#edges starting at 5'&gt;far&lt;/lex&gt;
    &lt;lex ner='O' begin='375' end='384' pos='JJR' label='None'&gt;wealthier&lt;/lex&gt;
    &lt;lex begin='384' end='385'&gt;.&lt;/lex&gt;
&lt;/s&gt;
</code></pre>

<p>Obviously it's not really human readable anymore, but that's not important as these are just features to be piped to a machine learning algorithm.  </p>

<p>For my purposes, I should only have to do this once, and less expensive features can just be added in right before training, e.g. is the word capitalized.  </p>

<p>My present solution, however, is really terrible and I am not sure how to re-factor it, so that someone in the future can put in their own hooks/functions (e.g. they want to quickly add new features from some other parser) easily.  Here is my working solution:</p>

<pre><code>xml_tokens_pattern = re.compile(r'&lt;TOKENS&gt;.+&lt;/TOKENS&gt;', re.DOTALL)
sentence_pattern = re.compile(r'&lt;s&gt;.+?&lt;/s&gt;', re.DOTALL)
lex_attrs_pattern = re.compile(r'(?&lt;=&lt;lex)[^&gt;]+')

class Feature_Process(object):
    """"""Wrapper for adding features to xmls.

    """"""
    def __init__(self, xmls, golddir, newdir='', suffix='++',
                 feature_functions=[], renew=False, debug=False):
        self.xmls = xmls
        self.golddir = golddir
        self.newdir = newdir
        self.suffix = suffix
        self.feature_functions = feature_functions
        self.renew = renew
        self.debug = debug
        self.heavy = False

    def process(self):
        for xml in self.xmls:
            path = setup_newdir(xml, self.golddir, self.newdir,
                                self.suffix, self.renew)
            if not path:
                continue
            mkparentdirs(path)
            with open(xml, 'r') as oldfile:
                text = oldfile.read()
            doc = Space_Document(xml)
            tags = [tag for tag in doc.tags if 'start' in tag.attrib]
            new_text = text
            for (i,m) in enumerate(re.finditer(sentence_pattern, text)):
                sentence = doc.sentences[i]
                doc_lexes = sentence.getchildren()
                xml_sentence = m.group()
                tokens = [''.join([c if ord(c) &lt; 128
                                   else u2ascii[c]
                                   for c in x.text]).encode('utf-8')
                          for x in doc_lexes]
                (pos_tags, ner_tags, edges) = ([], [], [])
                if self.heavy:
                    pos_tags = pos.tag(tokens)
                    ner_tags = ner.tag(tokens)
                    try:
                        if self.debug:
                            print ' '.join([x for x in tokens])
                        edges = p(' '.join([x for x in tokens]), split=True)
                    except:
                        'somehow got here'
                c = 0
                for (j, n) in enumerate(re.finditer(lex_attrs_pattern,
                                                    xml_sentence)):
                    doc_lex = doc_lexes[j]
                    new_lex = Lex(doc_lex.text, doc_lex.attrib)
                    attributes = n.group()
                    tag = binary_search((int(doc_lex.attrib['begin']),
                                         int(doc_lex.attrib['end']),
                                         doc_lex.text), tags)
                    label = 'None'
                    if type(tag) != type(None):
                        label = tag.tag
                    new_lex.add(('label', label))
                    new_lex.add(('word', new_lex.text.encode('utf-8')))
                    if type(tag) != type(None):
                            new_lex.addAll([(key, tag.attrib[key]) for key in tag.attrib])
                    if pos_tags:
                        if tokens[j] == pos_tags[c][0]:
                            new_lex.add(('pos', pos_tags[c][1]))
                            pos_tags.remove(pos_tags[c])
                    if ner_tags: #this error case comes up for RFC/Durango.xml
                        if tokens[j] == ner_tags[c][0]:
                            new_lex.add(('ner', ner_tags[c][1]))
                            ner_tags.remove(ner_tags[c])
                    if edges:
                        sparser_edge = ledge(edges, tokens[j])
                        if sparser_edge:
                            if sparser_edge.keyvalues:
                                keyvalues = sparser_edge.keyvalues[sparser_edge.keyvalues.keys()[0]]
                                new_lex.addAll([(key, keyvalues[key]) for key in keyvalues])
                    new_lex.addAll([function(new_lex) for function in self.feature_functions])
                    new_text = new_text.replace(attributes, str(new_lex))
            w = open(path, 'w')
            print&gt;&gt;w, new_text
            w.close()
</code></pre>
",Named Entity Recognition (NER),create modular extendable code arbitrary feature extraction machine learning working create python module performs feature extraction feature eventually used machine learning algorithm line approach ha augment initial gold standard dataset handcrafted feature thereby create new dataset training involve feature creation expensive believe norm datasets core feature always included e g part speech tag named entity tag semantic label etc dataset using includes sentence tokenized formatted xml tag example would like add additional information token e g part speech ner semantic label etc using stanford nlp po tagger stanford nlp ner tagger incredibly slow hopefully speed provides accurate po ner label also throw another parser get semantic label new sentence augmented feature obviously really human readable anymore important feature piped machine learning algorithm purpose le expensive feature added right training e g word capitalized present solution however really terrible sure factor someone future put hook function e g want quickly add new feature parser easily working solution
dbpedia NLP dataset used for Named entity extraction,"<p>I went through their github files as well as the official site, I can't find the named entity tagging training corpus they used in splotlight. </p>

<p>How Can I found the dataset  instead of a trained model?</p>
",Named Entity Recognition (NER),dbpedia nlp dataset used named entity extraction went github file well official site find named entity tagging training corpus used splotlight found dataset instead trained model
Wget to download Wikipedia text,"<p>This is what I want to do:
Given an initial url (eg. <a href=""http://en.wikipedia.org/wiki/Lists_of_scientists"" rel=""nofollow"">http://en.wikipedia.org/wiki/Lists_of_scientists</a>), I want to visit all the links on that page (relevant links of course).
Each link corresponds to another page containing several other links (eg. <a href=""http://en.wikipedia.org/wiki/List_of_American_scientists"" rel=""nofollow"">http://en.wikipedia.org/wiki/List_of_American_scientists</a>). I want to visit each such link so that I can extract xml information from them.
Can this be done using wget? Someone suggested I should use Scrapy, however I am facing problem installing it.
The hierarchy to crawl looks like this: List of Scientists->List of American Scientists->Bryan Hayes (And a lot more scientists).</p>

<p>My target is to extract basic information from these wiki texts, like a person's name, organization, age, etc.</p>

<p>PS: I am a NOOB with good understanding.</p>
",Named Entity Recognition (NER),wget download wikipedia text want given initial url eg want visit link page relevant link course link corresponds another page containing several link eg want visit link extract xml information done using wget someone suggested use scrapy however facing problem installing hierarchy crawl look like list scientist list american scientist bryan hayes lot scientist target extract basic information wiki text like person name organization age etc p noob good understanding
Identifying the context of word in sentence,"<p>I created classifier to classy the class of nouns,adjectives, Named entities in given sentence. I have used large Wikipedia dataset for classification.</p>

<p>Like : </p>

<p>Where Abraham Lincoln was born?</p>

<p>So classifier will give this short of result - <code>word - class</code></p>

<ul>
<li>Where  - question </li>
<li>Abraham Lincoln - Person, Movie, Book (because    classifier find Abraham Lincoln in all there categories) </li>
<li>born - time</li>
</ul>

<p>When Titanic was released?</p>

<ul>
<li>when - question </li>
<li>Titanic - Song, movie, Vehicle, Game (Titanic
classified in all these categories)</li>
</ul>

<p><strong>Is there any way to identify exact context for word?</strong></p>

<p>Please see :</p>

<ol>
<li>Word sense disambiguation would not help here. Because there might not be near by word in sentence which can help</li>
<li><p>Lesk algorithm with wordnet or sysnet also does not help. Because it for suppose word <code>Bank</code> lesk algo will behave like this</p>

<p>======== TESTING simple_lesk ===========</p>

<h1>TESTING simple_lesk() ...</h1>

<p>Context: I went to the bank to deposit my money</p>

<p>Sense: Synset('depository_financial_institution.n.01')</p>

<p>Definition: a financial institution that accepts deposits and channels the money into lending activities</p>

<h1>TESTING simple_lesk() with POS ...</h1>

<p>Context: The river bank was full of dead fishes</p>

<p>Sense: Synset('bank.n.01')</p>

<p>Definition: sloping land (especially the slope beside a body of water)</p></li>
</ol>

<p>Here for word <code>bank</code> it suggested as <code>financial institute</code> and <code>slopping land</code>. While in my case I am already getting such prediction like <code>Titanic</code> then it can be <code>movie</code> or <code>game</code>.</p>

<p>I want to know is there any other approach apart from <code>Lesk algo</code>, <code>baseline algo</code>, <code>traditional word sense disambiguation</code> which can help me to identify which class is correct for particular keyword? </p>

<p>Titanic - </p>
",Named Entity Recognition (NER),identifying context word sentence created classifier classy class noun adjective named entity given sentence used large wikipedia dataset classification like abraham lincoln wa born classifier give short result question abraham lincoln person movie book classifier find abraham lincoln category born time titanic wa released question titanic song movie vehicle game titanic classified category way identify exact context word please see word sense disambiguation would help might near word sentence help lesk algorithm wordnet sysnet also doe help suppose word lesk algo behave like testing simple lesk testing simple lesk context went bank deposit money sense synset depository financial institution n definition financial institution accepts deposit channel money lending activity testing simple lesk po context river bank wa full dead fish sense synset bank n definition sloping land especially slope beside body water word suggested case already getting prediction like want know approach apart help identify class correct particular keyword titanic
"Extracting U.S. common law case names (e.g., Smith v. Jones) systematically using off-the-shelf NLP software?","<p>I'd like to find a way to extract case names from U.S. courts from sentences. They usually take a predictable pattern, although I think they may be too varied to capture well with Regexs, so I was thinking about using NLP to locate them.</p>

<p>Here are a few examples of case names (bolded) as they might be used in partial sentences:</p>

<ul>
<li>In <strong>United States v. George</strong>, the Court held that...</li>
<li><strong>In re. Bankruptcy of Sir Walter Williams, III</strong> is a case from the Southern District of New York...</li>
<li>Not only is <strong>Ashcroft v. Iqbal</strong>, 556 U.S. 662 (2009) incorrectly decided, it also...</li>
<li>The Court's recent decision in <strong>Burwell v. Hobby Lobby Stores</strong>, No. 13-354 (U.S. Jun 30, 2014) implicates First Amendment rights...</li>
<li>The case of <strong>Trans World Airlines, Inc. v. Flight Attendants</strong> was correctly decided...</li>
</ul>

<p>I've been experimenting with off-the-shelf packages (like TextBlob for Python), which helps do things like extract noun phrases -- I just don't know how to take the next step and recognize case names as a unit.</p>
",Named Entity Recognition (NER),extracting u common law case name e g smith v jones using shelf nlp software like find way extract case name u court sentence usually take predictable pattern although think may varied capture well regexs wa thinking using nlp locate example case name bolded might used partial sentence united state v george court held bankruptcy sir walter williams iii case southern district new york ashcroft v iqbal u incorrectly decided also court recent decision burwell v hobby lobby store u jun implicates first amendment right case trans world airline inc v flight attendant wa correctly decided experimenting shelf package like textblob python help thing like extract noun phrase know take next step recognize case name unit
nltk NER word extraction,"<p>I have checked previous related threads, but did not solve my issue. I have written code to get NER from text. </p>

<pre><code>text = ""Stallone jason's film Rocky was inducted into the National Film Registry as well as having its film props placed in the Smithsonian Museum.""

tokenized = nltk.word_tokenize(text)
tagged = nltk.pos_tag(tokenized)
namedEnt = nltk.ne_chunk(tagged, binary = True)
print namedEnt
namedEnt = nltk.ne_chunk(tagged, binary = False)
</code></pre>

<p>which gives this short of result</p>

<pre><code>(S
  (NE Stallone/NNP)
  jason/NN
  's/POS
  film/NN
  (NE Rocky/NNP)
  was/VBD
  inducted/VBN
  into/IN
  the/DT
  (NE National/NNP Film/NNP Registry/NNP)
  as/IN
  well/RB
  as/IN
  having/VBG
  its/PRP$
  film/NN
  props/NNS
  placed/VBN
  in/IN
  the/DT
  (NE Smithsonian/NNP Museum/NNP)
  ./.)
</code></pre>

<p>while I expect only NE as a result, like</p>

<pre><code>Stallone
Rockey
National Film Registry
Smithsonian Museum
</code></pre>

<p>how to achieve this?</p>

<p><strong>UPDATE</strong></p>

<pre><code>result = ' '.join([y[0] for y in x.leaves()]) for x in namedEnt.subtrees() if x.node == ""NE""
print result
</code></pre>

<p>gives syntext error, what is correct way to write this?</p>

<p><strong>UPDATE2</strong></p>

<p>text = ""Stallone jason's film Rocky was inducted into the National Film Registry as well as having its film props placed in the Smithsonian Museum.""</p>

<pre><code>tokenized = nltk.word_tokenize(text)
tagged = nltk.pos_tag(tokenized)
namedEnt = nltk.ne_chunk(tagged, binary = True)
print namedEnt
np = [' '.join([y[0] for y in x.leaves()]) for x in namedEnt.subtrees() if x.node == ""NE""]
print np
</code></pre>

<p>error:</p>

<pre><code> np = [' '.join([y[0] for y in x.leaves()]) for x in namedEnt.subtrees() if x.node == ""NE""]
  File ""/usr/local/lib/python2.7/dist-packages/nltk/tree.py"", line 198, in _get_node
    raise NotImplementedError(""Use label() to access a node label."")
NotImplementedError: Use label() to access a node label.
</code></pre>

<p>so I tried with </p>

<pre><code>np = [' '.join([y[0] for y in x.leaves()]) for x in namedEnt.subtrees() if x.label() == ""NE""]
</code></pre>

<p>which gives emtpy result</p>
",Named Entity Recognition (NER),nltk ner word extraction checked previous related thread solve issue written code get ner text give short result expect ne result like achieve update give syntext error correct way write update text stallone jason film rocky wa inducted national film registry well film prop placed smithsonian museum error tried give emtpy result
input text classification using nltk,"<p>I have data set of 5500 questions -<a href=""https://github.com/karimkhanp/nltk_data/blob/master/corpora/qc/train.txt"" rel=""nofollow"">link1</a> and Keywords of 6 categories - <a href=""https://github.com/evandrix/nltk_data/tree/master/corpora/qc"" rel=""nofollow"">link2</a> </p>

<p>I want to perform two actions</p>

<ol>
<li>Classifying the category of input question based on given data set in both links</li>
<li>Classifying the category of Named entities in input sentence for above link2 data set</li>
</ol>

<p>Initially I started with gender classification just to know how classification can be done, and it works pretty nice.</p>

<pre><code>&gt;&gt;&gt; def gender_features(word):
...     return {'last_letter': word[-1]}
&gt;&gt;&gt; gender_features('Shrek')
{'last_letter': 'k'}

&gt;&gt;&gt; from nltk.corpus import names
&gt;&gt;&gt; labeled_names = ([(name, 'male') for name in names.words('male.txt')] +
... [(name, 'female') for name in names.words('female.txt')])
&gt;&gt;&gt; import random
&gt;&gt;&gt; random.shuffle(labeled_names)

&gt;&gt;&gt; featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]
&gt;&gt;&gt; train_set, test_set = featuresets[500:], featuresets[:500]
&gt;&gt;&gt; classifier = nltk.NaiveBayesClassifier.train(train_set)
&gt;&gt;&gt; classifier.classify(gender_features('Neo'))
'male'
&gt;&gt;&gt; classifier.classify(gender_features('Trinity'))
'female'
</code></pre>

<p>In similar manner I tried to classify input word from any of the category listed in <code>link2</code>. But I could not identify what  <code>feature extraction function</code> should I use here. </p>

<p>Any help or suggestion for 1,2 action would be much appreciable.</p>
",Named Entity Recognition (NER),input text classification using nltk data set question link keywords category link want perform two action classifying category input question based given data set link classifying category named entity input sentence link data set initially started gender classification know classification done work pretty nice similar manner tried classify input word category listed could identify use help suggestion action would much appreciable
How to extract full entities from a bunch of text (not partial entities),"<p>This is probably a classical NLP problem, but how do I extract the FULL entity in a bunch of tweets?</p>

<p>For instance, suppose there's a bunch of tweets that mention ""Boston"" and ""marathon"", both in the same tweet. How do I know I should I extract ""Boston marathon"" and not just Boston or marathon?</p>

<p>Similarly, suppose there's a lot of tweets that mention ""Game of Thrones"". How would I know the entity to be extracted is Game of Thrones, not just Game?</p>
",Named Entity Recognition (NER),extract full entity bunch text partial entity probably classical nlp problem extract full entity bunch tweet instance suppose bunch tweet mention boston marathon tweet know extract boston marathon boston marathon similarly suppose lot tweet mention game throne would know entity extracted game throne game
Ready package for semi-supervised learning,"<p>I need do NER with small train set. I think that the solution will realize with some semi-supervised learning algoritm. Is there any ready packages that will fulfill for this task?</p>
",Named Entity Recognition (NER),ready package semi supervised learning need ner small train set think solution realize semi supervised learning algoritm ready package fulfill task
Named entity recognition : For new/latest entities,"<p>Sorry for that weird ""question title"" , but I couldnt think of an appropriate title.</p>

<p>Im new to NLP concepts, so I used NER demo (<a href=""http://cogcomp.cs.illinois.edu/demo/ner/results.php"" rel=""nofollow"">http://cogcomp.cs.illinois.edu/demo/ner/results.php</a>). Now the issue is that ""how &amp; in what ways"" can I use these taggings done by NER. I mean these what answers or inferences can one draw from these named-entities which have been tagged in certain groups - location, person ,organization etc. If I have a data which has names of entirely new companies, places etc then how am I going to do these NER taggings for such a data ?</p>

<p>Pls dont downvote or block me, I just need guidance/expert suggestions thats it. Reading about a concept is another thing, while being able to know where &amp; when to apply it is another thing, which is where Im asking for guidance. Thanks a ton !!!</p>

<p>A snippet from the demo:-</p>

<p>Dogs have been used in cargo areas for some time, but have just been introduced recently in 
passenger areas at <strong>LOC Newark</strong>  and <strong>LOC JFK</strong>  airports. <strong>LOC JFK</strong>  has one dog and <strong>LOC Newark</strong>  has a 
handful, <strong>PER Farbstei</strong>n  said.</p>
",Named Entity Recognition (NER),named entity recognition new latest entity sorry weird question title couldnt think appropriate title im new nlp concept used ner demo issue way use taggings done ner mean answer inference one draw named entity tagged certain group location person organization etc data ha name entirely new company place etc going ner taggings data pls dont downvote block need guidance expert suggestion thats reading concept another thing able know apply another thing im asking guidance thanks ton snippet demo dog used cargo area time introduced recently passenger area loc newark loc jfk airport loc jfk ha one dog loc newark ha handful per farbstein said
Address Extraction from text in Objective C,"<p>I am working on a Business Card reader app which has some extra features other than business card reading. Presently, I am stuck in address extraction process. The addresses I am working on are not of any specific country. They are global addresses. 
The app is meant to run offline. So, I can't consume any online API to detect the address. I have gone trough several blogs and tried to implement most of them but with no effect. I tried with several regular expressions as well but nothing seemed to work so far. I have some Business Card reader apps installed in my mobile which detect the addresses beautifully. I need a logic which can extract address from an unstructured text.</p>
",Named Entity Recognition (NER),address extraction text objective c working business card reader app ha extra feature business card reading presently stuck address extraction process address working specific country global address app meant run offline consume online api detect address gone trough several blog tried implement effect tried several regular expression well nothing seemed work far business card reader apps installed mobile detect address beautifully need logic extract address unstructured text
Improve CoreNLP POS tagger and NER tagger?,"<p>The CoreNLP parts of speech tagger and name entity recognition tagger are pretty good out of the box, but I'd like to improve the accuracy further so that the overall program runs better. To explain more about accuracy -- there are situations in which the POS/NER is wrongly tagged. For instance:</p>

<ul>
<li>""Oversaw car manufacturing"" gets tagged as NNP-NN-NN</li>
</ul>

<p>Rather than VB* or something similar, since it's a verb-like phrase (I'm not a linguist, so take this with a grain of salt).</p>

<p>So what's the best way to accomplish accuracy improvement?</p>

<ul>
<li>Are there better models out there for POS/NER that can be incorporated into CoreNLP?</li>
<li>Should I switch to other NLP tools?</li>
<li>Or create training models with exception rules?</li>
</ul>
",Named Entity Recognition (NER),improve corenlp po tagger ner tagger corenlp part speech tagger name entity recognition tagger pretty good box like improve accuracy overall program run better explain accuracy situation po ner wrongly tagged instance oversaw car manufacturing get tagged nnp nn nn rather vb something similar since verb like phrase linguist take grain salt best way accomplish accuracy improvement better model po ner incorporated corenlp switch nlp tool create training model exception rule
How should I use machine learning classifiers on a a training set that contains text?,"<p>I am working on finding errors and predicting their possible causes by going through log files.
In order to apply a classifier on it I need the text to be a numeral. I can identify keywords by NER and need it to be used as a training set. Can anyone suggest me some ways to do that ?</p>
",Named Entity Recognition (NER),use machine learning classifier training set contains text working finding error predicting possible cause going log file order apply classifier need text numeral identify keywords ner need used training set anyone suggest way
How to extract corporate bonds informations using machine learning,"<p>I am working on a project where I need to extract corporate bonds information from the unstructured emails. After doing a lot of research, I found that machine learning can be used for information extraction. I tried Opennlp NER (Named entity recognizer) but I am not sure whether I picked up the correct library for this problem or not because I am getting the results but not up to the mark. </p>

<p>Could someone please suggest me any library or algorithms means how can I parse and extract data from it. I am planning to explore Naïve Bayes or N-gram or Support vector machine but not sure, this will help me or not. Please suggest.</p>

<p>Examples are like:</p>

<p><code>[/] Trading 10mm ABC 2.5 19   05/06 mkt  can use 50mm</code> ---> here I want to extract <code>""ABC 2.5 19""</code></p>

<p>Example 2:</p>

<p><code>XYZ 6.5   15 10-2B    106-107                B3   AAA- 1.646MM 2x2</code>   ---> here I want to extract <code>""XYZ 6.5   15""</code></p>
",Named Entity Recognition (NER),extract corporate bond information using machine learning working project need extract corporate bond information unstructured email lot research found machine learning used information extraction tried opennlp ner named entity recognizer sure whether picked correct library problem getting result mark could someone please suggest library algorithm mean parse extract data planning explore na bayes n gram support vector machine sure help please suggest example like want extract example want extract
Can I share a large array in memory between PHP processes?,"<p>I use PHP to do a lot of data processing ( realizing I'm probably pushing into territories where I should be using other languages and/or techniques ).</p>

<p>I'm doing entity extraction with a PHP process that loads an array containing ngrams to look for into memory. That array uses 3GB of memory and takes about 20 seconds to load each time I launch a process. I generate it once locally on the machine and each process loads it from a .json file. Each process then tokenizes the text it's processing and does an array_intersect between these two arrays to extract entities.</p>

<p>Is there any way to preload this into memory on the machine that is running all these processes and then share the resource across all the processes?</p>

<p>Since it's probably not possible with PHP: What type of languages/methods should I be researching to do this sort of entity extraction more efficiently?</p>
",Named Entity Recognition (NER),share large array memory php process use php lot data processing realizing probably pushing territory using language technique entity extraction php process load array containing ngrams look memory array us gb memory take second load time launch process generate locally machine process load json file process tokenizes text processing doe array intersect two array extract entity way preload memory machine running process share resource across process since probably possible php type language method researching sort entity extraction efficiently
Getting/Indexing what Named-Entities have said/quoted,"<p>I am trying to semi-automatically identify what people/advisers/economists or the like say on the news:</p>
<h1>1</h1>
<p>But Barclays chief economist Kieran Davies said companies were simply opting against bank funding.
“I don’t think there’s actually a ­constraint on corporate credit from the banks. I think it’s more the case that ­corporates in recent years have either had sufficient cash or access to offshore funding to enable them to do their investment,” Mr Davies said.</p>
<h1>2</h1>
<p>The the amount of commercial lending for every dollar of residential property lending has plunged from $3.84 to $1.62 over the past 25 years, says ­analysis from Industry Super Australia.
“Consistent analysis demonstrates that we have a systematic issue ­transitioning national savings to real productive capital, such as nation-building infrastructure,” chief ­executive David Whiteley said.</p>
<h1>3</h1>
<p>Following the settlement, Macquarie’s then chief executive in Asia, Alex Harvey, said he was “quite ­encouraged about SATC, about the risk management framework we’ve now put in place over the last few years and the sort of opportunities that are in front of the trust.”</p>
<p>so with example #1, i would like to get what Mr Davies was saying,
and with example #2, what chief executive David Whiteley was saying,
and with example #3, what Alex Harvey was saying.</p>
<p>what is the best way of getting / indexing what those 3 people said in other 10000 or more articles ?
it feels like a mixed of Name-Entity-Tagging and Relationship-Extraction. Is there a more specific name to what I want to achieve ?</p>
",Named Entity Recognition (NER),getting indexing named entity said quoted trying semi automatically identify people adviser economist like say news barclays chief economist kieran davy said company simply opting bank funding think actually constraint corporate credit bank think case corporates recent year either sufficient cash access offshore funding enable investment mr davy said amount commercial lending every dollar residential property lending ha plunged past year say analysis industry super australia consistent analysis demonstrates systematic issue transitioning national saving real productive capital nation building infrastructure chief executive david whiteley said following settlement macquarie chief executive asia alex harvey said wa quite encouraged satc risk management framework put place last year sort opportunity front trust example would like get mr davy wa saying example chief executive david whiteley wa saying example alex harvey wa saying best way getting indexing people said article feel like mixed name entity tagging relationship extraction specific name want achieve
How to merge new model after training with the old one?,"<p>I have a model ""en-ner-organization.bin"" which I downloaded from apache web-site. It's works fine, but I prefer to train it with my organizations database to increase recognition quality. But after I trained ""en-ner-organization.bin"" with my organization database - the size of model became less that it was. So it seems, it was overwritten with my data. </p>

<p>I see that there is no possibility to re-train existing model, but maybe there is a way to merge models? </p>

<p>If no - I guess I can add my train data into the .train file of original model, so generated model will consists of default data, plus my data from db. But I can't find such file in web. </p>

<p>So, the main question is: <strong>how to keep existing model data and add new data into model</strong>?</p>

<p>Thanks</p>
",Named Entity Recognition (NER),merge new model training old one model en ner organization bin downloaded apache web site work fine prefer train organization database increase recognition quality trained en ner organization bin organization database size model became le wa seems wa overwritten data see possibility train existing model maybe way merge model guess add train data train file original model generated model consists default data plus data db find file web main question keep existing model data add new data model thanks
Mapping dbpedia Wikipedia entity to the Wikipedia categories he&#39;s in,"<p>I'm using dbpedia spotlight to do NER. I would like to map each entity to wikipedia categories he's under. Is there an easy way to do this? </p>

<p>A simple example might be if I recognized the basketball player <code>Michael Jordan</code> I would want to fetch the categories he's under, probably <code>NBA Players</code>, <code>Sports</code>, <code>Chicago Bulls Past Players</code>, Etc...</p>
",Named Entity Recognition (NER),mapping dbpedia wikipedia entity wikipedia category using dbpedia spotlight ner would like map entity wikipedia category easy way simple example might recognized basketball player would want fetch category probably etc
CRF for NER with many classes,"<p>I'm going to develop named-entity recognition system with many (100+) classes. Assuming that they have roughly equal frequency, what algorithm should perform best? According to my understanding (sadly, far away from ideal) of how CRF works, it should be ok here. But in some sources (<a href=""http://books.google.com.ua/books?id=opn1BDfvs-0C&amp;pg=PA582&amp;lpg=PA582&amp;dq=conditional%20random%20field%20%22many%20classes%22&amp;source=bl&amp;ots=wsuxmGzVlB&amp;sig=h-8DuUcKuxMjiLN7UgV6b6KraeM&amp;hl=uk&amp;sa=X&amp;ei=nXfmU87kIMWc0AW9mID4BQ&amp;redir_esc=y#v=onepage&amp;q=conditional%20random%20field%20%22many%20classes%22&amp;f=false"" rel=""nofollow"" title=""google docs"">google books</a>) I found another opinion. </p>

<p>So, is CRF suitable algorithm for NER with huge number of classes?</p>
",Named Entity Recognition (NER),crf ner many class going develop named entity recognition system many class assuming roughly equal frequency algorithm perform best according understanding sadly far away ideal crf work ok source google book found another opinion crf suitable algorithm ner huge number class
Clarification about opennlp algorithm,"<p>I'm trying to get in with openNlp. I need it to get new organizations(startups) from news websites (for example: techcrunch). I have a model with organizations, which I use to recognize organizations in publications(en-ner-organization). And here I have a question:</p>

<blockquote>
  <p>In case there is a publication about new startup, which was born yesterday,
  will openNlp recognize it as organization?</p>
</blockquote>

<p>As far as I understand - no. Until I don't train model with this new startup, right? </p>

<p>If all my assumptions are correct, the model partially contains of organizations names, so if I want my model to recognize new organization, I have to train it with it's name.</p>

<p>Thanks</p>
",Named Entity Recognition (NER),clarification opennlp algorithm trying get opennlp need get new organization startup news website example techcrunch model organization use recognize organization publication en ner organization question case publication new startup wa born yesterday opennlp recognize organization far understand train model new startup right assumption correct model partially contains organization name want model recognize new organization train name thanks
"Analyse the sentences and extract person name, organization and location with the help of NLP","<p>I need to solve the following using NLP, can you give me pointers on how to achieve this using OpenNLP API</p>

<p>a. How to find out if a sentence implies a certain action in the past, present or future.</p>

<pre><code>(e.g.) I was very sad last week - past
       I feel like hitting my neighbor - present
       I am planning to go to New York next week - future
</code></pre>

<p>b. How to find the word which corresponds to a person or company or country</p>

<pre><code>(e.g.) John is planning to specialize in Electrical Engineering in UC Berkley and pursue a career with IBM).
</code></pre>

<p>Person = John</p>

<p>Company = IBM</p>

<p>Location = Berkley</p>

<p>Thanks</p>
",Named Entity Recognition (NER),analyse sentence extract person name organization location help nlp need solve following using nlp give pointer achieve using opennlp api find sentence implies certain action past present future b find word corresponds person company country person john company ibm location berkley thanks
How to detect named entities,"<p>I have a list of named entities. I would like to extract listed named entities from a certain text file with some Java libraries. </p>

<p>What I want to do are; </p>

<ul>
<li>""Currencies"" should be detected, even if the list includes only ""Currency"". Conversely, ""OECD country"" should be detected, even if the list includes only ""OECD countries.""</li>
<li>""recommendation system"" should be detected if the list includes only ""recommender system"" and vice versa. </li>
<li>""economic-buyer theory"" should be detected, if the list includes only ""economic buyer theory"" and vice versa.</li>
</ul>

<p>I tried GATE tools, but it did not work well.
How can I do that?
If you have any suggestions, please let me know. </p>
",Named Entity Recognition (NER),detect named entity list named entity would like extract listed named entity certain text file java library want currency detected even list includes currency oecd country detected even list includes oecd country recommendation system detected list includes recommender system vice versa economic buyer theory detected list includes economic buyer theory vice versa tried gate tool work well suggestion please let know
Pass array of floats when training Stanford CRFClassifier,"<p>For every word in a document, I'm looking to add in a series of floating point numbers as features for Stanford NER's <code>CRFClassifier</code> to train on.  Unfortunately, the documentation on Stanford NER's <code>.prop</code> files hasn't made it clear how to pass in custom features. In general, how does one go about adding custom features to a Stanford NER training set?</p>
",Named Entity Recognition (NER),pas array float training stanford crfclassifier every word document looking add series floating point number feature stanford ner train unfortunately documentation stanford ner file made clear pas custom feature general doe one go adding custom feature stanford ner training set
Identifying Universities mentioned in Tweet Text,"<p>I am looking for a means of identifying UK University names mentioned in Tweet text.</p>

<p>I have a list of full University names, but the issue is dealing with shortened versions such as ""aber uni"" (Aberystwyth Uni), ""staffs uni"" (Staffordshire University) or ""portsmouth"" (University of Portsmouth).</p>

<p>I have looked down the route of Apache Stanbol and OpenNLP to attempt Named Entity Recognition, and although these will match for the full names I cannot seem to find a means of training them to identify variations of the names (or indeed lowercase versions of the name which are not identified). </p>
",Named Entity Recognition (NER),identifying university mentioned tweet text looking mean identifying uk university name mentioned tweet text list full university name issue dealing shortened version aber uni aberystwyth uni staff uni staffordshire university portsmouth university portsmouth looked route apache stanbol opennlp attempt named entity recognition although match full name seem find mean training identify variation name indeed lowercase version name identified
Writing our own models in openNLP,"<p>If i use a query like this in command line</p>

<pre><code>./opennlp TokenNameFinder en-ner-person.bin ""input.txt"" ""output.txt""
</code></pre>

<p>I'll get person names printed in output.txt but I want to write own models such that i should print my own entities.</p>

<p>E.g.</p>

<ol>
<li>what is the risk value on icm2500.</li>
<li>Delivery of prd_234 will be arrived late.</li>
<li>Watson is handling router_34.</li>
</ol>

<p>If i pass these lines, it should parse and extract product_entities. icm2500, prd_234, router_34... etc these are all Products( we can save this information in a file and we can use it as look up kind of for models or openNLP).</p>

<p>Can anyone please tel me how to do this  ?</p>
",Named Entity Recognition (NER),writing model opennlp use query like command line get person name printed output txt want write model print entity e g risk value icm delivery prd arrived late watson handling router pas line parse extract product entity icm prd router etc product save information file use look kind model opennlp anyone please tel
How i can use LingPipe Tools to extract Arabic named entity,"<p>I'm trying to use Lingpipe tools to extract named entity recognition (Persons name) from Arabic text.
I was read a tutorial about this tools
<a href=""http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html"" rel=""nofollow"">http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html</a></p>

<p>how i can save the results in file.
Thanks</p>
",Named Entity Recognition (NER),use lingpipe tool extract arabic named entity trying use lingpipe tool extract named entity recognition person name arabic text wa read tutorial tool save result file thanks
Is UIMA provides only a wrapper or is it like StandfordCore NLP and GATE?,"<p>The Standford Core NLP and the GATE provides the various NLP operation like NER, POS tagging. There are some of the NLP operation like Tokenizer, Snowball Stemmer available as a UIMA component.
So, Is UIMA comparable with the StandfordCore NLP/GATE or it is to be used to wrap these kind of APIs for the pipeline ?</p>
",Named Entity Recognition (NER),uima provides wrapper like standfordcore nlp gate standford core nlp gate provides various nlp operation like ner po tagging nlp operation like tokenizer snowball stemmer available uima component uima comparable standfordcore nlp gate used wrap kind apis pipeline
Looking to Reason / Extract Information from Entity and Part of Speech Tagged Text,"<p>Let us say I start with the following text:</p>

<pre><code>I love Toyota Camrys and hate Ferraris
</code></pre>

<p>I use a POS tagger like Stanford CoreNLP and get the following Annotations:</p>

<pre><code>I_PRP love_VBP Toyota_NNP Camrys_NNPS and_CC hate_VB Ferraris_NNP
</code></pre>

<p>Let us assume I have a Named Entity Recognizer and am able to identify a Camry and Ferrari from the above notation.</p>

<p>I want to be able to reason about the above sentence where for example I deduce the following:</p>

<ul>
<li>I hate Camrys    </li>
<li>I love Ferraris</li>
</ul>

<p>possibly even:</p>

<ul>
<li>I hate something manufactured by Toyota</li>
<li>I hate something manufactured by Ferrari</li>
</ul>

<p>I am currently doing the above using manually coded heuristics and slot matching.</p>

<p>Question: Is there a more standard way to accomplish this?</p>

<p>For example I ran in to JAPE Java Annotation Patterns Engine from Gate -- is that part of the tool chain do something like this.</p>
",Named Entity Recognition (NER),looking reason extract information entity part speech tagged text let u say start following text use po tagger like stanford corenlp get following annotation let u assume named entity recognizer able identify camry ferrari notation want able reason sentence example deduce following hate camrys love ferraris possibly even hate something manufactured toyota hate something manufactured ferrari currently using manually coded heuristic slot matching question standard way accomplish example ran jape java annotation pattern engine gate part tool chain something like
enlarging a text corpus with classes,"<p>I have a text corpus of many sentences, with some named entities marked within it.
For example, the sentence:</p>

<p>what is the best restaurant in wichita texas?</p>

<p>which is tagged as:</p>

<p>what is the best restaurant in <code>&lt;location&gt;</code>?</p>

<p>I want to expand this corpus, by taking or sampling all the sentences already in it, and replacing the named entities with other similar entities from the same types, e.g. replacing ""wichita texas"" with ""new york"", so the corpus will be bigger (more sentences) and more complete (number of entities within it). I have lists of similar entities, including ones which doesn't appear in the corpus but I would like to have some probability of inserting them in my replacements.</p>

<p>Can you recommend on a method or direct me to a paper regarding this?</p>
",Named Entity Recognition (NER),enlarging text corpus class text corpus many sentence named entity marked within example sentence best restaurant wichita texas tagged best restaurant want expand corpus taking sampling sentence already replacing named entity similar entity type e g replacing wichita texas new york corpus bigger sentence complete number entity within list similar entity including one appear corpus would like probability inserting replacement recommend method direct paper regarding
"DKPro Core Groovy, NP recognition not working","<p>I am verry new to groovy and i am trying to use DKPro Core for some nlp stuff. At this point i am trying to recognise name phrases within a piece of text. I can recognise tokens sentences and named entities correctly but for some reason the same doesnt work for the NP class. My code is shown below, please point out the mistake.</p>

<pre><code>#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', version='1.5.0',
      module='de.tudarmstadt.ukp.dkpro.core.stanfordnlp-gpl')
@Grab(group='de.tudarmstadt.ukp.dkpro.core',
    module='de.tudarmstadt.ukp.dkpro.core.io.text-asl',
    version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core',
    module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl',
    version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core',
    module='de.tudarmstadt.ukp.dkpro.core.io.text-asl',
    version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core',
    module='de.tudarmstadt.ukp.dkpro.core.stanfordnlp-gpl',
    version='1.5.0')

import org.apache.uima.analysis_engine.AnalysisEngineProcessException;
import org.apache.uima.fit.component.JCasConsumer_ImplBase;
import org.apache.uima.fit.util.JCasUtil;
import org.apache.uima.jcas.JCas;
import de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity;
import de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Sentence;
import de.tudarmstadt.ukp.dkpro.core.api.syntax.type.constituent.NP;    
import de.tudarmstadt.ukp.dkpro.core.stanfordnlp.*;

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.factory.JCasFactory.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;
import static org.apache.uima.fit.util.JCasUtil.*;

import de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.*;
import de.tudarmstadt.ukp.dkpro.core.api.ner.type.*;


def doc = createJCas();
doc.documentText = """"""It is unfortunate that many Nigerians, especially the younger ones, 
express surprise at the mention of elephants and lions being found within the borders of the country. 
Admittedly, the number of these animals has diminished greatly over the years due to the activities of poachers thus pushing 
some of these animals to the verge of extinction. For example, it was discovered last year 
that there are not more than 34 lions in the wild. However there should be cause for 
optimism as a rundown of just a few animals across these parks show. The Yankari 
Game Reserve in Bauchi is Nigeria's most famous and arguably the best park for observing 
wildlife. Buffaloes, waterbucks, bushbucks, hyenas, leopards, baboons, elephants and lions 
are some of the animals that can be found here. 
""The animals are best seen during the dry season, 
especially from January to April,"" a 
tour guide told this reporter during a safari at Yankari. """"""
doc.documentLanguage = ""en"";

runPipeline(doc,
  createEngineDescription(StanfordSegmenter),
  createEngineDescription(StanfordPosTagger),
  createEngineDescription(StanfordNamedEntityRecognizer));

// for (Token token : select(doc, Token)) {  
    // println token.coveredText + ""\n\n\n""
    // }
// for (Sentence sentence : select(doc, Sentence)) {  
    // println sentence.coveredText + ""\n\n\n""
    // }
for (Sentence sentence : JCasUtil.select(doc, Sentence.class)) {
println sentence.getCoveredText()+""\n\n""
for (NP nounphrase : JCasUtil.selectCovered(doc, NP.class, sentence)) { 
    println ""||"" + nounphrase.getCoveredText() + ""||\n\n""
    }
}   
// for (Token token : select(doc, Token)) { 
    // def entity=selectCovering(NamedEntity,token).value
    // if(entity.toString().length()&gt;2)
    // println token.coveredText +""\n\n"" + entity.toString() + ""\n\n\n""
    // }
</code></pre>

<p>On my output the sentences are regognised correctly, but there is nothing printed for Named Phrases.</p>
",Named Entity Recognition (NER),dkpro core groovy np recognition working verry new groovy trying use dkpro core nlp stuff point trying recognise name phrase within piece text recognise token sentence named entity correctly reason doesnt work np class code shown please point mistake output sentence regognised correctly nothing printed named phrase
Span class in opennlp is not working,"<p>I am new to java opennlp and i am trying to implement a program that extracts city names from a file but i am testing my code on a string first and i get some errors 
the code is</p>

<pre><code> import java.io.FileInputStream;
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.InputStream;

 import main.java.opennlp.tools.namefind.NameFinderME;
 import main.java.opennlp.tools.namefind.TokenNameFinderModel;
 import main.java.opennlp.tools.util.InvalidFormatException;
 import main.java.opennlp.tools.util.Span;
 import opennlp.tools.tokenize.Tokenizer;
 import opennlp.tools.tokenize.TokenizerME;
 import opennlp.tools.tokenize.TokenizerModel;
 import opennlp.tools.tokenize.SimpleTokenizer;
 import opennlp.tools.sentdetect.SentenceDetectorME;
 import opennlp.tools.sentdetect.SentenceModel;

  import org.xml.sax.SAXException;

 public class CityFinder {

public String Tokens[];

public static void main(String[] args) throws IOException, SAXException {

    CityFinder toi = new CityFinder();
    String cnt;
    cnt=""John is planning to specialize in Electrical Engineering in UC Berkley and  pursue a career with IBM."";
    toi.tokenization(cnt);
    String cities = toi.namefind(toi.Tokens);
    String org = toi.orgfind(toi.Tokens);

    System.out.println(""City name is : ""+cities);
    System.out.println(""organization name is: ""+org);

}
    public String namefind(String cnt[]) {
    InputStream is;
    TokenNameFinderModel tnf;
    NameFinderME nf;
    String sd = """";
    try {
        is = new FileInputStream(""en-ner-location.bin"");
        tnf = new TokenNameFinderModel(is);
        nf = new NameFinderME(tnf);
        Span sp[] = nf.find(cnt); // &lt;-- Here is the Error 
        StringBuilder fd = new StringBuilder();
        int l = a.length;

        for (int j = 0; j &lt; l; j++) {
            fd = fd.append(a[j] + ""\n"");

        }
        sd = fd.toString();

    } catch (FileNotFoundException e) {

        e.printStackTrace();
    } catch (InvalidFormatException e) {

        e.printStackTrace();
    } catch (IOException e) {

        e.printStackTrace();
    }
    return sd;
}

public String orgfind(String cnt[]) {
    InputStream is;
    TokenNameFinderModel tnf;
    NameFinderME nf;
    String sd = """";
    try {
        is = new FileInputStream(""en-ner-organization.bin"");
        tnf = new TokenNameFinderModel(is);
        nf = new NameFinderME(tnf);
        Span sp[] = nf.find(cnt); // &lt;-- Here is the Error 
        String a[] = Span.spansToStrings(sp, cnt);
        StringBuilder fd = new StringBuilder();
        int l = a.length;
        for (int j = 0; j &lt; l; j++) {
            fd = fd.append(a[j] + ""\n"");

        }

        sd = fd.toString();

    } catch (FileNotFoundException e) {

        e.printStackTrace();
    } catch (InvalidFormatException e) {

        e.printStackTrace();
    } catch (IOException e) {

        e.printStackTrace();
    }
    return sd;

}
public void tokenization(String tokens) {

    InputStream is;
    TokenizerModel tm;
    try {
        is = new FileInputStream(""en-token.bin"");
        tm = new TokenizerModel(is);
        Tokenizer tz = new TokenizerME(tm);
        Tokens = tz.tokenize(tokens);
        // System.out.println(Tokens[1]);
    } catch (IOException e) {
        e.printStackTrace();
    }
}

  }
</code></pre>

<p>I have errors with the following line</p>

<pre><code>Span sp[] = nf.find(cnt);
</code></pre>

<p>the error is </p>

<pre><code>Type mismatch: cannot convert from opennlp.tools.util.Span[] to main.java.opennlp.tools.util.Span[]
</code></pre>

<p>I don't know how to solve it in both locations</p>

<p>Any suggestions....??
thanks in advance</p>
",Named Entity Recognition (NER),span class opennlp working new java opennlp trying implement program extract city name file testing code string first get error code error following line error know solve location suggestion thanks advance
Extracting properties and attributes for entities,"<p>I want to extract attributes and their values for name-entities. For example: </p>

<p>Lisa has a pet cat named Whiskers. Whiskers is black with a white spot on her chest. Whiskers also has white paws that look like little white mittens.  Whiskers likes to sleep in the sun on her favorite chair. Whiskers also likes to drink creamy milk. </p>

<p>One possible extraction of attributes for each entity is the following: </p>

<p>List: </p>

<ul>
<li>Has -> Whiskers </li>
</ul>

<p>Wiskers  </p>

<ul>
<li><p>Color -> Black </p></li>
<li><p>Likes to -> {Sleep in the sun on Lisa's favorite chair, drink creamy mik}</p></li>
</ul>
",Named Entity Recognition (NER),extracting property attribute entity want extract attribute value name entity example lisa ha pet cat named whisker whisker black white spot chest whisker also ha white paw look like little white mitten whisker like sleep sun favorite chair whisker also like drink creamy milk one possible extraction attribute entity following list ha whisker wiskers color black like sleep sun lisa favorite chair drink creamy mik
What features do NLP practitioners use to pick out English names?,"<p>I am trying named entity recognition for the first time. I'm looking for features that will pick out English names. I am using the methods outlined in the <a href=""https://class.coursera.org/nlp/lecture/preview"">coursera nlp course</a> (week three) and the <a href=""http://www.nltk.org/book/ch06.html"">nltk book</a>. In other words: I am defining features, identifying features of words and then running those words/features through a classifier that I train on labeled data. </p>

<p>What features are used to pick out English names?</p>

<p>I can imagine that you'd look for two capital words in a row, or a capital word and then an initial and then a capital word. (ex. John Smith or James P. Smith).</p>

<p>But what other features are used for NER?</p>
",Named Entity Recognition (NER),feature nlp practitioner use pick english name trying named entity recognition first time looking feature pick english name using method outlined word defining feature identifying feature word running word feature classifier train labeled data feature used pick english name imagine look two capital word row capital word initial capital word ex john smith james p smith feature used ner
Integrate Stanford NER in my application/ call web service,"<p>I am building an application in ASP .NET and want to use Stanford-Ner within my application.
Any idea on how to integrate it? 
I couldn't find the web service to use as well, do they have one?</p>

<p>I would like this functionality within my application: <a href=""http://nlp.stanford.edu:8080/ner/"" rel=""nofollow"">http://nlp.stanford.edu:8080/ner/</a>
Either as a web service or by integrating.</p>

<p>Thanks</p>
",Named Entity Recognition (NER),integrate stanford ner application call web service building application asp net want use stanford ner within application idea integrate find web service use well one would like functionality within application either web service integrating thanks
How to run multiple classifiers with Stanford NER?,"<p>I'd like to run one of the built-in classifiers on a file, then run my own classifier, merging the results.</p>

<p>How do I do so with Stanford NER, in particular, via the command line?</p>

<p>I am aware of <a href=""https://stackoverflow.com/questions/15746695/how-do-i-include-more-than-one-classifiers-when-using-stanford-named-entity-reco"">How do I include more than one classifiers when using Stanford named entity recogniser?</a> , but this is slightly different, as that questions asks about multiple classifiers with <strong>NERServer</strong>.</p>

<p>Looks like I need to use CoreNLP to run multiple NER models in sequence...can I do it without CoreNLP?</p>

<p>Say I had a file with contents ""the quick brown fox jumped over the lazy dog in America"". I run the one of the built-in classifiers, and it finds ""America"" as a location, then I run my own, and it finds ""fox"" and ""dog"", the result should be:</p>

<pre><code>the quick brown &lt;animal&gt;fox&lt;/animal&gt; jumped over the lazy &lt;animal&gt;dog&lt;/animal&gt; in &lt;location&gt;America&lt;/location
</code></pre>
",Named Entity Recognition (NER),run multiple classifier stanford ner like run one built classifier file run classifier merging result stanford ner particular via command line aware
How to use Naive Bayes for Named Entity Recognition,"<p>I am trying to program Named Entity recognition for a low resource language from the scratch. To start I would like to use Naive Bayes. 
My little Google search suggest me that it is widely used for text classification using naive bayes.
Can it be used to solve NER problem. Any suggestion/direction regarding this would be appreciated. I prefer Python language.</p>

<p>Other methods for solving the problem of NER from starts for a new language are also welcome.</p>
",Named Entity Recognition (NER),use naive bayes named entity recognition trying program named entity recognition low resource language scratch start would like use naive bayes little google search suggest widely used text classification using naive bayes used solve ner problem suggestion direction regarding would appreciated prefer python language method solving problem ner start new language also welcome
opennlp Span class in eclipse java,"<p>I am new to java opennlp and i am trying to implement a program that extracts city names from a file but i am testing my code on a string first and i get some errors 
the code is</p>

<pre><code> import java.io.FileInputStream;
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.InputStream;

 import main.java.opennlp.tools.namefind.NameFinderME;
 import main.java.opennlp.tools.namefind.TokenNameFinderModel;
 import main.java.opennlp.tools.util.InvalidFormatException;
 import main.java.opennlp.tools.util.Span;
 import opennlp.tools.tokenize.Tokenizer;
 import opennlp.tools.tokenize.TokenizerME;
 import opennlp.tools.tokenize.TokenizerModel;
 import opennlp.tools.tokenize.SimpleTokenizer;
 import opennlp.tools.sentdetect.SentenceDetectorME;
 import opennlp.tools.sentdetect.SentenceModel;

  import org.xml.sax.SAXException;

 public class CityFinder {

public String Tokens[];

public static void main(String[] args) throws IOException, SAXException {

    CityFinder toi = new CityFinder();
    String cnt;
    cnt=""John is planning to specialize in Electrical Engineering in UC Berkley and  pursue a career with IBM."";
    toi.tokenization(cnt);
    String cities = toi.namefind(toi.Tokens);
    String org = toi.orgfind(toi.Tokens);

    System.out.println(""City name is : ""+cities);
    System.out.println(""organization name is: ""+org);

}
    public String namefind(String cnt[]) {
    InputStream is;
    TokenNameFinderModel tnf;
    NameFinderME nf;
    String sd = """";
    try {
        is = new FileInputStream(""en-ner-location.bin"");
        tnf = new TokenNameFinderModel(is);
        nf = new NameFinderME(tnf);
        Span sp[] = nf.find(cnt);
        StringBuilder fd = new StringBuilder();
        int l = a.length;

        for (int j = 0; j &lt; l; j++) {
            fd = fd.append(a[j] + ""\n"");

        }
        sd = fd.toString();

    } catch (FileNotFoundException e) {

        e.printStackTrace();
    } catch (InvalidFormatException e) {

        e.printStackTrace();
    } catch (IOException e) {

        e.printStackTrace();
    }
    return sd;
}

public String orgfind(String cnt[]) {
    InputStream is;
    TokenNameFinderModel tnf;
    NameFinderME nf;
    String sd = """";
    try {
        is = new FileInputStream(""en-ner-organization.bin"");
        tnf = new TokenNameFinderModel(is);
        nf = new NameFinderME(tnf);
        Span sp[] = nf.find(cnt);
        String a[] = Span.spansToStrings(sp, cnt);
        StringBuilder fd = new StringBuilder();
        int l = a.length;
        for (int j = 0; j &lt; l; j++) {
            fd = fd.append(a[j] + ""\n"");

        }

        sd = fd.toString();

    } catch (FileNotFoundException e) {

        e.printStackTrace();
    } catch (InvalidFormatException e) {

        e.printStackTrace();
    } catch (IOException e) {

        e.printStackTrace();
    }
    return sd;

}
public void tokenization(String tokens) {

    InputStream is;
    TokenizerModel tm;
    try {
        is = new FileInputStream(""en-token.bin"");
        tm = new TokenizerModel(is);
        Tokenizer tz = new TokenizerME(tm);
        Tokens = tz.tokenize(tokens);
        // System.out.println(Tokens[1]);
    } catch (IOException e) {
        e.printStackTrace();
    }
}

  }
</code></pre>

<p>and the errors i am getting with the span verialble
    Span sp[] = nf.find(cnt);
the error is Type mismatch: cannot convert from opennlp.tools.util.Span[] to main.java.opennlp.tools.util.Span[]
I don't know how to solve it in both locations</p>

<p>Any suggestions....??
thanks in advance</p>
",Named Entity Recognition (NER),opennlp span class eclipse java new java opennlp trying implement program extract city name file testing code string first get error code error getting span verialble span sp nf find cnt error type mismatch convert opennlp tool util span main java opennlp tool util span know solve location suggestion thanks advance
Is there an established method for tagging your own corpus for supervised learning with NLTK?,"<p>I am getting ready to implement supervised named entity recognition on a specialized corpus. That means I need to label raw text by naming the entities. It seems like NLTK things about a <a href=""http://www.nltk.org/book/ch05.html"" rel=""nofollow noreferrer"">tagged token</a> as a tuple ""consisting of the token and the tag."" So my plan is to pull out some random lines from a file, put a word on each line, manually tag the words that are entities to make a csv file -- then read the csv file back in to create token/tag tuples. </p>

<p>Then do something like this (following the <a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow noreferrer"">example</a> from the docs):</p>

<pre><code>supervised = []
for line in file:
   token, tag = line.split("","")
   supervised.append(token, tag))

featuresets = [(feature_extractor(token), tag) for (token, tag) in supervised]
</code></pre>

<p>Is this how NLP practitioners usually do this? Is there a better way to do this? Is there a gold standard? Do people tag entities within the corpus structure? Sort of like this : <a href=""https://stackoverflow.com/questions/4951751/creating-a-new-corpus-with-nltk"">Creating a new corpus with NLTK</a></p>
",Named Entity Recognition (NER),established method tagging corpus supervised learning nltk getting ready implement supervised named entity recognition specialized corpus mean need label raw text naming entity seems like nltk thing tagged token tuple consisting token tag plan pull random line file put word line manually tag word entity make csv file read csv file back create token tag tuples something like following example doc nlp practitioner usually better way gold standard people tag entity within corpus structure sort like href new corpus nltk
"How do I tell if a noun is a person, place, or a thing?","<p>I am trying to classify text and then map the nouns on to a person, place, or a thing.  Is there a way or dictionary to do that?</p>
",Named Entity Recognition (NER),tell noun person place thing trying classify text map noun person place thing way dictionary
What is the difference between Named Entity Recognition and Named Entity Extraction?,"<p>Please help me understand the difference between Named Entity Recognition and Named Entity Extraction.</p>
",Named Entity Recognition (NER),difference named entity recognition named entity extraction please help understand difference named entity recognition named entity extraction
How to determine the &quot;sentiment&quot; between two named entities with Python/NLTK?,"<p>I'm using NLTK to extract named entities and I'm wondering how it would be possible to determine the sentiment between entities in the same sentence. So for example for ""Jon loves Paris."" i would get two entities Jon and Paris. How would I be able to determine the sentiment between these two entities? In this case should be something like Jon -> Paris = positive</p>
",Named Entity Recognition (NER),determine sentiment two named entity python nltk using nltk extract named entity wondering would possible determine sentiment entity sentence example jon love paris would get two entity jon paris would able determine sentiment two entity case something like jon paris positive
Using more than one model for entity extraction- OpenNLP,"<p>I have two model files as: 1)en-politicians-ner.bin 2)en-engineers-ner.bin</p>

<p>Now, is there a way to add these two models in a <strong>single NameFinderME object</strong>. Work around for using these two models may be to create two NameFinderME objects and Iterate over it to extract entities, But I don't want to do that.</p>
",Named Entity Recognition (NER),using one model entity extraction opennlp two model file en politician ner bin en engineer ner bin way add two model single namefinderme object work around using two model may create two namefinderme object iterate extract entity want
Named Entity Recognition Libraries for Java,"<p>I am looking for a simple but ""good enough"" Named Entity Recognition library (and dictionary) for java, I am looking to process emails and documents and extract some ""basic information"" like:
Names, places, Address and Dates</p>

<p>I've been looking around, and most seems to be on the heavy side and full NLP kind of projects. </p>

<p>Any recommendations ?</p>
",Named Entity Recognition (NER),named entity recognition library java looking simple good enough named entity recognition library dictionary java looking process email document extract basic information like name place address date looking around seems heavy side full nlp kind project recommendation
How I can get references for Natural Language processing(NLP) and Named Entity Recognition?,"<p>I am beginner researching in NLP and Named Entity Recognition.I want to have references tor theses.</p>
",Named Entity Recognition (NER),get reference natural language processing nlp named entity recognition beginner researching nlp named entity recognition want reference tor thesis
When manually tagging a corpus for NLP is it important to have untagged text as well?,"<p>I am doing manual tagging to train my own NER 
Do I have to include untagged text in sentences I am preparing for named entity recognition?</p>

<pre><code>&lt;START:person&gt; Olivier Grisel &lt;END&gt; is working on the &lt;START:software&gt; Stanbol &lt;END&gt; project .
</code></pre>

<p>Or can I omit untagged parts like this?</p>

<pre><code>&lt;START:person&gt; Olivier Grisel &lt;END&gt;
&lt;START:software&gt; Stanbol &lt;END&gt;
</code></pre>

<p><strong>PS:</strong>
Thanks for all the great answers. I tried omitting the untagged parts and in that case OpenNLP marked every line as an entity, so it didn't work. As the answers explain, untagged parts are necessary.</p>
",Named Entity Recognition (NER),manually tagging corpus nlp important untagged text well manual tagging train ner include untagged text sentence preparing named entity recognition omit untagged part like p thanks great answer tried omitting untagged part case opennlp marked every line entity work answer explain untagged part necessary
I need to maintain a multiple static lists of strings in Java for an NER Tagger,"<p>I'm working on a natural language processing project. I need to maintain multiple lists such as Most Common Cities, Most Common Names etc. I will check tokens against these list during the feature generation segment of my project. I am considering creating a static class with a collection of hash tables with look up methods for each list that returns a simple boolean. </p>

<p>Is there a more appropriate way to do this?</p>
",Named Entity Recognition (NER),need maintain multiple static list string java ner tagger working natural language processing project need maintain multiple list common city common name etc check token list feature generation segment project considering creating static class collection hash table look method list return simple boolean appropriate way
Named Entity recognition,"<p>I am using stanford ner for removing the identity from the essays.</p>

<p>It is detecting the names like Werner..But indian names such as ram, shyam etc. goes undetected.</p>

<p>What i should do to make them recognizable.</p>
",Named Entity Recognition (NER),named entity recognition using stanford ner removing identity essay detecting name like werner indian name ram shyam etc go undetected make recognizable
Open NLP Name Finder Output,"<p>I am starting to learn the OpenNLP API by Jave.
I found some good examples in this website</p>

<p><a href=""http://www.programcreek.com/2012/05/opennlp-tutorial/"" rel=""nofollow"">http://www.programcreek.com/2012/05/opennlp-tutorial/</a></p>

<p>I have tried the Name Finder API but I found something strange.
If I replace the input as</p>

<pre><code>String []sentence = new String[]{
            ""John"",
            ""is"",
            ""good""
            };
</code></pre>

<p>The code is still working, but if I change it as</p>

<pre><code>String []sentence = new String[]{
            ""John"",
            ""is"",
            ""fine""
            };
</code></pre>

<p>There is no output.</p>

<p>I cannot understand what causes the problem. Is it form the model I use? (en-ner-person.bin)
And does anyone know how can I build my own model?</p>

<p>Thanks!</p>
",Named Entity Recognition (NER),open nlp name finder output starting learn opennlp api jave found good example website tried name finder api found something strange replace input code still working change output understand cause problem form model use en ner person bin doe anyone know build model thanks
What is the best method to extract relevant info from Email?,"<p>My friend has a small business where customers order services using email. He receives several emails a day and sorting thru it is becoming cumbersome.</p>

<p>There are about 10 different kind of tasks the customer can request, and for each there are one or two words that specify it. The other info present in the emails is the place where the service is to be delivered, the time, and the involved people's names. The email also contains an ID, a long number with a fairly standard format.</p>

<p>The emails are very unstructured, but all contain the key info above. My question is: what is the best method to sweep thru these emails and extract the key info (such as type of service, place, people's names, the ID etc)? </p>

<p>I thought about some kind of pre-processing, then pass it thru AlchemyAPI and then test the Alchemy  output using Neural Networks for each feature (key info). This can be supervised learning as I can do a feedback loop all the time, as once the info is inputted, I can have someone to validate.</p>

<p>Any ideas? Thanks</p>
",Named Entity Recognition (NER),best method extract relevant info email friend ha small business customer order service using email receives several email day sorting thru becoming cumbersome different kind task customer request one two word specify info present email place service time involved people name email also contains id long number fairly standard format email unstructured contain key info question best method sweep thru email extract key info type service place people name id etc thought kind pre processing pas thru alchemyapi test alchemy output using neural network feature key info supervised learning feedback loop time info inputted someone validate idea thanks
How to &quot;update&quot; an existing Named Entity Recognition model - rather than creating from scratch?,"<p>Please see the tutorial steps for OpenNLP - Named Entity Recognition : <a href=""https://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html"" rel=""noreferrer"">Link to tutorial</a>
I am using the ""en-ner-person.bin"" model found <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""noreferrer"">here</a>
In the tutorial, there are instructions on Training and creating a new model. Is there any way to ""Update"" the existing ""en-ner-person.bin"" with additional training data? </p>

<p>Say I have a list of 500 additional person names that are otherwise not recognized as persons - how do I generate a new model?</p>
",Named Entity Recognition (NER),update existing named entity recognition model rather creating scratch please see tutorial step opennlp named entity recognition link tutorial using en ner person bin model found tutorial instruction training creating new model way update existing en ner person bin additional training data say list additional person name otherwise recognized person generate new model
How to tag text based on its category using OpenNLP?,"<p>I want to tag text based on the category it belongs to ... </p>

<p>For example ... </p>

<p>""Clutch and gear is monitored using microchip "" -> clutch /mechanical , gear/mechanical , microchip / electronic </p>

<p>""software used here to monitor hydrogen levels"" -> software/computer , hydrogen / chemistry ..</p>

<p>How to do this using openNLP or other NLP engines.</p>

<p><strong>MY WORKS</strong> <br>
I tried NER model , but It needs large number of training corpus which I don't have ?</p>

<p><strong>My Need</strong> </p>

<p>Do any ready made training corpus available for NER or  classification (it must contains scientific and engineering words).. ?</p>
",Named Entity Recognition (NER),tag text based category using opennlp want tag text based category belongs example clutch gear monitored using microchip clutch mechanical gear mechanical microchip electronic software used monitor hydrogen level software computer hydrogen chemistry using opennlp nlp engine work tried ner model need large number training corpus need ready made training corpus available ner classification must contains scientific engineering word
Menu extracting,"<p>I am interesting in extracting and structuring information about restaurant menus. What is needed is to extract the items from the menu in form <code>category / name / price</code></p>

<p>For instance, we have the following <a href=""http://www.vub.ac.be/english/infofor/prospectivestudents/restaurant.html#menu"" rel=""nofollow"">website</a>. Here we have a drinks sections, and there a number of items. For that website I'd like to be able to extract </p>

<pre><code>Drink / Cappuccino / € 1,50
SANDWICHES / filled sandwich, pistolet (round roll) or emperor roll / € 1,30
etc ...
</code></pre>

<p>Of course it shouldn't be limited only to this website.</p>

<p>The only way I can see to handle that is applying a bunch of regexps, but I don't believe listing all possible dish names is feasible. </p>

<p>I know that the topic might be too broad for a question, but anyway any suggestions or references to relevant articles or books will be much appreciated. </p>
",Named Entity Recognition (NER),menu extracting interesting extracting structuring information restaurant menu needed extract item menu form instance following website drink section number item website like able extract course limited website way see handle applying bunch regexps believe listing possible dish name feasible know topic might broad question anyway suggestion reference relevant article book much appreciated
"NLP discover city, state and Name from the given text","<p>I have a text file which was generated from an image using OCR (optical character recognition). The file contains records of information where part of each record contains a text of the format Customer name city and state. A sample of text is below</p>

<p>Benjamin Meeks Decatur , GA</p>

<p>Some times the text may be split across multiple lines. The text will always be in the given order. I have a static list of cities and states but still some records and states can come out of the list. The comma between the state and city may or may not present. The city and state text mostly would contain USA, UK, Canada, Australia etc. </p>

<p>From one my friend i came to know natural language processing can solve mining the categories of text from the given input.  I am noob to NLP so i am here for suggestions what are the techniques of NLP i can apply to extract the city, state and name. </p>

<p>I have googled for an openNLP library seems like apache openNLP seems to be the good library.</p>

<p>Thanks.</p>
",Named Entity Recognition (NER),nlp discover city state name given text text file wa generated image using ocr optical character recognition file contains record information part record contains text format customer name city state sample text benjamin meeks decatur ga time text may split across multiple line text always given order static list city state still record state come list comma state city may may present city state text mostly would contain usa uk canada australia etc one friend came know natural language processing solve mining category text given input noob nlp suggestion technique nlp apply extract city state name googled opennlp library seems like apache opennlp seems good library thanks
Extract a person&#39;s full name from a block of text in Perl?,"<p>I need to extract names (including uncommon names) from blocks of text using Perl. I've looked into <a href=""http://search.cpan.org/~dbourget/Text-Names-0.19/lib/Text/Names.pm"" rel=""nofollow"">this</a> module for extracting names, but it only has the top 1000 popular names and surnames in the US dating back to 1990; I need something a bit more comprehensive.</p>

<p>I've considered using the Social Security Index to make a database for comparison, but this seems very tedious and processing intensive. Is there a way to pull names from Perl using another method?</p>

<p>Example of text to parse:</p>

<blockquote>
  <p><b>LADNIER<br/> </b>  Louis Anthony Ladnier, [Louie] age 48, of Mobile, Alabama died at home Friday, November 16, 2012.<br/>   Louie was born January 9, 1964 in Mobile, Alabama. He was the son of John E. Ladnier, Sr. and Gloria Bosarge Ladnier.  He was a graduate of McGill-Toolen High School and attended University of South Alabama.  He was employed up until his medical retirement as Communi-cations Supervisor with the Bayou La Batre Police Department.  <br/>    He is preceded in death by his father, John. Survived by his mother, Gloria, nephews, Dominic Ladnier and Christian Rubio, whom he loved and help raise as his own sons, sisters, Marj Ladnier and Morgan Gordy [Julian], and brother Eddie Ladnier [Cindy], and nephews, Jamie, Joey, Eddie, Will, Ben and nieces, Anna and Elisabeth.<br/>   Memorial service will be held at St. Dominic's Catholic Church in Mobile on Wednesday at 1pm.   <br/>   Serenity Funeral Home is in charge of arrangements. <br/>  In lieu of flowers, memorials may be sent to St. Dominic School, 4160 Burma Road  Mobile, AL 36693, education fund for Christian Rubio and McGill-Toolen High School, 1501 Old Shell Road  Mobile, AL 36604, education Fund for Dominic Ladnier. <br/>   The family is grateful for all the prayers and support during this time.  Louie was a rock and a joy to us all.  <p> </p></p>
</blockquote>
",Named Entity Recognition (NER),extract person full name block text perl need extract name including uncommon name block text using perl looked module extracting name ha top popular name surname u dating back need something bit comprehensive considered using social security index make database comparison seems tedious processing intensive way pull name perl using another method example text parse ladnier louis anthony ladnier louie age mobile alabama died home friday november louie wa born january mobile alabama wa son john e ladnier sr gloria bosarge ladnier wa graduate mcgill toolen high school attended university south alabama wa employed medical retirement communi cation supervisor bayou la batre police department preceded death father john survived mother gloria nephew dominic ladnier christian rubio loved help raise son sister marj ladnier morgan gordy julian brother eddie ladnier cindy nephew jamie joey eddie ben niece anna elisabeth memorial service held st dominic catholic church mobile wednesday pm serenity funeral home charge arrangement lieu flower memorial may sent st dominic school burma road mobile al education fund christian rubio mcgill toolen high school old shell road mobile al education fund dominic ladnier family grateful prayer support time louie wa rock joy u
"Using Stanford CoreNLP/NER to extract titles (of books, articles, etc)?","<p>Is there some sequence of tags that could possibly indicate a title among a webpage? For example, extracting the title of the book from its amazon page, where other text/sentences may have similar sentence structures. I feel like this is an extremely fundamental task but cannot figure out exactly how to do it with Stanford's NER/CoreNLP.</p>

<p>Thanks in advance!</p>
",Named Entity Recognition (NER),using stanford corenlp ner extract title book article etc sequence tag could possibly indicate title among webpage example extracting title book amazon page text sentence may similar sentence structure feel like extremely fundamental task figure exactly stanford ner corenlp thanks advance
Named Entity Recognition Data and Features,"<p>I am building a Named Entity Recognizer with a Conditional Random Field and am looking for two things:</p>

<p>A) An open source, English NER dataset for Person, Location, and Organization entities</p>

<p>B) A list of English NER features</p>

<p>I have already looked at the CoNLL-2003 corpus and found this is exactly what I want but it is not readily available. I have been unsuccessful in finding a list of NER features; I am trying to avoid having to hand design these features. </p>

<p>Thanks</p>
",Named Entity Recognition (NER),named entity recognition data feature building named entity recognizer conditional random field looking two thing open source english ner dataset person location organization entity b list english ner feature already looked conll corpus found exactly want readily available unsuccessful finding list ner feature trying avoid hand design feature thanks
How to extract entities from html using natural language processing or other technique,"<p>I am trying to parse entities from web pages that contain a time, a place, and a name.  I read a little about natural language processing, and entity extraction, but I am not sure if I am heading down the wrong path, so I am asking here.</p>

<p>I haven't started implementing anything yet, so if certain open source libraries are only suitable for a specific language, that is ok.</p>

<p>A lot of times the data would not be found in sentences, but instead in html structures like lists (e.g. <ul><li>2013-02-01 - Name of Event - Arena Name</li></ul>).</p>

<p>The structure of the webpages will be vastly different (some might use lists, some might put them in a table, etc.).</p>

<p>What topics can I research to learn more about how to achieve this? 
Are there any open source libraries that take into account the structure of html when doing entity extraction?
Would extracting these (name, time, place) entities from html be better (or even possible) with machine vision where the CSS styling might make it easier to differentiate important parts (name, time, location) of the unstructured text?</p>

<p>Any guidance on topics/open source projects that I can research would help I think.</p>
",Named Entity Recognition (NER),extract entity html using natural language processing technique trying parse entity web page contain time place name read little natural language processing entity extraction sure heading wrong path asking started implementing anything yet certain open source library suitable specific language ok lot time data would found sentence instead html structure like list e g name event arena name structure webpage different might use list might put table etc topic research learn achieve open source library take account structure html entity extraction would extracting name time place entity html better even possible machine vision cs styling might make easier differentiate important part name time location unstructured text guidance topic open source project research would help think
Named-entity recognition with trained data,"<p>My text file t1.txt contained this</p>

<pre><code>&lt;START:name&gt; Ashish Sanadhya &lt;END&gt; , 61 years old , will join the board as a nonexecutive director Nov. 29 .
Mr . &lt;START:name&gt; mayank sharma &lt;END&gt; is chairman of Elsevier N.V. , the Dutch publishing group .
</code></pre>

<p>and t2.txt contained</p>

<pre><code>person mayank sharma
persons ashish sanadhya
organizations linkedin
</code></pre>

<p>I have  trained the data as the image shows<img src=""https://i.sstatic.net/Lk0o1.png"" alt=""enter image description here"">
but when tried to get back for the desired result as in</p>

<pre><code>&gt;s &lt;- paste(c(""I am ashish .""))
&gt; a2 &lt;- annotate(s, list(sent_token_annotator, word_token_annotator))
&gt; entity_annotator &lt;-  Maxent_Entity_Annotator(language = ""en"", kind = c(""person""), probs = FALSE,model =""C:\\apache-opennlp-1.5.3\\en-ner-person.bin"")
&gt;  entity_annotator(s, a2)
  [1] id    type  start end  
&lt;0 rows&gt; (or 0-length row.names)
</code></pre>

<p>and i was expecting the result after trained the person entity</p>

<pre><code> entity_annotator(s, a2)
 id type   start end features   
 1  entity 6    11  kind=person
 s[entity_annotator(s, a2)]
 ashish
</code></pre>

<p>any help ,why am I not getting expected result.thanks ,any help in this direction</p>

<p><strong>EDITED</strong></p>

<p>i have downloaded file <strong>en-ner-person.bin</strong> from <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow noreferrer"">here</a> and <strong>cutoff</strong> parameter work for me,i used this command</p>

<pre><code>c:\apache-opennlp-1.5.3&gt;bin\opennlp TokenNameFinderTrainer -cutoff 1 -lang en -encoding UTF-8 -data ""c:\t7.txt"" -model en-ner-person.bin
</code></pre>

<p>hope it help and special thanks to Daniel Naber.</p>
",Named Entity Recognition (NER),named entity recognition trained data text file txt contained txt contained trained data image show tried get back desired result wa expecting result trained person entity help getting expected result thanks help direction edited downloaded file en ner person bin cutoff parameter work used command hope help special thanks daniel naber
How to extract dates/places in text?,"<p>What are the best packages/software for extracting times/dates/places in text? </p>

<p>Or is there any dataset that could be used as dataset? </p>

<p>For example: </p>

<blockquote>
  <blockquote>
    <p>-- PLEASE JOIN US FOR COOKIES AND COFFEE AT [2:30PM] BEFORE THE SEMINAR IN [ROOM 154 COORDINATED SCIENCE LABORATORY] </p>
    
    <p>-- Then we'll meet on [Friday 6 pm] ... </p>
    
    <p>-- This week's seminar is moved to Tuesday, from [11:00 am to 12:00pm].</p>
    
    <p>Time: [11-12pm], [Oct. 29 Tuesday] </p>
    
    <p>Place: [SC 0216]</p>
  </blockquote>
</blockquote>

<p>Title Statistical significance of combinatorial features with frequent itemset mining ....</p>

<p>I can train a machine learning model for such a task, but I don't know any labelled dataset for this. Anyone aware of any labelled dataset?  </p>
",Named Entity Recognition (NER),extract date place text best package software extracting time date place text dataset could used dataset example please join u cooky coffee pm seminar room coordinated science laboratory meet friday pm week seminar moved tuesday pm time pm oct tuesday place sc title statistical significance combinatorial feature frequent itemset mining train machine learning model task know labelled dataset anyone aware labelled dataset
Detecting and ignoring mentioning of a named entity and extracting the valid named entity,"<p>I'm writing a java app that uses NLP for detecting named entities. I'm using the stanford university Named Entities code in my application. I've allready written a application to detect the names, compare them with a database. But I have a problem with the text itself.</p>
<p>I want to classify sentences in a text that have the mentioning of a name and ignore them.</p>
<p>Example:</p>
<blockquote>
<p>'....   This writer has the same writing style as Herman Melville.  .. '</p>
</blockquote>
<p>The named entity is Herman Melville, but the text is not about Herman Melville, but other writers. Herman Melville is a true negative then.</p>
<p>Another example</p>
<blockquote>
<p>The Orb.</p>
<p>Alex Paterson prides the Orb on manipulating obscure samples beyond recognition on its     albums and during its concerts; his unauthorised use of other artists’ works has led to     disputes with musicians, most notably with Rickie Lee Jones. During its live shows of the     1990s, the Orb performed using digital audio tape machines optimised for live mixing and     sampling before switching to laptops and digital media. Despite changes in performance     method, the Orb maintained its colourful light shows and psychedelic imagery in concert.     These visually intensive performances prompted critics to compare the group to Pink Floyd.</p>
</blockquote>
<p>The artists that are detected are 'The Orb' and 'Pink Floyd'. The text is about The Orb, but the group is compared with Pink Floyd. So I want to use NLP to ignore 'Pink Floyd' and detect 'The Orb' as the Named Entity as the subject.</p>
<p>I allready have a database with example texts, where the writers are allready detected. I could use this as a test set. And I have a database with all the writers that exist.</p>
<p>I would like to have some examples or stuff to read on how to solve this problem. Even a discussion would be nice.</p>
",Named Entity Recognition (NER),detecting ignoring mentioning named entity extracting valid named entity writing java app us nlp detecting named entity using stanford university named entity code application allready written application detect name compare database problem text want classify sentence text mentioning name ignore example writer ha writing style herman melville named entity herman melville text herman melville writer herman melville true negative another example orb alex paterson pride orb manipulating obscure sample beyond recognition album concert unauthorised use artist work ha led dispute musician notably rickie lee jones live show orb performed using digital audio tape machine optimised live mixing sampling switching laptop digital medium despite change performance method orb maintained colourful light show psychedelic imagery concert visually intensive performance prompted critic compare group pink floyd artist detected orb pink floyd text orb group compared pink floyd want use nlp ignore pink floyd detect orb named entity subject allready database example text writer allready detected could use test set database writer exist would like example stuff read solve problem even discussion would nice
How to use own ontology to classify dbpedia terms extracted with stanbol?,"<p>I am trying to build an application using Apache Stanbol which:</p>

<ol>
<li>recognizes entities from DBpedia</li>
<li><p>classifies these entities using an OWL ontology which extends the definition of the <code>dcterms:subject</code> of these entities to correspond to my custom OWL class, <code>OwnClass</code>:</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;owl:Class rdf:ID=""OwnClass""&gt;
&lt;/owl:Class&gt;  

&lt;rdf:Description rdf:about=""http://dbpedia.org/resource/Category:Branding""&gt;
  &lt;rdf:type rdf:resource=""#OwnClass""/&gt;
&lt;/rdf:Description&gt;
</code></pre></li>
</ol>

<p>So far I am able to extract the entities with their categories from DBpedia using the Stanbol Enhancer. But I cannot figure out where I have to integrate my ontology in order to classify these entities? My final goal is to have a JSON document reflecting the hierarchy: <code>OwnClass</code> &rArr; DBpedia category &rArr; entity. Is this possible with Stanbol and, if yes, how can I achieve this?</p>
",Named Entity Recognition (NER),use ontology classify dbpedia term extracted stanbol trying build application using apache stanbol recognizes entity dbpedia classifies entity using owl ontology extends definition entity correspond custom owl class far able extract entity category dbpedia using stanbol enhancer figure integrate ontology order classify entity final goal json document hierarchy dbpedia category entity possible stanbol yes achieve
Methods for extracting locations from text?,"<p>What are the recommended methods for extracting locations from free text? </p>

<p>What I can think of is to use regex rules like ""words ... in location"". But are there better approaches than this?</p>

<p>Also I can think of having a lookup hash table table with names for countries and cities and then compare every extracted token from the text to that of the hash table.</p>

<p>Does anybody know of better approaches?</p>

<p>Edit: I'm trying to extract locations from tweets text. So the issue of high number of tweets might also affect my choice for a method.</p>
",Named Entity Recognition (NER),method extracting location text recommended method extracting location free text think use regex rule like word location better approach also think lookup hash table table name country city compare every extracted token text hash table doe anybody know better approach edit trying extract location tweet text issue high number tweet might also affect choice method
Statistical language model: comparing word sequences of different lengths,"<p>I have an algorithm that extracts company names from text. It generally does a good job, however, it also sometimes extracts strings that look like company names, but obviously aren't.  For example, ""Contact Us"", ""Colorado Springs CO"", ""Cosmetic Dentist"" are obviously not company names. There are too many of such false positives to blacklist, so I want to introduce an algorithmic way of ranking the extracted strings, so that the lowest-ranking ones can be discarded.</p>

<p>Currently, I'm thinking of using a <a href=""http://en.wikipedia.org/wiki/Language_model"" rel=""nofollow"">statistical language model</a> to do this.  This model can score each string based on the product of the probabilities of each individual word in the string (considering the simplest <a href=""http://en.wikipedia.org/w/index.php?title=Unigram"" rel=""nofollow"">unigram</a> model). My question is: can such a model be used to compare word sequences of different lengths? Since probabilities are by definition less than 1, the probabilities for longer sequences is usually going to be smaller than for shorter sequences. This would bias the model against longer sequences, which isn't a good thing.</p>

<p>Is there a way to compare word sequences of different lengths using such statistical language models? Alternatively, is there a better way to achieve to score the sequences?</p>

<p>For example, with a bigram model and some existing data, this is what I get:</p>

<pre><code>python slm.py About NEC
        &lt;s&gt; about 6
        about nec 1
        nec &lt;/s&gt; 1
4.26701019773e-17
python slm.py NEC
        &lt;s&gt; nec 6
        nec &lt;/s&gt; 1
2.21887517189e-11
python slm.py NEC Corporation
        &lt;s&gt; nec 6
        nec corporation 3
        corporation &lt;/s&gt; 3593
4.59941029214e-13
python slm.py NEC Corporation of
        &lt;s&gt; nec 6
        nec corporation 3
        corporation of 41
        of &lt;/s&gt; 1
1.00929844083e-20
python slm.py NEC Corporation of America
        &lt;s&gt; nec 6
        nec corporation 3
        corporation of 41
        of america 224
        america &lt;/s&gt; 275
1.19561436587e-21
</code></pre>

<p>The indented lines show the bigrams and their frequency in the model. <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> are start and end of sentence, respectively. The problem is, the longer the sentence, the less probable it is, regardless of how often its constituent bigrams occur in the database.</p>
",Named Entity Recognition (NER),statistical language model comparing word sequence different length algorithm extract company name text generally doe good job however also sometimes extract string look like company name obviously example contact u colorado spring co cosmetic dentist obviously company name many false positive blacklist want introduce algorithmic way ranking extracted string lowest ranking one discarded currently thinking using statistical language model model score string based product probability individual word string considering simplest unigram model question model used compare word sequence different length since probability definition le probability longer sequence usually going smaller shorter sequence would bias model longer sequence good thing way compare word sequence different length using statistical language model alternatively better way achieve score sequence example bigram model existing data get indented line show bigram frequency model start end sentence respectively problem longer sentence le probable regardless often constituent bigram occur database
NoSuchFieldError Exception in Stanford NER,"<p>I'm using the Stanford NER, and whenever I try to initialize the classifier I get the following exception:</p>

<pre><code>Exception in thread ""main"" java.lang.NoSuchFieldError: featureFactoryArgs
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.&lt;init&gt;(AbstractSequenceClassifier.java:127)
    at edu.stanford.nlp.ie.crf.CRFClassifier.&lt;init&gt;(CRFClassifier.java:173)
    at edu.stanford.nlp.ie.crf.CRFClassifier.getClassifierNoExceptions(CRFClassifier.java:3518)
</code></pre>

<p>My code:</p>

<pre><code>String serializedClassifier = ""classifiers/english.all.3class.caseless.distsim.crf.ser.gz"";
        AbstractSequenceClassifier&lt;CoreLabel&gt; classifier =
                CRFClassifier.getClassifierNoExceptions(serializedClassifier);
</code></pre>
",Named Entity Recognition (NER),nosuchfielderror exception stanford ner using stanford ner whenever try initialize classifier get following exception code
Named entity recognition promlem to identify the text &quot;next monday&quot; as date?,"<p>I'm new to text mining and NLP. I'm trying to use named entity recognition (NER) (Stanford Named Entity Tagger) to extract dates from the given text. I'm using the online demo provided in Stanford NLP <a href=""http://nlp.stanford.edu:8080/ner/process"" rel=""nofollow"">http://nlp.stanford.edu:8080/ner/process</a> and GATE ANNIE <a href=""http://services.gate.ac.uk/annie/"" rel=""nofollow"">http://services.gate.ac.uk/annie/</a></p>

<p>This demo is not able to recognize the texts like complete ""last Sunday"", ""next Monday"", ""this month end"" ""till this Sunday evening"" as date. Sunday or Monday alone will not be useful to determine the date.  Is there any option to extract the actual text mentioned by the given example texts? </p>

<p>Example text:</p>

<p>Treat yourself with Puma as it offers Flat 50% off. Hurry offer valid till this Sunday. Happy Shopping.</p>

<p>Extracted date: 25-08-2013 (Considering today is 19-08-2013. Date format can be anything)</p>

<p>Any library provides this kind of date recognition feature or is it possible to build custom model to recognize date as given in the example text? </p>
",Named Entity Recognition (NER),named entity recognition promlem identify text next monday date new text mining nlp trying use named entity recognition ner stanford named entity tagger extract date given text using online demo provided stanford nlp gate annie demo able recognize text like complete last sunday next monday month end till sunday evening date sunday monday alone useful determine date option extract actual text mentioned given example text example text treat puma offer flat hurry offer valid till sunday happy shopping extracted date considering today date format anything library provides kind date recognition feature possible build custom model recognize date given example text
Java Algorithm To Extract Information From a String,"<p>I'm trying to implement a smart search feature in my application.
Usecase: The user enters the search term in a textbox</p>

<p>Eg: <em>Find me a christian male 28 years old from Brazil.</em> </p>

<p>I need to be parse the input into a map as follows:</p>

<p><strong>Gender:</strong> male
<strong>Age:</strong> 38
<strong>Location:</strong> Brazil
<strong>Relegion:</strong> Christian</p>

<p>Already had a glance on : OpenNLP, Cross Validate, Java Pattern Matching and Regex, Information Extraction. I'm confused which one I need to look deeper into.</p>

<p>Is there any <strong>java</strong> lib already available for this specific domain? </p>
",Named Entity Recognition (NER),java algorithm extract information string trying implement smart search feature application usecase user enters search term textbox eg find christian male year old brazil need parse input map follows gender male age location brazil relegion christian already glance opennlp cross validate java pattern matching regex information extraction confused one need look deeper java lib already available specific domain
how to use Entity Recognition with Apache solr and LingPipe or similar tools,"<p>I would like to use NLP while indexing the data with Apache Solr.</p>

<ol>
<li><p>Identify the synonyms of the words and index that also.</p></li>
<li><p>Identify thenamed entity and label it while indexing. </p></li>
<li><p>when some one query the Solr Index, I should able to extract the
named entity and intention from the query and form the query string,
so that it can effectively search the indexed file.</p></li>
</ol>

<p>Is there any tools / plugins available to satisfy my requirements? I believe it is a common use cases for most of the content based websites. How people handling it?  </p>
",Named Entity Recognition (NER),use entity recognition apache solr lingpipe similar tool would like use nlp indexing data apache solr identify synonym word index also identify thenamed entity label indexing one query solr index able extract named entity intention query form query string effectively search indexed file tool plugins available satisfy requirement believe common use case content based website people handling
How to extract entity using stanford parser?,"<p>I am using OpenNLP but it is not giving me exact name,location,organization entity. How to extract entity using stanford parser?</p>
",Named Entity Recognition (NER),extract entity using stanford parser using opennlp giving exact name location organization entity extract entity using stanford parser
How I train an Named Entity Recognizer identifier in OpenNLP?,"<p>Ok, I have the following code to train the NER Identifier from OpenNLP</p>

<pre><code>FileReader fileReader = new FileReader(""train.txt"");
ObjectStream fileStream = new PlainTextByLineStream(fileReader);
ObjectStream sampleStream = new NameSampleDataStream(fileStream);
TokenNameFinderModel model = NameFinderME.train(""pt-br"", ""train"", sampleStream, Collections.&lt;String, Object&gt;emptyMap());
nfm = new NameFinderME(model); 
</code></pre>

<p>I don't know if I'm doing something wrong of if something is missing, but the classifying is not working. I'm supposing that the train.txt is wrong.</p>

<p><strong>The error</strong> that occurs is that all tokens are classified to only one type.</p>

<p>My train.txt data is something like the following example, but with a lot more of variation and quantity of entries. Another thing is that I'm classifind word by word from a text per time, and not all tokens.</p>

<pre><code>&lt;START:distance&gt; 8000m &lt;END&gt;
&lt;START:temperature&gt; 100ºC &lt;END&gt;
&lt;START:weight&gt; 50kg &lt;END&gt;
&lt;START:name&gt; Renato &lt;END&gt;
</code></pre>

<p>Somebody can show what I doing wrong?</p>
",Named Entity Recognition (NER),train named entity recognizer identifier opennlp ok following code train ner identifier opennlp know something wrong something missing classifying working supposing train txt wrong error occurs token classified one type train txt data something like following example lot variation quantity entry another thing classifind word word text per time token somebody show wrong
How to increase the running time performance of Stanford Named Entity classifier?,"<p>I'm using the Stanford Named Entity toolkit with social media streams. However using that huge number of documents/sentences, I need to enhance the running time performance of the recognizer/classifier. I was wondering what are some techniques that I could do in order to solve this problem.</p>

<p>I need to mention that I only need to recognize one class of entities, organization.</p>
",Named Entity Recognition (NER),increase running time performance stanford named entity classifier using stanford named entity toolkit social medium stream however using huge number document sentence need enhance running time performance recognizer classifier wa wondering technique could order solve problem need mention need recognize one class entity organization
Stanford NER prop file meaning of DistSim,"<p>In one of the example .prop files coming with the Stanford NER software there are two options I do not understand:</p>

<pre><code>useDistSim = true
distSimLexicon = /u/nlp/data/pos_tags_are_useless/egw4-reut.512.clusters
</code></pre>

<p>Does anyone have a hint what DistSim stands for and where I can find any more documentation on how to use these options?</p>

<p>UPDATE: I just found out that DistSim means distributional similarity. I still wonder what that means in this context.</p>
",Named Entity Recognition (NER),stanford ner prop file meaning distsim one example prop file coming stanford ner software two option understand doe anyone hint distsim stand find documentation use option update found distsim mean distributional similarity still wonder mean context
Can I identify intranet page content using Named Entity Recognition?,"<p>I am new to Natural Language Processing and I want to learn more by creating a simple project. <a href=""http://nltk.org/"" rel=""nofollow"">NLTK</a> was suggested to be popular in NLP so I will use it in my project.</p>

<p>Here is what I would like to do:</p>

<ul>
<li>I want to scan our company's intranet pages; approximately 3K pages</li>
<li>I would like to parse and categorize the content of these pages based on certain criteria such as: HR, Engineering, Corporate Pages, etc...</li>
</ul>

<p>From what I have read so far, I can do this with Named Entity Recognition. I can describe entities for each category of pages, train the NLTK solution and run each page through to determine the category.</p>

<p>Is this the right approach? I appreciate any direction and ideas...</p>

<p>Thanks</p>
",Named Entity Recognition (NER),identify intranet page content using named entity recognition new natural language processing want learn creating simple project nltk wa suggested popular nlp use project would like want scan company intranet page approximately k page would like parse categorize content page based certain criterion hr engineering corporate page etc read far named entity recognition describe entity category page train nltk solution run page determine category right approach appreciate direction idea thanks
How to improve dutch NER chunkers in NLTK,"<p>Thanks to this great answer I got a good start training my own NE chunker for Dutch, using NLTK and the Conll2002 corpus: <a href=""https://stackoverflow.com/questions/11293149/nltk-named-entity-recognition-in-dutch"">NLTK named entity recognition in dutch</a>. Using these hints I was also able to easily train an improved tagger (bases on IIS classification) that tags at around 95% accuracy, which is enough for my purposes.</p>

<p>However, the F-measure of the named entity recognition is only around 40%. How can I improve this? I tried using built in algorithms like Maxent, but I only get a memory error. Then I moved on to try and get Megam to work, but it won't compile on my Windows machine and there is no binary available anymore. I also ran into dead ends trying to incorporate other software or methods like libSVM, YamCha, CRF++ and Weka. All have their own manual and problems, which seem to keep stacking up. So I'm feeling a bit overwhelmed.</p>

<p>What I need is a practical approach to NER for Dutch. There has been a lot of research and I found papers quoting F-measures between 70% and 85%. That would be great! Does anyone have a hint as to where I could find an improved implementation or how I could build one myself (using Windows)? I would prefer to use NLTK for it's flexibility, but if there is a standard solution in a different toolkit I'm game for that, too. Even commercial tools would be welcome.</p>

<p>Here is the code I use for the evaluation now:</p>

<pre><code>import nltk

from nltk.corpus import conll2002

tokenizer = nltk.data.load('tokenizers/punkt/dutch.pickle')
tagger = nltk.data.load('taggers/conll2002_ned_IIS.pickle')
chunker = nltk.data.load('chunkers/conll2002_ned_NaiveBayes.pickle')

test_sents = conll2002.tagged_sents(fileids=""ned.testb"")[0:1000]

print ""tagger accuracy on test-set: "" + str(tagger.evaluate(test_sents))

test_sents = conll2002.chunked_sents(fileids=""ned.testb"")[0:1000]

print chunker.evaluate(test_sents)

# chunker trained with following commandline: 
# python train_chunker.py conll2002 --fileids ned.train --classifier NaiveBayes --filename /nltk_data/chunkers/conll2002_ned_NaiveBayes.pickle
</code></pre>
",Named Entity Recognition (NER),improve dutch ner chunkers nltk thanks great answer got good start training ne chunker dutch using nltk conll corpus href named entity recognition dutch using hint wa also able easily train improved tagger base ii classification tag around accuracy enough purpose however f measure named entity recognition around improve tried using built algorithm like maxent get memory error moved try get megam work compile window machine binary available anymore also ran dead end trying incorporate software method like libsvm yamcha crf weka manual problem seem keep stacking feeling bit overwhelmed need practical approach ner dutch ha lot research found paper quoting f measure would great doe anyone hint could find improved implementation could build one using window would prefer use nltk flexibility standard solution different toolkit game even commercial tool would welcome code use evaluation
Extract only complete setences from emails using Python?,"<p>I have thousands of emails stored in either plain text or HTML.  All of the plain text emails are formatted pretty much the same, so extracting just the actual email message has been simple.</p>

<p>But the HTML emails are all over the place, and I'm finding it difficult to come up with a mathod of extracting the body message only.  There's a lot of other junk in the email that >I don't want, such as ""This email was generated by..."" and a bunch of other non-user generated text that changes from email to email.</p>

<p>Is there some way for Python to identify what resembles a body of text or complete sentences?</p>

<p>I've already tried using regular expressions found here:
<a href=""https://stackoverflow.com/questions/8465335/a-regex-for-extracting-sentence-from-a-paragraph-in-python"">a Regex for extracting sentence from a paragraph in python</a></p>

<p>But the problem with that was that I have a lot of lines that look like this:</p>

<p>Title* : Mr.</p>

<p>Which the regular expression thinks is a sentence and I don't want extracted.</p>

<p>I've also tried combining that regular expression with NLTK's POS tagger to only print out sentences that have both a Noun and a Verb, but I it doesn't seem to work to well as it's just the built in POS tagger and not trained on any dataset.</p>

<p>So I guess my question is: how can I fix my problem?  Am I missing something?</p>
",Named Entity Recognition (NER),extract complete setences email using python thousand email stored either plain text html plain text email formatted pretty much extracting actual email message ha simple html email place finding difficult come mathod extracting body message lot junk email want email wa generated bunch non user generated text change email email way python identify resembles body text complete sentence already tried using regular expression found href regex extracting sentence paragraph python problem wa lot line look like title mr regular expression think sentence want extracted also tried combining regular expression nltk po tagger print sentence noun verb seem work well built po tagger trained dataset guess question fix problem missing something
Training large data set using OpenNLP,"<p>I have data set with <code>.train</code> file and its very large file say 100MB file. I want to perform NER for extracting organization names. I trained using OpenNLP.</p>

<p>Sample Code:</p>

<pre><code>TokenNameFinderModel model=NameFinderME.train(""en"",""organization"",
              sampleStream,Collections.&lt;String, Object&gt;emptyMap()); 
</code></pre>

<p>But I get an error: <code>ArrayIndexOutofBoundException</code>.</p>

<p>Is there any way to train large data set using openNLP for NER? Could you post sample code?</p>

<p>When I Googled I found Class GIS and DataIndexer interface can be used to train large data set but I do know how? Could you post sample code?</p>
",Named Entity Recognition (NER),training large data set using opennlp data set file large file say mb file want perform ner extracting organization name trained using opennlp sample code get error way train large data set using opennlp ner could post sample code googled found class gi dataindexer interface used train large data set know could post sample code
Gate Named Entity with ANNIE using IKVM in .net,"<p>I am looking for some guidance on using Gate and ANNIE in a .net enviornment.  Has anyone converted GATE to a .NET DLL using IKVMC, and had much success running named entity recognition in .NET/C# using the converted DLL?</p>

<p>Thanks in advance.</p>
",Named Entity Recognition (NER),gate named entity annie using ikvm net looking guidance using gate annie net enviornment ha anyone converted gate net dll using ikvmc much success running named entity recognition net c using converted dll thanks advance
Which Twitter API should I use to extract large amounts of tweets for NLP research?,"<p>I'd like to extract as many tweets containing a given keyword (typically a company name) as possible.</p>

<p>I've been using the Twitter Search API, but it's limited to ""recent tweets"". So for a relatively rare keyword, I can get no more than 500 tweets.</p>

<p>Twitter say that you shouldn't use the Search API for research. So, which API should I use ?</p>
",Named Entity Recognition (NER),twitter api use extract large amount tweet nlp research like extract many tweet containing given keyword typically company name possible using twitter search api limited recent tweet relatively rare keyword get tweet twitter say use search api research api use
How can one resolve synonyms in named-entity recognition?,"<p>In natural language processing, named-entity recognition is the challenge of, well, recognizing named entities such as organizations, places, and most importantly <em>names</em>.</p>

<p>There is a major challenge in this though that I call that of <em>synonymy</em>: <em>The Count</em> and <em>Dracula</em> are in fact referring to the same person, but it it possible that this is never discussed directly in the text.</p>

<p>What would be the best algorithm to resolve these synonyms?</p>

<hr>

<p>If there is a feature for this in any Python-based library, I'm eager to be educated.  I'm using NLTK.</p>
",Named Entity Recognition (NER),one resolve synonym named entity recognition natural language processing named entity recognition challenge well recognizing named entity organization place importantly name major challenge though call synonymy count dracula fact referring person possible never discussed directly text would best algorithm resolve synonym feature python based library eager educated using nltk
Working with named-entity datasets,"<p>I am working on a classification task where we are building models that detect the type of an entity present in a span of text (ie, annotation). These models can be built with a dataset where each instance is represented by three independent text variables:</p>

<ul>
<li><strong>pre-context:</strong> document text before the annotation.</li>
<li><strong>annotation:</strong> span of the document where we want to detect the entity type. If no entity exists, all the entity type columns (isPerson, isOrganization, isTime) are marked 0</li>
<li><strong>post-context:</strong> document text after the annotation.</li>
</ul>

<p>Data Set 1: Entity type classification in spans of text.</p>

<pre><code>preContext  | annotation       | postContext | isOrganization | isPerson | isTime 
....        | on July 12, 2011 | ....        | 0              | 0        | 1 
With over 8 | million invested | in Chrysler | 0              | 0        | 0
</code></pre>

<p>Data Set 2: Boundary detection - ""start-of-entity""</p>

<p>In the first example, the transition between preContext and text marks the start of an organization-type entity. In the second example, there is no entity present at the transition between preContext and text, therefore all of the dependent variable columns are marked as zero.</p>

<pre><code>preContext          | text
    | isStartOfOrganization | isStartOfPerson | isStartOfTime
Private equity firm | Westbridge Capital could exit part or all of its stake in Hyderabad-based technology firm.
    | 1 | 0 | 0
</code></pre>

<p>I been using basic NLP techniques like TF/IDF, N-grams, Tokenizers, Stemmers, POS Taggers, Stoplist for the above problem. But I now really want to do is to experiment with some new technique other than what I tried. This is my Problem and I couldn't able to find any valid techniques. If you can suggest me It will be great i.e The only way to make significant further gains is to start to start thinking outside the box!. 
Could you please suggest me some new techniques for solving above problems? </p>
",Named Entity Recognition (NER),working named entity datasets working classification task building model detect type entity present span text ie annotation model built dataset instance represented three independent text variable pre context document text annotation annotation span document want detect entity type entity exists entity type column isperson isorganization istime marked post context document text annotation data set entity type classification span text data set boundary detection start entity first example transition precontext text mark start organization type entity second example entity present transition precontext text therefore dependent variable column marked zero using basic nlp technique like tf idf n gram tokenizers stemmer po tagger stoplist problem really want experiment new technique tried problem able find valid technique suggest great e way make significant gain start start thinking outside box could please suggest new technique solving problem
Food information extraction,"<p>I'm working with the USDA nutrition database, whose foods have the following description:</p>

<p>For example:</p>

<pre><code>Cheese, fontina
Cheese, cheddar
Cheese, cottage, lowfat, 2% milkfat
Cheese, cottage, lowfat, 1% milkfat
Apples, raw, with skin
Apples, dried, sulfured, uncooked
Apples, frozen, unsweetened, heated
McDONALD'S, BIG MAC (without Big Mac Sauce)
McDONALD'S, BIG MAC
Sandwiches and burgers, roast beef sandwich with cheese
</code></pre>

<p>There's a pattern here, the commas are clearly used to separate entities. Following the example above, cheese is a parent of cheddar, cottage and fontina.</p>

<p>I've already done some work in order to extract information from this source. I thought that with:</p>

<ul>
<li>POS tagging: if a word is an adjective or a verb, is not part of the food's name</li>
<li>freqdist/wordcount: this was done in order to obtain a hierarchy of words in a food's description</li>
</ul>

<p>But I get unnacurate results when I run it in large scale. The POS tagging failed in some descriptions and the freqdist/wordcount wasn't useful when in a same sentence there were word with similar frecuency.</p>

<p>This is an example of the result I would like to get:</p>

<p>input data:</p>

<pre><code>Cheese, fontina
Cheese, cheddar
Cheese, cottage, lowfat, 2% milkfat
Cheese, cottage, lowfat, 1% milkfat
</code></pre>

<p>output data:</p>

<pre><code>Cheese is the parent of fontina, cottage and cheddar. lowfat is a ""characteristic"" cheese cottage. Cottage, cheddar and fontina are the ""principal foods"".
</code></pre>

<p>input data:</p>

<pre><code>Sandwiches and burgers, roast beef sandwich with cheese
</code></pre>

<p>output data:</p>

<pre><code>Cheese is a characteristic of roast beef sandwich. The category of the food is    sandwiches and burgers and the ""principal food"" is roast beef sandwich. 
</code></pre>

<p>I'm a beginner so I'd like to get some guidance about it. There is a lot of information on NLP and it's hard to determine which path to take without having a wide knowledge in the subject.</p>
",Named Entity Recognition (NER),food information extraction working usda nutrition database whose food following description example pattern comma clearly used separate entity following example cheese parent cheddar cottage fontina already done work order extract information source thought po tagging word adjective verb part food name freqdist wordcount wa done order obtain hierarchy word food description get unnacurate result run large scale po tagging failed description freqdist wordcount useful sentence word similar frecuency example result would like get input data output data input data output data beginner like get guidance lot information nlp hard determine path take without wide knowledge subject
Efficiently Compare Successive Characters in String,"<p>I'm doing some text analysis, and need to record the frequencies of character transitions in a <code>String</code>. I have <em>n</em> categories of characters: for the sake of example, <code>isUpperCase()</code>, <code>isNumber()</code>, and <code>isSpace()</code>.</p>

<p>Given that there are <em>n</em> categories, there will be <em>n^2</em> categories of transitions, e.g. ""<code>isUpperCase()</code> --> <code>isUpperCase()</code>"", ""<code>isUpperCase</code> --> <code>isLetter()</code>"", ""<code>isLetter()</code> --> <code>isUpperCase()</code>"", etc.</p>

<p>Given a block of text, I would like to record the number of transitions that took place. I would imagine constructing a <code>Map</code> with the transition types as the <code>Keys</code>, and an <code>Integer</code> as each <code>Value</code>. </p>

<p>For the block of text ""<code>TO</code>"", the <code>Map</code> would look like <code>[isUpper -&gt; isUpper : 1, isUpper -&gt; isSpace : 1]</code></p>

<p>The part I cannot figure out, though, is how to construct a <code>Map</code> where, from what I can see, the <code>Key</code> would consist of 2 <code>boolean</code> methods.</p>
",Named Entity Recognition (NER),efficiently compare successive character string text analysis need record frequency character transition n category character sake example given n category n category transition e g etc given block text would like record number transition took place would imagine constructing transition type block text would look like part figure though construct see would consist method
Entity Extraction Library,"<p>I’m looking for a library that does text analysis and extract entities.</p>

<p>The type/classification of an entity is not critical, it’s the identification of something that’s worthwhile that is critical. The entities universe in this case is infinite, it’s not bounded by fixed dictionary.</p>

<p>It seems that there are a couple of web services that do that (NERD let you compare the results of these web services: <a href=""http://nerd.eurecom.fr/documentation"" rel=""nofollow noreferrer"">http://nerd.eurecom.fr/documentation</a> which is pretty useful), but I’m looking for a local library and not a remotely hosted service. I’d prefer Java or .NET but if it’s a good library I’ll learn whatever language that it’s written in. </p>

<p>There are few older threads on similar topic and I was hoping to find new development in this area, and/or libraries built on top of lower level NLP libraries:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/7455188/entity-extraction-recognition-with-free-tools-while-feeding-lucene-index"">Entity Extraction/Recognition with free tools while feeding Lucene Index</a></li>
<li><a href=""https://stackoverflow.com/questions/4199382/lucene-entity-extraction"">Lucene Entity Extraction</a></li>
<li><a href=""https://stackoverflow.com/questions/4308132/how-do-i-do-entity-extraction-in-lucene"">How do I do Entity Extraction in Lucene</a></li>
<li><a href=""https://stackoverflow.com/questions/tagged/named-entity-extraction"">https://stackoverflow.com/questions/tagged/named-entity-extraction</a></li>
<li><a href=""https://stackoverflow.com/questions/tagged/named-entity-recognition"">https://stackoverflow.com/questions/tagged/named-entity-recognition</a></li>
</ul>

<p>Does anyone know about a good library that does a decent job? </p>
",Named Entity Recognition (NER),entity extraction library looking library doe text analysis extract entity type classification entity critical identification something worthwhile critical entity universe case infinite bounded fixed dictionary seems couple web service nerd let compare result web service pretty useful looking local library remotely hosted service prefer java net good library learn whatever language written older thread similar topic wa hoping find new development area library built top lower level nlp library href ul doe anyone know good library doe decent job
How to perform website benchmarking?,"<p>I am trying to do competitive analysis of online trends prevailing in real estate domain at state level in a country. I have to create a report which is not biased towards any particular company but it compares or just shows how the companies are performing for a list of trends. I will use parameters of <code>Clickstream analysis</code> to show the statistics of how the websites of the company perform. The trend specific performance can be depicted by <code>Sentiment Analysis</code> in my opinion. If there is some other way to do it in an effective manner I am looking forward to any such approach.</p>

<p>Now, I am not able to find any trends that come in common.</p>

<ul>
<li><strong>How can I find general trends which will be common for all real estate comapnies ?</strong></li>
</ul>

<p>I tried using <code>Google Trends</code>. They provide graphical and demographic information regarding a particular search term and lists related terms to the search which I am clueless how to use. And as I drill down from country to state, the amount data is very less.</p>

<p>Once I have the trends then I've to find how people are reacting to those trends. <code>Sentiment Analysis</code> is the thing which will provide me this info. </p>

<ul>
<li><strong>But even if I get the trends how will I get trend specific data from which I can calculate its polarity ?</strong></li>
</ul>

<p>Twitter and other social media sites can provide some data on which sentiment analysis can be performed. I used <a href=""http://twitrratr.com/"" rel=""nofollow"">this</a> site which gives the positive, negative and neutral behaviour related to some term on twitter. I need something analogous to this but the dataset on which this analysis can be performed should not be limited to social media only.</p>

<ul>
<li><strong>Are there any other entities I can add in this competitive analysis
report ?</strong></li>
</ul>

<p>The report will be generated on monthly basis. And I want maximum amount of automation in above tasks. I am thinking of using web-scraping also to scrape data of similar format. I would also like to know what data I should scrape and what data I should manually extract.</p>
",Named Entity Recognition (NER),perform website benchmarking trying competitive analysis online trend prevailing real estate domain state level country create report biased towards particular company compare show company performing list trend use parameter show statistic website company perform trend specific performance depicted opinion way effective manner looking forward approach able find trend come common find general trend common real estate comapnies tried using provide graphical demographic information regarding particular search term list related term search clueless use drill country state amount data le trend find people reacting trend thing provide info even get trend get trend specific data calculate polarity twitter social medium site provide data sentiment analysis performed used site give positive negative neutral behaviour related term twitter need something analogous dataset analysis performed limited social medium entity add competitive analysis report report generated monthly basis want maximum amount automation task thinking using web scraping also scrape data similar format would also like know data scrape data manually extract
Focused Named Entity Recognition (NER)?,"<p>I want to recognize named entities in a specific field (e.g. baseball). I know there are  tools available like StanfordNER, LingPipe, AlchemyAPI and I have done a little testing with them. But what I want them to be is field specific as I mentioned earlier. How this is possible? </p>
",Named Entity Recognition (NER),focused named entity recognition ner want recognize named entity specific field e g baseball know tool available like stanfordner lingpipe alchemyapi done little testing want field specific mentioned earlier possible
NLTK extracting terms of chunker parse tree,"<blockquote>
<p><strong>John Edward Grey</strong> started <strong>running</strong> now that he knows he is <strong>fat</strong></p>
<p><strong>She</strong> was <strong>listening</strong> to <strong>smack that</strong> by that <strong>awful singer</strong></p>
</blockquote>
<p>I want to extract interesting terms from a sentence. I currently use POS tagging to identify grammatical types of each entity. Then I update each token to a counter (with different weights for nouns, verbs and adjectives).</p>
<p>I now wish to use a chunker for this. I think the <strong>leaf nodes of the parse tree holds all interesting words and phrases</strong>. How do I extract the terms from a chunker output?</p>
",Named Entity Recognition (NER),nltk extracting term chunker parse tree john edward grey started running know fat wa listening smack awful singer want extract interesting term sentence currently use po tagging identify grammatical type entity update token counter different weight noun verb adjective wish use chunker think leaf node parse tree hold interesting word phrase extract term chunker output
LingPipe dictionary size,"<p>I have created a test program based on the LingPipe DictionaryChunker example.  I am reading in dictionary values into the MapDictionary from a file.  When the file exceeds 100,000 entries the parser startes to return garbage:</p>

<p>for 10k rows (tail -10000 nameList.txt > shortNameList.txt)</p>

<p>TEXT=now is the time for all good men to come to the aid of their country Zoe Rogers now is the time for all good men to come to the aid of their country</p>

<p>Chunker. All matches=false Case sensitive=false
     phrase=|Zoe Rogers| start=69 end=79 type= PLAYER  score=1.0</p>

<p>for 100k rows (tail -100000 nameList.txt > shortNameList.txt)</p>

<p>TEXT=now is the time for all good men to come to the aid of their country Zoe Rogers now is the time for all good men to come to the aid of their country</p>

<p>Chunker. All matches=false Case sensitive=false
     phrase=|now is the time for all good men| start=0 end=32 type= PLAYER  score=1.0
     phrase=|to come to the aid of their country| start=33 end=68 type= PLAYER  score=1.0
     phrase=|Zoe Rogers now is the time for all| start=69 end=103 type= PLAYER  score=1.0
     phrase=|good men to come to the aid of| start=104 end=134 type= PLAYER  score=1.0</p>

<p>is there a better option for creating the MapDictionary?</p>

<p>I have twiddled the memory constraints on the VM, and that does not seem to help.</p>

<p>any ideas?</p>
",Named Entity Recognition (NER),lingpipe dictionary size created test program based lingpipe dictionarychunker example reading dictionary value mapdictionary file file exceeds entry parser startes return garbage k row tail namelist txt shortnamelist txt text time good men come aid country zoe rogers time good men come aid country chunker match false case sensitive false phrase zoe rogers start end type player score k row tail namelist txt shortnamelist txt text time good men come aid country zoe rogers time good men come aid country chunker match false case sensitive false phrase time good men start end type player score phrase come aid country start end type player score phrase zoe rogers time start end type player score phrase good men come aid start end type player score better option creating mapdictionary twiddled memory constraint vm doe seem help idea
Are there APIs for text analysis/mining in Java?,"<p>I want to know if there is an API to do text analysis in Java. Something that can extract all words in a text, separate words, expressions, etc.  Something that can inform if a word found is a number, date, year, name, currency, etc.</p>

<p>I'm starting the text analysis now, so I only need an API to kickoff. I made a web-crawler, now I need something to analyze the downloaded data. Need methods to count the number of words in a page, similar words, data type and another resources related to the text.</p>

<p>Are there APIs for text analysis in Java?</p>

<p>EDIT: Text-mining, I want to mining the text. An API for Java that provides this.</p>
",Named Entity Recognition (NER),apis text analysis mining java want know api text analysis java something extract word text separate word expression etc something inform word found number date year name currency etc starting text analysis need api kickoff made web crawler need something analyze downloaded data need method count number word page similar word data type another resource related text apis text analysis java edit text mining want mining text api java provides
Training Data set for Name entity recognition using StanfordNLP,"<p>I have used OpenNLP to create an model. But now i am looking into StanfordNLP which uses Condition Random Field. I want to know how to train a data for NER using stanfordNLP.
For OpenNLp we use START and END tag but i do no how to train using StanfordNLP. please give me an example.</p>
",Named Entity Recognition (NER),training data set name entity recognition using stanfordnlp used opennlp create model looking stanfordnlp us condition random field want know train data ner using stanfordnlp opennlp use start end tag train using stanfordnlp please give example
Maximum Entropy Markov Model for Named Entity Recognition in Java,"<p>I have a parsing problem that would be solved really well by a MEMM.  But I have spent far to much time trying to find a good implementation of the algorithm (ideally in java).  Has anyone done this before?  Alternatively I could implement it myself if some-one has some readable documentation.</p>

<p>Thanks!</p>

<p>(I have already tried Mallet and the trainer in the jar was unimplemented)</p>
",Named Entity Recognition (NER),maximum entropy markov model named entity recognition java parsing problem would solved really well memm spent far much time trying find good implementation algorithm ideally java ha anyone done alternatively could implement one ha readable documentation thanks already tried mallet trainer jar wa unimplemented
"GATE: In ANNIE, Extracting named entities after running Pronominal Coreference module","<p><strong>Summary:</strong>
From the GATE site there is this example. <a href=""http://gate.ac.uk/wiki/jape-repository/coordinated-annotations.html#section-3"" rel=""nofollow"">http://gate.ac.uk/wiki/jape-repository/coordinated-annotations.html#section-3</a>.</p>

<p>This rule references a result that only occurs after running the co-reference module. Is there a way to extract the entity without running the NE transducer again after the co-reference module?</p>

<p><strong>My specific problem:</strong>
Currently I have a rule that extracts a noun phrase which I mark with an ""Object"" annotation. This rule doesn't match phrases that end on pronouns so it doesn't match ""it"". However, I am trying to using the Pronominal Coreference module in ANNIE to link instances of ""it"" to other ""Object""s that it maybe coreferencing. </p>

<p>The problem is that since my ""Object"" annotation doesn't match instances of ""it"", my other rules which use the ""Object"" annotation won't match sentences where the noun phrase is the word ""it"" but it is really referencing another ""Object"". So in actuality, my rule should match this sentence. </p>

<p>This is solved if I run the NE transducer again after the Pronominal Coreference module, but of course this would be expensive and redundant. </p>

<p>Is there a better way to do this?  The obvious thing would be to alter my rules to match instances of ""it"" but if there is a better way, I'd like to try that.</p>
",Named Entity Recognition (NER),gate annie extracting named entity running pronominal coreference module summary gate site example rule reference result occurs running co reference module way extract entity without running ne transducer co reference module specific problem currently rule extract noun phrase mark object annotation rule match phrase end pronoun match however trying using pronominal coreference module annie link instance object maybe coreferencing problem since object annotation match instance rule use object annotation match sentence noun phrase word really referencing another object actuality rule match sentence solved run ne transducer pronominal coreference module course would expensive redundant better way obvious thing would alter rule match instance better way like try
Which NLP toolkit to use in JAVA?,"<p>i'm working on a project that consists of a website that connects to the NCBI(National Center for Biotechnology Information) and searches for articles there.  Thing is that I have to do some text mining on all the results. 
I'm using the JAVA language for textmining and AJAX with ICEFACES for the development of the website.
 What do I have :
A list of articles returned from a search.
Each article has an ID and an abstract.
The idea is to get keywords from each abstract text.
And then compare all the keywords from all abstracts and find the ones that are the most repeated. So then show in the website the related words for the search.
Any ideas ? 
I searched a lot in the web, and I know there is Named Entity Recognition,Part Of Speech tagging, there is teh GENIA thesaurus for NER on genes and proteins, I already tried stemming ... Stop words lists, etc...
I just need to know the best aproahc to resolve this problem.
Thanks a lot.</p>
",Named Entity Recognition (NER),nlp toolkit use java working project consists website connects ncbi national center biotechnology information search article thing text mining result using java language textmining ajax icefaces development website list article returned search article ha id abstract idea get keywords abstract text compare keywords abstract find one repeated show website related word search idea searched lot web know named entity recognition part speech tagging teh genia thesaurus ner gene protein already tried stemming stop word list etc need know best aproahc resolve problem thanks lot
Named Entity Recognition for Italian,"<p>I'd like to use an NLP tool to extract names and numbers from an <strong>Italian</strong> text.</p>

<p>Sadly, neither <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml#Download"" rel=""nofollow"">The Standford NLP</a> nor <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow"">Apache OpenNLP</a> provide a model for Italian.</p>

<p>Were can I find one, or find the training data to make one? (at least 15,000 sentences)</p>
",Named Entity Recognition (NER),named entity recognition italian like use nlp tool extract name number italian text sadly neither standford nlp apache opennlp provide model italian find one find training data make one least sentence
Reintroducing spaces into a document,"<p>Imagine we have some reference text on hand</p>

<blockquote>
  <p>Four score and seven years ago our fathers brought forth on this
  continent a new nation, conceived in liberty, and dedicated to the
  proposition that all men are created equal. Now we are engaged in a
  great civil war, testing whether that nation, or any nation, so
  conceived and so dedicated, can long endure. We are met on a great
  battle-field of that war. We have come to dedicate a portion of that
  field, as a final resting place for those who here gave their lives
  that that nation might live. It is altogether fitting and proper that
  we should do this. But, in a larger sense, we can not dedicate, we can
  not consecrate, we can not hallow this ground. The brave men, living
  and dead, who struggled here, have consecrated it, far above our poor
  power to add or detract. The world will little note, nor long remember
  what we say here, but it can never forget what they did here. It is
  for us the living, rather, to be dedicated here to the unfinished work
  which they who fought here have thus far so nobly advanced. It is
  rather for us to be here dedicated to the great task remaining before
  us—that from these honored dead we take increased devotion to that
  cause for which they gave the last full measure of devotion—that we
  here highly resolve that these dead shall not have died in vain—that
  this nation, under God, shall have a new birth of freedom—and that
  government of the people, by the people, for the people, shall not
  perish from the earth.</p>
</blockquote>

<p>and we receive snippets of that text back to us with no spaces or punctuation, and some characters deleted, inserted, and substituted</p>

<pre><code>ieldasafinalrTstingplaceforwhofoughtheregavetheirliZesthatthatn
</code></pre>

<p>Using the reference text what are some tools (in any programming language) we can use to try properly space the words</p>

<pre><code>ield as a final rTsting place for who fought here gave their liZes that that n
</code></pre>

<p>correcting errors is not necessary, just spacing</p>
",Named Entity Recognition (NER),reintroducing space document imagine reference text hand four score seven year ago father brought forth continent new nation conceived liberty dedicated proposition men created equal engaged great civil war testing whether nation nation conceived dedicated long endure met great battle field war come dedicate portion field final resting place gave life nation might live altogether fitting proper larger sense dedicate consecrate hallow ground brave men living dead struggled consecrated far poor power add detract world little note long remember say never forget u living rather dedicated unfinished work fought thus far nobly advanced rather u dedicated great task remaining u honored dead take increased devotion cause gave last full measure devotion highly resolve dead shall died vain nation god shall new birth freedom government people people people shall perish earth receive snippet text back u space punctuation character deleted inserted substituted using reference text tool programming language use try properly space word correcting error necessary spacing
How to avoid sequential processing in NLP?,"<p>The general approach in NLP is a chain of process looking like:</p>

<ol>
<li>Tokenization  </li>
<li>Morphological analysis  </li>
<li>POS-tagging  </li>
<li>Syntactic analysis, or Named Entity Recognition, or Noun-phrase chunking, etc.</li>
<li>Classification (or any ""end goal"" of the program)</li>
</ol>

<p>I've always found strange that each step makes decisions without ""consulting with"" posterior steps. For instance, you might POS-tag a word as a noun, even if it makes any syntactic analysis impossible further down the processing.</p>

<p>I was wondering if there were some approaches to this general NLP problem which take into account posterior steps. A kind of belief propagation, if you will.</p>
",Named Entity Recognition (NER),avoid sequential processing nlp general approach nlp chain process looking like tokenization morphological analysis po tagging syntactic analysis named entity recognition noun phrase chunking etc classification end goal program always found strange step make decision without consulting posterior step instance might po tag word noun even make syntactic analysis impossible processing wa wondering approach general nlp problem take account posterior step kind belief propagation
Feasibility of extracting arbitrary locations from a given string?,"<p>I have many spreadsheets with travel information on them amongst other things.</p>

<p>I need to extract start and end locations where the row describes travel, and one or two more things from the row, but what those extra fields are shouldn't be important.</p>

<p>There is no known list of all locations and no fixed pattern of text, all that I can look for is location names.</p>

<p>The field I'm searching in has 0-2 locations, sometimes locations have aliases.</p>

<h3>The Problem</h3>

<p>If we have this:</p>

<pre class=""lang-none prettyprint-override""><code>00229 | 445 | RTF | Jan   |  trn_rtn_co  | Chicago to Base1 
00228 | 445 | RTF | Jan   |  train       | Metroline to home coming from Base1
00228 | 445 | RTF | Jan   |  train_s     | Standard train journey to Friends
</code></pre>

<p>I, for instance (though it will vary), will want this:</p>

<pre class=""lang-none prettyprint-override""><code>RTF|Jan|Chicago   |Base1
RTF|Jan|Home      |Base1
RTF|Jan|NULL      |Friends
</code></pre>

<p>And then to go though, look up what <code>Base1</code> and <code>Friends</code> mean for that person (whose unique ID is RTF) and replace them with sensible locations (assuming they only have one set of 'friends'):</p>

<pre class=""lang-none prettyprint-override""><code>RTF|Jan|Chicago   |Rockford
RTF|Jan|Home      |Rockword
RTF|Jan|NULL      |Milwaukee
</code></pre>

<h3>What I need</h3>

<p>I need a way to pick out key words from the final column, such as: <em>Metroline to home coming from Base1</em>.</p>

<p>There are three types of words I'm looking for: </p>

<ol>
<li><strong>Home Locations</strong><br>These are known and limited, I can get these from a list</li>
<li><strong>Home Aliases</strong><br>These are known and limited, I can get these from a list</li>
<li><strong>Away Locations</strong><br>These are <em>unknown</em> but cities/towns/etc in the UK I don't know how to recognize these in the string. This is my main problem</li>
</ol>

<h3>My Ideas</h3>

<p>My go to program I thought of was <code>awk</code>, but I don't know if I can reliably search to find where a proper noun (i.e. location) is used for the location names. </p>

<p>Is there a package, library or dictionary of standard locations?</p>

<p>Can I get a program to scour the spreadsheets and 'learn' the names of locations?</p>

<p>This seems like a problem that would have been solved already (i.e. find words in a string of text), but I'm not certain what I'm doing, and I'm only a novice programmer.</p>

<p>Any help on what I can do would be appreciated.</p>

<p>Edit:</p>

<p>Any answer such as ""US_Locations_Cities is something you could check against"", ""Check for strings mentioned in a file in awk using ..."", ""There is a library for language X that will let a program learn to recognise location names, it's not RegEx, but it might work"", or ""There is a dictionary of location names <em>here</em>"" would be fine.</p>

<p>Ultimately anything that helps me do what I want to do (i.e <em>get the location names</em>!) would be excellent.</p>
",Named Entity Recognition (NER),extracting arbitrary location given string many spreadsheet travel information amongst thing need extract start end location row describes travel one two thing row extra field important known list location fixed pattern text look location name field searching ha location sometimes location alias problem instance though vary want go though look mean person whose unique id rtf replace sensible location assuming one set friend need need way pick key word final column metroline home coming base three type word looking home locationsthese known limited get list home aliasesthese known limited get list away locationsthese unknown city town etc uk know recognize string main problem idea go program thought wa know reliably search find proper noun e location used location name package library dictionary standard location get program scour spreadsheet learn name location seems like problem would solved already e find word string text certain novice programmer help would appreciated edit answer u location city something could check check string mentioned file awk using library language x let program learn recognise location name regex might work dictionary location name would fine ultimately anything help want e get location name would excellent
Extract terminology from sentences quickly,"<p>I am working in Text Mining and my work is focused on biomedical entities (genes, proteins, drugs and diseases). I would like to share with you some questions.</p>

<p>Now, my goal is to find biomedical entities in biomedical text (from Medline) and through of dictionaries of terms, I can identify each entity found with its unique identifier.</p>

<p>To store text, dicitionaries and results, I am using MongoDB (a nonSQL database). Each abstract is splitted in sentences, and each sentence is store in a new record (with list of tokens, chunks and Part-of-Speech tags). To find entities, I get all senteces and for each one I create a regular expresion for each term in the dictionary (in Python):</p>

<pre><code>for term in dicitonary:
     matches = re.finditer(r'(' + term + ')', sentence)
     for m in matches:
          ini = m.start()
          end = m.end()
          result.append(ini, end, dictionary.get_identification[term])
</code></pre>

<p>But it is really slow, I have several subsets of 150,000 abstracts (>1,000,000 of sentences).</p>

<p>For me, it is really interesting soft-matching to extract more entities where their terminology is not exactly in my dictionary, but it can increase my running time.</p>

<p>I think that my problem is to do a lot of regular expressions (I have dictionary with 300,000 entries) for each sentence, because I have to find the terms in sentence. Without Machine Learning algorithm, how could you resolve this problem? And with ML algorithms? Now, I am flexible to change my programming language, databases...</p>

<p>Thank you very much!!!</p>

<p>Regards,</p>

<p>àlex.</p>
",Named Entity Recognition (NER),extract terminology sentence quickly working text mining work focused biomedical entity gene protein drug disease would like share question goal find biomedical entity biomedical text medline dictionary term identify entity found unique identifier store text dicitionaries result using mongodb nonsql database abstract splitted sentence sentence store new record list token chunk part speech tag find entity get senteces one create regular expresion term dictionary python really slow several subset abstract sentence really interesting soft matching extract entity terminology exactly dictionary increase running time think problem lot regular expression dictionary entry sentence find term sentence without machine learning algorithm could resolve problem ml algorithm flexible change programming language database thank much regard lex
Financial news headers classification to positive/negative classes,"<p>I'm doing a small research project where I should try to split financial news articles headers to positive and negative classes.For classification I'm using SVM approach.The main problem which I see now it that not a lot of features can be produced for ML. News articles contains a lot of Named Entities and other ""garbage"" elements (from my point of view of course).</p>

<p>Could you please suggest ML features which can be used for ML training? Current results are: precision =0.6, recall=0.8</p>

<p>Thanks</p>
",Named Entity Recognition (NER),financial news header classification positive negative class small research project try split financial news article header positive negative class classification using svm approach main problem see lot feature produced ml news article contains lot named entity garbage element point view course could please suggest ml feature used ml training current result precision recall thanks
NLTK named entity recognition in dutch,"<p>I am trying to extract named entities from dutch text. I used <a href=""https://github.com/japerk/nltk-trainer/"" rel=""noreferrer"">nltk-trainer</a> to train a tagger and a chunker on the conll2002 dutch corpus. However, the parse method from the chunker is not detecting any named entities. Here is my code:</p>

<pre><code>str = 'Christiane heeft een lam.'

tagger = nltk.data.load('taggers/dutch.pickle')
chunker = nltk.data.load('chunkers/dutch.pickle')

str_tags = tagger.tag(nltk.word_tokenize(str))
print str_tags

str_chunks = chunker.parse(str_tags)
print str_chunks
</code></pre>

<p>And the output of this program:</p>

<pre><code>[('Christiane', u'N'), ('heeft', u'V'), ('een', u'Art'), ('lam', u'Adj'), ('.', u'Punc')]
(S Christiane/N heeft/V een/Art lam/Adj ./Punc)
</code></pre>

<p>I was expecting Christiane to be detected as a named entity.
Any help?</p>
",Named Entity Recognition (NER),nltk named entity recognition dutch trying extract named entity dutch text used nltk trainer train tagger chunker conll dutch corpus however parse method chunker detecting named entity code output program wa expecting christiane detected named entity help
Unconventional named-entity recognition,"<p>I'm trying to design a somewhat unconventional NER system that marks certain multiword strings as single units/tokens. </p>

<p>There are a lot of cool NER tools out there, but I have a few special needs that make it pretty much impossible to use something straight out of the box:</p>

<p>First, the entities can't just be extracted and printed out in a list--they need to be marked in some way and consolidated into tokens.</p>

<p>Second, categorization is not important--Person/Organization/Location doesn't matter (at least in the output).</p>

<p>Third, these aren't just your typical ENAMEX named entities we're looking for.  We want companies and organizations, but also concepts like 'climate change' and 'gay marriage.'  I've seen tags like these on some tools out there, but all of them were 'extraction-style'.</p>

<p>How would I got about getting this type of functionality?  Would training the Stanford tagger on my own, hand-annotated dataset do the job (where 'climate change'-esque phrases are labeled MISC or something)?  Or am I better off just making a shortlist of the 'weird' entities and checking the text against that after it's been run through a regular NER system?</p>

<p>Thanks so much!</p>
",Named Entity Recognition (NER),unconventional named entity recognition trying design somewhat unconventional ner system mark certain multiword string single unit token lot cool ner tool special need make pretty much impossible use something straight box first entity extracted printed list need marked way consolidated token second categorization important person organization location matter least output third typical enamex named entity looking want company organization also concept like climate change gay marriage seen tag like tool extraction style would got getting type functionality would training stanford tagger hand annotated dataset job climate change esque phrase labeled misc something better making shortlist weird entity checking text run regular ner system thanks much
NER naive algorithm,"<p>I never really dealt with NLP but had an idea about NER which should NOT have worked and somehow DOES exceptionally well in one case. I do not understand why it works, why doesn't it work or weather it can be extended. </p>

<p>The idea was to extract names of the main characters in a story through:</p>

<ol>
<li>Building a dictionary for each word</li>
<li>Filling for each word a list with the words that appear right next to it in the text</li>
<li>Finding for each word a word with the max correlation of lists (meaning that the words are used similarly in the text)</li>
<li>Given that one name of a character in the story, the words that are used like it, should be as well (Bogus, that is what should not work but since I never dealt with NLP until this morning I started the day naive)  </li>
</ol>

<p>I ran the overly simple code (attached below) on <a href=""http://www.umich.edu/~umfandsf/other/ebooks/alice30.txt"" rel=""nofollow"">Alice in Wonderland</a>, which for ""Alice"" returns:</p>

<blockquote>
  <p>21 ['Mouse', 'Latitude', 'William', 'Rabbit', 'Dodo', 'Gryphon', 'Crab', 'Queen', 'Duchess', 'Footman', 'Panther', 'Caterpillar', 'Hearts', 'King', 'Bill', 'Pigeon', 'Cat', 'Hatter', 'Hare', 'Turtle', 'Dormouse']</p>
</blockquote>

<p>Though it filters for upper case words (and receives ""Alice"" as the word to cluster around), originally there are ~500 upper case words, and it's still pretty spot on as far as <a href=""http://en.wikipedia.org/wiki/Alice%27s_Adventures_in_Wonderland#Characters"" rel=""nofollow"">main characters</a> goes.</p>

<p>It does not work that well with other characters and in other stories, though gives interesting results. </p>

<p>Any idea if this idea is usable, extendable or why does it work at all in this story for ""Alice"" ?</p>

<p>Thanks!</p>

<pre><code>#English Name recognition
import re
import sys
import random
from string import upper

def mimic_dict(filename):
  dict = {}
  f = open(filename)
  text = f.read()
  f.close()
  prev = """"
  words = text.split()
  for word in words:
    m = re.search(""\w+"",word)
    if m == None:
      continue
    word = m.group()
    if not prev in dict:
      dict[prev] = [word]
    else :
      dict[prev] = dict[prev] + [word] 
    prev = word
  return dict

def main():
  if len(sys.argv) != 2:
    print 'usage: ./main.py file-to-read'
    sys.exit(1)

  dict = mimic_dict(sys.argv[1])
  upper = []
  for e in dict.keys():
    if len(e) &gt; 1 and  e[0].isupper():
      upper.append(e)
  print len(upper),upper

  exclude = [""ME"",""Yes"",""English"",""Which"",""When"",""WOULD"",""ONE"",""THAT"",""That"",""Here"",""and"",""And"",""it"",""It"",""me""]
  exclude = [ x  for x in exclude if dict.has_key(x)] 
  for s in exclude :
    del dict[s]

  scores = {}
  for key1 in dict.keys():
    max = 0
    for key2 in dict.keys():
      if key1 == key2 : continue
      a =  dict[key1]
      k =  dict[key2]
      diff = []
      for ia in a:
        if ia in k and ia not in diff:
          diff.append( ia)
      if len(diff) &gt; max:
        max = len(diff)
        scores[key1]=(key2,max)
  dictscores = {}
  names = []
  for e in scores.keys():
    if scores[e][0]==""Alice"" and e[0].isupper():
      names.append(e)
  print len(names), names     


if __name__ == '__main__':
  main()
</code></pre>
",Named Entity Recognition (NER),ner naive algorithm never really dealt nlp idea ner worked somehow doe exceptionally well one case understand work work weather extended idea wa extract name main character story building dictionary word filling word list word appear right next text finding word word max correlation list meaning word used similarly text given one name character story word used like well bogus work since never dealt nlp morning started day naive ran overly simple code attached alice wonderland alice return mouse latitude william rabbit dodo gryphon crab queen duchess footman panther caterpillar heart king bill pigeon cat hatter hare turtle dormouse though filter upper case word receives alice word cluster around originally upper case word still pretty spot far main character go doe work well character story though give interesting result idea idea usable extendable doe work story alice thanks
How to tell if two web contents are similar?,"<p>Given 2 html sources, I want to first extract the main content out of it using something like <a href=""http://code.google.com/p/boilerpipe/"" rel=""nofollow"">this</a>. Are there any <a href=""http://tomazkovacic.com/blog/122/evaluating-text-extraction-algorithms/"" rel=""nofollow"">other better libraries</a> - I am specifically looking for Python/Javascript ones?</p>

<p>Once I have the two extracted contents, I want to return a score between 0 and 1 denoting how similar they are e.g. news articles on the same topic from CNN and BBC would have higher similarity scores since they are on the same topic or webpages pertaining to the same product on Amazon.com and Walmart.com would have a high score too. How can I do this? Are there existing libraries that do this already? What are some good libraries I can use? Basically I am looking for a combination of <a href=""http://en.wikipedia.org/wiki/Automatic_summarization"" rel=""nofollow"">automatic summarization</a>, <a href=""http://en.wikipedia.org/wiki/Terminology_extraction"" rel=""nofollow"">keyword extraction</a>, <a href=""http://en.wikipedia.org/wiki/Named_entity_recognition"" rel=""nofollow"">named-entity recognition</a> and <a href=""http://en.wikipedia.org/wiki/Sentiment_analysis"" rel=""nofollow"">sentiment-analysis</a>.</p>
",Named Entity Recognition (NER),tell two web content similar given html source want first extract main content using something like better library specifically looking python javascript one two extracted content want return score denoting similar e g news article topic cnn bbc would higher similarity score since topic webpage pertaining product amazon com walmart com would high score existing library already good library use basically looking combination automatic summarization keyword extraction named entity recognition sentiment analysis
Spell checker solution in java,"<p>I need to implement a spell checker in java , let me give you an example for a string lets say ""sch aproblm iseasili solved"" my output is ""such a problem is easily solved"".The maximum length of the string to correct is 64.As you can see my string can have spaces inserted in the wrong places or not at all and even misspelled words.I need a little help in finding a efficient algorithm of coming up with the corrected string. I am currently trying to delete all spaces in my string and inserting spaces in every possible position , so lets say for the word (it apply to a sentence as well) ""hot"" i generate the next possible strings to afterwords be corrected word by word using levenshtein distance : h o t ; h ot; ho t; hot. As you can see i have generated 2^(string.length() -1) possible strings. So for a string with a length of 64 it will generate 2^63 possible strings, which is damn high, and afterwords i need to process them one by one and select the best one by a different set of parameters such as : - total editing distance (must take the smallest one)
-if i have more strings with same editing distance i have to choose the one with the fewer number of words
-if i have more strings with the same number of words i need to choose the one with the total maximum frequency the words have( i have a dictionary of the most frequent 8000 words along with their frequency )
-and finally if there are more strings with the same total frequency i have to take the smallest lexicographic one.</p>

<p>So basically i generate all possible strings (inserting spaces in all possible positions into the original string) and then one by one i calculate their total editing distance, nr of words ,etc. and then choose the best one, and output the corrected string. I want to know if there is a easier(in terms of efficiency) way of doing this , like not having to generate all possible combinations of strings etc.</p>

<p>EDIT:So i thought that i should take another approach on this one.Here is what i have in mind: I take the first letter from my string , and extract from the dictionary all the words that begin with that letter.After that i process all of them and extract from my string all possible first words. I will remain at my previous example , for the word ""hot"" by generating all possible combinations i got 4 results , but with my new algorithm i obtain only 2 ""hot"" , and ""ho"" , so it's already an improvement.Though i need a little bit of help in creating a recursive or PD algorithm for doing this . I need a way to store all possible strings for the first word , then for all of those all possible strings for the second word and so on and finally to concatenate all possibilities and add them into an array or something. There will still be a lot of combinations for large strings but not as many as having to do ALL of them. Can someone help me with a pseudocode or something , as this is not my strong suit.</p>

<p>EDIT2: here is the code where i generate all the possible first word from my string <a href=""http://pastebin.com/d5AtZcth"" rel=""nofollow"">http://pastebin.com/d5AtZcth</a> .I need to somehow implement this to do the same for the rest and combine for each first word with each second word and so on , and store all these concatenated into an array or something.</p>
",Named Entity Recognition (NER),spell checker solution java need implement spell checker java let give example string let say sch aproblm iseasili solved output problem easily solved maximum length string correct see string space inserted wrong place even misspelled word need little help finding efficient algorithm coming corrected string currently trying delete space string inserting space every possible position let say word apply sentence well hot generate next possible string afterwords corrected word word using levenshtein distance h h ot ho hot see generated string length possible string string length generate possible string damn high afterwords need process one one select best one different set parameter total editing distance must take smallest one string editing distance choose one fewer number word string number word need choose one total maximum frequency word dictionary frequent word along frequency finally string total frequency take smallest lexicographic one basically generate possible string inserting space possible position original string one one calculate total editing distance nr word etc choose best one output corrected string want know easier term efficiency way like generate possible combination string etc edit thought take another approach one mind take first letter string extract dictionary word begin letter process extract string possible first word remain previous example word hot generating possible combination got result new algorithm obtain hot ho already improvement though need little bit help creating recursive pd algorithm need way store possible string first word possible string second word finally concatenate possibility add array something still lot combination large string many someone help pseudocode something strong suit edit code generate possible first word string need somehow implement rest combine first word second word store concatenated array something
How can I differentiate between a person&#39;s name and other names that are derived from verbs,"<p>How can I extract person names from the text?</p>

<p>I have applied some NLP toolkit for this, specifically I used the Stanford NER toolkit to extract names from text. With that, I can extract person names from the text, but when I want the program to extract words like 'programmer', 'lecturer' or 'engineer', the libraries couldn't extract those. Is there any way to extract these from the text?</p>
",Named Entity Recognition (NER),differentiate person name name derived verb extract person name text applied nlp toolkit specifically used stanford ner toolkit extract name text extract person name text want program extract word like programmer lecturer engineer library extract way extract text
Named Entity Recognition in political domain,"<p>For my research project in text classification, I need to identify named entities in the political domain (using NER to improve the text classification). </p>

<p>Where can I find the named entities in the political domain, so that I can train the classifier with?</p>

<p>If you know of any other dataset than the political domain let me know.</p>

<p>Thanks!</p>
",Named Entity Recognition (NER),named entity recognition political domain research project text classification need identify named entity political domain using ner improve text classification find named entity political domain train classifier know dataset political domain let know thanks
Difference between named entity recognition and resolution?,"<p>What is the difference between named entity recognition and named entity resolution? Would appreciate a practical example.</p>
",Named Entity Recognition (NER),difference named entity recognition resolution difference named entity recognition named entity resolution would appreciate practical example
"Need to extract information from free text, information like location, course etc","<p>I need to write a text parser for the education domain which can extract out the information like institute, location, course etc from the free text.</p>

<p>Currently i am doing it through lucene, steps are as follows:</p>

<ol>
<li>Index all the data related to institute, courses and location.</li>
<li>Making shingles of the free text and searching each shingle in location, course and institute index dir and then trying to find out which part of text represents location, course etc. </li>
</ol>

<p>In this approach I am missing lot of cases like B.tech can be written as btech, b-tech or b.tech.</p>

<p>I want to know is there any thing available which can do all these kind of things, I have heard about Ling-pipe and Gate but don't know how efficient they are.</p>
",Named Entity Recognition (NER),need extract information free text information like location course etc need write text parser education domain extract information like institute location course etc free text currently lucene step follows index data related institute course location making shingle free text searching shingle location course institute index dir trying find part text represents location course etc approach missing lot case like b tech written btech b tech b tech want know thing available kind thing heard ling pipe gate know efficient
Extract business titles and time periods from string,"<p>I am extracting information about certain companies from Reuters using Python. I have been able to get the officer/executive names, biographies, and compensation from  <a href=""http://www.reuters.com/finance/stocks/companyOfficers?symbol=WWW.N"" rel=""nofollow"">this page</a></p>

<p>Now, I want to extract previous position titles and companies from the biography section, which looks something like this:</p>

<blockquote>
  <p>Mr. Donald T. Grimes is Senior Vice President, Chief Financial Officer and Treasurer of Wolverine World Wide, Inc., since May 2008. From 2007 to 2008, he was the Executive Vice President and Chief Financial Officer for Keystone Automotive Operations, Inc., a distributor of automotive accessories and equipment. Prior to Keystone, Mr. Grimes held a series of senior corporate and divisional finance roles at Brown-Forman Corporation, a manufacturer and marketer of premium wines and spirits. During his employment at Brown-Forman, Mr. Grimes was Vice President, Director of Beverage Finance from 2006 to 2007; Vice President, Director of Corporate Planning and Analysis from 2003 to 2006; and Senior Vice President, Chief Financial Officer of Brown-Forman Spirits America from 1999 to 2003.</p>
</blockquote>

<p>I can use simple regex to get the from and to years, but I am at a loss on how to write regex to get the titles and the company name as well. I know the string format is inconsistent, so I would take an answer that works for at least 70% of cases. Here's the output I would like:</p>

<pre><code>2007-2008, executive vice president and chief financial officer, Keystone Automotive operations
</code></pre>
",Named Entity Recognition (NER),extract business title time period string extracting information certain company reuters using python able get officer executive name biography compensation page want extract previous position title company biography section look something like mr donald grime senior vice president chief financial officer treasurer wolverine world wide inc since may wa executive vice president chief financial officer keystone automotive operation inc distributor automotive accessory equipment prior keystone mr grime held series senior corporate divisional finance role brown forman corporation manufacturer marketer premium wine spirit employment brown forman mr grime wa vice president director beverage finance vice president director corporate planning analysis senior vice president chief financial officer brown forman spirit america use simple regex get year loss write regex get title company name well know string format inconsistent would take answer work least case output would like
"Can extract generic entities using Lingpipe other than People, Org and Loc?","<p>I have read through Lingpipe for NLP and found that we have a capability there to identify mentions of names of people, locations and organizations. My questions is that if I have a training set of documents that have mentions of let's say software projects inside the text, can I use this training set to train a named entity recognizer? Once the training is complete, I should be able to feed a test set of textual documents to the trained model and I should be able to identify mentions of software projects there.</p>

<p>Is this generic NER possible using NER? If so, what features should I be using that I should feed?</p>

<p>Thanks
Abhishek S</p>
",Named Entity Recognition (NER),extract generic entity using lingpipe people org loc read lingpipe nlp found capability identify mention name people location organization question training set document mention let say software project inside text use training set train named entity recognizer training complete able feed test set textual document trained model able identify mention software project generic ner possible using ner feature using feed thanks abhishek
Autocorrect spelling mistakes in text input,"<p>I am writing a natural language processor in C# that extracts the sentiment (positive/negative) of a sentence.  There is something of an issue, though, in being able to discern the sentiment of a misspelled word - if it's not in the dictionary, I can neither tag it nor rate it!</p>

<p>I know there has to be a way to handle this.  Google gives accurate suggestions all the time, I simply need to take the top suggestion from a similar algorithm and hit the database with it.  The problem is, I'm not sure where to start with algorithm names and so forth.  I need help figuring that out.</p>

<p>I checked around on the site for similar questions, and found some concepts that seemed useful, but the basic way of handling the distance between a misspelling and a real word basically relied on hitting every word in your data set, which seems horribly inefficient.  Some help with ideas to make the algorithm run quickly would also be much appreciated; this analysis engine is supposed to be able to handle multiple thousands of items a day.</p>

<p>Thanks in advance.</p>
",Named Entity Recognition (NER),autocorrect spelling mistake text input writing natural language processor c extract sentiment positive negative sentence something issue though able discern sentiment misspelled word dictionary neither tag rate know ha way handle google give accurate suggestion time simply need take top suggestion similar algorithm hit database problem sure start algorithm name forth need help figuring checked around site similar question found concept seemed useful basic way handling distance misspelling real word basically relied hitting every word data set seems horribly inefficient help idea make algorithm run quickly would also much appreciated analysis engine supposed able handle multiple thousand item day thanks advance
c# tools for named entities recognizer,"<p>I am looking for a simple but ""good enough"" Named Entity Recognition tool (nlp tool) or library for C#, I am looking to process name entities in  medical domains.</p>

<p>Any recommendations?</p>

<p>Thanks.</p>
",Named Entity Recognition (NER),c tool named entity recognizer looking simple good enough named entity recognition tool nlp tool library c looking process name entity medical domain recommendation thanks
Named Entity Recognition from personal Gazetter using Python,"<p>I try to do named entity recognition in python using NLTK.
I want to extract personal list of skills.
I have the list of skills and would like to search them in requisition and tag the skills.
I noticed that NLTK has NER tag for predefine tags like Person, Location etc.
Is there a external gazetter tagger in Python I can use?
any idea how to do it more sophisticated than search of terms ( sometimes multi words term )?</p>

<p>Thanks,
Assaf</p>
",Named Entity Recognition (NER),named entity recognition personal gazetter using python try named entity recognition python using nltk want extract personal list skill list skill would like search requisition tag skill noticed nltk ha ner tag predefine tag like person location etc external gazetter tagger python use idea sophisticated search term sometimes multi word term thanks assaf
A corpus with semantic role tags for an NLP application,"<p>So, I've constructed a NLP program that learns to extract a semantic event description from a sentence, but right now my training set is limited to sentences I've parsed into semantic event components my hand.</p>

<p>While this method does get the job done, its hardly a proper substitute for a large pre-parsed corpus of text. Unfortunately, all of my attempts at finding such a corpus have proven futile.</p>

<p>What I need specifically is a corpus that has tagged the semantic roles of each word (or group of words) in a sentence. Examples of roles I had in mind are things like:</p>

<ul>
<li>agent</li>
<li>action</li>
<li>patient</li>
<li>instrument</li>
<li>co-agent</li>
<li>co-patient</li>
<li>location</li>
<li>adverb</li>
</ul>

<p>If any more specifics are needed, feel free to ask, or refer to <a href=""http://psych.stanford.edu/~jlm/papers/StJohnMcC90.pdf"" rel=""nofollow"">this paper</a> that uses a toy corpa with the same constraints as mine.</p>
",Named Entity Recognition (NER),corpus semantic role tag nlp application constructed nlp program learns extract semantic event description sentence right training set limited sentence parsed semantic event component hand method doe get job done hardly proper substitute large pre parsed corpus text unfortunately attempt finding corpus proven futile need specifically corpus ha tagged semantic role word group word sentence example role mind thing like agent action patient instrument co agent co patient location adverb specific needed feel free ask refer paper us toy corpa constraint mine
social media search engine question,"<p>I came across this site called <a href=""http://socialmention.com/"" rel=""nofollow noreferrer"">social mention</a> and am curious about how applications like this work, hopefully somebody can offer some glimpses/suggestions on this.</p>
<ol>
<li><p>Upon looking at the search results, I realize that they grab results from facebook, twitter, google.... I suppose this is done on the fly, probably through some REST api exposed by the mentioned?</p>
</li>
<li><p>If what I mention in point 1 is probably true, does that means sentiment analysis on the documents/links return is done on the fly too? Wouldn't that be too computationally intensive? I am curious because other than sentiments, they also return the top keywords in the document set.</p>
</li>
<li><p>They have something called the &quot;trends&quot;. They looked like the trendingtopics in twitter, but seems like they also include phrases &gt;3 words long. Is this relevant to nlp's entity extraction or more to keyphrase extraction? Is there apis other than that of Twitter that provides this? Is &quot;trends&quot; generally done on search queries submitted by users or do the system actually processes the pages?</p>
</li>
</ol>
<p>A curious man.</p>
",Named Entity Recognition (NER),social medium search engine question came across site called social mention curious application like work hopefully somebody offer glimpse suggestion upon looking search result realize grab result facebook twitter google suppose done fly probably rest api exposed mentioned mention point probably true doe mean sentiment analysis document link return done fly computationally intensive curious sentiment also return top keywords document set something called trend looked like trendingtopics twitter seems like also include phrase word long relevant nlp entity extraction keyphrase extraction apis twitter provides trend generally done search query submitted user system actually process page curious man
Finding words from a dictionary in a string of text,"<p>How would you go about parsing a string of free form text to detect things like locations and names based on a dictionary of location and names? In my particular application there will be tens of thousands if not more entries in my dictionaries so I'm pretty sure just running through them all is out of the question. Also, is there any way to add ""fuzzy"" matching so that you can also detect substrings that are within <code>x</code> edits of a dictionary word? If I'm not mistaken this falls within the field of natural language processing and more specifically named entity recognition (NER); however, my attempt to find information about the algorithms and processes behind NER have come up empty. I'd prefer to use Python for this as I'm most familiar with that although I'm open to looking at other solutions.</p>
",Named Entity Recognition (NER),finding word dictionary string text would go parsing string free form text detect thing like location name based dictionary location name particular application ten thousand entry dictionary pretty sure running question also way add fuzzy matching also detect substring within edits dictionary word mistaken fall within field natural language processing specifically named entity recognition ner however attempt find information algorithm process behind ner come empty prefer use python familiar although open looking solution
Part-Of-Speech tagging and Named Entity Recognition for C/C++/Obj-C,"<p>need some help!</p>

<p>I'm trying to write some code in objective-c that requires part-of-speech tagging, and ideally also named entity recognition.  I don't have much interest in ""rolling my own"", so I'm looking for a decent library to use for this purpose.  Obviously the more accurate the better, but we're not talking anything critical here -- so as long as it's generally pretty accurate that's good enough.</p>

<p>It's going to be English-only, at least for the time being, but I don't want to have to do any training of models myself.  So whatever the solution, it has to have an English language model already built.</p>

<p>And finally, it has to be available via a commercial-friendly license (e.g. BSD/Berkeley, LGPL).  Can't do GPL or anything restrictive like that, though I'm open to paying a small amount for a commercial license if that's the only option.</p>

<p>C, C++ or Obj-C code is all fine.</p>

<p>So: Anyone familiar with something that'd do the trick here?  Thanks!!</p>
",Named Entity Recognition (NER),part speech tagging named entity recognition c c obj c need help trying write code objective c requires part speech tagging ideally also named entity recognition much interest rolling looking decent library use purpose obviously accurate better talking anything critical long generally pretty accurate good enough going english least time want training model whatever solution ha english language model already built finally ha available via commercial friendly license e g bsd berkeley lgpl gpl anything restrictive like though open paying small amount commercial license option c c obj c code fine anyone familiar something trick thanks
Does OpenNLP use WordNet under the hood for the Named Entity Recognition,"<p>I have tried using OpenNLP Tools 1.5 from SourceForge for getting the Named Entites from a text. I did find a JWNL file in the OpenNLP download. Does that imply that OpenNLP in turn is using WordNet for the Named Entity Recognition. (Specifically, does it mean that the Name Model files were generated with WordNet) ?</p>
",Named Entity Recognition (NER),doe opennlp use wordnet hood named entity recognition tried using opennlp tool sourceforge getting named entites text find jwnl file opennlp download doe imply opennlp turn using wordnet named entity recognition specifically doe mean name model file generated wordnet
opennlp vs stanford nlptools vs berkeley,"<p>Hi the aim is to parse a sizeable corpus like wikipedia to generate the most probable parse tree,and named entity recognition. Which is the best library to achieve this in terms of performance and accuracy?  Has anyone used more than one of the above libraries?</p>
",Named Entity Recognition (NER),opennlp v stanford nlptools v berkeley hi aim parse sizeable corpus like wikipedia generate probable parse tree named entity recognition best library achieve term performance accuracy ha anyone used one library
Parser to parse search terms and extract valuable information,"<p>I would like to understand the serarh term of a user. Think of someone is searching for ""staples in NY"" - I would like to understand that its a location search where keyword is staples and location is new york. Similarly if someone types ""cat in hat"", the parser should not flag that also as a location search, here the entire keyword is ""cat in hat"".
Is there any algorithm or open source library available to parse a search term and understand its a comparison (like A vs B) or its a location based search (like A in X)?</p>
",Named Entity Recognition (NER),parser parse search term extract valuable information would like understand serarh term user think someone searching staple ny would like understand location search keyword staple location new york similarly someone type cat hat parser flag also location search entire keyword cat hat algorithm open source library available parse search term understand comparison like v b location based search like x
"Programmatically detect the name, location and start time of a social event on any html page","<p>I would love to see an app/browser extension that automatically detects meatspace events on a page by looking for words like location, date, time, etc... and then offers to add these to a calendar</p>

<p>If there is a large enough sample of these occurrences is there anyway to train an app to detect it? Are there any machine learning technique that could help with this? </p>
",Named Entity Recognition (NER),programmatically detect name location start time social event html page would love see app browser extension automatically detects meatspace event page looking word like location date time etc offer add calendar large enough sample occurrence anyway train app detect machine learning technique could help
Named entity recognition with Java,"<p>I would like to use named entity recognition (NER) to find adequate tags for texts in a database. Instead of using tools like NLTK or Lingpipe I want to build my own tool.</p>

<p>So my questions are:</p>

<ul>
<li><p>Which algorithm should I use?</p></li>
<li><p>How hard is to build this tool?</p></li>
</ul>
",Named Entity Recognition (NER),named entity recognition java would like use named entity recognition ner find adequate tag text database instead using tool like nltk lingpipe want build tool question algorithm use hard build tool
Indexing and Searching Over Word Level Annotation Layers in Lucene,"<p>I have a data set with multiple layers of annotation over the underlying text, such as <a href=""http://en.wikipedia.org/wiki/Part-of-speech_tagging"" rel=""noreferrer"">part-of-tags</a>, <a href=""http://www.cnts.ua.ac.be/conll2000/chunking/"" rel=""noreferrer"">chunks from a shallow parser</a>, <a href=""http://en.wikipedia.org/wiki/Named_entity_recognition"" rel=""noreferrer"">name entities</a>, and others from various  <a href=""http://en.wikipedia.org/wiki/Natural_language_processing"" rel=""noreferrer"">natural language processing</a> (NLP) tools. For a sentence like <code>The man went to the store</code>, the annotations might look like:</p>

<pre>

Word  POS  Chunk       NER
====  ===  =====  ========
The    DT     NP    Person     
man    NN     NP    Person
went  VBD     VP         -
to     TO     PP         - 
the    DT     NP  Location
store  NN     NP  Location
</pre>

<p>I'd like to index a bunch of documents with annotations like these using Lucene and then perform searches across the different layers. An example of a simple query would be to retrieve all documents where <strong>Washington</strong> is tagged as a <strong>person</strong>. While I'm not absolutely committed to the notation, syntactically end-users might enter the query as follows:</p>

<p><strong>Query</strong>: <code>Word=Washington,NER=Person</code> </p>

<p>I'd also like to do more complex queries involving the <strong>sequential order of annotations</strong> across different layers, e.g. find all the documents where there's a word tagged <strong>person</strong> followed by the words <strong><code>arrived at</code></strong> followed by a word tagged <strong>location</strong>. Such a query might look like:</p>

<p><strong>Query</strong>: <code>""NER=Person Word=arrived Word=at NER=Location""</code></p>

<p>What's a good way to go about approaching this with Lucene? Is there anyway to index and search over document fields that contain structured tokens?</p>

<p><strong>Payloads</strong></p>

<p>One suggestion was to try to use Lucene <a href=""http://lucene.apache.org/java/2_9_2/api/all/org/apache/lucene/search/payloads/package-summary.html"" rel=""noreferrer"">payloads</a>. But, I thought payloads could only be used to adjust the rankings of documents, and that they aren't used to select what documents are returned. </p>

<p>The latter is important since, for some use-cases, the <strong>number of documents</strong> that contain a pattern is really what I want.</p>

<p>Also, only the payloads on terms that match the query are examined. This means that <strong>payloads could only even help with the rankings of the first example query</strong>, <code>Word=Washington,NER=Person</code>, whereby we just want to make sure the term <strong><code>Washingonton</code></strong> is tagged as a <strong><code>Person</code></strong>. However, for the second example query,  <code>""NER=Person Word=arrived Word=at NER=Location""</code>, I need to check the tags on unspecified, and thus non-matching, terms.   </p>
",Named Entity Recognition (NER),indexing searching word level annotation layer lucene data set multiple layer annotation underlying text part tag chunk shallow parser name entity others various natural language processing nlp tool sentence like annotation might look like word po chunk ner dt np person man nn np person went vbd vp pp dt np location store nn np location like index bunch document annotation like using lucene perform search across different layer example simple query would retrieve document washington tagged person absolutely committed notation syntactically end user might enter query follows query also like complex query involving sequential order annotation across different layer e g find document word tagged person followed word followed word tagged location query might look like query good way go approaching lucene anyway index search document field contain structured token payload one suggestion wa try use lucene payload thought payload could used adjust ranking document used select document returned latter important since use case number document contain pattern really want also payload term match query examined mean payload could even help ranking first example query whereby want make sure term tagged however second example query need check tag unspecified thus non matching term
Text Parsing Design,"<p>Let's say I have a paragraph of text like so:</p>

<blockquote>
  <p><em>Snails <strong>can</em> be found</strong> in a very wide
  range of environments including
  ditches, deserts, and the abyssal
  depths of the sea. Numerous kinds of <em>snail <strong>can</em>
  also be</strong> found in fresh waters. <a href=""http://en.wikipedia.org/wiki/Sea_snail"" rel=""nofollow"" title=""source"">(source)</a></p>
</blockquote>

<p>I have 10,000 regex rules to match text, which can overlap. For example, the regex <code>/Snails? can/i</code> will find two matches (italicized in the text). The regex <code>/can( also)? be/i</code> has two matches (bolded).</p>

<p>After iterating through my regexes and finding matches, what is the best data structure to use, that given some place in the text, it returns all regexes that mached it? For example, if I want the matches for line 1, character 8 (0-based, which is the <code>a</code> in <code>can</code>), I would get a match for both regexes previously described.</p>

<p>I can create a hashmap: (key: character location, value: set of all matching regexes). Is this optimal? Is there a better way to parse the text with thousands of regexes (to not loop through each one)?</p>

<p>Thanks!</p>
",Named Entity Recognition (NER),text parsing design let say paragraph text like snail found wide range environment including ditch desert abyssal depth sea numerous kind snail also found fresh water source regex rule match text overlap example regex find two match italicized text regex ha two match bolded iterating regexes finding match best data structure use given place text return regexes mached example want match line character based would get match regexes previously described create hashmap key character location value set matching regexes optimal better way parse text thousand regexes loop one thanks
What are some software and techniques for extracting proper names from a text?,"<p>I have a large corpus of text-based documents (100,000+) from which I want to extract proper names (e.g. a person's name).</p>

<p>Could anyone recommend techniques and/or software that would be useful in accomplishing this goal. I'm not particularly interested in low-level text parsing, so much as I am in more high-level things such as recognizing and/or ranking.</p>
",Named Entity Recognition (NER),software technique extracting proper name text large corpus text based document want extract proper name e g person name could anyone recommend technique software would useful accomplishing goal particularly interested low level text parsing much high level thing recognizing ranking
What commercially-available platforms similar to OpenCalais or AlchemyAPI are there for entity extraction for Chinese and Japanese languages?,"<p>In particular, I would like to be able to extract people, places, films, music, etc. entities and have the entities available in widely used linked data IDs such as DBpedia, Freebase, or OpenCyc.</p>
",Named Entity Recognition (NER),commercially available platform similar opencalais alchemyapi entity extraction chinese japanese language particular would like able extract people place film music etc entity entity available widely used linked data id dbpedia freebase opencyc
Is NER necessary for Coreference resolution?,"<p>... or is gender information enough?
More specifically, I'm interested in knowing if I can reduce the number of models loaded by the Stanford Core NLP to extract coreferences. I am not interested in actual named entity recognition.</p>

<p>Thank you</p>
",Named Entity Recognition (NER),ner necessary coreference resolution gender information enough specifically interested knowing reduce number model loaded stanford core nlp extract coreference interested actual named entity recognition thank
Medical information extraction using Python,"<p>I am a nurse and I know python but I am not an expert, just used it to process DNA sequences<br>
We got hospital records written in human languages and I am supposed to insert these data into a database or csv file but they are more than 5000 lines and this can be so hard. All the data are written in a consistent format let me show you an example</p>

<pre><code>11/11/2010 - 09:00am : He got nausea, vomiting and died 4 hours later
</code></pre>

<p>I should get the following data</p>

<pre><code>Sex: Male
Symptoms: Nausea
    Vomiting
Death: True
Death Time: 11/11/2010 - 01:00pm
</code></pre>

<p>Another example</p>

<pre><code>11/11/2010 - 09:00am : She got heart burn, vomiting of blood and died 1 hours later in the operation room
</code></pre>

<p>And I get</p>

<pre><code>Sex: Female
Symptoms: Heart burn
    Vomiting of blood
Death: True
Death Time: 11/11/2010 - 10:00am
</code></pre>

<p>the order is not consistent by when I say in ....... so in is a keyword and all the text after is a place until i find another keyword<br>
At the beginnning He or She determine sex, got ........ whatever follows is a group of symptoms that i should split according to the separator which can be a comma, hypen or whatever but it's consistent for the same line<br>
died ..... hours later also should get how many hours, sometimes the patient is stil alive and discharged ....etc<br>
That's to say we have a lot of conventions and I think if i can tokenize the text with keywords and patterns i can get the job done. So please if you know a useful function/modules/tutorial/tool for doing that preferably in python (if not python so a gui tool would be nice)  </p>

<p>Some few information:</p>

<pre><code>there are a lot of rules to express various medical data but here are few examples
- Start with the same date/time format followed by a space followd by a colon followed by a space followed by He/She followed space followed by rules separated by and
- Rules:
    * got &lt;symptoms&gt;,&lt;symptoms&gt;,....
    * investigations were done &lt;investigation&gt;,&lt;investigation&gt;,&lt;investigation&gt;,......
    * received &lt;drug or procedure&gt;,&lt;drug or procedure&gt;,.....
    * discharged &lt;digit&gt; (hour|hours) later
    * kept under observation
    * died &lt;digit&gt; (hour|hours) later
    * died &lt;digit&gt; (hour|hours) later in &lt;place&gt;
other rules do exist but they follow the same idea
</code></pre>
",Named Entity Recognition (NER),medical information extraction using python nurse know python expert used process dna sequence got hospital record written human language supposed insert data database csv file line hard data written consistent format let show example get following data another example get order consistent say keyword text place find another keyword beginnning determine sex got whatever follows group symptom split according separator comma hypen whatever consistent line died hour later also get many hour sometimes patient stil alive discharged etc say lot convention think tokenize text keywords pattern get job done please know useful function module tutorial tool preferably python python gui tool would nice information
"Dictionary-Based Named Entity Recognition with zero edit distance: LingPipe, Lucene or what?","<p>I'm trying to perform a dictionary-based NER on some documents. My dictionary, regardless of the datatype, consists of key-value pairs of strings. I want to search for all the keys in the document, and return the corresponding value for that key whenever a match occurs.</p>

<p>The problem is, my dictionary is fairly large: ~7 million key-values - average length of keys: 8 and average length of values: 20 characters.</p>

<p>I've tried LingPipe with MapDictionary but on my desired environment setup, it runs out of memory after 200,000 rows are inserted. I don't know clearly why LingPipe uses a map and not a hashmap in their algorithm.</p>

<p>So the thing is, I don't have any previous experience with Lucene and I want to know if it makes such thing with such number possible in an easier way.</p>

<p>ps. I've already tried chunking the data into several dictionaries and writing them on disk but it's relatively slow.</p>

<p>Thanks for any help.</p>

<p>Cheers 
Parsa</p>
",Named Entity Recognition (NER),dictionary based named entity recognition zero edit distance lingpipe lucene trying perform dictionary based ner document dictionary regardless datatype consists key value pair string want search key document return corresponding value key whenever match occurs problem dictionary fairly large million key value average length key average length value character tried lingpipe mapdictionary desired environment setup run memory row inserted know clearly lingpipe us map hashmap algorithm thing previous experience lucene want know make thing number possible easier way p already tried chunking data several dictionary writing disk relatively slow thanks help cheer parsa
Improving entity naming with custom file/code in NLTK,"<p>We've been working with the NLTK library in a recent project where we're 
mainly interested in the named entities part. </p>

<p>In general we're getting good results using the NEChunkParser class. 
However, we're trying to find a way to provide our own terms to the 
parser, without success. </p>

<p>For example, we have a test document where my name (Shay) appears in 
several places. The library finds me as GPE while I'd like it to find 
me as PERSON... </p>

<p>Is there a way to provide some kind of a custom file/ 
code so the parser will be able to interpret the named entity as I 
want it to? </p>

<p>Thanks!</p>
",Named Entity Recognition (NER),improving entity naming custom file code nltk working nltk library recent project mainly interested named entity part general getting good result using nechunkparser class however trying find way provide term parser without success example test document name shay appears several place library find gpe like find person way provide kind custom file code parser able interpret named entity want thanks
are there any c# libraries for Named Entity Recognition?,"<p>I am looking for any free libraries for Named Entity Recognition in c# or any other .net language.</p>
",Named Entity Recognition (NER),c library named entity recognition looking free library named entity recognition c net language
How does twitter&#39;s trending topics algorithm decide which words to extract from tweets?,"<p>I saw <a href=""https://stackoverflow.com/questions/787496/what-is-the-best-way-to-compute-trending-topics-or-tags"">this question</a>, which focuses on the ""Brittney Spears"" problem.  But I have a bit of a different question.  How does the algorithm determine which words or phrases need to be ranked?  For instance, if I send out a tweet that says ""Michael Jackson died"", how does it know to pull out ""Michael Jackson"" but not ""died""?</p>

<p>Or suppose that Alec Baldwin and Steven Baldwin were in the news that day and thus were both mentioned in a lot of tweets.  How would it know to treat both names differently instead of just pulling out ""Baldwin""?</p>

<p>Done naively, I could see this problem as being NP-complete (you'd have to compare all potential phrases in the tweet with all potential phrases in everyone else's tweets).</p>
",Named Entity Recognition (NER),doe twitter trending topic algorithm decide word extract tweet saw suppose alec baldwin baldwin news day thus mentioned lot tweet would know treat name differently instead pulling baldwin done naively could see problem np complete compare potential phrase tweet potential phrase everyone else tweet
