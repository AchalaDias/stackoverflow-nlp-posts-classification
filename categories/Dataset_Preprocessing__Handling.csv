Title,Description,category,combined_text
dl4j does not contain text and models modules,"<p>In my project, I need to use the Word2Vec model. I decide to use dl4j for this. I load all dependencies, but there are no modules such as deeplearning4j.text and deeplearning4j.models </p>

<p>I tried to change the version of dl4j from beta5 to 4 and 3 but the problem still here
I even tried download jar files of dl4j but it didn't help too</p>
",Dataset Preprocessing & Handling,dl j doe contain text model module project need use word vec model decide use dl j load dependency module deeplearning j text deeplearning j model tried change version dl j beta problem still even tried download jar file dl j help
Bertopic assign topics to data frame,"<p>I have build a topic model with Bertopic.
After getting topic how could I assign them to dataset.</p>
<p>My main aim is to convert unsupervised topic modelling to supervised multi label classification problem.</p>
",Dataset Preprocessing & Handling,bertopic assign topic data frame build topic model bertopic getting topic could assign dataset main aim convert unsupervised topic modelling supervised multi label classification problem
How to read gguf format file and print content in python such as in NLP meta&#39;&#39;codellama-7b-instruct.Q4_K_S.gguf&#39;&#39;,"<p>How to read gguf format file in python and print content in python such as in NLP  meta''codellama-7b-instruct.Q4_K_S.gguf''</p>
<p>I tried using Python's open() function to read the gguf format file, expecting to retrieve its content and print it to the console or store it in a variable. However, I encountered errors due to the unsupported file format, preventing me from accessing the content as intended.</p>
",Dataset Preprocessing & Handling,read gguf format file print content python nlp meta codellama b instruct q k gguf read gguf format file python print content python nlp meta codellama b instruct q k gguf tried using python open function read gguf format file expecting retrieve content print console store variable however encountered error due unsupported file format preventing accessing content intended
Cannot Keep My Datetime Data and &#39;No&#39; Word in My Pandas DataFrame,"<p>I have a pandas dataframe from csv and I want to clean it using Regex in Python. The data that I have look like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Name</th>
<th>Date</th>
<th>Status</th>
<th>Number</th>
</tr>
</thead>
<tbody>
<tr>
<td>A/bCDef</td>
<td>2022-07-11</td>
<td>Yes</td>
<td>io123-07</td>
</tr>
<tr>
<td>GhIjK-l</td>
<td>2022-07-12</td>
<td>No</td>
<td>io456-08</td>
</tr>
</tbody>
</table>
</div>
<p>I'm trying to clean the dataframe so it will be easier to process, but the thing is, my code deletes the date, the word 'no', and the hyphen.</p>
<p>This the data that I got so far:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>name</th>
<th>date</th>
<th>status</th>
<th>number</th>
</tr>
</thead>
<tbody>
<tr>
<td>abcdef</td>
<td></td>
<td>yes</td>
<td>io</td>
</tr>
<tr>
<td>ghijkl</td>
<td></td>
<td>no</td>
<td>io</td>
</tr>
</tbody>
</table>
</div>
<p>This is the code that I found on the internet and tried on my dataframe:</p>
<pre><code>def regex_values(cols):
    nltk.download(&quot;stopwords&quot;)
    stemmer = nltk.SnowballStemmer('english')
    stopword = set(stopwords.words('english'))

    cols = str(cols).lower()
    cols = re.sub('\[.*?\]', '', cols)
    cols = re.sub('https?://\S+|www\.\S+', '', cols)
    cols = re.sub('&lt;.*?&gt;+/', '', cols)
    cols = re.sub('[%s]' % re.escape(string.punctuation), '', cols)
    cols = re.sub('\n', '', cols)
    cols = re.sub('\w*\d\w*', '', cols)
    cols = re.sub(r'^\s+|\s+$', '', cols)
    cols = re.sub(' +', ' ', cols)
    cols = re.sub(r'\b(\w+)(?:\W\1\b)+', 'r\1', cols, flags = re.IGNORECASE)
    cols = [word for word in cols.split(' ') if word not in stopword]
    cols = &quot; &quot;.join(cols)
    
    return cols
</code></pre>
<p>This is the pandas dataframe that I wish to have at the end:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>name</th>
<th>date</th>
<th>status</th>
<th>number</th>
</tr>
</thead>
<tbody>
<tr>
<td>abcdef</td>
<td>2022-07-11</td>
<td>yes</td>
<td>io123-07</td>
</tr>
<tr>
<td>ghijkl</td>
<td>2022-07-12</td>
<td>no</td>
<td>io456-08</td>
</tr>
</tbody>
</table>
</div>
<p>I'm new to Regex so I wish anyone can help me to code the right code. Or if there is a simpler way to clean my data I would much appreciate the help. Thanks in advance.</p>
",Dataset Preprocessing & Handling,keep datetime data word panda dataframe panda dataframe csv want clean using regex python data look like name date status number bcdef yes io ghijk l io trying clean dataframe easier process thing code deletes date word hyphen data got far name date status number abcdef yes io ghijkl io code found internet tried dataframe panda dataframe wish end name date status number abcdef yes io ghijkl io new regex wish anyone help code right code simpler way clean data would much appreciate help thanks advance
How to gradually train a model on transformers library?,"<p>I have seen <a href=""https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=o6sASa36Nf-N"" rel=""nofollow noreferrer"">this tutorial on how to train a BERT model</a> from scratch on Hugging Face Transformers library. </p>

<p>I'm trying to train a GPT-2 model on 1.5 GB data on Google Colab. I load up all the data using this code:</p>

<pre><code>dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path=""./my-1.5gb-large-file.txt"",
    block_size=128,
)
</code></pre>

<p>The file gets too large, training fails because of limited memory in the GPU. Is there a way I could train my GPT-2 model gradually by splitting the dataset?</p>
",Dataset Preprocessing & Handling,gradually train model transformer library seen tutorial train bert model scratch hugging face transformer library trying train gpt model gb data google colab load data using code file get large training fails limited memory gpu way could train gpt model gradually splitting dataset
Removing various symbols from a text,"<p>I am trying to <strong>clean</strong> some texts that are very different from one another. I would like to remove the headlines, quotation marks, abbreviations, special symbols and points that don't actually end sentences.</p>
<p>Example input:</p>
<pre><code>This is a headline

And inside the text there are 'abbreviations', e.g. &quot;bzw.&quot; in German or some German dates, like 2. Dezember 2017. Sometimes there are even enumerations, that I might just eliminate completely.
• they have
◦ different bullet points
- or even equations and 
Sometimes there are special symbols. ✓
</code></pre>
<p>Example output:</p>
<pre><code>And inside the text there are abbreviations, for example beziehungsweise in German or some German dates, like 2 Dezember 2017. Sometimes there are even enumerations, that I might just eliminate completely. Sometimes there are special symbols.
</code></pre>
<p><strong>What I did:</strong></p>
<pre><code>with open(r'C:\\Users\me\\Desktop\\ex.txt', 'r', encoding=&quot;utf8&quot;) as infile: 
    data = infile.read()
    data = data.replace(&quot;'&quot;, '')
    data = data.replace(&quot;e.g.&quot;, 'for example') 
    #and so on
with open(r'C:\\Users\me\\Desktop\\ex.txt', 'w', encoding=&quot;utf8&quot;) as outfile:
    outfile.write(data)
</code></pre>
<p><strong>My problems (although number 2 is the most important):</strong></p>
<ol>
<li><p>I just want a string with this input, but it obviously breaks because of the quotation marks, is there any way to do this other than working with files like I did? In reality, I'm copy-pasting a text and want an app to clean it.</p>
</li>
<li><p>The code seems very inefficient because I just manually write the things that I remember to delete/clean, but I don't know all the abbreviations by heart. How do I clean it in one go, so to say?</p>
</li>
<li><p>Is there any way to eliminate the headline and enumeration, and the point <code>.</code> that appears in that German date? My code doesn't do that.</p>
</li>
</ol>
<p>Edit: I just remembered stuff like <code>text = re.sub(r&quot;(@\[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?&quot;, &quot;&quot;, text)</code>, but regex is inefficient for huge texts, isn't it?</p>
",Dataset Preprocessing & Handling,removing various symbol text trying clean text different one another would like remove headline quotation mark abbreviation special symbol point actually end sentence example input example output problem although number important want string input obviously break quotation mark way working file like reality copy pasting text want app clean code seems inefficient manually write thing remember delete clean know abbreviation heart clean one go say way eliminate headline enumeration point appears german date code edit remembered stuff like regex inefficient huge text
Fuzzy String Comparison,"<p>What I am striving to complete is a program which reads in a file and will compare each sentence according to the original sentence. The sentence which is a perfect match to the original will receive a score of 1 and a sentence which is the total opposite will receive a 0. All other fuzzy sentences will receive a grade in between 1 and 0. </p>

<p>I am unsure which operation to use to allow me to complete this in Python 3. </p>

<p>I have included the sample text in which the Text 1 is the original and the other preceding strings are the comparisons.  </p>

<h2>Text: Sample</h2>

<p>Text 1: It was a dark and stormy night. I was all alone sitting on a red chair. I was not completely alone as I had three cats.</p>

<p>Text 20: It was a murky and stormy night. I was all alone sitting on a crimson chair. I was not completely alone as I had three felines
// Should score high point but not 1</p>

<p>Text 21: It was a murky and tempestuous night. I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines
// Should score lower than text 20</p>

<p>Text 22: I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines. It was a murky and tempestuous night.
// Should score lower than text 21 but NOT 0</p>

<p>Text 24: It was a dark and stormy night. I was not alone. I was not sitting on a red chair. I had three cats.
// Should score a 0!</p>
",Dataset Preprocessing & Handling,fuzzy string comparison striving complete program read file compare sentence according original sentence sentence perfect match original receive score sentence total opposite receive fuzzy sentence receive grade unsure operation use allow complete python included sample text text original preceding string comparison text sample text wa dark stormy night wa alone sitting red chair wa completely alone three cat text wa murky stormy night wa alone sitting crimson chair wa completely alone three feline score high point text wa murky tempestuous night wa alone sitting crimson cathedra wa completely alone three feline score lower text text wa alone sitting crimson cathedra wa completely alone three feline wa murky tempestuous night score lower text text wa dark stormy night wa alone wa sitting red chair three cat score
How to convert character indices to BERT token indices,"<p>I am working with a question-answer dataset <code>UCLNLP/adversarial_qa</code>.</p>
<pre><code>from datasets import load_dataset
ds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)
</code></pre>
<p>How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset:</p>
<pre><code>d0 = ds['train'][0]
d0

{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',
 'title': 'Brain',
 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',
 'question': 'What sare the benifts of the blood brain barrir?',
 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},
 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}
</code></pre>
<p>After tokenization, the answer indices are 56  and 16:</p>
<pre><code>from transformers import BertTokenizerFast
bert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)

bert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])
'isolated from the bloodstream'
</code></pre>
<p>I want to create a new dataset with the answer's token indices, e.g., 56 ad 60.</p>
<p>This is from a <a href=""https://www.linkedin.com/learning/introduction-to-transformer-models-for-nlp/bert-for-question-answering?autoSkip=true&amp;resume=false"" rel=""nofollow noreferrer"">linkedin learning class</a>. The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:<a href=""https://i.sstatic.net/GsZ6mfcQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GsZ6mfcQ.png"" alt=""QA dataset with token answer indices"" /></a></p>
",Dataset Preprocessing & Handling,convert character index bert token index working question answer dataset map character based answer index token based index tokenizing context question together using tokenizer like bert example row dataset tokenization answer index want create new dataset answer token index e g ad linkedin learning class instructor conversion created csv file share code expected result
"With spaCy, how can I get all lemmas from a string?","<p>I have a pandas data frame with a column of text values (documents).  I want to apply lemmatization on these values with the spaCy library using the pandas <code>apply</code> function.  I've defined my <code>to_lemma</code> function to iterate through the words in the document and concatenate the corresponding lemmas in the output string, however this is very slow.  Is there a way to extract the lemmatized form of a document in spaCy?</p>
<pre><code>def to_lemma(text):
    tp = nlp(text)
    line = &quot;&quot;
    for word in tp:
        line = line + word.lemma_ + &quot; &quot;
    return line
</code></pre>
",Dataset Preprocessing & Handling,spacy get lemma string panda data frame column text value document want apply lemmatization value spacy library using panda function defined function iterate word document concatenate corresponding lemma output string however slow way extract lemmatized form document spacy
Reading Documents Directly from Llama Index as Files Instead of Specifying a Folder Path,"<p>I'm using llama index and want to directly read documents as files instead of specifying the folder path as described in the official documentation. The current method assumes that llama index users always have a downloaded file in a directory. Here's the code snippet from the documentation:</p>
<pre class=""lang-py prettyprint-override""><code>from llama_index.core import SimpleDirectoryReader

documents = SimpleDirectoryReader(&quot;./data&quot;).load_data()
</code></pre>
<p>How can I modify this to read documents directly without specifying the folder path?</p>
",Dataset Preprocessing & Handling,reading document directly llama index file instead specifying folder path using llama index want directly read document file instead specifying folder path described official documentation current method assumes llama index user always downloaded file directory code snippet documentation modify read document directly without specifying folder path
Improvement on context based search(Now using BM25),"<p>Currently I'm working on improving search result on my project. I'm not familiar with algorithm, so may ask something stupid.</p>
<p>Let's say the project has millions restaurants stored in elasticsearch like this:</p>
<pre><code>{
  &quot;id&quot;, 1,
  &quot;name&quot;, &quot;some name&quot;,
  &quot;address&quot;, &quot;some address&quot;,
  &quot;score&quot;, 5,
  &quot;desc&quot;, &quot;some desc&quot;
}
</code></pre>
<p>Currently we are using elastic search built in tokenizer to tokenize user input and searching using BM25.</p>
<p>e.g. &quot;Best pizza in London&quot;, the user input will be tokenized like &quot;Best pizza&quot;, &quot;London&quot;, etc, and then searching using BM25.</p>
<p>However, it comes the issue that from user's perspective, their expectation with priority is:</p>
<ol>
<li>Best to find an list for &quot;Best pizza in London&quot;</li>
<li>Then followed by &quot;Best pizza-like food in London&quot;</li>
<li>Then followed by &quot;Best food other than pizza in London&quot;
But not &quot;Best pizza in York&quot;.</li>
</ol>
<p>The issue is BM25 relies on frequency and document length, and treat all tokens equally. However, in real world, their importance varies.</p>
<p>It's not only the location, due to various user inputs, it could be the person, the event, etc that matters.</p>
<p>So nowadays, for such a requirement, what could be a better way to solve?</p>
",Dataset Preprocessing & Handling,improvement context based search using bm currently working improving search result project familiar algorithm may ask something stupid let say project ha million restaurant stored elasticsearch like currently using elastic search built tokenizer tokenize user input searching using bm e g best pizza london user input tokenized like best pizza london etc searching using bm however come issue user perspective expectation priority best find list best pizza london followed best pizza like food london followed best food pizza london best pizza york issue bm relies frequency document length treat token equally however real world importance varies location due various user input could person event etc matter nowadays requirement could better way solve
spacy doc.char_span raises error whenever there is any number in string,"<p>I was trying to train a model from spacy. I have strings and their token offsets saved into the JSON file.</p>
<p>I have read that file using <code>utf-8</code> encoding and there is no special character in it. But it raises <code>TypeError: object of type 'NoneType' has no len()</code></p>
<pre class=""lang-py prettyprint-override""><code># code for reading file
with open(&quot;data/results.json&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as file:
    training_data = json.loads(file.read())
</code></pre>
<p>I have also tried changing <code>alignment_type</code> from <code>strict</code> to <code>contract</code> &amp; <code>expand</code>. The <code>expand</code> works but shows incorrect spans.</p>
<pre class=""lang-py prettyprint-override""><code>span = doc.char_span(start, end, label, alignment_mode=&quot;contract&quot;)
</code></pre>
<p>The code that I'm using</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.tokens import DocBin

nlp = spacy.blank(&quot;en&quot;)
db = DocBin()
training_dataset = [[
        &quot;Department of Chemistry,Central University of Las Villas,Santa Clara,Villa Clara,54830,Cuba.&quot;,
        [
            [
                57,
                68,
                &quot;city_name&quot;
            ],
            [
                87,
                91,
                &quot;country_name&quot;
            ]
        ]
    ]]
for text, annotations in training_dataset:
    doc = nlp(text)
    ents = []
    for start, end, label in annotations:
        span = doc.char_span(start, end, label)
        ents.append(span)
    doc.ents = ents
    db.add(doc)
</code></pre>
<p>I have pasted the JSON object that is read from the file, directly into the program for debugging purposes.</p>
<p>When I tried after removing the <code>54830,</code> part, the program runs successfully.</p>
<p>I have also referred to this <a href=""https://stackoverflow.com/questions/69976538/spacy-preparing-training-data-doc-char-span-returning-none"">Issue</a>, but that issue has a special character. But this string doesn't have any special character.</p>
<p>Can anyone know why this is happening with all strings that contain a number in them?</p>
",Dataset Preprocessing & Handling,spacy doc char span raise error whenever number string wa trying train model spacy string token offset saved json file read file using encoding special character raise also tried changing work show incorrect span code using pasted json object read file directly program debugging purpose tried removing part program run successfully also referred href issue ha special character string special character p anyone know happening string contain number
How to optimize this function and improve running time?,"<p>I have function aimed at creating a data-frame with three columns; bigram-phrase, count (of the bigram-phrase), and PMI score (for the bigram-phrase). Since I want to run this on a large dataset with over a million phrases, the compute time is incredibly long. I recognize that the nested for-loops and matching conditions are contributing to the computation difficulties. Is there an alternative way to do the same thing and cut down run-time?</p>
<p>Here's my code:</p>
<pre><code>def pmi_count_phrase_create(pmi_tups,freq_list):

    import pandas as pd

    &quot;&quot;&quot;pmi_tups is result of running pmi_tups = [i for i in finder.score_ngrams(bigram_measures.pmi)]  
       freq_list is a result of running freq_list= finder.ngram_fd.items() 
       
       -&gt; df made up of columns for  pmi list, count list, phrase list&quot;&quot;&quot;
    pmi3_list =[]
    count3_list =[]
    phrase3_list =[]
    for phrase, pmi in pmi_tups: #pmi_tups is list of tuples of form:[((phrase),pmi),..]
        for item in freq_list:  
            quadgram,count = item
            if quadgram == phrase:
                pmi3_list.append(pmi)
                count3_list.append(count)
                phrase3_list.append(phrase)

                # create dataframe
    df = pd.DataFrame({'Phrase':phrase3_list,'PMI':pmi3_list,'Count':count3_list})
    return df 
</code></pre>
<p>Running this code on my pmi_tups and freq_list, it is still running and it's been over 1000 minutes. I'm open to also using a different library to evaluate the bi-gram phrases, pmi's and frequencies.</p>
",Dataset Preprocessing & Handling,optimize function improve running time function aimed creating data frame three column bigram phrase count bigram phrase pmi score bigram phrase since want run large dataset million phrase compute time incredibly long recognize nested loop matching condition contributing computation difficulty alternative way thing cut run time code running code pmi tup freq list still running minute open also using different library evaluate bi gram phrase pmi frequency
Extracting titles from PDFs based on formatting via classification,"<p>I have about 20000 pdfs with probably 100 different layouts. Unfortunately, not all PDFs contain clean metadata. People are often lazy and did not always provide the title with it or sometimes a very short one or even worse just the file name as title.</p>
<p>Because of the number of documents, I started with PyMuPDF before diving into deep learning (e.g layout-parser, layout_lm etc.).</p>
<p>A naive implementation would be to filter for the text with the largest font size at the beginning of the document. But as can be seen here that does not work well for different layouts:
<a href=""https://www.ema.europa.eu/en/documents/overview/vectormune-nd-epar-summary-public_en.pdf"" rel=""nofollow noreferrer"">title and subtitle</a>,
<a href=""https://www.ema.europa.eu/en/documents/scientific-guideline/ich-q-7-good-manufacturing-practice-active-pharmaceutical-ingredients-step-5_en.pdf"" rel=""nofollow noreferrer"">old format</a>,
<a href=""https://www.ema.europa.eu/system/files/documents/scientific-guideline/wc500191492_en.pdf"" rel=""nofollow noreferrer"">simple example for which a naive solution works well</a></p>
<p>I thought about training a classifier based on a feature matrix created like:</p>
<pre><code>def create_feature_matrix(blocks):
    &quot;&quot;&quot;
    blocks created using pymupdf like:
    # Open the PDF file
    doc = fitz.open(pdf_path)

    # Extract data from the first page
    page = doc[0]
    text_instances = page.get_text(&quot;dict&quot;)[&quot;blocks&quot;]
    &quot;&quot;&quot;
    text_instances = blocks

    # Initialize the feature matrix
    feature_matrix = []

    # Iterate through text instances and extract features
    for instance in text_instances:
        if &quot;lines&quot; in instance:
            for line in instance[&quot;lines&quot;]:
                for span in line[&quot;spans&quot;]:
                    # Extract text, color, and positional information
                    text = span[&quot;text&quot;]
                    color = span[&quot;color&quot;]
                    size = span[&quot;size&quot;]
                    font = span[&quot;font&quot;]
                    bbox = span[&quot;bbox&quot;]  # bbox = (x0, y0, x1, y1)
                    feature_matrix.append({
                        &quot;text&quot;: text,
                        &quot;color&quot;: color,
                        &quot;size&quot;: size,
                        &quot;font&quot;: font,
                        &quot;x0&quot;: bbox[0],
                        &quot;y0&quot;: bbox[1],
                        &quot;x1&quot;: bbox[2],
                        &quot;y1&quot;: bbox[3]
                    })

    return feature_matrix
</code></pre>
<p>this code could be used together with PyMuPDF like so:</p>
<pre><code>import pandas as pd
import fitz
pdf_path = some_path
doc = fitz.open(pdf_path)
# Extract data from the first page
page = doc[0]
blocks = page.get_text(&quot;dict&quot;)[&quot;blocks&quot;]
FM_for_one_page = pd.DataFrame(create_feature_matrix(blocks))
</code></pre>
<p>My plan is to manually label the rows based on the text like: text is title: 1, not a title: 0.</p>
<p>This solution might be an improvement to my naive rule based classifier. However, I doubt my approach is very robust and it opens more questions:</p>
<ul>
<li>Would it be ok to just concatenate the feature matrices for all first pages? I do loose the information about the page boundaries. But I have no idea what else I could do.</li>
<li>The features of the titles depend on the surrounding structure and sequence. What model could capture that?</li>
<li>Do you think I am on a right path? If not, how would you tackle such a challenge?</li>
</ul>
",Dataset Preprocessing & Handling,extracting title pdfs based formatting via classification pdfs probably different layout unfortunately pdfs contain clean metadata people often lazy always provide title sometimes short one even worse file name title number document started pymupdf diving deep learning e g layout parser layout lm etc naive implementation would filter text largest font size beginning document seen doe work well different layout title subtitle old format simple example naive solution work well thought training classifier based feature matrix created like code could used together pymupdf like plan manually label row based text like text title title solution might improvement naive rule based classifier however doubt approach robust open question would ok concatenate feature matrix first page loose information page boundary idea else could feature title depend surrounding structure sequence model could capture think right path would tackle challenge
Enhancing Document Layout Analysis by Adding Positional and Character Information to CNN Inputs,"<p>I am working on document layout analysis and have been exploring CNNs and transformer-based networks for this task. Typically, images are passed as 3-channel RGB inputs to these networks. However, my data source is in PDF format, from which I can extract the exact position and character information directly.</p>
<p>I am concerned that converting this PDF data into images for analysis will result in the loss of valuable positional and character information. My idea is to modify the input dimensions of the CNN from the standard 3 RGB channels to a higher-dimensional input that includes this additional positional and character information.</p>
<p>I understand how CNNs work and highly suspect that this approach might not work, but I would appreciate any feedback or suggestions from the community. Has anyone experimented with augmenting input channels in this way, or does anyone have insights into integrating positional and character data directly into CNNs?</p>
",Dataset Preprocessing & Handling,enhancing document layout analysis adding positional character information cnn input working document layout analysis exploring cnns transformer based network task typically image passed channel rgb input network however data source pdf format extract exact position character information directly concerned converting pdf data image analysis result loss valuable positional character information idea modify input dimension cnn standard rgb channel higher dimensional input includes additional positional character information understand cnns work highly suspect approach might work would appreciate feedback suggestion community ha anyone experimented augmenting input channel way doe anyone insight integrating positional character data directly cnns
Identify starting row of actual data in Pandas DataFrame with merged header cells,"<p>My original df looks like this -
<a href=""https://i.sstatic.net/H3ZGyouO.png"" rel=""nofollow noreferrer"">df</a></p>
<p>Note in the data frame:</p>
<ol>
<li>The headers are there till row 3 &amp; from row 4 onwards, the values for those headers are starting.</li>
<li>The numbers of rows &amp; columns merged to create the headers are not same</li>
<li>The values (from row 4 onwards) can contain missing values.</li>
</ol>
<p>In Jupyter Notebook, I get something like this -
<a href=""https://i.sstatic.net/DJwLsM4E.png"" rel=""nofollow noreferrer"">pandas_df</a></p>
<ul>
<li><strong>The requirement: Identify row number i.e., the row from where the values for the headers are starting and from df to df, this row number can vary (i.e., header may include more/less number of rows.</strong></li>
</ul>
<p>One can use this code to generate the given table -</p>
<pre><code>import numpy as np

data = {'Col1': ['Column1', np.nan, np.nan, np.nan, 11.0, 32.0, 22.0],
 'Col2': ['Column2', np.nan, 'Col2PartA', 'P', 'A', 'M', 'C'],
 'Col3': [np.nan, np.nan, 'Col2PartB', 'PP', 'HJ', 'KL', 'IO'],
 'Col4': ['Column3', 'Col3PartA', np.nan, 10, np.nan, 24, 43],
 'Col5': [np.nan, 'Col3PartB', np.nan, 23, np.nan, 21, 56],
 'Col6': ['Column4', 'Col4PartA', 'Col4PartA_pt1', 'KLJ', 'TYI', 'MOP', np.nan],
 'Col7': [np.nan, np.nan, 'Col4PartA_pt2', 'WER', 'FYI', 'NOI', np.nan],
 'Col8': [np.nan, 'Col4PartB', 'Col4PartB_pt1', 'DFG', np.nan, 'UIT', np.nan]}

pandas_df = pd.DataFrame(data)
</code></pre>
<p>I am expecting some solution around NLP, but if there are any simple logic that would also be a great help.</p>
",Dataset Preprocessing & Handling,identify starting row actual data panda dataframe merged header cell original df look like df note data frame header till row row onwards value header starting number row column merged create header value row onwards contain missing value jupyter notebook get something like panda df requirement identify row number e row value header starting df df row number vary e header may include le number row one use code generate given table expecting solution around nlp simple logic would also great help
Why my nlp model reload many times when processing question?,"<p>After receiving question, my program calls the run_predict function then finds the best paragraph match with the question.
After that, my model is constantly reloaded without knowing the reasons.</p>
<pre><code>from flask import Flask, render_template, request, jsonify
from flask_socketio import SocketIO, emit
import os
import json
import logging
from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs
from multiprocessing import freeze_support
from models.find_top_paragraphs import main as find_top_paragraphs

app = Flask(__name__)
app.config['SECRET_KEY'] = 'secret!'
socketio = SocketIO(app)

# Configure the model
model_args = QuestionAnsweringArgs()
model_args.eval_batch_size = 16

# Get the absolute path of the current directory
current_dir = os.path.abspath(os.getcwd())

# Path to the outputs directory
outputs_dir = os.path.join(current_dir, &quot;outputs&quot;, &quot;best_model&quot;)

print(f&quot;Model directory: {outputs_dir}&quot;)

# Global variable to store the model
model = None

def load_model():
    global model
    if model is None:
        model = QuestionAnsweringModel(
            model_type=&quot;bert&quot;, 
            model_name=outputs_dir, 
            args=model_args,
            use_cuda=False  # Set to True if you have a GPU and want to use it
        )
        print(&quot;Load model successfully!&quot;)
    else:
        print(&quot;Model is already loaded.&quot;)
    return model

# Load the model only once and reuse it
model = load_model()

def run_predict(question):
    print(&quot;Running predict function&quot;)
    # Directly call the find_top_paragraphs function
    find_top_paragraphs(question)

    # Read the result from the top_paragraphs.json file
    output_path = os.path.join(os.path.dirname(__file__), &quot;models&quot;, &quot;top_paragraphs.json&quot;)
    with open(output_path, &quot;r&quot;, encoding='utf-8') as file:
        data = json.load(file)
    
    question = data[&quot;question&quot;]
    top_paragraph = data[&quot;top_paragraphs&quot;][0]  # Only take the most relevant paragraph
    print(f&quot;Top Paragraph: {top_paragraph}&quot;)

    # Make a prediction with the most relevant paragraph
    to_predict = [
        {
            &quot;context&quot;: top_paragraph,
            &quot;qas&quot;: [
                {
                    &quot;question&quot;: question,
                    &quot;id&quot;: &quot;0&quot;,
                }
            ],
        }
    ]

    # Get the global model
    model = load_model()

    # Make a prediction
    answers, probabilities = model.predict(to_predict)

    # Display the prediction results
    all_answers = []
    for answer in answers:
        for a in answer['answer']:
            all_answers.append(a)
        
        # Find the best answer
        try:
            probability = probabilities[0]['probability']
            best_answer_idx = probability.index(max(probability))
            best_answer = answer['answer'][best_answer_idx]
            print(f&quot;Best Answer: {best_answer}&quot;)
            return all_answers, best_answer
        except Exception as e:
            print(f&quot;An error occurred while selecting the best answer: {e}&quot;)
            return all_answers, None

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('send_message')
def handle_message(data):
    question = data['message']
    all_answers, best_answer = run_predict(question)
    response = {
        'all_answers': all_answers,
        'best_answer': best_answer
    }
    emit('receive_message', response)

if __name__ == '__main__':
    freeze_support()
    socketio.run(app, debug=False)

</code></pre>
<p><a href=""https://i.sstatic.net/19aMM8N3.png"" rel=""nofollow noreferrer"">my terminal when processing the question</a></p>
<p>I tried to load the model in a separate file and then use it in my current file but the problem is still there.
Maybe my code has some problems but i cant really figure it out</p>
",Dataset Preprocessing & Handling,nlp model reload many time processing question receiving question program call run predict function find best paragraph match question model constantly reloaded without knowing reason terminal processing question tried load model separate file use current file problem still maybe code ha problem cant really figure
Text summarizations of comments and replace the duplicates with the first occurrence if the meaning is comment is same,"<p>Context - Doing an NLP project to analyze comments column in a data frame. I want to replace the duplicates with the first occurrence if the meaning of the comments are same.</p>
<p>I wants to compare all the sentences and consider the sentences which have similar meaning.</p>
<p>I have tried transformer for summarization and cosine to find similarity between sentences. but it is not giving me the desired results.</p>
<p>Any help will be deeply appreciated.</p>
<p>code is here -</p>
<pre><code>#functions to lemmatize and remove punctuation
def remove_punctuation(text):
    text = text.lower()
    text=text.strip()
    text = text.translate(str.maketrans('','',string.punctuation))
    text = sent_tokenize(text)
    text_list=[t.strip() for t in text]
    return text_list


def lemmatize_text(text):
    lemmatizer = WordNetLemmatizer()
    words = word_tokenize(text.lower())
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
    return &quot; &quot;.join(lemmatized_words)

model=SentenceTransformer('paraphrase-MiniLM-L6-v2')

#this will have unique values of other comments column. values after summarizations will be replaced in other comments column to bring uniformity
Other_comments_unique=df1[&quot;DUPLICATE - Other comments&quot;].apply(lambda x:x.strip().lower().translate(str.maketrans('','',string.punctuation))).unique()
df1['DUPLICATE - Other comments']=df1['DUPLICATE - Other comments'].apply(remove_punctuation)
df1['DUPLICATE - Other comments'].head()

#summarization - meaning extracting the core message of the sentence. done in this cell
changed_values={}
for i in range(0,len(df1)):
    items = [n for n in df1['test'][i]]
    for u in Other_comments_unique:
        if u!=&quot;nothing more&quot;:
            for a in items:
                embeddings1=model.encode(u)
                embeddings2=model.encode(a)
                cosine_sim = util.cos_sim(embeddings1,embeddings2)
                if cosine_sim.item()&gt;0.5 and cosine_sim.item()&lt;.99:
                changed_values.update({u:a})
        items=[]
</code></pre>
",Dataset Preprocessing & Handling,text summarization comment replace duplicate first occurrence meaning comment context nlp project analyze comment column data frame want replace duplicate first occurrence meaning comment want compare sentence consider sentence similar meaning tried transformer summarization cosine find similarity sentence giving desired result help deeply appreciated code
Trying to read a .wav file but was able to convert only 20s conversation,"<p>I am trying to read a .wav file but was able to convert only 20s conversation into text.</p>
<pre><code>   # code to read .wav file 
   import speech_recognition as sr
    r = sr.Recognizer()
    audio = r&quot;D:\Fraud_Call_Detection\audio1.wav&quot;
    with sr.AudioFile(audio) as source:
        audio = r.record(source)
        print ('Done!')

    try:
        
        text = (r.recognize_google(audio, language=&quot;en-HK&quot;))
        print (text)

    except Exception as e:
        print (e)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/gY8w8qeI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gY8w8qeI.png"" alt=""enter image description here"" /></a></p>
",Dataset Preprocessing & Handling,trying read wav file wa able convert conversation trying read wav file wa able convert conversation text output
FileNotFoundError when loading SQuAD dataset with datasets library,"<p>I am trying to load the SQuAD dataset using the datasets library in Python, but I am encountering a FileNotFoundError. Here is the code I am using:</p>
<pre><code>from datasets import load_dataset
dataset = load_dataset(&quot;squad&quot;)
</code></pre>
<p>However, this results in the following error:</p>
<pre><code>Traceback (most recent call last):   File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;   
File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/load.py&quot;, line 1797, in load_dataset     **config_kwargs,   File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/load.py&quot;, line 1520, in load_dataset_builder     
data_files=data_files,   File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/load.py&quot;, line 1164, in dataset_module_factory     
path, data_dir=data_dir, data_files=data_files, download_mode=download_mode   
File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/load.py&quot;, line 645, in get_module     
allowed_extensions=ALL_ALLOWED_EXTENSIONS,   
File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/data_files.py&quot;, line 798, in from_local_or_remote     
if not isinstance(patterns_for_key, DataFilesList)   
File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/data_files.py&quot;, line 748, in from_local_or_remote     
data_files = resolve_patterns_locally_or by_urls(base_path, patterns, allowed_extensions)   File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/data_files.py&quot;, line 355, in resolve_patterns_locally_or_by_urls     
raise FileNotFoundError(error_msg) FileNotFoundError: Unable to resolve any data file that matches '['**']' at /root/retraining-free-pruning/squad with any supported extension ['.csv', 
'.tsv', '.json', '.jsonl', '.parquet', '.arrow', '.txt', '.blp', '.bmp', '.dib', '.bufr', 
'.cur', '.pcx', '.dcx', '.dds', '.ps', '.eps', '.fit', '.fits', '.fli', '.flc', '.ftc', 
'.ftu', '.gbr', '.gif', '.grib', '.h5', '.hdf', '.png', '.apng', '.jp2', '.j2k', '.jpc', 
'.jpf', '.jpx', '.j2c', '.icns', '.ico', '.im', '.iim', '.tif', '.tiff', '.jfif', '.jpe', 
'.jpg', '.jpeg', '.mpg', '.mpeg', '.msp', '.pcd', '.pxr', '.pbm', '.pgm', '.ppm', '.pnm', 
'.psd', '.bw', '.rgb', '.rgba', '.sgi', '.ras', '.tga', '.icb', '.vda', '.vst', '.webp', 
'.wmf', '.emf', '.xbm', '.xpm', '.BLP', '.BMP', '.DIB', '.BUFR', '.CUR', '.PCX', '.DCX', 
'.DDS', '.PS', '.EPS', '.FIT', '.FITS', '.FLI', '.FLC', '.FTC', '.FTU', '.GBR', '.GIF', 
'.GRIB', '.H5', '.HDF', '.PNG', '.APNG', '.JP2', '.J2K', '.JPC', '.JPF', '.JPX', '.J2C', 
'.ICNS', '.ICO', '.IM', '.IIM', '.TIF', '.TIFF', '.JFIF', '.JPE', '.JPG', '.JPEG', '.MPG', 
'.MPEG', '.MSP', '.PCD', '.PXR', '.PBM', '.PGM', '.PPM', '.PNM', '.PSD', '.BW', '.RGB', 
'.RGBA', '.SGI', '.RAS', '.TGA', '.ICB', '.VDA', '.VST', '.WEBP', '.WMF', '.EMF', '.XBM', 
'.XPM', '.aiff', '.au', '.avr', '.caf', '.flac', '.htk', '.svx', '.mat4', '.mat5', '.mpc2k',
 '.ogg', '.paf', '.pvf', '.raw', '.rf64', '.sd2', '.sds', '.ircam', '.voc', '.w64', '.wav', 
'.nist', '.wavex', '.wve', '.xi', '.mp3', '.opus', '.AIFF', '.AU', '.AVR', '.CAF', '.FLAC', 
'.HTK', '.SVX', '.MAT4', '.MAT5', '.MPC2K', '.OGG', '.PAF', '.PVF', '.RAW', '.RF64', '.SD2', 
'.SDS', '.IRCAM', '.VOC', '.W64', '.WAV', '.NIST', '.WAVEX', '.WVE', '.XI', '.MP3', '.OPUS', 
'.zip']
</code></pre>
<p>It seems like the code is trying to find the SQuAD dataset in my local directory instead of downloading it. I am not sure why this is happening or how to fix it.</p>
<p>How can I correctly load the SQuAD dataset using the datasets library without encountering this FileNotFoundError? What steps do I need to take to resolve this issue?</p>
<ul>
<li>Python version: 3.7</li>
<li>datasets library version: 2.13.2</li>
</ul>
<p>It runs successfully on Colab but not success on autodl cloud GPU platforms. It doesn't seem to be an internet connectivity issue; rather, it appears that some required files or configurations are not being found. And I tried Glue datasets, <code>load_dataset()</code> could find the link and give me the error massage, but squad just give me they could not find files. I don't know the reason.</p>
",Dataset Preprocessing & Handling,filenotfounderror loading squad dataset datasets library trying load squad dataset using datasets library python encountering filenotfounderror code using however result following error seems like code trying find squad dataset local directory instead downloading sure happening fix correctly load squad dataset using datasets library without encountering filenotfounderror step need take resolve issue python version datasets library version run successfully colab success autodl cloud gpu platform seem internet connectivity issue rather appears required file configuration found tried glue datasets could find link give error massage squad give could find file know reason
letter and bigram composition for each word in the dataframe,"<p>I have a data frame with words and I want to extract the letter and bigram composition for each word.</p>
<p><strong>Data:</strong></p>
<pre><code>df$text

[1] &quot;table&quot;
[2] &quot;run&quot;
[3] &quot;mug&quot;`
</code></pre>
<p>And in the end I want to receive the output:</p>
<pre><code>
 1      a b c d e..z aa ab bb...zz 
table   1 1 0 0 0..0  0 1  0    0
</code></pre>
<p>First, I was trying to extract all the letters using Quanteda:</p>
<pre><code>text &lt;- c(&quot;table&quot;, &quot;run&quot;, &quot;mug&quot;)

dict &lt;- dictionary(list(a= &quot;a&quot;,
                        b = &quot;b&quot;,
                        c = &quot;c&quot;,
                        d = &quot;d&quot;, 
                        e = &quot;e&quot;,
                        f = &quot;f&quot;,
                        g = &quot;g&quot;, 
                        h = &quot;h&quot;,
                        i = &quot;i&quot;,
                        j = &quot;j&quot;,
                        k = &quot;k&quot;,
                        l =&quot;l&quot;,
                        m = &quot;m&quot;,
                        n = &quot;n&quot;,
                        o = &quot;o&quot;,
                        p = &quot;p&quot;,
                        q = &quot;q&quot;,
                        r = &quot;r&quot;, 
                        s = &quot;s&quot;,
                        t = &quot;t&quot;,
                        u = &quot;u&quot;,
                        v = &quot;v&quot;, 
                        w = &quot;w&quot;,
                        x = &quot;x&quot;,
                        y = &quot;y&quot;,
                        z = &quot;z&quot;))


corp&lt;- corpus(text)

tokens(corp) |&gt;
  tokens_lookup(dictionary = dict) |&gt;
  dfm()
</code></pre>
<p>But it did not work out:</p>
<pre><code>Document-feature matrix of: 3 documents, 26 features (100.00% sparse) and 0 docvars.
       features
docs    a b c d e f g h i j
  text1 0 0 0 0 0 0 0 0 0 0
  text2 0 0 0 0 0 0 0 0 0 0
  text3 0 0 0 0 0 0 0 0 0 0
[ reached max_nfeat ... 16 more features ]
</code></pre>
<p>I am totally new in this, and if you have any hint how to do that, please, help. Thank you!</p>
",Dataset Preprocessing & Handling,letter bigram composition word dataframe data frame word want extract letter bigram composition word data end want receive output first wa trying extract letter using quanteda work totally new hint please help thank
How to fix RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn,"<p>I am trying to use a custom csv dataset to finetune a model: TheBloke/Mistral-7B-Instruct-v0.1-GPTQ.I performed data preprocessing and I split the dataset into train, validation and test set and then I pass the train data and validation data into transformers.Trainer().  However, I am getting</p>
<blockquote>
<p>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn error.</p>
</blockquote>
<p>How can I fix this error?</p>
<p>Here is the code that is throwing the error:</p>
<pre><code>import transformers
from datetime import datetime

project = &quot;Mixtral-alpaca-finance-finetune&quot;
base_model_name = &quot;mixtral&quot;
run_name = base_model_name + &quot;-&quot; + project
output_dir = &quot;./&quot; + run_name

tokenizer.pad_token = tokenizer.eos_token

trainer = transformers.Trainer(
    model=model,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_val_dataset,
    #dataset_text_field=&quot;text&quot;,
    #max_seq_length=512,
    args=transformers.TrainingArguments(
        output_dir=output_dir,
        warmup_steps=5,
        per_device_train_batch_size=1,
        gradient_checkpointing=True,
        gradient_accumulation_steps=4,
        max_steps=1000,
        learning_rate=2.5e-5,
        lr_scheduler_type=&quot;cosine&quot;,
        logging_steps=25,
        fp16=True,
        optim=&quot;paged_adamw_8bit&quot;,
        logging_dir=&quot;./logs&quot;,        # Directory for storing logs
        save_strategy=&quot;steps&quot;,       # Save the model checkpoint every logging step
        save_steps=50,                # Save checkpoints every 50 steps
        evaluation_strategy=&quot;steps&quot;, # Evaluate the model every logging step
        eval_steps=50,               # Evaluate and save checkpoints every 50 steps
        do_eval=True,                # Perform evaluation at the end of training
        # report_to=&quot;wandb&quot;,           # Comment this out if you don't want to use weights &amp; baises
        run_name=f&quot;{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}&quot;          # Name of the W&amp;B run (optional)
),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),

)

model.config.use_cache = False  # silence the warnings. Re-enable for inference!
trainer.train()
</code></pre>
<p>And here is the error that I am getting:</p>
<blockquote>
<p>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</p>
</blockquote>
<p>I tried to upgrade the torch, accelerate auto-gptq but it still did not work.</p>
",Dataset Preprocessing & Handling,fix runtimeerror element tensor doe require grad doe grad fn trying use custom csv dataset finetune model thebloke mistral b instruct v gptq performed data preprocessing split dataset train validation test set pas train data validation data transformer trainer however getting runtimeerror element tensor doe require grad doe grad fn error fix error code throwing error error getting runtimeerror element tensor doe require grad doe grad fn tried upgrade torch accelerate auto gptq still work
How to fix error `OSError: &lt;model&gt; does not appear to have a file named config.json.` when loading custom fine-tuned model?,"<p><strong>Preface</strong></p>
<p>I am new to implementing the NLP model. I have successfully fine-tuned LLaMA 3-8B variants with QLORA and uploaded them to HuggingFace.</p>
<p>The directories are filled with these files:</p>
<pre><code>-  .gitattributes
- adapter_config.json
- adapter_model.safetensors
- special_tokens_map.json
- tokenizer.json
- tokenizer_config.json
- training_args.bin
</code></pre>
<p><strong>Implementation</strong></p>
<ol>
<li>I am trying to load this model through this:</li>
</ol>
<pre><code>model_id_1 = &quot;ferguso/llama-8b-pcl-v3&quot;

tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1)

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
)

model_1 = AutoModelForCausalLM.from_pretrained(
    model_id_1,
    quantization_config=quantization_config,
)
</code></pre>
<p>But it shows the error <code>OSError: ferguso/llama-8b-pcl-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/ferguso/llama-8b-pcl-v3/tree/main' for available files.</code></p>
<ol start=""2"">
<li>So then I am trying to load the config.json from the original model which is <code>meta-llama/Meta-Llama-3-8B</code>:</li>
</ol>
<pre><code>original_model = &quot;meta-llama/Meta-Llama-3-8B&quot;
model_id_1 = &quot;ferguso/llama-8b-pcl-v3&quot;

tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1)

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
)

original_config = AutoConfig.from_pretrained(original_model)
original_config.save_pretrained(model_id_1)

model_1 = AutoModelForCausalLM.from_pretrained(
    model_id_1,
    quantization_config=quantization_config,
    config = original_config
)
</code></pre>
<p>But still, it shows another error <code>OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ferguso/llama-8b-pcl-v3.</code></p>
<p><strong>Questions</strong></p>
<p>How to load the fine-tuned model properly?</p>
",Dataset Preprocessing & Handling,fix error loading custom fine tuned model preface new implementing nlp model successfully fine tuned llama b variant qlora uploaded huggingface directory filled file implementation trying load model show error trying load config json original model still show another error question load fine tuned model properly
Downloading transformers models to use offline,"<p>I have a trained transformers NER model that I want to use on a machine not connected to the internet. When loading such a model, currently it downloads cache files to the .cache folder.</p>
<p>To load and run the model offline, you need to copy the files in the .cache folder to the offline machine. However, these files have long, non-descriptive names, which makes it really hard to identify the correct files if you have multiple models you want to use. Any thoughts on this?</p>
<p>Example of model files</p>
<p><a href=""https://i.sstatic.net/0CFZj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0CFZj.png"" alt=""enter image description here"" /></a></p>
",Dataset Preprocessing & Handling,downloading transformer model use offline trained transformer ner model want use machine connected internet loading model currently downloads cache file cache folder load run model offline need copy file cache folder offline machine however file long non descriptive name make really hard identify correct file multiple model want use thought example model file
Not able to install spacy==2.3.5 version,"<p>I tried to install spacy==2.3.5 for a resume analyser program. Encountered with a pip subprocess to install build dependencies did not run successfully error.</p>
<p>Using Python 3.12.3</p>
<p>Also it gives a E053 config file error when running the program regarding pyresparser:
&quot;OSError: [E053] Could not read config file from C:\Smart_Resume_Analyser_App-master.venv\Lib\site-packages\pyresparser\config.cfg&quot;</p>
<pre><code>`(.venv) PS C:\Smart_Resume_Analyser_App-master&gt; pip install spacy==2.3.5
</code></pre>
<p><code> </code></p>
<pre><code>    Getting requirements to build wheel did not run successfully.
    exit code: 1

    [267 lines of output]

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
        len_t* widths
        int i
        int nr_layer
        int batch_size

        __init__(len_t* widths, int nr_layer, int batch_size) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:140:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            this._nr_feat = &lt;len_t*&gt;calloc(batch_size, sizeof(len_t))
            this._is_valid = &lt;int*&gt;calloc(batch_size * widths[nr_layer-1], sizeof(int))
            this._costs = &lt;weight_t*&gt;calloc(batch_size * widths[nr_layer-1], sizeof(weight_t))
            this.signatures = &lt;uint64_t*&gt;calloc(batch_size, sizeof(uint64_t))

        __dealloc__() nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:157:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            free(this._nr_feat)
            free(this._is_valid)
            free(this._costs)
            free(this.signatures)

        void reset() nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:172:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            for i in range(this.i):
                free(this._feats[i])
                this._feats[i] = NULL
            this.i = 0

        int nr_in() nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:189:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            this.i = 0

        int nr_in() nogil:
            return this.widths[0]

        int nr_out() nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:192:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this.widths[0]

        int nr_out() nogil:
            return this.widths[this.nr_layer - 1]

        int push_back(const FeatureC* feats, int nr_feat,
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:195:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
                for i in range(this.nr_out()):
                    this.is_valid(this.i)[i] = 1
            this.i += 1
            return this.i &gt;= this.batch_size

        FeatureC* features(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:226:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this.i &gt;= this.batch_size

        FeatureC* features(int i) nogil:
            return this._feats[i]

        int nr_feat(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:229:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._feats[i]

        int nr_feat(int i) nogil:
            return this._nr_feat[i]

        weight_t* fwd(int i, int j) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:232:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._nr_feat[i]

        weight_t* fwd(int i, int j) nogil:
            return this._fwd[i] + (j * this.widths[i])

        weight_t* bwd(int i, int j) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:235:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._fwd[i] + (j * this.widths[i])

        weight_t* bwd(int i, int j) nogil:
            return this._bwd[i] + (j * this.widths[i])

        weight_t* scores(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:238:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._bwd[i] + (j * this.widths[i])

        weight_t* scores(int i) nogil:
            return this.fwd(this.nr_layer-1, i)

        weight_t* losses(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:241:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this.fwd(this.nr_layer-1, i)

        weight_t* losses(int i) nogil:
            return this.bwd(this.nr_layer-1, i)

        weight_t* costs(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:244:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this.bwd(this.nr_layer-1, i)

        weight_t* costs(int i) nogil:
            return this._costs + (i * this.nr_out())

        int* is_valid(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:247:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._costs + (i * this.nr_out())

        int* is_valid(int i) nogil:
            return this._is_valid + (i * this.nr_out())

        int guess(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:250:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._is_valid + (i * this.nr_out())

        int guess(int i) nogil:
            return VecVec.arg_max_if_true(this.scores(i), this.is_valid(i), this.nr_out())

        int best(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:253:4: function definition in pxd file must be declared 'cdef inline'
    warning: thinc\linalg.pxd:14:0: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310
    warning: thinc\linalg.pxd:90:8: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310
    warning: thinc\linalg.pxd:174:8: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310
    Compiling thinc/linalg.pyx because it changed.
    Compiling thinc/structs.pyx because it changed.
    Compiling thinc/typedefs.pyx because it changed.
    Compiling thinc/linear/avgtron.pyx because it changed.
    Compiling thinc/linear/features.pyx because it changed.
    Compiling thinc/linear/serialize.pyx because it changed.
    Compiling thinc/linear/sparse.pyx because it changed.
    Compiling thinc/linear/linear.pyx because it changed.
    Compiling thinc/neural/optimizers.pyx because it changed.
    Compiling thinc/neural/ops.pyx because it changed.
    Compiling thinc/neural/_aligned_alloc.pyx because it changed.
    Compiling thinc/extra/eg.pyx because it changed.
    Compiling thinc/extra/mb.pyx because it changed.
    Compiling thinc/extra/search.pyx because it changed.
    Compiling thinc/extra/cache.pyx because it changed.
    [ 1/15] Cythonizing thinc/extra/cache.pyx
    [ 2/15] Cythonizing thinc/extra/eg.pyx
    Traceback (most recent call last):
      File &quot;C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line 353, in &lt;module&gt;
        main()
      File &quot;C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line 335, in main
        json_out['return_val'] = hook(**hook_input['kwargs'])
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File &quot;C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line 118, in get_requires_for_build_wheel      
        return hook(config_settings)
               ^^^^^^^^^^^^^^^^^^^^^
      File &quot;C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 325, in get_requires_for_build_wheel
        return self._get_build_requires(config_settings, requirements=['wheel'])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File &quot;C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 295, in _get_build_requires
        self.run_setup()
      File &quot;C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 311, in run_setup
        exec(code, locals())
      File &quot;&lt;string&gt;&quot;, line 258, in &lt;module&gt;
      File &quot;&lt;string&gt;&quot;, line 195, in setup_package
      File &quot;C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\Cython\Build\Dependencies.py&quot;, line 1154, in cythonize
        cythonize_one(*args)
      File &quot;C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\Cython\Build\Dependencies.py&quot;, line 1321, in cythonize_one
        raise CompileError(None, pyx_file)
    Cython.Compiler.Errors.CompileError: thinc/extra/eg.pyx
    [end of output]

    note: This error originates from a subprocess, and is likely not a problem with pip.
  error: subprocess-exited-with-error

  Getting requirements to build wheel did not run successfully.
  exit code: 1

  See above for output.

  note: This error originates from a subprocess, and is likely not a problem with pip.
  [end of output]
</code></pre>
<p>note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error</p>
<p>× pip subprocess to install build dependencies did not run successfully.
│ exit code: 1
╰─&gt; See above for output.</p>
<p>note: This error originates from a subprocess, and is likely not a problem with pip.
`</p>
",Dataset Preprocessing & Handling,able install spacy version tried install spacy resume analyser program encountered pip subprocess install build dependency run successfully error using python also give e config file error running program regarding pyresparser oserror e could read config file c smart resume analyser app master venv lib site package pyresparser config cfg note error originates subprocess likely problem pip error subprocess exited error pip subprocess install build dependency run successfully exit code see output note error originates subprocess likely problem pip
Calculate coherence for non-gensim topic model,"<p>I've built a topic model, with:</p>
<ul>
<li><strong>Input</strong>: list of tokenized lists</li>
<li><strong>Output</strong>: a <em>m x t</em> matrix (with each cell indicating the probability of word <em>i</em> appearing in topic <em>k</em>).</li>
<li><strong>Output</strong>: a <em>k x n</em> matrix (with each cell indicating the probability of topic <em>k</em> in document <em>j</em>).</li>
</ul>
<p>To find the optimal number of topics, I want to calculate the coherence for a model. However, I am only aware of <code>Gensim</code>'s <code>Coherencemodel</code>, which seems to require a Gensim model as input.</p>
<p>Are there any other packages/implementations that I could use to calculate the coherence of a computed topic model? Or, if it is indeed possible to use the <code>Coherencemodel</code> without inputting a LDAmodel, could someone show me how to do that?</p>
",Dataset Preprocessing & Handling,calculate coherence non gensim topic model built topic model input list tokenized list output x matrix cell indicating probability word appearing topic k output k x n matrix cell indicating probability topic k document j find optimal number topic want calculate coherence model however aware seems require gensim model input package implementation could use calculate coherence computed topic model indeed possible use without inputting ldamodel could someone show
How to lemmatize text column in pandas dataframes using stanza?,"<p>I read csv file into pandas dataframe.</p>
<p>my text column is df['story'].</p>
<p>how do I lemmatize  this colummn ?</p>
<p>should I tokenize before?</p>
",Dataset Preprocessing & Handling,lemmatize text column panda dataframes using stanza read csv file panda dataframe text column df story lemmatize colummn tokenize
in R converting a text file into a data frame,"<p>in R have a .txt file that i would like to extract data from as a character string. my .txt file is formatted like the following with a list separated by numbers. 1. [text1] 2. [text2] 3. [text3] and so on to 400. i want each observation to be extracted so that the first observation is [text1] and second is [text2] and so on. how would i do this? see below for actual example.</p>
<pre><code>1. 
Readability and Quality of Online Information on Sickle Cell Retinopathy for 
Patients [embase.com]
Gbedemah Z.E.E., Fuseini M.-S.N., Fordjuor S.K.E.J., Baisie-Nkrumah E.J., Beecham R.-
M.E.M., Amissah-Arthur K.N. 
[In Process] Am. J. Ophthalmol. 2024 259: (45-52) 
Embase, MEDLINE
Go to publisher for the full text [dx.doi.org]
Embase Open URL: redirect to full text [embase.com]
Abstract
PURPOSE: This study aims to evaluate the readability and quality of Internet-based health 
information on sickle cell retinopathy. DESIGN: Retrospective cross-sectional website analysis. 
METHODS: To simulate a patient's online search, the terms “sickle cell retinopathy” and “sickle 
cell disease in the eye” were entered into the top 3 search engines (Google, Bing and Yahoo). The 
first 20 results of each search were retrieved and screened for analysis. The DISCERN 
questionnaire, the Journal of the American Medical Association (JAMA) standards, and the Health 
on the Net (HON) criteria were used to evaluate the quality of the information. The Flesch–Kincaid 
Grade Level (FKGL), the Flesch Reading Ease (FRES), and the Automated Readability Index 
(ARI) were used to assess the readability of each website. RESULTS: Of 16 online sources, 12 
(75%) scored moderately on the DISCERN tool. The mean DISCERN score was 40.91 (SD, 
10.39; maximum possible, 80). None of the sites met all of the JAMA benchmarks, and only 3 
(18.75%) of the websites had HONcode certification. All of the websites had scores above the 
target American Medical Association grade level of 6 on both the FKGL and ARI. The mean FRES 
was 57.76 (±4.61), below the recommended FRES of 80 to 90. CONCLUSION: There is limited 
online information available on sickle cell retinopathy. Most included websites were fairly difficult to 
read and of substandard quality. The quality and readability of Internet-based, patient-focused 
information on sickle cell retinopathy needs to be improved.


 
2. 
Multicomponent Strategy Improves Human Papillomavirus Vaccination Rates 
Among Adolescents with Sickle Cell Disease [embase.com]
Aurora T., Cole A., Rai P., Lavoie P., McIvor C., Klesges L.M., Kang G., Liyanage J.S.S., 
Brandt H.M., Hankins J.S. 
J. Pediatr. 2024 265: 
Embase, MEDLINE, NURSING
Go to publisher for the full text [dx.doi.org]
Embase Open URL: redirect to full text [embase.com]
Abstract
Objective: To evaluate the effectiveness of a vaccine strategy bundle to increase human 
papillomavirus (HPV) vaccine initiation and completion in a specialty clinic setting. Study design: 
Our Hematology clinic utilized an implementation framework from October 1, 2018, to December 
31, 2019, involving nurses, nursing coordinators, and clinicians in administering the HPV 
vaccination series to our adolescent sickle cell sample of nearly 500 patients. The bundle included 
education for staff on the need for HPV vaccine administration, provider incentives, vaccines 
offered to patients in SCD clinics, and verification of patients' charts of vaccine completion. 
Results: Following the implementation of the bundle, the cumulative incidence of HPV vaccination 
initiation and completion improved from 28% to 46% and 7% to 49%, respectively. Both rates 
remained higher postimplementation as well. HPV vaccination series completion was associated 
with a decreased distance to the health care facility, lower state deprivation rank, and increased 
hospitalizations. Conclusion: Our clinic's implementation strategy successfully improved vaccine 
completion rates among adolescents with sickle cell disease (SCD) while continuing to educate 
staff, patients, and families on the importance of cancer prevention among people living with SCD.
</code></pre>
<pre><code># Sample character string
cleaned_text &lt;- &quot;1. Readability and Quality of Online Information on Sickle Cell Retinopathy for Patients [embase.com] Gbedemah 2. Another text here 3. Yet another text 4. More text 5. Even more text&quot;

# Split the text based on the numbering pattern
text_split &lt;- strsplit(cleaned_text, &quot;\\d+\\.\\s+&quot;)[[1]]

# Remove the first empty element
text_split &lt;- text_split[-1]

# Convert the result into a data frame
df &lt;- data.frame(Text = text_split)
</code></pre>
",Dataset Preprocessing & Handling,r converting text file data frame r txt file would like extract data character string txt file formatted like following list separated number text text text want observation extracted first observation text second text would see actual example
FastText language_identification in R returns too many arguments - how to match to texts?,"<p>FastText language_identification returns multiple predictions per original text, and also fails to indicate which belong to which original document.</p>
<p>There are differing numbers of predictions per original document too -- their GitHub forums are closed now, but does anyone know how to match the output to the original texts?</p>
<p>Code:</p>
<pre><code>DF = data.frame(doc_id = seq(1, 5),
speechtext = c(&quot;Hello. Fake text entry 1.&quot;, &quot;Fake text entry 2&quot;, &quot;more text&quot;, &quot;Text in a
different language&quot;, &quot;Hola&quot;))

library(fastText)
# download .ftz pretrained model from https://fasttext.cc/docs/en/language-identification.html
file_ftz = system.file(&quot;language_identification/lid.176.ftz&quot;, package = &quot;fastText&quot;)
lang1 = language_identification(DF$speechtext,
                                pre_trained_language_model_path = file_ftz,
                                verbose = T)
</code></pre>
<p>I was expecting one prediction per original text, or at least a consistent number, or some way of marking which document the predictions align with.</p>
<p>Really I could guess based on the largest number per series of a few elements outputted, but this doesn't seem optimal -- it does seem like a bug.</p>
<p>(I tried adding intern = T as an argument per <a href=""https://stackoverflow.com/questions/65130621/r-fasttext-how-to-load-output-into-a-dataframe-from-command-line"">R - fasttext how to load output into a dataframe from command line</a> -- this is not recognized as an argument).</p>
",Dataset Preprocessing & Handling,fasttext language identification r return many argument match text fasttext language identification return multiple prediction per original text also fails indicate belong original document differing number prediction per original document github forum closed doe anyone know match output original text code wa expecting one prediction per original text least consistent number way marking document prediction align really could guess based largest number per series element outputted seem optimal doe seem like bug tried adding intern argument per href fasttext load output dataframe command line recognized argument
Get document ID from LDA output R,"<p>I'm trying to do LDA over two very large corpus of documents.
I need to compare the LDA output (planning to use the Kullback-Leibler similarity measure) across time for each pair of documents. Therefore, at some point, I need to &quot;merge&quot; my documents by time.</p>
<pre><code>output_directory= &quot;fed documents/&quot;
names_files=list.files(output_directory)

## FIRST GO WITH MINUTES

## MINUTES
# 5) MINUTES
minutes_names &lt;- names_files[grepl(&quot;(minutes|histmin|moa|ropa)&quot;, names_files, ignore.case = TRUE)]

minutes=list()

for (this_gb in minutes_names) {
  this_books_path=paste(output_directory,this_gb, sep=&quot;/&quot;)
  this_book=pdf_text(this_books_path)
  
  minutes[[this_gb]]=this_book
  remove(this_book)
  
}


mins_corpus &lt;- Corpus(VectorSource(minutes))
mins_corpus &lt;- tm_map(mins_corpus, tolower)
mins_corpus &lt;- tm_map(mins_corpus, removePunctuation)
mins_corpus &lt;- tm_map(mins_corpus, function(x) removeWords(x, stopwords(&quot;english&quot;)))
mins_corpus &lt;- tm_map(mins_corpus, stemDocument, language = &quot;english&quot;)
mins_corp_matrix=DocumentTermMatrix(mins_corpus)

# GET THE LDA MEASURE
mins_lda=LDA(mins_corp_matrix, k=30, control=list( seed=141096)) # ACOSTA USES 30
mins_topics=tidy(mins_lda, matrix=&quot;gamma&quot;) # GET THE PROBABILITIES BY DOCUMENT AND TOPIC


</code></pre>
<p>This is my first corpus. It's basically the set of minutes, minutes of policy actions and record of policy actions from the Fed's historical archive.</p>
<p>The second set is similarly set up, and consists of speeches given by the chair of the Fed.</p>
<pre><code>output_directory= &quot;fed chair speeches/&quot;
speeches_files &lt;- list.files(output_directory)
speeches_chairs_fed=list()

for(this_speech in speeches_files){
  this_speech_path=paste(output_directory,this_speech, sep=&quot;/&quot;)
  this_speech=pdf_text(this_speech_path)
  
  if (grepl(&quot;\\b(high school|opening|closing|education|summit|school|club)\\b&quot;, this_speech, ignore.case = TRUE)) {
    # Skip this speech and move to the next
    next
  }
  
  speeches_chairs_fed[[this_speech]]
  
}



speech_corpus &lt;- Corpus(VectorSource(speeches_chairs_fed))
speech_corpus &lt;- tm_map(speech_corpus, tolower)
speech_corpus &lt;- tm_map(speech_corpus, removePunctuation)
speech_corpus &lt;- tm_map(speech_corpus, function(x) removeWords(x, stopwords(&quot;english&quot;)))
speech_corpus &lt;- tm_map(speech_corpus, stemDocument, language = &quot;english&quot;)
speech_corp_matrix=DocumentTermMatrix(speech_corpus)

# GET THE LDA MEASURE
speech_lda=LDA(speech_corp_matrix, k=30, control=list( seed=141096)) # ACOSTA USES 30
speeches_topics=tidy(speech_lda, matrix=&quot;gamma&quot;) # GET THE PROBABILITIES BY DOCUMENT AND TOPIC
</code></pre>
<p>I have both sets now, and I need to get a way to merge (either the output of LDA, or the lists) by a common date.
That is, I have two desired outputs:</p>
<ol>
<li>LDA output (the gamma and beta matrices, using LDA notation) for a pair of speech-minutes in a given date</li>
<li>a list that merges, by date, the speeches and the minutes, and then applies LDA. I know I could merge the two lists rather easily, but the problem I see in this approach is that I need to apply LDA to each speech and minute separately and compute the gamma and beta matrices for each, not jointly.</li>
</ol>
",Dataset Preprocessing & Handling,get document id lda output r trying lda two large corpus document need compare lda output planning use kullback leibler similarity measure across time pair document therefore point need merge document time first corpus basically set minute minute policy action record policy action fed historical archive second set similarly set consists speech given chair fed set need get way merge either output lda list common date two desired output lda output gamma beta matrix using lda notation pair speech minute given date list merges date speech minute applies lda know could merge two list rather easily problem see approach need apply lda speech minute separately compute gamma beta matrix jointly
Matching strings containing &#39;and&#39; in different languages and ampersands,"<p>Suppose that in 2 different data frames <code>df1</code>, <code>df2</code> I have 2 columns</p>
<pre><code>df1['film'] = pd.Series(['Beavis &amp; Butthead', 'Bonnie e Clyde', 'Adam &amp; Eve'])
df2['film'] = pd.Series(['Beavis und Butthead', 'Bonnie &amp; Clyde', 'Adam et Eve'])

</code></pre>
<p>of film titles in multiple different languages. (In reality my columns are much larger than this and don't exclusively contain titles with ampersands or different languages' equivalents of the word 'the'. Hence I cannot use something like stopwords for my problem, as it would amends these titles in too 'destructive' a way.)</p>
<p>None of these records result in matches when merging the data frames on these 2 columns, and they would still not all match if I did a simple <code>str.replace</code> since there is no way of knowing from any of the titles whether the ampersand should be 'e', 'et', 'und', 'and', etc.</p>
<p>Since I do not have a list of all the languages that the film titles are written in, how do I approach this problem?</p>
",Dataset Preprocessing & Handling,matching string containing different language ampersand suppose different data frame column film title multiple different language reality column much larger exclusively contain title ampersand different language equivalent word hence use something like stopwords problem would amends title destructive way none record result match merging data frame column would still match simple since way knowing title whether ampersand e et und etc since list language film title written approach problem
Preprocessing a large dataset with tf.layers.TextVectorization gives Memory Errors,"<p>I have around 300k files, which is around 9GB of medical literature.</p>
<p>My goal is to establish the frequency of ALL tokens in the dataset and serialize them to a csv file (token, frequency).</p>
<p>To achieve this, I used <code>layers.TextVectorization</code> with <code>output_mode='count'</code>.</p>
<p>Adapting the huge dataset went fine, but when retrieving the vocabulary I got:</p>
<pre><code>2024-04-20 22:38:56.832518: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-20 22:38:56.833545: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Discoverd 323719 files
Finished adapting
Traceback (most recent call last):
  File &quot;(file_location_of_source_code)&quot;, line 55, in &lt;module&gt;
    inverse_vocab = vectorize_layer.get_vocabulary()
  File &quot;D:\Anaconda\envs\src\lib\site-packages\keras\layers\preprocessing\text_vectorization.py&quot;, line 487, in get_vocabulary
    return self._lookup_layer.get_vocabulary(include_special_tokens)
  File &quot;D:\Anaconda\envs\src\lib\site-packages\keras\layers\preprocessing\index_lookup.py&quot;, line 385, in get_vocabulary
    self._tensor_vocab_to_numpy(vocab),
  File &quot;D:\Anaconda\envs\src\lib\site-packages\keras\layers\preprocessing\string_lookup.py&quot;, line 416, in _tensor_vocab_to_numpy
    [tf.compat.as_text(x, self.encoding) for x in vocabulary]
  File &quot;D:\Anaconda\envs\src\lib\site-packages\keras\layers\preprocessing\string_lookup.py&quot;, line 416, in &lt;listcomp&gt;
    [tf.compat.as_text(x, self.encoding) for x in vocabulary]
MemoryError

Process finished with exit code 1
</code></pre>
<p>Relevant piece from my code:</p>
<pre><code>files_root = pathlib.Path(r&quot;directoryname&quot;)
files = tf.data.TextLineDataset.list_files(str(files_root/'*'))
text_ds = tf.data.TextLineDataset(files).filter(lambda x: tf.cast(tf.strings.length(x), bool))
# by default this tokenizes elements by new line
# we use dataset class bec it is for large amounts of data

# Converting the vocab to integer indexes, wrt their frequency
vectorize_layer = layers.TextVectorization(
    standardize=custom_standardization,
    output_mode='count')
# vectorize_layer.adapt(text_ds.batch(1024))
print(f&quot;Discoverd {len(files)} files&quot;)
vectorize_layer.adapt(text_ds.batch(1024))
print(&quot;Finished adapting&quot;)
inverse_vocab = vectorize_layer.get_vocabulary() #&lt;----ERROR 
print(&quot;Vocabulary Retrieved, with the len:&quot;)
size_vocab = len(inverse_vocab) 
print(size_vocab)
</code></pre>
<p>Further, I was planning to combine the frequency arrays for all sequences, the approach worked on <em>considerably</em> smaller datasets:</p>
<pre><code>text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()
print(&quot;Finished vectorization&quot;)
it = text_vector_ds.as_numpy_iterator()
freq_arr = None
for i, entry in enumerate(text_vector_ds.as_numpy_iterator()):
    if i == 0:
        freq_arr = np.zeros(len(entry))
        freq_arr += entry.astype(int)
    else:
        freq_arr += entry.astype(int)
</code></pre>
<p>What could be the solution to this issue? I need the vocabulary in order to map the actual tokens to their frequency.</p>
<p>I am fairly new to tensorflow and keras, any advice or critique to my approach are welcome. I really need some guidance on working with huge datasets. My ultimate goal is to feed them to a neural network (more specific, some skip grams).
Many thanks.</p>
",Dataset Preprocessing & Handling,preprocessing large dataset tf layer textvectorization give memory error around k file around gb medical literature goal establish frequency token dataset serialize csv file token frequency achieve used adapting huge dataset went fine retrieving vocabulary got relevant piece code wa planning combine frequency array sequence approach worked considerably smaller datasets could solution issue need vocabulary order map actual token frequency fairly new tensorflow kera advice critique approach welcome really need guidance working huge datasets ultimate goal feed neural network specific skip gram many thanks
Unable to read docx file using python-docx,"<p>I have this code:</p>
<pre><code>import os
import traceback

import pdfplumber
from docx import Document

def read_docx(file_path):
    try:
        doc = Document(file_path)
        content = [paragraph.text for paragraph in doc.paragraphs]
        return '\n'.join(content)
    except Exception as e:
        return f&quot;Error reading DOCX file: {e}&quot;

def read_pdf(file_path):
    try:
        text = ''
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                text += page.extract_text() or ''
        return text
    except Exception as e:
        return f&quot;Error reading PDF file: {e}&quot;

def read_txt(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()
    except Exception as e:
        return f&quot;Error reading TXT file: {e}&quot;

def read_file(folder_path, patent_name=None, file_name=None):
    if file_name:
        file_path = os.path.join(folder_path, patent_name, file_name)
    elif patent_name:
        file_path = os.path.join(folder_path, patent_name)
    else:
        file_path = os.path.join(folder_path)

    if os.path.basename(file_path).startswith('~$'):
        return &quot;Skipping temporary file.&quot;

    if file_path.endswith('.docx'):
        return read_docx(file_path)
    elif file_path.endswith('.pdf'):
        return read_pdf(file_path)
    elif file_path.endswith('.txt') and file_name:
        return read_txt(file_path)
    else:
        raise ValueError(&quot;Unsupported file format or missing file name for .txt file&quot;)

def save_text_to_file(folder_path, text, patent_name, file_name):
    try:
        file_path = os.path.join(folder_path, patent_name)
        if not os.path.exists(file_path):
            os.makedirs(file_path)
        with open(os.path.join(file_path, file_name), &quot;w&quot;, encoding='utf-8') as f:
            f.write(text)
    except Exception as err:
        print(err, traceback.format_exc())
</code></pre>
<p>For some reason, I am able to read all of the docx files except one. The files are read on the basis of <code>docID</code>. Here's the structure of <code>docID</code>:</p>
<pre><code>Mode                 LastWriteTime         Length Name
----                 -------------         ------ ----
d-----        16/04/2024     18:00                custom_template
-a----        16/04/2024     17:48         162129 input_disclosure.docx
</code></pre>
<pre><code>Mode                 LastWriteTime         Length Name
----                 -------------         ------ ----
-a----        16/04/2024     17:47          39928 input_customTemplate.docx
</code></pre>
<p>Here's the error that I am getting:</p>
<pre><code>raise PackageNotFoundError(&quot;Package not found at '%s'&quot; % pkg_file)
docx.opc.exceptions.PackageNotFoundError: Package not found at 'C:\Users\hp\Desktop\Projects\PatentGenie\temp\234\custom_template\~$put_customTemplate.docx'
</code></pre>
<p>Note:</p>
<ul>
<li>I haven't opened the file on MS Word or any other editor during execution.</li>
<li>All other files are getting read except this one.</li>
<li>I have copied and pasted the file by manually creating the directory; still no luck.</li>
<li>I have also changed the content inside of the file (by adding couple of spaces) still no luck.</li>
<li>I have also made a new file with the same name. Used the 'Save As' method. Still no luck.</li>
</ul>
",Dataset Preprocessing & Handling,unable read docx file using python docx code reason able read docx file except one file read basis structure error getting note opened file word editor execution file getting read except one copied pasted file manually creating directory still luck also changed content inside file adding couple space still luck also made new file name used save method still luck
Is there a way to save a Spacy Model after adding custom pipes to a Snowflake Stage?,"<p>I am working on Snowflake's UI Snowsight with a Python worksheet. This is my code:</p>
<p>import snowflake.snowpark as snowpark
from snowflake.snowpark.functions import col
import spacy
from spacy.language import Language</p>
<p>def main(session: snowpark.Session):
session.add_import(&quot;@my_python_packages_stage/classy_classification.zip&quot;)
import classy_classification
nlp=spacy.load(&quot;en_core_web_lg&quot;)
data={&quot;Encryption required&quot;:['This','in'],&quot;Encryption not required&quot;:['0','1']}
nlp.add_pipe(&quot;classy_classification&quot;,config={&quot;data&quot;: data, &quot;model&quot;: &quot;spacy&quot;})
i=&quot;This is in 1&quot;
doc = nlp(i)
print(doc._.cats)
return &quot;Classification Done&quot;</p>
<p>This consists of a third party package - Classy Classification which is included as a Pipe in Spacy. I am trying to Save these updates(After adding Pipes) and save a final copy of the entire model with this pipe into a Snowflake Stage.
This would help me to make use of this model in another Python file. Saving this model into a zip file would be good too, to import it as a Stage Package in Snowflake. Please help me out with this approach or suggest some alternate approaches.</p>
",Dataset Preprocessing & Handling,way save spacy model adding custom pipe snowflake stage working snowflake ui snowsight python worksheet code import snowflake snowpark snowpark snowflake snowpark function import col import spacy spacy language import language def main session snowpark session session add import python package stage classy classification zip import classy classification nlp spacy load en core web lg data encryption required encryption required nlp add pipe classy classification config data data model spacy doc nlp print doc cat return classification done consists third party package classy classification included pipe spacy trying save update adding pipe save final copy entire model pipe snowflake stage would help make use model another python file saving model zip file would good import stage package snowflake please help approach suggest alternate approach
Langchain ParentDocumetRetriever: Save and load,"<p>I am using the PartentDocumentRetriever from Langchain.
Now I first want to build my vector database and then want to retrieve stuff.</p>
<p>Here is my file that builds the database:</p>
<pre><code># =========================
#  Module: Vector DB Build
# =========================
import box
import yaml
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.storage import InMemoryStore
from langchain.retrievers import ParentDocumentRetriever
from langchain.vectorstores import Chroma

# Import config vars
with open('config/config.yml', 'r', encoding='utf8') as ymlfile:
    cfg = box.Box(yaml.safe_load(ymlfile))

# Build vector database
def run_db_build():
    loader = DirectoryLoader(cfg.DATA_PATH,
                             glob='*.pdf',
                             loader_cls=PyPDFLoader)
    documents = loader.load()
    
    embeddings = HuggingFaceEmbeddings(model_name=cfg.EMBEDDING_MODEL_NAME,
                                       model_kwargs={'device': 'mps'}, encode_kwargs={'device': 'mps', 'batch_size': 32})
    
    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)
    store = InMemoryStore()

    vectorstore = Chroma(collection_name=&quot;split_parents&quot;, embedding_function=embeddings,
                         persist_directory=&quot;chroma_db/&quot;) 
    big_chunks_retriever = ParentDocumentRetriever(
        vectorstore=vectorstore,
        docstore=store,
        child_splitter=child_splitter,
        parent_splitter=parent_splitter,
    )
    big_chunks_retriever.add_documents(documents)

if __name__ == &quot;__main__&quot;:
    run_db_build()
</code></pre>
<p>So I am saving the Chroma Database in the folder &quot;chroma_db&quot;. However I want to save PartentDocumentRetriever (big_chunk_objects) with the added documents to use it later when building a RetrievalQa chain. So how do I load &quot;big_chunk_objects&quot; in the following code?</p>
<pre><code>def build_retrieval_qa(llm, prompt, vectordb):
    chain_type_kwargs={
        #&quot;verbose&quot;: True,
        &quot;prompt&quot;: prompt,
        &quot;memory&quot;: ConversationBufferMemory(
            memory_key=&quot;history&quot;,
            input_key=&quot;question&quot;)}
    
    dbqa = RetrievalQA.from_chain_type(llm=llm,
                                       chain_type='stuff',
                                       retriever=&quot;HOW TO SET PARENTDOCUMENTRETRIEVER HERE?&quot;,                                       
                                       return_source_documents=cfg.RETURN_SOURCE_DOCUMENTS,
                                       chain_type_kwargs=chain_type_kwargs,
                                      )
return dbqa
</code></pre>
",Dataset Preprocessing & Handling,langchain parentdocumetretriever save load using partentdocumentretriever langchain first want build vector database want retrieve stuff file build database saving chroma database folder chroma db however want save partentdocumentretriever big chunk object added document use later building retrievalqa chain load big chunk object following code
How to Predict New Data Topics with BERTopic Model Loaded from Hugging Face in Python? (Automated solution),"<p>I've developed a BERTopic model for analyzing mobile app reviews and have successfully pushed it to Hugging Face. I'm able to load the model in my Python script but am facing challenges in predicting topics for new incoming feedback.</p>
<p>My goal is to automatically assign a topic to new app feedback using the trained BERTopic model. Here's the workflow I've implemented:</p>
<p>I load the BERTopic model from Hugging Face.
I use the SentenceTransformer model to encode the new documents (app feedback).
I attempt to transform the new documents using the loaded BERTopic model to predict their topics.</p>
<p>However, I'm uncertain about the last part of my code, where I aim to merge the predicted topics with the original dataset containing app feedback. I want to end up with a dataset that includes the original feedback and the assigned topic names, ideally with automatic incrementation for new data.</p>
<p>Here's the code snippet I'm using:</p>
<pre><code>from huggingface_hub import login
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
import pandas as pd

# Login to Hugging Face
access_token_read = &quot;hf_xynbKhivF..........&quot;
login(token=access_token_read)

# Load the BERTopic model
model = BERTopic.load(&quot;shantanudave/BERTopic_ArXiv&quot;)

# Sample new document

new_docs = [&quot;There is an issue with payment on the checkout.&quot;]

# Generate embeddings for the new document
embedding_model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)
embeddings = embedding_model.encode(new_docs, show_progress_bar=True)

# Predict the topic for the new document
new_topics, new_probs = model.transform(new_docs, embeddings)

# Code for merging predicted topics with the original dataset (uncertain part)
...

</code></pre>
<p>Questions:</p>
<p>Is the approach I'm using to predict and assign topics to new feedback using BERTopic correct?
How can I effectively merge the predicted topics back into the original dataset, ensuring that each piece of feedback gets its corresponding topic and topic name?</p>
<pre><code>#My manual approach : - 

T = topic_model.get_document_info(docs)
T_df = T[['Document', 'Topic', 'Name', 'CustomName']]
# Remove duplicates from this new DataFrame
T_clean = T_df.drop_duplicates(subset=['Document'], keep='first')
T_clean

# Performing a left join where 'Document' in T_clean matches 'eng_content' in df , 'eng_content' is the review/feedback

merged_df = pd.merge(df, T_clean, how='left', left_on='eng_content', right_on='Document')
topic_doc_df_1 = merged_df
# Rename columns
topic_doc_df_1 = topic_doc_df_1.rename(columns={
    'CustomName': 'Topic_name',
    'score': 'Rating',
    'date': 'Date',
    'language': 'Language',
    'sentiment': 'Sentiment',
    'probability': 'Probability',
    'app_version': 'App_version'
})

# Rearrange columns
topic_doc_df_1 = topic_doc_df_1[['Topic', 'Topic_name', 'Document', 'Rating', 'Date', 'Language', 'Sentiment', 'Probability', 'App_version']]

topic_doc_df_1

![image](https://github.com/MaartenGr/BERTopic/assets/54185486/d1a96b23-7766-4b34-b615-4a9ce8e804f9)


</code></pre>
<p>I have another function clean_doc() which basically reads new data from df['eng_content'] and provide list 'new_docs'</p>
<p>I am sure there must be a better way of doing this that I am missing,</p>
<p>Thanks in advance,
Shantanu</p>
",Dataset Preprocessing & Handling,predict new data topic bertopic model loaded hugging face python automated solution developed bertopic model analyzing mobile app review successfully pushed hugging face able load model python script facing challenge predicting topic new incoming feedback goal automatically assign topic new app feedback using trained bertopic model workflow implemented load bertopic model hugging face use sentencetransformer model encode new document app feedback attempt transform new document using loaded bertopic model predict topic however uncertain last part code aim merge predicted topic original dataset containing app feedback want end dataset includes original feedback assigned topic name ideally automatic incrementation new data code snippet using question approach using predict assign topic new feedback using bertopic correct effectively merge predicted topic back original dataset ensuring piece feedback get corresponding topic topic name another function clean doc basically read new data df eng content provide list new doc sure must better way missing thanks advance shantanu
&#39;Unknown Encoding&#39; TikToken Error in exe that is compiled with Nuitka,"<p>I'm working with the tiktoken library in Python to count the number of tokens in a series of messages. My code runs well when executed as a Python script. However, when I compile it into an executable, I encounter an 'unknown encoding' error. Attempts to debug the library or replace it with custom code have so far been unsuccessful.</p>
<p>In both scenario in code below failed to get the encoding model from tiktoken for unknown reason. I've tried to load the cl100k_based.tiktoken file from local PC rather than sending the link to openai in the openai_public.py in tiktoken_ext. For pyinstaller we only need to add this line --hidden-import=tiktoken_ext.openai_public --hidden-import=tiktoken_ext to solve the problem. I've used below command</p>
<pre><code>python -m nuitka script.py --onefile --show-modules --include-package=tiktoken --include-package=tiktoken_ext --include-package=blobfile
</code></pre>
<p>But the problem still persist.</p>
<pre><code>def num_tokens_from_messages(messages, model):
    &quot;&quot;&quot;Returns the number of tokens used by a list of messages.&quot;&quot;&quot;
    try:
        encoding = tiktoken.encoding_for_model(model)
        print(&quot;getting encoding_for_mdel&quot;,encoding)
    except Exception as e:
        print(&quot;Warning: model not found. Using cl100k_base encoding.&quot;,e)
        encoding = tiktoken.get_encoding(&quot;cl100k_base&quot;)
        print(&quot;in exception&quot;,encoding)
</code></pre>
<p>Any pointers or insights would be greatly appreciated.</p>
",Dataset Preprocessing & Handling,unknown encoding tiktoken error exe compiled nuitka working tiktoken library python count number token series message code run well executed python script however compile executable encounter unknown encoding error attempt debug library replace custom code far unsuccessful scenario code failed get encoding model tiktoken unknown reason tried load cl k based tiktoken file local pc rather sending link openai openai public py tiktoken ext pyinstaller need add line hidden import tiktoken ext openai public hidden import tiktoken ext solve problem used command problem still persist pointer insight would greatly appreciated
How to assign topics to individual documents/ tweets in Bi-term Topic Modeling?,"<p>I am a newbie at this, so I apologize if I am asking the obvious here. I ran a bi-term topic modeling algorithm to model short text data and discover topics among them. I am using LDAvis package to visualize and understand the data. However, as I understand, reading the raw data qualitatively will help me understand what the underlying topics are and what people are talking about. A keyword search from the raw text data isn't helping since keywords typically overlap with several topics and selecting one keyword may lead to subset which may contain data related to several topics. To this end, I want to assign each tweet/ document to one topic to be able to analyze them manually and get what the topic is talking about since the customer feedback data that I am analyzing typically contains keywords that could indicate several things at once.</p>
<p>I tried assigning the max value from each row in the scores array generated by BTM package <a href=""https://cran.r-project.org/web/packages/BTM/BTM.pdf"" rel=""nofollow noreferrer"">scores array indicated here</a> as the topic that is most likely associated with each document, and that was syntaxically successful. However the generated topics didn't match visually from LDAvis. For instance, I could see a keyword &quot;apps&quot; as being present in only topic &quot;10&quot; as indicated by LDAvis but manually selecting the subset of data that contained keyword &quot;apps&quot; led to two tweets/ documents that were both assigned to different topics per scores array. Am I doing this correctly? I am currently writing the topics to a csv file since I am new to R and didn't want to figure out a way to add the topic to the original dataframe (named data) and then write that file to disk.</p>
<p>#Run bi-term topic modeling</p>
<pre><code>
set.seed(9082374)
model &lt;- BTM(x, k = 10, alpha = 0.1, beta = 0.01, iter = 2000, trace = 100, detailed=TRUE)

</code></pre>
<p>#Predict topic scores for new data
<code>scores &lt;- predict(model, x)</code></p>
<p>#assign topic to each score</p>
<pre><code>final.topic&lt;-apply(scores, 1, which.min)
fwrite(list(final.topic), file=&quot;topic_max.csv&quot;)

</code></pre>
<p>Here is the code if you want to see the code I am running. Specially, the LDAvis part since I did have some challenges in running LDAvis.</p>
<pre><code>term.table1 &lt;- table(x$text)

# Create JSON data for LDAvis
docsize &lt;- table(x$id)
scores &lt;- scores[names(docsize), ]
json &lt;- createJSON(
  phi = t(model$phi),
  theta = scores,
  doc.length = as.integer(docsize),
  vocab = as.character(rownames(model$phi)),
  term.frequency =  term.table1)
serVis(json)
library(data.table)
final.topic&lt;-apply(scores, 1, which.max)
fwrite(list(final.topic), file=&quot;topic_max.csv&quot;)
</code></pre>
",Dataset Preprocessing & Handling,assign topic individual document tweet bi term topic modeling newbie apologize asking obvious ran bi term topic modeling algorithm model short text data discover topic among using ldavis package visualize understand data however understand reading raw data help understand underlying topic people talking keyword search raw text data helping since keywords typically overlap several topic selecting one keyword may lead subset may contain data related several topic end want assign tweet document one topic able analyze manually get topic talking since customer feedback data analyzing typically contains keywords could indicate several thing tried assigning max value row score array generated btm package score array indicated topic likely associated document wa syntaxically successful however generated topic match visually ldavis instance could see keyword apps present topic indicated ldavis manually selecting subset data contained keyword apps led two tweet document assigned different topic per score array correctly currently writing topic csv file since new r want figure way add topic original dataframe named data write file disk run bi term topic modeling predict topic score new data assign topic score code want see code running specially ldavis part since challenge running ldavis
Using trained BERT Model and Data Preprocessing,"<p>When using a pre-trained BERT embeddings from pytorch (which are then fine-tuned), should the text data fed into the model be pre-processed like in any standard NLP task?</p>
<p>For instance, should  stemming, removing low frequency words, de-captilisation, be performed or should the raw text simply be passed to `transformers.BertTokenizer'?</p>
",Dataset Preprocessing & Handling,using trained bert model data preprocessing using pre trained bert embeddings pytorch fine tuned text data fed model pre processed like standard nlp task instance stemming removing low frequency word de captilisation performed raw text simply passed transformer berttokenizer
Feature engineering on BERT,"<p>I'm trying to develop a Tweet classifier using the BERT model (<code>bert-base-uncased</code>, <code>BertForSequenceClassification</code>). During the preprocessing of the dataset, my teacher told me that it would be better if I extracted characteristic features such as the length of the tweet, the number of emojis and profanity words, etc.</p>
<p>So, I gathered some useful features into a data frame like:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>profanity_word</th>
<th>positive_emoji</th>
<th>negative_emoji</th>
<th>tweet_length</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>1</td>
<td>0</td>
<td>123</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>1</td>
<td>52</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
<td>87</td>
</tr>
</tbody>
</table></div>
<p>However, I didn't find any way to input these values into the model for fine-tuning in the documentation. Is there any way to achieve this? Or am I missing some point here?</p>
",Dataset Preprocessing & Handling,feature engineering bert trying develop tweet classifier using bert model preprocessing dataset teacher told would better extracted characteristic feature length tweet number emojis profanity word etc gathered useful feature data frame like profanity word positive emoji negative emoji tweet length however find way input value model fine tuning documentation way achieve missing point
"How to extract structured data from a PDF document using Langchain, and use this data as input to ChatGPT","<p>I'm working on a project where I need to extract data from a PDF document and use that extracted data as input for ChatGPT. I came across Langchain, a language extraction library.</p>
<p>Specifically, I would like to know how to:</p>
<p>Extract text or structured data from a PDF document using Langchain.
Transform the extracted data into a format that can be passed as input to ChatGPT.
Integrate the extracted data with ChatGPT to generate responses based on the provided information.
Any guidance, code examples, or resources would be greatly appreciated. Thank you!</p>
<p>I've been using the Langchain library, UnstructuredFileLoader from langchain.document_loaders to successfully extract data from a PDF document.</p>
<p>Now, I'm attempting to use the extracted data as input for ChatGPT by utilizing the OpenAIEmbeddings. However, I'm encountering an issue where ChatGPT does not seem to respond correctly to the provided data.</p>
<p>I would like to seek advice and suggestions on how to address this problem.</p>
<p>I appreciate any insights, code snippets, or resources that can help me resolve this issue and improve the integration between Langchain and ChatGPT. Thank you in advance for your assistance!</p>
",Dataset Preprocessing & Handling,extract structured data pdf document using langchain use data input chatgpt working project need extract data pdf document use extracted data input chatgpt came across langchain language extraction library specifically would like know extract text structured data pdf document using langchain transform extracted data format passed input chatgpt integrate extracted data chatgpt generate response based provided information guidance code example resource would greatly appreciated thank using langchain library unstructuredfileloader langchain document loader successfully extract data pdf document attempting use extracted data input chatgpt utilizing openaiembeddings however encountering issue chatgpt doe seem respond correctly provided data would like seek advice suggestion address problem appreciate insight code snippet resource help resolve issue improve integration langchain chatgpt thank advance assistance
Model not being executed on Multiple GPUs when using Huggingface Seq2SeqTrainer with accelerate,"<p>I have a setup with a single node having 8 A100 GPUs.</p>
<ul>
<li>python = 3.8.10</li>
<li>torch==2.0.1+cu117</li>
<li>accelerate==0.26.1</li>
<li>CUDA Version: 11.7</li>
</ul>
<p>I am using AutoModelForSeq2SeqLM to load a model for finetuning and use Seq2SeqTrainer. The dataset is copied to multiple GPUs but the model is not being copied (as seen from memory usage using nvidia-smi). Could someone please explain what am I missing for DDP? I can see that 8 different losses are calculated during validation in logs. I am not able to find out why the computation is not being shared as the GPU utilization for 7 GPUs stays zero.</p>
<p>Code:</p>
<pre><code>model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, cache_dir=cache_dir,torch_dtype='torch.bfloat16', device_map='auto')

num_gpus = torch.cuda.device_count()

args = Seq2SeqTrainingArguments(
     output_dir=f&quot;/outdir&quot;,
     evaluation_strategy=“epoch”,
     learning_rate=learning_rate,
     per_device_train_batch_size=batch_size//num_gpus,
     per_device_eval_batch_size=batch_size//num_gpus,
     weight_decay=0.01,
     save_total_limit=1,
     num_train_epochs=num_train_epochs,
     predict_with_generate=True,
     logging_steps=logging_steps,
     push_to_hub=False,
    )
trainer = Seq2SeqTrainer(
     model,
     args,
     train_dataset=tokenized_datasets[“train”],
     eval_dataset=tokenized_datasets[“validation”].select(range(32)),
     data_collator=data_collator,
     tokenizer=tokenizer,
     compute_metrics=compute_metrics,
    )
</code></pre>
<p><a href=""https://i.sstatic.net/ygr0C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ygr0C.png"" alt=""nvidia-smi output"" /></a></p>
",Dataset Preprocessing & Handling,model executed multiple gpus using huggingface seq seqtrainer accelerate setup single node gpus python torch cu accelerate cuda version using automodelforseq seqlm load model finetuning use seq seqtrainer dataset copied multiple gpus model copied seen memory usage using nvidia smi could someone please explain missing ddp see different loss calculated validation log able find computation shared gpu utilization gpus stay zero code
Is it possible to use Google BERT to calculate similarity between two textual documents?,"<p>Is it possible to use Google BERT for calculating similarity between two textual documents? As I understand BERT's input is supposed to be a limited size sentences. Some works use BERT for similarity calculation for sentences like:</p>

<p><a href=""https://github.com/AndriyMulyar/semantic-text-similarity"" rel=""noreferrer"">https://github.com/AndriyMulyar/semantic-text-similarity</a></p>

<p><a href=""https://github.com/beekbin/bert-cosine-sim"" rel=""noreferrer"">https://github.com/beekbin/bert-cosine-sim</a></p>

<p>Is there an implementation of BERT done to use it for large documents instead of sentences as inputs ( Documents with thousands of words)?</p>
",Dataset Preprocessing & Handling,possible use google bert calculate similarity two textual document possible use google bert calculating similarity two textual document understand bert input supposed limited size sentence work use bert similarity calculation sentence like implementation bert done use large document instead sentence input document thousand word
How to save Keras TextVectorization layer configuration with custom standardization function into a pickle file and reload it?,"<p>I have a <code>Keras TextVectorization</code> layer which uses a custom standardization function.</p>
<pre><code>def custom_standardization(input_string, preserve=['[', ']'], add=['¿']):

    strip_chars = string.punctuation
    for item in add:
        strip_chars += item
    
    for item in preserve:
        strip_chars = strip_chars.replace(item, '')

    lowercase = tf.strings.lower(input_string)
    output = tf.strings.regex_replace(lowercase, f'[{re.escape(strip_chars)}]', '')

    return output
</code></pre>
<pre><code>target_vectorization = keras.layers.TextVectorization(max_tokens=vocab_size,
                                                output_mode='int',
                                                output_sequence_length=sequence_length + 1,
                                                standardize=custom_standardization)
</code></pre>
<pre><code>target_vectorization.adapt(train_spanish_texts)
</code></pre>
<p>I want to save the adapted configuration for an inference model to make use of.</p>
<p>One way, as described <a href=""https://stackoverflow.com/a/65225240/15281128"">here</a>, is to save the <code>weights</code> and <code>config</code> separately as a pickle file and reload them.</p>
<p>However, <code>target_vectorization.get_config()</code>  returns</p>
<pre><code>{'name': 'text_vectorization_5',
 'trainable': True,
 ...
 'standardize': &lt;function __main__.custom_standardization(input_string, preserve=['[', ']'], add=['¿'])&gt;,
 ...
 'vocabulary_size': 15000}
</code></pre>
<p>which is being saved into the pickle file.</p>
<p>Trying to load this config using <code>keras.layers.TextVectorization.from_config(pickle.load(open('ckpts/spanish_vectorization.pkl', 'rb'))['config'])</code> results in <code>TypeError: Could not parse config: &lt;function custom_standardization at 0x2a1973a60&gt;</code>, because the file does not have any information about this custom standardization function.</p>
<p>What is a good way to save the TextVectorization weights and configuration for an inference model to make use of, in this scenario?</p>
",Dataset Preprocessing & Handling,save kera textvectorization layer configuration custom standardization function pickle file reload layer us custom standardization function want save adapted configuration inference model make use one way described href save separately pickle file reload p however return saved pickle file trying load config using result file doe information custom standardization function good way save textvectorization weight configuration inference model make use scenario
How to leverage llm to refine and clean text without additional comments?,"<p>I am trying to get chatgpt 3.5 to refine a dataset I have extracted from PDFs. The dataset will act as a knowledge base for an RAG system. The initial tests show great improvement, except for the bot returning the answer with a comment at the end such as &quot;here you have the text with formatted tables, rewritten sentences, etc... &quot;. Of course, I don't want to have this in my knowledge base, as I want to apply this to a lot of text. How can I avoid this?
Note: look at line 10</p>
<pre><code># Prompt
prompt_text = &quot;&quot;&quot;As an assistant, your objective is to improve text and table readability. Here's your guide:

1. Reframe sentences and sections for better understanding.
2. Eliminate unclear text. Example: &quot;Text containing excessive symbols or gibberish.&quot;
3. Shorten text where possible without losing information. Suggestion: Summarize lengthy phrases when feasible.
4. Rectify poorly formatted tables. Example: Adjust column alignment for clarity.
5. Preserve clear, understandable text as it is. Example: &quot;Use direct and easily comprehensible sentences.&quot;
6. If text is entirely unclear or ambiguous, refrain from providing a response. Example: &quot;Incomprehensible or garbled content.&quot;
7. Remove standalone numbers or letters not associated with text. Example: &quot;Eliminate isolated digits or letters lacking context.&quot;
8. Exclude :selection marks, x , and other non-factual elements.
9. Ensure modifications maintain the original text's clarity and don't compromise conveyed information.
10. Return the revised text without any additional comments before or after. 

Please revise the following text based on the listed guidelines: {element} &quot;&quot;&quot;
prompt = ChatPromptTemplate.from_template(prompt_text)
</code></pre>
",Dataset Preprocessing & Handling,leverage llm refine clean text without additional comment trying get chatgpt refine dataset extracted pdfs dataset act knowledge base rag system initial test show great improvement except bot returning answer comment end text formatted table rewritten sentence etc course want knowledge base want apply lot text avoid note look line
PyCharm can&#39;t find Spacy Model &#39;en&#39;,"<p>I am trying to load a NLP model 'en' from SpaCy in my PyCharm and I am using Python 2.7 .<br>
My code to load the 'en' model is 
<code>nlp = spacy.load('en', disable=['parser', 'ner'])</code><br>
However, I received the following error<br>
<code>IOError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.</code><br>
I then realised that I didn't download the model, so I used the terminal provided in PyCharm to download the model, I used <code>python -m spacy download en</code>  </p>

<p>This was the following output:  </p>

<blockquote>
  <p>Requirement already satisfied: en_core_web_sm==2.0.0 from <a href=""https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0"" rel=""noreferrer"">https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0</a>.
  tar.gz#egg=en_core_web_sm==2.0.0 in c:\python27\lib\site-packages<br>
  You are using pip version 9.0.1, however version 18.0 is available.<br>
  You should consider upgrading via the 'python -m pip install --upgrade pip' command.<br>
  You do not have sufficient privilege to perform this operation.</p>
  
  <p>Linking successful
      C:\Python27\lib\site-packages\en_core_web_sm -->
      C:\Python27\lib\site-packages\spacy\data\en</p>
  
  <p>You can now load the model via spacy.load('en')  </p>
</blockquote>

<p>So I am quite confused... I presume that I was unable to download the 'en' model as I do not have enough privileges to do so, but how was the linking successful?<br>
Upon seeing this message, I tried running my Python file again ( since the terminal stated that linking was successful) but the initial error popped out again.  </p>

<p>Has anybody encountered this problem before, or knows how to solve this error? How am I able to 'escalate' my privileges in PyCharm terminal so that I will be able to download the model? </p>
",Dataset Preprocessing & Handling,pycharm find spacy model en trying load nlp model en spacy pycharm using python code load en model however received following error realised download model used terminal provided pycharm download model used wa following output requirement already satisfied en core web sm tar gz egg en core web sm c python lib site package using pip version however version available consider upgrading via python pip install upgrade pip command sufficient privilege perform operation linking successful c python lib site package en core web sm c python lib site package spacy data en load model via spacy load en quite confused presume wa unable download en model enough privilege wa linking successful upon seeing message tried running python file since terminal stated linking wa successful initial error popped ha anybody encountered problem know solve error able escalate privilege pycharm terminal able download model
How to fine tune output of PrivateGPT on CSV or PDF file to get only requisite word or numbers so as to save it in a txt file,"<p>I am presently working on a project. So a dear friend of mine is in an accounting firm. I saw a problem that involved too much of human effort. Like the guys would read bank statement, categorize entries in excel file, recheck debit and credit total with the total given in the statement and then enter in their system (quickbooks and excel files). I came up with an idea to use privateGPT after watching some videos to read their bank statements and give the desired output. That way much of the reading and organization time will be finished. So questions are as follows: Has anyone been able to fine tune privateGPT to give tabular or csv or json style output? Any links to article of exact video since I have been getting generic info. How can we save the desired output of private GPT to a csv? I was looking for something like $Python3 privategpt.py -m &quot;What is the closing balance of $1&quot; -i xyz.pdf &gt;&gt; ABC.txt where -i would mean input file and -m would mean prompt. This works only when I have the output fine tuned to the way I want. Does private GPT have model stacking capabilities? I want to expand this to reading scanned bank statements. how to finetune responses of Private GPT. For instance I just want the closing balance or sum of debit and credit transaction, not the extra info. How to remove extra details? Moreover how can we save the responses to txt file? Tried various video tutorials and docs. It did not cover the issue</p>
",Dataset Preprocessing & Handling,fine tune output privategpt csv pdf file get requisite word number save txt file presently working project dear friend mine accounting firm saw problem involved much human effort like guy would read bank statement categorize entry excel file recheck debit credit total total given statement enter system quickbooks excel file came idea use privategpt watching video read bank statement give desired output way much reading organization time finished question follows ha anyone able fine tune privategpt give tabular csv json style output link article exact video since getting generic info save desired output private gpt csv wa looking something like python privategpt py closing balance xyz pdf abc txt would mean input file would mean prompt work output fine tuned way want doe private gpt model stacking capability want expand reading scanned bank statement finetune response private gpt instance want closing balance sum debit credit transaction extra info remove extra detail moreover save response txt file tried various video tutorial doc cover issue
How to Load the pre-trained word embeddings in .npy files,"<p>I'm trying to use the word embeddings pre-trained in <a href=""https://nlp.stanford.edu/projects/histwords/"" rel=""nofollow noreferrer"">HistWords Project</a> by the Stanford NLP team. But when I ran the document <strong>example.py</strong> from the <a href=""https://github.com/williamleif/histwords"" rel=""nofollow noreferrer"">GitHub website</a>, there was an error: <strong>ModuleNotFoundError: No module named 'representations.sequentialembedding'</strong>.
How can I solve this problem?</p>
<p>I've installed the &quot;representations&quot; module, but it doesn't work. The pre-trained word embeddings are of &quot;.npy&quot; format, is there any Python-based method for uploading them?</p>
",Dataset Preprocessing & Handling,load pre trained word embeddings npy file trying use word embeddings pre trained histwords project stanford nlp team ran document example py github website wa error modulenotfounderror module named representation sequentialembedding solve problem installed representation module work pre trained word embeddings npy format python based method uploading
unable to map color onto a network graph from an additional variable using ggraph,"<p>I am attempting to create a text network graph. I am working with pivoted survey data, and attempting to associate words from open-ended comments with associated numeric responses. I've constructed word correlations and graphed them, but am having a devil of a time associating numeric values back into the network graph. I have experience with R, but I've not had formal training/classes and I feel confident I'm missing something pretty basic right now.</p>
<p>I was able to successfully create a plot using the following code, assuming graph is my data frame, containing variables x (raw numeric score from the survey data), row_number (to tie individual word used back to its initial open ended comment), word, n (# of times &quot;word&quot; appears in the dataset), and y (average of x per word).</p>
<pre><code>graph %&gt;%
  group_by(word) %&gt;%
  filter(n() &gt;= 1000)%&gt;%
  pairwise_cor(word, row_number, upper=FALSE) %&gt;%
  filter(correlation &gt; .09) %&gt;%
  graph_from_data_frame() %&gt;%
  ggraph(layout = &quot;fr&quot;) +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = &quot;lightblue&quot;, size = 5) +
  scale_color_gradient(low = &quot;red&quot;, high = &quot;green&quot;) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
</code></pre>
<p>The pairwise_cor function essentially reshapes the dataframe into item1, item2, and correlation, dropping all other variables, meaning my relevant color-assigning variables were dropped, so I created a correlated words dataset, and then a final_df that joins individual word average scores (y) with the correlated_words dataset:</p>
<pre><code>final &lt;- cor_df %&gt;%
  left_join(filt_df, by = join_by(item1 == word)) %&gt;%
  left_join(filt_df, by = join_by(item2 == word))
</code></pre>
<p>&quot;final&quot; now contains item1 (word 1), item2 (word 2), correlation, n.1, y.1, n.2, and y.2 (where n is count of words and y is a weird stat: the average of X, the original survey numeric score associated that that word).</p>
<p>With the &quot;final&quot; data frame, I've now attempted a multitude of ways to map either y.1 or y.2 to the color of the nodes, generally something like:</p>
<pre><code>as_tbl_graph(final)
ggraph(final, layout = &quot;fr&quot;) +
 geom_node_point(aes(color = y.1), size = 5) +
 geom_node_text(aes(label = name), repel = TRUE) +
 scale_color_gradient(low = &quot;red&quot;, high = &quot;green&quot;) +
 theme_void()
</code></pre>
<p>This is the error I receive:</p>
<p>Error in <code>geom_node_point()</code>:
! Problem while computing aesthetics.
ℹ Error occurred in the 1st layer.
Caused by error in <code>FUN()</code>:
! object 'y.1' not found</p>
<p>Not sure exactly where I'm going wrong, although I have been poring through the documentation for ggraph and tidygraph. I don't have a full conceptual understanding of the various layout possibilities, which I feel is likely where my issues lie (or possibly my confusion starts in the construction of the dataframe itself via as_tbl_graph?), and would really welcome any additional resources or documentation towards understanding those algorithms/customizing layouts. (I've read <a href=""https://cran.r-project.org/web/packages/ggraph/vignettes/Layouts.html"" rel=""nofollow noreferrer"">https://cran.r-project.org/web/packages/ggraph/vignettes/Layouts.html</a> and all of the ggraph vignettes!)</p>
<p>My question, boiled down, is: how can I use a numeric variable to add a color dimension to nodes in a network graph using ggraph (or more specifically, what the heck am I doing wrong)? Thanks in advance for any help!</p>
",Dataset Preprocessing & Handling,unable map color onto network graph additional variable using ggraph attempting create text network graph working pivoted survey data attempting associate word open ended comment associated numeric response constructed word correlation graphed devil time associating numeric value back network graph experience r formal training class feel confident missing something pretty basic right wa able successfully create plot using following code assuming graph data frame containing variable x raw numeric score survey data row number tie individual word used back initial open ended comment word n time word appears dataset average x per word pairwise cor function essentially reshapes dataframe item item correlation dropping variable meaning relevant color assigning variable dropped created correlated word dataset final df join individual word average score correlated word dataset final contains item word item word correlation n n n count word weird stat average x original survey numeric score associated word final data frame attempted multitude way map either color node generally something like error receive error problem computing aesthetic error occurred st layer caused error object found sure exactly going wrong although poring documentation ggraph tidygraph full conceptual understanding various layout possibility feel likely issue lie possibly confusion start construction dataframe via tbl graph would really welcome additional resource documentation towards understanding algorithm customizing layout read ggraph vignette question boiled use numeric variable add color dimension node network graph using ggraph specifically heck wrong thanks advance help
Detect rows in a column containing only emojis in a data frame,"<p>How to detect rows in a column containing only emojis in a data frame? The rows containing text with emojis will not be considered.</p>
<p>Given DF:</p>
<h2>content</h2>
<p>😎🤘🏾</p>
<p>Wow Amazing!!!</p>
<p>I am loving it😍😘</p>
<p>🤘🏾 Rocking</p>
<p>Not Recommended 😒😼</p>
<p>😾</p>
<p>Expected Output:</p>
<h2>content</h2>
<p>😎🤘🏾</p>
<p>😾</p>
<p>I tried:</p>
<pre><code>df = df[df['content'].str.contains(r'[\u263a-\U0001f645]')]
df.head()
</code></pre>
<p>But it shows all emojis including text.</p>
",Dataset Preprocessing & Handling,detect row column containing emojis data frame detect row column containing emojis data frame row containing text emojis considered given df content wow amazing loving rocking recommended expected output content tried show emojis including text
Do I need to retrain an NLP model everytime because of incompatible shape after transforming?,"<p>I'm trying to build an NLP model that uses XGBoost. In my following code.</p>
<pre><code>loaded_model = joblib.load('fraud.sav')
def clean_data(user_input):
    '''
    Cleaning data, removing digits, punctuation etc.
    '''
    return data

data_processed = clean_data(data_raw)
input_cleaned = clean_data(user_data)

total_data = pd.concat([data_processed,input_cleaned])

vectorizer=TfidfVectorizer(strip_accents='unicode',
                            analyzer='word',
                            ngram_range=(1, 2),
                            max_features=15000,
                            smooth_idf=True,
                            sublinear_tf=True)
vectorizer.fit(total_data['text'])

X_training_vectorized = vectorizer.transform(total_data['text'])
X_test = vectorizer.transform(input_cleaned['text'])

pca = PCA(n_components=0.95)
pca.fit(X_training_vectorized.toarray())
X_test_pca = pca.transform(X_test.toarray())

y_test = loaded_model.predict(X_test_pca)
</code></pre>
<p>What I don't understand is, I previously trained my dataset with 10000+ data and got good results. I then decide to save the model so that I can make predictions using user data. My model detects whether a text document is fraudulent or real. I have a dataset thats labelled for fraudulent data.</p>
<p>I understand that when transforming data, vectorizer and pca should both be fitted to our whole dataset so that it will result in the same shape.</p>
<p>What I dont understand is, how do I make it such that, I can transform user input, and have it the same shape as my model that I pretrained? Whats the proper procedure for this? Would love answers that also consider performance/time needed to process the data.</p>
",Dataset Preprocessing & Handling,need retrain nlp model everytime incompatible shape transforming trying build nlp model us xgboost following code understand previously trained dataset data got good result decide save model make prediction using user data model detects whether text document fraudulent real dataset thats labelled fraudulent data understand transforming data vectorizer pca fitted whole dataset result shape dont understand make transform user input shape model pretrained whats proper procedure would love answer also consider performance time needed process data
to determine if the results of two arbitrary sql statements are the same,"<p>In a nl2sql scenario，different LLMs generates different sql statements for a certain natural language query. How can I write codes to determine if the results of these sql statements are the same or not?</p>
<p>For a certain natural language query, different LLMs may generate different sql statements selecting different rows from a table( or maybe even from different tables).</p>
<p>Firstly I can determine these sql statements are essentially the same by handling them in a string way: deleting additional whitespaces and other operations. But it is obviously not enough.</p>
<p>Wit JDBC ResultSet, I think I can figure out if different sql statements have the same count of results(the count of records in the resultset). But for a single resultset, how can I determine how many columns it have, and what the data type it is for a single column? Without these information, I cannot compare the sql statements generated by LLMs in a column data prospect.</p>
<p>Can this task be accomplished by writing codes or not?</p>
",Dataset Preprocessing & Handling,determine result two arbitrary sql statement nl sql scenario different llm generates different sql statement certain natural language query write code determine result sql statement certain natural language query different llm may generate different sql statement selecting different row table maybe even different table firstly determine sql statement essentially handling string way deleting additional whitespaces operation obviously enough wit jdbc resultset think figure different sql statement count result count record resultset single resultset determine many column data type single column without information compare sql statement generated llm column data prospect task accomplished writing code
Fine-tuning pretrained LLM using HuggingFace transformers throws &quot;index out of range in self&quot;,"<p>I am totally new to ML and learning as I go for a work project, where we are attempting to fine-tune a pretrained LLM using the company's data, which consists of magazine articles, podcast transcripts, and discussion threads. Our goal is to create a useful, custom chatbot for our online community.</p>
<p>It is my understanding that the HuggingFace transformers <code>load_dataset</code> function can use rather unstructured plaintext, as opposed to requiring the text to be structured within a JSON object or JSONL file; however, when I attempt to pass in data of this type, I am getting the generic error, <strong>&quot;index out of range in self&quot;</strong>.</p>
<p>Below is a reduced version of the code, which runs successfully up until the trainer.train() line is executed, but it throws the error rather quickly after about 10 seconds.</p>
<pre><code>base_model = &quot;tiiuae/falcon-7b&quot;  # I have tried numerous models, like mpt_7b, distilbert_base_uncased, and moe but always get the same error.
number_of_threads = 4

tokenizer = AutoTokenizer.from_pretrained(base_model, cache_dir=hugging_face_cache_dir)
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': padding_token})

train_dataset = load_dataset('text', data_files={'train': '/path/to/my/train/files',
    'test': '/path/to/my/test/files'},
    cache_dir=hugging_face_cache_dir, sample_by=&quot;paragraph&quot;)
tokenized_train_dataset = train_dataset.map(
    lambda examples: tokenizer(examples\[&quot;text&quot;\], padding=&quot;max_length&quot;,
    truncation=True, return_tensors=&quot;np&quot;),
    batched=True, num_proc=number_of_threads)

val_dataset = load_dataset('text', data_files={'validation': val_split_filename},
    cache_dir=hugging_face_cache_dir, sample_by=&quot;paragraph&quot;)
tokenized_val_dataset = val_dataset.map(
    lambda examples: tokenizer(examples\[&quot;text&quot;\], padding=&quot;max_length&quot;,
    truncation=True, return_tensors=&quot;np&quot;),
    batched=True, num_proc=number_of_threads)

train_dataset = tokenized_train_dataset\['train'\].shuffle(seed=42)
eval_dataset = tokenized_val_dataset\['validation'\]
model = AutoModel.from_pretrained(base_model,
    trust_remote_code=True,
    cache_dir=hugging_face_cache_dir)
training_args = TrainingArguments(
    output_dir=FileMgr.checkpoint_batch_dir,
    evaluation_strategy=IntervalStrategy.EPOCH,
    save_strategy=IntervalStrategy.EPOCH,
    num_train_epochs=3,
    save_total_limit=2,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    logging_dir=FileMgr.checkpoint_batch_dir,
    eval_steps=500,
    load_best_model_at_end=True,
    save_steps=500,
    remove_unused_columns=True
)
metric = evaluate.load(&quot;accuracy&quot;)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics
)
trainer.train()
</code></pre>
<p>Here is an example of what our txt file content looks like:</p>
<pre><code>Some data on the first line.
Some data on the second line.
And this continues on and on.
We have tried putting entire magazine articles on this line, replacing newlines with [SEP].
We've also tried ensuring lines don't exceed the max seq length of a model, as explained below.
</code></pre>
<p>It should maybe be noted that I have my cache system pointing to a directory off of the C drive (Windows!), but I am running PyCharm as an administrator and appear to not be having any issues reading/writing files.</p>
<p><em>Side questions: is it fine to have an entire article on one line even if it exceeds the model's sequence length? And if so, should I set sample_by to &quot;document&quot; instead of &quot;paragraph&quot;? Or would that be more for like reading a bunch of individually-relevant files, not a conglomerate of articles as I am creating?</em></p>
<p>Initially, I read that each line could be very long, such as an <em>entire</em> magazine article on each line of the .txt file, an <em>entire</em> transcript on each line, etc., and so I replaced each newline character with &quot;[SEP]&quot;, and then accounted for this special token as below.</p>
<pre><code>if tokenizer.sep_token is None:     
    tokenizer.add_special_tokens({'sep_token': '[SEP]'})
</code></pre>
<p>But then I read about the &quot;index out of range in self&quot; error having to do with the <strong>training inputs being too long</strong>, and so I came up with a process of first harvesting the data &quot;as is&quot;, and then for every unique maximum sequence length for the models we are trying to experiment with, I create/cache a new batch as necessary to ensure each line is less than the maximum token length.</p>
<p>To ensure that I was not exceeding the maximum token length and determine if this was the issue, I did a test where each line is only 1024 <strong>CHARACTERS</strong> to ensure it was far less than the actual sequence lengths of 512/2048/etc. <strong>TOKENS</strong>; however, after doing this I am still getting the same error.</p>
<p>I have also tried with and without the last line being blank to ensure the out of bounds error was not related, but it is not working.</p>
<p>I have done large tests using our entire dataset, which is about 2.15 GB of data spread out over 53 files, where each one is 7MB to 50MB, and when we account for each line not exceeding the sequence length ends up being hundreds of thousands of training inputs. Same error.</p>
<p>I have done small tests using just 12 files, each with only 4 lines, each line being only about 1,000 characters long, as well as having only alphanumeric, commas, periods, and no [SEP] token. Same error.</p>
<p>I have tried using a <code>per_device_train_batch_size</code> and <code>per_device_eval_batch_size</code> of 1, 8, and 500 to ensure this was not the issue, but no luck.</p>
<p><strong>In the full version of the code, I cache the tokenized datasets (as below), but when the program tries to load them on subsequent runs, it gives an error saying &quot;An error occurred while generating the dataset&quot;, which indicates to me that even though we can tokenize the dataset without error, it is not actually in the correct format, and so is likely where the issue is.</strong></p>
<p>Saving tokenized dataset:
<code>tokenized_train_dataset.save_to_disk(tokenized_train_dataset_cache_path)</code></p>
<p>Loading tokenized dataset:
<code>tokenized_train_dataset = load_dataset(tokenized_train_dataset_cache_path)</code></p>
<p>I realize that this training input wont necessarily create the desired output for a true <em>chat</em>bot, but we want to get this running to understand a baseline before we look into formatting our data further to include input and output labels.</p>
<p>It is also probably really important to point out that, for testing purposes, the test and validation files are basically just placeholders for now, where each file is just three sample inputs from our training data, as I am not yet sure how to format these for text training input as we're working with.</p>
<p>I would be very grateful to anybody who can shed some light or point me in the right direction. Thank you in advance.`</p>
",Dataset Preprocessing & Handling,fine tuning pretrained llm using huggingface transformer throw index range self totally new ml learning go work project attempting fine tune pretrained llm using company data consists magazine article podcast transcript discussion thread goal create useful custom chatbot online community understanding huggingface transformer function use rather unstructured plaintext opposed requiring text structured within json object jsonl file however attempt pas data type getting generic error index range self reduced version code run successfully trainer train line executed throw error rather quickly second example txt file content look like maybe noted cache system pointing directory c drive window running pycharm administrator appear issue reading writing file side question fine entire article one line even exceeds model sequence length set sample document instead paragraph would like reading bunch individually relevant file conglomerate article creating initially read line could long entire magazine article line txt file entire transcript line etc replaced newline character sep accounted special token read index range self error training input long came process first harvesting data every unique maximum sequence length model trying experiment create cache new batch necessary ensure line le maximum token length ensure wa exceeding maximum token length determine wa issue test line character ensure wa far le actual sequence length etc token however still getting error also tried without last line blank ensure bound error wa related working done large test using entire dataset gb data spread file one mb mb account line exceeding sequence length end hundred thousand training input error done small test using file line line character long well alphanumeric comma period sep token error tried using ensure wa issue luck full version code cache tokenized datasets program try load subsequent run give error saying error occurred generating dataset indicates even though tokenize dataset without error actually correct format likely issue saving tokenized dataset loading tokenized dataset realize training input wont necessarily create desired output true chatbot want get running understand baseline look formatting data include input output label also probably really important point testing purpose test validation file basically placeholder file three sample input training data yet sure format text training input working would grateful anybody shed light point right direction thank advance
I have Dataframe Spark and I want to generate Ngrams but the way gensim bigram model does it,"<p>I have a text dataframe (tweets), I am using Spark for high volume data handling and I want to generate Bigrams in the same way as Gensim bigrams models do. I have been using Spark NLP for preprocessing the texts but the Bigrams generation with NgramGenerator performs it with all the words in the text and I want it to be considered a Bigram only when a sequence of words is continuously repeated throughout the texts(Gensim). It would be nice if it can be done with Spark NLP tool. If not, Spark MLlib would work for me, the important thing is that we keep the Spark context.</p>
<p>In the context of Gensim, &quot;bigrams&quot; refers to a technique for detecting and working with sequences of two consecutive words, rather than individual words.</p>
<p>I would be very grateful if someone can help me, thanks.</p>
<p>I tried with SparkNLP's NGramGenerator function but the Ngrams generation is done with all the words of each text.</p>
<pre><code>ngrams = NGramGenerator() \
        .setInputCols([&quot;lemmatized&quot;]) \
        .setOutputCol(&quot;ngrams&quot;) \
        .setN(2) \
        .setEnableCumulative(False)\
        .setDelimiter(&quot;_&quot;)
</code></pre>
<hr />
<p>I tried this way using UDF with gensim but it is not correct, because the process would be done by chuck(row) and gensim uses the whole column to define the bigrams.</p>
<pre><code>from gensim.models import Phrases
from gensim.models.phrases import Phraser

def generate_bigrams(tokens):
    bigram = Phrases(tokens, min_count=5, threshold=100)
    bigram_phraser = Phraser(bigram)
    return list(bigram_phraser[tokens])

generate_bigrams_udf = udf(generate_bigrams, ArrayType(StringType()))
tweets_bigrams = process.withColumn(&quot;bigrams&quot;, generate_bigrams_udf(process[&quot;lemmatized&quot;]))
</code></pre>
",Dataset Preprocessing & Handling,dataframe spark want generate ngrams way gensim bigram model doe text dataframe tweet using spark high volume data handling want generate bigram way gensim bigram model using spark nlp preprocessing text bigram generation ngramgenerator performs word text want considered bigram sequence word continuously repeated throughout text gensim would nice done spark nlp tool spark mllib would work important thing keep spark context context gensim bigram refers technique detecting working sequence two consecutive word rather individual word would grateful someone help thanks tried sparknlp ngramgenerator function ngrams generation done word text tried way using udf gensim correct process would done row gensim us whole column define bigram
k-fold Cross Validation for determining k in k-means?,"<p>In a document clustering process, as a data pre-processing step, I first applied singular vector decomposition to obtain <code>U</code>, <code>S</code> and <code>Vt</code> and then by choosing a suitable number of eigen values I truncated <code>Vt</code>, which now gives me a good document-document correlation from what I read <a href=""http://en.wikipedia.org/wiki/Latent_semantic_analysis"" rel=""nofollow noreferrer"">here</a>. Now I am performing clustering on the columns of the matrix <code>Vt</code> to cluster similar documents together and for this I chose k-means and the initial results looked acceptable to me (with k = 10 clusters) but I wanted to dig a bit deeper on choosing the k value itself. To determine the number of clusters <code>k</code> in k-means, I was <a href=""https://stackoverflow.com/questions/6615665/kmeans-without-knowing-the-number-of-clusters"">suggested</a> to look at cross-validation. </p>

<p>Before implementing it I wanted to figure out if there is a built-in way to achieve it using numpy or scipy. Currently, the way I am performing <code>kmeans</code> is to simply use the function from scipy.</p>

<pre><code>import numpy, scipy

# Preprocess the data and compute svd
U, S, Vt = svd(A) # A is the TFIDF representation of the original term-document matrix

# Obtain the document-document correlations from Vt
# This 50 is the threshold obtained after examining a scree plot of S
docvectors = numpy.transpose(self.Vt[0:50, 0:]) 

# Prepare the data to run k-means
whitened = whiten(docvectors)
res, idx = kmeans2(whitened, 10, iter=20)
</code></pre>

<p>Assuming my methodology is correct so far (please correct me if I am missing some step), at this stage, what is the standard way of using the output to perform cross-validation? Any reference/implementations/suggestions on how this would be applied to k-means would be greatly appreciated.</p>
",Dataset Preprocessing & Handling,k fold cross validation determining k k mean document clustering process data pre processing step first applied singular vector decomposition obtain choosing suitable number eigen value truncated give good document document correlation read performing clustering column matrix cluster similar document together chose k mean initial result looked acceptable k cluster wanted dig bit deeper choosing k value determine number cluster k mean wa href look cross validation p implementing wanted figure built way achieve using numpy scipy currently way performing simply use function scipy assuming methodology correct far please correct missing step stage standard way using output perform cross validation reference implementation suggestion would applied k mean would greatly appreciated
Append processed data to a csv file and keep reord of last procssed row,"<p>I have 4000 csv files in a folder in windows 10, each files has around 500 rows,where i read text column and few identity column, each file got processed in a loop and saved. Because of system limitation sometime process got interrupted.
So instead of saving whole file after processing,I want to kept on appending output csv file with processed individual records. anytime if process interrupted for suppose 'python process' closed or 'system restarted', script should restart itself, start processing last file + last record and begin appending again.
I don't have admin access.</p>
<p>Please suggest some efficient way. Process code has  NLP cleaning, alot of custom regex, and custom processing. its very busy process.</p>
<p>sample code:</p>
<pre><code>clean process(df):
  some code

def read_save_csv():
   logging.debug('start reading files&quot;)
   files_path=&quot;some path&quot;
   for file in glob.glob(file_path):
       df=pd.read_csv(file)
       logging.debug('ended file read')
       try:
          df_process= clean_process(df)
          logging.debug(&quot;start saving file&quot; +filename)
       except exception as e:
          logging.debug(&quot;error&quot; + str(e))

read_save()
</code></pre>
",Dataset Preprocessing & Handling,append processed data csv file keep reord last procssed row csv file folder window file ha around row read text column identity column file got processed loop saved system limitation sometime process got interrupted instead saving whole file processing want kept appending output csv file processed individual record anytime process interrupted suppose python process closed system restarted script restart start processing last file last record begin appending admin access please suggest efficient way process code ha nlp cleaning alot custom regex custom processing busy process sample code
pke - extractor.load_document (Spacy) limitation of 1000000 characters,"<p>While using  extractor.load_document() function of python package pke (<a href=""https://stackoverflow.com"">https://github.com/boudinfl/pke</a>) encountering this error:</p>
<p>ValueError: [E088] Text of length 1717453 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the nlp.max_length limit. The limit is in number of characters, so you can check whether your inputs are too long by checking len(text).</p>
<p>Referred the below issue link:
<a href=""https://stackoverflow.com"">https://github.com/boudinfl/pke/issues/68</a></p>
<p>Code used:</p>
<pre><code>def pke_topicrank(text):
    # initialize keyphrase extraction model, here TopicRank
    extractor = pke.unsupervised.TopicRank()

    # load the content of the document, here document is expected to be a simple 
    # test string and preprocessing is carried out using spacy
    
    #docs = list(nlp.pipe(text, batch_size=1000))
    extractor.load_document(input=text, language=&quot;en&quot;, \
                            normalization=None)

    # keyphrase candidate selection, in the case of TopicRank: sequences of nouns
    # and adjectives (i.e. `(Noun|Adj)*`)
    pos = {'NOUN', 'PROPN', 'ADJ'}
    extractor.candidate_selection(pos=pos)
    #extractor.candidate_selection()
    
    #grammar selection
    extractor.grammar_selection(grammar=&quot;NP: {&lt;ADJ&gt;*&lt;NOUN|PROPN&gt;+}&quot;)

    # candidate weighting, in the case of TopicRank: using a random walk algorithm
    extractor.candidate_weighting(threshold=0.74, method='average')

    # N-best selection, keyphrases contains the 10 highest scored candidates as
    # (keyphrase, score) tuples
    keyphrases = extractor.get_n_best(n=10, redundancy_removal=True, stemming=True)
    keyphrases = ', '.join(set([candidate for candidate, weight in keyphrases]))
    return keyphrases
</code></pre>
<p>Solutions tried:</p>
<ul>
<li>Increasing nlp.max_length to a higher value manually, while loading the spacy pre-trained model. I have installed spacy following the steps listed for GPU</li>
</ul>
<pre><code># Install spacy
website: https://spacy.io/usage#gpu
pip install -U pip setuptools wheel
pip install -U 'spacy[cuda-autodetect]'
python -m spacy download en_core_web_sm
import spacy
activated = spacy.prefer_gpu()
</code></pre>
<pre><code>nlp = spacy.load('en_core_web_sm',exclude=['parser', 'tagger','ner'])
# nlp.add_pipe(nlp.create_pipe('sentencizer'))
nlp.max_length = 2000000
</code></pre>
<ul>
<li>Passing input text through loaded nlp model</li>
</ul>
<pre><code>extractor = pke.unsupervised.TopicRank()
# nlp.add_pipe('sentencizer')
extractor.load_document(input=nlp(text), language=&quot;en&quot;, \
                        normalization='none')
pos = {'NOUN', 'PROPN', 'ADJ'}
extractor.candidate_selection(pos=pos)
extractor.candidate_weighting( threshold=0.74, method='average', heuristic='none')
keyphrases = extractor.get_n_best(n=10, redundancy_removal=True, stemming=False)
keyphrases = ', '.join(set([candidate for candidate, weight in keyphrases]))
</code></pre>
<p>resulting in this error</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[18], line 3
      1 extractor = pke.unsupervised.TopicRank()
      2 # nlp.add_pipe('sentencizer')
----&gt; 3 extractor.load_document(input=nlp(text), language=&quot;en&quot;, \
      4                         normalization='none')
      5 pos = {'NOUN', 'PROPN', 'ADJ'}
      6 extractor.candidate_selection(pos=pos)

File ~/.conda/envs/mainak_multi_intent/lib/python3.9/site-packages/pke/base.py:94, in LoadFile.load_document(self, input, language, stoplist, normalization, spacy_model)
     92 if isinstance(input, spacy.tokens.doc.Doc):
     93     parser = SpacyDocReader()
---&gt; 94     sents = parser.read(spacy_doc=input)
     95 # check whether input is a string
     96 elif isinstance(input, str):

File ~/.conda/envs/mainak_multi_intent/lib/python3.9/site-packages/pke/readers.py:124, in SpacyDocReader.read(self, spacy_doc)
    122 def read(self, spacy_doc):
    123     sentences = []
--&gt; 124     for sentence_id, sentence in enumerate(spacy_doc.sents):
    125         sentences.append(Sentence(
    126             words=[token.text for token in sentence],
    127             pos=[token.pos_ or token.tag_ for token in sentence],
   (...)
    132             }
    133         ))
    134     return sentences

File ~/.conda/envs/mainak_multi_intent/lib/python3.9/site-packages/spacy/tokens/doc.pyx:923, in sents()

ValueError: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.
</code></pre>
<p>while adding sentencizer it returns no keywords.</p>
<p>How to fix it?</p>
",Dataset Preprocessing & Handling,pke extractor load document spacy limitation character using extractor load document function python package pke code used solution tried increasing nlp max length higher value manually loading spacy pre trained model installed spacy following step listed gpu passing input text loaded nlp model resulting error adding sentencizer return keywords fix
Incompatibility between graph NLP and leaflet,"<p>I have a shiny R application in which I have a leaflet card and also an NLP object from LDAvis. There seems to be an incompatibility between the LDAvis graph object and leaflet, as the geometries on the map are not displayed.</p>
<p>To display the LDAvis graph, execute the commented code and generate the objects (json, js, ...) in the line <code>lda_model$plot(...)</code> and then load the generated json file into the <code>readlines</code> function</p>
<pre><code>library(shiny)
library(LDAvis)
library(leaflet)

ui &lt;- fluidPage(
  leaflet::leafletOutput(&quot;mymap&quot;),
  tags$br(),
  tags$br(),
  
  LDAvis::visOutput('ldavis_plot')
)





server &lt;- function(input, output) {
  

  output$mymap &lt;- leaflet::renderLeaflet({
    leaflet::leaflet() %&gt;% leaflet::addTiles() %&gt;%
      leaflet::addRectangles(
        lng1=-118.456554, lat1=34.078039,
        lng2=-118.436383, lat2=34.062717,
        fillColor = &quot;transparent&quot;
      )
  })
  
  
  
  
  output$ldavis_plot &lt;- LDAvis::renderVis({
    # tokens = tolower(movie_review$review[1:1000])
    # tokens = word_tokenizer(tokens)
    # it = itoken(tokens, ids = movie_review$id[1:1000], progressbar = FALSE)
    # v = create_vocabulary(it)
    # v = prune_vocabulary(v, term_count_min = 10, doc_proportion_max = 0.2)
    # 
    # vectorizer = vocab_vectorizer(v)
    # dtm = create_dtm(it, vectorizer, type = &quot;dgTMatrix&quot;)
    # 
    # lda_model = text2vec::LDA$new(n_topics = 5, doc_topic_prior = 0.1, topic_word_prior = 0.01)
    # doc_topic_distr = 
    #   lda_model$fit_transform(x = dtm, n_iter = 100, 
    #                           convergence_tol = 0.001, n_check_convergence = 25, 
    #                           progressbar = FALSE)
    # 
    # lda_model$plot(out.dir = &quot;PATH_TO&quot;, open.browser = FALSE)
    
    
    readLines(&quot;PATH_TO/lda.json&quot;)
  })
  
}

shinyApp(ui, server)

</code></pre>
<p>Does anyone have any ideas on how to get around this incompatibility?</p>
",Dataset Preprocessing & Handling,incompatibility graph nlp leaflet shiny r application leaflet card also nlp object ldavis seems incompatibility ldavis graph object leaflet geometry map displayed display ldavis graph execute commented code generate object json j line load generated json file function doe anyone idea get around incompatibility
load csv file from azure blob storage with langchain,"<p>I am trying to load a csv file from azure blob storage.</p>
<p>However in terminal I can print the data, but it is not directly fed to my chatbot, but for a general data.
the code works fine for CSVloader in a local file but not for azure blob storage. I am following the langchain documentation:</p>
<p><a href=""https://python.langchain.com/docs/integrations/document_loaders/azure_blob_storage_file"" rel=""nofollow noreferrer"">https://python.langchain.com/docs/integrations/document_loaders/azure_blob_storage_file</a></p>
<p>code:</p>
<pre><code>`def __init__(self, conversation_state: ConversationState):
        self.conversation_state = conversation_state
        self.session_accessor = self.conversation_state.create_property(&quot;Session&quot;)
        #loader = CSVLoader(file_path=&quot;data.csv&quot;, encoding=&quot;utf-8&quot;, csv_args={'delimiter': ','})
        loader = AzureBlobStorageFileLoader(
        conn_str=&quot;DefaultEndpointsProtocol=https;AccountName=##;AccountKey=+###==;EndpointSuffix=core.windows.net&quot;,
        container=&quot;dataset&quot;,
        blob_name=&quot;data.csv&quot;)
        data = loader.load() 
        print(type(data)) #&lt;class 'list'&gt;
        print(type(data[0])) #langchain.schema.document.Document'&gt;
        print(data) #I can see the data         `

     
                                 
</code></pre>
<p>Retrying langchain.embeddings.openai.embed_with_retry.._embed_with_retry in 4.0 seconds as it raised APIError: OpenAI API returned an empty embedding.</p>
<p>when asking the bot, as mentioned it answer a general data not my custom data.</p>
",Dataset Preprocessing & Handling,load csv file azure blob storage langchain trying load csv file azure blob storage however terminal print data directly fed chatbot general data code work fine csvloader local file azure blob storage following langchain documentation code retrying langchain embeddings openai embed retry embed retry second raised apierror openai api returned empty embedding asking bot mentioned answer general data custom data
"Filtering Documents Using Word Embeddings: Keep Job Postings, Exclude Resumes","<p>I have a DataFrame containing a column of various documents, and I'm trying to filter out documents that resemble resumes while keeping job postings. To achieve this, I've utilized a CSV file provided <a href=""https://www.kaggle.com/code/akashkotal/resume-screening-with-nlp/input"" rel=""nofollow noreferrer"">here</a> to find similarities between my document contents and resumes.</p>
<p>However, my current approach seems to return both resumes and job postings. I'm interested in retaining the job postings but excluding the resumes from my DataFrame.</p>
<pre><code>def calculate_word_embedding_similarity(dataframe, text_to_compare, column_name='processed_content', embedding_model=None):
    text_tokens = text_to_compare.lower().split()
    dataframe_tokens = dataframe[column_name].str.lower().str.split()

    text_vector = sum(embedding_model[word] for word in text_tokens if word in embedding_model)
    dataframe_vectors = [
        sum(embedding_model[word] for word in tokens if word in embedding_model)
        for tokens in dataframe_tokens
    ]

    cosine_similarities = [
        cosine_similarity([text_vector], [dataframe_vector])[0][0]
        for dataframe_vector in dataframe_vectors
    ]

    dataframe['similarity'] = cosine_similarities

    dataframe = dataframe.sort_values(by='word_embedding_similarity', ascending=False)

    return dataframe
</code></pre>
<p>Could anyone suggest a method or modification to my approach that would allow me to achieve this filtering task effectively? I want to ensure that only job postings are retained in my DataFrame while eliminating resumes.</p>
",Dataset Preprocessing & Handling,filtering document using word embeddings keep job posting exclude resume dataframe containing column various document trying filter document resemble resume keeping job posting achieve utilized csv file provided find similarity document content resume however current approach seems return resume job posting interested retaining job posting excluding resume dataframe could anyone suggest method modification approach would allow achieve filtering task effectively want ensure job posting retained dataframe eliminating resume
AIML chatbot not learning from aiml file,"<p>I am making a voice-assistant for food ordering in fast-food restaurants. For dialog management tasks I am making aiml chatbot.
This is my std-startup.xml code
'''

</p>
<pre><code>&lt;!-- Category is an atomic AIML unit --&gt;
&lt;category&gt;

    &lt;!-- Pattern to match in user input --&gt;
    &lt;!-- If user enters &quot;LOAD AIML B&quot; --&gt;
    &lt;pattern&gt;LOAD AIML B&lt;/pattern&gt;

    &lt;!-- Template is the response to the pattern --&gt;
    &lt;!-- This learn an aiml file --&gt;
    &lt;template&gt;
        &lt;learn&gt;output.aiml&lt;/learn&gt;
        &lt;!-- You can add more aiml files here --&gt;
        &lt;!--&lt;learn&gt;more_aiml.aiml&lt;/learn&gt;--&gt;
    &lt;/template&gt;
    
&lt;/category&gt;
</code></pre>

<p>BELOW IS MAIN.PY CODE</p>
<p>import aiml
import os</p>
<p>kernel = aiml.Kernel()</p>
<p>if os.path.isfile(&quot;bot_brain.brn&quot;):
kernel.bootstrap(brainFile = &quot;bot_brain.brn&quot;)
else:
kernel.bootstrap(learnFiles = &quot;std-startup.xml&quot;, commands = &quot;load aiml b&quot;)
kernel.saveBrain(&quot;bot_brain.brn&quot;)</p>
<h1>kernel now ready for use</h1>
<p>while True:
print (kernel.respond(input(&quot;Enter your message &gt;&gt; &quot;)))
'''</p>
<p>After entering a msg which is from output.aiml itself the bot says the msg does not exists
<a href=""https://i.sstatic.net/l3nRG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/l3nRG.png"" alt=""enter image description here"" /></a></p>
<p>PLease can someone tell how to fix it.
The bot should generate a response as given in output.aiml file</p>
<p>Here is another example with category
<a href=""https://i.sstatic.net/CKIuX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CKIuX.png"" alt=""enter image description here"" /></a></p>
<p>and here is bot's repsonse
<a href=""https://i.sstatic.net/mEwrF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mEwrF.png"" alt=""enter image description here"" /></a></p>
",Dataset Preprocessing & Handling,aiml chatbot learning aiml file making voice assistant food ordering fast food restaurant dialog management task making aiml chatbot std startup xml code main py code import aiml import kernel aiml kernel path isfile bot brain brn kernel bootstrap brainfile bot brain brn else kernel bootstrap learnfiles std startup xml command load aiml b kernel savebrain bot brain brn kernel ready use true print kernel respond input enter message entering msg output aiml bot say msg doe exists please someone tell fix bot generate response given output aiml file another example category bot repsonse
Reading Bengali with python Natural Language Toolkit,"<p>I want to read Bengali texts in NLTK's CategorizedPlainCorpusReader. For this Snapshot of my Bengali text file in gedit text editor:</p>

<p><a href=""https://i.sstatic.net/tXERR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tXERR.png"" alt=""enter image description here""></a></p>

<p>Snapshot of file in sublime text editor:</p>

<p><a href=""https://i.sstatic.net/qYlyK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qYlyK.png"" alt=""enter image description here""></a></p>

<p>From the snapshots you can see the problem. The problem is Unicode composition problem (the dotted ring is a dead giveaway). And here is the code segment for reading texts:</p>

<pre><code>&gt;&gt;&gt; path = os.path.expanduser('~/nltk_data/corpora/Bangla')
&gt;&gt;&gt; from nltk.corpus.reader import CategorizedPlaintextCorpusReader
&gt;&gt;&gt; from nltk import RegexpTokenizer
&gt;&gt;&gt; word_tokenize = RegexpTokenizer(""[\w']+"")
&gt;&gt;&gt; reader = CategorizedPlaintextCorpusReader(path,r'.*\.txt',cat_pattern=r'(.*)_.*',word_tokenizer=word_tokenize)
&gt;&gt;&gt; reader.sents(categories='pos')
</code></pre>

<p>The output is:</p>

<p><a href=""https://i.sstatic.net/knvjX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/knvjX.png"" alt=""enter image description here""></a></p>

<p>The output should be 'একবার' rather than 'একব' 'র'. What can be done?? Thanks in advance.</p>
",Dataset Preprocessing & Handling,reading bengali python natural language toolkit want read bengali text nltk categorizedplaincorpusreader snapshot bengali text file gedit text editor snapshot file sublime text editor snapshot see problem problem unicode composition problem dotted ring dead giveaway code segment reading text output output rather done thanks advance
"How can I identify the number of occurrences of multiple custom emotions, grouped by line, team, and personal ID?","<p>I have a data frame like the following (but much larger and with repeated observations across time):</p>
<pre><code>df &lt;- data.frame(
participant_ID = 1:4, 
TeamID = c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;),
Text1 = c(
    &quot;I shouted angrily, but then felt happy. Now I am very happy.&quot;,
    &quot;He is always calm and never gets angry&quot;,
    &quot;She laughed, making everyone happy&quot;,
    &quot;I'm feeling sad today&quot;
  ), 
)
</code></pre>
<p>I also have the following custom dictionary with examples of what I am looking for within the text as an example of each emotion. Using simple emotions as an example:</p>
<pre><code>emotions &lt;- c(
  anger = c(&quot;angry&quot;, &quot;shouted&quot;),
  happiness = c(&quot;happy&quot;, &quot;laughed&quot;),
  sadness = c(&quot;sad&quot;)
)
</code></pre>
<p>My goal is to count the number of each emotion group, separated by ID, and store it in a new data frame. Ideally, the output looks something like:</p>
<pre><code>  ID TeamID anger happiness sadness 
1  1      A     1         2       0
2  2      A     1         0       0
3  3      B     0         2       0
4  4      B     0         0       1
</code></pre>
<p>I tried creating an empty data frame to store the emotion counts with participant IDs:</p>
<pre><code>emotion_count_df &lt;- data.frame(ID = integer(0), Emotion = character(0), Count = integer(0))
</code></pre>
<p>and wrote a function to count how many times each emotion occurred in the text (and converting to lowercase):</p>
<pre><code>count_emotions &lt;- function(text, emotions) {
  text &lt;- tolower(text)
  counts &lt;- sapply(emotions, function(emotion) sum(grepl(paste(emotion, collapse = &quot;|&quot;), text)))
  return(counts)
}
</code></pre>
<p>Then I tried to loop through the df, count emotions for each emotion and row, and store my results in the new data frame:</p>
<pre><code>for (i in 1:nrow(df)) {
  counts &lt;- count_emotions(df$Text[i], emotions)
  for (j in 1:length(emotions)) {
    emotion_count_df &lt;- rbind(emotion_count_df, data.frame(ID = df$ID[i], Emotion = names(emotions)[j], Count = counts[j]))
  }
}
</code></pre>
<p>But I received the following error:</p>
<pre><code>Error in data.frame(ID = df$ID[i], Emotion = names(emotions)[j],  : 
  arguments imply differing number of rows: 1, 0
</code></pre>
<p>Any help would be greatly appreciated!</p>
",Dataset Preprocessing & Handling,identify number occurrence multiple custom emotion grouped line team personal id data frame like following much larger repeated observation across time also following custom dictionary example looking within text example emotion using simple emotion example goal count number emotion group separated id store new data frame ideally output look something like tried creating empty data frame store emotion count participant id wrote function count many time emotion occurred text converting lowercase tried loop df count emotion emotion row store result new data frame received following error help would greatly appreciated
How to convert back the string object type to tensor object,"<p>I am working on a csv. I have embedded a column and converted into Tensors. Like,</p>
<pre><code>tensor([-1.7110e-01,  1.3811e-01, -2.5881e-01, -1.8281e-01, -3.3073e-01,
    -1.1071e-01])
</code></pre>
<p>Saved those tensors as a new column and saved that into csv. Now when I load that csv again and see the value of the embedded column it looks like this</p>
<pre><code>'tensor([-1.7110e-01,  1.3811e-01, -2.5881e-01, -1.8281e-01, -3.3073e-01,\n        -1.1071e-01])'
</code></pre>
<p>Let me know how to convert it back and use?</p>
",Dataset Preprocessing & Handling,convert back string object type tensor object working csv embedded column converted tensor like saved tensor new column saved csv load csv see value embedded column look like let know convert back use
Get word frequencies from a pretrained word2vec model in gensim,"<p>I am trying to get the CBOW represention (word frequencies) out of a pretrained word2vec model in gensim. I know that the word2vec model I am using was originally trained using CBOW so the word2vec object must store word frequencies somewhere but I cannot find it. I did read the documentation but did not help at all, or maybe I am misunderstanding something.</p>
<p>Edit: If there's no way to retrieve the word frequencies except by having the original dataset, please tell me straight away.</p>
",Dataset Preprocessing & Handling,get word frequency pretrained word vec model gensim trying get cbow represention word frequency pretrained word vec model gensim know word vec model using wa originally trained using cbow word vec object must store word frequency somewhere find read documentation help maybe misunderstanding something edit way retrieve word frequency except original dataset please tell straight away
Remove duplicates and keep top one record,"<p>I have extracted the titles from the multiple documents in a folder where one document contains multiple titles in one file using nltk library which is working fine. The only issue is that it contains repeating titles such as</p>
<p>Output :</p>
<pre><code> Daily Update (6/20) REDACTED.,
 Daily Update (9/9).,
 Daily Update (10/10).,
 RE: General/ABC Update.
 RE: General/ABC Update RELEASE IN PART BS.
 General/ABC Update.
 RE: General/ABC Article.
 RE: General/ABC Articie.
 Wrap Up for Friday, September 2017.,
 Wrap Up for Monday, January 2018.,
 Wrap Up for Monday, January 2018.
</code></pre>
<p>My question is can I apply the fuzzy matching to clean it and how? Or
Can I say keep one top record from the file and remove others?</p>
<p>What would be the best way to deal with this?</p>
<p>Here is the code that I applied:</p>
<pre><code>from itertools import chain
from nltk import sent_tokenize, word_tokenize
from nltk.tokenize.treebank import TreebankWordDetokenizer

word_detokenize = TreebankWordDetokenizer().detokenize


tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(data1)]

sent_idx_with_event= [idx for idx, sent in enumerate(tokenized_text) 
                       if 'Event' in sent or 'Subject' in sent]

window = 1 # If you want 2 sentences before and after.

list1 = []
for idx in sent_idx_with_event:
    start = max(idx - window, 1)
    end = min(idx+window, len(tokenized_text))
    result = ' '.join(word_detokenize(sent) for sent in tokenized_text[start:end])
    #result = result.split(':')[-1] 
    result = re.split(&quot;Subject:|Event:&quot;, result)[-1]
    result = re.sub(r&quot;Re&quot;,&quot;RE&quot;,result)
    result = re.sub(r&quot;R.E&quot;,&quot;RE&quot;,result)
    result = re.sub(r&quot;Fw&quot;,&quot;RE&quot;,result)
    result = re.sub(r&quot;FW&quot;,&quot;RE&quot;,result)
    result = re.sub(r&quot;Fwd&quot;,&quot;RE&quot;,result)
    result = re.sub(r&quot;REd&quot;,&quot;RE&quot;,result)
    #print(result)
    
    list1.append(result)
print(list1) 
</code></pre>
<p>and at the end I removed duplicates.</p>
<p>I am expecting one result as below:</p>
<pre><code> Daily Update (6/20) REDACTED.,
 Daily Update (9/9).,
 Daily Update (10/10).
 General/ABC Article.
 Wrap Up for Friday, September 2017.,
 Wrap Up for Monday, January 2018.,
 Wrap Up for Monday, January 2018.
</code></pre>
",Dataset Preprocessing & Handling,remove duplicate keep top one record extracted title multiple document folder one document contains multiple title one file using nltk library working fine issue contains repeating title output question apply fuzzy matching clean say keep one top record file remove others would best way deal code applied end removed duplicate expecting one result
Python KeyError problem when loading the saved model in pytorch,"<p>I am new to deep learning. I am using a transformer model for bengali language stemming process. Now, I trainied the model and test it with some data and it worked fine. So i saved the model. But when i delete the runtime and create a new runtime and load the previously trained model i get a KeyError. I don't know why I'm seeing this. I hope you guys can help.</p>
<p>Here is the notebook (when i run the model within same runtime) : <a href=""https://colab.research.google.com/drive/1TaLOluMViu8jVqCN5F0UbJyP8rZjlhE5?usp=sharing"" rel=""nofollow noreferrer"">Google colab notebook</a></p>
<p>Here you can find the dataset : <a href=""https://drive.google.com/file/d/1-HjpNum2GpgnBe-qGeqfkXjyBHitXJy2/view?usp=sharing"" rel=""nofollow noreferrer"">Dataset</a></p>
<p>Here is what happens when i try to load a trained model : <a href=""https://colab.research.google.com/drive/1q8moMBJpFrMa6EJAegSqYPn7k2wvSj90#scrollTo=AQ423XQWAgc6"" rel=""nofollow noreferrer"">colab link</a></p>
<p>Trained model link : <a href=""https://drive.google.com/file/d/18pqiBHU3MCVZIIxe9jUE4wZ4sA-mdxVe/view?usp=sharing"" rel=""nofollow noreferrer"">model link</a></p>
<p>I think the problem is when I am loading a trained model a 'PAD' token is being added with the given word. But it doesn't happen if i train the model and use it in the same runtime. It only happens when i change the runtime and load a saved model. I've tried to fix it but i can't really find the solution. I hope you guys can find it.</p>
<p>I'll be of great help for me if someone can help. Thank you in advance.</p>
",Dataset Preprocessing & Handling,python keyerror problem loading saved model pytorch new deep learning using transformer model bengali language stemming process trainied model test data worked fine saved model delete runtime create new runtime load previously trained model get keyerror know seeing hope guy help notebook run model within runtime google colab notebook find dataset dataset happens try load trained model colab link trained model link model link think problem loading trained model pad token added given word happen train model use runtime happens change runtime load saved model tried fix really find solution hope guy find great help someone help thank advance
splitting large dataset to use langchain,"<p>I am currently using langchain to make a conversational chatbot from an existing data among this data I have some excel and csv files that contain a huge datasets.
My question is how can I handle the case when I want loading this kind of data to the vector database? is good to split it row by row to maintain the meaning of the data but I am afraid to crush the database or load it as it is without spliting.</p>
",Dataset Preprocessing & Handling,splitting large dataset use langchain currently using langchain make conversational chatbot existing data among data excel csv file contain huge datasets question handle case want loading kind data vector database good split row row maintain meaning data afraid crush database load without spliting
How to match rows of a dataframe rows to another dataframe row using Python NLP techniques,"<p>I have two data frames (<code>df1</code> and <code>df2</code>).
<code>df1</code> looks something like this:</p>
<pre><code>data = [['RJ REYNOLDS VAPOR', 'Tobacco Products', 21, '4442fa51-d4b7-4d56-ad7c-d18cfd2aff84', 'Mature'],
['BEYOND MEAT', 'Packaged Meat', 30, 'e73f7957-0e65-4466-9588-795bdc5f67ac', 'Pantry'],
['BEYOND MEAT', 'Plant-Based Meat', 1584, 'd8cb60e5-b0c6-478a-971d-c6c55b17831f', 'Meat &amp; Seafood'],
['BEYOND MEAT', 'Frozen Plant-Based Meat', 313, '8e0a9431-5462-4810-9f65-68fe36adf454', 'Frozen'],
['TARGET', 'Wine', 39, 'ca1c0f4d-3efc-4978-8357-69862996f416', 'Alcohol'],
['TARGET', 'Deodorant &amp; Antiperspirant', 192, 'ca1c0f4d-3efc-4978-8357-69862996f416', 'Health &amp; Wellness'],
['DOVE', 'Bath &amp; Body', 15821, '47fae368-9b64-40be-82c9-898f953e9d66', 'Health &amp; Wellness']]

df1 = pd.DataFrame(data, columns=['BRAND', 'PRODUCT_CATEGORY', 'RECEIPTS', 'CATEGORY_ID', 'PARENT_CATEGORY'])
</code></pre>
<p>Looks something like this (the original is 9906 x 5):</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>BRAND</th>
<th>PRODUCT_CATEGORY</th>
<th>RECEIPTS</th>
<th>CATEGORY_ID</th>
<th>PARENT_CATEGORY</th>
</tr>
</thead>
<tbody>
<tr>
<td>RJ REYNOLDS VAPOR</td>
<td>Tobacco Products</td>
<td>21</td>
<td>4442fa51-d4b7-4d56-ad7c-d18cfd2aff84</td>
<td>Mature</td>
</tr>
<tr>
<td>BEYOND MEAT</td>
<td>Packaged Meat</td>
<td>30</td>
<td>e73f7957-0e65-4466-9588-795bdc5f67ac</td>
<td>Pantry</td>
</tr>
<tr>
<td>BEYOND MEAT</td>
<td>Plant-Based Meat</td>
<td>1584</td>
<td>d8cb60e5-b0c6-478a-971d-c6c55b17831f</td>
<td>Meat &amp; Seafod</td>
</tr>
</tbody>
</table>
</div>
<p><code>df2</code> looks something like this:</p>
<pre><code>offer_data = [['Beyond Meat® Plant-Based products, spend $25', None, 'BEYOND MEAT'],
['Beyond Steak™ Plant-Based seared tips, 10 ounce at Target', 'TARGET', 'BEYOND MEAT'],
['Beyond Steak™ Plant-Based seared tips, 10 ounce at H-E-B', 'H-E-B', 'BEYOND MEAT'],
['Dove Hand Wash select varieties buy 2 at TARGET', 'TARGET', 'DOVE'],
['Dove Hand Wash select varieties at Target', 'TARGET', 'DOVE']]

df2 = pd.DataFrame(offer_data, columns = ['OFFER', 'RETAILER', 'BRAND'])
</code></pre>
<p><code>df2</code> Original is 384 x 3</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>OFFER</th>
<th>RETAILER</th>
<th>BRAND</th>
</tr>
</thead>
<tbody>
<tr>
<td>Beyond Meat® Plant-Based products, spend $25</td>
<td>None</td>
<td>BEYOND MEAT</td>
</tr>
<tr>
<td>Beyond Steak™ Plant-Based seared tips, 10 ounce at Target</td>
<td>TARGET</td>
<td>BEYOND MEAT</td>
</tr>
</tbody>
</table>
</div>
<p>My goal is to add the appropriate 'PRODUCT_CATEGORY', 'PARENT_CATEGORY' and 'CATEGORY_ID' to each offer.</p>
<p>First I removed all non-alphanumeric characters from the 'OFFER&quot; column in <code>df2</code></p>
<pre><code>df2['OFFER'] = df2.OFFER.str.replace('[^\w\s]', '')
</code></pre>
<p>then I extracted keywords from the 'OFFERS' column using the yake library:</p>
<pre><code>import yake

offers = df2['OFFER'].unique()

kw_extractor = yake.KeywordExtractor()
language = 'en'
max_ngram_size = 3
deduplication_threshold = 0.9
numOfKeywords = 6
custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)


tags = []
for offer in offers:
    keywords = custom_kw_extractor.extract_keywords(offer)
    subtags = []
    for kw in keywords:
        subtags.append(kw[0])
        
    tags.append(subtags)
</code></pre>
<p>Which gave me a list of lists of keywords for each offer. I am pretty new to using NLP, so this is where I am getting stuck. I can match the 'BRAND' in <code>df1</code> to the 'BRAND' in <code>df2</code> to filter out most of the rows in <code>df1</code>. Some offers are only available at specific retailers and all offers belong to a 'PRODUCT'/'PARENT' category. My idea is to use the keywords extracted from the offers and find a closely related row match from <code>df1</code>. In the example data frame above some offers can fit into multiple categories like the 'BEYOND MEAT' categories. What is the best way to iterate through both data frames to find the appropriate 'PRODUCT_CATEGORY'/'PARENT_CATEGORY' for each offer? Or would there be an easier (more effective/accurate?) method to go about this?</p>
",Dataset Preprocessing & Handling,match row dataframe row another dataframe row using python nlp technique two data frame look something like look something like original x brand product category receipt category id parent category rj reynolds vapor tobacco product fa b ad c cfd aff mature beyond meat packaged meat e f e bdc f ac pantry beyond meat plant based meat cb e b c c c b f meat seafod look something like original x offer retailer brand beyond meat plant based product spend none beyond meat beyond steak plant based seared tip ounce target target beyond meat goal add appropriate product category parent category category id offer first removed non alphanumeric character offer column extracted keywords offer column using yake library gave list list keywords offer pretty new using nlp getting stuck match brand brand filter row offer available specific retailer offer belong product parent category idea use keywords extracted offer find closely related row match example data frame offer fit multiple category like beyond meat category best way iterate data frame find appropriate product category parent category offer would easier effective accurate method go
"dividing each sample by its maximum feature value separately, or dividing all samples by the maximum value across the entire dataset","<p>I am trying to reproduce a paper that uses the tf-idf method. During the data preprocessing, there is a step that involves feature scaling. In the original paper, it says, &quot;We restrict the words to the most common 10,000 words in the train set; then, we scale each feature to be within [-1, 1], by dividing by the maximum absolute value of the feature across the train set.&quot; So, when it mentions dividing by the maximum absolute value, is it dividing each sample by its maximum feature value separately, or is it dividing all samples by the maximum value across the entire dataset?</p>
<p>not done yet......................</p>
",Dataset Preprocessing & Handling,dividing sample maximum feature value separately dividing sample maximum value across entire dataset trying reproduce paper us tf idf method data preprocessing step involves feature scaling original paper say restrict word common word train set scale feature within dividing maximum absolute value feature across train set mention dividing maximum absolute value dividing sample maximum feature value separately dividing sample maximum value across entire dataset done yet
Create CSV file based on a dataframe with list as an elements,"<p>Consider the following example of a dataframe with lists</p>
<pre><code>import pandas as pd

# Example DataFrame with lists as elements
data = {
    'ColumnA': [['A', 'B', 'C'], ['D', 'E'], ['F'], ['G', 'H', 'I', 'J']],
    'ColumnB': [['1', '2', '3', '4'], ['5'], ['6', '7'], ['8', '9', '10']]
}

df = pd.DataFrame(data)

----------------------------
     ColumnA        ColumnB
0  [A, B, C]  [1, 2, 3, 4]
1     [D, E]           [5]
2        [F]        [6, 7]
3  [G, H, I, J]  [8, 9, 10]

</code></pre>
<p>I would like to create a csv file with the following format based only on the first row</p>
<pre><code>ColumnA,ColumnB
A,1
B,2
C,3
NaN,4
</code></pre>
<p><strong>How can I implemented?</strong></p>
",Dataset Preprocessing & Handling,create csv file based dataframe list element consider following example dataframe list would like create csv file following format based first row implemented
Text extraction from .docx file,"<p>I'm aware of libraries that are capable of this, but they aren't useful to me given constraints in their licensing structure or the cost associated.  I'd like to extract the text contents from a Word file (docx) and am close, but encountering a small issue.</p>
<p>First, I extract to a temporary directory:</p>
<pre><code>using (ZipArchive archive = ZipFile.OpenRead(_Filename))
{
    archive.ExtractToDirectory(_TempDirectory);
}
</code></pre>
<p>Then load the <code>XmlDocument</code> and find the body:</p>
<pre><code>XmlDocument doc = new XmlDocument();
doc.Load(_TempDirectory + &quot;word/document.xml&quot;);
XmlNodeList nodes = doc.GetElementsByTagName(&quot;w:body&quot;);
</code></pre>
<p>And then iterate over each of the nodes and joining the extracted text using a <code>StringBuilder</code>:</p>
<pre><code>StringBuilder sb = new StringBuilder();
foreach (XmlNode node in nodes)
{
    sb.Append(ExtractAllText(node));
}

return sb.ToString();
</code></pre>
<p>The contents of <code>ExtractAllText</code> is:</p>
<pre><code>private string ExtractAllText(XmlNode node)
{
    StringBuilder sb = new StringBuilder();

    if (node.NodeType == XmlNodeType.Text)
    {
        Console.WriteLine(&quot;Text: node name &quot; + node.Name + &quot; &quot; + node.Value);

        sb.Append(node.InnerText);

        if (node.Attributes != null)
        {
            foreach (var attr in node.Attributes) Console.WriteLine(attr.ToString());
            // displays nothing for any node!
        }
    }

    if (node.HasChildNodes)
    {
        foreach (XmlNode curr in node.ChildNodes)
        {
            sb.Append(ExtractAllText(curr));
        }
    }

    return sb.ToString();
}
</code></pre>
<p>The problem I'm facing is this: sometimes strings are split into two separate nodes and sometimes they aren't.  Further, the XML gives indication that whitespace should be preserved (e.g. <code>&lt;w:t xml:space=&quot;preserve&quot;&gt;, &lt;/w:t&gt;</code> but I cannot seem to access this via <code>node.Attributes</code>.   I'm assuming that accessing this attribute (<code>xml:space</code> and <code>preserve</code>) would allow me to understand whether or not to append whitespace to the <code>StringBuilder</code> before or after the text of that particular node.</p>
<p>This results in a Word doc that looks like this:
<a href=""https://i.sstatic.net/S96m3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/S96m3.png"" alt=""word doc"" /></a></p>
<p>Resulting in a string like this:
<code>My Name123 Vine Street, San Jose CA 95128 USA |(408) 555-1212my@email.com | www.mysite.com | linkedinobjectiveThis is my resume objective section!</code></p>
<p>Note how &quot;My Name&quot; and &quot;123 Vine Street&quot; are joined, and other similar examples exist.</p>
<p>Any input or advice would be much appreciated!</p>
<p>Edit:
Also tried with <code>DocumentFormat.OpenXml</code> and encountered similar results:</p>
<pre><code>using (WordprocessingDocument doc = WordprocessingDocument.Open(_Filename, false))
{
    body = doc.MainDocumentPart.Document.Body;
    return body.InnerText;
}
</code></pre>
<p>Shows:</p>
<pre><code>My Name123 Vine Street, San Jose CA 95128 USA  |  (408) 555-1212my@email.com | www.mysite.com | linkedinobjectiveThis is my resume objective section!
</code></pre>
<p>Edit 2: another attempt with <code>DocumentFormat.OpenXml</code> produced again the same problem:</p>
<pre><code>using (WordprocessingDocument doc = WordprocessingDocument.Open(_Filename, false))
{
    foreach (OpenXmlElement element in doc.MainDocumentPart.Document.Body.Elements())
    {
        if (element is Table)
        {
            Console.WriteLine(&quot;Table: &quot; + element.InnerText);
        }
        else if (element is Paragraph)
        {
            Console.WriteLine(&quot;Paragraph: &quot; + element.InnerText);
        }
        else if (element is Run)
        {
            Console.WriteLine(&quot;Run: &quot; + element.InnerText);
        }
    }
}
</code></pre>
<p>Results in:</p>
<pre><code>Table: My Name123 Vine Street, San Jose CA 95128 USA  |  (408) 555-1212my@email.com | www.mysite.com | linkedin
Paragraph: objective
Paragraph: This is my resume objective section!
</code></pre>
",Dataset Preprocessing & Handling,text extraction docx file aware library capable useful given constraint licensing structure cost associated like extract text content word file docx close encountering small issue first extract temporary directory load find body iterate node joining extracted text using content problem facing sometimes string split two separate node sometimes xml give indication whitespace preserved e g seem access via assuming accessing attribute would allow understand whether append whitespace text particular node result word doc look like resulting string like note name vine street joined similar example exist input advice would much appreciated edit also tried encountered similar result show edit another attempt produced problem result
pandas: create rows of sentences (with identifier) from text,"<p>I have a pandas dataframe that looks like this:</p>
<pre><code>textID1, text1, othermetadata1
textID2, text2, othermetadata2
textID3, text3, othermetadata3
</code></pre>
<p>I would like to break the texts into sentences in a new data frame that would look like this:</p>
<pre><code>textID1-001, sentence1 (of text1), othermetadata1
textID1-002, sentence2 (of text1), othermetadata1
textID2-001, sentence1 (of text2), othermetadata2
</code></pre>
<p>I know how to break texts into sentences using either the NLTK or spaCy, e.g.:</p>
<pre class=""lang-py prettyprint-override""><code>sentences = [ sent_tokenize(text) for text in texts ]
</code></pre>
<p>But pandas continues to confound me: how do I take the output and pack it back into a data frame? Moreover, how do I add numbers either to an extant column or create a new column that restarts numbering with each text -- my assumption being that I could merge the <strong>textID</strong> and <strong>sentenceID</strong> columns afterwards?</p>
",Dataset Preprocessing & Handling,panda create row sentence identifier text panda dataframe look like would like break text sentence new data frame would look like know break text sentence using either nltk spacy e g panda continues confound take output pack back data frame moreover add number either extant column create new column restarts numbering text assumption could merge textid sentenceid column afterwards
Training TFBertForSequenceClassification with custom X and Y data,"<p>I am working on a TextClassification problem, for which I am trying to traing my model on TFBertForSequenceClassification given in huggingface-transformers library.</p>

<p>I followed the example given on their <a href=""https://github.com/huggingface/transformers#quick-tour-tf-20-training-and-pytorch-interoperability"" rel=""noreferrer"">github</a> page, I am able to run the sample code with given sample data using <code>tensorflow_datasets.load('glue/mrpc')</code>.
However, I am unable to find an example on how to load my own custom data and pass it in 
<code>model.fit(train_dataset, epochs=2, steps_per_epoch=115, validation_data=valid_dataset, validation_steps=7)</code>. </p>

<p>How can I define my own X, do tokenization of my X and prepare train_dataset with my X and Y. Where X represents my input text and Y represents classification category of given X.</p>

<p>Sample Training dataframe : </p>

<pre><code>    text    category_index
0   Assorted Print Joggers - Pack of 2 ,/ Gray Pri...   0
1   ""Buckle"" ( Matt ) for 35 mm Width Belt  0
2   (Gagam 07) Barcelona Football Jersey Home 17 1...   2
3   (Pack of 3 Pair) Flocklined Reusable Rubber Ha...   1
4   (Summer special Offer)Firststep new born baby ...   0
</code></pre>
",Dataset Preprocessing & Handling,training tfbertforsequenceclassification custom x data working textclassification problem trying traing model tfbertforsequenceclassification given huggingface transformer library followed example given github page able run sample code given sample data using however unable find example load custom data pas define x tokenization x prepare train dataset x x represents input text represents classification category given x sample training dataframe
Make Whisper use the LAST 30 sec chunk (and not the first),"<p>According to Whisper, the notion is as follows:</p>
<blockquote>
<p>Internally, the transcribe() method reads the entire file and processes the audio with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions on each window.</p>
</blockquote>
<p>It is mentioned that only the first 30-sec window is considered for further analysis (and thus language allocation). However, what if I would like to take into account (for the language allocation task) only the last 30-sec window? What could be the possible solution for the task?</p>
",Dataset Preprocessing & Handling,make whisper use last sec chunk first according whisper notion follows internally transcribe method read entire file process audio sliding second window performing autoregressive sequence sequence prediction window mentioned first sec window considered analysis thus language allocation however would like take account language allocation task last sec window could possible solution task
What is the difference between BertModel vs Bertforsequenceclassification?,"<p>I have a data frame with 2 columns(a column of text data and a target feature), and would like to train the data for classification. . I was curious what is the main difference between these two?</p>
<p>So far, I have been using Bertforsequenceclassification, but I saw that mostly use BertModel for this purpose in Kaggle competition etc</p>
",Dataset Preprocessing & Handling,difference bertmodel v bertforsequenceclassification data frame column column text data target feature would like train data classification wa curious main difference two far using bertforsequenceclassification saw mostly use bertmodel purpose kaggle competition etc
Dropping non-English text with langdetect,"<p>I am trying to use langdetect to drop all the languages which are not English in my text.</p>
<pre><code>def det(x):
    try:
        language = detect(x)
    except:
        language = 'Other'
    return language

df['langue'] = df['Tweet'].apply(det)
filtered_for_english = df.loc[df['langue'] == 'en']
</code></pre>
<p>The above code is what I have tried. It detects the language used in each tweet but does not drop the non-English tweets from my data frame.</p>
<p>The resulting data frame:</p>
<pre><code>0        es
1        es
2        es
3        en
4        en
         ..
14272    en
14273    en
14274    en
14275    it
14276    en
Name: langue, Length: 14277, dtype: object
</code></pre>
<p>How can I fix this code?</p>
",Dataset Preprocessing & Handling,dropping non english text langdetect trying use langdetect drop language english text code tried detects language used tweet doe drop non english tweet data frame resulting data frame fix code
Submit NLP classification model to Kaggle,"<p>I am having a hard time converting my RNN/NLP model to Kaggle competition in csv file. I am a total beginner in file converting. I am already at the point where I am comparing my test model to validation data set, but I am not sure how to convert those to predict in pandas, tabular, csv format. Here is the link to the competition: <a href=""https://www.kaggle.com/competitions/learn-ai-bbc/data"" rel=""nofollow noreferrer"">https://www.kaggle.com/competitions/learn-ai-bbc/data</a></p>
<p>Loading the file part:</p>
<pre><code>with open(&quot;./bbc-text.csv&quot;, 'r') as csvfile:
    print(f&quot;First line (header) looks like this:\n\n{csvfile.readline()}&quot;)
    print(f&quot;Each data point looks like this:\n\n{csvfile.readline()}&quot;)
</code></pre>
<p>This is my model:</p>
<pre><code>def create_model(num_words, embedding_dim, maxlen):
    
    tf.random.set_seed(123)
    
    
    model = tf.keras.Sequential([ 
        tf.keras.layers.Embedding(num_words, embedding_dim, input_length=maxlen),
        tf.keras.layers.GlobalAveragePooling1D(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(5, activation='softmax')
    ])
    
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy']) 

    return model
</code></pre>
<p>testing and validating part:</p>
<pre><code>model = create_model(NUM_WORDS, EMBEDDING_DIM, MAXLEN)

history = model.fit(train_padded_seq, train_label_seq, epochs=30, validation_data=(val_padded_seq, val_label_seq))
</code></pre>
<p>I tried adding the test.csv file to <code>model.predict</code>, but it does not work.</p>
",Dataset Preprocessing & Handling,submit nlp classification model kaggle hard time converting rnn nlp model kaggle competition csv file total beginner file converting already point comparing test model validation data set sure convert predict panda tabular csv format link competition loading file part model testing validating part tried adding test csv file doe work
Reduce the computation and storage burden for row-wise comparison in Python,"<p>Consider we have the following data frame:</p>
<pre><code>      message
0       ABC
1       abc
2       cba
3      abcd
4      dcsa
5      adcd
6      abcd
7       cba                   
</code></pre>
<p>I want to perform row-wise comparisons, then finally group the data using the similarity vector for some metric calculation.</p>
<p>For now, I am doing the following:</p>
<pre><code>     message      similarity
0       ABC  [1, 0, 0, 0, 1, 0, 1, 0]
1       abc  [1, 0, 1, 0, 1, 0, 1, 0]
2       cba  [1, 0, 0, 0, 1, 0, 1, 0]
3      abcd  [1, 0, 1, 0, 1, 0, 1, 0]
4      dcsa  [1, 0, 0, 0, 1, 0, 1, 0]
5      adcd  [1, 0, 0, 0, 1, 0, 1, 0]
6      abcd  [1, 1, 0, 0, 1, 0, 1, 0]
7       cba  [1, 1, 0, 0, 1, 0, 1, 0]
</code></pre>
<p>From the above, it is clear that I am doing a brute-force job. That being said, starting from row 1, I compare it with all the rows (including itself), and if they have sufficient similarity, then I store it as <code>1</code>, otherwise it is <code>0</code>, and thus, the vector size is the number of rows (<code>N</code>), and it is a <code>O(N^2)</code> problem. For a smaller dataset, this is acceptable, but it does not scale for bigger datasets. For example, let us imagine that there are <code>1M</code> rows, and for each row, there is a <code>1M</code>-sized similarity vector—this is really crazy.</p>
<p>Therefore, I am here for input regarding two questions:</p>
<ol>
<li><p>Can we reduce the <code>O(N^2)</code> complexity? My current thought is we can order the <code>message</code> column and perform comparison on a reduced basis. e.g., for row1, comparing it with row2 till rowN, and for row2, comparing it with row3 till rowN, etc. But the similarity vector will vary on its length among all the rows.</p>
</li>
<li><p>Is there a better way to store the similarity vector? We need it for grouping purpose, and thus, such a vector is unavoidable. Can we use another method to handle it?</p>
</li>
</ol>
<p>A minimal reproducible example is provided below to play with.</p>
<pre><code>import pandas as pd
from difflib import SequenceMatcher

df = pd.DataFrame({'message': ['ABC', 'ABCD', 'DCBA', 'abcde', 'CBDA', 'abcde', 'edcba']})

def similarity_score(s1, s2):
    coef = SequenceMatcher(None, s1, s2).ratio()
    if coef &gt;= 0.75:
        return 1
    else:
        return 0

def similarity(x, df):
    sim_score = []
    for i in df['message']:
        sim_score.append(similarity_score(x, i))
    return sim_score

df['similarity'] = df['message'].apply(lambda x: similarity(x, df)).astype(str)
</code></pre>
<p>Below I'd like to illustrate my motivation for the questions above using a reproducible example. For each message, there is an count (<code>cnt</code>). After the similarity vector (<code>similarity</code>) is created, I used it as a grouping variable to categorize the message into different groups (<code>group</code>), and finally, sum up the count per group.</p>
<pre><code>import pandas as pd
from difflib import SequenceMatcher

df = pd.DataFrame({'message': ['ABC','abc','cba','abcd','dcsa','adcd','abcd','cba'], 
                  'cnt': [1, 2, 3, 4, 5, 6, 7, 8]})

def similarity_score(s1, s2):
    coef = SequenceMatcher(None, s1, s2).ratio()
    if coef &gt;= 0.80:
        return 1
    else:
        return 0

def similarity(x,df):
    sim_score = []
    for i in df['message']:
        sim_score.append(similarity_score(x, i))
    return sim_score

df['similarity'] = df['message'].apply(lambda x: similarity(x, df)).astype(str)
df[&quot;group&quot;] = pd.factorize(df[&quot;similarity&quot;].astype(str))[0] + 1
print(df)
</code></pre>
<p>The output is as follows:</p>
<pre><code>   message  cnt         similarity          group
0     ABC    1  [1, 0, 0, 0, 0, 0, 0, 0]      1
1     abc    2  [0, 1, 0, 1, 0, 0, 1, 0]      2
2     cba    3  [0, 0, 1, 0, 0, 0, 0, 1]      3
3    abcd    4  [0, 1, 0, 1, 0, 0, 1, 0]      2
4    dcsa    5  [0, 0, 0, 0, 1, 0, 0, 0]      4
5    adcd    6  [0, 0, 0, 0, 0, 1, 0, 0]      5
6    abcd    7  [0, 1, 0, 1, 0, 0, 1, 0]      2
7     cba    8  [0, 0, 1, 0, 0, 0, 0, 1]      3
</code></pre>
<p>Finally, I sum up the count by <code>group</code> as following (that is why at the beginning of the question, I said &quot;group the data using the similarity vector for some metric calculation&quot;.</p>
<pre><code>df_final = df.groupby(&quot;group&quot;).sum(&quot;cnt&quot;)
print(df_final)
</code></pre>
<p>The output is as follows:</p>
<pre><code>group   cnt    
1        1
2       13
3       11
4        5
5        6
</code></pre>
<p>Hopefully the added example is clear enough. Thank you.</p>
",Dataset Preprocessing & Handling,reduce computation storage burden row wise comparison python consider following data frame want perform row wise comparison finally group data using similarity vector metric calculation following clear brute force job said starting row compare row including sufficient similarity store otherwise thus vector size number row problem smaller dataset acceptable doe scale bigger datasets example let u imagine row row sized similarity vector really crazy therefore input regarding two question reduce complexity current thought order column perform comparison reduced basis e g row comparing row till rown row comparing row till rown etc similarity vector vary length among row better way store similarity vector need grouping purpose thus vector unavoidable use another method handle minimal reproducible example provided play like illustrate motivation question using reproducible example message count similarity vector created used grouping variable categorize message different group finally sum count per group output follows finally sum count following beginning question said group data using similarity vector metric calculation output follows hopefully added example clear enough thank
Categorize rows per their similarity in Python,"<p>I am here to look for input for a data manipulation problem related to natural language processing.</p>
<p>To make life easier, I am using a mock dataset posted several years ago from <a href=""https://stackoverflow.com/questions/47159996/how-to-group-text-data-based-on-document-similarity"">How to group text data based on document similarity?</a>.</p>
<pre><code>import pandas as pd
from difflib import SequenceMatcher

df = pd.DataFrame({'Questions': ['What are you doing?','What are you doing tonight?','What are you doing now?','What is your name?','What is your nick name?','What is your full name?','Shall we meet?',
                             'How are you doing?' ]})

def similarity_score(s1, s2):
    return SequenceMatcher(None, s1, s2).ratio()

def similarity(x,df):
    sim_score = []
    for i in df['Questions']:
        sim_score.append(similarity_score(x,i))
    return sim_score

df['similarity'] = df['Questions'].apply(lambda x : similarity(x, df)).astype(str)
print(df)
</code></pre>
<p>The output is as following</p>
<pre><code>Questions  \
0          What are you doing?   
1  What are you doing tonight?   
2      What are you doing now?   
3           What is your name?   
4      What is your nick name?   
5      What is your full name?   
6               Shall we meet?   
7           How are you doing?   

                                          similarity  
0  [1.0, 0.8260869565217391, 0.9047619047619048, ...  
1  [0.8260869565217391, 1.0, 0.84, 0.533333333333...  
2  [0.9047619047619048, 0.84, 1.0, 0.585365853658...  
3  [0.6486486486486487, 0.5333333333333333, 0.585...  
4  [0.5714285714285714, 0.52, 0.5217391304347826,...  
5  [0.5714285714285714, 0.52, 0.5652173913043478,...  
6  [0.36363636363636365, 0.34146341463414637, 0.3...  
7  [0.8108108108108109, 0.6666666666666666, 0.731...  
</code></pre>
<p>The logic is that I go through each row in the data frame to compare it to all over rows (including itself) in order to compute their similarity. I then store the similarity score as a list in another column called &quot;similarity&quot;.</p>
<p>Next, I want to categorize the questions in the first column. If the similarity score &gt; 0.9, then those rows should be assigned to the same group. How can I achieve this?</p>
",Dataset Preprocessing & Handling,categorize row per similarity python look input data manipulation problem related natural language processing make life easier using mock dataset posted several year ago href group text data based document similarity output following logic go row data frame compare row including order compute similarity store similarity score list another column called similarity next want categorize question first column similarity score row assigned group achieve
Identify abbreviations in a string column,"<p>Given the following data frame for instance (mind you the original data for this column is a dtype('0'))</p>
<pre><code>df = pd.DataFrame({'product_description': [&quot;CUTLERY HVY DUTY FORKS&quot;, &quot;XYZ DISP LQD SOAP&quot;, &quot;ABCD FOOD STRG CNTNR&quot;]})
</code></pre>
<p>How can I effectively identify and separate the abbreviations and produce a result like</p>
<pre><code>product_description            abbreviations
0  CUTLERY HVY DUTY FORKS        [HVY]

1  XYZ DISP LQD SOAP             [XYZ,DISP,LQD]

2  ABCD FOOD STRG CNTNR          [ABCD,STRG,CNTNR]
</code></pre>
<p>So i convert the abbreviations into full words.</p>
<p>i have tried this:</p>
<pre><code>import pandas as pd
import re

df = pd.DataFrame({'product_description': [&quot;CUTLERY HVY DUTY FORKS&quot;, &quot;XYZ DISP LQD SOAP&quot;, &quot;ABCD FOOD STRG CNTNR&quot;]})

def extract_abbreviations(description):
    abbreviation_pattern = r'\b[A-Z]{2,}(?![a-z])'  # Updated regular expression pattern to match abbreviations
    abbreviations = re.findall(abbreviation_pattern, description)
    return abbreviations

df['abbreviations'] = df['product_description'].apply(extract_abbreviations)
print(df)
</code></pre>
<p>but this is what i get :</p>
<pre><code>product_description            abbreviations
0  CUTLERY HVY DUTY FORKS        [CUTLERY,HVY,DUTY,FORKS]

1  XYZ DISP LQD SOAP             [XYZ,DISP,LQD,SOAP]

2  ABCD FOOD STRG CNTNR          [ABCD,FOOD,STRG,CNTNR]
</code></pre>
<p>Your help is much appreciated.
Thank you</p>
",Dataset Preprocessing & Handling,identify abbreviation string column given following data frame instance mind original data column dtype effectively identify separate abbreviation produce result like convert abbreviation full word tried get help much appreciated thank
How to count specific keywords in a transcript with a condition,"<p>I got a big data frame with a &quot;Transcript&quot; column between an bot and a user.
I need to count how many times in the transcript the user is asking for an agent/representative before giving the bot a chance.</p>
<p>The transcript looks as follow but longer:</p>
<pre><code>&quot;User : Order status.\nBot : Your order status is your orders tab. \nUser : representative.&quot;

&quot;User : Agent please.\nBot : Waiting time is longer than usual.&quot;
</code></pre>
<p>I tried to use Regular Expression:</p>
<pre><code>df[&quot;Transcript&quot;] = df[&quot;Transcript&quot;].str.lower()
df.loc[df[&quot;Transcript&quot;].str.contains('agent|representative'),:]
</code></pre>
<p>But it will just output observations with those keywords.
How can I output a number that count when user first input is agent/representative?</p>
",Dataset Preprocessing & Handling,count specific keywords transcript condition got big data frame transcript column bot user need count many time transcript user asking agent representative giving bot chance transcript look follow longer tried use regular expression output observation keywords output number count user first input agent representative
Can&#39;t load from AutoTokenizer.from_pretrained - TypeError: duplicate file name (sentencepiece_model.proto),"<p>I'm trying to load tokenizer and seq2seq model from pretrained models.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(&quot;ozcangundes/mt5-small-turkish-summarization&quot;)

model = AutoModelForSeq2SeqLM.from_pretrained(&quot;ozcangundes/mt5-small-turkish-summarization&quot;)
</code></pre>
<p>But I got this error.</p>
<pre><code>File ~/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py:1028, in FileDescriptor.__new__(cls, name, package, options, serialized_options, serialized_pb, dependencies, public_dependencies, syntax, pool, create_key)
   1026     raise RuntimeError('Please link in cpp generated lib for %s' % (name))
   1027 elif serialized_pb:
-&gt; 1028   return _message.default_pool.AddSerializedFile(serialized_pb)
   1029 else:
   1030   return super(FileDescriptor, cls).__new__(cls)

    TypeError: Couldn't build proto file into descriptor pool: duplicate file name (sentencepiece_model.proto)
</code></pre>
<p>I tried updating or downgrading the protobuf version. But I couldn't fix</p>
",Dataset Preprocessing & Handling,load autotokenizer pretrained typeerror duplicate file name sentencepiece model proto trying load tokenizer seq seq model pretrained model got error tried updating downgrading protobuf version fix
How to implement this model using BiLSTM with attention?,"<p>I have a dataset,the each sample in dataset is &lt;Question,Document,Answer&gt; ,Answer mayebe in document or not, Now,I want implement model using BiLSTM with attention.</p>
<p>The model have two input layers ,the one layer is question layer and the other layer is documnet layer。</p>
<p>the model output layer should predict whether each word in the Document is the start and end of the answer or not</p>
<p>For example, given a Question as input:
“who discovered neptune the planet?”</p>
<p>Given Document as input :
“With a prediction by Urbain Le Verrier , telescopic observations confirming the existence of a major planet were made on the night of September 23–24, 1846”</p>
<p>The Answer for Question is “Urbain Le Verrier”</p>
<pre><code>the output layer predict word “with” is not start word or end of answer
the output layer predict word “a” is not start word or end of answer
the output layer predict word “prediction” is not start word or end of answer
the output layer predict word “by” is not start word or end of answer
the output layer predict word “Urbain” is start word of answer
the output layer predict word “Le” is not start word or end of answer
the output layer predict word “Verrier” is end word of answer
</code></pre>
<p>How to implement this model using BiLSTM with attention by pytorch?</p>
",Dataset Preprocessing & Handling,implement model using bilstm attention dataset sample dataset question document answer answer mayebe document want implement model using bilstm attention model two input layer one layer question layer layer documnet layer model output layer predict whether word document start end answer example given question input discovered neptune planet given document input prediction urbain le verrier telescopic observation confirming existence major planet made night september answer question urbain le verrier implement model using bilstm attention pytorch
How to load a trained model from BERTSUM keep facing ModuleNotFoundError: No module named &#39;models.optimizers&#39;,"<p>I trained a model using Bertsum now i want to use it for my task but i keep facing this issue
Bertsum github url:<a href=""https://github.com/nlpyang/BertSum"" rel=""nofollow noreferrer"">https://github.com/nlpyang/BertSum</a></p>
<pre><code>import torch

# specify the path to the saved checkpoint file
checkpoint_path = '~/Desktop/fyp/models/bert_transformer/model_step_44000.pt'
# load the checkpoint file
checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))
</code></pre>
<p>can't load the model that trained using BERTSUM
This is what i find in the source code of how it is saved</p>
<pre><code>    def _save(self, step):
        real_model = self.model
        # real_generator = (self.generator.module
        #                   if isinstance(self.generator, torch.nn.DataParallel)
        #                   else self.generator)

        model_state_dict = real_model.state_dict()
        # generator_state_dict = real_generator.state_dict()
        checkpoint = {
            'model': model_state_dict,
            # 'generator': generator_state_dict,
            'opt': self.args,
            'optim': self.optim,
        }
        checkpoint_path = os.path.join(self.args.model_path, 'model_step_%d.pt' % step)
        logger.info(&quot;Saving checkpoint %s&quot; % checkpoint_path)
        # checkpoint_path = '%s_step_%d.pt' % (FLAGS.model_path, step)
        if (not os.path.exists(checkpoint_path)):
            torch.save(checkpoint, checkpoint_path)
            return checkpoint, checkpoint_path
</code></pre>
<p>can anyone help me ?</p>
<p>Is it because it is using it own optimizers in one of the file call optimizers.py in his file ?
If so how to solve it ?</p>
",Dataset Preprocessing & Handling,load trained model bertsum keep facing modulenotfounderror module named model optimizers trained model using bertsum want use task keep facing issue bertsum github url load model trained using bertsum find source code saved anyone help using optimizers one file call optimizers py file solve
How to correctly load and merge finetuned LLaMA models in different formats?,"<p>I am new to NLP and currently exploring the LLaMA model. I understand that there are different formats for this model - the original format and the Hugging Face format. I have fine-tuned the LLaMA model on my dataset using this tool <a href=""https://github.com/lxe/llama-peft-tuner"" rel=""nofollow noreferrer"">https://github.com/lxe/llama-peft-tuner</a>, and it saves the models in a certain way (see below):</p>
<pre class=""lang-bash prettyprint-override""><code>$ ll llama-peft-tuner/models/csco-llama-7b-peft/
total 16456
drwxrwxr-x 8 lachlan lachlan     4096 May 10 10:42 ./
drwxrwxr-x 5 lachlan lachlan     4096 May 10 10:06 ../
drwxrwxr-x 2 lachlan lachlan     4096 May 10 10:21 checkpoint-1000/
drwxrwxr-x 2 lachlan lachlan     4096 May 10 10:28 checkpoint-1500/
drwxrwxr-x 2 lachlan lachlan     4096 May 10 10:35 checkpoint-2000/
drwxrwxr-x 2 lachlan lachlan     4096 May 10 10:42 checkpoint-2500/
drwxrwxr-x 2 lachlan lachlan     4096 May 10 10:13 checkpoint-500/
drwxrwxr-x 2 lachlan lachlan     4096 May 10 10:42 model-final/
-rw-rw-r-- 1 lachlan lachlan 16814911 May 10 10:42 params.p

$ ll llama-peft-tuner/models/csco-llama-7b-peft/checkpoint-2500/
total 7178936
drwxrwxr-x 2 lachlan lachlan       4096 May 10 10:42 ./
drwxrwxr-x 8 lachlan lachlan       4096 May 10 10:42 ../
-rw-rw-r-- 1 lachlan lachlan   33629893 May 10 10:42 optimizer.pt
-rw-rw-r-- 1 lachlan lachlan 7317523229 May 10 10:42 pytorch_model.bin
-rw-rw-r-- 1 lachlan lachlan      14575 May 10 10:42 rng_state.pth
-rw-rw-r-- 1 lachlan lachlan        557 May 10 10:42 scaler.pt
-rw-rw-r-- 1 lachlan lachlan        627 May 10 10:42 scheduler.pt
-rw-rw-r-- 1 lachlan lachlan      28855 May 10 10:42 trainer_state.json
-rw-rw-r-- 1 lachlan lachlan       3899 May 10 10:42 training_args.bin
</code></pre>
<p>I am not quite sure about the relationship between <code>pytorch_model.bin</code>, the original model, and <code>adapter_model.bin</code>. I suppose <code>pytorch_model.bin</code> is in the Hugging Face format. Now, I want to create a .pth model that I can load in <a href=""https://github.com/juncongmoo/pyllama/tree/main/apps/gradio"" rel=""nofollow noreferrer"">https://github.com/juncongmoo/pyllama/tree/main/apps/gradio</a>.</p>
<p>I followed the manual conversion guide at <a href=""https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Manual-Conversion"" rel=""nofollow noreferrer"">https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Manual-Conversion</a> to convert the Hugging Face format into Hugging Face format (.bin) or PyTorch format (.pth). I tried treating <code>pytorch_model.bin</code> as the Hugging Face format and modified the code to ignore the LoRA, but I couldn't achieve the desired result.The fine-tuning repository mentioned below provided a way to load the trained model by combining the original model and the learned parameters. I tried to adapt this approach into <a href=""https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/merge_llama_with_chinese_lora.py"" rel=""nofollow noreferrer"">https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/merge_llama_with_chinese_lora.py</a> and tried different combinations, but the result either doesn't incorporate the trained parameters or generates meaningless outputs.</p>
<p>Can someone help me understand how to correctly load and merge these models? Any help would be greatly appreciated. Thank you.</p>
",Dataset Preprocessing & Handling,correctly load merge finetuned llama model different format new nlp currently exploring llama model understand different format model original format hugging face format fine tuned llama model dataset using tool save model certain way see quite sure relationship original model suppose hugging face format want create pth model load followed manual conversion guide convert hugging face format hugging face format bin pytorch format pth tried treating hugging face format modified code ignore lora achieve desired result fine tuning repository mentioned provided way load trained model combining original model learned parameter tried adapt approach tried different combination result either incorporate trained parameter generates meaningless output someone help understand correctly load merge model help would greatly appreciated thank
pandas data frame to make words as columns in pandas,"<p>How to make unique words which are in a list format in every row of a dataframe a column</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>name</th>
<th>desc</th>
</tr>
</thead>
<tbody>
<tr>
<td>x</td>
<td>['red','white','yellow']</td>
</tr>
<tr>
<td>y</td>
<td>['red','black','blue']</td>
</tr>
</tbody>
</table>
</div>
<p>as</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>name</th>
<th>red</th>
<th>white</th>
<th>yellow</th>
<th>black</th>
<th>blue</th>
</tr>
</thead>
<tbody>
<tr>
<td>x</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
<td></td>
<td></td>
</tr>
<tr>
<td>y</td>
<td>yes</td>
<td></td>
<td></td>
<td>yes</td>
<td>yes</td>
</tr>
</tbody>
</table>
</div>",Dataset Preprocessing & Handling,panda data frame make word column panda make unique word list format every row dataframe column name desc x red white yellow red black blue name red white yellow black blue x yes yes yes yes yes yes
Extraction of tables and their preceding words,"<p>I have a list of words(it is capital sensitive) and each word appears in a text which sometimes  is followed by a table. I extracted text using pypdf2.</p>
<p>How to pull pairs of each table and given word which appears in text above each table. I want to use tabula and put each pairs of words and table into a dictionary such as key is the given word and value is the related table ? I dont know how to implement it. any help would be so much appreciated.</p>
<p>Text is example of one page of pdf.</p>
<blockquote>
<p>text ='''id: Res ID\nRes ID\nBased upon  3,302 valid cases out of 3,30
total .\n•Mean:
54361.66\n•Minimum: 10005.00\n•Maximum: 99992.00\n•Standard Deviation:
25723.74\nLocation: 1-5 (width: 5; decimal: 0)\nVariable Type:  numeric \nVIS: Study \nSwan  visit \nValue Label
Unweighted\nFrequency%\n00- 3302 100.0 %\n Total 3,302 100%\nBased
upon  3,302 valid cases out of 3,302 total cases.\nLocation: 6-7
(width: 2; decimal: 0)\nVariable Type:  character \nINT: Interview
\nDate form \nValue Label Unweighted\nFrequency%\n0- 3302 100.0 %\n
Total 3,302 100%\nBased upon 3,302 valid  cases out of 3,302 total
cases.\n•Mean:
0.00\n•Median: 0.00\n•Mode: 0.00\n•Minimum:
0.00\n•Maximum: 0.00\n•Standard Deviation:
0.00\nLocation: 8-8 (width: 1; decimal: 0)\nVariable Type:  numeric'''</p>
</blockquote>
<pre><code>
    import tabula
        lis= [&quot;id&quot;,&quot;VIS&quot;,&quot;INT&quot;]
    
        results_dict = {}
        for word in lis:
            if word in text:
                try:
                    table = tabula.read_pdf(&quot;df.pdf&quot;, pages='all', multiple_tables=True)            
                    results_dict[word] = table
                except:
                    pass
        
        print(results_dict)

</code></pre>
<p>Update :</p>
<p>Expected output :</p>
<pre><code>    dic = {id:&quot;no_table&quot;,VIS:Value  Label  Unweighted/Frequency   %
                             00     -            3302            100.0 %
                                    Total        3,302           100%
                         ,INT:Value Label  Unweighted/Frequency   %
                             00     -            3302            100.0 %
                                    Total        3,302           100%  

  
    
</code></pre>
<p>the value should be a table that is converted to a data frame.
how should value look:</p>
<pre><code>data = {'Value':  ['0.0', 'NaN'],
        'Label': ['-', 'Total'],
        'Unweighted\rFrequency':  ['3302', '3,302'],
        '%': ['100.0 %', '100%'],
        }

df = pd.DataFrame(data)

df
</code></pre>
",Dataset Preprocessing & Handling,extraction table preceding word list word capital sensitive word appears text sometimes followed table extracted text using pypdf pull pair table given word appears text table want use tabula put pair word table dictionary key given word value related table dont know implement help would much appreciated text example one page pdf text id id nres id nbased upon valid case total n mean n minimum n maximum n standard deviation nlocation width decimal nvariable type numeric nvis study nswan visit nvalue label unweighted nfrequency n n total nbased upon valid case total case nlocation width decimal nvariable type character nint interview ndate form nvalue label unweighted nfrequency n n total nbased upon valid case total case n mean n median n mode n minimum n maximum n standard deviation nlocation width decimal nvariable type numeric update expected output value table converted data frame value look
Using a custom trained word2vec model,"<p>I have a corpus is the form of a CSV file or text file. I want to use it to train a word2vec model. Then I want to use the trained model to vectorize a CSV file that contains class and class description, ie each class will correspond to a vector. How to do it using python?</p>
<p>I have so far looked into training a custom word2vec model, but all of them are giving the vector of a single word as output. I want to pass a description and get a single vector as output</p>
",Dataset Preprocessing & Handling,using custom trained word vec model corpus form csv file text file want use train word vec model want use trained model vectorize csv file contains class class description ie class correspond vector using python far looked training custom word vec model giving vector single word output want pas description get single vector output
Read each table of pdf and assign a search word that appears at top of table to it,"<p>I am able to read each table of pdf using tabula library and the output of table is as follows. Imagine in the page 6th we only have 2 tables.</p>
<p>The code for execrating tables:</p>
<pre><code>mytable = tabula.io.read_pdf(&quot;x.pdf&quot;, pages = 6, multiple_tables = True)  
</code></pre>
<pre><code>mytable[0]

    Val     Label   %
0   0.0     nan     100.0 %
1   NaN     Total   100%


</code></pre>
<pre><code>mytable[1]

    Val     Label   %
0   0.0     a       100.0 %
0   4.0     b       6 %
1   NaN     Total   100%
</code></pre>
<p>I can extract the whole text of page 6 with code as follows :</p>
<pre><code>from PyPDF2 import PdfFileReader
def text_extractor(path):
    with open(path, 'rb') as f:
        reader = PdfReader (f)
        page = reader.pages[5]
        print(page)
        print('Page type: {}'.format(str(type(page))))
        text = page.extract_text()
        print(text)
    return text

path = &quot;x.pdf&quot;
c= text_extractor(path)
</code></pre>
<p>I also have some keyword that appears at the top of each table in pdf file. one keyword at the top of each table randomly.</p>
<p>I already have a list of keywords as</p>
<pre><code>search keywords = [&quot;id&quot;,&quot;type&quot;, &quot;name&quot;, etc]
</code></pre>
<p>The problem is I want to search and assign each of this keyword that appears at the top of table to the table itself, the format really doesn't matter can be dictionary or creating a column in the table. I want to know how can I assign this search key that appears at top of table to table itself?</p>
",Dataset Preprocessing & Handling,read table pdf assign search word appears top table able read table pdf using tabula library output table follows imagine page th table code execrating table extract whole text page code follows also keyword appears top table pdf file one keyword top table randomly already list keywords problem want search assign keyword appears top table table format really matter dictionary creating column table want know assign search key appears top table table
How to pull specific feature from table corresponding to its ID in a text,"<p>I have some pdf files with many pages. I read the file into a dictionary, with file name as a key and its text as value.</p>
<p>After each of some given words (search key: ID1 and type in this example) there is a table with <strong>lable</strong>.</p>
<p>I need to extract the information of label that can be separated by &quot;,&quot; in list format and insert them into a column of data frame. how should I develop my code ?</p>
<p><strong>Final out put should looks like :</strong></p>
<pre><code>    ID       label
0   ID1     [yes, no]
1   type    [f1, f2]
</code></pre>
<p><strong>Sample of  raw table</strong></p>
<p>ID1: drug ever used</p>
<p>aaaa bbb</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>label</th>
<th>header</th>
</tr>
</thead>
<tbody>
<tr>
<td>yes</td>
<td>a1</td>
</tr>
<tr>
<td>no</td>
<td>b1</td>
</tr>
</tbody>
</table>
</div>
<p>type: pill ever used</p>
<p>cccc</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>label</th>
<th>header</th>
</tr>
</thead>
<tbody>
<tr>
<td>f1</td>
<td>a1</td>
</tr>
<tr>
<td>f3</td>
<td>b1</td>
</tr>
</tbody>
</table>
</div>
<pre><code>
   

#text : is the content of pdf that I converted to a text.


def read_pdfs(dictionary):
    dfs_names_text_dic = {}
    for pdf_file, name in dictionary.items():
        pdfFileObj = open(pdf_file, 'rb')
        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)
        output = []
        for i in range(pdfReader.numPages):
            pageObj = pdfReader.getPage(i)
            output.append(pageObj.extractText())
        dfs_names_text_dic[name] = output
        pdfFileObj.close()
    return dfs_names_text_dic

out put example:
dic = {pdf_name1 : text, pdf_name2:text }


</code></pre>
",Dataset Preprocessing & Handling,pull specific feature table corresponding id text pdf file many page read file dictionary file name key text value given word search key id type example table lable need extract information label separated list format insert column data frame develop code final put look like sample raw table id drug ever used aaaa bbb label header yes b type pill ever used cccc label header f f b
Read text and their corresponding page numbers from the .docx in R,"<p>How can I read a Microsoft .docx file in R and get the text as one field and page number as another?</p>

<p>From the readtext R libraries, I can read the text, but wondering if you know how to get the page number as well? </p>

<pre><code>install.packages(""readtext"")

library(readtext)

doc &lt;- readtext(system.file(""examples/realworld.docx"", package=""docxtractr""))
</code></pre>

<p>So the desired output should be</p>

<pre><code>text                page_number
text from page 1     1
text from page 2     2
</code></pre>

<p>Please advise.</p>
",Dataset Preprocessing & Handling,read text corresponding page number docx r read microsoft docx file r get text one field page number another readtext r library read text wondering know get page number well desired output please advise
QnA model using Bert,"<p>I'm trying to build a bert model containing document as input. As bert's limitation is 512 tokens, it's unable to give accurate answer. Now, I'm trying to find NLP model/way/algorithm which should help bert model to find the correct answer.</p>
<p>I tried with document as input and was expecting accurate answer as it was giving with small passages.</p>
",Dataset Preprocessing & Handling,qna model using bert trying build bert model containing document input bert limitation token unable give accurate answer trying find nlp model way algorithm help bert model find correct answer tried document input wa expecting accurate answer wa giving small passage
How to load a WordLevel Tokenizer trained with tokenizers in transformers,"<p>I would like to use WordLevel encoding method to establish my own wordlists, and it saves the model with a vocab.json under the my_word2_token folder. The code is below and it works.</p>
<pre><code>import pandas as pd
from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer
from transformers import BertTokenizerFast
from tokenizers.pre_tokenizers import Whitespace
import os
tokenizer = Tokenizer(models.WordLevel())
tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()
special_tokens = [&quot;[UNK]&quot;, &quot;[PAD]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[MASK]&quot;]
trainer = trainers.WordLevelTrainer(vocab_size=1400, special_tokens=special_tokens)

            
tokenizer.train(files=[&quot;./data/material.txt&quot;], trainer=trainer)
# 最终得到该语料的Tonkernize，查看下词汇大小
print(&quot;Trained vocab size: {}&quot;.format(tokenizer.get_vocab_size()))
# 保存训练的tokenizer
tokenizer.model.save('./my_word2_token/')
</code></pre>
<p>But when I try to use BartTokenizer or BertTokenizer to load my <code>vocab.json</code>, it does not work. Especially, in terms of BertTokenizer, the tokenized result are all [UNK], as below.
<img src=""https://i.sstatic.net/LdD8F.png"" alt=""UNK pic"" />
As for BartTokenizer, it errors as</p>
<blockquote>
<p>ValueError: Calling BartTokenizer.from_pretrained() with the path to a single file or url is not supported for this tokenizer. Use a model identifier or the path to a directory instead.</p>
</blockquote>
<p>Could anyone help me out?</p>
<p>I would like to use WordLevel encoding method to establish my own wordlists and tokenize them using WordLevel encoding but not BEP encoding</p>
",Dataset Preprocessing & Handling,load wordlevel tokenizer trained tokenizers transformer would like use wordlevel encoding method establish wordlists save model vocab json word token folder code work try use barttokenizer berttokenizer load doe work especially term berttokenizer tokenized result unk barttokenizer error valueerror calling barttokenizer pretrained path single file url supported tokenizer use model identifier path directory instead could anyone help would like use wordlevel encoding method establish wordlists tokenize using wordlevel encoding bep encoding
Unable to build vocab for a torchtext text classification,"<p>I'm trying to prepare a custom dataset loaded from a csv file in order to use in a torchtext text binary classification problem. It's a basic dataset with news headlines and a market sentiment label assigned &quot;positive&quot; or &quot;negative&quot;. I've been following some online tutorials on PyTorch to get this far but they've made some significant changes in the latest torchtext package so most of the stuff is out of date.</p>
<p>Below I've successfully parsed my csv file into a pandas dataframe with two columns - text headline and a label which is either 0 or 1 for positive/negative, split into a training and test dataset then wrapped them as a PyTorch dataset class:</p>
<pre><code>train, test = train_test_split(eurusd_df, test_size=0.2)
class CustomTextDataset(Dataset):
def __init__(self, text, labels):
    self.text = text
    self.labels = labels
    
def __getitem__(self, idx):
    label = self.labels.iloc[idx]
    text = self.text.iloc[idx]
    sample = {&quot;Label&quot;: label, &quot;Text&quot;: text}
    return sample

def __len__(self):
    return len(self.labels)
train_dataset = CustomTextDataset(train['Text'], train['Labels'])
test_dataset = CustomTextDataset(test['Text'], test['Labels'])
</code></pre>
<p>I'm now trying to build a vocabulary of tokens following this tutorial <a href=""https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-simple-guide-to-text-classification"" rel=""nofollow noreferrer"">https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-simple-guide-to-text-classification</a> and the official pytorch tutorial <a href=""https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html</a> .</p>
<p>However using the below code</p>
<pre><code>from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator

tokenizer = get_tokenizer('basic_english')
train_iter = train_dataset

def yield_tokens(data_iter):
    for _, text in data_iter:
        yield tokenizer(text)
        
vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[&quot;&lt;unk&gt;&quot;])
vocab.set_default_index(vocab[&quot;&lt;unk&gt;&quot;])
</code></pre>
<p>yields a very small length of vocabulary, and applying the example <code>vocab(['here', 'is', 'an', 'example'])</code> on a text field taken from the original dataframe yields a list of 0s, implying the vocab is being built from the label field, containing only 0s and 1s, not the text field. Could anyone review and show me how to build the vocab targeting the text field?</p>
",Dataset Preprocessing & Handling,unable build vocab torchtext text classification trying prepare custom dataset loaded csv file order use torchtext text binary classification problem basic dataset news headline market sentiment label assigned positive negative following online tutorial pytorch get far made significant change latest torchtext package stuff date successfully parsed csv file panda dataframe two column text headline label either positive negative split training test dataset wrapped pytorch dataset class trying build vocabulary token following tutorial official pytorch tutorial however using code yield small length vocabulary applying example text field taken original dataframe yield list implying vocab built label field containing text field could anyone review show build vocab targeting text field
How can I provide a chat interface to search my structured database,"<p>I have an application that is used by medical professionals to search medical documents (structured). The documents have a variety of fields and can be searched, filtered and sorted in many different ways. The data can be transformed into CSV or other formats if required.</p>
<p>The current implementation is documents are stored in an ElasticSearch database. We then have a UI with inputs and filters to help users find what they are looking for. (SearchKit).</p>
<p>Because the data is complex and numerous ways to query this increases the complexity of the app. Many users are not technically literate so we need to strike a balance between usability and providing the options required. Different medical professionals also use different terms for the same thing making it increasingly difficult to manage.</p>
<p>With the increased popularity of chat interfaces I wondered if it would be possible to provide our users with the ability to query the database using natural language queries too.</p>
<p>I have done some research into this but unsure if it is possible with today's technology. I have explored tools such as AWS Lex but these appear to be more rigid in their applications. I've also read about NLP and NLQ.</p>
<p>This question is asking what advice or documentation I could look at to achieve this (if it is possible).</p>
<p>Many thanks in advance</p>
",Dataset Preprocessing & Handling,provide chat interface search structured database application used medical professional search medical document structured document variety field searched filtered sorted many different way data transformed csv format required current implementation document stored elasticsearch database ui input filter help user find looking searchkit data complex numerous way query increase complexity app many user technically literate need strike balance usability providing option required different medical professional also use different term thing making increasingly difficult manage increased popularity chat interface wondered would possible provide user ability query database using natural language query done research unsure possible today technology explored tool aws lex appear rigid application also read nlp nlq question asking advice documentation could look achieve possible many thanks advance
Got the &quot;Unable to load vocabulary from file.&quot; while using pipelines,"<p>I have been trying to use the &quot;csebuetnlp/mT5_multilingual_XLSum&quot; model for summarization purposes.<br />
The code I tried is listed as below:</p>
<pre class=""lang-py prettyprint-override""><code>
!pip install transformers
!pip install sentencepiece
import transformers
text_example = &quot;&quot;&quot; 
En düşük emekli aylığının 5 bin 500 liradan 7 bin 500 liraya yükseltilmesi için TBMM'de yasal düzenleme yapılacak; ardından zamlı aylıkların nisan ayında hesaplara aktarılması planlanıyor.

AKP'li Cumhurbaşkanı Recep Tayyip Erdoğan'ın dün katıldığı televizyon programında en düşük emekli aylığının 7 bin 500 liraya yükseltildiği yönündeki açıklaması, emekliler tarafından memnuniyetle karşılandı.

Bu müjdenin ardından gözler, söz konusu kararın uygulanması için TBMM'de yapılacak yasal düzenlemeye çevrildi.

En düşük emekli aylığının 7 bin 500 liraya yükseltilmesi yönündeki kararın ilerleyen günlerde yasalaşması ve zamlı aylıkların nisan ayında hesaplara yatırılması bekleniyor.

Söz konusu artıştan, EYT düzenlemesiyle emekli olanlarla beraber emeklilerin yarısından fazlası yararlanacak.
&quot;&quot;&quot;

from transformers import pipeline
summarizer = pipeline(&quot;summarization&quot;, model= &quot;csebuetnlp/mT5_multilingual_XLSum&quot;)
summarizer(text_example)
</code></pre>
<p>The output I got is listed as below:</p>
<pre class=""lang-tsx prettyprint-override""><code>Requirement already satisfied: transformers in d:\anaaac\lib\site-packages (4.24.0)
Requirement already satisfied: regex!=2019.12.17 in d:\anaaac\lib\site-packages (from transformers) (2023.3.23)
Requirement already satisfied: pyyaml&gt;=5.1 in d:\anaaac\lib\site-packages (from transformers) (6.0)
Requirement already satisfied: requests in d:\anaaac\lib\site-packages (from transformers) (2.28.1)
Requirement already satisfied: tqdm&gt;=4.27 in d:\anaaac\lib\site-packages (from transformers) (4.65.0)
Requirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.10.0 in d:\anaaac\lib\site-packages (from transformers) (0.11.0)
Requirement already satisfied: packaging&gt;=20.0 in d:\anaaac\lib\site-packages (from transformers) (23.0)
Requirement already satisfied: tokenizers!=0.11.3,&lt;0.14,&gt;=0.11.1 in d:\anaaac\lib\site-packages (from transformers) (0.13.2)
Requirement already satisfied: filelock in d:\anaaac\lib\site-packages (from transformers) (3.9.0)
Requirement already satisfied: numpy&gt;=1.17 in d:\anaaac\lib\site-packages (from transformers) (1.24.2)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in d:\anaaac\lib\site-packages (from huggingface-hub&lt;1.0,&gt;=0.10.0-&gt;transformers) (4.4.0)
Requirement already satisfied: colorama in d:\anaaac\lib\site-packages (from tqdm&gt;=4.27-&gt;transformers) (0.4.6)
Requirement already satisfied: certifi&gt;=2017.4.17 in d:\anaaac\lib\site-packages (from requests-&gt;transformers) (2022.12.7)
Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in d:\anaaac\lib\site-packages (from requests-&gt;transformers) (2.0.4)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in d:\anaaac\lib\site-packages (from requests-&gt;transformers) (3.4)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in d:\anaaac\lib\site-packages (from requests-&gt;transformers) (1.26.14)
Requirement already satisfied: sentencepiece in d:\anaaac\lib\site-packages (0.1.97)
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
File D:\anaaac\lib\site-packages\transformers\tokenization_utils_base.py:1932, in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)
   1931 try:
-&gt; 1932     tokenizer = cls(*init_inputs, **init_kwargs)
   1933 except OSError:

File D:\anaaac\lib\site-packages\transformers\models\t5\tokenization_t5.py:155, in T5Tokenizer.__init__(self, vocab_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, sp_model_kwargs, **kwargs)
    154 self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)
--&gt; 155 self.sp_model.Load(vocab_file)

File D:\anaaac\lib\site-packages\sentencepiece\__init__.py:905, in SentencePieceProcessor.Load(self, model_file, model_proto)
    904   return self.LoadFromSerializedProto(model_proto)
--&gt; 905 return self.LoadFromFile(model_file)

File D:\anaaac\lib\site-packages\sentencepiece\__init__.py:310, in SentencePieceProcessor.LoadFromFile(self, arg)
    309 def LoadFromFile(self, arg):
--&gt; 310     return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)

OSError: Not found: &quot;C:\Users\ist/.cache\huggingface\hub\models--csebuetnlp--mT5_multilingual_XLSum\snapshots\2437a524effdbadc327ced84595508f1e32025b3\spiece.model&quot;: No such file or directory Error #2

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
Cell In[4], line 17
      4 text_example = &quot;&quot;&quot; 
      5 En düşük emekli aylığının 5 bin 500 liradan 7 bin 500 liraya yükseltilmesi için TBMM'de yasal düzenleme yapılacak; ardından zamlı aylıkların nisan ayında hesaplara aktarılması planlanıyor.
      6 
   (...)
     13 Söz konusu artıştan, EYT düzenlemesiyle emekli olanlarla beraber emeklilerin yarısından fazlası yararlanacak.
     14 &quot;&quot;&quot;
     16 from transformers import pipeline
---&gt; 17 summarizer = pipeline(&quot;summarization&quot;, model= &quot;csebuetnlp/mT5_multilingual_XLSum&quot;)
     18 summarizer(text_example)

File D:\anaaac\lib\site-packages\transformers\pipelines\__init__.py:801, in pipeline(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)
    798             tokenizer_identifier = tokenizer
    799             tokenizer_kwargs = model_kwargs
--&gt; 801         tokenizer = AutoTokenizer.from_pretrained(
    802             tokenizer_identifier, use_fast=use_fast, _from_pipeline=task, **hub_kwargs, **tokenizer_kwargs
    803         )
    805 if load_feature_extractor:
    806     # Try to infer feature extractor from model or config name (if provided as str)
    807     if feature_extractor is None:

File D:\anaaac\lib\site-packages\transformers\models\auto\tokenization_auto.py:619, in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    615     if tokenizer_class is None:
    616         raise ValueError(
    617             f&quot;Tokenizer class {tokenizer_class_candidate} does not exist or is not currently imported.&quot;
    618         )
--&gt; 619     return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
    621 # Otherwise we have to be creative.
    622 # if model is an encoder decoder, the encoder tokenizer class is used by default
    623 if isinstance(config, EncoderDecoderConfig):

File D:\anaaac\lib\site-packages\transformers\tokenization_utils_base.py:1777, in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
   1774     else:
   1775         logger.info(f&quot;loading file {file_path} from cache at {resolved_vocab_files[file_id]}&quot;)
-&gt; 1777 return cls._from_pretrained(
   1778     resolved_vocab_files,
   1779     pretrained_model_name_or_path,
   1780     init_configuration,
   1781     *init_inputs,
   1782     use_auth_token=use_auth_token,
   1783     cache_dir=cache_dir,
   1784     local_files_only=local_files_only,
   1785     _commit_hash=commit_hash,
   1786     **kwargs,
   1787 )

File D:\anaaac\lib\site-packages\transformers\tokenization_utils_base.py:1807, in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)
   1805 has_tokenizer_file = resolved_vocab_files.get(&quot;tokenizer_file&quot;, None) is not None
   1806 if (from_slow or not has_tokenizer_file) and cls.slow_tokenizer_class is not None:
-&gt; 1807     slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
   1808         copy.deepcopy(resolved_vocab_files),
   1809         pretrained_model_name_or_path,
   1810         copy.deepcopy(init_configuration),
   1811         *init_inputs,
   1812         use_auth_token=use_auth_token,
   1813         cache_dir=cache_dir,
   1814         local_files_only=local_files_only,
   1815         _commit_hash=_commit_hash,
   1816         **(copy.deepcopy(kwargs)),
   1817     )
   1818 else:
   1819     slow_tokenizer = None

File D:\anaaac\lib\site-packages\transformers\tokenization_utils_base.py:1934, in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)
   1932     tokenizer = cls(*init_inputs, **init_kwargs)
   1933 except OSError:
-&gt; 1934     raise OSError(
   1935         &quot;Unable to load vocabulary from file. &quot;
   1936         &quot;Please check that the provided vocabulary is accessible and not corrupted.&quot;
   1937     )
   1939 # Save inputs and kwargs for saving and re-loading with ``save_pretrained``
   1940 # Removed: Now done at the base class level
   1941 # tokenizer.init_inputs = init_inputs
   1942 # tokenizer.init_kwargs = init_kwargs
   1943 
   1944 # If there is a complementary special token map, load it
   1945 special_tokens_map_file = resolved_vocab_files.pop(&quot;special_tokens_map_file&quot;, None)

OSError: Unable to load vocabulary from file. Please check that the provided vocabulary is accessible and not corrupted.
</code></pre>
<p>The text is about the <em>raise for the retirement wage in Turkey</em>.</p>
<p>This part of the output is really weird considering the <em>spiece.model</em> file exist in the exact same directory.</p>
<pre><code>OSError: Not found: &quot;C:\Users\ist/.cache\huggingface\hub\models--csebuetnlp--mT5_multilingual_XLSum\snapshots\2437a524effdbadc327ced84595508f1e32025b3\spiece.model&quot;: No such file or directory Error #2
</code></pre>
<p><strong>Version info:</strong></p>
<pre><code>Server Information:

You are using Jupyter Notebook.

The version of the notebook server is: **6.5.3**  
The server is running on this version of Python:

`Python 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]`

</code></pre>
<ol>
<li>I tried upgrading transformers sentencepiece, it didnt help.</li>
<li>I tried using the models named <code>sshleifer/distilbart-cnn-12-6</code> and <code>flax-community/t5-base-cnn-dm</code>. They both worked as expected but i need a multilingual model.</li>
<li>I tried running the same code in Google Colaboratory it worked as expected and the output is:</li>
</ol>
<pre class=""lang-tsx prettyprint-override""><code>Downloading (…)lve/main/config.json: 100%
730/730 [00:00&lt;00:00, 14.0kB/s]
Downloading pytorch_model.bin: 100%
2.33G/2.33G [00:23&lt;00:00, 109MB/s]
Downloading (…)okenizer_config.json: 100%
375/375 [00:00&lt;00:00, 9.51kB/s]
Downloading spiece.model: 100%
4.31M/4.31M [00:00&lt;00:00, 14.0MB/s]
Downloading (…)cial_tokens_map.json: 100%
65.0/65.0 [00:00&lt;00:00, 2.70kB/s]
/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
[{'summary_text': &quot;Cumhurbaşkanı Recep Tayyip Erdoğan'ın dün açıkladığı en düşük emekli aylığının 7 bin 500 liraya yükseltilmesi yönündeki açıklaması, emekliler tarafından memnuniyetle karşılandı.&quot;}]
</code></pre>
<p>The output is not weird and the summary is understandable.</p>
<hr />
<p>I belive the error is more about sentencepiece than pipelines. I checked some similar issues in github, stackoverflow and some chineese forums. None of the issues helped. I need some help with the code. Thanks.</p>
",Dataset Preprocessing & Handling,got unable load vocabulary file using pipeline trying use csebuetnlp mt multilingual xlsum model summarization purpose code tried listed output got listed text raise retirement turkey part output really weird considering spiece model file exist exact directory version info tried upgrading transformer sentencepiece didnt help tried using model named worked expected need multilingual model tried running code google colaboratory worked expected output output weird summary understandable belive error sentencepiece pipeline checked similar issue github stackoverflow chineese forum none issue helped need help code thanks
Equivalent of Apache Lucene &quot;proximity searches&quot; in R,"<p>I'm working on a corpus of documents (clinical narratives from hospital stays), mainly using the <strong>Quanteda</strong> package.
The objective is to be able to classify documents based on the presence/absence of a feature, let's say &quot;spastic cough&quot;.</p>
<p>I would like to be able to reproduce the behaviour of an Apache Lucene &quot;proximity search&quot; (<a href=""https://lucene.apache.org/core/8_11_2/queryparser/org/apache/lucene/queryparser/classic/package-summary.html#Proximity_Searches"" rel=""nofollow noreferrer"">https://lucene.apache.org/core/8_11_2/queryparser/org/apache/lucene/queryparser/classic/package-summary.html#Proximity_Searches</a>) using R.</p>
<p>Let's take an <strong>example</strong>:
&quot;<em>spastic</em> and productive <em>cough</em> in a 91-year-old patient following femoral neck surgery&quot;</p>
<p>I would begin tokenizing the phrase as follows:</p>
<pre><code>toks = 
tokens(
c(text1 = &quot;spastic and productive cough in a 91-year-old patient following femoral neck surgery&quot;), 
remove_punct = T, remove_symbols = T, remove_numbers = T, padding = T
) %&gt;% 
tokens_remove(pattern = stopwords(&quot;en&quot;,source = &quot;nltk&quot;))
</code></pre>
<p>which yields the following output:</p>
<pre><code>Tokens consisting of 1 document.
text1 :
[1] &quot;spastic&quot;     &quot;productive&quot;  &quot;cough&quot;       &quot;91-year-old&quot; &quot;patient&quot;     &quot;following&quot;   &quot;femoral&quot;    
[8] &quot;neck&quot;        &quot;surgery&quot; 
</code></pre>
<p>I can then proceed to generate n-grams and skip-grams:</p>
<pre><code>toks = tokens_ngrams(toks,n=4,skip = 0:3)

toks
[1] &quot;spastic_productive_cough_91-year-old&quot;     &quot;spastic_productive_cough_patient&quot;        
  [3] &quot;spastic_productive_cough_following&quot;       &quot;spastic_productive_cough_femoral&quot;        
  [5] &quot;spastic_productive_91-year-old_patient&quot;   &quot;spastic_productive_91-year-old_following&quot;
  [7] &quot;spastic_productive_91-year-old_femoral&quot;   &quot;spastic_productive_91-year-old_neck&quot;     
  [9] &quot;spastic_productive_patient_following&quot;     &quot;spastic_productive_patient_femoral&quot;      
 [11] &quot;spastic_productive_patient_neck&quot;          &quot;spastic_productive_patient_surgery&quot;      
 [13] &quot;spastic_productive_following_femoral&quot;     &quot;spastic_productive_following_neck&quot;       
 [15] &quot;spastic_productive_following_surgery&quot;     &quot;spastic_cough_91-year-old_patient&quot;       
 [17] &quot;spastic_cough_91-year-old_following&quot;      &quot;spastic_cough_91-year-old_femoral&quot;       
 [19] &quot;spastic_cough_91-year-old_neck&quot;           &quot;spastic_cough_patient_following&quot;         
 [21] &quot;spastic_cough_patient_femoral&quot;            &quot;spastic_cough_patient_neck&quot;              
 [23] &quot;spastic_cough_patient_surgery&quot;            &quot;spastic_cough_following_femoral&quot;         
 [25] &quot;spastic_cough_following_neck&quot;             &quot;spastic_cough_following_surgery&quot;         
 [27] &quot;spastic_cough_femoral_neck&quot;               &quot;spastic_cough_femoral_surgery&quot;           
 [29] &quot;spastic_91-year-old_patient_following&quot;    &quot;spastic_91-year-old_patient_femoral&quot;     
 [31] &quot;spastic_91-year-old_patient_neck&quot;         &quot;spastic_91-year-old_patient_surgery&quot;     
.........
</code></pre>
<p>At this point i guess i could simply:</p>
<pre><code>any(str_detect(as.character(toks),&quot;spastic_cough&quot;))
[1] TRUE
</code></pre>
<p>but I'm not sure I'm using the correct approach as it feels clunky compared to how a Lucene query would work. If I were trying to identify patients with &quot;spastic cough&quot; using Apache Lucene to query the corpus I may use something like &quot;spastic cough&quot;~3 where &quot;~3&quot; means that any skip-gram 0:3 would match.</p>
<p>Any input about how and where I could improve my method?</p>
<h2>EDIT:</h2>
<p>This may do the trick: <a href=""https://search.r-project.org/CRAN/refmans/corpustools/html/search_features.html"" rel=""nofollow noreferrer"">https://search.r-project.org/CRAN/refmans/corpustools/html/search_features.html</a></p>
<p>but, at the moment, I can't figure out how to include it in the workflow.</p>
<h2>EDIT 2:</h2>
<p>It seems like i can query the corpus using subset_query using a Lucene like syntax. The big problem i'm facing now is that &quot;<em>corpustools</em>&quot; isn't accepting as input tokens object and the function <strong>tokens_to_corpus()</strong> isn't working for me. <strong>This prevents me from being able to control the tokenization process</strong></p>
",Dataset Preprocessing & Handling,equivalent apache lucene proximity search r working corpus document clinical narrative hospital stay mainly using quanteda package objective able classify document based presence absence feature let say spastic cough would like able reproduce behaviour apache lucene proximity search using r let take example spastic productive cough year old patient following femoral neck surgery would begin tokenizing phrase follows yield following output proceed generate n gram skip gram point guess could simply sure using correct approach feel clunky compared lucene query would work trying identify patient spastic cough using apache lucene query corpus may use something like spastic cough mean skip gram would match input could improve method edit may trick moment figure include workflow edit seems like query corpus using subset query using lucene like syntax big problem facing corpustools accepting input token object function token corpus working prevents able control tokenization process
Can&#39;t import the encoder code for fine tuning GPT-2,"<p>I'm trying to reproduce the example from this article: <a href=""https://medium.com/@ngwaifoong92/beginners-guide-to-retrain-gpt-2-117m-to-generate-custom-text-content-8bb5363d8b7f"" rel=""nofollow noreferrer"">https://medium.com/@ngwaifoong92/beginners-guide-to-retrain-gpt-2-117m-to-generate-custom-text-content-8bb5363d8b7f</a> </p>

<p>The example code is from the following repo: <a href=""https://github.com/nshepperd/gpt-2"" rel=""nofollow noreferrer"">https://github.com/nshepperd/gpt-2</a></p>

<p>After installing the requirements and downloading the model, the following step is to train the model, for which this code has to be executed: </p>

<pre><code>python encode.py lyric.txt lyric.npz
</code></pre>

<p>The issue here is that this requires to import the following modules: </p>

<pre><code>import argparse
import numpy as np

import encoder
from load_dataset import load_dataset
</code></pre>

<p>Where <strong>encoder</strong> and <strong>load_dataset</strong> are on a child directory:</p>

<pre><code>|--encode.py
 --src
   |--encoder.py
   |--load_dataset.py
</code></pre>

<p>This generates the following error: </p>

<pre><code>ModuleNotFoundError: No module named 'encoder'
</code></pre>

<p>I tried creating the <code>__init__.py</code> files and importing them as </p>

<p><strong>src.encoder</strong> and <strong>src.load_dataset</strong> but that those not work either.</p>

<p>In the medium post the author proposes to move the file <strong>encoder.py</strong> to src and execute the code from there, the issue there is that doing it breaks the relative path for the model too and although I handled that the issue with the paths keeps going for other files as well.   </p>
",Dataset Preprocessing & Handling,import encoder code fine tuning gpt trying reproduce example article example code following repo installing requirement downloading model following step train model code ha executed issue requires import following module encoder load dataset child directory generates following error tried creating file importing src encoder src load dataset work either medium post author proposes move file encoder py src execute code issue break relative path model although handled issue path keep going file well
NLP how can i match multiple token words from a larger text,"<p>I am currently having trouble with the following. I receive a job offer and I have to extract certain words from my CSV file. These words that I am trying to extract can be multiple tokens long (up to 4 tokens long)  However, I have to keep in mind that there can be instances of misspellings and use of abbreviations. So a direct matching algorithm wouldn't give me a good result. What can I do to check whether the words in my CSV file are mentioned in the text? Keep in mind, I do not have a large dataset.</p>
<p>My original plan was to do a similarity match between the words in my CSV file and the whole text. To solve misspellings and abbreviations, I added a column with possible variations/abbreviations and also did a similarity match on those. If a similarity score would be above a certain threshold, and is the highest match, then it would be a 'match'. To do multiple-word matching, I added n-grams when doing the similarity match. However, I got a lot of false positives. Even setting a higher threshold did not solve my issue.</p>
<p>I also tried building a custom NER model. This worked decently. I even used my NER model to extract potentially relevant words and then did a similarity match to get good results. However, my solution needs to be easily expandable. Adding new words to the CSV file is easy, but retraining the NER model each time isn't ideal.</p>
",Dataset Preprocessing & Handling,nlp match multiple token word larger text currently trouble following receive job offer extract certain word csv file word trying extract multiple token long token long however keep mind instance misspelling use abbreviation direct matching algorithm give good result check whether word csv file mentioned text keep mind large dataset original plan wa similarity match word csv file whole text solve misspelling abbreviation added column possible variation abbreviation also similarity match similarity score would certain threshold highest match would match multiple word matching added n gram similarity match however got lot false positive even setting higher threshold solve issue also tried building custom ner model worked decently even used ner model extract potentially relevant word similarity match get good result however solution need easily expandable adding new word csv file easy retraining ner model time ideal
Text recognition from PDF file,"<p>I want to write a code that loads a PDF file, converts its text to string and finds certain information within it. There are hundreds of these PDF files and each contain a certain non-tabular piece of information that I am interested in. Let's say each PDF file belongs to specific individuals that contain their address, name, age, and income. However, these PDF files do not contain this data in a consistent format. PDF1 might say &quot;Mike's income is $100,000&quot; while PDF2 says &quot;Jane's salary: $110,000&quot; and PDF3 says &quot;Andy makes $70,000&quot;. I want to extract the salary from different PDF files and convert them to a nice panda data frame and I do not know where to begin. Can I use NLP for this? Any guidance can help me do more educated search. Thank you!</p>
<p>I have authored the code below to read PDF files.</p>
<p>I have done the following so far and it is working just fine.</p>
<pre><code>#! pip install PyPDF2

import PyPDF2 as pdf

file = open ('File April 17th.pdf','rb')

pdf_reader = pdf.PdfReader(file)

Text = ''

for i in range(len(pdf_reader.pages)):
    page = pdf_reader.pages[i]
    text = page.extract_text()
    Text = Text + text
   
</code></pre>
",Dataset Preprocessing & Handling,text recognition pdf file want write code load pdf file convert text string find certain information within hundred pdf file contain certain non tabular piece information interested let say pdf file belongs specific individual contain address name age income however pdf file contain data consistent format pdf might say mike income pdf say jane salary pdf say andy make want extract salary different pdf file convert nice panda data frame know begin use nlp guidance help educated search thank authored code read pdf file done following far working fine
Find words in a column per row after list of specific words in Python,"<p>I have a Pandas DataFrame with a column named &quot;warranty&quot; that contains records of various ways to fix different issues, as shown below:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th style=""text-align: right;"">wnum</th>
<th style=""text-align: right;"">warranty</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td style=""text-align: right;"">100</td>
<td style=""text-align: right;"">replace battery wire from car</td>
</tr>
<tr>
<td>1</td>
<td style=""text-align: right;"">101</td>
<td style=""text-align: right;"">clean fuel tank</td>
</tr>
<tr>
<td>2</td>
<td style=""text-align: right;"">102</td>
<td style=""text-align: right;"">remove nail from tire</td>
</tr>
</tbody>
</table>
</div>
<p>Code for initialize:</p>
<pre class=""lang-py prettyprint-override""><code>df.pdDataFrame({
  'wnum'    : [100, 101, 102],
  'warranty': ['replace battery wire from car', 'clean fuel tank', 'remove nail from tire']
})
</code></pre>
<p>My goal is to extract the words following the specific words listed below:</p>
<pre><code>word_list=['replace', 'clean', 'remove']
</code></pre>
<p>How can I achieve this, and get the expected output of a new column added to the above DataFrame with the values &quot;replace battery,&quot; &quot;wire clean,&quot; &quot;fuel tank,&quot; and &quot;remove nail&quot;?</p>
",Dataset Preprocessing & Handling,find word column per row list specific word python panda dataframe column named warranty contains record various way fix different issue shown wnum warranty replace battery wire car clean fuel tank remove nail tire code initialize goal extract word following specific word listed achieve get expected output new column added dataframe value replace battery wire clean fuel tank remove nail
How to read tagged PDF in C# using iText 7?,"<p>I am trying to read pieces of information from tables in PDF files in C# using iText 7.</p>
<p>Currently I'm using <code>TextRegionEventFilter</code> and <code>PdfTextExtractor</code> to get the text in the cells using the cell's X, Y locations.</p>
<p>If you have programmed it like this before you know that this sorta works if the X, Y location of your data elements stay constant in each PDF file.</p>
<p>But because the table's row height and column width in my PDF files keep changing, this approach won't work very well.</p>
<p>So, I thought if I can extract the PDF content as a string that includes the tags, then I can precisely pinpoint the pieces of information I need by parsing the raw string much like an HTML file.</p>
<p>To my disappointment, the PDF content I extract using <code>LocationTextExtractionStrategy</code>'s <code>GetResultantText</code> as a string doesn't contain any of the tags.</p>
<p>So, my question is: Is it possible to extract PDF content with tags using iText 7? I searched high and low for how to read tagged PDF in iText, but nothing comes up.</p>
<p><a href=""https://i.sstatic.net/ZdjVk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZdjVk.png"" alt=""enter image description here"" /></a></p>
",Dataset Preprocessing & Handling,read tagged pdf c using itext trying read piece information table pdf file c using itext currently using get text cell using cell x location programmed like know sorta work x location data element stay constant pdf file table row height column width pdf file keep changing approach work well thought extract pdf content string includes tag precisely pinpoint piece information need parsing raw string much like html file pdf content extract using string contain tag question possible extract pdf content tag using itext searched high low read tagged pdf itext nothing come
Couting repeated sentences from corpus,"<p>I have a number of CSV files containing emails in one of the columns.
Each row has one complete email. Some of these emails are auto-responses and occur multiple times.</p>
<p>What is the best way to read through all the emails and count those sentences that keep repeating?</p>
<p>With just <code>pandas</code>, each row would have to contain just one sentence, but the data has emails, each consisting of many sentences. <a href=""https://stackoverflow.com/questions/59943539/how-to-find-frequency-of-repeated-sentence-in-a-file"">Related question</a></p>
<p>The input text:</p>
<blockquote>
<p>This is a new first line.
This line is being added to file.
And another line here.</p>
<p>This is more text being added to file.
And another line here.
This is a new first line</p>
</blockquote>
<p><strong>Expected output:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Sentence</th>
<th style=""text-align: center;"">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">This is a new first line</td>
<td style=""text-align: center;"">2</td>
</tr>
<tr>
<td style=""text-align: center;"">And another line here.</td>
<td style=""text-align: center;"">2</td>
</tr>
<tr>
<td style=""text-align: center;"">This line is being added to file.</td>
<td style=""text-align: center;"">1</td>
</tr>
<tr>
<td style=""text-align: center;"">This is more text being added to file.</td>
<td style=""text-align: center;"">1</td>
</tr>
</tbody>
</table>
</div>
<p>What I have so far:</p>
<pre><code># Getting number of sentences with spacy
sentence_tokens = [[token.text for token in sent] for sent in doc.sents]
print(len(sentence_tokens))

# Getting sentence in list
[sent.text for sent in doc.sents ]
</code></pre>
<p>Output:</p>
<pre><code>5

['This is a new first line.\n',
 'This line is being appended to file\nAnd another line here.\n',
 'This is more text being appended to file.\n',
 'And another line here.\n',
 'This is a new first line\n']
</code></pre>
<p>Using a counter for sentences.</p>
<pre><code>from collections import Counter
counts = Counter([sent.text for sent in doc.sents ])
print(counts)
</code></pre>
<p>Output:</p>
<pre><code>Counter({'This is a new first line.\n': 1, 'This line is being appended to file\nAnd another line here.\n': 1, 'This is more text being appended to file.\n': 1, 'And another line here.\n': 1, 'This is a new first line\n': 1})
</code></pre>
",Dataset Preprocessing & Handling,couting repeated sentence corpus number csv file containing email one column row ha one complete email email auto response occur multiple time best way read email count sentence keep repeating row would contain one sentence data ha email consisting many sentence sentence count new first line another line line added file text added file far output using counter sentence output
Didnt get the expected results when calculate Cosine similarity between strings,"<p>I want to calculate the pairwise cosine similarity between two strings  that are in the same row of a pandas data frame.</p>
<p>I used the following lines of codes:</p>
<pre><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity


pd.set_option('display.float_format', '{:.4f}'.format)


df = pd.DataFrame({'text1': ['The quick brown fox jumps over the lazy dog', 'The red apple', 'The big blue sky'],
                   'text2': ['The lazy cat jumps over the brown dog', 'The red apple', 'The big yellow sun']})


vectorizer = CountVectorizer().fit_transform(df['text1'] + ' ' + df['text2'])


cosine_similarities = cosine_similarity(vectorizer)[:, 0:1]


df['cosine_similarity'] = cosine_similarities


print(df)  
</code></pre>
<p>It gave me following output, which seems incorrect:</p>
<p><a href=""https://i.sstatic.net/dVOTF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dVOTF.png"" alt=""enter image description here"" /></a></p>
<p>Can anyone help me to figure out what I did incorrectly?</p>
<p>Thank you.</p>
",Dataset Preprocessing & Handling,didnt get expected result calculate cosine similarity string want calculate pairwise cosine similarity two string row panda data frame used following line code gave following output seems incorrect anyone help figure incorrectly thank
"How can I use Regex to differentiate between a fully uppercase word, and an uppercase word attached to a lower case character with missing whitespace?","<p>Apologies for the convoluted title. I am trying to process text, with some undesirable features: some words are all in upper-case, such as 'EXAMPLE WORD', whilst in other cases there are two words attached, with missing whitespace, as in 'exampleWord'. How can I use regular expressions to separate the two attached words based on the lower-case &gt; upper-case pattern, without affecting the words completely in upper case?</p>
<p>I am currently using</p>
<p>.apply(lambda x: re.sub( r&quot;([A-Z])&quot;, r&quot; \1&quot;, x))</p>
<p>across that column in a Pandas data frame. This inserts a space before each capital letter, transforming upper case letters into a string of individual characters.</p>
<p>I would like a regex expression to only detect when a lowercase character is followed immediately by an upper case character, to separate the words at that point, which would solve the issue of having to affect the words in uppercase. Is this possible?</p>
",Dataset Preprocessing & Handling,use regex differentiate fully uppercase word uppercase word attached lower case character missing whitespace apology convoluted title trying process text undesirable feature word upper case example word whilst case two word attached missing whitespace exampleword use regular expression separate two attached word based lower case upper case pattern without affecting word completely upper case currently using apply lambda x sub r z r x across column panda data frame insert space capital letter transforming upper case letter string individual character would like regex expression detect lowercase character followed immediately upper case character separate word point would solve issue affect word uppercase possible
Create different dataframe inside of a &#39;for&#39; loop,"<p>I have a dataset that looks something like the following. I would like to create dataframes that contains only texts for each authors, for example as you can see the df1 contains only texts from the author0, etc. Is there any way to do that for many authors?</p>
<pre><code>import pandas as pd

data = {
    'text' : ['text0', 'text1', 'text2'],
    'author': ['author0', 'author1', 'author1'],
    'title': ['Comunicación', 'Administración', 'Ventas']
}

df = pd.DataFrame(data)
df1 = df[df[&quot;author&quot;]==&quot;author0&quot;]

df2 = df[df[&quot;author&quot;]==&quot;author1&quot;]
</code></pre>
<p>I have tried this, but it's not working</p>
<pre><code>list_author = df['author'].unique().tolist()
for i in list_author: 
  dt_str(i) = dt[dt[&quot;author&quot;]==&quot;i&quot;]
</code></pre>
<p>It would be helpful if the data frames have the name df_'author' (eg df_George)</p>
",Dataset Preprocessing & Handling,create different dataframe inside loop dataset look something like following would like create dataframes contains text author example see df contains text author etc way many author tried working would helpful data frame name df author eg df george
Sklearn Pipeline ValueError: could not convert string to float,"<p>I'm playing around with sklearn and NLP for the first time, and thought I understood everything I was doing up until I didn't know how to fix this error. Here is the relevant code (largely adapted from <a href=""http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html"" rel=""noreferrer"">http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html</a>):</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import TruncatedSVD
from sgboost import XGBClassifier
from pandas import DataFrame

def read_files(path):
    for article in os.listdir(path):
        with open(os.path.join(path, doc)) as f:
            text = f.read()
        yield os.path.join(path, article), text

def build_data_frame(path, classification)
    rows = []
    index = []
    for filename, text in read_files(path):
        rows.append({'text': text, 'class': classification})
        index.append(filename)
    df = DataFrame(rows, index=index)
    return df

data = DataFrame({'text': [], 'class': []})
for path, classification in SOURCES: # SOURCES is a list of tuples
    data = data.append(build_data_frame(path, classification))
data = data.reindex(np.random.permutation(data.index))

classifier = Pipeline([
    ('features', FeatureUnion([
        ('text', Pipeline([
            ('tfidf', TfidfVectorizer()),
            ('svd', TruncatedSVD(algorithm='randomized', n_components=300)
            ])),
        ('words', Pipeline([('wscaler', StandardScaler())])),
    ])),
    ('clf, XGBClassifier(silent=False)),
])
classifier.fit(data['text'].values, data['class'].values)
</code></pre>

<p>The data loaded into the DataFrame is preprocessed text with all stopwords, punctuation, unicode, capitals, etc. taken care of. This is the error I'm getting once I call fit on the classifier where the ... represents one of the documents that should have been vecorized in the pipeline:</p>

<pre><code>ValueError: could not convert string to float: ...
</code></pre>

<p>I first thought the TfidfVectorizer() is not working, causing an error on the SVD algorithm, but after I extracted each step out of the pipeline and implemented them sequentially, the same error only came up on XGBClassifer.fit().</p>

<p>Even more confusing to me, I tried to piece this script apart step-by-step in the interpreter, but when I tried to import either read_files or build_data_frame, the same ValueError came up with one of my strings, but this was merely after:</p>

<pre><code>from classifier import read_files
</code></pre>

<p>I have no idea how that could be happening, if anyone has any idea what my glaring errors may be, I'd really appreciate it. Trying to wrap my head around these concepts on my own but coming across a problem likes this leaves me feeling pretty incapacitated.</p>
",Dataset Preprocessing & Handling,sklearn pipeline valueerror could convert string float playing around sklearn nlp first time thought understood everything wa know fix error relevant code largely adapted data loaded dataframe preprocessed text stopwords punctuation unicode capital etc taken care error getting call fit classifier represents one document vecorized pipeline first thought tfidfvectorizer working causing error svd algorithm extracted step pipeline implemented sequentially error came xgbclassifer fit even confusing tried piece script apart step step interpreter tried import either read file build data frame valueerror came one string wa merely idea could happening anyone ha idea glaring error may really appreciate trying wrap head around concept coming across problem like leaf feeling pretty incapacitated
How can I pass all the values from my csv file to fit_transform?,"<pre><code>for i in range(1,6):
    df1 = pd.read_csv('./starReviews/' + str(i)  + 'Star.csv')
    tfidf_vectorizer = TfidfVectorizer()
    doc_vec = tfidf_vectorizer.fit_transform(df1.loc[0])
</code></pre>
<p>I want to pass all the values of my csv file to fit_transform but currently I am only able to pass the first line. Anyone know how I'd pass everything at once?</p>
",Dataset Preprocessing & Handling,pas value csv file fit transform want pas value csv file fit transform currently able pas first line anyone know pas everything
How to merge predicted values to original pandas test data frame where X_test has been converted using CountVectorizer before splitting,"<p>I want to merge my predicted results of my test data to my X_test. I was able to merge it with y_test but since my X_test is a corpus I'm not sure how I can identify the indexes to merge.
My codes are as below</p>
<pre><code>def lr_model(df):

    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    import pandas as pd
   
    # Create corpus as a list
    corpus = df['text'].tolist()
    cv = CountVectorizer()
    X = cv.fit_transform(corpus).toarray()
    y = df.iloc[:, -1].values

    # Splitting to testing and training
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

    # Train Logistic Regression on Training set
    classifier = LogisticRegression(random_state = 0)
    classifier.fit(X_train, y_train)

    # Predicting the Test set results
    y_pred = classifier.predict(X_test)

    # Merge true vs predicted labels
    true_vs_pred = pd.DataFrame(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

    return true_vs_pred
</code></pre>
<p>This gives me the y_test and y_pred but I'm not sure how I can add the X_test as an original data frame (the ids of the X_test) to this.
Any guidance is much appreciated. Thanks</p>
",Dataset Preprocessing & Handling,merge predicted value original panda test data frame x test ha converted using countvectorizer splitting want merge predicted result test data x test wa able merge test since x test corpus sure identify index merge code give test pred sure add x test original data frame id x test guidance much appreciated thanks
Gensim: Not able to load the id2word file,"<p>I am working on topic inference on a new corpus given a previously derived lda model. I am able to load the model perfectly, while I am not able to load the id2word file to create the <code>corpora.Dictionary</code> object needed to map the new corpus into numbers: the <code>load</code> method returns a dict attribute error that I don't know why. Below is the minimal code that replicates the situation, and I have attached the code (and packages used) here.</p>
<p>Thank you in advance for your response...</p>
<pre><code>import numpy as np
import os
import pandas as pd
import gensim
from gensim import corpora
import datetime
import nltk

model_name = &quot;lda_sub_full_35&quot;

dictionary_name = &quot;lda_sub_full_35.id2word&quot;

model_for_inference = gensim.models.LdaModel.load(model_name, mmap='r')
print('Successfully load the model')
lda_dictionary = corpora.Dictionary.load(dictionary_name, mmap='r')
</code></pre>
<p>I expect to have both the dictionary and the model loaded, but it turns out that when I load the dictionary, I got the below error:</p>
<pre><code>File &quot;topic_inference.py&quot;, line 31, in &lt;module&gt;
    lda_dictionary = corpora.Dictionary.load(dictionary_name, mmap='r')
File &quot;/topic_modeling/env/lib/python3.8/site-packages/gensim/utils.py&quot;, line 487, in load
    obj._load_specials(fname, mmap, compress, subname)
AttributeError: 'dict' object has no attribute '_load_specials'```
</code></pre>
",Dataset Preprocessing & Handling,gensim able load id word file working topic inference new corpus given previously derived lda model able load model perfectly able load id word file create object needed map new corpus number method return dict attribute error know minimal code replicates situation attached code package used thank advance response expect dictionary model loaded turn load dictionary got error
Given a word can we get all possible lemmas for it using Spacy?,"<p>The input word is standalone and not part of a sentence but I would like to get all of its possible lemmas as if the input word were in different sentences with all possible POS tags. I would also like to get the lookup version of the word's lemma.</p>
<p>Why am I doing this?</p>
<p>I have extracted lemmas from all the documents and I have also calculated the number of dependency links between lemmas. Both of which I have done using <code>en_core_web_sm</code>. Now, given an input word, I would like to return the lemmas that are linked most frequently to all the possible lemmas of the input word.</p>
<p>So in short, I would like to replicate the behaviour of <code>token._lemma</code> for the input word with all possible POS tags to maintain consistency with the lemma links I have counted.</p>
",Dataset Preprocessing & Handling,given word get possible lemma using spacy input word standalone part sentence would like get possible lemma input word different sentence possible po tag would also like get lookup version word lemma extracted lemma document also calculated number dependency link lemma done using given input word would like return lemma linked frequently possible lemma input word short would like replicate behaviour input word possible po tag maintain consistency lemma link counted
How to use contextdata in node-nlp properly?,"<p>I am currently trying to create a chatbot and adding some contextdata to it, like in this example: <a href=""https://github.com/axa-group/nlp.js/tree/master/examples/14-ner-corpus"" rel=""nofollow noreferrer"">https://github.com/axa-group/nlp.js/tree/master/examples/14-ner-corpus</a></p>
<p>I have tried several variants of configs, filenames, file paths etc. I even looked up the source code if the contextdata corpus gets read, which it does.</p>
<p>But at the end when running my code against &quot;what is the real name of spiderman?&quot; the {{ hero }} part gets replaced, but the {{ _data[entities.hero.option].realName }} doesn't.</p>
<p>My code currently looks like this:</p>
<pre><code>import { NlpManager, ConversationContext } from 'node-nlp'
const manager = new NlpManager({
    languages: ['en'],
    forceNER: true,
    autoSave: false,
    nlu: { useNoneFeature: true }
})
const context = new ConversationContext()
manager.addCorpora('./corpus.json')
await manager.train()

const response = await manager.process(
    'en',
    'what is the real name of spiderman?',
    context
)
console.log(response)

</code></pre>
<p>The corpus files i use are those linked in the example above:</p>
<p><a href=""https://github.com/axa-group/nlp.js/blob/master/examples/14-ner-corpus/corpus.json"" rel=""nofollow noreferrer"">https://github.com/axa-group/nlp.js/blob/master/examples/14-ner-corpus/corpus.json</a></p>
<p><a href=""https://github.com/axa-group/nlp.js/blob/master/examples/14-ner-corpus/heros.json"" rel=""nofollow noreferrer"">https://github.com/axa-group/nlp.js/blob/master/examples/14-ner-corpus/heros.json</a></p>
<p>I hope someone can give me a pointer at what i am doing wrong here.</p>
",Dataset Preprocessing & Handling,use contextdata node nlp properly currently trying create chatbot adding contextdata like example tried several variant configs filename file path etc even looked source code contextdata corpus get read doe end running code real name spiderman hero part get replaced data entity hero option realname code currently look like corpus file use linked example hope someone give pointer wrong
Analysing the frequency of Job titles in a corpus,"<p>I have a set of data that contains brief job descriptions and job titles. I am trying to analyse this data to understand the frequency of job titles in the data.</p>
<p>The data looks a little like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Name</th>
<th>Job</th>
</tr>
</thead>
<tbody>
<tr>
<td>John</td>
<td>Digital Product Lead</td>
</tr>
<tr>
<td>Jane</td>
<td>Account Manager, Head of Workplace Experience</td>
</tr>
<tr>
<td>Bill</td>
<td>Executive Assistant at EY</td>
</tr>
<tr>
<td>Tom</td>
<td>Senior Recruiter, People Experience, Talent Branding #EX #DX</td>
</tr>
</tbody>
</table>
</div>
<p>Within the Job variable, there are strings we are not interested in.</p>
<p>One way I have analysed the data is to use simple NLP to create a data frame that contains a word and its frequency within the corpus. I have used the following R code:</p>
<pre><code>text &lt;- df$job

docs &lt;- VCorpus(VectorSource(text))

docs &lt;- docs %&gt;%
  tm_map(removeNumbers) %&gt;%
  tm_map(removePunctuation) %&gt;%
  tm_map(stripWhitespace)

docs &lt;- tm_map(docs, content_transformer(tolower))
docs &lt;- tm_map(docs, removeWords, stopwords(&quot;english&quot;))

dtm &lt;- TermDocumentMatrix(docs) 
matrix &lt;- as.matrix(dtm) 
words &lt;- sort(rowSums(matrix),decreasing=TRUE) 
JobFreq &lt;- data.frame(word = names(words),freq=words)
</code></pre>
<p>This returns data that looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Word</th>
<th>Freq</th>
</tr>
</thead>
<tbody>
<tr>
<td>Senior</td>
<td>1656</td>
</tr>
<tr>
<td>Analyst</td>
<td>798</td>
</tr>
</tbody>
</table>
</div>
<p>The returned data is close to what I am looking for, but these words become out of context due to the nature of most job titles.</p>
<p>Is there a way to insert a corpus of common job titles so that when two (or more) strings in the data match a job in the corpus, they are not tokenised and so are treated as one? And if so, how would I implement this using R?</p>
<p>Thanks</p>
",Dataset Preprocessing & Handling,analysing frequency job title corpus set data contains brief job description job title trying analyse data understand frequency job title data data look little like name job john digital product lead jane account manager head workplace experience bill executive assistant ey tom senior recruiter people experience talent branding ex dx within job variable string interested one way analysed data use simple nlp create data frame contains word frequency within corpus used following r code return data look like word freq senior analyst returned data close looking word become context due nature job title way insert corpus common job title two string data match job corpus tokenised treated one would implement using r thanks
What is Key Error 111 In sentence transformers?,"<p>I am working on product matching projects by extracting semantic text in the Data Frame.
I used Sentence Transformers lot of times for encoding the text but now i am getting the problem like Key Error: 111.
I tried so many ways but i am not able to solve it</p>
<pre><code>a = model.encode(H['Text_HE'], convert_to_tensor=True)
b = model.encode(DN['Text_DNE'], convert_to_tensor=True)
c = model.encode(TH['Text_TE'], convert_to_tensor=True)
d = model.encode(TH['Text_TE'], convert_to_tensor=True)
</code></pre>
<p><a href=""https://i.sstatic.net/Vo0ft.png"" rel=""nofollow noreferrer"">About the Key Error:111.
</a></p>
",Dataset Preprocessing & Handling,key error sentence transformer working product matching project extracting semantic text data frame used sentence transformer lot time encoding text getting problem like key error tried many way able solve key error
Processing large text files in R (Speed up a loop for separating sentences),"<p>I have large text documents (150K lines per document; 160 documents). Then I read them in as a large VCorpus and convert them to a dataframe it runs quite quickly. Now, I want to separate each sentence in rows and remove those that do not contain a certain keyword. Then I run the code R crashes. If I try it with one document the code runs for approximately 10 minutes.</p>
<p>The data (text) looks like this (just with much more text):</p>
<p>| Company1   | Company1 hat engaged in child safety in South Africa. The Community rewards this with great recognition. Charity  is a big part of the companies culture.</p>
<p>| Company2  | Company2 opened up several factories in the that do not pay the minimum wage. Affordable  Housing is not one of thair priorities.</p>
<p>There is also a small example structure below.</p>
<pre><code>library(readr)
library(qdap)
library(tm)

corp &lt;- VCorpus(DirSource(&quot;test&quot;))

text &lt;- as.data.frame(corp)
names(text)[1] &lt;- 'doc_id'
names(text)[2] &lt;- 'text'

text &lt;- dput(text)

text &lt;-  structure(list(doc_id = c(&quot;Company1.txt&quot;, &quot;Company2.txt&quot;, &quot;Company3.txt&quot;
), text = c(&quot;Acucap's market cap on listing in 2002 was a meagre R372m, which would make it one of the smallest stocks in the real estate sector in today's terms. This may be a reflection of how the real estate sector of the JSE has evolved over the past 10 years. But, it is also amazing that Acucap - at a current market cap of more than R7bn - has managed to retain much of the same level of entrepreneurial spirit as at the time of its listing. Still at the helm is sector veteran Paul Theodosiou. The team has grown since listing. The core operational supporting team is Jonathan Rens, Gavin Jones and Craig Kotze, and the finance team is led by founding chief financial officer Baden Marlow. Acucap's physical portfolio is concentrated in retail, which comprises 74%, offices make up 24% and industrial takes the remaining 2%. Some of the larger retail properties include Festival Mall (the largest single asset in the portfolio) in Kempton Park, Bayside Mall in Tableview and Keywest shopping centre in Krugersdorp. The office portfolio includes assets like Mowbray Golf Park in Pinelands in the Cape, the Microsoft offices in Bryanston and 82 Grayston Drive in the Sandton CBD. Though small in Acucap's portfolio, the industrial portfolio is of a high quality and offers good scope for growth as expansion continues at the N1 Business Park in Midrand and the Montague Business Park in Cape Town. Acucap's other investments include a 17,2% shareholding in Sycom Property Fund (a fellow listed real estate property company) as well as a 100% shareholding in Sycom's management company. Sycom's other significant shareholder is Hyprop Investments. The latter's 33,9% stake in Sycom has been the subject of much speculation over the past few years as the ownership structure has created somewhat of an impasse. Acucap has, however, stated its strategic intent to retain Sycom as a separate fund. Company1 hat engaged in child safety in South Africa. The Community rewards this with great recognition. Charity is a big part of the companies culture.&quot;, 
&quot;Australian copper producer Aditya Birla Minerals Ltd. said June 16 that there is a risk that the overall reserves at its Nifty copper mine in Western Australia may be adversely affected due to the effects of an earlier ground collapse at the mine. The company also said it will be unable to provide its annual reserve update until all investigative activities are finalized. Aditya Birla is expecting site costs for the June quarter to be in the range of A$17 million to A$19 million, higher than the earlier estimate of A$12 million to A$15 million due to higher-than-expected mine activity, unplanned maintenance on critical infrastructure and additional employee costs. The company was forced to halt operations at the Nifty mine in March following suspected underground subsidence, which led to the standing down of 350 mine employees until at least July 15. Phase two of probe drilling, which is being undertaken as part of efforts to assess mine safety, is progressing ahead of schedule and is now expected to be completed by the end of June instead of mid-July. Aditya Birla said preliminary observations from this second phase are similar to those from phase one drilling, in that there is less water being intersected between levels 16 and 20 than previously expected. Areas with potential to self-propagate into new sinkholes are being reviewed and, as Aditya Birla expected, the rock mass strength has deteriorated on top of mined-out areas. Management has begun an initial review of the results to identify new gaps and/or the need for additional confirmatory drilling. The first phase of the seismic system installation has been commissioned and is fully functional, albeit limited in coverage to upper parts of the mine. The pit has been dewatered with residual mud left at the pit sump. Aditya Birla added that while surface cracking has continued to develop in the area affected by the sinkhole, this was expected and does not present a hazard. However, washouts will occur after heavy rainfall, leading to widening and deepening of the cracks.&quot;, 
&quot;In preparation for the potential completion of the merger with AGL Resources Inc., the board of directors of Nicor Inc. announced a special pro rata dividend. The dividend is contingent upon the merger being completed prior to Nicor's next scheduled dividend record date, Dec. 31, to ensure that \&quot;shareholders continue to receive a dividend at the current rate until the closing of the merger,\&quot; the company said. In a Nov. 1 news release, Nicor said its board of directors declared a pro rata dividend of 0.5 cent per share per day from Oct. 1 until and including the day before the merger effective date. The dividend is the daily equivalent of the current quarterly dividend rate of 46.5 cents per share. It will be paid to Nicor shareholders of record at the close of business on the day immediately before the effective merger date, which is expected in the fourth quarter. \&quot;The dividend will be paid as soon as practical following the completion of the merger,\&quot; Nicor said. \&quot;This pro rata dividend is in addition to the previously announced Nov. 1 dividend that was paid to shareholders of record Sept. 30. Following the merger closing, AGL Resources is expected to pay a dividend at the rate of $0.004945055 per share, per day for the remainder of the current quarterly dividend period. These dividend payment scenarios, together with a similar plan announced today by AGL Resources, will synchronize the companies' dividends as of the merger effective date in accordance with the merger agreement.\&quot; If the merger is not completed by Dec. 31, Nicor shareholders of record Dec. 31 will receive the regular quarterly dividend of 46.5 cents per share, payable Feb. 1, 2012, and a new pro rata dividend will be announced to ensure that shareholders receive a dividend at the current rate until the merger is completed. Company2 opened up several factories in the that do not pay the minimum wage. Affordable Housing is not one of thair priorities.&quot;
)), class = &quot;data.frame&quot;, row.names = c(NA, 3L))

options(stringsAsFactors = FALSE)
Sys.setlocale('LC_ALL','C')

library(dplyr)
library(tidyr)

sckeywords &lt;- c(&quot;Affordable Housing&quot;, &quot;Benefit The Masses&quot;,
                &quot;Charitability&quot;, &quot;Charitable&quot;, &quot;Charitably&quot;, &quot; Charities &quot;, &quot; Charity &quot;)
pat &lt;- paste0(sckeywords, collapse = '|')

text2 &lt;- (text) %&gt;%
  separate_rows(text, sep = '\\.\\s*') %&gt;%
  slice({
    tmp &lt;- grep(pat, text, ignore.case = TRUE)
    sort(unique(c(tmp-1, tmp, tmp + 1)))
  })
</code></pre>
<p>Can I make it run faster somehow without needing more hardware capacity?
I have 16 GB of RAM and a 4 core CPU (i5-10210U).</p>
",Dataset Preprocessing & Handling,processing large text file r speed loop separating sentence large text document k line per document document read large vcorpus convert dataframe run quite quickly want separate sentence row remove contain certain keyword run code r crash try one document code run approximately minute data text look like much text company company hat engaged child safety south africa community reward great recognition charity big part company culture company company opened several factory pay minimum affordable housing one thair priority also small example structure make run faster somehow without needing hardware capacity gb ram core cpu u
How to replace multiple strings in a class list in python?,"<p>I have a list of tweet that contains hundreds of tweets. I want to replace the nonstandard words in the list of tweets with their standard equivalents by looping through the tweets.</p>
<p>I used this code to read the file containing the standard words</p>
<pre><code>dk = open('Aword.txt','r')
dlist = []
for x in dk.readlines():
    dlist.append(x.replace('\n',''))

dlist
</code></pre>
<p>then I use this code to print words that are not in the list</p>
<pre><code>standard = dlist
tweets = df


for idx, tweet in enumerate(tweets):
    for word in tweet.split():
        if word.lower() in equivalences:
            tweet = tweet.replace(word, equivalences[word.lower()])
    tweets[idx] = tweet

print(tweets)
</code></pre>
<p>this is what the dataframe containing the tweets looks like:
<a href=""https://i.sstatic.net/g5m9R.png"" rel=""nofollow noreferrer"">tweets</a></p>
<p>this is what the dict containing the standard words looks like:
<a href=""https://i.sstatic.net/X02kb.png"" rel=""nofollow noreferrer"">dlist</a></p>
<p>but it is limited to printing only the first five strings, I'm looking for a way to print all of the strings without limititations (flexible to the numbers of strings in each tweets). Thank you for your help:)</p>
",Dataset Preprocessing & Handling,replace multiple string class list python list tweet contains hundred tweet want replace nonstandard word list tweet standard equivalent looping tweet used code read file containing standard word use code print word list dataframe containing tweet look like tweet dict containing standard word look like dlist limited printing first five string looking way print string without limititations flexible number string tweet thank help
Using parsnip to call multinomial_naive_bayes,"<p>I want to use tidymodels to build a workflow for an NLP problem. I have a basic flow built in the traditional way using the <code>naivebayes</code> package, which basically feeds a document-term matrix (counts of terms occurring in each document) to the <code>multinomial_naive_bayes</code> function.</p>
<p>While there is a <code>parsnip</code> <a href=""https://parsnip.tidymodels.org/reference/details_naive_Bayes_naivebayes.html"" rel=""nofollow noreferrer"">interface for the naivebayes package</a> it only seems to work with the generic <code>naive_bayes</code> function. According to the <a href=""https://cran.r-project.org/web/packages/naivebayes/naivebayes.pdf"" rel=""nofollow noreferrer"">naivebayes documentation</a> it seems to be the only format that can't be accessed through the generic function:</p>
<blockquote>
<p>Please note that the Multinomial Naive Bayes is not available through the naive_bayes function.</p>
</blockquote>
<p>So... my 3 questions are:</p>
<ol>
<li>Is there a way to access the <code>multinomial_naive_bayes</code> function using <code>parsnip</code>?</li>
<li>Is there a way to use the generic <code>naive_bayes</code> function with data in this format (counts of features)?</li>
<li>What's the best alternative? I see <code>parsnip</code> also supports <code>h2o</code> and <code>klaR</code> but I'm not familiar with those packages.</li>
</ol>
<p>I'm expecting the answers to Qs 1 &amp; 2 are &quot;no&quot;, but worth checking. Advice on Q3 would be welcome.</p>
",Dataset Preprocessing & Handling,using parsnip call multinomial naive bayes want use tidymodels build workflow nlp problem basic flow built traditional way using package basically feed document term matrix count term occurring document function interface naivebayes package seems work generic function according naivebayes documentation seems format accessed generic function please note multinomial naive bayes available naive bayes function question way access function using way use generic function data format count feature best alternative see also support familiar package expecting answer q worth checking advice q would welcome
Upload text document in R,"<p>I am trying to upload several text document into a data frame in R. My desired output is a matrix with two colums:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>DOCUMENT</th>
<th>CONTENT</th>
</tr>
</thead>
<tbody>
<tr>
<td>Document A</td>
<td>This is the content.</td>
</tr>
<tr>
<td>: ----</td>
<td>: -------:</td>
</tr>
<tr>
<td>Document B</td>
<td>This is the content.</td>
</tr>
<tr>
<td>: ----</td>
<td>: -------:</td>
</tr>
<tr>
<td>Document C</td>
<td>This is the content.</td>
</tr>
</tbody>
</table>
</div>
<p>Within the column &quot;CONTENT&quot;, all the text information from the text document (10-K report) shall be shown.</p>
<pre><code>&gt; setwd(&quot;C:/Users/folder&quot;)
&gt; folder &lt;- getwd()
&gt; corpus &lt;- Corpus(DirSource(directory = folder, pattern = &quot;*.txt&quot;))
</code></pre>
<p>This will create a corpus and I can tokenize it. But I don't achieve to convert to a data frame nor my desiret output.</p>
<p>Can somebody help me?</p>
",Dataset Preprocessing & Handling,upload text document r trying upload several text document data frame r desired output matrix two colums document content document content document b content document c content within column content text information text document k report shall shown create corpus tokenize achieve convert data frame desiret output somebody help
How to remove Rows that have English (or a specific languages) sentences in pandas,"<p>I have a pandas data frame which has 2 columns, first contains Arabic sentences and the second one contain labels (1,0)</p>
<p>I want to remove all rows that contain English sentences.</p>
<p>Any suggestions?</p>
<p>Here is an example, I want to remove the second row</p>
<blockquote>
<p>إيطاليا لتسريع القرارات اللجوء المهاجرين، الترحيل [0]</p>
<p>Border Patrol Agents Recover 44 Migrants from Stash House [0]</p>
<p>الديمقراطيون مواجهة الانتخابات رفض عقد اجتماعات تاون هول  [0]</p>
<p>شاهد لايف: احتفال ترامب &quot;اجعل أمريكا عظيمة مرة أخرى&quot; - بريتبارت   [0]</p>
<p>المغني البريطاني إم آي إيه: إدارة ترامب مليئة بـ &quot;كذابون باثولوجي&quot;    [0]</p>
</blockquote>
",Dataset Preprocessing & Handling,remove row english specific language sentence panda panda data frame ha column first contains arabic sentence second one contain label want remove row contain english sentence suggestion example want remove second row border patrol agent recover migrant stash house
How to do sentiment analysis with topic modeling or NER [ Python]?,"<p>I have the following code for sentiment analysis. I was wondering how can I include topic modeling or NER within it? (the dataset is of customers' reviews of 3 websites, a csv file of 2 columns, one the reviews and one the rating of 0 as negative and 1 for positive)</p>
<pre><code> from nltk.corpus import stopwords
    from nltk.stem.porter import PorterStemmer
    
    dataset = pd.read_csv('full_db.csv') 
    X = dataset.iloc[:,0].values
    y = dataset.iloc[:, 1].values
    
    corpus = []
    
    for i in range(0, len(X)):
        review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])  #replace punctuations with space
        review = review.lower()  #transfering all the letters to lower-case
        review = review.split()  #spliting the review into words
        #apply stemming
        ps = PorterStemmer()
        all_stopwords = stopwords.words('english')
        no_stopwords = [&quot;not&quot;,&quot;don't&quot;,'aren','don','ain',&quot;aren't&quot;, 'couldn', &quot;couldn't&quot;, &quot;wasn't&quot;]
        for Nostopword in no_stopwords:
            all_stopwords.remove(Nostopword)
        review = [ps.stem(word) for word in review if not word in set(all_stopwords)] 
        review = ' '.join(review)
        corpus.append(review)

    #Splitting the dataset into Training set and Test set
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)
    
    #logistic regression
    # Initialize a logistic regression model 
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import confusion_matrix, accuracy_score,classification_report
    logistic = LogisticRegression(random_state=42, solver='lbfgs',
                                multi_class='multinomial')
    # Train the model
    logistic = logistic.fit(X_train, y_train)
    y_pred = logistic.predict(X_test)
</code></pre>
",Dataset Preprocessing & Handling,sentiment analysis topic modeling ner python following code sentiment analysis wa wondering include topic modeling ner within dataset customer review website csv file column one review one rating negative positive
How to split a conversation on WhatsApp in multiple blocks based on the context?,"<p>Let's imagine I download a csv file that includes all the conversations I had with a friend for the past 6 months (WhatsApp chat). I would like to divide that csv file in multiple &quot;blocks&quot; (each block defines a different conversation). Eg:</p>
<p>Day 1:</p>
<ul>
<li>U1: Hey, how's going?</li>
<li>U2: Fine! Any plan for tomorrow?</li>
<li>U1: Nope</li>
</ul>
<p>Day 2:</p>
<ul>
<li>U2: Hello!</li>
</ul>
<p>Day 3:</p>
<ul>
<li>U1: Morning!</li>
<li>U2: ....</li>
</ul>
<p>So the idea is to identify that in my WhatsApp Chat, if we follow the example I have provided, there should be 3 blocks of different conversations, two initiated by U1, and one initiated by U2.</p>
<p>I cannot split it by time because some of the users could take long enough to reply the previous message. So it seems I should be able to identify if the new sentence that appears in the chat is related to the previous &quot;block&quot; of conversation or if it is actually starting a new block.</p>
<p>Any ideas of what steps I need to follow if I want to identify different conversations in one chat, or if a sentence is continuing the previous conversation/starting a new one?</p>
<p>Thanks!!</p>
",Dataset Preprocessing & Handling,split conversation whatsapp multiple block based context let imagine download csv file includes conversation friend past month whatsapp chat would like divide csv file multiple block block defines different conversation eg day u hey going u fine plan tomorrow u nope day u hello day u morning u idea identify whatsapp chat follow example provided block different conversation two initiated u one initiated u split time user could take long enough reply previous message seems able identify new sentence appears chat related previous block conversation actually starting new block idea step need follow want identify different conversation one chat sentence continuing previous conversation starting new one thanks
Load Pretrained glove vectors in python,"<p>I have downloaded pretrained glove vector file from the internet. It is a .txt file. I am unable to load and access it. It is easy to load and access a word vector binary file using gensim but I don't know how to do it when it is a text file format.</p>
",Dataset Preprocessing & Handling,load pretrained glove vector python downloaded pretrained glove vector file internet txt file unable load access easy load access word vector binary file using gensim know text file format
How to avoid reloading ML model every time when I call python script?,"<p>I have two files, <code>file1.py</code> which have ML model size of 1GB and <code>file2.py</code> which calls <code>get_vec()</code> method from file1 and receives vectors in return. ML <code>model</code> is being loaded everytime when file1 get_vec() method is called. This is where it is taking lots of time (around 10s) to load the model from disk.</p>

<p>I want to tell file1 somehow not to reload model every time but utilize loaded model from earlier calls.</p>

<p>Sample code is as follows</p>

<pre><code># File1.py

import spacy
nlp = spacy.load('model')

def get_vec(post):
    doc = nlp(post)
    return doc.vector

</code></pre>

<pre><code>File2.py

from File1 import get_vec

df['vec'] = df['text'].apply(lambda x: get_vec(x))

</code></pre>

<p>So here, it is taking 10 to 12 seconds in each call. This seems small code but it is a part of a large project and I can not put both in the same file. </p>

<p><strong>Update1:</strong></p>

<p>I have done some research and came to know that I can use Redis to store model in cache first time it runs and thereafter I can read the model from cache directly. I tried it for testing with Redis as follows</p>

<pre><code>import spacy
import redis

nlp = spacy.load('en_core_web_lg')
r = redis.Redis(host = 'localhost', port = 6379, db = 0)
r.set('nlp', nlp)
</code></pre>

<p>It throws an error</p>

<pre><code>DataError: Invalid input of type: 'English'. Convert to a bytes, string, int or float first.
</code></pre>

<p>Seems, <code>type(nlp)</code> is <code>English()</code> and it need to convert in a suitable format. So I tried to use pickle as well to convert it. But again, pickle is taking lots of time in encoding and decoding. Is there anyway to store this in Redis?</p>

<p>Can anybody suggest me how can I make it faster? Thanks.</p>
",Dataset Preprocessing & Handling,avoid reloading ml model every time call python script two file ml model size gb call method file receives vector return ml loaded everytime file get vec method called taking lot time around load model disk want tell file somehow reload model every time utilize loaded model earlier call sample code follows taking second call seems small code part large project put file update done research came know use redis store model cache first time run thereafter read model cache directly tried testing redis follows throw error seems need convert suitable format tried use pickle well convert pickle taking lot time encoding decoding anyway store redis anybody suggest make faster thanks
read full review from the dataset with the specific word that shown in wordcloud,"<p>Hy! I am finding for the reviews from dataset related to those specific words that are shown in wordCloud
here is my code for visualizing  and counting for words</p>
<h1><strong>visualize wordcloud with removing stopwords</strong></h1>
<pre><code>stopwords=set(STOPWORDS)
stopwords.update([&quot;car&quot;,&quot;vehicle&quot;,&quot;time&quot;,&quot;buy&quot;,&quot;new&quot;,&quot;infiniti&quot;,])
wordcloud_spam=WordCloud(stopwords=stopwords,background_color=&quot;white&quot;).generate(negitive_reviews_str)
plt.figure(figsize=(10,10))
plt.imshow(wordcloud_spam, interpolation = 'bilinear')
plt.axis(&quot;off&quot;)
plt.show()
</code></pre>
<h1><strong>count first 10 words from cloud</strong></h1>
<pre><code>filtered_word_spam=[word for word in negitive_reviews_str.split()if word not in stopwords]
counted_word_spam=collections.Counter(filtered_word_spam)
word_count_spam={}
for letter, count in counted_word_spam.most_common(10):
    word_count_spam[letter]=count
for i,j in word_count_spam.items():
    print('Word: {0}, count: {1}'.format(i,j))
</code></pre>
",Dataset Preprocessing & Handling,read full review dataset specific word shown wordcloud hy finding review dataset related specific word shown wordcloud code visualizing counting word visualize wordcloud removing stopwords count first word cloud
Calculate TF-IDF using sklearn for n-grams in python,"<p>I have a vocabulary list that include n-grams as follows. </p>

<pre><code>myvocabulary = ['tim tam', 'jam', 'fresh milk', 'chocolates', 'biscuit pudding']
</code></pre>

<p>I want to use these words to calculate TF-IDF values.</p>

<p>I also have a dictionary of corpus as follows (key = recipe number, value = recipe).</p>

<pre><code>corpus = {1: ""making chocolates biscuit pudding easy first get your favourite biscuit chocolates"", 2: ""tim tam drink new recipe that yummy and tasty more thicker than typical milkshake that uses normal chocolates"", 3: ""making chocolates drink different way using fresh milk egg""}
</code></pre>

<p>I am currently using the following code.</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(vocabulary = myvocabulary, stop_words = 'english')
tfs = tfidf.fit_transform(corpus.values())
</code></pre>

<p>Now I am printing tokens or n-grams of the recipe 1 in <code>corpus</code> along with the tF-IDF value as follows.</p>

<pre><code>feature_names = tfidf.get_feature_names()
doc = 0
feature_index = tfs[doc,:].nonzero()[1]
tfidf_scores = zip(feature_index, [tfs[doc, x] for x in feature_index])
for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:
  print(w, s)
</code></pre>

<p>The results I get is <code>chocolates 1.0</code>. However, my code does not detect n-grams (bigrams) such as <code>biscuit pudding</code> when calculating TF-IDF values. Please let me know where I make the code wrong.</p>

<p>I want to get the TD-IDF matrix for <code>myvocabulary</code> terms by using the recipe documents in the <code>corpus</code>. In other words, the rows of the matrix represents <code>myvocabulary</code> and the columns of the matrix represents the recipe documents of my <code>corpus</code>. Please help me.</p>
",Dataset Preprocessing & Handling,calculate tf idf using sklearn n gram python vocabulary list include n gram follows want use word calculate tf idf value also dictionary corpus follows key recipe number value recipe currently using following code printing token n gram recipe along tf idf value follows result get however code doe detect n gram bigram calculating tf idf value please let know make code wrong want get td idf matrix term using recipe document word row matrix represents column matrix represents recipe document please help
lemmatizing verbs in SVOs,"<p><em>I've looked at the suggested similar questions, and I think that this question has enough specificity that it warrants being asked, but I am completely okay if someone can point to an already answered question that solves my problem.</em></p>
<p>I have a corpus of texts which I have rendered as (textacy) SVOs, and then saved to a data frame, only to discover that it would be better if the verbs in question were lemmatized so that &quot;talk&quot;, &quot;talked&quot;, &quot;was talking&quot;, &quot;is talking&quot;, etc. are rendered as the same verb and not 4 or more different verbs.</p>
<p>The current code grabs the texts out of a data frame and places them in a list:</p>
<pre class=""lang-py prettyprint-override""><code>texts_women = talks_f.text.tolist()
texts_w = [text.lower() for text in texts_women]
</code></pre>
<p>Then it creates a spaCy pipe and runs the texts through it:</p>
<pre class=""lang-py prettyprint-override""><code>nlp = spacy.load('en_core_web_sm')
docs_w = list(nlp.pipe(texts_w))
</code></pre>
<p>I had written the following code before realizing that the verbs could use being normalized:</p>
<pre class=""lang-py prettyprint-override""><code>def createSVOs(doc, svo_list):
    # Create the list of tuples for the document
    svotriples = list(textacy.extract.triples.subject_verb_object_triples(doc))
    # Convert to list of dictionaries
    for item in svotriples:
        svo_list.append(
            {
                'subject': item[0][-1], 
                'verb': item[1][-1], 
                'object': item[2]
            }
        )
</code></pre>
<p>Originally we had converted the dictionary values to strings -- `'subject': str(item[0][-1]) -- and then this list of dictionaries was converted to a pandas dataframe, where we do a number of other things.</p>
<p>Backtracking into the code, I first tried lemmatizing the df['verb'] column, with a variety of errors depending on what I tried, with most of them being:</p>
<pre class=""lang-py prettyprint-override""><code>TypeError: 'spacy.tokens.token.Token' object is not subscriptable
</code></pre>
<p>I eventually decided to try to lemmatize the docs before I fed them into the textacy SVO creator, but then I get:</p>
<pre class=""lang-py prettyprint-override""><code>AttributeError: 'str' object has no attribute 'sents'
</code></pre>
<p>Is it possible to get both SVOs and lemmas? That is, an SVO where the V, verb, is lemmatized? What am I missing here?</p>
",Dataset Preprocessing & Handling,lemmatizing verb svos looked suggested similar question think question ha enough specificity warrant asked completely okay someone point already answered question solves problem corpus text rendered textacy svos saved data frame discover would better verb question lemmatized talk talked wa talking talking etc rendered verb different verb current code grab text data frame place list creates spacy pipe run text written following code realizing verb could use normalized originally converted dictionary value string subject str item list dictionary wa converted panda dataframe number thing backtracking code first tried lemmatizing df verb column variety error depending tried eventually decided try lemmatize doc fed textacy svo creator get possible get svos lemma svo v verb lemmatized missing
Could not read meta.json,"<p>I'm making a resume parser and facing an error.</p>
<p>Parser_1:</p>
<pre><code>import os
import multiprocessing as mp
import io
import spacy
import pprint
from spacy.matcher import Matcher
import utils



class ResumeParser(object):

    def __init__(
        self,
        resume,
        skills_file=None,
        custom_regex=None
    ):
        nlp = spacy.load(r&quot;C:\Users\Sauravi\env\Lib\site-packages\en_core_web_sm\en_core_web_sm-3.0.0&quot;)
        custom_nlp = spacy.load(os.path.dirname(os.path.abspath(__file__)))
        self.__skills_file = skills_file
        self.__custom_regex = custom_regex
        self.__matcher = Matcher(nlp.vocab)
        self.__details = {
            'name': None,
            'email': None,
            'mobile_number': None,
            'address': None,
            'skills': None,
            'college_name': None,
            'projects': None,
            'designation': None,
            'experience': None,
            'company_names': None,
            'no_of_pages': None,
            'total_experience': None,
        }
        self.__resume = resume
        if not isinstance(self.__resume, io.BytesIO):
            ext = os.path.splitext(self.__resume)[1].split('.')[1]
        else:
            ext = self.__resume.name.split('.')[1]
        self.__text_raw = utils.extract_text(self.__resume, '.' + ext)
        self.__text = ' '.join(self.__text_raw.split())
        self.__nlp = nlp(self.__text)
        self.__custom_nlp = custom_nlp(self.__text_raw)
        self.__noun_chunks = list(self.__nlp.noun_chunks)
        self.__get_basic_details()
</code></pre>
<p>Error:</p>
<p><a href=""https://i.sstatic.net/HG6uD.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>How do I solve this error?
I installed en_core_web_sm using <a href=""https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz"" rel=""nofollow noreferrer"">https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz</a></p>
",Dataset Preprocessing & Handling,could read meta json making resume parser facing error parser error enter image description solve error installed en core web sm using
Prepare json file for GPT,"<p>I would like to create a dataset to use it for fine-tuning GPT3. As I read from the following site <a href=""https://beta.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">https://beta.openai.com/docs/guides/fine-tuning</a>, the dataset should look like this</p>
<pre><code>{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
...
</code></pre>
<p>For this reason I am creating the dataset with the following way</p>
<pre><code>import json

# Data to be written
dictionary = {
    &quot;prompt&quot;: &quot;&lt;text1&gt;&quot;, &quot;completion&quot;: &quot;&lt;text to be generated1&gt;&quot;}, {
    &quot;prompt&quot;: &quot;&lt;text2&gt;&quot;, &quot;completion&quot;: &quot;&lt;text to be generated2&gt;&quot;}

with open(&quot;sample2.json&quot;, &quot;w&quot;) as outfile:
    json.dump(dictionary, outfile)
</code></pre>
<p>However, when I am trying to load it, it looks like this which is not as we want</p>
<pre><code>import json
 
# Opening JSON file
with open('sample2.json', 'r') as openfile:
 
    # Reading from json file
    json_object = json.load(openfile)
 
print(json_object)
print(type(json_object))

&gt;&gt; [{'prompt': '&lt;text1&gt;', 'completion': '&lt;text to be generated1&gt;'}, {'prompt': '&lt;text2&gt;', 'completion': '&lt;text to be generated2&gt;'}]
&lt;class 'list'&gt;
</code></pre>
<p><strong>Could you please let me know how can I face this problem?</strong></p>
",Dataset Preprocessing & Handling,prepare json file gpt would like create dataset use fine tuning gpt read following site dataset look like reason creating dataset following way however trying load look like want could please let know face problem
"When doing biterm topic modeling, how can I get the top documents for a topic returned in the original format?","<p>I literally don't know anything about coding, but I am in a situation, and I need help. I am topic modeling on a bunch of short text (comments) and I would like for the topic model to return the top documents and currently I have an output like this at the part where it says &quot;Interactive report interface&quot;:</p>
<p><a href=""https://tmplot.readthedocs.io/en/latest/tutorial.html#Visualization"" rel=""nofollow noreferrer"">https://tmplot.readthedocs.io/en/latest/tutorial.html#Visualization</a></p>
<p>I understand what I am looking at, and I see that I have some version of the top documents for each topic, but  the &quot;top documents&quot; don't have stop words or anything, so they just seem like gibberish, and I want to read the original comments and export these original comments into a data file.</p>
<p>I am going insane for this simple thing and I don't know how to even start. I have tried everything and nothing. Please someone just give me the very simple answer, some code I can copy and paste. I don't need to understand how it works. I just want to be able to read the original documents. ANd if you can't give me the code please tell me how to learn and where I can read how to get the original top documents.</p>
",Dataset Preprocessing & Handling,biterm topic modeling get top document topic returned original format literally know anything coding situation need help topic modeling bunch short text comment would like topic model return top document currently output like part say interactive report interface understand looking see version top document topic top document stop word anything seem like gibberish want read original comment export original comment data file going insane simple thing know even start tried everything nothing please someone give simple answer code copy paste need understand work want able read original document give code please tell learn read get original top document
How to average of n vectorized vectors with tdfid vectorizer.transform?,"<p>First I vectorized all the documents with <code>docs_vectorized = vectorizer.fit_transform(docs['content'])</code> then let's assume I have different vectorized vectors:</p>
<pre><code>df = docs[docs['category'] == 'category1']
a = vectormaker.transform(df['content'])
df = docs[docs['category'] == 'category2']
b = vectormaker.transform(df['content'])
</code></pre>
<p>How can I make a new vector that is average of a and b keeping the same type? Assuming that both are csr_matrix.</p>
",Dataset Preprocessing & Handling,average n vectorized vector tdfid vectorizer transform first vectorized document let assume different vectorized vector make new vector average b keeping type assuming csr matrix
Retain original document element index of argument passed through sklearn&#39;s CountVectorizer() in order to access corresponding part of speech tag,"<p>I have a data frame with sentences and the respective part of speech tag for each word (Below is an extract of the data I'm working with (data taken from <a href=""https://nlp.stanford.edu/projects/snli/"" rel=""nofollow noreferrer"">SNLI</a> corpus). For each sentence in my collection I would like to extract unigrams and the corresponding pos-tag of that word.</p>
<p>For instance if I've the following:</p>
<pre><code>vectorizer_unigram = CountVectorizer(analyzer='word', ngram_range=(1, 1), stop_words = 'english')

doc = {'sent' : ['Two women are embracing while holding to go packages .'], 'tags' : ['NUM NOUN AUX VERB SCONJ VERB PART VERB NOUN PUNCT']}

sentence = vectorizer_unigram.fit(doc['sent'])
sentence_unigrams = sentence.get_feature_names_out()

</code></pre>
<p>Then I would get the following unigrams output:</p>
<pre><code>array(['embracing', 'holding', 'packages', 'women'], dtype=object)
</code></pre>
<p>But I don't know how to retain the part of speech tag after this. I tried to do a lookup version with the unigrams, but as they may differ from the words in the sentence (if you for instance do <code>sentence.split(' ')</code>) you don't necessarily get the same tokens. Any suggestions of how I can extract unigrams and retain the corresponding part-of-speech tag?</p>
",Dataset Preprocessing & Handling,retain original document element index argument passed sklearn countvectorizer order access corresponding part speech tag data frame sentence respective part speech tag word extract data working data taken snli corpus sentence collection would like extract unigrams corresponding po tag word instance following would get following unigrams output know retain part speech tag tried lookup version unigrams may differ word sentence instance necessarily get token suggestion extract unigrams retain corresponding part speech tag
Text segmentation: dictionary-based word splitting,"<h2>Background</h2>

<p>Split database column names into equivalent English text to seed a data dictionary. The English dictionary is created from a corpus of corporate documents, wikis, and email. The dictionary (<code>lexicon.csv</code>) is a CSV file with words and probabilities. Thus, the more often someone writes the word ""therapist"" (in email or on a wiki page) the higher the chance of ""therapistname"" splits to ""therapist name"" as opposed to something else. (The lexicon probably won't even include the word rapist.)</p>

<h2>Source Code</h2>

<ul>
<li>TextSegmenter.java @ <a href=""http://pastebin.com/taXyE03L"" rel=""nofollow noreferrer"">http://pastebin.com/taXyE03L</a></li>
<li>SortableValueMap.java @ <a href=""http://pastebin.com/v3hRXYan"" rel=""nofollow noreferrer"">http://pastebin.com/v3hRXYan</a></li>
</ul>

<h2>Data Files</h2>

<ul>
<li>lexicon.csv - <a href=""http://pastebin.com/0crECtXY"" rel=""nofollow noreferrer"">http://pastebin.com/0crECtXY</a></li>
<li>columns.txt - <a href=""http://pastebin.com/EtN9Qesr"" rel=""nofollow noreferrer"">http://pastebin.com/EtN9Qesr</a></li>
</ul>

<h2>Problem (updated 2011-01-03)</h2>

<p>When the following problem is encountered:</p>

<pre><code>dependentrelationship::end depend ent dependent relationship
end=0.86
ent=0.001
dependent=0.8
relationship=0.9
</code></pre>

<p>These possible solutions exist:</p>

<pre><code>dependentrelationship::dependent relationship
dependentrelationship::dep end ent relationship
dependentrelationship::depend ent relationship
</code></pre>

<p>The lexicon contains words with their relative probabilities (based on word frequency): <code>dependent 0.8</code>, <code>end 0.86</code>, <code>relationship 0.9</code>, <code>depend 0.3</code>, and <code>ent 0.001</code>.</p>

<p>Eliminate the solution of <code>dep end ent relationship</code> because <code>dep</code> is not in the lexicon (i.e., 75% word usage), whereas the other two solutions cover 100% of words in the lexicon. Of the remaining solutions, the probability of <code>dependent relationship</code> is <em>0.72</em> whereas <code>depend ent relationship</code> is <em>0.00027</em>. We can therefore select <code>dependent relationship</code> as the correct solution.</p>

<h2>Related</h2>

<ul>
<li><a href=""https://stackoverflow.com/questions/3856630/how-to-separate-words-in-a-sentence-with-spaces"">How to separate words in a &quot;sentence&quot; with spaces?</a></li>
<li><a href=""https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxkanBkZnN0b3JlfGd4OjMzMTEzYTA5NTk3NjFjOTc"" rel=""nofollow noreferrer"">Top Coder - Text Segmentation Presentation 1/2</a></li>
<li><a href=""https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxkanBkZnN0b3JlfGd4OjQ1YmFiZTNhODVjMzY2MmY"" rel=""nofollow noreferrer"">Top Coder - Text Segmentation Presentation 2/2</a></li>
<li><a href=""https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxkanBkZnN0b3JlfGd4OjYyN2I4NTA0OTIxZDNlMGE"" rel=""nofollow noreferrer"">Linear Text Segmentation using Dynamic Programming Algorithm</a></li>
<li><a href=""https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxkanBkZnN0b3JlfGd4OjUxZmFkNzc0MmMzOTc5ZmI"" rel=""nofollow noreferrer"">Dynamic Programming: Segmentation</a></li>
<li><a href=""https://sites.google.com/site/djpdfstore/text-segmentation-02.pdf?attredirects=0&amp;d=1"" rel=""nofollow noreferrer"">Dynamic Programming: A Computational Tool</a></li>
</ul>

<h2>Question</h2>

<p>Given:</p>

<pre><code>// The concatenated phrase or database column (e.g., dependentrelationship).
String concat;

// All words (String) in the lexicon within concat, in left-to-right order; and
// the ranked probability of those words (Double). (E.g., {end, 0.97}
// {dependent, 0.86}, {relationship, 0.95}.)
Map.Entry&lt;String, Double&gt; word;
</code></pre>

<p>How would you implement a routine that generates the most likely solution based on lexicon coverage and probabilities? For example:</p>

<pre><code>for( Map.Entry&lt;String, Double&gt; word : words ) {
  result.append( word.getKey() ).append( ' ' );

  // What goes here?

  System.out.printf( ""%s=%f\n"", word.getKey(), word.getValue() );
}
</code></pre>

<p>Thank you!</p>
",Dataset Preprocessing & Handling,text segmentation dictionary based word splitting background split database column name equivalent english text seed data dictionary english dictionary created corpus corporate document wikis email dictionary csv file word probability thus often someone writes word therapist email wiki page higher chance therapistname split therapist name opposed something else lexicon probably even include word rapist source code textsegmenter java sortablevaluemap java data file lexicon csv column txt problem updated following problem encountered possible solution exist lexicon contains word relative probability based word frequency eliminate solution lexicon e word usage whereas two solution cover word lexicon remaining solution probability whereas therefore select correct solution related top coder text segmentation presentation top coder text segmentation presentation linear text segmentation using dynamic programming algorithm dynamic programming segmentation dynamic programming computational tool question given would implement routine generates likely solution based lexicon coverage probability example thank
How to handle with large dataset in spacy,"<p>I use the following code to clean my dataset and print all tokens (words).</p>
<pre><code>with open(&quot;.data.csv&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as file:
    text = file.read()
text = re.sub(r&quot;[^a-zA-Z0-9ß\.,!\?-]&quot;, &quot; &quot;, text)
text = text.lower()
nlp = spacy.load(&quot;de_core_news_sm&quot;)
doc = nlp(text)
for token in doc:
     print(token.text)
</code></pre>
<p>When I execute this code with a small string it works fine. But when I use a 50 megabyte csv I get the message</p>
<pre><code>Text of length 62235045 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
</code></pre>
<p>When I increase the limit to this size my computer gets hard problems..
How can I fix this? It can't be anything special to want to tokenize this amount of data.</p>
",Dataset Preprocessing & Handling,handle large dataset spacy use following code clean dataset print token word execute code small string work fine use megabyte csv get message increase limit size computer get hard problem fix anything special want tokenize amount data
Delete rows with a certain condition in pandas,"<p>I have a data frame and I want to delete rows that in the column &quot;Phrase&quot;, pattern &quot;___&quot; exists.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Index</th>
<th>PHRASE</th>
<th>Label</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>proposed by the president of the</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>Living ___</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>&quot;Murder, ___ Wrote&quot;</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<pre><code>But Imagin that the data fram has 2,000,000 enteries
</code></pre>
<pre><code>import re

df_clean = pd.DataFrame()
z = 0
y = 0
for i in df_original[&quot;PHRASE&quot;]:
  x = re.search(&quot;___&quot;, i)
  if x:
    y = y + 1
  else:
    df_clean.append([i])
    z = z + 1

this is what I came up with so far, I know it's not right, Does anyone know the answer? (by the way append takes a lot of time)
</code></pre>
",Dataset Preprocessing & Handling,delete row certain condition panda data frame want delete row column phrase pattern exists index phrase label proposed president living murder wrote
Merge CountVectorizer output from 4 text columns back into one dataset,"<p>I have a collection of ~100,000 documents in a dataset with a unique doc_id and four columns containing text (like below). </p>

<p><a href=""https://i.sstatic.net/Q15w1.png"" rel=""nofollow"">original dataset</a></p>

<p>I want to vectorize each of the four text columns individually and then combine all of those features back together to create one large dataset for the purpose of building a model for prediction. I approached the vectorization for each text feature using code like below:</p>

<pre><code>stopwords = nltk.corpus.stopwords.words(""english"")

subject_transformer = CountVectorizer(stop_words=stopwords)
subject_vectorized = subject_transformer.fit_transform(full_docs['subject'])

body_transformer = CountVectorizer(stop_words=stopwords)
body_vectorized = body_transformer.fit_transform(full_docs['body'])

excerpt_transformer = CountVectorizer(stop_words=stopwords)
excerpt_vectorized = excerpt_transformer.fit_transform(full_docs['excerpt'])

regex_transformer = CountVectorizer(stop_words=stopwords)
regex_vectorized = regex_transformer.fit_transform(full_docs['regex'])
</code></pre>

<p>Each vectorization yields a sparse matrix like below where <strong>column one</strong> is the document number, <strong>column two</strong> is the column number (one for each word in the original text column), and the <strong>last column</strong> is the actual count. </p>

<p><a href=""https://i.sstatic.net/CYZpm.png"" rel=""nofollow"">sparse matrix</a></p>

<p>What I want to do is the following:</p>

<ol>
<li>Transpose each sparse matrix to a full dataframe of dimensions nxp where n=number of documents &amp; p=number of words in that corpus</li>
<li>Merge each of these matrices/dataframes back together for the purpose of building a model for prediction</li>
</ol>

<p>I initially tried the following:</p>

<p>regex_vectorized_df = pd.DataFrame(regex_vectorized.toarray())</p>

<p>Then I could merge the four individual dataframes back together. This doesn't work because toarray() is too memory intensive. What is the best way to merge these four sparse matrices into one dataset with one unique line per document?</p>
",Dataset Preprocessing & Handling,merge countvectorizer output text column back one dataset collection document dataset unique doc id four column containing text like original dataset want vectorize four text column individually combine feature back together create one large dataset purpose building model prediction approached vectorization text feature using code like vectorization yield sparse matrix like column one document number column two column number one word original text column last column actual count sparse matrix want following transpose sparse matrix full dataframe dimension nxp n number document p number word corpus merge matrix dataframes back together purpose building model prediction initially tried following regex vectorized df pd dataframe regex vectorized toarray could merge four individual dataframes back together work toarray memory intensive best way merge four sparse matrix one dataset one unique line per document
How can I get specific columns form txt file and save them to new file using python,"<p>I have this txt file <strong>sentences.txt</strong> that contains texts below</p>
<pre><code>a01-000u-s00-00 0 ok 154 19 408 746 1661 89 A|MOVE|to|stop|Mr.|Gaitskell|from
</code></pre>
<p><code>a01-000u-s00-01 0 ok 156 19 395 932 1850 105 nominating|any|more|Labour|life|Peers</code></p>
<p>which contains 10 columns
I want to use the panda's data frame to extract only the file name (at column 0) and corresponding text (column 10) without the <strong>(|)</strong> character
I wrote this code</p>
<pre><code>def load() -&gt; pd.DataFrame:

 df = pd.read_csv('sentences.txt',sep=' ', header=None)
 data = []
 with open('sentences.txt') as infile:
    for line in infile:
        file_name, _, _, _, _, _, _, _, _, text = line.strip().split(' ')
        data.append((file_name, cl_txt(text)))

 df = pd.DataFrame(data, columns=['file_name', 'text'])
 df.rename(columns={0: 'file_name', 9: 'text'}, inplace=True)
 df['file_name'] = df['file_name'].apply(lambda x: x + '.jpg')
 df = df[['file_name', 'text']]
 return df

def cl_txt(input_text: str) -&gt; str:
 text = input_text.replace('+', '-')
 text = text.replace('|', ' ')
 return text

load()
</code></pre>
<p>the error I got</p>
<p>ParserError: Error tokenizing data. C error: Expected 10 fields in line 4, saw 11</p>
<p>where my expected process.txt file results should look like below without \n</p>
<pre><code>a01-000u-s00-00  A MOVE to stop Mr. Gaitskell from
a01-000u-s00-01  nominating any more Labour life Peers
</code></pre>
<p><a href=""https://i.sstatic.net/udNIh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/udNIh.png"" alt=""enter image description here"" /></a></p>
",Dataset Preprocessing & Handling,get specific column form txt file save new file using python txt file sentence txt contains text contains column want use panda data frame extract file name column corresponding text column without character wrote code error got parsererror error tokenizing data c error expected field line saw expected process txt file result look like without n
Find overlap in terms between a pair of documents,"<p>I have a sparse term-document matrix produced by tm's TermDocumentMatrix.</p>
<p>I am trying to write a function that takes two document names and k as its arguments, finds all terms that occur in both documents, sorts that list in descending order by the word count of the term, and returns the top k. Words in each term are separated by underscores (like bob_raids_crops).</p>
<p>Here's a toy example (where I sort by length instead of term wordcount):</p>
<pre><code>library(tm)
library(dplyr)
data(&quot;crude&quot;)
tdm &lt;- TermDocumentMatrix(crude,
                          control = list(removePunctuation = TRUE,
                                         stopwords = TRUE))

df  &lt;- data.frame(term = row.names(tdm), as.matrix(tdm[, c(&quot;127&quot;, &quot;144&quot;)]), row.names = NULL)
df$in.both &lt;- ifelse(df[,2]&gt;0 &amp; df[,3]&gt;0, TRUE, FALSE)
df &lt;- df%&gt;%
  subset(in.both == TRUE) %&gt;%
  arrange(desc(str_length(term))) %&gt;%
  select(term) %&gt;%
  top_n(5,str_length(term))
df
</code></pre>
<p>This returns:</p>
<pre><code>       term
1 companies
2   markets
3    market
4    prices
5    reuter
</code></pre>
<p>I was going to write a function, but am wondering if there is an existing way to do this. If not, can I make the above more efficient (like avoiding data frames)?</p>
",Dataset Preprocessing & Handling,find overlap term pair document sparse term document matrix produced tm termdocumentmatrix trying write function take two document name k argument find term occur document sort list descending order word count term return top k word term separated underscore like bob raid crop toy example sort length instead term wordcount return wa going write function wondering existing way make efficient like avoiding data frame
"Package to compare document semantic similarity (LSA, TF-IDF, Cosine Similarity, Language Models)","<p>I'm looking for a package (any programming language) that I can use on a corpus of 50 documents to perform interdocument similarity testing using various metrics like term frequency-inverse document frequency (<a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">TF-IDF</a>), Okapi best matching (<a href=""https://en.wikipedia.org/wiki/Okapi_BM25"" rel=""nofollow noreferrer"">Okapi-BM25</a>), <a href=""https://en.wikipedia.org/wiki/Language_model"" rel=""nofollow noreferrer"">language models</a> (probability distributions over a sequence of words), <a href=""https://en.wikipedia.org/wiki/Latent_semantic_analysis"" rel=""nofollow noreferrer"">LSA</a>, etc.</p>
<p>As the result, I want a document similarity matrix (i.e. <code>doc1</code> is <code>x</code>% similar to <code>doc2</code> etc.). This is for research purposes, not for production. I specifically want the document similarity matrix as I want to correlate this with human ratings.</p>
",Dataset Preprocessing & Handling,package compare document semantic similarity lsa tf idf cosine similarity language model looking package programming language use corpus document perform interdocument similarity testing using various metric like term frequency inverse document frequency tf idf okapi best matching okapi bm language model probability distribution sequence word lsa etc result want document similarity matrix e similar etc research purpose production specifically want document similarity matrix want correlate human rating
NLP: pre-processing dataset into a new dataset,"<p>I need help with processing an unsorted dataset.  Sry, if I am a complete noob. I never did anything like that before. So as you can see, each conversation is identified by a dialogueID which consists of multiple rows of &quot;from&quot; &amp; &quot;to&quot;, as well as text messages.
I would like to concatenate the text messages from the same sender of a dialogueID to one column and from the receiver to another column. This way, I could have a new csv-file with just [dialogueID, sender, receiver].</p>
<p><a href=""https://i.sstatic.net/L5JDq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/L5JDq.png"" alt=""dataset"" /></a>
the new dataset should look like this
<a href=""https://i.sstatic.net/3wuNB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3wuNB.png"" alt=""new dataset"" /></a></p>
<p>I watched multiple tutorials and really struggle to figure out how to do it. I read in this <a href=""https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas"">9-year-old post</a> that iterating through data frames are not a good idea. Could someone help me out with a code snippet or give me a hint on how to properly do it without overcomplicating things? I thought something like this pseudo code below, but the performance with 1 million rows is not great, right?</p>
<pre><code>while !endOfFile
  for dialogueID in range (0, 1038324)
    if dialogueID+1 == dialogueID and toValue.isnull()
      concatenate textFromPrevRow + &quot; &quot; + textFromCurrentRow
      add new string to table column sender
    else
      add text to column receiver
</code></pre>
",Dataset Preprocessing & Handling,nlp pre processing dataset new dataset need help processing unsorted dataset sry complete noob never anything like see conversation identified dialogueid consists multiple row well text message would like concatenate text message sender dialogueid one column receiver another column way could new csv file dialogueid sender receiver new dataset look like watched multiple tutorial really struggle figure read href post iterating data frame good idea could someone help code snippet give hint properly without overcomplicating thing thought something like pseudo code performance million row great right
How to find similarity score between two rows in a pandas data frame,"<p>I want to find the similarity of given sentences between two rows.</p>
<p>In my sample data frame:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = [f'Sent {str(i)}' for i in range(10)]
df = pd.DataFrame(data=data, columns=['Sentences'])
</code></pre>
<pre><code>  Sentences
0    Sent 0
1    Sent 1
2    Sent 2
3    Sent 3
4    Sent 4
5    Sent 5
6    Sent 6
7    Sent 7
8    Sent 8
9    Sent 9
</code></pre>
<p>I want to find the similarity score between every two sentences for n number of sentences.</p>
<p>Approach #1: Create two new columns, the first one is containing each sentence copied n times (n is the total number of sentences), this creates a row of length $n^2$. The second column would be all the sentences copied n times as well (but in groups) still creating $n^2$ rows.
From here I can get the similarities and put them in just one column.</p>
<p>Approach #2: Create a loop that would iterate over the sentences and create the total $nC2$ similarity scores. (for now I don't know how to do this)</p>
<p>How to do approach #2? Are there better ways to do this?</p>
",Dataset Preprocessing & Handling,find similarity score two row panda data frame want find similarity given sentence two row sample data frame want find similarity score every two sentence n number sentence approach create two new column first one containing sentence copied n time n total number sentence creates row length n second column would sentence copied n time well group still creating n row get similarity put one column approach create loop would iterate sentence create total nc similarity score know approach better way
Create a sentence-term matrix using CountVectorizer(),"<p>I am trying to create a document-term matrix in Python based on the following list of sentences with the help of <code>CountVectorizer()</code>:</p>
<pre class=""lang-py prettyprint-override""><code>tokens_sents = [
    'go local restaraunt yesterday evening try pasta .',
    'expect delicious , eatable .',
    'smell really bad delicious .',
    'never eat pasta restaraunt taste pasta awful']
</code></pre>
<p>I can get the desired outcome by processing sentences one by one (e.g. <code>tokens_sents[0]</code> ...) using <code>vectorizer.fit_transform()</code> as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

vectorizer = CountVectorizer()
X = vectorizer.fit_transform([tokens_sents[0],tokens_sents[1],tokens_sents[2]])   
df_bow_sklearn = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())
df_bow_sklearn.head()
</code></pre>
<p>which produces the result:</p>
<pre class=""lang-none prettyprint-override""><code>bad delicious   eatable evening expect  go  local   pasta   really  restaraunt  smell   try yesterday
0   0   0   0   1   0   1   1   1   0   1   0   1   1
1   0   1   1   0   1   0   0   0   0   0   0   0   0
2   1   1   0   0   0   0   0   0   1   0   1   0   0
</code></pre>
<p>However, I wonder if there is a way to create this matrix by iteration, because usually there is a large number of sentences and simply writing them one by one isn't very convenient.</p>
<p>I tried this, but the iteration only processes the last sentence:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

vectorizer = CountVectorizer()
for i in range (0,len(tokens_sents)):
    X = vectorizer.fit_transform([tokens_sents[i]])
    i=i+1
df_bow_sklearn = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())
df_bow_sklearn.head()
</code></pre>
<p>which produces the result:</p>
<pre class=""lang-none prettyprint-override""><code>awful   eat never   pasta   restaraunt  taste
0   1   1   1   2   1   1
</code></pre>
<p>I am thinking maybe since <code>X</code> is a sparse matrix, there will be a way to append each vector by iteration?</p>
",Dataset Preprocessing & Handling,create sentence term matrix using countvectorizer trying create document term matrix python based following list sentence help get desired outcome processing sentence one one e g using follows produce result however wonder way create matrix iteration usually large number sentence simply writing one one convenient tried iteration process last sentence produce result thinking maybe since sparse matrix way append vector iteration
An NLP Model that Suggest a List of Words in an Incomplete Sentence,"<p>I have somewhat read a bunch of papers which talks about predicting missing words in a sentence. What I really want is to create a model that suggest a word from an incomplete sentence. </p>

<pre><code>  Example:

  Incomplete Sentence :
  I bought an ___________  because its rainy.

  Suggested Words:
      umbrella
      soup
      jacket
</code></pre>

<p>From the journal I have read, they have utilized Microsoft Sentence Completion Dataset for predicting missing words from a sentence. </p>

<pre><code>  Example :

  Incomplete Sentence :

  Im sad because you are __________

  Missing Word Options:
  a) crying
  b) happy
  c) pretty
  d) sad
  e) bad
</code></pre>

<p>I don't want to predict a missing word from a list of options. I want to suggest a list of words from an incomplete sentence. Is it feasible? Please enlighten me cause Im really confused. What is state of the art model I can use for suggesting a list of words (semantically coherent) from an incomplete sentence?</p>

<p>Is it necessary that the list of suggested words as an output is included in the training dataset? </p>
",Dataset Preprocessing & Handling,nlp model suggest list word incomplete sentence somewhat read bunch paper talk predicting missing word sentence really want create model suggest word incomplete sentence journal read utilized microsoft sentence completion dataset predicting missing word sentence want predict missing word list option want suggest list word incomplete sentence feasible please enlighten cause im really confused state art model use suggesting list word semantically coherent incomplete sentence necessary list suggested word output included training dataset
Minimum term/doc frequency for Seeded Latent Dirichlet Allocation:?,"<p>I'm a newbie at language technology using R's <code>seededlda</code> package to perform SLDA on a document term matrix created from a corpus of ~400k texts (scientific abstracts) to classify each document into one of two categories, say category &quot;A&quot; and &quot;B&quot;.</p>
<p>I get wildly different results based on how I shorten the document term matrix. I first decided to limit the DTM to words used at least 1,000 times in 10,000 different documents. This got me some weird results on the distribution of categories across documents--based on my knowledge of the problem I'm studying, I was expecting category A to outweigh B at least 1.5:1, if not 2:1 or greater. Instead, Category B outweighed A 3:1.</p>
<p>However, if I do not impose any limits on term and document frequency, I get results that align with my prior expectations. Since I'm not a computational linguist, I'm not sure which &quot;approach&quot; is more desirable.</p>
<p><strong>Is there any literature out there on &quot;optimal&quot; minimum document/term frequency when performing (seeded) LDA?</strong> On a quick Google search, I haven't seem to find anything that would help me decide which of my results are closer to the &quot;correct&quot; answer, or what the threshold should be for cutting terms by document or term frequency.</p>
",Dataset Preprocessing & Handling,minimum term doc frequency seeded latent dirichlet allocation newbie language technology using r package perform slda document term matrix created corpus k text scientific abstract classify document one two category say category b get wildly different result based shorten document term matrix first decided limit dtm word used least time different document got weird result distribution category across document based knowledge problem studying wa expecting category outweigh b least greater instead category b outweighed however impose limit term document frequency get result align prior expectation since computational linguist sure approach desirable literature optimal minimum document term frequency performing seeded lda quick google search seem find anything would help decide result closer correct answer threshold cutting term document term frequency
Detecting address chunk within a word document,"<p>I have a word document with some paragraphs and address details within it. I used textract to extract the sentences of this document line by line into a list. What i want to do is to detect the complete address chunk as one whole sentence string. The address template is not fixed and can or cannot have all the details some times,  how can i achieve that ?</p>
<p>the input document looks like -</p>
<pre><code>some paragraph1

Employee’s address: Mr. A John Doe
9 hackers way
a state in US
2192
Telephone: 1411567323
Telefax: - 
E-mail: someemail@gmail.com

some paragraph 2
next page
some paragraph 3
</code></pre>
<p>what i want the complete address chunk to be detected is -</p>
<pre><code>Employee’s address: Mr. A John Doe
    9 hackers way
    a state in US
    2192
    Telephone: 1411567323
    Telefax: - 
    E-mail: someemail@gmail.com
</code></pre>
",Dataset Preprocessing & Handling,detecting address chunk within word document word document paragraph address detail within used textract extract sentence document line line list want detect complete address chunk one whole sentence string address template fixed detail time achieve input document look like want complete address chunk detected
Merging data sets based on timestamp but there has to be 48hrs between the timestamps,"<p><strong>UPDATE:</strong></p>
<p><strong>This is how I currently matched my data frames, on matching date stamps and patient number. I want to match on patient number and datestamp 48h earlier. I took hours and seconds away to gather more matching entries</strong></p>
<p><a href=""https://i.sstatic.net/sXCC5.png"" rel=""nofollow noreferrer"">How I changed time stamps to date stamps</a></p>
<p><a href=""https://i.sstatic.net/kaFyP.png"" rel=""nofollow noreferrer"">How I merged the labels to the set</a></p>
<p><strong>Dummy samples of <a href=""https://i.sstatic.net/Zlnaz.png"" rel=""nofollow noreferrer"">feature set </a> (logs) and <a href=""https://i.sstatic.net/U3Vm6.png"" rel=""nofollow noreferrer"">label set</a> (scores)</strong></p>
<p><strong>Dummy <a href=""https://i.sstatic.net/q4B3o.png"" rel=""nofollow noreferrer"">expected output</a></strong></p>
<p><strong>----------------------------------------------------------</strong></p>
<p>I created a data frame from 2 sources:</p>
<ol>
<li>Logs that nurses wrote over 2020, including timestamp and patient number</li>
<li>Patient vitality scores over 2020, including timestamp and patient number</li>
</ol>
<p>I want to predict the vitality scores off of the nurses' logs, using NLP and binary classification.</p>
<p>I think I need to create a new data set, where I have the logs, scores and patient numbers. The important thing is:</p>
<p><strong>I need logs that were written 48hrs prior to scoring for each patient. So difference between <strong>log_time</strong> and <strong>score_time</strong> should be 2 roughly days.</strong></p>
<p>What should the timestamps look like? Should I make them numerical?</p>
<p>The time stamps are written like this: <strong>09MAR2020:23:06:00</strong>
I created date stamps like this: <strong>09MAR20</strong>
Maybe I should convert it to something like this: <strong>03-09-20-00:00:00?</strong></p>
<p>If anyone could give me any tips, that would be really helpful.</p>
<p>Thanks in advance!!</p>
",Dataset Preprocessing & Handling,merging data set based timestamp ha hr timestamps update currently matched data frame matching date stamp patient number want match patient number datestamp h earlier took hour second away gather matching entry changed time stamp date stamp merged label set dummy sample feature set log label set score dummy expected output created data frame source log nurse wrote including timestamp patient number patient vitality score including timestamp patient number want predict vitality score nurse log using nlp binary classification think need create new data set log score patient number important thing need log written hr prior scoring patient difference log time score time roughly day timestamps look like make numerical time stamp written like mar created date stamp like mar maybe convert something like anyone could give tip would really helpful thanks advance
How to pass several columns from a DataFrame all together in Word2Vec,"<p>While using Word2Vec algorithm, how to pass several text columns from a pandas data frame all together for the model to train.</p>
<p>Image showing columns present in DataFrame][1]
[1]: <a href=""https://i.sstatic.net/80iiq.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/80iiq.png</a></p>
<p>'task' is the name of the dataframe.</p>
<p><strong>Code is as follows:</strong></p>
<p>import gensim
from gensim import corpora, models, similarities</p>
<p>model_taskname = gensim.models.Word2Vec(task.iloc[:,0], min_count=1, size = 32, window = 10, sg=1)
model_elementname = gensim.models.Word2Vec(task.iloc[:,1], min_count=1, size = 32, window = 10, sg=1)</p>
",Dataset Preprocessing & Handling,pas several column dataframe together word vec using word vec algorithm pas several text column panda data frame together model train image showing column present dataframe task name dataframe code follows import gensim gensim import corpus model similarity model taskname gensim model word vec task iloc min count size window sg model elementname gensim model word vec task iloc min count size window sg
How to extract phrases from text using specific noun-verb-noun NLTK PoS tag patterns?,"<p>I have a data frame that has a column containing some text.</p>
<p>I want to extract phrases from the text with the format <code>NN + VB + NN</code> or <code>NN + NN + VB + NN</code> or <code>NN + ... + NN + VB + NN</code> et cetera. Basically, I want to get the simple phrases with 1 to n <code>nouns</code> before the first encountered <code>verb</code>, followed by a <code>noun</code>.</p>
<p>I'm using <code>nltk.pos_tag</code> after tokenizing the texts to get the tag of each word, however I cannot find a way to get what I want.</p>
<p>I also thought about <code>bigrams</code>, <code>trigrams</code>, <code>ngrams</code> etc. but couldn't find a way to apply it.</p>
<p>Any help, please?</p>
",Dataset Preprocessing & Handling,extract phrase text using specific noun verb noun nltk po tag pattern data frame ha column containing text want extract phrase text format et cetera basically want get simple phrase n first encountered followed using tokenizing text get tag word however find way get want also thought etc find way apply help please
How to efficiently count the number of two word combinations in human names?,"<p>I have a dataframe with a single column which contains the names of people as shown below.</p>
<pre><code>name
--------------
john doe
john david doe
doe henry john
john henry
</code></pre>
<p>I want to count the number of time each two words appear together in a name regardless of the order. In this example, the words <code>john</code> and <code>doe</code> appear in the same same three times names <code>john doe</code>, <code>john henry doe</code> and <code>doe john</code>.</p>
<p><strong>Expected output</strong></p>
<pre><code>name1 | name2 | count
----------------------
david | doe   | 1
doe   | henry | 1
doe   | john  | 3
henry | john  | 2
</code></pre>
<p>Notice that <code>name1</code> is the word that comes first in alphabetical order. Currently I have a  brute force solution.</p>
<ol>
<li>Create a list of all unique words in the dataframe</li>
<li>For each unique word <code>W</code> in this list, filter the records in the original data frame which contain this <code>W</code></li>
<li>From the filtered records, count frequency of other words. This gives the number of time <code>W</code> appears with various other words</li>
</ol>
<p><strong>Question</strong>: This works fine for small number of records but is not efficient if we have a large number of records as it runs in quadratic complexity. How can it generate the output in a faster way? Is there any function or package that can give these counts?</p>
<p><strong>Note</strong>: I tried using n-gram extraction from NLP packages but this over estimates the counts because it internally appends all the names to form a long string due to which the last word on a name and the first word of the next name shows up as a a sequence of words in the appended string which adds up to the count.</p>
",Dataset Preprocessing & Handling,efficiently count number two word combination human name dataframe single column contains name people shown want count number time two word appear together name regardless order example word appear three time name expected output notice word come first alphabetical order currently brute force solution create list unique word dataframe unique word list filter record original data frame contain filtered record count frequency word give number time appears various word question work fine small number record efficient large number record run quadratic complexity generate output faster way function package give count note tried using n gram extraction nlp package estimate count internally appends name form long string due last word name first word next name show sequence word appended string add count
An error in implementing regex function on a list,"<p>I was trying to implement a regex on a list of grammar tags in python, for finding the tense form of the list of grammar. And I wrote the following code to implement it.</p>
<p>Data preprocessing:</p>
<pre><code>from nltk import word_tokenize, pos_tag
import nltk

text = &quot;He will have been doing his homework.&quot; 

tokenized = word_tokenize(text)
tagged = pos_tag(tokenized)
tags = []
for i in range(len(tagged)):
    t = tagged[i]
    tags.append(t[1])
print(tags)
</code></pre>
<p>regex formula i.e. to be implemented</p>
<pre><code>grammar = r&quot;&quot;&quot;
Future_Perfect_Continuous: {&lt;MD&gt;&lt;VB&gt;&lt;VBN&gt;&lt;VBG&gt;}
Future_Continuous:         {&lt;MD&gt;&lt;VB&gt;&lt;VBG&gt;}
Future_Perfect:            {&lt;MD&gt;&lt;VB&gt;&lt;VBN&gt;}
Past_Perfect_Continuous:   {&lt;VBD&gt;&lt;VBN&gt;&lt;VBG&gt;}
Present_Perfect_Continuous:{&lt;VBP|VBZ&gt;&lt;VBN&gt;&lt;VBG&gt;}
Future_Indefinite:         {&lt;MD&gt;&lt;VB&gt;}
Past_Continuous:           {&lt;VBD&gt;&lt;VBG&gt;}
Past_Perfect:              {&lt;VBD&gt;&lt;VBN&gt;}
Present_Continuous:        {&lt;VBZ|VBP&gt;&lt;VBG&gt;}
Present_Perfect:           {&lt;VBZ|VBP&gt;&lt;VBN&gt;}
Past_Indefinite:           {&lt;VBD&gt;}
Present_Indefinite:        {&lt;VBZ&gt;|&lt;VBP&gt;}
</code></pre>
<p>Function to implement the regex on the list <code>tags</code></p>
<pre><code>def check_grammar(grammar, tags):
    cp = nltk.RegexpParser(grammar)
    result = cp.parse(tags)
    print(result)
    result.draw()
 
check_grammar(grammar, tags)
</code></pre>
<p>But it returned an error as:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/samar/Desktop/twitter_tense/main.py&quot;, line 35, in &lt;module&gt;
    check_grammar(grammar, tags)
  File &quot;/home/samar/Desktop/twitter_tense/main.py&quot;, line 31, in check_grammar
    result = cp.parse(tags)
  File &quot;/home/samar/.local/lib/python3.8/site-packages/nltk/chunk/regexp.py&quot;, line 1276, in parse
    chunk_struct = parser.parse(chunk_struct, trace=trace)
  File &quot;/home/samar/.local/lib/python3.8/site-packages/nltk/chunk/regexp.py&quot;, line 1083, in parse
    chunkstr = ChunkString(chunk_struct)
  File &quot;/home/samar/.local/lib/python3.8/site-packages/nltk/chunk/regexp.py&quot;, line 95, in __init__
    tags = [self._tag(tok) for tok in self._pieces]
  File &quot;/home/samar/.local/lib/python3.8/site-packages/nltk/chunk/regexp.py&quot;, line 95, in &lt;listcomp&gt;
    tags = [self._tag(tok) for tok in self._pieces]
  File &quot;/home/samar/.local/lib/python3.8/site-packages/nltk/chunk/regexp.py&quot;, line 105, in _tag
    raise ValueError(&quot;chunk structures must contain tagged &quot; &quot;tokens or trees&quot;)
ValueError: chunk structures must contain tagged tokens or trees
</code></pre>
",Dataset Preprocessing & Handling,error implementing regex function list wa trying implement regex list grammar tag python finding tense form list grammar wrote following code implement data preprocessing regex formula e implemented function implement regex list returned error
"Pandas read CSV, i cannot read Vietnamese text","<p><a href=""https://i.sstatic.net/LFf26.png"" rel=""nofollow noreferrer"">enter image description here</a>
I'm trying to read Vietnamese text, but this is what I've received, something's wrong here, these words are not Vietnamese, can anyone help me to figure it out? Thank you so much!</p>
",Dataset Preprocessing & Handling,panda read csv read vietnamese text enter image description trying read vietnamese text received something wrong word vietnamese anyone help figure thank much
pandas: text analysis: Transfer raw data to dataframe,"<p>I need to read lines from a text file and extract the
quoted person name and quoted text from each line.</p>
<p>lines look similar to this:</p>
<blockquote>
<p>&quot;Am I ever!&quot;, Homer Simpson responded.</p>
</blockquote>
<blockquote>
<p>Remarks:</p>
<p>Hint: Use the returned object from the '<code>open</code>' method to get the file
handler. Each line you read is expected to contain a new-line in the
end of the line. Remove the new-line as following: <code>line_cln =line.strip()</code></p>
</blockquote>
<blockquote>
<p>There are the options for each line (assume one of these
three options): The first set of patterns, for which the person name
appears before the quoted text. The second set of patterns, for which
the quoted text appears before the person. Empty lines.</p>
</blockquote>
<blockquote>
<p>Complete the <code>transfer_raw_text_to_dataframe</code> function to return a
dataframe    with the extracted person name and text as explained
above.  The information is expected to be extracted from the lines of
the given <code>'filename'</code> file.</p>
</blockquote>
<blockquote>
<p>The returned dataframe should include two columns:</p>
<ul>
<li><code>person_name</code> - containing the extracted person name for each line.</li>
<li><code>extracted_text</code> - containing the extracted quoted text for each line.</li>
</ul>
<p>The returned values:</p>
<ul>
<li>dataframe - The dataframe with the extracted information as described above.</li>
</ul>
<ul>
<li>Important Note: if a line does not contain any quotation pattern, no information should be saved in the
corresponding row in the dataframe.</li>
</ul>
</blockquote>
<p>what I got so far: [edited]</p>
<pre><code>def transfer_raw_text_to_dataframe(filename):

    data = open(filename)
    
    quote_pattern ='&quot;(.*)&quot;'
    name_pattern = &quot;\w+\s\w+&quot;
    
    df = open(filename, encoding='utf8')
    lines = df.readlines()
    df.close()
    dataframe = pd.DataFrame(columns=('person_name', 'extracted_text'))
    i = 0  

    for line in lines:
        quote = re.search(quote_pattern,line)
        extracted_quotation = quote.group(1)

        name = re.search(name_pattern,line)
        extracted_person_name = name.group(0)
        
        df2 = {'person_name': extracted_person_name, 'extracted_text': extracted_quotation}
        dataframe = dataframe.append(df2, ignore_index = True)

        dataframe.loc[i] = [person_name, extracted_text]
        i =i+1
            
    return dataframe
</code></pre>
<p>the dataframe is created with the correct shape, problem is, the person name in each row is: 'Oh man' and the quote is 'Oh man, that guy's tough to love.' (in all of them)
which is weird because it's not even in the txt file...</p>
<p><strong>can anyone help me fix this?</strong></p>
<p><strong>Edit:</strong> I need to extract from a simple txt file that contains these lines only:</p>
<pre><code>&quot;Am I ever!&quot;, Homer Simpson responded.
&quot;Hmmm. So... is it okay if I go to the women's conference with Chloe?&quot;, Lisa Simpson answered.
&quot;Really? Uh, sure.&quot;, Bart Simpson answered.
&quot;Sounds great.&quot;, Bart Simpson replied.
Homer Simpson responded: &quot;Danica Patrick in my thoughts!&quot;
C. Montgomery Burns: &quot;Trust me, he'll say it, or I'll bust him down to Thursday night vespers.&quot;
&quot;Gimme that torch.&quot; Lisa Simpson said.
&quot;No! No, I've got a lot more mothering left in me!&quot;, Marge Simpson said.
&quot;Oh, Homie, I don't care if you're a billionaire. I love you just because you're...&quot; Marge Simpson said.
&quot;Damn you, e-Bay!&quot; Homer Simpson answered.
</code></pre>
",Dataset Preprocessing & Handling,panda text analysis transfer raw data dataframe need read line text file extract quoted person name quoted text line line look similar ever homer simpson remark hint use returned object method get file handler line read expected contain new line end line remove new line following option line assume one three option first set pattern person name appears quoted text second set pattern quoted text appears person empty line complete function return dataframe extracted person name text explained information expected extracted line given file returned dataframe include two column containing extracted person name line containing extracted quoted text line returned value dataframe dataframe extracted information described important note line doe contain quotation pattern information saved corresponding row dataframe got far edited dataframe created correct shape problem person name row oh man quote oh man guy tough love weird even txt file anyone help fix edit need extract simple txt file contains line
Finding the dominant topic in each sentence in topic modeling,"<p>One question that I can't find the answer for in R is that how I can find the dominant topic in NLP model for each sentence?
Imagine I have data frame like this:</p>
<pre><code>comment &lt;- c(&quot;outstanding renovation all improvements are topoftheline and done with energy efficiency in mind low monthly utilities even the interior&quot;,
             &quot;solidly constructed lovingly maintained sf crest built&quot;,
             &quot;one year since built new this well designed storey home&quot;,
             &quot;beautiful street large bdm in the heart of lynn valley over sqft bathrooms&quot;,
             &quot;rare to find legal beautiful upgr in port moody centre with a mountain view all bedroom units were nicely renovated&quot;,
             &quot;fantastic opportunity to get value for the money excellent family home in desirable blueridge with legal selfcontained bachelor suite on the main floor great location close to swimming ice skating community&quot;,
             &quot;original owner tired but rock solid perfect location half a block to norquay elementary school and short quiet blocks to slocan park and sky train station&quot;)

id &lt;- c(1,2,3,4,5,6,7)

data &lt;- data.frame(id, comment)
</code></pre>
<p>I do preprocess as shown below:</p>
<pre><code>text_cleaning_tokens &lt;- data %&gt;% 
  tidytext::unnest_tokens(word, comment)
text_cleaning_tokens$word &lt;- gsub('[[:digit:]]+', '', text_cleaning_tokens$word)
text_cleaning_tokens$word &lt;- gsub('[[:punct:]]+', '', text_cleaning_tokens$word)


text_cleaning_tokens &lt;- text_cleaning_tokens %&gt;% filter(!(nchar(word) == 1))%&gt;% 
  anti_join(stop_words)

stemmed_token &lt;- text_cleaning_tokens %&gt;% mutate(word=wordStem(word))


tokens &lt;- stemmed_token %&gt;% filter(!(word==&quot;&quot;))
tokens &lt;- tokens %&gt;% mutate(ind = row_number())
tokens &lt;- tokens %&gt;% group_by(id) %&gt;% mutate(ind = row_number()) %&gt;%
  tidyr::spread(key = ind, value = word)
tokens [is.na(tokens)] &lt;- &quot;&quot;
tokens &lt;- tidyr::unite(tokens, clean_remark,-id,sep =&quot; &quot; )
tokens$clean_remark &lt;- trimws(tokens$clean_remark)
</code></pre>
<p>The I ran <code>FitLdaModel</code> function on this data and finally, found the best topics based on 2 groups:</p>
<pre><code>             t_1            t_2
1         beauti          built
2          block           home
3          renov          legal
4       bathroom          locat
5            bdm       bachelor
6      bdm_heart  bachelor_suit
7  beauti_street  block_norquai
8    beauti_upgr       blueridg
9        bedroom blueridg_legal
10  bedroom_unit   built_design
</code></pre>
<p>now based on the result I have, I want to find the most dominant topic in each sentence in topic modelling. For example, I want to know that for comment 1 (&quot;outstanding renovation all improvements are topoftheline and done with energy efficiency in mind low monthly utilities even the interior&quot;), which topic (topic 1 or topic 2) is the most dominant?</p>
<p>Can anyone help me with this question? do we have any package that can do this?</p>
",Dataset Preprocessing & Handling,finding dominant topic sentence topic modeling one question find answer r find dominant topic nlp model sentence imagine data frame like preprocess shown ran function data finally found best topic based group based result want find dominant topic sentence topic modelling example want know comment outstanding renovation improvement topoftheline done energy efficiency mind low monthly utility even interior topic topic topic dominant anyone help question package
Sorted document topic matrix gensim LDA,"<p>I have a corpus that I ran LDA on using gensim, and I'm trying to get a matrix in which rows are documents and columns are topics. I ran used the line of code below, but in the output, scores don't correspond to columns. I want to change this so that in the 0 column, you only have the probability of topic 0, likewise in the 1, 2, etc. columns.</p>
<p>Does anyone know how to do this?</p>
<pre><code>DocTopMat = pd.DataFrame(model.get_document_topics(corpus),columns=[i for i in range(model.num_topics)])
</code></pre>
<p><a href=""https://i.sstatic.net/eHBlQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eHBlQ.jpg"" alt=""enter image description here"" /></a></p>
",Dataset Preprocessing & Handling,sorted document topic matrix gensim lda corpus ran lda using gensim trying get matrix row document column topic ran used line code output score correspond column want change column probability topic likewise etc column doe anyone know
Word2Vec load error: No such file or directory: &#39;...word2vec_withString10-100-200.model.wv.vectors.npy,"<p>When I load the Word2Vec model, I have a problem that I cannot solve.</p>
<p>The code to run is as follows:</p>
<pre><code>from gensim.models import Word2Vec
w2v_model = Word2Vec.load('E:/projectlzy/data/word2vec_withString10-100-200.model')
</code></pre>
<p>The error is as follows:</p>
<pre><code>FileNotFoundError                         Traceback (most recent call last)
Input In [1], in &lt;cell line: 2&gt;()
      1 from gensim.models import Word2Vec
----&gt; 2 w2v_model = Word2Vec.load('E:/projectlzy/data/word2vec_withString10-100-200.model')

File D:\Anaconda\envs\mykeras\lib\site-packages\gensim\models\word2vec.py:1141, in Word2Vec.load(cls, *args, **kwargs)
   1122 &quot;&quot;&quot;Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.
   1123 
   1124 See Also
   (...)
   1138 
   1139 &quot;&quot;&quot;
   1140 try:
-&gt; 1141     model = super(Word2Vec, cls).load(*args, **kwargs)
   1143     # for backward compatibility for `max_final_vocab` feature
   1144     if not hasattr(model, 'max_final_vocab'):

File D:\Anaconda\envs\mykeras\lib\site-packages\gensim\models\base_any2vec.py:1230, in BaseWordEmbeddingsModel.load(cls, *args, **kwargs)
   1199 @classmethod
   1200 def load(cls, *args, **kwargs):
   1201     &quot;&quot;&quot;Load a previously saved object (using :meth:`~gensim.models.base_any2vec.BaseWordEmbeddingsModel.save`) from file.
   1202 
   1203     Also initializes extra instance attributes in case the loaded model does not include them.
   (...)
   1228 
   1229     &quot;&quot;&quot;
-&gt; 1230     model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
   1231     if not hasattr(model, 'ns_exponent'):
   1232         model.ns_exponent = 0.75

File D:\Anaconda\envs\mykeras\lib\site-packages\gensim\models\base_any2vec.py:602, in BaseAny2VecModel.load(cls, fname_or_handle, **kwargs)
    575 @classmethod
    576 def load(cls, fname_or_handle, **kwargs):
    577     &quot;&quot;&quot;Load a previously saved object (using :meth:`gensim.models.base_any2vec.BaseAny2VecModel.save`) from a file.
    578 
    579     Parameters
   (...)
    600 
    601     &quot;&quot;&quot;
--&gt; 602     return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)

File D:\Anaconda\envs\mykeras\lib\site-packages\gensim\utils.py:436, in SaveLoad.load(cls, fname, mmap)
    433 compress, subname = SaveLoad._adapt_by_suffix(fname)
    435 obj = unpickle(fname)
--&gt; 436 obj._load_specials(fname, mmap, compress, subname)
    437 logger.info(&quot;loaded %s&quot;, fname)
    438 return obj

File D:\Anaconda\envs\mykeras\lib\site-packages\gensim\utils.py:467, in SaveLoad._load_specials(self, fname, mmap, compress, subname)
    465     logger.info(&quot;loading %s recursively from %s.* with mmap=%s&quot;, attrib, cfname, mmap)
    466     with ignore_deprecation_warning():
--&gt; 467         getattr(self, attrib)._load_specials(cfname, mmap, compress, subname)
    469 for attrib in getattr(self, '__numpys', []):
    470     logger.info(&quot;loading %s from %s with mmap=%s&quot;, attrib, subname(fname, attrib), mmap)

File D:\Anaconda\envs\mykeras\lib\site-packages\gensim\utils.py:478, in SaveLoad._load_specials(self, fname, mmap, compress, subname)
    476     val = np.load(subname(fname, attrib))['val']
    477 else:
--&gt; 478     val = np.load(subname(fname, attrib), mmap_mode=mmap)
    480 with ignore_deprecation_warning():
    481     setattr(self, attrib, val)

File D:\Anaconda\envs\mykeras\lib\site-packages\numpy\lib\npyio.py:390, in load(file, mmap_mode, allow_pickle, fix_imports, encoding)
    388     own_fid = False
    389 else:
--&gt; 390     fid = stack.enter_context(open(os_fspath(file), &quot;rb&quot;))
    391     own_fid = True
    393 # Code to distinguish from NumPy binary files and pickles.

FileNotFoundError: [Errno 2] No such file or directory: 'E:/projectlzy/data/word2vec_withString10-100-200.model.wv.vectors.npy'
</code></pre>
<p>Is the .npy file generated when the .model file is generated? Is it necessary to load the word2vec.model file with the corresponding .npy file? Or am I using the wrong version of gensim? I used gensim==3.8.3.</p>
",Dataset Preprocessing & Handling,word vec load error file directory word vec withstring model wv vector npy load word vec model problem solve code run follows error follows npy file generated model file generated necessary load word vec model file corresponding npy file using wrong version gensim used gensim
How can I use a dataframe of multi-value in each cell as an input to machine learning for classification,"<p>I build a data frame with multivalued in each cell as picture below <a href=""https://i.sstatic.net/4DPhW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4DPhW.png"" alt=""enter image description here"" /></a>
and I want to use logistic regression for classification&gt;&gt;&gt;&gt;
my code is :</p>
<pre><code>fds1 = pd.DataFrame(featuresdata)
    fds1.fillna('', inplace=True)
    from sklearn.model_selection import train_test_split, cross_val_score
    X_train, X_test, y_train, y_test = train_test_split(fds1, y, test_size=0.30, random_state=100)
    from sklearn.linear_model import LogisticRegression
    classifier = LogisticRegression()
    classifier.fit(X_train, y_train)
    score = classifier.score(X_test, y_test)
    print(&quot;Accuracy for logistic regression:&quot;, score)
</code></pre>
<p>but there was an error with this code:</p>
<pre><code>File &quot;C:\Users\hp\PycharmProjects\pythonProject\FE2.py&quot;, line 317, in CLS2butclick
    classifier.fit(X_train, y_train)
  File &quot;C:\Users\hp\PycharmProjects\pythonProject\venv\lib\site-packages\sklearn\linear_model\_logistic.py&quot;, line 1138, in fit
    X, y = self._validate_data(
  File &quot;C:\Users\hp\PycharmProjects\pythonProject\venv\lib\site-packages\sklearn\base.py&quot;, line 596, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File &quot;C:\Users\hp\PycharmProjects\pythonProject\venv\lib\site-packages\sklearn\utils\validation.py&quot;, line 1074, in check_X_y
    X = check_array(
  File &quot;C:\Users\hp\PycharmProjects\pythonProject\venv\lib\site-packages\sklearn\utils\validation.py&quot;, line 856, in check_array
    array = np.asarray(array, order=order, dtype=dtype)
  File &quot;C:\Users\hp\PycharmProjects\pythonProject\venv\lib\site-packages\pandas\core\generic.py&quot;, line 2064, in __array__
    return np.asarray(self._values, dtype=dtype)
ValueError: setting an array element with a sequence.
</code></pre>
<p>How to fix that?</p>
",Dataset Preprocessing & Handling,use dataframe multi value cell input machine learning classification build data frame multivalued cell picture want use logistic regression classification code wa error code fix
How to build a Subject-Verb-Object extraction model in Python?,"<p>I have a pandas data frame object with one text column containing one or two sentences of text in each row. I would like to build a Subject-Verb-Object Model to extract the best SVOs from the text column for all the rows.</p>

<p>I am completely new to this, so please do provide additional inputs as to how to proceed.</p>

<p>Thanks!</p>
",Dataset Preprocessing & Handling,build subject verb object extraction model python panda data frame object one text column containing one two sentence text row would like build subject verb object model extract best svos text column row completely new please provide additional input proceed thanks
How to check if a column has a word based on words from another column with three different conditions in Pandas?,"<p>Input:</p>
<p><a href=""https://i.sstatic.net/SXlvS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SXlvS.png"" alt=""enter image description here"" /></a></p>
<pre><code>import pandas as pd

df_input = pd.DataFrame({'Keyword': {0: 'apple banana, orange',
  1: 'apple orange ?banana &quot;',
  2: 'potato, piercing pot hole',
  3: 'armor hard known'},
 'Returns': {0: 'Fruit; Banana Vendor',
  1: 'Blendor :Kaka Orange',
  2: 'piercing Fruit Banana takes a lot',
  3: 'bullet jacket gun'}})

df_input
</code></pre>
<p>For every word in Keyword column,</p>
<ol>
<li>if any of it appears in Returns column, Score = 1</li>
<li>if none of it appears in Returns column, Score = 0</li>
<li>if any of it appears in first half of the words in Returns column, Score_before = 1</li>
<li>if any of it appears in second half of the words in Returns column, Score_after = 1</li>
</ol>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/KAmEZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KAmEZ.png"" alt=""enter image description here"" /></a></p>
<pre><code>import pandas as pd

df_output = pd.DataFrame({'Keyword': {0: 'apple banana, orange',
  1: 'apple orange ?banana &quot;',
  2: 'potato, piercing pot hole',
  3: 'armor hard known'},
 'Returns': {0: 'Fruit; Banana Vendor',
  1: 'Blendor :Kaka Orange',
  2: 'piercing Fruit Banana takes a lot',
  3: 'bullet jacket gun'},
 'Score': {0: 1, 1: 1, 2: 1, 3: 0},
 'Score_before': {0: 0, 1: 0, 2: 1, 3: 0},
 'Score_after': {0: 0, 1: 1, 2: 0, 3: 0}})

df_output
</code></pre>
<p>Original data frame has a million rows, how do I even tokenize the words efficiently? Should I use string operations instead?
(I've used <code>from nltk.tokenize import word_tokenize</code> before, but how to apply it on the whole data frame if that's the way?)</p>
<p>Edit: My custom function for tokenization:</p>
<pre><code>import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
import string

def tokenize(s):
  translator = str.maketrans('', '', string.punctuation)
  s = s.translate(translator)
  s = word_tokenize(s)
  return s

tokenize('Finity: Stocks%, Direct$ MF, ETF')
</code></pre>
",Dataset Preprocessing & Handling,check column ha word based word another column three different condition panda input every word keyword column appears return column score none appears return column score appears first half word return column score appears second half word return column score output original data frame ha million row even tokenize word efficiently use string operation instead used apply whole data frame way edit custom function tokenization
Group similar words/synonyms in a text and find word frequency,"<p>I am trying to find the count of each word in a text.
But, there are words which are synonyms in it. I want to group them together and count them as one.
I am doing this in python. I am currently using wordnet to find the synonyms but not sure how to go about comparing and grouping synonyms and counting them as one.</p>
<p>Below is the text as a string datatype.
Text = &quot;This is a big one. Quite large.
It is a bad one. But it is also large. The second one is a also evil.&quot;</p>
<p>I have cleaned and split the words and counted them as separate as below using python:</p>
<pre><code>for char in '-?.,\n':
    Text=Text.replace(char,' ')
Text = Text.lower()
# split returns a list of words delimited by sequences of whitespace (including tabs, newlines, etc, like re's \s) 
word_list = Text.split()

from collections import Counter
Counter(word_list).most_common()

df = pd.DataFrame (Counter(word_list).most_common(), columns = ['Word','count'])
</code></pre>
<p>The above gives me a count of each word.</p>
<p>But,
My desired output in the form of data frame is:</p>
<pre><code>    Word  Synonym      Count
0   Big   {big,large}   3
1   Bad   {bad,evil}    2
2   Quite               1
3   one                 1
.......................................and so on
</code></pre>
<p>Help appreciated in getting the above output.Thanks.</p>
<p>Here is the way I am finding synonyms:</p>
<pre><code>import nltk
#nltk.download()
from nltk.corpus import wordnet
#syns = wordnet.synsets(&quot;dog&quot;)
#print(syns)


synonyms = []

for syn in wordnet.synsets(&quot;big&quot;):
    for l in syn.lemmas():
        synonyms.append(l.name())

print(set(synonyms))
</code></pre>
",Dataset Preprocessing & Handling,group similar word synonym text find word frequency trying find count word text word synonym want group together count one python currently using wordnet find synonym sure go comparing grouping synonym counting one text string datatype text big one quite large bad one also large second one also evil cleaned split word counted separate using python give count word desired output form data frame help appreciated getting output thanks way finding synonym
UMAP on batch data,"<p>I have a dataset consisting of more than 300M records each with around 800 features. I have broken the dataset into 1000 CSV files (each around 2.5Gig). I want to use UMAP to reduce the 800 dimensions space to a lower dimensions space (e.g., 10). Since I cannot load the whole dataset into the memory, I was wondering if there is any batch-learning approach for UMAP that receives each of my CSV files separately and output a single UMAP model.</p>
",Dataset Preprocessing & Handling,umap batch data dataset consisting record around feature broken dataset csv file around gig want use umap reduce dimension space lower dimension space e g since load whole dataset memory wa wondering batch learning approach umap receives csv file separately output single umap model
How to get topic-probs matrix in bertopic modeling,"<p>I ran BERTopic to get topics for 3,500 documents. How could I get the topic-probs matrix for each document and export them to csv? When I export them, I want to export the identifier of each document too.</p>
<p>I tried two approaches: First, I found topic_model.visualize_distribution(probs[#]) gives the information that I want. But how can I export the topics-probs data for each document to csv?</p>
<p>Second, I found this thread (<a href=""https://stackoverflow.com/questions/69740911/how-to-get-all-docoments-per-topic-in-bertopic-modeling"">How to get all docoments per topic in bertopic modeling</a>) can be useful if I can add the column for probabilities to the data frame it generates. Is there any way to do that?</p>
<p>Please share any other approaches that can produce and export the topic-probabilities matrix for all documents.</p>
<p>For your information, this is my BERTopic code. Thank you!</p>
<pre><code>embedding_model = SentenceTransformer('all-mpnet-base-v2')
umap_model = UMAP(n_neighbors=15)
hdbscan_model = HDBSCAN(min_cluster_size=20, min_samples=1,
                        gen_min_span_tree=True,
                        prediction_data=True)

stopwords = list(stopwords.words('english')) + ['http', 'https', 'amp', 'com']
vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words=stopwords)

model1 = BERTopic(
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    embedding_model=embedding_model,
    vectorizer_model=vectorizer_model,
    language='english',
    calculate_probabilities=True,
    verbose=True
)
topics, probs = model1.fit_transform(data)
</code></pre>
",Dataset Preprocessing & Handling,get topic probs matrix bertopic modeling ran bertopic get topic document could get topic probs matrix document export csv export want export identifier document tried two approach first found topic model visualize distribution probs give information want export topic probs data document csv second found thread href get docoments per topic bertopic modeling useful add column probability data frame generates way please share approach produce export topic probability matrix document information bertopic code thank
Is there a faster alternative to better-profanity 0.7.0 in python?,"<p>I'm using it in Google Colab to make a seperate column in a dataframe that checks whether the 'Text' column contains a curse word. Data frame has more than a million rows and it will take around 5 days using this code, takes 6 mins per 1000 sampled rows. Is there a more efficient alternative? Maybe using deep learning?</p>
<pre><code>import language_tool_python
tool = language_tool_python.LanguageTool('en-US')
from better_profanity import profanity
profanity.load_censor_words()

profanity_col = []
for x in df.Text.values:
  matches = tool.check(x)
  bad_words = profanity.contains_profanity(x)
  if bad_words == True:
    profanity_col.append(int(1))
  elif bad_words == False:
    profanity_col.append(int(0))

df = df.assign(profanity=pd.Series(profanity_col).values)
print(df[['profanity']].value_counts())
</code></pre>
",Dataset Preprocessing & Handling,faster alternative better profanity python using google colab make seperate column dataframe check whether text column contains curse word data frame ha million row take around day using code take min per sampled row efficient alternative maybe using deep learning
How to get BioBERT embeddings,"<p>I have field within a pandas dataframe with a text field for which I want to generate BioBERT embeddings. Is there a simple way with which I can generate the vector embeddings? I want to use them within another model.</p>
<p>here is a hypothetical sample of the data frame</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Visit Code</th>
<th>Problem Assessment</th>
</tr>
</thead>
<tbody>
<tr>
<td>1234</td>
<td>ge reflux working diagnosis well</td>
</tr>
<tr>
<td>4567</td>
<td>medication refill order working diagnosis note called in brand benicar 5mg qd 30 prn refill</td>
</tr>
</tbody>
</table>
</div>
<p>I have tried this package, but receive an error upon installation
<a href=""https://pypi.org/project/biobert-embedding"" rel=""nofollow noreferrer"">https://pypi.org/project/biobert-embedding</a></p>
<p>Error:</p>
<pre><code>Collecting biobert-embedding
  Using cached biobert-embedding-0.1.2.tar.gz (4.8 kB)
ERROR: Could not find a version that satisfies the requirement torch==1.2.0 (from biobert-embedding) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2, 1.7.1)
ERROR: No matching distribution found for torch==1.2.0 (from biobert-embedding)
</code></pre>
<p>Any help is GREATLY appreciated!</p>
",Dataset Preprocessing & Handling,get biobert embeddings field within panda dataframe text field want generate biobert embeddings simple way generate vector embeddings want use within another model hypothetical sample data frame visit code problem assessment ge reflux working diagnosis well medication refill order working diagnosis note called brand benicar mg qd prn refill tried package receive error upon installation error help greatly appreciated
Sparse Data Frame Alternative,"<pre><code>vectorizer = tfidf()
x = vectorizer.fit_transform(clean_articles)
x.shape

x_df = pd.SparseDataFrame(x, columns=vectorizer.get_feature_names(),index=range(len(clean_articles)))
</code></pre>
<p>As Sparse Dataframe is discontinued, how can I replace this code?</p>
",Dataset Preprocessing & Handling,sparse data frame alternative sparse dataframe discontinued replace code
Removing words to the right of a string in a dataframe,"<p>I have a data frame that contains a description of services performed on a vehicle. I would like to remove the word VIN followed by all words to the right of it.</p>
<pre><code>Description
Install Ceramic Film on the front 2 roll up on a 18 CX-5 Vin.#J1452239 St.#G3056 Per.Wally/Joseph Blenkinsop
Install Ceramic Film on the front 2 roll up on a 18 Terrain Vin.#JL225216 St.#218369 Per.Toby
Install Ceramic Film on the front 2 roll up on a 18 Terrain Vin.#JL286535 St.# Per.Tanner
Install Ceramic Film on the front 2 roll up on a 18 Yukon Vin.#JR297209 St.# Per.Randy/victoria Celaya
Install Ceramic Film on the front 2 roll up on a 19 1500 Vin.#KG174232 St.# Per.Colby
Install Ceramic Film on the front 2 roll up on a 19 1500 Vin.#KG265533 St.# Per.Colby
Install Ceramic Film on the front 2 roll up on a 19 Blazer Vin.#KS644905 St.# Per. J.C.
</code></pre>
<p>I have not gotten any useful output using prefix/suffix. Thank you.</p>
",Dataset Preprocessing & Handling,removing word right string dataframe data frame contains description service performed vehicle would like remove word vin followed word right gotten useful output using prefix suffix thank
Confusion regarding countvectorizer,"<p>why do I have to apply Countvectorizer on a smaller sample and then make the data frame? why can't I apply a count vectorizer to a large sample and create a data frame out of it?</p>
<p>here is my code :=
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()</p>
<p>X = cv.fit_transform(data['Pros_clean_lemma'])
print(X.shape)</p>
<p>output ; (128734, 26537)</p>
",Dataset Preprocessing & Handling,confusion regarding countvectorizer apply countvectorizer smaller sample make data frame apply count vectorizer large sample create data frame code sklearn feature extraction text import countvectorizer cv countvectorizer x cv fit transform data pro clean lemma print x shape output
how to search multiple of word in a data frame rows?,"<p>Let's see my dataframe,</p>
<pre><code>data = pd.DataFrame({
'Tweets' : ['This is bad', 'This is terrible', 'This is good', 'This is great but trouble of life']})

negative_words = ['bad', 'terrible', 'trouble of life']
</code></pre>
<p>Here I need to extract the negative_words from dataframe.</p>
<p>my code is only extract the single word in negative_words.</p>
<pre><code>data['searched_text'] =  data['Tweets'].apply(lambda x: ' '.join(entry for tag in x.split() for entry in negative_words if tag == entry))
</code></pre>
<p>expected output should include the 'trouble of life' word also in the column.</p>
",Dataset Preprocessing & Handling,search multiple word data frame row let see dataframe need extract negative word dataframe code extract single word negative word expected output include trouble life word also column
Unrecognized configuration class &lt;class &#39;transformers.models.bert.configuration_bert.BertConfig&#39;&gt; for this kind of AutoModel: AutoModelForSeq2SeqLM,"<p>Model type should be one of BartConfig, PLBartConfig, BigBirdPegasusConfig, M2M100Config, LEDConfig, BlenderbotSmallConfig, MT5Config, T5Config, PegasusConfig, MarianConfig, MBartConfig, BartConfig, BlenderbotConfig, FSMTConfig, XLMProphetNetConfig, ProphetNetConfig, EncoderDecoderConfig.</p>
<p>I am trying to load a fine-tuned Bert model for machine translation using AutoModelForSeq2SeqLM but it can't recognize the configuration class.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

model = AutoModelForSeq2SeqLM.from_pretrained('/content/drive/MyDrive/Models/CSE498')
</code></pre>
<p><strong>Config File</strong></p>
<pre><code>
{
  &quot;_name_or_path&quot;: &quot;ckiplab/albert-tiny-chinese&quot;,
  &quot;architectures&quot;: [
    &quot;BertForMaskedLM&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.0,
  &quot;bos_token_id&quot;: 101,
  &quot;classifier_dropout&quot;: null,
  &quot;classifier_dropout_prob&quot;: 0.1,
  &quot;down_scale_factor&quot;: 1,
  &quot;embedding_size&quot;: 128,
  &quot;eos_token_id&quot;: 102,
  &quot;gap_size&quot;: 0,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.0,
  &quot;hidden_size&quot;: 312,
  &quot;initializer_range&quot;: 0.02,
  &quot;inner_group_num&quot;: 1,
  &quot;intermediate_size&quot;: 1248,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 512,
  &quot;model_type&quot;: &quot;bert&quot;,
  &quot;net_structure_type&quot;: 0,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_groups&quot;: 1,
  &quot;num_hidden_layers&quot;: 4,
  &quot;num_memory_blocks&quot;: 0,
  &quot;pad_token_id&quot;: 0,
  &quot;position_embedding_type&quot;: &quot;absolute&quot;,
  &quot;tokenizer_class&quot;: &quot;BertTokenizerFast&quot;,
  &quot;torch_dtype&quot;: &quot;float32&quot;,
  &quot;transformers_version&quot;: &quot;4.18.0&quot;,
  &quot;type_vocab_size&quot;: 2,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 30522
}
</code></pre>
",Dataset Preprocessing & Handling,unrecognized configuration class class transformer model bert configuration bert bertconfig kind automodel automodelforseq seqlm model type one bartconfig plbartconfig bigbirdpegasusconfig config ledconfig blenderbotsmallconfig mt config config pegasusconfig marianconfig mbartconfig bartconfig blenderbotconfig fsmtconfig xlmprophetnetconfig prophetnetconfig encoderdecoderconfig trying load fine tuned bert model machine translation using automodelforseq seqlm recognize configuration class config file
Python: OSError can&#39;t load config for bert,"<p>I am trying to train a <code>bert-base-multilingual-uncased</code> model for a task. I have all the required files present in my dataset including the <code>config.json</code> bert file but when I run the model it gives an error.</p>
<h3>Config</h3>
<pre><code>class config:
    DEVICE = &quot;cuda:2&quot;
    MAX_LEN = 256
    TRAIN_BATCH_SIZE = 8
    VALID_BATCH_SIZE = 4
    EPOCHS = 1
    BERT_PATH = &quot;workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased&quot;
    MODEL_PATH = &quot;workspace/data/jigsaw-multilingual/model.bin&quot;
    TOKENIZER = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)
</code></pre>
<h3>Model</h3>
<pre><code>class BERTBaseUncased(nn.Module):
    def __init__(self):
        super(BERTBaseUncased, self).__init__()
        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH)
        self.bert_drop = nn.Dropout(0.3)
        self.out = nn.Linear(768 * 2, 1) # *2 since we have 2 pooling layers

    def forward(self, ids, mask, token_type_ids):
        o1, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)
        
        mean_pooling = torch.mean(o1, 1)
        max_pooling, _ = torch.max(o1, 1)
        cat = torch.cat((mean_pooling, max_pooling), 1)
        
        bo = self.bert_drop(cat)
        output = self.out(bo)
        return output
</code></pre>
<h3>Error</h3>
<pre><code>---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
/opt/conda/lib/python3.6/site-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    241             if resolved_config_file is None:
--&gt; 242                 raise EnvironmentError
    243             config_dict = cls._dict_from_json_file(resolved_config_file)

OSError: 

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
&lt;ipython-input-64-9f2999c88020&gt; in &lt;module&gt;
     79 
     80 if __name__ == &quot;__main__&quot;:
---&gt; 81     run()

&lt;ipython-input-64-9f2999c88020&gt; in run()
     38 
     39     device = torch.device(config.DEVICE)
---&gt; 40     model = BERTBaseUncased()
     41     model.to(device)
     42 

&lt;ipython-input-60-8e1508eac60a&gt; in __init__(self)
      2     def __init__(self):
      3         super(BERTBaseUncased, self).__init__()
----&gt; 4         self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH)
      5         self.bert_drop = nn.Dropout(0.3)
      6         self.out = nn.Linear(768 * 2, 1) # *2 since we have 2 pooling layers

/opt/conda/lib/python3.6/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    601                 proxies=proxies,
    602                 local_files_only=local_files_only,
--&gt; 603                 **kwargs,
    604             )
    605         else:

/opt/conda/lib/python3.6/site-packages/transformers/configuration_utils.py in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    198 
    199         &quot;&quot;&quot;
--&gt; 200         config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)
    201         return cls.from_dict(config_dict, **kwargs)
    202 

/opt/conda/lib/python3.6/site-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    249                 f&quot;- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\n\n&quot;
    250             )
--&gt; 251             raise EnvironmentError(msg)
    252 
    253         except json.JSONDecodeError:

OSError: Can't load config for 'workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased'. Make sure that:

- 'workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased' is the correct path to a directory containing a config.json file
</code></pre>
<p>These are the files present in my <code>bert</code> dataset:<br />
-&gt; <code>config.json</code><br />
-&gt;<code>pytorch_model.bin</code><br />
-&gt; <code>vocab.txt</code></p>
<p>How to fix this issue?</p>
",Dataset Preprocessing & Handling,python oserror load config bert trying train model task required file present dataset including bert file run model give error config model error file present dataset fix issue
In spacy: Add a span (doc[a:b]) as entity in a spacy doc (python),"<p>I am using regex over a whole document to catch the spans in which such regex occurs:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
import re

nlp = spacy.load(&quot;en_core_web_sm&quot;)
doc = nlp(&quot;The United States of America (USA) are commonly known as the United States (U.S. or US) or America.&quot;)

expression = r&quot;[Uu](nited|\.?) ?[Ss](tates|\.?)&quot;
for match in re.finditer(expression, doc.text):
    start, end = match.span()
    span = doc.char_span(start, end)
    # This is a Span object or None 
    # if match doesn't map to valid token sequence
    if span is not None:
        print(&quot;Found match:&quot;, span.text)
</code></pre>
<p>There is a way to get the span (list of tokens) corresponding to the regex match on the doc even if the boundaries of the regex match do not correspond to token boundaries.
See:
How can I expand the match to a valid token sequence? In <a href=""https://spacy.io/usage/rule-based-matching"" rel=""nofollow noreferrer"">https://spacy.io/usage/rule-based-matching</a></p>
<p>So far so good.</p>
<p>Now that I have a collectuon of spans how do I convert them into entities?
I am aware of the entity ruler:
The EntityRuler is a pipeline component (see also the link above) but that entityruler takes patterns as inputs to search in the doc and not spans.</p>
<p>If I want to use regex over the whole document to get the collection os spans I want to convert into ents what is the next step here? Entityruler? How? Or something else?</p>
<p>Put simpler:</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_sm&quot;)
doc = nlp(&quot;The aplicable law is article 102 section b sentence 6 that deals with robery&quot;)
</code></pre>
<p>I would like to generate an spacy ent (entity) out of doc[5,10] with label &quot;law&quot; in order to be able to:
A) loop over all the law entities in the texts
B) use the visualizer to display the different entities contained in the doc</p>
",Dataset Preprocessing & Handling,spacy add span doc b entity spacy doc python using regex whole document catch span regex occurs way get span list token corresponding regex match doc even boundary regex match correspond token boundary see expand match valid token sequence far good collectuon span convert entity aware entity ruler entityruler pipeline component see also link entityruler take pattern input search doc span want use regex whole document get collection span want convert ents next step entityruler something else put simpler would like generate spacy ent entity doc label law order able loop law entity text b use visualizer display different entity contained doc
joining characters on a string from a list of characters with a certain condition,"<p>I'm working on a pronunciation feature to evaluate the quality of the speech from a certain text.</p>
<p>I have a list of the differences between the given text and the speech-to-text like this:</p>
<pre class=""lang-py prettyprint-override""><code>import difflib

from difflib import SequenceMatcher

speech = &quot;chapter 1 it was a bright cold day in April and The clocks were striking 13 Winston Smith his chin nuzzled into his breast in an effort to escape the vile wind slipped quickly through the glass doors of victory Mansions though not quickly enough to prevent a swirl of gritty dust from entering along with him&quot;

groundtruth =  &quot;Chapter 1 It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.&quot;

# Normalization
speech = speech.lower().replace('\n',&quot; &quot;).replace('.',&quot;&quot;).replace(',',&quot;&quot;).replace('-',&quot;&quot;).replace('_',&quot;&quot;)
groundtruth = groundtruth.lower().replace('\n',&quot; &quot;).replace('.',&quot;&quot;).replace(',',&quot;&quot;).replace('-',&quot;&quot;).replace('_',&quot;&quot;)

speech_diff = [li for li in difflib.ndiff(speech, groundtruth) if li[0] != ' ']

speech_diff
['- 1',
 '- 3',
 '+ t',
 '+ h',
 '+ i',
 '+ r',
 '+ t',
 '+ e',
 '+ e',
 '+ n',
 '- w',
 '- o',
 '- l',
 '- e',
 '-  ',
 '- w',
 '- y',
 '-  ',
 '- s',
 '- m',
 '- e',
 '- t',
 '-  ',
 '- o',
 '- f',
...
 '+  ',
 '+ d',
 '+ u',
 '+ r',
 ...]

</code></pre>
<p>As you can see, this last list separates the characters from groundtruth and speech. I want to create a data frame to compare the differences between them, like this:</p>
<p>13      -&gt; thirteen
ole wy  -&gt; hallway</p>
<p>¿What alternatives do we have to archive this?</p>
",Dataset Preprocessing & Handling,joining character string list character certain condition working pronunciation feature evaluate quality speech certain text list difference given text speech text like see last list separate character groundtruth speech want create data frame compare difference like thirteen ole wy hallway alternative archive
Reading POS tag models in Android,"<p>I have tried doing POS tagging using <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow noreferrer"">openNLP POS Models</a> on a normal Java application. Now I would like to implement it on Android platform. I am not sure what is the Android requirement or restrictions as I am not able to read the models (binary file) and execute the POS tagging properly.</p>
<p>I tried getting the .bin file from external storage as well as putting it in an external libraries but still it couldn't work. These are my codes:</p>
<pre><code>InputStream modelIn = null;
POSModel model = null;

String path = Environment.getExternalStorageDirectory().getPath() + &quot;/TextSumIt/en-pos-maxent.bin&quot;;

modelIn = new BufferedInputStream( new FileInputStream(path));
model = new POSModel(modelIn);
</code></pre>
<p>The error I got:</p>
<pre><code>11-15 06:39:35.072: W/System.err(565): opennlp.tools.util.InvalidFormatException: The profile data stream has an invalid format!
11-15 06:39:35.177: W/System.err(565):  at opennlp.tools.dictionary.serializer.DictionarySerializer.create(DictionarySerializer.java:224)
11-15 06:39:35.177: W/System.err(565):  at opennlp.tools.postag.POSDictionary.create(POSDictionary.java:282)
11-15 06:39:35.182: W/System.err(565):  at opennlp.tools.postag.POSModel$POSDictionarySerializer.create(POSModel.java:48)
11-15 06:39:35.182: W/System.err(565):  at opennlp.tools.postag.POSModel$POSDictionarySerializer.create(POSModel.java:44)
11-15 06:39:35.182: W/System.err(565):  at opennlp.tools.util.model.BaseModel.&lt;init&gt;(BaseModel.java:135)
11-15 06:39:35.197: W/System.err(565):  at opennlp.tools.postag.POSModel.&lt;init&gt;(POSModel.java:93)
11-15 06:39:35.197: W/System.err(565):  at com.main.textsumit.SummarizationActivity.postagWords(SummarizationActivity.java:676)
11-15 06:39:35.205: W/System.err(565):  at com.main.textsumit.SummarizationActivity.generateSummary(SummarizationActivity.java:252)
11-15 06:39:35.205: W/System.err(565):  at com.main.textsumit.SummarizationActivity.onCreate(SummarizationActivity.java:127)
</code></pre>
<p>What is it that cause it not reading the model properly? And how should I resolve this? Please help.</p>
<p>Thank you.</p>
",Dataset Preprocessing & Handling,reading po tag model android tried po tagging using opennlp po model normal java application would like implement android platform sure android requirement restriction able read model binary file execute po tagging properly tried getting bin file external storage well putting external library still work code error got cause reading model properly resolve please help thank
How to use multiple clean up patterns in Normalizer (spark nlp)?,"<p>I am working with pyspark dataframe. I need to perform tf-idf and for that I am used prior steps of tokenizing, normalization, etc using <a href=""https://medium.com/trustyou-engineering/topic-modelling-with-pyspark-and-spark-nlp-a99d063f1a6e"" rel=""nofollow noreferrer"">spark NLP</a>.</p>
<p>I have df that looks like this after applying tokenizer:</p>
<pre><code>df.select('tokenizer').show(5, truncate = 130)

+----------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                  tokenized       |
+----------------------------------------------------------------------------------------------------------------------------------+
|[content, type, multipart, alternative, boundary, nextpart, da, df, nextpart, da, df, content, type, text, plain, charset, asci...|
|[receive, ameurht, eop, eur, prod, protection, outlook, com, cyprmb, namprd, prod, outlook, com, https, via, cyprca, namprd, pr...|
|[plus, every, photographer, need, mm, lens, digital, photography, school, email, newsletter, http, click, aweber, com, ct, l, m...|
|[content, type, multipart, alternative, boundary, nextpart, da, beb, nextpart, da, beb, content, type, text, plain, charset, as...|
|[original, message, customer, service, mailto, ilpjmwofnst, qssadxnvrvc, narrig, stepmotherr, eviews, com, send, thursday, dece...|
+----------------------------------------------------------------------------------------------------------------------------------+
only showing top 5 rows
</code></pre>
<p>The next step is to apply normalizer:</p>
<p>I want to set multiple clean up patterns:</p>
<pre><code>1) remove all numerics and numerics from words
-&gt; example: [jhghgb56, 5897t95, fhgbg4, 7474, hfgbgb]
-&gt; expected output: [jhghgb, fhgbg, hfgbgb]

2) remove all words less than 4
-&gt; example: [gfh, ehfufibf, hi, df, jdfh]
-&gt; expected output: [ehfufibf, jdfh]
</code></pre>
<p>I tried this:</p>
<pre><code>tokenizer = Tokenizer()\
     .setInputCols(['document'])\
     .setOutputCol('tokenized')\
     .setMinLength(3)

cleanup = [&quot;[^A-Za-z]&quot;]
normalizer = Normalizer()\
     .setInputCols(['tokenized'])\
     .setOutputCol('normalized')\
     .setLowercase(True)\
     .setCleanupPatterns(cleanup)
</code></pre>
<p>so far <code>cleanup = [&quot;[^A-Za-z]&quot;]</code> fulfils the first condition. But now I get clean words which are less than 4 characters and I don't understand how to remove those words.
Help would be much appreciated !</p>
",Dataset Preprocessing & Handling,use multiple clean pattern normalizer spark nlp working pyspark dataframe need perform tf idf used prior step tokenizing normalization etc using spark nlp df look like applying tokenizer next step apply normalizer want set multiple clean pattern tried far fulfils first condition get clean word le character understand remove word help would much appreciated
"Column not iterable, PySpark","<p>I'm trying to perform a count vectorization using this function I've created however, I keep having an error returned stating &quot;column not iterable&quot; which I cannot figure out why and how to resolve it.</p>
<p>(dfNew is the data frame with just two of the columns, both of StringType)</p>
<pre><code>import string
import re
import nltk
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer
ps = PorterStemmer()

dfNew = df.select(F.col('Description'), F.col('ID'))

def clean_description(text):
        text = &quot;&quot;.join([word.lower() for word in text if word not in string.punctuation])
        text = re.sub('[\n\r]+', ' ', text).lower()
        Description = re.split('\W+', text)
        text = [ps.stem(word) for word in Description if word not in nltk.corpus.stopwords.words('english')]
        more_stop_words = ['please', 'search', 'submitted', 'thank', 'search', 'com', 'url', 'https', 'via', 'www']
        text = [ps.stem(word) for word in Description if word not in more_stop_words]
        return text

count_vectorize = CountVectorizer(analyzer=clean_description) 
vectorized = count_vectorize.fit_transform(dfNew['Description'])
</code></pre>
<p>What am I doing wrong, and how can this be resolved?</p>
",Dataset Preprocessing & Handling,column iterable pyspark trying perform count vectorization using function created however keep error returned stating column iterable figure resolve dfnew data frame two column stringtype wrong resolved
Extracted Quality Attributes from user stories,"<p>How can I Extract Quality Attributes from user stories by using a .csv file in the spacy library and NLP? In CSV file, column 1 contains 2000 user stories.</p>
",Dataset Preprocessing & Handling,extracted quality attribute user story extract quality attribute user story using csv file spacy library nlp csv file column contains user story
Keyword assignment (not keyword extraction) in python machine learning: where to start?,"<p>I want to do keyword assignments (not keyword extraction) using python machine learning to a collection of articles, i.e. classifying a text using keywords from a predefined list. Google gives me an abundance of results on keyword extraction instead. Could you please direct me to any blogs or articles on the steps of keyword assignment (even better with recommendations to libraries)?</p>
<p>As shown in the screenshot (please advise how to share the CSV file), ten existing questions have already been manually tagged, and a new eleventh question is waiting to be tagged based on the patterns.</p>
<p><a href=""https://i.sstatic.net/7Eb4M.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7Eb4M.png"" alt=""enter image description here"" /></a></p>
",Dataset Preprocessing & Handling,keyword assignment keyword extraction python machine learning start want keyword assignment keyword extraction using python machine learning collection article e classifying text using keywords predefined list google give abundance result keyword extraction instead could please direct blog article step keyword assignment even better recommendation library shown screenshot please advise share csv file ten existing question already manually tagged new eleventh question waiting tagged based pattern
Split data frame of comments into multiple rows,"<p>I have a data frame with long comments and I want to split them into indiviual sentences using spacy sentencizer.</p>
<pre><code>Comments = pd.read_excel('Comments.xlsx', sheet_name = 'Sheet1')  
Comments
&gt;&gt;&gt;
         reviews
    0    One of the rare films where every discussion leaving the theater is about how much you 
         just had, instead of an analysis of its quotients.
    1    Gorgeous cinematography, insane flying action sequences, thrilling, emotionally moving, 
         and a sequel that absolutely surpasses its predecessor. Well-paced, executed &amp; has that 
         re-watchability factor.

</code></pre>
<p>I loaded the model like this</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_news_sm&quot;)
</code></pre>
<p>And using sentencizer</p>
<pre><code>from spacy.lang.en import English
nlp = English()
nlp.add_pipe('sentencizer')
Data = Comments.reviews.apply(lambda x : list( nlp(x).sents))
</code></pre>
<p>But when I check the sentence is in just one row like this</p>
<pre><code>[One of the rare films where every discussion leaving the theater is about how much you just had.,
 Instead of an analysis of its quotients.]
</code></pre>
<p>Thanks a lot for any help. I'm new using NLP tools in Data Frame.</p>
",Dataset Preprocessing & Handling,split data frame comment multiple row data frame long comment want split indiviual sentence using spacy sentencizer loaded model like using sentencizer check sentence one row like thanks lot help new using nlp tool data frame
Web Scrapping Using Python for nlp project,"<p>I have to scrap text data from <a href=""https://www.kalyanjewellers.net/"" rel=""nofollow noreferrer"">this website.</a> I have read some blogs on web scrap. But the major challenge that I have found is parsing HTML code. I am entirely new to this field. Can I get some help about how to scrap text data(which is possible) and make it into a CSV? Is this possible at all without knowledge about html? Can I expect a good demonstration of python code solving my problem then I will try this on my own for other websites?</p>
<p>TIA</p>
",Dataset Preprocessing & Handling,web scrapping using python nlp project scrap text data website read blog web scrap major challenge found parsing html code entirely new field get help scrap text data possible make csv possible without knowledge html expect good demonstration python code solving problem try website tia
Create Document Term Matrix with N-Grams in R,"<p>I am using ""tm"" package to create DocumentTermMatrix in R. It works well for one - gram but i am trying to create a DocumenttermMatrix of N-Grams(N = 3 for now) using tm package and tokenize_ngrams function from ""tokenizers"" package.
But im not able to create it.</p>

<p>I searched for possible solution but i didnt get much help.
For privacy reasons i can not share the data.
Here is what i have tried,  </p>

<pre><code>library(tm)  
library(tokenizers)
</code></pre>

<p>data is a dataframe with around 4.5k rows and 2 columns namely ""doc_id"" and ""text""</p>

<pre><code>data_corpus = Corpus(DataframeSource(data))
</code></pre>

<p>custom function for n-gram tokenization :  </p>

<pre><code>ngram_tokenizer = function(x){
  temp = tokenize_ngrams(x, n_min = 1, n = 3, stopwords = FALSE, ngram_delim = ""_"")
  return(temp)
}
</code></pre>

<p>control list for DTM creation :<br>
1-gram  </p>

<pre><code>control_list_unigram = list(tokenize = ""words"",
                          removePunctuation = FALSE,
                          removeNumbers = FALSE, 
                          stopwords = stopwords(""english""), 
                          tolower = T, 
                          stemming = T, 
                          weighting = function(x)
                            weightTf(x)
)
</code></pre>

<p>for N-gram tokenization</p>

<pre><code>control_list_ngram = list(tokenize = ngram_tokenizer,
                    removePunctuation = FALSE,
                    removeNumbers = FALSE, 
                    stopwords = stopwords(""english""), 
                    tolower = T, 
                    stemming = T, 
                    weighting = function(x)
                      weightTf(x)
                    )


dtm_unigram = DocumentTermMatrix(data_corpus, control_list_unigram)
dtm_ngram = DocumentTermMatrix(data_cropus, control_list_ngram)

dim(dtm_unigram)
dim(dtm_ngram)
</code></pre>

<p>The dimension of both the dtm's were same.<br>
Please correct me!</p>
",Dataset Preprocessing & Handling,create document term matrix n gram r using tm package create documenttermmatrix r work well one gram trying create documenttermmatrix n gram n using tm package tokenize ngrams function tokenizers package im able create searched possible solution didnt get much help privacy reason share data tried data dataframe around k row column namely doc id text custom function n gram tokenization control list dtm creation gram n gram tokenization dimension dtm please correct
How to calculate TF-IDF values of noun documents excluding spaCy stop words?,"<p>I have a data frame, <code>df</code> with <code>text</code>, <code>cleaned_text</code>, and <code>nouns</code> as column names. <code>text</code> and <code>cleaned_text</code> contains string document, <code>nouns</code> is a list of nouns extracted from <code>cleaned_text</code> column. <code>df.shape = (1927, 3)</code>.</p>
<p>I am trying to calculate <code>TF-IDF</code> values for all documents within <code>df</code> <strong>only for nouns, excluding spaCy stopwords</strong>.</p>
<hr />
<p><strong>What I have tried?</strong></p>
<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.lang.en import English

nlp = spacy.load('en_core_web_sm')

# subclass to modify stop word lists recommended from spaCy version 3.0 onwards
excluded_stop_words = {'down'}
included_stop_words = {'dear', 'regards'}

class CustomEnglishDefaults(English.Defaults):
    stop_words = English.Defaults.stop_words.copy()
    stop_words -= excluded_stop_words
    stop_words |= included_stop_words
    
class CustomEnglish(English):
    Defaults = CustomEnglishDefaults
</code></pre>
<pre class=""lang-py prettyprint-override""><code># function to extract nouns from cleaned_text column, excluding spaCy stowords.
nlp = CustomEnglish()

def nouns(text):
    doc = nlp(text)
    return [t for t in doc if t.pos_ in ['NOUN'] and not t.is_stop and not t.is_punct]
</code></pre>
<pre class=""lang-py prettyprint-override""><code># calculate TF-IDF values for nouns, excluding spaCy stopwords.
from sklearn.feature_extraction.text import TfidfVectorizer

documents = df.cleaned_text

tfidf = TfidfVectorizer(stop_words=CustomEnglish)
X = tfidf.fit_transform(documents)
</code></pre>
<hr />
<p><strong>What I am expecting?</strong></p>
<p>I am expecting to have an output as a list of tuples ranked in descending order;
<code>nouns = [('noun_1', tf-idf_1), ('noun_2', tf-idf_2), ...]</code>. All nouns in <code>nouns</code> should match those of <code>df.nouns</code> (this is to check whether I am on the right way).</p>
<hr />
<p><strong>What is my issue?</strong></p>
<p>I got confused about how to apply <code>TfidfVectorizer</code> such that to calculate only TF-IDF values for Nouns extracted from <code>cleaned_text</code>. I am also not sure whether SkLearn <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn-feature-extraction-text-tfidfvectorizer"" rel=""nofollow noreferrer"">TfidfVectorizer</a> can calculate TF-IDF as I am expecting.</p>
",Dataset Preprocessing & Handling,calculate tf idf value noun document excluding spacy stop word data frame column name contains string document list noun extracted column trying calculate value document within noun excluding spacy stopwords tried expecting expecting output list tuples ranked descending order noun match check whether right way issue got confused apply calculate tf idf value noun extracted also sure whether sklearn tfidfvectorizer calculate tf idf expecting
Creating a new corpus with NLTK,"<p>I reckoned that often the answer to my title is to go and read the documentations, but I ran through the <a href=""http://www.nltk.org/book"" rel=""noreferrer"">NLTK book</a> but it doesn't give the answer. I'm kind of new to Python.</p>

<p>I have a bunch of <code>.txt</code> files and I want to be able to use the corpus functions that NLTK provides for the corpus <code>nltk_data</code>. </p>

<p>I've tried <code>PlaintextCorpusReader</code> but I couldn't get further than:</p>

<pre><code>&gt;&gt;&gt;import nltk
&gt;&gt;&gt;from nltk.corpus import PlaintextCorpusReader
&gt;&gt;&gt;corpus_root = './'
&gt;&gt;&gt;newcorpus = PlaintextCorpusReader(corpus_root, '.*')
&gt;&gt;&gt;newcorpus.words()
</code></pre>

<p>How do I segment the <code>newcorpus</code> sentences using punkt? I tried using the punkt functions but the punkt functions couldn't read <code>PlaintextCorpusReader</code> class?</p>

<p>Can you also lead me to how I can write the segmented data into text files?</p>
",Dataset Preprocessing & Handling,creating new corpus nltk reckoned often answer title go read documentation ran nltk book give answer kind new python bunch file want able use corpus function nltk provides corpus tried get segment sentence using punkt tried using punkt function punkt function read class also lead write segmented data text file
reveal negation in the Text,"<p>I have a program that analyzes user comments (reviews) in the data frame. The idea is that everything that meets the word cheap increases the variable x.</p>
<pre><code>import pandas as pd
import nltk

def get_review_words(review):
    review_words = nltk.word_tokenize(review)
    for i in range (0, len(review_words)):
        review_words[i] = review_words[i].lower()
    return review_words

df = pd.read_csv(&quot;lastFile3.csv&quot;, sep=',' )


wordList= ['cheap']
x = 0
review_words =  get_review_words(str(df['review_text'].loc[i]))
if any(item in wordList for item in review_words):
    x += 1


</code></pre>
<p>My question is, how can I take into account cases of negation, so that, for example, if the text not cheap is mentioned, it is not taken into account.
Likewise for all cases  (not cheap,never,not be, ...)
Can someone help me, thanks in advance</p>
",Dataset Preprocessing & Handling,reveal negation text program analyzes user comment review data frame idea everything meet word cheap increase variable x question take account case negation example text cheap mentioned taken account likewise case cheap never someone help thanks advance
how to de-duplicate the record in data frame without null value in rows in python using record linkage toolkit?,"<p>This is the data frame, here i need to deduplicate the records, for dedup the record there i need to exclude the not available value and compare with other each records.</p>
<pre><code>df = pd.DataFrame({'fname':['richard','patricia','william','michael','richard','william'],
'lname': ['dietzen','economides','macdonald','boothby','dietzen','macdonald'],
'contact':['18708667706','not available','12404471426','18177395663','18708667706','12404471426'],
'bus_contact':['not available','12406789540','15619984476','not available','not available','15619984476'],
'email':['not available','patricia.economides@gmail.com','dpjpropertymanagement@gmail.com','mboothby@osmifw.com','not available','dpjpropertymanagement@gmail.com'],
'bus_mail':['rdietzen@hotmail.com','wmacdonald@mcrtrust.com','not available','michael.boothby@yahoo.com','rdietzen@hotmail.com','abc.com']})
</code></pre>
<p>this is my code for record linkage</p>
<pre><code> import recordlinkage
 indexer = recordlinkage.Index()
 indexer.block(&quot;LASTNAME&quot;)
 candidate_links = indexer.index(master_data)

 compare = recordlinkage.Compare()

 compare.string(&quot;fname&quot;, &quot;fname&quot;, method=&quot;jarowinkler&quot;, threshold=0.85, missing_value = 'not available', label=&quot;fname&quot;)

compare.string(&quot;lname&quot;, &quot;lname&quot;, method=&quot;jarowinkler&quot;, threshold=0.85, missing_value = 'not available', label=&quot;lname&quot;)
compare.exact(&quot;contact&quot;, &quot;contact&quot;, missing_value = 'not available', label='contact')
compare.exact(&quot;bus_contact&quot;, &quot;bus_contact&quot;, missing_value = 'not available', label=&quot;bus_contact&quot;)
compare.string(&quot;email&quot;, &quot;email&quot;, method=&quot;jarowinkler&quot;, threshold=0.85,

          missing_value='not available', label=&quot;email&quot;)
compare.string(&quot;bus_mail&quot;, &quot;bus_mail&quot;, method=&quot;jarowinkler&quot;, threshold=0.85,
           missing_value='not available', label=&quot;bus_mail&quot;)
features = compare.compute(candidate_links, df)
</code></pre>
<p>actual output of record linkage score:</p>
<pre><code>|      | fname| lname| contact| bus_contact| email | bus_mail|
|      |-----------------------------------------------------|
|(4, 0)|  1   |  1  |  1      |   1        |  1    |   1     |
|(5, 2)|  1   |  1  |  1      |   1        |  1    |   0     |
</code></pre>
<p>expected output of record linkage score should be exclude the not available value when compute the scores. assume it as NaN value</p>
<p>expected output of record linkage score:</p>
<pre><code>|      | fname| lname| contact| bus_contact| email | bus_mail|
|      |-----------------------------------------------------|
|(4, 0)|  1   |  1  |  1      |   0        |  0    |   1     |
|(5, 2)|  1   |  1  |  1      |   1        |  1    |   0     |
</code></pre>
<p>If the both records are not available it should be return as 0. please can anyone help me out.</p>
",Dataset Preprocessing & Handling,de duplicate record data frame without null value row python using record linkage toolkit data frame need record dedup record need exclude available value compare record code record linkage actual output record linkage score expected output record linkage score exclude available value compute score assume nan value expected output record linkage score record available return please anyone help
How to extract Person Names from a data frame in Python using Spacy,"<p>I have a table which has people names in text. I would like to de identify that text by removing the people's names from every instance, while maintaining the rest of the sentence.</p>
<pre><code>Row Num            Current Sent                            Ideal Sent
1                 Garry bought a cracker.                 bought a cracker.
2                 He named the parrot Eric.               He named the parrot.
3                 The ship was maned by Captain Jones.    The ship was maned by Captain.
</code></pre>
<p>How can I do that with Spacy? I know you have to identify the label as a 'PERSON' and then apply it to each row, but I can't seem to get the intended result. This is what I have so far:</p>
<pre><code>def pro_nn_finder(text):
    doc = nlp(text)
    return[ent.text for ent in doc.ents if ent.label_ == 'PERSON']

df.apply(pro_nn_finder)
</code></pre>
",Dataset Preprocessing & Handling,extract person name data frame python using spacy table ha people name text would like de identify text removing people name every instance maintaining rest sentence spacy know identify label person apply row seem get intended result far
Memory Error when applying spacy model to large log file,"<p>I am currently working on tokenizing a large log file that contains 39296844 characters. I am using the <code>nlp = spacy.load('en_core_web_sm')</code> model for this text file. Additionally I established the <code>nlp.max_length = 100000000000</code> so that I can read very large files. However, when I run the code <code>doc = nlp(df.iloc[161][1], disable=['ner', 'parser', &quot;textcat&quot;])</code> where <code>df.iloc[161][1]</code> contains the text of the log file, I run into the following memory error:</p>
<pre><code>---------------------------------------------------------------------------
MemoryError                               Traceback (most recent call last)
Input In [36], in &lt;cell line: 1&gt;()
----&gt; 1 df[&quot;build_log&quot;] = df[&quot;build_log&quot;].apply(preprocess)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\series.py:4433, in Series.apply(self, func, convert_dtype, args, **kwargs)
   4323 def apply(
   4324     self,
   4325     func: AggFuncType,
   (...)
   4328     **kwargs,
   4329 ) -&gt; DataFrame | Series:
   4330     &quot;&quot;&quot;
   4331     Invoke function on values of Series.
   4332 
   (...)
   4431     dtype: float64
   4432     &quot;&quot;&quot;
-&gt; 4433     return SeriesApply(self, func, convert_dtype, args, kwargs).apply()

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\apply.py:1088, in SeriesApply.apply(self)
   1084 if isinstance(self.f, str):
   1085     # if we are a string, try to dispatch
   1086     return self.apply_str()
-&gt; 1088 return self.apply_standard()

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\apply.py:1143, in SeriesApply.apply_standard(self)
   1137         values = obj.astype(object)._values
   1138         # error: Argument 2 to &quot;map_infer&quot; has incompatible type
   1139         # &quot;Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],
   1140         # Dict[Hashable, Union[Union[Callable[..., Any], str],
   1141         # List[Union[Callable[..., Any], str]]]]]&quot;; expected
   1142         # &quot;Callable[[Any], Any]&quot;
-&gt; 1143         mapped = lib.map_infer(
   1144             values,
   1145             f,  # type: ignore[arg-type]
   1146             convert=self.convert_dtype,
   1147         )
   1149 if len(mapped) and isinstance(mapped[0], ABCSeries):
   1150     # GH#43986 Need to do list(mapped) in order to get treated as nested
   1151     #  See also GH#25959 regarding EA support
   1152     return obj._constructor_expanddim(list(mapped), index=obj.index)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\_libs\lib.pyx:2870, in pandas._libs.lib.map_infer()

Input In [35], in preprocess(text)
      1 def preprocess(text):
----&gt; 2     doc = nlp(text, disable=['ner', 'parser'])
      3     lemmas = [token.lemma_ for token in doc]
      4     commands = get_commands(&quot;command-words.txt&quot;)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\language.py:1025, in Language.__call__(self, text, disable, component_cfg)
   1023     raise ValueError(Errors.E109.format(name=name)) from e
   1024 except Exception as e:
-&gt; 1025     error_handler(name, proc, [doc], e)
   1026 if doc is None:
   1027     raise ValueError(Errors.E005.format(name=name))

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\util.py:1630, in raise_error(proc_name, proc, docs, e)
   1629 def raise_error(proc_name, proc, docs, e):
-&gt; 1630     raise e

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\language.py:1020, in Language.__call__(self, text, disable, component_cfg)
   1018     error_handler = proc.get_error_handler()
   1019 try:
-&gt; 1020     doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]
   1021 except KeyError as e:
   1022     # This typically happens if a component is not initialized
   1023     raise ValueError(Errors.E109.format(name=name)) from e

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\pipeline\trainable_pipe.pyx:56, in spacy.pipeline.trainable_pipe.TrainablePipe.__call__()

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\util.py:1630, in raise_error(proc_name, proc, docs, e)
   1629 def raise_error(proc_name, proc, docs, e):
-&gt; 1630     raise e

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\pipeline\trainable_pipe.pyx:52, in spacy.pipeline.trainable_pipe.TrainablePipe.__call__()

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\pipeline\tok2vec.py:125, in Tok2Vec.predict(self, docs)
    123     width = self.model.get_dim(&quot;nO&quot;)
    124     return [self.model.ops.alloc((0, width)) for doc in docs]
--&gt; 125 tokvecs = self.model.predict(docs)
    126 batch_id = Tok2VecListener.get_batch_id(docs)
    127 for listener in self.listeners:

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\model.py:315, in Model.predict(self, X)
    311 def predict(self, X: InT) -&gt; OutT:
    312     &quot;&quot;&quot;Call the model's `forward` function with `is_train=False`, and return
    313     only the output, instead of the `(output, callback)` tuple.
    314     &quot;&quot;&quot;
--&gt; 315     return self._func(self, X, is_train=False)[0]

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\layers\chain.py:54, in forward(model, X, is_train)
     52 callbacks = []
     53 for layer in model.layers:
---&gt; 54     Y, inc_layer_grad = layer(X, is_train=is_train)
     55     callbacks.append(inc_layer_grad)
     56     X = Y

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\model.py:291, in Model.__call__(self, X, is_train)
    288 def __call__(self, X: InT, is_train: bool) -&gt; Tuple[OutT, Callable]:
    289     &quot;&quot;&quot;Call the model's `forward` function, returning the output and a
    290     callback to compute the gradients via backpropagation.&quot;&quot;&quot;
--&gt; 291     return self._func(self, X, is_train=is_train)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\layers\with_array.py:40, in forward(model, Xseq, is_train)
     38     return model.layers[0](Xseq, is_train)
     39 else:
---&gt; 40     return _list_forward(cast(Model[List2d, List2d], model), Xseq, is_train)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\layers\with_array.py:75, in _list_forward(model, Xs, is_train)
     73 lengths = layer.ops.asarray1i([len(seq) for seq in Xs])
     74 Xf = layer.ops.flatten(Xs, pad=pad)  # type: ignore
---&gt; 75 Yf, get_dXf = layer(Xf, is_train)
     77 def backprop(dYs: List2d) -&gt; List2d:
     78     dYf = layer.ops.flatten(dYs, pad=pad)  # type: ignore

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\model.py:291, in Model.__call__(self, X, is_train)
    288 def __call__(self, X: InT, is_train: bool) -&gt; Tuple[OutT, Callable]:
    289     &quot;&quot;&quot;Call the model's `forward` function, returning the output and a
    290     callback to compute the gradients via backpropagation.&quot;&quot;&quot;
--&gt; 291     return self._func(self, X, is_train=is_train)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\layers\chain.py:54, in forward(model, X, is_train)
     52 callbacks = []
     53 for layer in model.layers:
---&gt; 54     Y, inc_layer_grad = layer(X, is_train=is_train)
     55     callbacks.append(inc_layer_grad)
     56     X = Y

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\model.py:291, in Model.__call__(self, X, is_train)
    288 def __call__(self, X: InT, is_train: bool) -&gt; Tuple[OutT, Callable]:
    289     &quot;&quot;&quot;Call the model's `forward` function, returning the output and a
    290     callback to compute the gradients via backpropagation.&quot;&quot;&quot;
--&gt; 291     return self._func(self, X, is_train=is_train)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\layers\residual.py:40, in forward(model, X, is_train)
     37     else:
     38         return d_output + dX
---&gt; 40 Y, backprop_layer = model.layers[0](X, is_train)
     41 if isinstance(X, list):
     42     return [X[i] + Y[i] for i in range(len(X))], backprop

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\model.py:291, in Model.__call__(self, X, is_train)
    288 def __call__(self, X: InT, is_train: bool) -&gt; Tuple[OutT, Callable]:
    289     &quot;&quot;&quot;Call the model's `forward` function, returning the output and a
    290     callback to compute the gradients via backpropagation.&quot;&quot;&quot;
--&gt; 291     return self._func(self, X, is_train=is_train)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\layers\chain.py:54, in forward(model, X, is_train)
     52 callbacks = []
     53 for layer in model.layers:
---&gt; 54     Y, inc_layer_grad = layer(X, is_train=is_train)
     55     callbacks.append(inc_layer_grad)
     56     X = Y

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\model.py:291, in Model.__call__(self, X, is_train)
    288 def __call__(self, X: InT, is_train: bool) -&gt; Tuple[OutT, Callable]:
    289     &quot;&quot;&quot;Call the model's `forward` function, returning the output and a
    290     callback to compute the gradients via backpropagation.&quot;&quot;&quot;
--&gt; 291     return self._func(self, X, is_train=is_train)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\layers\chain.py:54, in forward(model, X, is_train)
     52 callbacks = []
     53 for layer in model.layers:
---&gt; 54     Y, inc_layer_grad = layer(X, is_train=is_train)
     55     callbacks.append(inc_layer_grad)
     56     X = Y

    [... skipping similar frames: Model.__call__ at line 291 (1 times)]

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\layers\chain.py:54, in forward(model, X, is_train)
     52 callbacks = []
     53 for layer in model.layers:
---&gt; 54     Y, inc_layer_grad = layer(X, is_train=is_train)
     55     callbacks.append(inc_layer_grad)
     56     X = Y

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\model.py:291, in Model.__call__(self, X, is_train)
    288 def __call__(self, X: InT, is_train: bool) -&gt; Tuple[OutT, Callable]:
    289     &quot;&quot;&quot;Call the model's `forward` function, returning the output and a
    290     callback to compute the gradients via backpropagation.&quot;&quot;&quot;
--&gt; 291     return self._func(self, X, is_train=is_train)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\layers\maxout.py:49, in forward(model, X, is_train)
     47 W = model.get_param(&quot;W&quot;)
     48 W = model.ops.reshape2f(W, nO * nP, nI)
---&gt; 49 Y = model.ops.gemm(X, W, trans2=True)
     50 Y += model.ops.reshape1f(b, nO * nP)
     51 Z = model.ops.reshape3f(Y, Y.shape[0], nO, nP)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\thinc\backends\numpy_ops.pyx:94, in thinc.backends.numpy_ops.NumpyOps.gemm()

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\blis\py.pyx:79, in blis.py.gemm()

MemoryError: Unable to allocate 7.87 GiB for an array with shape (7331886, 288) and data type float32
</code></pre>
<p>I have been trying to figure out the issue for a while and was wondering if anyone knew how to fix this issue? I thought disabling certain components would help but that doesn't seem to be the case. Any suggestion would be greatly appreciated!</p>
",Dataset Preprocessing & Handling,memory error applying spacy model large log file currently working tokenizing large log file contains character using model text file additionally established read large file however run code contains text log file run following memory error trying figure issue wa wondering anyone knew fix issue thought disabling certain component would help seem case suggestion would greatly appreciated
how to use natural language generation from a csv file input .which python module we should use.can any one share a sample tutorial?,"<p>take a input as a csv file and generate text/sentence using nlg. I have tried with pynlg and markov chain.But nothing worked .What else I can use?</p>
",Dataset Preprocessing & Handling,use natural language generation csv file input python module use one share sample tutorial take input csv file generate text sentence using nlg tried pynlg markov chain nothing worked else use
"TypeError: tuple indices must be integers or slices, not str, facing this error in keras model","<p>I am running a keras model, <a href=""https://keras.io/examples/nlp/nl_image_search/"" rel=""nofollow noreferrer"">LINK IS HERE</a>. I have just changed the dataset for this model and when I run my model it throwing this error <code>TypeError: tuple indices must be integers or slices, not str</code>. As it's a image captioning model and the dataset is difficult for me to understand.
See the blow code and read also the location of the error.</p>
<pre><code>`reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor=&quot;val_loss&quot;, factor=0.2, patience=3
 )
 # Create an early stopping callback.
 early_stopping = tf.keras.callbacks.EarlyStopping(
 monitor=&quot;val_loss&quot;, patience=5, restore_best_weights=True 
 )
 history = dual_encoder.fit(
 train_dataloader,
 epochs=num_epochs,
 #validation_data=val_dataloader,
 #callbacks=[reduce_lr, early_stopping],
 )
 print(&quot;Training completed. Saving vision and text encoders...&quot;)
 vision_encoder.save(&quot;vision_encoder&quot;)
 text_encoder.save(&quot;text_encoder&quot;)
 print(&quot;Models are saved.&quot;)


 TypeError                                 Traceback (most recent call last)
 &lt;ipython-input-31-745dd79762e6&gt; in &lt;module&gt;()
      15 history = dual_encoder.fit(
      16     train_dataloader,
 ---&gt; 17     epochs=num_epochs,
      18     #validation_data=val_dataloader,
      19     #callbacks=[reduce_lr, early_stopping],

  11 frames
  &lt;ipython-input-26-0696c83bf387&gt; in call(self, features, training)
      16         with tf.device(&quot;/gpu:0&quot;):
      17             # Get the embeddings for the captions.
 ---&gt; 18             caption_embeddings = text_encoder(features[&quot;caption&quot;], training=training)
      19             #caption_embeddings = text_encoder(train_inputs, training=training)
      20         with tf.device(&quot;/gpu:1&quot;):

  TypeError: tuple indices must be integers or slices, not str'
</code></pre>
<p>The error is pointing to this location <code>caption_embeddings = text_encoder(features[&quot;caption&quot;], training=training)</code></p>
<p>Now I am confused, I don't  know whether this error is due to the data which I am passing to my model like this <code>history = dual_encoder.fit(train_dataloader)</code> OR this error is related to <code>caption_embeddings = text_encoder(features[&quot;caption&quot;], training=training)</code> and <code>image_embeddings = vision_encoder(features[&quot;image&quot;], training=training)</code> which is defined in <code>class DualEncoder</code>.</p>
<p>Because I don't know what are these <code>features[&quot;caption&quot;]</code> and <code>features[&quot;image&quot;]</code> which is defined in <code>Class DualEncoder</code> as I have not changed these two with my new dataset if You check my <a href=""https://colab.research.google.com/drive/1wjmlb2hx-VSMhRTYOQ2jTmBlR1KcuBs8?usp=sharing"" rel=""nofollow noreferrer"">CODE HERE IN THIS COLAB FILE</a>.</p>
",Dataset Preprocessing & Handling,typeerror tuple index must integer slice str facing error kera model running kera model link changed dataset model run model throwing error image captioning model dataset difficult understand see blow code read also location error error pointing location confused know whether error due data passing model like error related defined know defined changed two new dataset check code colab file
Python_Pandas: Eliminate repeated character,"<p>I have a pandas data frame with a text column, in the text, many personal data has been replaced by XX, so there is a lot of X,XX, XXX...
How I can eliminate all the repeated X?
I'm trying with the following code, but I had to put all the possibilities of X, so it doesn't look like a practical approach.</p>
<pre><code>def cleanning(Complaint):

 Complaint = re.sub(r'#+/', ' ', Complaint)
 Complaint = re.sub(&quot;\d&quot;, &quot;\s&quot;, Complaint)
 Complaint = re.sub(&quot;XX&quot;, &quot;XXXX&quot;, Complaint)
 Complaint = re.sub(&quot;xx&quot;, &quot;xxxx&quot;, Complaint)
 Complaint = re.sub(&quot;@&quot;, &quot;XXXXXXXX&quot;, Complaint)
 Complaint = Complaint.replace('\n', ' ')
 Complaint = Complaint.replace('\r', ' ')

return Complaint
</code></pre>
",Dataset Preprocessing & Handling,python panda eliminate repeated character panda data frame text column text many personal data ha replaced xx lot x xx xxx eliminate repeated x trying following code put possibility x look like practical approach
Pandas create columns based on paragraph in dataframe,"<p>There is a data frame where the column <code>reason</code> has a paragraph of data that looks like this:</p>
<p>&quot;Alert summary as available: Provider name: &quot;&quot;Doe, Jane&quot;&quot; Provider specialty: Medical Oncology Referring provider name:  Referring provider specialty:  Account name: Cancer Clinic Masonic Cancer Clinic And Surgery Center&quot;</p>
<p>I want to parse through and create separate columns for the data in this paragraph. The data frame should look like:</p>
<pre><code> Reason       | Provider Name| Provider Specialty| Referring provider name| Referring provider specialty| Account name| 
 &quot;Alert Sum..&quot;| Doe, Jane    | Medical Oncology  |                        |                   | Cancer Clinic Masonic Cancer Clinic And Surgery Center&quot;
</code></pre>
<p>Is there a way I can do this?</p>
",Dataset Preprocessing & Handling,panda create column based paragraph dataframe data frame column ha paragraph data look like alert summary available provider name doe jane provider specialty medical oncology referring provider name referring provider specialty account name cancer clinic masonic cancer clinic surgery center want parse create separate column data paragraph data frame look like way
Using for loop to search through string and create data frame,"<p>I am trying to use openNLP to look through rows of text and classify sentences into thematic buckets. Here is a sample df:</p>
<pre><code>dat &lt;- data.frame(text=c(&quot;A fluffy crab discovered off the coast of Western Australia has been named after the ship that carried Charles Darwin around the world. The new species, Lamarckdromia beagle, belongs to the Dromiidae family, commonly known as sponge crabs. Crustaceans in this family fashion and use sea sponges and ascidians – animals including sea squirts – for protection. They trim the creatures using their claws and wear them like hats. &quot;,
                         &quot;The inadmissibility of such actions, which violate the relevant legal and political obligations of the European Union and lead to an escalation of tensions, was pointed out, the ministry said in a statement. Speaking shortly after the meeting, Ederer said he had called on the Russian government to remain calm and resolve this issue diplomatically, the Russian news agency Tass reported.&quot;),
                  date=c(as.Date(&quot;2020-12-26&quot;),as.Date(&quot;2020-12-31&quot;)), 
                  id= c(&quot;1&quot;, &quot;2&quot;))
</code></pre>
<p>Ive gotten as splitting the text into sentences, and then searching for the keywords using the following code:</p>
<pre><code>
#split sentences search for keywords

all_sentence &lt;- as.String(dat$text)

sent_annotator &lt;- Maxent_Sent_Token_Annotator()
annotation &lt;- annotate(all_sentence, sent_annotator)

split_text &lt;- all_sentence[annotation]

# word list to search for 

word_dat &lt;- data.frame(words=c(&quot;animal&quot;, &quot;species&quot;, &quot;political&quot;, &quot;government&quot;),
                  theme=c(&quot;nature&quot;, &quot;nature&quot;, &quot;geopolitics&quot;, &quot;geopolitics&quot;))

stem_keyword &lt;- wordStem(word_dat$words, language = &quot;english&quot;)


for(kw in stem_keyword) {
  x=grep(kw, split_text)
  print(split_text[x])
  print(stem_keyword[x])
}
</code></pre>
<p>However my for loop doesnt print exactly what im looking for.. for example, print(stem_keyword) is giving me the wrong keyword for the wrong sentence. In the end I dont want to print, I want to write the results to a new dataframe with this structure:</p>
<pre><code>final_df &lt;- data.frame(text=c(&quot;A fluffy crab discovered off the coast of Western Australia has been named after the ship that carried Charles Darwin around the world.&quot;, &quot;The new species, Lamarckdromia beagle, belongs to the Dromiidae family, commonly known as sponge crabs.&quot;,&quot;Crustaceans in this family fashion and use sea sponges and ascidians – animals including sea squirts – for protection.&quot;, &quot;They trim the creatures using their claws and wear them like hats.&quot;,
                              &quot;The inadmissibility of such actions, which violate the relevant legal and political obligations of the European Union and lead to an escalation of tensions, was pointed out, the ministry said in a statement.&quot;,
                              &quot;Speaking shortly after the meeting, Ederer said he had called on the Russian government to remain calm and resolve this issue diplomatically, the Russian news agency Tass reported.&quot;),
                  keyword=c(&quot;null&quot;, &quot;species&quot;, &quot;animal&quot;, &quot;null&quot;, &quot;political&quot;, &quot;government&quot;),
                  theme=c(&quot;null&quot;, &quot;nature&quot;, &quot;nature&quot;, &quot;null&quot;, &quot;geopolitics&quot;, &quot;geopolitics&quot;), 
                  id= c(&quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;2&quot;))
</code></pre>
<p>Any advice or help getting my for loop to where I need it to be? TIA</p>
<p>EDIT: I would also like for sentences that cannot be classified to appear in the final dataframe with 'null' keywords and themes</p>
",Dataset Preprocessing & Handling,using loop search string create data frame trying use opennlp look row text classify sentence thematic bucket sample df ive gotten splitting text sentence searching keywords using following code however loop doesnt print exactly im looking example print stem keyword giving wrong keyword wrong sentence end dont want print want write result new dataframe structure advice help getting loop need tia edit would also like sentence classified appear final dataframe null keywords theme
How to read or open a qrel format file?,"<p>I was working with TREC qrel file and I would like to have a look at the file. I was wondering how to read a qrel file? or how can I open the file? what is the format&gt; what library should I use?</p>
",Dataset Preprocessing & Handling,read open qrel format file wa working trec qrel file would like look file wa wondering read qrel file open file format library use
How can I load tf.js CDN in chrome Extension?,"<p>I am making a Machine-learning Chrome Extension so I need to use Tf.js but when I'm loading tf.js CDN it's given me an error?</p>
<pre><code>Refused to load the script 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs' because it violates the following Content Security Policy directive: &quot;script-src 'self' https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest 'unsafe-eval'&quot;. Note that 'script-src-elem' was not explicitly set, so 'script-src' is used as a fallback.
</code></pre>
<p>Can anyone help me with this?</p>
<p>manifest.json</p>
<pre><code>&quot;content_security_policy&quot;: &quot;script-src 'self' 'unsafe-eval'; object-src 'self'&quot;,
&quot;mainfest_version&quot;:2
</code></pre>
<p>background.js</p>
<pre><code>const urls = {
  model:
    &quot;https://storage.googleapis.com/tfjs-models/tfjs/sentiment_cnn_v1/model.json&quot;,
  metadata:
    &quot;https://storage.googleapis.com/tfjs-models/tfjs/sentiment_cnn_v1/metadata.json&quot;,
};

// load model
async function loadModel(url) {
  try {
    const model = await tf.loadLayersModel(url);
    console.log(&quot;model Loaded&quot;);
    return model;
  } catch (err) {
    console.log(err);
  }
}

// load meta data
async function loadMetadata(url) {
  try {
    const metadataJson = await fetch(url);
    const metadata = await metadataJson.json();
    console.log(&quot;Metadata Loaded&quot;);
    return metadata;
  } catch (err) {
    console.log(err);
  }
}
</code></pre>
<p>index.html</p>
<pre><code>&lt;html&gt;
  &lt;head&gt;
    &lt;!-- load tf.js model --&gt;
    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/@tensorflow/tfjs&quot;&gt;&lt;/script&gt;


</code></pre>
",Dataset Preprocessing & Handling,load tf j cdn chrome extension making machine learning chrome extension need use tf j loading tf j cdn given error anyone help manifest json background j index html
What is the ideal &quot;size&quot; of the vector for each word in Word2Vec?,"<p>I have a dataset of over 1 million rows. Each row has 40 token words. Based on these tokens, a classification is made with a neural network. The vocabulary is 20,000 unique words. It is a <strong>binary classification</strong> problem. I set the size (dimension) of the vectors in <code>gensim</code> <code>Word2Vec</code> as 150 and saved these vectors for each data point in a <code>json</code> file. The <code>json</code> file's size is really huge: 250 GB. I cannot load this file into memory in one scoop as my RAM is only 128 GB. I am trying to see if I can reduce the physical size of these vectors by reducing them to the right size. I went through some of the suggestions made in this website such as <a href=""https://stackoverflow.com/questions/46560861/relation-between-word2vec-vector-size-and-total-number-of-words-scanned"">Relation between Word2Vec vector size and total number of words scanned?</a>. But the vector size is mentioned to be 100-300 and also depends on the problem.</p>
<p>Here is what I am doing:</p>
<pre><code># for training the word2vec model
w2vmodel = gensim.models.Word2Vec(one_mil_tokens,vector_size=150, window=2, min_count=1, sg=0, seed=1)
w2vmodel.save(&quot;w2model.trained&quot;)
</code></pre>
<p>and</p>
<pre><code>model = gensim.models.Word2Vec.load(&quot;w2model.trained&quot;)
vec = []
finalvecs = []

#tokens is a list of over a 1 million rows
for token in tokens:
  for word in token:
    vec.append(model.wv[eachtoken].tolist())
  finalvecs.append(vec)
</code></pre>
<p>I am doing <code>json.dump()</code> for <code>finalvecs</code>.</p>
<ol>
<li>How can I determine the right size (dimension) of the vector for each token based on the given problem?</li>
<li>I use skip-gram model to train Word2Vec. Should I use CBOW to optimize the size?</li>
<li>Is <code>json</code> the fight format to store/retrieve these vectors or are there other efficient ways?</li>
</ol>
",Dataset Preprocessing & Handling,ideal size vector word word vec dataset million row row ha token word based token classification made neural network vocabulary unique word binary classification problem set size dimension vector saved vector data point file file size really huge gb load file memory one scoop ram gb trying see reduce physical size vector reducing right size went suggestion made website href word vec vector size total number word scanned vector size mentioned also depends problem determine right size dimension vector token based given problem use skip gram model train word vec use cbow optimize size fight format store retrieve vector efficient way
Producing word clouds from pandas data frame,"<p>I have a dataframe with three columns 1.Word, 2.Frequency,3.Category and now I want a world clouds for the unigram words that I have in Word column and I need to assign it frequency as its size using frequency column and I want the color that word based on its category</p>
",Dataset Preprocessing & Handling,producing word cloud panda data frame dataframe three column word frequency category want world cloud unigram word word column need assign frequency size using frequency column want color word based category
How can I extract some contents in the cells of web-scraped csv file?,"<p>I am struggling with dealing with a csv file that scraped one crowdfunding website.</p>
<p>My goal is successfully load all information as separate columns, but I found some information are mixed in a single column when I load it using 1) R, 2) Stata, and 3) Python.</p>
<p>Since the real data is really dirty, let me suggest abbreviate version of current dataset.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Pledge</th>
<th>creator</th>
</tr>
</thead>
<tbody>
<tr>
<td>000001</td>
<td>13.7</td>
<td>{&quot;urls&quot;:{&quot;web&quot;:{&quot;user&quot;:&quot;www.kickstarter.com/profile/731&quot;}}, &quot;name&quot;:John&quot;,&quot;id&quot;:709510333}</td>
</tr>
<tr>
<td>000002</td>
<td>26.4</td>
<td>{&quot;urls&quot;:{&quot;web&quot;:{&quot;user&quot;:&quot;www.kickstarter.com/profile/759&quot;}}, &quot;name&quot;:Kellen&quot;,&quot;id&quot;:703514812}</td>
</tr>
<tr>
<td>000003</td>
<td>7.6</td>
<td>{&quot;urls&quot;:{&quot;web&quot;:{&quot;user&quot;:&quot;www.kickstarter.com/profile/7522&quot;}}, &quot;name&quot;:Jach&quot;,&quot;id&quot;:609542647}</td>
</tr>
</tbody>
</table>
</div>
<p>My goal was extracting the &quot;name&quot; and &quot;id&quot; as separate columns, though they are all mixed with URLs in the creator column.</p>
<p>Is there any way that I can extract names (John, Kellen, Jach) and ids as separate columns?
I prefer R, but Stata and Python would also be helpful!</p>
<p>Thank you so much for considering this.</p>
",Dataset Preprocessing & Handling,extract content cell web scraped csv file struggling dealing csv file scraped one crowdfunding website goal successfully load information separate column found information mixed single column load using r stata python since real data really dirty let suggest abbreviate version current dataset id pledge creator url web user name john id url web user name kellen id url web user name jach id goal wa extracting name id separate column though mixed url creator column way extract name john kellen jach id separate column prefer r stata python would also helpful thank much considering
How to read data from CSV and use it to convert it to Bratt(.ann) format python,"<p>So I have a <a href=""https://github.com/Da-vid21/Outputs/blob/main/test_singlefile.csv"" rel=""nofollow noreferrer"">csv file</a> and I wanted to convert it to Bratt (ann) format so it would <a href=""https://github.com/Da-vid21/Outputs/blob/main/test.ann"" rel=""nofollow noreferrer"">look like this</a> is there a script I can use to do this? Thanks</p>
",Dataset Preprocessing & Handling,read data csv use convert bratt ann format python csv file wanted convert bratt ann format would look like script use thanks
How can I extract some contents in the web-scraped csv file?,"<p>I am struggling with dealing with a csv file that scraped one crowdfunding website.</p>
<p>My goal is loading the csv file on Stata, and save it in .dta format. (Stata dataset).</p>
<p>Stata successfully loaded the dataset, like the screenshot I attached.
v20, v21 are variables I wanted.</p>
<p>In v22, I wanted to know the name of &quot;creator&quot;. However, it is not successfully excluded. As the selected cell in the screenshot shows.
<a href=""https://i.sstatic.net/xq0OX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xq0OX.jpg"" alt=""enter image description here"" /></a>
Is there any way that I can extract &quot;name&quot; and &quot;id&quot; of the creator as separate columns?</p>
<p>Also, in v23, I want to extract the location of each row.
<a href=""https://i.sstatic.net/36lMX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/36lMX.jpg"" alt=""enter image description here"" /></a>
However, I found the &quot;name&quot;, &quot;state&quot;, &quot;short_name&quot; are in the cell, without being extracted separately.
<a href=""https://i.sstatic.net/iOlcD.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iOlcD.jpg"" alt=""enter image description here"" /></a></p>
<p>Would there be any way that I can load them as separate columns?</p>
<p>Thank you for reading my post! Any help will be appreciated:)</p>
",Dataset Preprocessing & Handling,extract content web scraped csv file struggling dealing csv file scraped one crowdfunding website goal loading csv file stata save dta format stata dataset stata successfully loaded dataset like screenshot attached v v variable wanted v wanted know name creator however successfully excluded selected cell screenshot show way extract name id creator separate column also v want extract location row however found name state short name cell without extracted separately would way load separate column thank reading post help appreciated
Corpus vs Vocabulary vs Document in NLP,"<p>In a very simple and understandable term, because I read a lot of blogs which further more confused me,</p>
<ol>
<li><a href=""https://stats.stackexchange.com/questions/472640/in-nlp-what-is-the-difference-between-corpus-and-vocabulary-or-are-they-the-sa#:%7E:text=Corpus%3A%20Collection%20of%20texts%20used,to%20train%20an%20NLP%20model."">read but couldnt understand</a></li>
<li><a href=""https://www.quora.com/In-NLP-what-is-the-difference-between-a-Lexicon-and-a-Corpus"" rel=""nofollow noreferrer"">read but couldnt understand</a></li>
</ol>
<p>Suppose I have five rows in my DataFrame</p>
<pre><code>1. This is Foo, how can I help you
2. It might rain today
3. I love football
4. Crazy, Stupid &amp; Love
5. I shot the sheriff
</code></pre>
<p>In this, Can anyone help me to understand which one should be called as a Document, Vocabulary and Corpus</p>
",Dataset Preprocessing & Handling,corpus v vocabulary v document nlp simple understandable term read lot blog confused read couldnt understand suppose five row dataframe anyone help understand one called document vocabulary corpus
Read file lines and merge them based on their length,"<p><strong>EDIT Here is a a text file:</strong>
<a href=""https://www.gutenberg.org/files/9830/9830-0.txt"" rel=""nofollow noreferrer"">https://www.gutenberg.org/files/9830/9830-0.txt</a></p>
<p>I  have a file <code>test_file.txt </code> that consists of lines of various length sizes (number of words).
<strong>I want to load each line; check its length,</strong> if the length is more than or equal <code>&gt;=</code> a minimum threshold (say 20 words), then I append that line to the list named container: <code>container =  []</code>.
Else, <strong>I will have to load another line, and merge it with the current line till I reach that desired length size,</strong> then append the resulted line merge into the list <code>container</code>. I will have to do that for all the lines in the file.</p>
<p>Here is my code, it works until the last two lines, it ignores them.</p>
<pre><code># Creating a generator to load file lines, one by one:

def gen_file_reader(file_path):
    with open(file_path, encoding='utf-8') as file:
        for line in file.readlines():
            yield line

container = [] # List that will contain the results
lines = gen_file_reader('test_file.txt') # Calling the generator function


x = &quot;&quot;
for line in lines:
    while len(x.split()) &lt; 20:
        x = x + line
        break
    else:
        container.append(x)
        x = &quot;&quot;
        container.append(line)
</code></pre>
<p>I noticed my code doesn't work for the last two lines in the file, maybe because of the <code>break</code> keyword in the while statement ... There could be other bugs I am not aware of!</p>
<p><strong>EDIT: The End Result for the example file (Assuming we go rid of blank and empty lines), would look like this for the first 4 items in the list</strong> <code>container</code>:</p>
<pre><code>[&quot;Project Gutenberg's The Beautiful and Damned, by F. Scott Fitzgerald This eBook is for the use of anyone anywhere at no cost and with&quot;,
 'almost no restrictions whatsoever.  You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included',
 'with this eBook or online at www.gutenberg.org Title: The Beautiful and Damned Author: F. Scott Fitzgerald Release Date: October 22, 2003 [EBook #9830]',
 'Last updated: January 29, 2020 Language: English Character set encoding: UTF-8 *** START OF THIS PROJECT GUTENBERG EBOOK THE BEAUTIFUL AND DAMNED ***']

</code></pre>
",Dataset Preprocessing & Handling,read file line merge based length edit text file file consists line various length size number word want load line check length length equal minimum threshold say word append line list named container else load another line merge current line till reach desired length size append resulted line merge list line file code work last two line ignores noticed code work last two line file maybe keyword statement could bug aware edit end result example file assuming go rid blank empty line would look like first item list
Word frequency over time : How to count the word frequency by date?,"<p>I have a data frame look like this :</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>date</th>
<th>text</th>
</tr>
</thead>
<tbody>
<tr>
<td>201901</td>
<td>Thank you for helping me</td>
</tr>
<tr>
<td>201902</td>
<td>You  are amazing</td>
</tr>
<tr>
<td>201902</td>
<td>For helping with this</td>
</tr>
</tbody>
</table>
</div>
<p>My aim is to calculate the word frequency in each line, and eventually look like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">date</th>
<th style=""text-align: center;"">thank</th>
<th style=""text-align: right;"">you</th>
<th style=""text-align: right;"">for</th>
<th style=""text-align: right;"">helping</th>
<th style=""text-align: right;"">me</th>
<th style=""text-align: right;"">are</th>
<th style=""text-align: right;"">amazing</th>
<th style=""text-align: right;"">with</th>
<th style=""text-align: right;"">this</th>
<th style=""text-align: right;"">for</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">201901</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td style=""text-align: left;"">201902</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1</td>
</tr>
</tbody>
</table>
</div>
<p>The actual data set is like this frame, but contains millions of text lines. So I was wondering how to automate this process using R, without typing all those texts lines.</p>
",Dataset Preprocessing & Handling,word frequency time count word frequency date data frame look like date text thank helping amazing helping aim calculate word frequency line eventually look like date thank helping amazing actual data set like frame contains million text line wa wondering automate process using r without typing text line
Prepocessing textual data for Machine Learning,"<pre><code>'''
    import pandas as pd 
    import re
    articles_data = pd.read_csv('C:/Users/amrit/Downloads/data.csv') 
    print(articles_data.apply(lambda x: sum(x.isnull()))) 
    articles_nonNull = articles_data.dropna(subset=['text']) 
    articles_nonNull.reset_index(inplace=True)


    def clean_text(text):



    #Make text lowercase, remove text in square brackets,remove \n,remove punctuation and 
    remove words containing numbers.

    
    text = str(text).lower()
    text = re.sub('&lt;.*?&gt;+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text

    articles_nonNull['text_clean’] = articles_nonNull['text']
                                     .apply(lambda x:clean_text(x))

'''
</code></pre>
<p>I am trying to use this code to preprocess my data and keep hitting the &quot;invalid syntax error&quot; on the last two lines from articles_nonNull['text_clean']. Could someone help with this and why this is happening?</p>
<p>P.S. I am new to NLP and this is the first time I am handling an exceptionally large unstructured dataset.</p>
",Dataset Preprocessing & Handling,prepocessing textual data machine learning trying use code preprocess data keep hitting invalid syntax error last two line article nonnull text clean could someone help happening p new nlp first time handling exceptionally large unstructured dataset
Removing named entities from a document using spacy,"<p>I have tried to remove words from a document that are considered to be named entities by spacy, so basically removing ""Sweden"" and ""Nokia"" from the string example. I could not find a way to work around the problem that entities are stored as a span. So when comparing them with single tokens from a spacy doc, it prompts an error.</p>

<p>In a later step, this process is supposed to be a function applied to several text documents stored in a pandas data frame.</p>

<p>I would appreciate any kind of help and advice on how to maybe better post questions as this is my first one here.</p>

<pre><code>
nlp = spacy.load('en')

text_data = u'This is a text document that speaks about entities like Sweden and Nokia'

document = nlp(text_data)

text_no_namedentities = []

for word in document:
    if word not in document.ents:
        text_no_namedentities.append(word)

return "" "".join(text_no_namedentities)

</code></pre>

<p>It creates the following error:</p>

<blockquote>
  <p>TypeError: Argument 'other' has incorrect type (expected spacy.tokens.token.Token, got spacy.tokens.span.Span)</p>
</blockquote>
",Dataset Preprocessing & Handling,removing named entity document using spacy tried remove word document considered named entity spacy basically removing sweden nokia string example could find way work around problem entity stored span comparing single token spacy doc prompt error later step process supposed function applied several text document stored panda data frame would appreciate kind help advice maybe better post question first one creates following error typeerror argument ha incorrect type expected spacy token token token got spacy token span span
how to transform a posting file to a pytorch tensor,"<p>Is there a python package that transforms a postings file to a pytorch tensor?
By a posting file I mean a csv file with the following format:
&quot;docID&quot; ,&quot;wordID&quot; ,&quot;count&quot;</p>
<p>I also have a dictionary.txt which associates each wordID to a word.</p>
<p>At the end, my text data consists of postings file and a dictionary and I want to use it with a deep learning model that I have implemented with Pytorch.</p>
",Dataset Preprocessing & Handling,transform posting file pytorch tensor python package transforms posting file pytorch tensor posting file mean csv file following format docid wordid count also dictionary txt associate wordid word end text data consists posting file dictionary want use deep learning model implemented pytorch
NLP processing: splitting a review and keeping the rating associated with the review before split,"<p>I need help with this question.  I have a data frame that has a list of reviews.  The reviews are essays and i want to split the essays into sentences and assign them a sentiment score.  My goal is to create a weighted average of scores against the overall rating for the review.  I want to identify the reason why someone would give a low score against a high score and give a thorough analysis on what the underlying reason for the review.</p>
<p>Eg.  I love Disneyland.  This one i went to was a horrible experience.  There weren't enough rides and the bottled water was stupidly expensive.</p>
<p>In the above context (lets assume rank was 2 out of 5), i have split the essay into 3 sentences and assigned them a sentiment score:
[1, -0.45, -0.2]</p>
<p>I need to assign them the rank of 2 as well because they are all in the same essay.  Here is the code I'm using so far:</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_sm&quot;) # load spacey
sentences = [] # list my sentences in a review

for i in range(len(low_reviews['Review_Text'])):
  doc = nlp(low_reviews['Review_Text'].iloc[i]
  for sent in doc.sents:
    sentences.append(sent.text)
</code></pre>
<p>With the sentences, I apply the sentiment score using textblob:</p>
<pre><code># sentence sentiment analysis
sentiment = []
for i in sentences:
  blob = TextBlob(i)
  sentiment.append(blob.sentiment.polarity)
</code></pre>
<p>Then I create a Date Frame with the sentences and sentiment.  What I need is the Rank as well that was assigned for the review before i split it into its sentences.  I'm not sure how I can do this.</p>
<p>I would greatly appreciate if anyone can point me in the right direction.</p>
",Dataset Preprocessing & Handling,nlp processing splitting review keeping rating associated review split need help question data frame ha list review review essay want split essay sentence assign sentiment score goal create weighted average score overall rating review want identify reason someone would give low score high score give thorough analysis underlying reason review eg love disneyland one went wa horrible experience enough ride bottled water wa stupidly expensive context let assume rank wa split essay sentence assigned sentiment score need assign rank well essay code using far sentence apply sentiment score using textblob create date frame sentence sentiment need rank well wa assigned review split sentence sure would greatly appreciate anyone point right direction
Matching multiple strings in R/nlp/spacyr,"<p>I have a data frame</p>
<pre><code>myDataframe &lt;- data.frame(keyword = c(c(&quot;meeting&quot;, &quot;laptop&quot;),c(&quot;attend a meeting&quot;, &quot;fan&quot;)))
description &lt;- &quot;I have to attend a meeting.&quot;
</code></pre>
<p>I have to match the description with the column keyword in the dataframe and return &quot;attend a meeting&quot;.
Is there any solution to get a particular output</p>
",Dataset Preprocessing & Handling,matching multiple string r nlp spacyr data frame match description column keyword dataframe return attend meeting solution get particular output
Why is the term frequency displayed in my pyldavis visualization changing?,"<p>I am currently building an LDA model using bigrams as tokens.  I have a basic cleaning function that removes stopwords, lowercases, removes punctuation, and a function to take the tokenized column of documents and converts them to bigrams.</p>
<p>I am using Gensim for this, as the built-in corpus / dictionary methods make LDA pretty easy.  Once I get the documents cleaned and converted to bigrams, I create the dictionary / corpus and use the built-in methods to view occurrences of a certain bigram.</p>
<p>I have another function that takes as input; the dictionary / corpus objects, the column of documents as a list of lists of bigrams, and an integer range.  The function creates and saves an LDA model for each number in the integer range (number of topics), and a corresponding coherence score for the number of topics.  I use the returned num_topics / con_scores lists to plot the coh_scores by number of topics; the goal being to find an optimal number of topics for the input documents.</p>
<p>The problem is that each time I run this function for the exact same data, or use pyLDAvis to view the topics of one of the saved models, the term frequencies change; As in, for one of my bigrams 'stored_procedure_', the built-in Gensim methods confirm every time that the number of occurrences for that bigram is 98, but the pyLDAvis visualization term frequency (blue bar on the right hand side of the vis that is supposed to represent the total number of times the term occurs in the corpus) changes, which doesn't make any sense as the corpus is the same and never changes.  The term frequency also changes when I visualize different models saved by my function;  I.e. model6 (the model created with 6 topics) has a different term frequency than model8 (the model with 8 topics).  This doesn't add up to me as I am using the same corpus.</p>
<p>Please help.  Why does the term frequency change.  Screenshots below.</p>
<p><a href=""https://i.sstatic.net/Smfaz.png"" rel=""nofollow noreferrer"">output of cleaning function</a></p>
<p><a href=""https://i.sstatic.net/RJjOS.png"" rel=""nofollow noreferrer"">output of bigram function</a></p>
<p><a href=""https://i.sstatic.net/wwqEe.png"" rel=""nofollow noreferrer"">Vis for model with 6 topics (note term frequency on right of 'stored_procedure_')</a></p>
<p><a href=""https://i.sstatic.net/b9erl.png"" rel=""nofollow noreferrer"">Vis for model with 10 topics (note differing term frequency for 'stored_procedure_'</a></p>
",Dataset Preprocessing & Handling,term frequency displayed pyldavis visualization changing currently building lda model using bigram token basic cleaning function remove stopwords lowercase remove punctuation function take tokenized column document convert bigram using gensim built corpus dictionary method make lda pretty easy get document cleaned converted bigram create dictionary corpus use built method view occurrence certain bigram another function take input dictionary corpus object column document list list bigram integer range function creates save lda model number integer range number topic corresponding coherence score number topic use returned num topic con score list plot coh score number topic goal find optimal number topic input document problem time run function exact data use pyldavis view topic one saved model term frequency change one bigram stored procedure built gensim method confirm every time number occurrence bigram pyldavis visualization term frequency blue bar right hand side vi supposed represent total number time term occurs corpus change make sense corpus never change term frequency also change visualize different model saved function e model model created topic ha different term frequency model model topic add using corpus please help doe term frequency change screenshots output cleaning function output bigram function vi model topic note term frequency right stored procedure vi model topic note differing term frequency stored procedure
"Pandas, splitting a StringArray into an Array of StringArray","<p>I have a column in a pandas data frame where one of the columns is an array of strings as shown below.</p>
<pre><code>|column1                                                  |
|:--------------------------------------------------------|
|['abc&lt;t&gt;def&lt;t&gt;ghi', 'jkl&lt;t&gt;mno&lt;t&gt;pqr']                   |
|['abc&lt;t&gt;def&lt;t&gt;ghi', 'jkl&lt;t&gt;mno&lt;t&gt;pqr', 'def&lt;t&gt;pqr&lt;t&gt;jkl']|
|['ghi&lt;t&gt;jkl&lt;t&gt;pqr']                                      |
</code></pre>
<p>I need to split the column into an array of arrays such that the output looks like the table below</p>
<pre><code>|column2                                                             |
|:-------------------------------------------------------------------|
|[['abc', 'def', 'ghi'], ['jkl', 'mno', 'pqr']]                      |
|['abc', 'def', 'ghi'], ['jkl', 'mno', 'pqr'], ['def', 'pqr', 'jkl']]|
|[['ghi', 'jkl', 'pqr']]                                             |
</code></pre>
<p>I have tried using split as shown below but this returns not a number for all values</p>
<pre><code>dataset[&quot;column1&quot;].str.split(&quot;&lt;t&gt;&quot;)
</code></pre>
",Dataset Preprocessing & Handling,panda splitting stringarray array stringarray column panda data frame one column array string shown need split column array array output look like table tried using split shown return number value
NLP using XLM dataset,"<p>I am trying to do NLP on the dataset consisting of the following row</p>
<pre><code>00001 B 74457
00002 C 12804123 16026213 14627885
00004 A 15329425 9058342 11279767
</code></pre>
<p>where 1st element in the row is the identifier  2nd on is a label recommends, it can have only three labels $A, B, C$ and the number for examples 12804123 represent the id of the XML, it contains data, for example, text, location, etc. Based on this I need to extract the data from the XML file and use it to make a model. So first of all I want to extract some of the data from the XML file and make a data frame of structure data. An example of the XML file is below.
When I run the command pd.read_xml(xml) it gives</p>
<pre><code>    medlinecitation     pubmeddata
0   NaN     NaN
</code></pre>
<p>Any example from Kaggle or any other source etc I can follow to do the analysis.</p>
<pre><code>74457.xml = '''
&lt;pubmedarticleset&gt;
&lt;pubmedarticle&gt;
&lt;medlinecitation owner=&quot;NLM&quot; status=&quot;MEDLINE&quot;&gt;
&lt;pmid version=&quot;1&quot;&gt; 74457 &lt;/pmid&gt;
&lt;datecreated&gt;
&lt;year&gt; 1978 &lt;/year&gt;
&lt;month&gt; 03 &lt;/month&gt;
&lt;day&gt; 21 &lt;/day&gt;
&lt;/datecreated&gt;
&lt;datecompleted&gt;
&lt;year&gt; 1978 &lt;/year&gt;
&lt;month&gt; 03 &lt;/month&gt;
&lt;day&gt; 21 &lt;/day&gt;
&lt;/datecompleted&gt;
&lt;daterevised&gt;
&lt;year&gt; 2007 &lt;/year&gt;
&lt;month&gt; 11 &lt;/month&gt;
&lt;day&gt; 15 &lt;/day&gt;
&lt;/daterevised&gt;
&lt;article pubmodel=&quot;Print&quot;&gt;
&lt;journal&gt;
&lt;issn issntype=&quot;Print&quot;&gt; 0140-6736 &lt;/issn&gt;
&lt;journalissue citedmedium=&quot;Print&quot;&gt;
&lt;volume&gt; 1 &lt;/volume&gt;
&lt;issue&gt; 7984 &lt;/issue&gt;
&lt;pubdate&gt;
&lt;year&gt; 1976 &lt;/year&gt;
&lt;month&gt; Sep &lt;/month&gt;
&lt;day&gt; 4 &lt;/day&gt;
&lt;/pubdate&gt;
&lt;/journalissue&gt;
&lt;title&gt; Lancet &lt;/title&gt;
&lt;isoabbreviation&gt; Lancet &lt;/isoabbreviation&gt;
&lt;/journal&gt;
&lt;articletitle&gt;
Prophylactic treatment of alcoholism by lithium carbonate. A controlled study.
&lt;/articletitle&gt;
&lt;pagination&gt;
&lt;medlinepgn&gt; 481-2 &lt;/medlinepgn&gt;
&lt;/pagination&gt;
&lt;abstract&gt;
&lt;abstracttext&gt;
Lithium therapy has been shown to have a therapeutic influence in reducing the drinking and incapacity by alcohol in depressive alcoholics in a prospective double-blind placebo-controlled trial conducted over one year, but it had no significant effect on non-depressed patients. Patients in the trial treated by placebo had significantly greater alcoholic morbidity if they were depressive than if they were non-depressive.
&lt;/abstracttext&gt;
&lt;/abstract&gt;
&lt;authorlist completeyn=&quot;Y&quot;&gt;
&lt;author validyn=&quot;Y&quot;&gt;
&lt;lastname&gt; Merry &lt;/lastname&gt;
&lt;forename&gt; J &lt;/forename&gt;
&lt;initials&gt; J &lt;/initials&gt;
&lt;/author&gt;
&lt;author validyn=&quot;Y&quot;&gt;
&lt;lastname&gt; Reynolds &lt;/lastname&gt;
&lt;forename&gt; C M &lt;/forename&gt;
&lt;initials&gt; CM &lt;/initials&gt;
&lt;/author&gt;
&lt;author validyn=&quot;Y&quot;&gt;
&lt;lastname&gt; Bailey &lt;/lastname&gt;
&lt;forename&gt; J &lt;/forename&gt;
&lt;initials&gt; J &lt;/initials&gt;
&lt;/author&gt;
&lt;author validyn=&quot;Y&quot;&gt;
&lt;lastname&gt; Coppen &lt;/lastname&gt;
&lt;forename&gt; A &lt;/forename&gt;
&lt;initials&gt; A &lt;/initials&gt;
&lt;/author&gt;
&lt;/authorlist&gt;
&lt;language&gt; eng &lt;/language&gt;
&lt;publicationtypelist&gt;
&lt;publicationtype&gt; Clinical Trial &lt;/publicationtype&gt;
&lt;publicationtype&gt; Comparative Study &lt;/publicationtype&gt;
&lt;publicationtype&gt; Journal Article &lt;/publicationtype&gt;
&lt;publicationtype&gt; Randomized Controlled Trial &lt;/publicationtype&gt;
&lt;/publicationtypelist&gt;
&lt;/article&gt;
&lt;medlinejournalinfo&gt;
&lt;country&gt; ENGLAND &lt;/country&gt;
&lt;medlineta&gt; Lancet &lt;/medlineta&gt;
&lt;nlmuniqueid&gt; 2985213R &lt;/nlmuniqueid&gt;
&lt;issnlinking&gt; 0140-6736 &lt;/issnlinking&gt;
&lt;/medlinejournalinfo&gt;
&lt;chemicallist&gt;
&lt;chemical&gt;
&lt;registrynumber&gt; 0 &lt;/registrynumber&gt;
&lt;nameofsubstance&gt; Placebos &lt;/nameofsubstance&gt;
&lt;/chemical&gt;
&lt;chemical&gt;
&lt;registrynumber&gt; 7439-93-2 &lt;/registrynumber&gt;
&lt;nameofsubstance&gt; Lithium &lt;/nameofsubstance&gt;
&lt;/chemical&gt;
&lt;/chemicallist&gt;
&lt;citationsubset&gt; AIM &lt;/citationsubset&gt;
&lt;citationsubset&gt; IM &lt;/citationsubset&gt;
&lt;meshheadinglist&gt;
&lt;meshheading&gt;
&lt;descriptorname majortopicyn=&quot;N&quot;&gt; Adult &lt;/descriptorname&gt;
&lt;/meshheading&gt;
&lt;meshheading&gt;
&lt;descriptorname majortopicyn=&quot;N&quot;&gt; Alcohol Drinking &lt;/descriptorname&gt;
&lt;/meshheading&gt;
&lt;meshheading&gt;
&lt;descriptorname majortopicyn=&quot;N&quot;&gt; Alcoholism &lt;/descriptorname&gt;
&lt;qualifiername majortopicyn=&quot;Y&quot;&gt; drug therapy &lt;/qualifiername&gt;
&lt;/meshheading&gt;
&lt;meshheading&gt;
&lt;descriptorname majortopicyn=&quot;N&quot;&gt; Clinical Trials as Topic &lt;/descriptorname&gt;
&lt;/meshheading&gt;
&lt;meshheading&gt;
&lt;descriptorname majortopicyn=&quot;N&quot;&gt; Depression &lt;/descriptorname&gt;
&lt;qualifiername majortopicyn=&quot;N&quot;&gt; chemically induced &lt;/qualifiername&gt;
&lt;qualifiername majortopicyn=&quot;Y&quot;&gt; prevention &amp; control &lt;/qualifiername&gt;
&lt;/meshheading&gt;
&lt;meshheading&gt;
&lt;descriptorname majortopicyn=&quot;N&quot;&gt; Double-Blind Method &lt;/descriptorname&gt;
&lt;/meshheading&gt;
&lt;meshheading&gt;
&lt;descriptorname majortopicyn=&quot;N&quot;&gt; Drug Evaluation &lt;/descriptorname&gt;
&lt;/meshheading&gt;
&lt;meshheading&gt;
&lt;descriptorname majortopicyn=&quot;N&quot;&gt; Female &lt;/descriptorname&gt;
&lt;/meshheading&gt;
&lt;meshheading&gt;
&lt;descriptorname majortopicyn=&quot;N&quot;&gt; Humans &lt;/descriptorname&gt;
&lt;/meshheading&gt;
&lt;meshheading&gt;
&lt;descriptorname majortopicyn=&quot;N&quot;&gt; Lithium &lt;/descriptorname&gt;
&lt;qualifiername majortopicyn=&quot;Y&quot;&gt; therapeutic use &lt;/qualifiername&gt;
&lt;/meshheading&gt;
&lt;meshheading&gt;
&lt;descriptorname majortopicyn=&quot;N&quot;&gt; Male &lt;/descriptorname&gt;
&lt;/meshheading&gt;
&lt;meshheading&gt;
&lt;descriptorname majortopicyn=&quot;N&quot;&gt; Middle Aged &lt;/descriptorname&gt;
&lt;/meshheading&gt;
&lt;meshheading&gt;
&lt;descriptorname majortopicyn=&quot;N&quot;&gt; Placebos &lt;/descriptorname&gt;
&lt;/meshheading&gt;
&lt;/meshheadinglist&gt;
&lt;/medlinecitation&gt;
&lt;pubmeddata&gt;
&lt;history&gt;
&lt;pubmedpubdate pubstatus=&quot;pubmed&quot;&gt;
&lt;year&gt; 1976 &lt;/year&gt;
&lt;month&gt; 9 &lt;/month&gt;
&lt;day&gt; 4 &lt;/day&gt;
&lt;/pubmedpubdate&gt;
&lt;pubmedpubdate pubstatus=&quot;medline&quot;&gt;
&lt;year&gt; 1976 &lt;/year&gt;
&lt;month&gt; 9 &lt;/month&gt;
&lt;day&gt; 4 &lt;/day&gt;
&lt;hour&gt; 0 &lt;/hour&gt;
&lt;minute&gt; 1 &lt;/minute&gt;
&lt;/pubmedpubdate&gt;
&lt;pubmedpubdate pubstatus=&quot;entrez&quot;&gt;
&lt;year&gt; 1976 &lt;/year&gt;
&lt;month&gt; 9 &lt;/month&gt;
&lt;day&gt; 4 &lt;/day&gt;
&lt;hour&gt; 0 &lt;/hour&gt;
&lt;minute&gt; 0 &lt;/minute&gt;
&lt;/pubmedpubdate&gt;
&lt;/history&gt;
&lt;publicationstatus&gt; ppublish &lt;/publicationstatus&gt;
&lt;articleidlist&gt;
&lt;articleid idtype=&quot;pubmed&quot;&gt; 74457 &lt;/articleid&gt;
&lt;/articleidlist&gt;
&lt;/pubmeddata&gt;
&lt;/pubmedarticle&gt;
&lt;/pubmedarticleset&gt;'''
</code></pre>
<p>Please help me to understand what is happening? And how can I make it a data frame?</p>
",Dataset Preprocessing & Handling,nlp using xlm dataset trying nlp dataset consisting following row st element row identifier nd label recommends three label b c number example represent id xml contains data example text location etc based need extract data xml file use make model first want extract data xml file make data frame structure data example xml file run command pd read xml xml give example kaggle source etc follow analysis please help understand happening make data frame
NLP Text Pre Processing,"<p>I'm trying to pre-process some case judgement files available on the internet. These files are in JSON format but some of the files have strange text in them. I initially thought this could be utf-8 encoding issue but it doesn't solve the problem.</p>
<p>Below is a sample json text:
I would like to clean the text and perform text analytics to identify Named Entities.</p>
<pre><code>Karnataka High Court
Shinde, on 4 January, 2010
Author: Mohan Shantanagoudar
EN 'THE 1-ma COURT OF KARNATAKA CIRCUIT Bmca
AT GUI.-BARGA T '

DA'1'ED THIS THE ow: DAY OF' JANUAR§f&quot;'2C}.'1~{3._j:    

BEFORE

HONBLE MRJUSTICE MOHAN SE3-®;si*;';axt$:;é; GOU:&gt;%a$gi=:;;vT 3
\,v.P_Ng.40334;2QQ8 (éMl&lt;:pcT;« &quot;&quot;  
Bmmfaam:     

SURESE-I CHANDRA ,   - 
S/O LATE,' SHANKECRRAO .SHI'E$II);E§ &quot;
AGE ABOUT 52 YEARS, ' L 9'   
occ:AGR1CUL*mR1=:. &amp;A9voc';acY ' 
R:E:s1DENT Oms'-Q'NME§,KuN_19A..4  _ .«
TALUK: B}iALKI, §*éOW_R.E.S;IDING M' &quot; 
H.No.19--1C'--,-'4, NfEW*&quot;A'DARf3}i&quot;*~COLCNY_ '
BIDAR. D1533: BEAR.   

 1111    ._ ..PETITEONERS
{SR1 K;..M.&lt;j;HAi:r;..;x);3voa::é:1?:x;3-- I

SU'N'I'i'A xa§;&quot;0% SU3§iE_§SA1~~§ bciégxmorarx SE-{KNEE
AG  93395'? 4-:2. YE2}\RSs
ace: 1~§Q&quot;TU_SI*3WII'%&quot;i9:., ..... .4 «

 &quot; R.E;s:E&gt;::é§'r%'a3:§«f H.N9.§9/'Q 1, I\z'IUKEEI\EiE) NAGAR,

 ' Eﬁ}£;'§E{Ekf§iE'E3E§TEEi§._S0L§s.£3&quot;ﬁE?;{

., REZSELQNDE,-'1'éf&quot;§'



'EE'1§¢:~:.i£'€;:&quot;§i. ?€'€.§%i&lt;3:;. 32$ féieé :m.{§%3:' :%r%':££:E€$ 226 5: 22?

 :3? %;E:{7:-§:{m::s€é:§':,z':'E.i:';:: Q? §E.1€:§§.f&amp;-'£1; grag,-iiiggg if: q':,:.::2;-33%: {her {ﬁfﬁﬁf
</code></pre>
<p>Can anyone help me with some direction to solve this problem? Are these files corrupted?</p>
",Dataset Preprocessing & Handling,nlp text pre processing trying pre process case judgement file available internet file json format file strange text initially thought could utf encoding issue solve problem sample json text would like clean text perform text analytics identify named entity anyone help direction solve problem file corrupted
Spacy NLP with data from a Pandas DataFrame,"<p>I have a large pandas data frame of survey string responses, and we would like to trial some features of Spacy's NLP. We are just exploring the capabilities at the moment, but struggling with how to format the data into a format that works with the nlp function of spacy.</p>

<p>Eventually we would like to be able to look at popular topics in the string responses against their user data. </p>

<p>How do I run the nlp pipeline on a column of a dataframe? Or am I going around this the wrong way?</p>
",Dataset Preprocessing & Handling,spacy nlp data panda dataframe large panda data frame survey string response would like trial feature spacy nlp exploring capability moment struggling format data format work nlp function spacy eventually would like able look popular topic string response user data run nlp pipeline column dataframe going around wrong way
In Python - How to count the occurrences of the word “price” in each row of the column “content”?,"<p><a href=""https://i.sstatic.net/Dyof6.png"" rel=""nofollow noreferrer"">data frame</a></p>
<p>I hope the data is visible. It is a small textual data frame with row and columns. I need to count the occurrences of a specific word like &quot;price&quot; in each row of a column named &quot;content&quot;.</p>
",Dataset Preprocessing & Handling,python count occurrence word price row column content data frame hope data visible small textual data frame row column need count occurrence specific word like price row column named content
How to train a model in SageMaker Studio with .train and .test extension dataset files?,"<p>I'm trying to implement ML models with Amazon SageMaker Studio, the thing is that the model that I want to implement is from hugging face and It uses a Dataset from CONLL Corpora.</p>
<p>Following the instructions from the Hugging Face documentation, I have to read
a csv file with this instruction: <strong>train = pd.read_csv</strong>. But the problem comes with the dataset file extension because it's a .train and .test extension. The error I'm getting is: <em><strong>&quot;ParserError: Error tokenizing data. C error: Expected 1 fields in line 13, saw 3&quot;</strong></em></p>
<p>Is there a way to convert .test files to csv files? Or how should I read these files extensions?</p>
<p><em><strong>Links</strong></em></p>
<p>Dataset: <a href=""https://www.kaggle.com/nltkdata/conll-corpora"" rel=""nofollow noreferrer"">https://www.kaggle.com/nltkdata/conll-corpora</a></p>
<p>Model: <a href=""https://huggingface.co/mrm8488/bert-spanish-cased-finetuned-ner"" rel=""nofollow noreferrer"">https://huggingface.co/mrm8488/bert-spanish-cased-finetuned-ner</a></p>
",Dataset Preprocessing & Handling,train model sagemaker studio train test extension dataset file trying implement ml model amazon sagemaker studio thing model want implement hugging face us dataset conll corpus following instruction hugging face documentation read csv file instruction train pd read csv problem come dataset file extension train test extension error getting parsererror error tokenizing data c error expected field line saw way convert test file csv file read file extension link dataset model
How do I remove elements that do not directly contain text themselves - Python BeautifulSoup?,"<p><strong>Problem summary:</strong> I want to preprocess HTML documents for NLP tasks, and one step of preprocessing is to remove elements/tags that do not contain text directly. I've tried my best to find an approach that works, but even the simple version of the <kbd>remove_tags_without_text()</kbd> function doesn't work. Can anyone help me with this?</p>
<p><strong>Test input:</strong></p>
<pre class=""lang-py prettyprint-override""><code>&quot;&quot;&quot;
&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;body&gt;
    &lt;p&gt;
      &lt;p&gt;
        &lt;p&gt;this should be here
          &lt;br/&gt;this should be here
        &lt;/p&gt;
      &lt;/p&gt;
    &lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/body&gt;
&lt;/html&gt;
&quot;&quot;&quot;
</code></pre>
<p><strong>Expected output:</strong></p>
<pre class=""lang-py prettyprint-override""><code>&quot;&quot;&quot;
&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;body&gt;
    &lt;p&gt;this should be here
      &lt;br/&gt;this should be here
    &lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/body&gt;
&lt;/html&gt;
&quot;&quot;&quot;
</code></pre>
<p><strong>Code:</strong></p>
<pre class=""lang-py prettyprint-override""><code>from bs4 import BeautifulSoup

def remove_tags_without_text(text:str) -&gt; str:
  locked = [&quot;html&quot;, &quot;body&quot;, &quot;ul&quot;, &quot;br&quot;]
  soup = BeautifulSoup(text, 'html.parser')
  #Remove tags with no text
  for tag in soup.find_all():
    if tag.name in locked:
      continue
    children = tag.findChildren(recursive=False)
    if len(children) &lt; 1:
      continue
    first_child = children[0]
    if tag.text is None:
      tag.replaceWith(first_child)
    children = children[1:]
    last_child = first_child
    for child in children:
      current_child = child
      last_child.insert_after(current_child)
      last_child = current_child
  return soup.prettify()

def main():
  test = &quot;&quot;&quot;
  &lt;!DOCTYPE html&gt;
  &lt;html&gt;
    &lt;body&gt;
      &lt;p&gt;
        &lt;p&gt;
          &lt;p&gt;this should be here
            &lt;br/&gt;this should be here
          &lt;/p&gt;
        &lt;/p&gt;
      &lt;/p&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;/li&gt;
        &lt;li&gt;&lt;/li&gt;
        &lt;li&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/body&gt;
  &lt;/html&gt;
  &quot;&quot;&quot;
  result = remove_tags_without_text(test)
  print(result)
  return 0

if __name__ == '__main__':
  main()
</code></pre>
",Dataset Preprocessing & Handling,remove element directly contain text python beautifulsoup problem summary want preprocess html document nlp task one step preprocessing remove element tag contain text directly tried best find approach work even simple version remove tag without text function work anyone help test input expected output code
Output 2D array to a Matrix as a CSV - Python,"<p>I have a 2D array with vectorised rows with each row representing a document in the corpus:</p>
<pre><code>array[[ 0.0 0.0 0.4583 0.6584 0.0]
                              ...
      [0.4390 0.0 0.0 0.5749 0.0]]
</code></pre>
<p>I have calculated cosine similarity for each row/vector in the 2D array with every other vector like so:</p>
<pre><code>#calculate semantic similarity for all permutations all in one go
for i in range(Vectors.shape[0]): #for each vector/row in 2D array
    for j in range(i + 1, Vectors.shape[0]): #for each row + 1 in the 2D array
        cosine_similarities = linear_kernel(Vectors[i], Vectors[j]).flatten()
        #np.savetxt(&quot;foo.csv&quot;, cosine_similarities, delimiter=&quot;,&quot;)
        pd.DataFrame(cosine_similarities).to_csv(&quot;test_matrix.csv&quot;, mode = 'a') #save into csv as a matirix
</code></pre>
<p>The output prior to saving into a csv looks like:</p>
<pre><code>[0.5748389]
[0.5847379]
...
[0.3257490]
</code></pre>
<p>How am I able to transform the output into a <strong>matrix</strong> and save that into a csv?</p>
<p>The output I'm looking for is:</p>
<pre><code>   0          1           ...  76
0  0.5748389  0.5847379        0.3257490
1  ...        ...         ...   ...
...
76
</code></pre>
<hr />
<p><strong>UPDATE:</strong>
I followed this and it worked out! Using cosine similarity function directly on a sparse matrix worked, and then converted it to a list and then dataframe. See: <a href=""https://stackoverflow.com/questions/17627219/whats-the-fastest-way-in-python-to-calculate-cosine-similarity-given-sparse-mat"">What&#39;s the fastest way in Python to calculate cosine similarity given sparse matrix data?</a> for more info!</p>
",Dataset Preprocessing & Handling,output array matrix csv python array vectorised row row representing document corpus calculated cosine similarity row vector array every vector like output prior saving csv look like able transform output matrix save csv output looking update followed worked using cosine similarity function directly sparse matrix worked converted list dataframe see href fastest way python calculate cosine similarity given sparse matrix data info
The best and simple way to convert labeled text classification data to spaCy v3 format,"<p>Let's suppose we have labeled data for text classification in a nice CSV file. We have 2 columns - &quot;text&quot; and &quot;label&quot;. I am kind of struggling to understand spacy V3. documentation. If I understand the correctly main sources of examples of spacy v3 documentation are THIS PROJECTS ()<a href=""https://github.com/explosion/projects/tree/v3/tutorials"" rel=""nofollow noreferrer"">https://github.com/explosion/projects/tree/v3/tutorials</a>).</p>
<p>However, the training data are already prepared in the expected JSON nested structure format.</p>
<p>If I want to perform costume text classification in spacy v3 I need to convert the data to the example structure - e.g LIKE HERE (<a href=""https://github.com/explosion/projects/blob/v3/tutorials/textcat_docs_issues/assets/docs_issues_eval.jsonl"" rel=""nofollow noreferrer"">https://github.com/explosion/projects/blob/v3/tutorials/textcat_docs_issues/assets/docs_issues_eval.jsonl</a>).</p>
<p>How to get from pandas data frame to here? Does prodigy support labeled data to spacy format? Let's have small example of the dataset</p>
<pre class=""lang-py prettyprint-override""><code>pd.DataFrame({
    &quot;TEXT&quot;:[
    &quot;i really like this post&quot;,
    &quot;thanks for that comment&quot;,
    &quot;i enjoy this friendly forum&quot;,
    &quot;this is a bad post&quot;,
    &quot;i dislike this article&quot;,
    &quot;this is not well written&quot;,
    &quot;who came up with this stupid idea?&quot;,
    &quot;This is just completely wrong!!&quot;,
    &quot;Get out of here now!!!!&quot;],
    &quot;LABEL&quot;: [
        &quot;POS&quot;, &quot;POS&quot;, &quot;POS&quot;, &quot;NEG&quot;, &quot;NEG&quot;, &quot;NEG&quot;, &quot;RUDE&quot;, &quot;RUDE&quot;, &quot;RUDE&quot;
    ]

})
</code></pre>
",Dataset Preprocessing & Handling,best simple way convert labeled text classification data spacy v format let suppose labeled data text classification nice csv file column text label kind struggling understand spacy v documentation understand correctly main source example spacy v documentation project however training data already prepared expected json nested structure format want perform costume text classification spacy v need convert data example structure e g like get panda data frame doe prodigy support labeled data spacy format let small example dataset
kwic() function returns less rows than it should,"<p>I'm currently trying to perform a sentiment analysis on a <code>kwic</code> object, but I'm afraid that the <code>kwic()</code> function does not return all rows it should return. I'm not quite sure what exactly the issue is which makes it hard to post a reproducible example, so I hope that a detailed explanation of what I'm trying to do will suffice.</p>
<p>I subsetted the original dataset containing speeches I want to analyze to a new data frame that only includes speeches mentioning certain keywords. I used the following code to create this subset:</p>
<pre><code>ostalgie_cluster &lt;- full_data %&gt;%
  filter(grepl('Schwester Agnes|Intershop|Interflug|Trabant|Trabi|Ostalgie',
                speechContent,
                ignore.case = TRUE))
</code></pre>
<p>The resulting data frame consists of 201 observations. When I perform <code>kwic()</code> on the same initial dataset using the following code, however, it returns a data frame with only 82 observations. Does anyone know what might cause this? Again, I'm sorry I can't provide a reproducible example, but when I try to create a reprex from scratch it just.. works...</p>
<pre><code>#create quanteda corpus object
qtd_speeches_corp &lt;- corpus(full_data,
                            docid_field = &quot;id&quot;,
                            text_field = &quot;speechContent&quot;)

#tokenize speeches
qtd_tokens &lt;- tokens(qtd_speeches_corp, 
                     remove_punct = TRUE,
                     remove_numbers = TRUE,
                     remove_symbols = TRUE,
                     padding = FALSE) %&gt;%
  tokens_remove(stopwords(&quot;de&quot;), padding = FALSE) %&gt;%
  tokens_compound(pattern = phrase(c(&quot;Schwester Agnes&quot;)), concatenator = &quot; &quot;)

ostalgie_words &lt;- c(&quot;Schwester Agnes&quot;, &quot;Intershop&quot;, &quot;Interflug&quot;, &quot;Trabant&quot;, &quot;Trabi&quot;, &quot;Ostalgie&quot;)

test_kwic &lt;- kwic(qtd_tokens,
                  pattern = ostalgie_words,
                  window = 5)
</code></pre>
",Dataset Preprocessing & Handling,kwic function return le row currently trying perform sentiment analysis object afraid function doe return row return quite sure exactly issue make hard post reproducible example hope detailed explanation trying suffice subsetted original dataset containing speech want analyze new data frame includes speech mentioning certain keywords used following code create subset resulting data frame consists observation perform initial dataset using following code however return data frame observation doe anyone know might cause sorry provide reproducible example try create reprex scratch work
Data Extraction in Python,"<p>I've been given a data set consisting of three columns. One column has transaction information, one has a store number, and one has sections. My goal is to extract the store number from the transaction information column for 300 different stores using entity extraction. My thought process behind this was to make something similar to how companies search resumes for key words using a word bank, since I have the store numbers in a separate column already. I have the .csv file read into my program, and I have the store numbers stored into their own array. I'm trying to figure out how to search the transaction information column for those store numbers.</p>
<p>Code so far:</p>
<pre><code>import pandas as pd
import numpy as np

file = pd.read_csv(r'C:\Users\cspea\Desktop\assignment.csv')
print(file)

store_number_array = file['store_number'].to_numpy()
print(store_number_array)
</code></pre>
<p>Sample data set (in .csv format):</p>
<pre><code>transaction_descriptor,store_number,dataset
DOLRTREE 2257 00022574 ROSWELL,2257,train
AUTOZONE #3547,3547,train
TGI FRIDAYS 1485 0000,1485,train
BUFFALO WILD WINGS 003,3,train
J. CREW #568 0,568,train
</code></pre>
<p>Any tips would be greatly appreciated. Thanks for your time and assistance in advance :)</p>
",Dataset Preprocessing & Handling,data extraction python given data set consisting three column one column ha transaction information one ha store number one ha section goal extract store number transaction information column different store using entity extraction thought process behind wa make something similar company search resume key word using word bank since store number separate column already csv file read program store number stored array trying figure search transaction information column store number code far sample data set csv format tip would greatly appreciated thanks time assistance advance
How do I find most frequent words by each observation in R?,"<p><em>I am very new to NLP. Please, don't judge me strictly.</em></p>

<p>I have got a very big data-frame on customers' feedback, my goal is to analyze feedbacks. I tokenized words in feedbacks, deleted stop-words (SMART). Now, I need to receive a table of most and less frequent used words.</p>

<p>The code looks like this:</p>

<pre><code>library(tokenizers)
library(stopwords)
words_as_tokens &lt;- 
     tokenize_words(dat$description, 
                    stopwords = stopwords(language = ""en"", source = ""smart""))
</code></pre>

<p>The dataframe looks like this: there are lots of feedbacks (variable ""description"") and customers by whom the feedbacks were given (each customer is not unique, they can be repeated). I want to receive a table with 3 columns: a) customer name b) word c) its frequency. This ""ranking"" should be in a decreasing order.</p>
",Dataset Preprocessing & Handling,find frequent word observation r new nlp please judge strictly got big data frame customer feedback goal analyze feedback tokenized word feedback deleted stop word smart need receive table le frequent used word code look like dataframe look like lot feedback variable description customer feedback given customer unique repeated want receive table column customer name b word c frequency ranking decreasing order
Value error trying to fit a logistic regression with SentenceTransformer output (embeddig),"<p>My code:</p>
<p><code>model = SentenceTransformer('hiiamsid/sentence_similarity_spanish_es')</code></p>
<p>I apply the model to the text column of the data frame</p>
<p><code>prueba['encoder'] = prueba.texto.apply(lambda x: model.encode(x))</code></p>
<p>Then I Fit a logistic regression with the <code>encoder</code> column and the <code>label</code> column.</p>
<p><code>clf = LogisticRegression(random_state=0).fit(prueba.encoder, prueba.label)</code></p>
<p>And I got this error:</p>
<p><code>ValueError: setting an array element with a sequence.</code></p>
",Dataset Preprocessing & Handling,value error trying fit logistic regression sentencetransformer output embeddig code apply model text column data frame fit logistic regression column column got error
R: How can I add titles based on grouping variable in word_associate?,"<p>I am using the <a href=""https://rdrr.io/cran/qdap/man/word_associate.html"" rel=""nofollow noreferrer"">word_associate package</a> in R Markdown to create word clouds across a grouping variable with multiple categories. I would like the titles of each word cloud to be drawn from the character values of the grouping variable.</p>
<p>I have added trans_cloud(title=TRUE) to my code, but have not been able to resolve my problem. Here's my code, which runs but doesn't produce graphs with titles:</p>
<pre><code>library(qdap)
word_associate(df$text, match.string=c(&quot;cat&quot;), grouping.var=c(df$id), 
               text.unit=&quot;sentence&quot;, stopwords=c(Top200Words), wordcloud=TRUE, cloud.colors=c(&quot;#0000ff&quot;,&quot;#FF0000&quot;), 
               trans_cloud(title=TRUE)) 
</code></pre>
<p>I have also tried the following, which does not run:</p>
<pre><code>library(qdap)
word_associate(df$text, match.string=c(&quot;cat&quot;), grouping.var=c(df$id), 
text.unit=&quot;sentence&quot;, stopwords=c(Top200Words), wordcloud=TRUE, cloud.colors=c(&quot;#0000ff&quot;,&quot;#FF0000&quot;), 
title=TRUE) 
</code></pre>
<p>Can anyone help me figure this out? I can't find any guidance in the documentation and there's hardly any examples of or discussions about word_associate on the web.</p>
<p>Here's an example data frame that reproduces the problem:</p>
<pre><code>id          text
question1   I love cats even though I'm allergic to them.
question1   I hate cats because I'm allergic to them.
question1   Cats are funny, cute, and sweet.
question1   Cats are mean and they scratch me.     
question2   I only pet cats when I have my epipen nearby.
question2   I avoid petting cats at all cost.
question2   I visit a cat cafe every week. They have 100 cats.
question2   I tried to pet my friend's cat and it attacked me.
</code></pre>
<p>Note that if I run this in R (instead of Markdown), the figures automatically print the &quot;question1_list1&quot; and &quot;question2_list1&quot; in bright blue at the top of the figure file. This doesn't work for me because I need the titles to exclude &quot;_list1&quot; and be written in black. These automatically generated titles do not respond to changes in my trans_cloud specifications.
Ex:</p>
<pre><code>library(qdap)
word_associate(df$text, match.string=c(&quot;cat&quot;), grouping.var=c(df$id), 
               text.unit=&quot;sentence&quot;, stopwords=c(Top200Words), wordcloud=TRUE, cloud.colors=c(&quot;#0000ff&quot;,&quot;#FF0000&quot;), 
               trans_cloud(title=TRUE, title.color=&quot;#000000&quot;)) 
</code></pre>
<p>In addition, I'm locked in to using this package (as opposed to other options for creating word clouds in R) because I'm using it to create network plots, too.</p>
",Dataset Preprocessing & Handling,r add title based grouping variable word associate using word associate package r markdown create word cloud across grouping variable multiple category would like title word cloud drawn character value grouping variable added trans cloud title true code able resolve problem code run produce graph title also tried following doe run anyone help figure find guidance documentation hardly example discussion word associate web example data frame reproduces problem note run r instead markdown figure automatically print question list question list bright blue top figure file work need title exclude list written black automatically generated title respond change trans cloud specification ex addition locked using package opposed option creating word cloud r using create network plot
How to count the number of spoken syllables in an audio file?,"<p>I have many audio files with clean audio and only spoken voice in Mandarin Chinese. I need to estimate of how many syllables are spoken in each file. Is there a tool for OS X, Windows, or Linux that can estimate these?</p>

<pre><code>sample01.wav 15
sample02.wav 8
sample03.wav 5
sample04.wav 1
sample05.wav 18
</code></pre>

<p>As there are many files, command-line or batch-capable software is preferred, e.g.:</p>

<pre><code>$ application sample01.wav
15
</code></pre>

<ul>
<li>A solution that uses speech-to-text, then counts the number of characters present would be suitable to.</li>
</ul>
",Dataset Preprocessing & Handling,count number spoken syllable audio file many audio file clean audio spoken voice mandarin chinese need estimate many syllable spoken file tool x window linux estimate many file command line batch capable software preferred e g solution us speech text count number character present would suitable
Extract a 100-Character Window around Keywords in Text Data with R (Quanteda or Tidytext Packages),"<p>This is my first time asking a question on here so I hope I don't miss any crucial parts. I want to perform sentiment analysis on windows of speeches around certain keywords. My dataset is a large csv file containing a number of speeches, but I'm only interest in the sentiment of the words immediately surrounding certain key words.</p>
<p>I was told that the quanteda package in R would likely be my best bet for finding such a function, but I've been unsuccessful in locating it so far. If anyone knows how to do such a task it would be <em>greatly</em> appreciated !!!</p>
<p>Reprex (I hope?) below:</p>
<pre><code>speech = c(&quot;This is the first speech. Many words are in this speech, but only few are relevant for my research question. One relevant word, for example, is the word stackoverflow. However there are so many more words that I am not interested in assessing the sentiment of&quot;, &quot;This is a second speech, much shorter than the first one. It still includes the word of interest, but at the very end. stackoverflow.&quot;, &quot;this is the third speech, and this speech does not include the word of interest so I'm not interested in assessing this speech.&quot;)

data &lt;- data.frame(id=1:3, 
                   speechContent = speech)
</code></pre>
",Dataset Preprocessing & Handling,extract character window around keywords text data r quanteda tidytext package first time asking question hope miss crucial part want perform sentiment analysis window speech around certain keywords dataset large csv file containing number speech interest sentiment word immediately surrounding certain key word wa told quanteda package r would likely best bet finding function unsuccessful locating far anyone know task would greatly appreciated reprex hope
R: Calculating the Cosine Similarity Between Restaurant Reviews,"<p>I am working with the R programming language.</p>
<p>Suppose I have the following data frame that contains data on 8 different restaurant reviews (taken from here: <a href=""https://www.consumeraffairs.com/food/mcd.html?page=2#scroll_to_reviews=true"" rel=""nofollow noreferrer"">https://www.consumeraffairs.com/food/mcd.html?page=2#scroll_to_reviews=true</a>) :</p>
<pre><code>text = structure(list(id = 1:8, reviews = c(&quot;I guess the employee decided to buy their lunch with my card my card hoping I wouldn't notice but since it took so long to run my car I want to head and check my bank account and sure enough they had bought food on my card that I did not receive leave. Had to demand for and for a refund because they acted like it was my fault and told me the charges are still pending even though they are for 2 different amounts.&quot;, 
&quot;I went to McDonald's and they charge me 50 for Big Mac when I only came with 49. The casher told me that I can't read correctly and told me to get glasses. I am file a report on your casher and now I'm mad.&quot;, 
&quot;I really think that if you can buy breakfast anytime then I should be able to get a cheeseburger anytime especially since I really don't care for breakfast food. I really like McDonald's food but I preferred tree lunch rather than breakfast. Thank you thank you thank you.&quot;, 
&quot;I guess the employee decided to buy their lunch with my card my card hoping I wouldn't notice but since it took so long to run my car I want to head and check my bank account and sure enough they had bought food on my card that I did not receive leave. Had to demand for and for a refund because they acted like it was my fault and told me the charges are still pending even though they are for 2 different amounts.&quot;, 
&quot;Never order McDonald's from Uber or Skip or any delivery service for that matter, most particularly one on Elgin Street and Rideau Street, they never get the order right. Workers at either of these locations don't know how to follow simple instructions. Don't waste your money at these two locations.&quot;, 
&quot;Employees left me out in the snow and wouldn’t answer the drive through. They locked the doors and it was freezing. I asked the employee a simple question and they were so stupid they answered a completely different question. Dumb employees and bad food.&quot;, 
&quot;McDonalds food was always so good but ever since they add new/more crispy chicken sandwiches it has come out bad. At first I thought oh they must haven't had a good day but every time I go there now it's always soggy, and has no flavor. They need to fix this!!!&quot;, 
&quot;I just ordered the new crispy chicken sandwich and I'm very disappointed. Not only did it taste horrible, but it was more bun than chicken. Not at all like the commercial shows. I hate sweet pickles and there were two slices on my sandwich. I wish I could add a photo to show the huge bun and tiny chicken.&quot;
)), class = &quot;data.frame&quot;, row.names = c(NA, -8L))
</code></pre>
<p>I would like to find out which reviews are similar to each other (e.g. perhaps 1,2 and 3 are similar to each other, 5,7,1 are similar to each other, 7,2 are similar to each other, etc.). I tried to research to see if there is some method that can be used to accomplish this task -  in particular, I found out about something called the &quot;Cosine Distance&quot; which is apparently used often for similar tasks in NLP and Text Mining.</p>
<p>I tried to follow the instructions here to accomplish this task: <a href=""https://stackoverflow.com/questions/67811628/cosine-similarity-matrix-in-r"">Cosine Similarity Matrix in R</a></p>
<pre><code>library(tm)
library(proxy)

text = text[,2]

corpus &lt;- VCorpus(VectorSource(text))
tdm &lt;- TermDocumentMatrix(corpus, 
    control = list(wordLengths = c(1, Inf)))
occurrence &lt;- apply(X = tdm, 
    MARGIN = 1, 
    FUN = function(x) sum(x &gt; 0) / ncol(tdm))

tdm_mat &lt;- as.matrix(tdm[names(occurrence)[occurrence &gt;= 0.5], ])

dist(tdm_mat, method = &quot;cosine&quot;, upper = TRUE)
</code></pre>
<p><strong>My Question:</strong> The above code seems to run without errors, but I am not sure if this code is able to indicate which restaurant reviews are similar to one another.</p>
<p><strong>Can someone please show me how to do this?</strong></p>
<p>Thanks!</p>
",Dataset Preprocessing & Handling,r calculating cosine similarity restaurant review working r programming language suppose following data frame contains data different restaurant review taken would like find review similar e g perhaps similar similar similar etc tried research see method used accomplish task particular found something called cosine distance apparently used often similar task nlp text mining tried follow instruction accomplish task href similarity matrix r question code seems run without error sure code able indicate restaurant review similar one another someone please show thanks
How to read online text file in pandas?,"<p>I am doing a Machine Translation, I don't want to download data, instead of I need to use pandas to read the file online, But I won't. Please help meeeeee</p>
<pre><code>import pandas as pd
df = pd.read_csv('https://www.statmt.org/europarl/v7/fr-en.tgz')
</code></pre>
<p>I am getting many error, please help me with this. ...</p>
",Dataset Preprocessing & Handling,read online text file panda machine translation want download data instead need use panda read file online please help meeeeee getting many error please help
Sentences are splitting into letters,"<p>I am creating a chatbot and i am new to NLP. I am trying to extract the Action and Sentence title from the csv file. The sentences are being split into letters.</p>
<p>Here is the code and a screenshot of the sentences being split into letters rather than being on a rows.</p>
<pre><code>data = pd.read_csv('dataset.csv')

dataset = pd.DataFrame(columns=['Action', 'Sentence', 'Category'])
for index, item in data.iterrows():
    intent = item.Action
    for t, r in zip(item['Sentence'], item['Category']):
        # print(t,r)
        row = {'Action': intent, 'Sentence': t, 'Category':r}
        dataset = dataset.append(row, ignore_index=True)
dataset
</code></pre>
<p><a href=""https://i.sstatic.net/sop6Q.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Any help is greatly appreciated please.</p>
",Dataset Preprocessing & Handling,sentence splitting letter creating chatbot new nlp trying extract action sentence title csv file sentence split letter code screenshot sentence split letter rather row enter image description help greatly appreciated please
"Extracting mentions, hashtags and, urls and placing them in a new column in a Twitter Dataset with R","<p>I have a Twitter dataset of 30000 tweets and I'm trying to prepare the data for text analysis. I downloaded the dataset with academictwitteR package in R. Inside the dataset, some columns (such as; &quot;user.metrics&quot;, &quot;public.metrics&quot;, &quot;entities&quot; are seperate data frames. I managed to extract  the columns from &quot;user.metrics&quot; and &quot;public.metrics&quot; and merge the extracted columns with my original dataset as following, without a problem;</p>
<pre><code>#extract
extract_publicmetrics &lt;- as.data.frame(mytwitterdata$public_metrics)
colnames(extract_publicmetrics)
[1] &quot;retweet_count&quot; &quot;reply_count&quot;   &quot;like_count&quot;    &quot;quote_count&quot;

#add observation column to bind with the original data (mytwitterdata)
addconsecutivenumbers1 &lt;- cbind(extract_publicmetrics, &quot;observation&quot;=1:nrow(deneme2_publicmetrics)) 
addconsecutivenumbers2 &lt;- cbind(mytwitterdata, &quot;observation&quot;=1:nrow(joined_deneme2))
#merge two data
merged.data &lt;- merge(addconsecutivenumbers1, addconsecutivenumbers2, by=&quot;observation&quot;)
</code></pre>
<p>But, I could not manage to extract &quot;mentions&quot;, &quot;urls&quot;, &quot;hastags&quot; columns from &quot;Entities&quot; dataframe in my dataset.I think it's because &quot;mentions&quot;, &quot;urls&quot;, &quot;hashtags&quot; are nested lists in that data frame (e.g.):</p>
<pre><code>class(mytwitterdata$entities$hashtags)
[1] &quot;list&quot;
</code></pre>
<p>For example, a Tweet may contain no hashtag, one hashtag, or more than one hashtag. I want to create a new column from that list in which the value of the row is NA when there is no hashtag, or the row includes the hashtag as text in the row ( or hashtags separated with commas when it includes more than one hashtag).</p>
<p>Attached is s sample data of 10 rows extracted from the &quot;Entities&quot; dataframe from my dataset:</p>
<p><a href=""https://drive.google.com/file/d/1vfyFIObRS9tCxGNJCG9AMyKgxwgwBDMZ/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1vfyFIObRS9tCxGNJCG9AMyKgxwgwBDMZ/view?usp=sharing</a></p>
",Dataset Preprocessing & Handling,extracting mention hashtags url placing new column twitter dataset r twitter dataset tweet trying prepare data text analysis downloaded dataset academictwitter package r inside dataset column user metric public metric entity seperate data frame managed extract column user metric public metric merge extracted column original dataset following without problem could manage extract mention url hastags column entity dataframe dataset think mention url hashtags nested list data frame e g example tweet may contain hashtag one hashtag one hashtag want create new column list value row na hashtag row includes hashtag text row hashtags separated comma includes one hashtag attached sample data row extracted entity dataframe dataset
How to extract out unique words and there pos tags in separate columns while working with Dataset,"<p>I am working through Indonesian Data to use data for NER and as I get to know, there is no pretrained NLTK model to help for this language. So, to do this manually I tried to extract all the unique words used in the entire data frame, I still don't know how to apply tags to the words but this is what I did so far.</p>
<p><a href=""https://i.sstatic.net/gvfab.jpg"" rel=""nofollow noreferrer"">the first step</a>,</p>
<p><a href=""https://i.sstatic.net/mYLiD.jpg"" rel=""nofollow noreferrer"">the second step</a>,</p>
<p><a href=""https://i.sstatic.net/j2PdH.jpg"" rel=""nofollow noreferrer"">the third step</a>,</p>
<p><a href=""https://i.sstatic.net/br6Ba.jpg"" rel=""nofollow noreferrer"">the fourth step</a></p>
<p>please let me know if there is any other convenient way to do this, what I did in the following codes. also, let me know how to add tags to each row(if possible) and how to do NER for this.</p>
<p>(I am new to coding that's why I don't know how to ask, but I am trying my best to provide as much information as possible.)</p>
",Dataset Preprocessing & Handling,extract unique word po tag separate column working dataset working indonesian data use data ner get know pretrained nltk model help language manually tried extract unique word used entire data frame still know apply tag word far first step second step third step fourth step please let know convenient way following code also let know add tag row possible ner new coding know ask trying best provide much information possible
Prodigy + Spacy to train dataset,"<p>I would like an example of using Prodigy to train a dataset (text file with some named entities). This file would be in Portuguese. The idea would be to train a model with Prodigy to make notes for the recognition of named entities (I'm not sure how Prodigy works). And then use Spacy + Python to load a trained model and apply it to new files to recognize named entities.</p>

<p>Thank you</p>
",Dataset Preprocessing & Handling,prodigy spacy train dataset would like example using prodigy train dataset text file named entity file would portuguese idea would train model prodigy make note recognition named entity sure prodigy work use spacy python load trained model apply new file recognize named entity thank
number of matches for keywords in specified categories,"<p>For a large scale text analysis problem, I have a data frame containing words that fall into different categories, and a data frame containing a column with strings and (empty) counting columns for each category. I now want to take each individual string, check which of the defined words appear, and count them within the appropriate category.</p>
<p>As a simplified example, given the two data frames below, i want to count how many of each animal type appear in the text cell.</p>
<pre><code>df_texts &lt;- tibble(
  text=c(&quot;the ape and the fox&quot;, &quot;the tortoise and the hare&quot;, &quot;the owl and the the 
  grasshopper&quot;),
  mammals=NA,
  reptiles=NA,
  birds=NA,
  insects=NA
)

df_animals &lt;- tibble(animals=c(&quot;ape&quot;, &quot;fox&quot;, &quot;tortoise&quot;, &quot;hare&quot;, &quot;owl&quot;, &quot;grasshopper&quot;),
           type=c(&quot;mammal&quot;, &quot;mammal&quot;, &quot;reptile&quot;, &quot;mammal&quot;, &quot;bird&quot;, &quot;insect&quot;))
</code></pre>
<p>So my desired result would be:</p>
<pre><code>df_result &lt;- tibble(
  text=c(&quot;the ape and the fox&quot;, &quot;the tortoise and the hare&quot;, &quot;the owl and the the 
  grasshopper&quot;),
  mammals=c(2,1,0),
  reptiles=c(0,1,0),
  birds=c(0,0,1),
  insects=c(0,0,1)
)
</code></pre>
<p>Is there a straightforward way to achieve this keyword-matching-and-counting that would be applicable to a much larger dataset?</p>
<p>Thanks in advance!</p>
",Dataset Preprocessing & Handling,number match keywords specified category large scale text analysis problem data frame containing word fall different category data frame containing column string empty counting column category want take individual string check defined word appear count within appropriate category simplified example given two data frame want count many animal type appear text cell desired result would straightforward way achieve keyword matching counting would applicable much larger dataset thanks advance
Efficient way to creating a Term Frequency Matrix from a Pandas Dataframe,"<p>Given a pandas data frame with 2 columns - column 1 is the user name, and column 2 is the content linked to the user.</p>
<p><a href=""https://i.sstatic.net/JNWtv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JNWtv.png"" alt=""enter image description here"" /></a></p>
<p>How does one create a Term Frequency Matrix that looks like the following?</p>
<p><a href=""https://i.sstatic.net/7ruXO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7ruXO.png"" alt=""enter image description here"" /></a></p>
<p>My attempt:
<a href=""https://i.sstatic.net/KXTwk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KXTwk.png"" alt=""enter image description here"" /></a></p>
<p>So it seems like this is working, but I want it to show column and row names in the final matrix form.</p>
",Dataset Preprocessing & Handling,efficient way creating term frequency matrix panda dataframe given panda data frame column column user name column content linked user doe one create term frequency matrix look like following attempt seems like working want show column row name final matrix form
Simpletransformers always generating empty strings,"<p>so i was trying to train a chatbot using transformers for my ai assistant , i thought simpletransformer package in python would help me speed up alot of my tasks . I soon gathered a good dataset over kaggle (<a href=""https://www.kaggle.com/datasets/arnavsharmaas/chatbot-dataset-topical-chat"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/arnavsharmaas/chatbot-dataset-topical-chat</a>) to train my chatbot , i loaded up the data did some preprocessing and transformed it into one column input_text another target_text as mentioned in the <a href=""https://simpletransformers.ai/docs/seq2seq-model/"" rel=""nofollow noreferrer"">docs</a>. Then i trained my model with encoder type as roberta and decoder type as bert as thats what selected by default and it cannot be changed i saw it in the docs . I trained it on the first 1k samples and see if the code is working at first try i gave it one line from the dataset and it just spammed the word my the result was <code>#mymymymymy</code> i restarted my runtime and trained again this time it always generated an empty string , i was expecting proper results . Here are the code snippets:-</p>
<p>Loading and preprocessing data :-</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
df=pd.read_csv(&quot;../input/chatbot-dataset-topical-chat/topical_chat.csv&quot;)
#converting to required format
new_df={&quot;input_text&quot;:[],'target_text':[]}
for i in range(0,df.shape[0]):
    if i%2==0:
        new_df['input_text'].append(df['message'][i])
    else:
        new_df['target_text'].append(df['message'][i])
new_df=pd.DataFrame(new_df)
new_df.head()
</code></pre>
<p>works uptil here
and the code for training the transformer is</p>
<pre class=""lang-py prettyprint-override""><code>!pip install simpletransformers
from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs


model_args = Seq2SeqArgs()
model_args.num_train_epochs = 3
model_args.overwrite_output_dir = True
model = Seq2SeqModel(
    &quot;roberta&quot;,
    &quot;roberta-base&quot;,
    &quot;bert-base-cased&quot;,
    args=model_args,
)
model.train_model(new_df.head(1000))
</code></pre>
<p>here are the results
<a href=""https://i.sstatic.net/EQT12.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EQT12.png"" alt=""enter image description here"" /></a></p>
<p>finally i asked it to predict a sample from the dataframe it once spammed a word like i said after restart it produces empty string
Can anyone help me please?</p>
",Dataset Preprocessing & Handling,simpletransformers always generating empty string wa trying train chatbot using transformer ai assistant thought simpletransformer package python would help speed alot task soon gathered good dataset kaggle train chatbot loaded data preprocessing transformed one column input text another target text mentioned doc trained model encoder type roberta decoder type bert thats selected default changed saw doc trained first k sample see code working first try gave one line dataset spammed word result wa restarted runtime trained time always generated empty string wa expecting proper result code snippet loading preprocessing data work uptil code training transformer result finally asked predict sample dataframe spammed word like said restart produce empty string anyone help please
Obtaining the index of a word between two columns in pandas,"<p>I am checking on which words the SpaCy Spanish lemmatizer works on using the .has_vector method. In the two columns of the datafame I have the output of the function that indicates which words can be lemmatized and in the other one the corresponding phrase.</p>
<p>I would like to know how I can extract all the words that have False output to correct them so that I can lemmatize.</p>
<p>So I created the function:</p>
<pre><code>def lemmatizer(text):
doc = nlp(text)
return ' '.join([str(word.has_vector) for word in doc])
</code></pre>
<p>And applied it to the column sentences in the DataFrame</p>
<pre><code>df[&quot;Vectors&quot;] = df.reviews.apply(lemmatizer)
</code></pre>
<p>And put in another data frame as:</p>
<pre><code>df2= pd.DataFrame(df[['Vectors', 'reviews']])
</code></pre>
<p>The output is</p>
<pre><code>index             Vectors              reviews
  1     True True True False        'La pelicula es aburridora'
</code></pre>
",Dataset Preprocessing & Handling,obtaining index word two column panda checking word spacy spanish lemmatizer work using ha vector method two column datafame output function indicates word lemmatized one corresponding phrase would like know extract word false output correct lemmatize created function applied column sentence dataframe put another data frame output
How to make text processing operations on big corpus work (more efficiently),"<p>How to get my Python script to work (efficiently) on a 33-million large set of documents?</p>
<p>Creating a Topic Model from <a href=""https://pubmed.ncbi.nlm.nih.gov/download/"" rel=""nofollow noreferrer"">33-million PubMed abstracts</a>. At this point I have a binary pickle file that contains a Python list with the 33m abstracts (pubmed_abstracts.bin) as strings. I am trying to unpickle this list, tokenize the documents and then pickle the result, so I can use that in my next steps.</p>
<p>What I'd like to achieve is that I could tokenize these 33m documents with a 100% CPU-load (hence the chunking and asynchronous processing), while using less than 100% RAM and that it finishes without errors (these last two I cannot figure out yet). Feel free to download the <a href=""https://phitech.dev/pubmed_abstracts/pubmed_abstracts_test.zip"" rel=""nofollow noreferrer"">pubmed abstracts pickled 6k and 1m test sets</a>. Full 33m corpus available upon request.</p>
<p>This script below works fine on smaller dataset (n=6k/6MB, or n=1m/600MB). But it doesn't work on the full n=33m/19GB dataset. In one case the script stopped because the memory ran out. In another case I'm getting &quot;Process SpawnProcess-7:Process SpawnProcess-8:Process SpawnProcess-9:etc..&quot;. I'm working with i5/32GB, Win11, Anaconda, Python v3, Spyder v5.</p>
<pre><code>## TOKENIZED CORPUS TO BINARY
## Spyder 5.1.5
## Python 3.9.7 64-bit


# %%
# Essentials
import pickle
import time
import concurrent.futures # Async script execution
import math

# Gensim
import gensim
from gensim.utils import simple_preprocess


# %%
## Set environment
if __name__ == '__main__':
    is_test = False # Set to false to process full datasets (can take long)
    if is_test:
        file_location = 'C:/.../datasets_test/'; # Small dataset (n=5000) for demo
    else:
        file_location = 'C:/.../datasets_full/'; # Big dataset (n=33,000,000)
    
    
    # %%
    ## Retrieve binary file
    documents = pickle.load(open(file_location + &quot;pubmed_abstracts.bin&quot;, 'rb'))
    #print(documents[55]) # Debugging
    
    
    # %%
    ## Separate full corpus into chunks
    big_chunks = []
    documents_per_big_chunk = 1000
    documents_per_small_chunk = 100
    number_of_big_chunks = math.ceil(len(documents) / documents_per_big_chunk) # Define enough chunks to contain entire corpus
    number_of_small_chunks = math.ceil(documents_per_big_chunk / documents_per_small_chunk) # Equally divide big chunk up into small chunks
    range_from = 0
    range_to = documents_per_small_chunk
    for _ in range(number_of_big_chunks):
        big_chunk = []
        for _ in range(number_of_small_chunks):
            if len(documents[range_from:range_to]) &gt; 0:
                big_chunk.append(documents[range_from:range_to+1]) # If it isn't range_to+1, every 100th document is omitted. So range_to+1.
            print(f&quot;range_from = {range_from}, range_to = {range_to}&quot;)
            range_from = range_to + 1
            range_to = range_to + documents_per_small_chunk
        big_chunks.append(big_chunk)
    del documents # Free up some much needed memory
    #print(len(big_chunks[0][0])) # Debugging
    #print(big_chunks[0][0][55]) # Debugging
    
    
    # %%
    ## 
    def tokenize_documents(documents):
        tokenized_documents = [gensim.utils.simple_preprocess(document, deacc=True) for document in documents]
        return tokenized_documents
    
    
    # %%
    ## Asynchronously tokenize documents
    ## https://www.youtube.com/watch?v=fKl2JW_qrso
    ## https://www.youtube.com/watch?v=8OKTAedgFYg
    tokenized_documents = []
    time_start = time.perf_counter()
    for i in range(len(big_chunks)):
        with concurrent.futures.ProcessPoolExecutor() as executor:
            results = executor.map(tokenize_documents, big_chunks[i])
            for result in results:
                [tokenized_documents.append(document) for document in result]
    time_spent = str(round(time.perf_counter() - time_start, 2))
    print (f&quot;time_spent = {time_spent}&quot;)
    
    # %%
    ## Pickle the resulting tokenized corpus
    pickle.dump(tokenized_documents, open(file_location + &quot;pubmed_abstracts_tokenized.bin&quot;, 'wb'))
</code></pre>
<p>What I am looking for by posting this on SO is that someone may help me point at my mistakes and help me make a few steps in the right direction. I have made steps by myself like adding <code>if _name_ == '_main_':</code> and chunking my data and processing it asynchronously. I just figure that you, experienced folks on SO may help me save precious masters thesis time by taking a look at my code. What mistakes can you identify and what changes would you suggest?</p>
",Dataset Preprocessing & Handling,make text processing operation big corpus work efficiently get python script work efficiently million large set document creating topic model million pubmed abstract point binary pickle file contains python list abstract pubmed abstract bin string trying unpickle list tokenize document pickle result use next step like achieve could tokenize document cpu load hence chunking asynchronous processing using le ram finish without error last two figure yet feel free download pubmed abstract pickled k test set full corpus available upon request script work fine smaller dataset n k mb n mb work full n gb dataset one case script stopped memory ran another case getting process spawnprocess process spawnprocess process spawnprocess etc working gb win anaconda python v spyder v looking posting someone may help point mistake help make step right direction made step like adding chunking data processing asynchronously figure experienced folk may help save precious master thesis time taking look code mistake identify change would suggest
How to go through each row with pandas apply() and lambda to clean sentence tokens?,"<p>My goal is to created a cleaned column of the tokenized sentence within the existing dataframe.
The dataset is a pandas dataframe looking like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Index</th>
<th>Tokenized_sents</th>
</tr>
</thead>
<tbody>
<tr>
<td>First</td>
<td>[Donald, Trump, just, couldn, t, wish, all, Am]</td>
</tr>
<tr>
<td>Second</td>
<td>[On, Friday, ,, it, was, revealed, that]</td>
</tr>
</tbody>
</table>
</div>
<pre><code>dataset['cleaned_sents'] = dataset.apply(lambda row: [w for w in row[&quot;tokenized_sents&quot;] if len(w)&gt;2 and w.lower() not in stop_words], axis = 1)
</code></pre>
<p>My current output is the dataframe without that extra column.</p>
<p>Current outout:</p>
<pre><code>    tokenized_sents  \
0  [Donald, Trump, just, couldn, t, wish, all, Am...  
</code></pre>
<p>Wanted output:</p>
<pre><code>  tokenized_sents  \
0  [Donald, Trump, just, couldn, wish, all...   
</code></pre>
<p>Basically removing all the stopwords &amp; short words</p>
",Dataset Preprocessing & Handling,go row panda apply lambda clean sentence token goal created cleaned column tokenized sentence within existing dataframe dataset panda dataframe looking like index tokenized sent first donald trump wish second friday wa revealed current output dataframe without extra column current outout wanted output basically removing stopwords short word
How to implement text metadata to spacy output?,"<p>I am trying to build a corpus and parse it with spacy. Corpus consists of over 2000 individual text files each has specific document meta such as filename, gender, nationally, etc.stored in each row in the data frame.</p>
<p>what I did so far is to create a EXCEL file consisting of metadata and text_field. text_field is where the actual texts are stored. Then I imported it as pandas, parsed text_field with Spacy via following codes;</p>
<pre><code>import spacy
import pandas as pd
nlp = spacy.load('en_core_web_sm')
df = pd.read_excel('C:/Users/Desktop/Data.xlsx')
df['docs'] = list(nlp.pipe(df.text_field))
</code></pre>
<p>However, I would like to iterate over all docs stored in data frame and extract outputs with metadata provided in data frame as well.</p>
<p><strong>For instance, common Spacy output is like this;</strong></p>
<pre><code>doc = nlp('this is a test sentence')
for token in doc:
print(token.lemma_, token.text, token.pos_)

lemma / text / pos_
this   this   DET
be     is     AUX
a      a      DET
test   test   NOUN
sentence   sentence   NOUN
</code></pre>
<p><strong>Expected out is like this</strong>;</p>
<pre><code> file(text/doc metadata) /  lemma / text / pos_
    text1                   this    this   DET
    text1                   be      is     AUX
    text1                   a       a      DET
    text1                   test    test   NOUN
    text1                   sentence sentence NOUN
    text2                   this     this     DET
    text2                   be       is       AUX
    text2                   another  another  DET
    text2                   sentence sentence NOUN
</code></pre>
<ol>
<li>When I apply the df['docs'] = list(nlp.pipe(df.text_field)), docs column only consists of text, not doc objects.</li>
<li>How should I proceed to get the expected output? This is possible in R with Quanteda package, creating corpus and tokenizing etc, but is there a way to do the same on Python with Spacy?</li>
</ol>
",Dataset Preprocessing & Handling,implement text metadata spacy output trying build corpus parse spacy corpus consists individual text file ha specific document meta filename gender nationally etc stored row data frame far create excel file consisting metadata text field text field actual text stored imported panda parsed text field spacy via following code however would like iterate doc stored data frame extract output metadata provided data frame well instance common spacy output like expected like apply df doc list nlp pipe df text field doc column consists text doc object proceed get expected output possible r quanteda package creating corpus tokenizing etc way python spacy
How to get a list of two words before or after a keyword in python?,"<p>I collected this data and I am trying to identify if the keyword exact what are the two word before it and after it</p>
<p>data = pd.read_csv( 'jobs.csv')</p>
<p>view(data)</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Job</th>
<th>Discerption</th>
</tr>
</thead>
<tbody>
<tr>
<td>Engineer</td>
<td>the job requires x,y,z.....</td>
</tr>
<tr>
<td>Driver</td>
<td>this job need a high-school and Communication skills</td>
</tr>
</tbody>
</table>
</div>
<p>The data length is about 10k</p>
<p>For example the keyword &quot;Communication&quot;
Can i find the words before and after Communication and make the results look like this</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Job</th>
<th>Discerption</th>
<th>after</th>
<th>before</th>
</tr>
</thead>
<tbody>
<tr>
<td>Engineer</td>
<td>the job requires x,y,z</td>
<td>NA</td>
<td>NA</td>
</tr>
<tr>
<td>Driver</td>
<td>this job need a high-school and Communication skills</td>
<td>skills</td>
<td>high-school,  and</td>
</tr>
</tbody>
</table>
</div>
<p>Na because the keyword doesn't exist</p>
<p>I tired pandas and regex but nothing is working for me :/</p>
<p>I would really appreciate the help</p>
",Dataset Preprocessing & Handling,get list two word keyword python collected data trying identify keyword exact two word data pd read csv job csv view data job discerption engineer job requires x z driver job need high school communication skill data length k example keyword communication find word communication make result look like job discerption engineer job requires x z na na driver job need high school communication skill skill high school na keyword exist tired panda regex nothing working would really appreciate help
Get zero tf_idf from dfm with quanteda r,"<p>I want to create a Document-feature matrix with tf_idf as weights. If I calculate the tf_idf like in <a href=""https://quanteda.io/reference/dfm_tfidf.html"" rel=""nofollow noreferrer"">https://quanteda.io/reference/dfm_tfidf.html</a>, I get only zeros. The same if I try to get tf_idf with tidytext from the same token dataset. Looks to me like the information about the number of documents in the corpus cannot be calculated. If i usetidy text from scratch, it works.</p>
<pre><code>harry_token &lt;-harry_data %&gt;% corpus() %&gt;%  
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %&gt;% 
  tokens_remove( c(stopwords(&quot;english&quot;))) %&gt;%
  tokens_ngrams( n = 1) 


harry_token &lt;-   tokens_replace(tokens(harry_token), pattern = lemma_en$token, replacement = lemma_en$lemma)



harry_token[1]
[1] &quot;Boy&quot;       &quot;live&quot;      &quot;Mr&quot;        &quot;Mrs&quot;       &quot;Dursley&quot;   &quot;number&quot;    &quot;four&quot;      &quot;Privet&quot;    &quot;drive&quot;    
[10] &quot;proud&quot;     &quot;say&quot;       &quot;perfectly&quot;
[ ... and 40,770 more ]



harry_token  %&gt;%
    +   dfm(verbose = F)

Document-feature matrix of: 7 documents, 13,528 features (52.85% sparse) and 1 docvar.
       features
docs    boy live  mr mrs dursley number four privet drive proud
  text1  99   25  81  46     104     19   32     16    31     7
  text2  60   23 135  96      39      8   25      7    15     2
  text3  61   23  60  24      29     18   14      9    21     6
  text4 121   34 521 155      41     35   54     16    26     6
  text5 102   41 240 276      43     69   65     24    42    11
  text6 105   38 102 154      23     34   34      7    15    10
[ reached max_ndoc ... 1 more document, reached max_nfeat ... 13,518 more features ]

harry_token %&gt;%
+   dfm(verbose = F)  %&gt;% 
+   dfm_tfidf()
Document-feature matrix of: 7 documents, 13,528 features (52.85% sparse) and 1 docvar.
       features
docs    boy live mr mrs dursley number four privet drive proud
  text1   0    0  0   0       0      0    0      0     0     0
  text2   0    0  0   0       0      0    0      0     0     0
  text3   0    0  0   0       0      0    0      0     0     0
  text4   0    0  0   0       0      0    0      0     0     0
  text5   0    0  0   0       0      0    0      0     0     0
  text6   0    0  0   0       0      0    0      0     0     0
[ reached max_ndoc ... 1 more document, reached max_nfeat ... 13,518 more features ]




harry_dfm &lt;- harry_token  %&gt;%
              dfm(verbose = FALSE)

tidy(harry_dfm) %&gt;% 
    bind_tf_idf(term = term, document = document, n = count)

# A tibble: 44,646 x 6
   document term    count       tf   idf tf_idf
   &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
 1 text1    boy        99 0.00243      0      0
 2 text1    live       25 0.000613     0      0
 3 text1    mr         81 0.00199      0      0
 4 text1    mrs        46 0.00113      0      0
 5 text1    dursley   104 0.00255      0      0
 6 text1    number     19 0.000466     0      0
 7 text1    four       32 0.000785     0      0
 8 text1    privet     16 0.000392     0      0
 9 text1    drive      31 0.000760     0      0
10 text1    proud       7 0.000172     0      0
# ... with 44,636 more rows
</code></pre>
<p>If I calculate tf_idf from scratch with tidytext ist works.</p>
<pre><code> harry_data %&gt;% unnest_tokens(word, text) %&gt;% group_by(title) %&gt;%
+     count(word) %&gt;%
+     bind_tf_idf(word, title, n) 
# A tibble: 67,881 x 6
# Groups:   title [7]
   title              word      n        tf   idf     tf_idf
   &lt;chr&gt;              &lt;chr&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;
 1 Chamber of Secrets 0         1 0.0000117 0.847 0.00000992
 2 Chamber of Secrets 1         6 0.0000703 0.154 0.0000108 
 3 Chamber of Secrets 1,5       1 0.0000117 1.95  0.0000228 
 4 Chamber of Secrets 1,520     1 0.0000117 1.95  0.0000228 
 5 Chamber of Secrets 100       1 0.0000117 1.95  0.0000228 
 6 Chamber of Secrets 101       1 0.0000117 1.95  0.0000228 
 7 Chamber of Secrets 102       1 0.0000117 1.95  0.0000228 
 8 Chamber of Secrets 104       1 0.0000117 1.95  0.0000228 
 9 Chamber of Secrets 105       1 0.0000117 1.95  0.0000228 
10 Chamber of Secrets 106       1 0.0000117 1.95  0.0000228 
</code></pre>
",Dataset Preprocessing & Handling,get zero tf idf dfm quanteda r want create document feature matrix tf idf weight calculate tf idf like get zero try get tf idf tidytext token dataset look like information number document corpus calculated usetidy text scratch work calculate tf idf scratch tidytext ist work
How to use metadata for document retrieval using Sentence Transformers?,"<p>I'm trying to use Sentence Transformers and Haystack for document retrieval, focusing on searching documents on other metadata beside document text.</p>
<p>I'm using a dataset of academic publication titles, and I've appended a fake publication year (which I want to use as a search term). From reading around I've combined the columns and just added a separator between the title and publication year, and included the column titles since I thought maybe this could add context. An example input looks like:</p>
<p><code>title Sparsity-certifying Graph Decompositions [SEP] published year 1980</code></p>
<p>I have a document store and method of retrieving here, based on <a href=""https://github.com/yashprakash13/haystack-search-engine"" rel=""nofollow noreferrer"">this</a>:</p>
<pre><code>document_store_faiss = FAISSDocumentStore(faiss_index_factory_str=&quot;Flat&quot;,
                                          return_embedding=True,
                                          similarity='cosine')

retriever_faiss = EmbeddingRetriever(document_store_faiss,
                                     embedding_model='all-mpnet-base-v2',
                                     model_format='sentence_transformers')

document_store_faiss.write_documents(df.rename(columns={'combined':'content'}).to_dict(orient='records'))
document_store_faiss.update_embeddings(retriever=retriever_faiss)

def get_results(query, retriever, n_docs = 25):
  return [(item.content) for item in retriever.retrieve(q, top_k = n_docs)]

q = 'published year 1999'
print('Results: ')
res = get_results(q, retriever_faiss) 
for r in res:
  print(r) 
</code></pre>
<p>I do a check to see if any inputs actually have a publication year matching the search term, but when I look at my search results I'm getting entries with seemingly random published years. I was hoping that at least the results would all be the same published year, since I hoped to do more complicated queries like <em>&quot;published year before 1980&quot;</em>.</p>
<p>If anyone could either tell me what I'm doing wrong, or whether I have misunderstood this process / expected results it would be much appreciated.</p>
",Dataset Preprocessing & Handling,use metadata document retrieval using sentence transformer trying use sentence transformer haystack document retrieval focusing searching document metadata beside document text using dataset academic publication title appended fake publication year want use search term reading around combined column added separator title publication year included column title since thought maybe could add context example input look like document store method retrieving based check see input actually publication year matching search term look search result getting entry seemingly random published year wa hoping least result would published year since hoped complicated query like published year anyone could either tell wrong whether misunderstood process expected result would much appreciated
How to Count the occurances of all entities in a python spacy document?,"<p>I am using SciSpasy to identify all entities in a text document. From this, I would like to return a two column data frame. With the left column being a list of unique entities found in the text document and the right side being the number of times the entity appears in the document. How can I go about this using spacy?</p>
",Dataset Preprocessing & Handling,count occurances entity python spacy document using scispasy identify entity text document would like return two column data frame left column list unique entity found text document right side number time entity appears document go using spacy
Distance of Noun from Verb,"<p>Is there a way to get the distance of a Noun from the Verb from multiple sentences in a csv file using NLTK and Python?</p>
<p>Example of sentences in a .csv file:</p>
<pre><code>video shows adam stabbing the bystander.
woman quickly ran from the police after the incident.
</code></pre>
<p>Output:</p>
<p>1st sentence: <code>1 (Verb is right after the noun)</code></p>
<p>2nd sentence:
<code>2 (Verb is after another POS tag)</code></p>
",Dataset Preprocessing & Handling,distance noun verb way get distance noun verb multiple sentence csv file using nltk python example sentence csv file output st sentence nd sentence
Combine Two Data Set CSV Files in Python,"<p>I have 2 datasets and each has 2 columns namely code, description. I want to read both the files search for similar text and map the codes of dataset 1 and dataset 2 together. For example.</p>
<p>file1.csv</p>
<pre><code>code, description
111, Milk producer
112, IT specialist
</code></pre>
<p>file2.csv</p>
<pre><code>code, description
001, Milkman
002, Driver
</code></pre>
<p>Now the combined dataset would be,
file_combined.csv</p>
<pre><code>code1, description1, code2, description2
111,milk producer,001,milk man
112,IT specialist,002,drvier
</code></pre>
",Dataset Preprocessing & Handling,combine two data set csv file python datasets ha column namely code description want read file search similar text map code dataset dataset together example file csv file csv combined dataset would file combined csv
How to save data into multiple CSV files based on column specific values,"<p>I want to save data, based on column values. Below is the data set that I'm working on</p>
<blockquote>
<p>created_at,tweet,category<br>
7/25/2021,Great Sunny day for Cricket at London Great Score put on by England batting Olympic is to held in Japan,sports<br>
7/25/2021,President Made a clear statement An election is to be kept next year,politics<br>
7/25/2021,A terrorist attack have killed 10 people,crime<br>
7/26/2021,Srilanka have lost the T20 series Australia have won the series,sports<br>
7/26/2021,Minister have given up his role last monday President is challenging the opposite leader,politics<br>
7/27/2021,Rainy day for Cricket at London poor Score put on by zimabwe batting next Olympic is to held in Srilanka,sports<br>
7/27/2021,President Made a poor statement No any election is to be kept next year,politics<br>
7/27/2021,100 of people are being killed due to terror attack,crime<br>
7/28/2021,IPL will be happening next year Velentino Rossy is to lead the MotoGP,sports<br>
7/28/2021,Minister have opt to take strict decisions The election nominations are not given to Mr XYS, politics<br></p>
</blockquote>
<p>So as per the data above what I want to save data into .csv file based on the category. Which means I want to store all sports related data(category name sports) in <strong>sports.csv</strong> file as below wrt above dataset</p>
<blockquote>
<p>created_at,tweet,category<br>
7/25/2021,Great Sunny day for Cricket at London Great Score put on by England batting Olympic is to held in Japan,sports<br>
7/26/2021,Srilanka have lost the T20 series Australia have won the series,sports<br>
7/27/2021,Rainy day for Cricket at London poor Score put on by zimabwe batting next Olympic is to held in Srilanka,sports<br>
7/28/2021,IPL will be happening next year Velentino Rossy is to lead the MotoGP,sports<br></p>
</blockquote>
<p>In the similar way I want to store politics related data in <strong>politics.csv</strong> file that it include following data wrt above data</p>
<blockquote>
<p>created_at,tweet,category<br>
7/25/2021,President Made a clear statement An election is to be kept next year,politics<br>
7/26/2021,Minister have given up his role last monday President is challenging the opposite leader,politics<br>
7/27/2021,President Made a poor statement No any election is to be kept next year,politics<br>
7/28/2021,Minister have opt to take strict decisions The election nominations are not given to Mr XYS, politics</p>
</blockquote>
<p>And in the same way for other fields as well. It would be very helpful if someone can help with this</p>
",Dataset Preprocessing & Handling,save data multiple csv file based column specific value want save data based column value data set working created tweet category great sunny day cricket london great score put england batting olympic held japan sport president made clear statement election kept next year politics terrorist attack killed people crime srilanka lost series australia series sport minister given role last monday president challenging opposite leader politics rainy day cricket london poor score put zimabwe batting next olympic held srilanka sport president made poor statement election kept next year politics people killed due terror attack crime ipl happening next year velentino rossy lead motogp sport minister opt take strict decision election nomination given mr xy politics per data want save data csv file based category mean want store sport related data category name sport sport csv file wrt dataset created tweet category great sunny day cricket london great score put england batting olympic held japan sport srilanka lost series australia series sport rainy day cricket london poor score put zimabwe batting next olympic held srilanka sport ipl happening next year velentino rossy lead motogp sport similar way want store politics related data politics csv file include following data wrt data created tweet category president made clear statement election kept next year politics minister given role last monday president challenging opposite leader politics president made poor statement election kept next year politics minister opt take strict decision election nomination given mr xy politics way field well would helpful someone help
Unable to Load FastText model,"<p>I am trying to load the FastText and save that as a model so that I can deploy that on production as the file size is 1.2 gb and wont be a good practice to use that on Prod.
Can anyone suggest an approach to save and load the model for production (&quot;fasttext-wiki-news-subwords-300&quot;)
<a href=""https://i.sstatic.net/PgACr.png"" rel=""nofollow noreferrer"">Loading the file using gensim.downloader api</a></p>
",Dataset Preprocessing & Handling,unable load fasttext model trying load fasttext save model deploy production file size gb wont good practice use prod anyone suggest approach save load model production fasttext wiki news subwords loading file using gensim downloader api
Loading pre-trained CBOW/skip-gram embeddings from a file that has unknown encoding?,"<p>I'm trying to load pre-trained word embeddings for the Arabic language (Mazajak embeddings: <a href=""http://mazajak.inf.ed.ac.uk:8000/"" rel=""nofollow noreferrer"">http://mazajak.inf.ed.ac.uk:8000/</a>). The embeddings file does not have a particular extension and I'm struggling to get it to load. What's the usual process to load these embeddings?</p>
<p>I've tried doing <code>with open(&quot;get_sg250&quot;, encoding = encoding) as file: file.readlines() </code> for different encodings but it seems like none of them are the answer (utf-8 does not work at all), if I try windows-1256 I get gibberish:</p>
<p>e.g.</p>
<pre><code>['8917028 300\n',
 '&lt;/s&gt; Hل®:0\x16ء:؟X§؛R8ڈ؛\xa0سî9K\u200fƒ::m¤9¼»“8¤p\u200c؛tعA:UU¾؛“_ع9‚Nƒ¹®G§¹قفگ؛ww$؛\u200eba:\x14.„:R¸پ:0–\x0b:–ü\x06:×#¦؛Yٍ²؛m ظ:{\x14¦:µ\x01‡:ه\x17S¹Yr¯:j\x03-¹ff€9×£P¸\n',
 'W‚؛UUه9¼»é¹&quot;&quot;§؛\u200c¶د:UU؟:\u200eb؟¹{\x14\u200d¸,ù19ïî\u200d؛ئ\x12¯؛\x00\x00ا:\u200c6°7A§a؛ذé„؛ذi†؛®G\x14:حجŒ8\x03\u200cè9ه\x17¸؛ق]¦؛ڈآ5¸قفا9حج^:\x00€ٹ؛q=²:\x00\x00¢9\x14®أ9×£T¹لz‚:\x1bèG؛®G7؛ڑ™&lt;:m\xa0ƒ¹&quot;&quot;´9\x14®\x1d:&quot;¢²؛®G-؛ڑ™~:±ن¸:\x18ث«:¸\x1e…؛`,8؛Hل\u200d¹±ن.:\x1f…¥؛لْ‚:ڑ™s:R¸\x0b؛ئ’\x07؛0–C؛ڈآ¸:ذéھ:ة/خ¹A\'¸:ڑ™ز:m\xa0\x1e:è´ظ::ي‡؛\n',
 '×\x05؛Œ%8؛ش\x06~؛أُu:\x00\x00\n',
 &quot;:‰ˆ\x149\x14®?؛ِ(\x05:«ھ…:)\\‡833G:Haط؛\x1f…¼:¼»'9\x00\x00 ؛=\n&quot;,
 '6؛R¸‚¹¼;€؛\x1bè¾؛\x1bèw؛قف؛:A§\x1a؛&quot;&quot;j؛K~J:Hل\x14؛ىرد:\u200c6\x0c؛–|ب؛‚Nm:cةد·:mک؛‰ˆھ9\x00\x00ü9DD(¹ذi\x1f:ذé¬؛,ù™9¼»\x1e:wwƒ؛\x03\u200cF87ذ©·×£Q؛\x1f…w؛ئ\x12ح؛\x00\x00\x007ٍ‹U8\x0etZ6“ك«؛cةط؛Haد؛–ü¼؛33?¹Œ%َ9أُخ9=\n',
 '‹؛ق]ع:ڈآ/؛0–ق¹¤pُ¹Dؤخ:¤p¤؛\x1bèت9\u200ebé¹ùE‹:–üb7=ٹ؛:؟Xv؛×£c؛ِ(·؛è4\xa0؛cة‹؛0\x16ˆ؛ئ’U:&quot;&quot;#؛ة/j:R8،:أُى9ذé€:ىQX:\x1f…L:&quot;&quot;›؛K\u200f•؛ڈآں؛‰ˆ8¸ww´:&quot;&quot;o؛è´…؛\n',
 'W·؛¤pگ:{”¶؛\x0etJ¹\u200eb&gt;:ùإة؛`¬أ؛ِ(ü9K\u200f™:‚N؛:لz;:ِ(ٹ:Œ¥ˆ؛§\n',
 'ں؛ِ¨\xad:ڑ™q؛\u200c6\x19:×£H9¤p\x1c:\x03\u200cخ¹–üٹ8UU\x13؛Hلؤ¹è´ء؛ïnژ؛®Gک:è´¯9\x0etN؛O\x1b\x0b؛\x00\x00Z:\n',
 'Wڑ؛&quot;&quot;J؛؟طخ:\x03\u200c¹:لْ¬؛\u200c6ک9ڑ™D؛\x1bèT8ق]ƒ:¼»س:0–-:~±³:,y‰:è´،¸jƒأ:m\xa0]:A\'د:j\x03\x15؛Haد:&quot;&quot;½:wwù¹ه\x17ء؛×#س:&amp;؟œ9×£5؛Hلz¹\\ڈ€¹)\\¨؛O\x1bْ¹ه\x17\x1b¹ڈB×؛\x03\u200c™؛ىQز¹لz¤¹ذi\x1c:\\ڈژ9ùإV¹R¸€:ùإü9ww?9‰\x08\u200d:~±ؤ¹‚Nù¹‰ˆ\x10¹UUn؛\x11\x11ƒ؛ٍ‹چ8‰ˆ½:\x1bèî¹O\x1bè¶`¬´؛=\n',
 '¢:\n',
</code></pre>
<p>I've also tried using pickle but that also doesn't work.</p>
<p>Any suggestions on what I could try out?</p>
",Dataset Preprocessing & Handling,loading pre trained cbow skip gram embeddings file ha unknown encoding trying load pre trained word embeddings arabic language mazajak embeddings embeddings file doe particular extension struggling get load usual process load embeddings tried different encoding seems like none answer utf doe work try window get gibberish e g also tried using pickle also work suggestion could try
Count the number of occurrences of each word in a file and load into pandas,"<p>How do I count the number of occurrences of each word in a .txt file and also load it into the pandas dataframe with columns name and count, also sort the dataframe on column count?</p>
",Dataset Preprocessing & Handling,count number occurrence word file load panda count number occurrence word txt file also load panda dataframe column name count also sort dataframe column count
How to label multi-word entities?,"<p>I'm quite new to data analysis (and Python in general), and I'm currently a bit stuck in my project.</p>
<p>For my NLP-task I need to create training data, i.e. find specific entities in sentences and label them. I have multiple csv files containing the entities I am trying to find, many of them consisting of multiple words. I have tokenized and lemmatized the unlabeled sentences with spaCy and loaded them into a <code>pandas.DataFrame</code>.</p>
<p>My main problem is: how do I now compare the tokenized sentences with the entity-lists and label the (often multi-word) entities? Having around 0.5 GB of sentences, I don't think it is feasible to just for-loop every sentence and then for-loop every entity in every class-list and do a simple substring-search. Is there any smart way to use pandas.Series or DataFrame to do this labeling?</p>
<p>As mentioned, I don't really have any experience regarding pandas/numpy etc. and after a lot of web searching I still haven't seemed to find the answer to my problem</p>
<p>Say that this is a sample of finance.csv, one of my entity lists:</p>
<pre><code>&quot;Frontwave Credit Union&quot;,
&quot;St. Mary's Bank&quot;,
&quot;Center for Financial Services Innovation&quot;,
...
</code></pre>
<p>And that this is a sample of sport.csv, another one of my entity lists:</p>
<pre><code>&quot;Christiano Ronaldo&quot;,
&quot;Lewis Hamilton&quot;,
...
</code></pre>
<p>And an example (dumb) sentence:</p>
<pre><code>&quot;Dear members of Frontwave Credit Union, any credit demanded by Lewis Hamilton is invalid, said Ronaldo&quot;
</code></pre>
<p>The result I'd like would be something like a table of tokens with the matching entity labels (with IOB labeling):</p>
<pre><code>&quot;Dear &quot;- O
&quot;members&quot; - O
&quot;of&quot; - O
&quot;Frontwave&quot; - B-FINANCE
&quot;Credit&quot; - I-FINANCE
&quot;Union&quot; - I-FINANCE
&quot;,&quot; - O
&quot;any&quot; - O
...
&quot;Lewis&quot; - B-SPORT
&quot;Hamilton&quot; - I-SPORT
...
&quot;said&quot; - O
&quot;Ronaldo&quot; - O
</code></pre>
",Dataset Preprocessing & Handling,label multi word entity quite new data analysis python general currently bit stuck project nlp task need create training data e find specific entity sentence label multiple csv file containing entity trying find many consisting multiple word tokenized lemmatized unlabeled sentence spacy loaded main problem compare tokenized sentence entity list label often multi word entity around gb sentence think feasible loop every sentence loop every entity every class list simple substring search smart way use panda series dataframe labeling mentioned really experience regarding panda numpy etc lot web searching still seemed find answer problem say sample finance csv one entity list sample sport csv another one entity list example dumb sentence result like would something like table token matching entity label iob labeling
How to load and preprocess a dataset by chunks?,"<p>I have a large data frame to which I would like to apply a set of functions to one of its columns using <code>pipeline</code> and <code>progress_apply()</code>.</p>
<p>Here is my code snippet.</p>
<pre class=""lang-py prettyprint-override""><code>df = # a dataFrame object with multiple columns where df.columns[-1] == 'text' 
from tqdm.auto import tqdm
tqdm.pandas()

pipeline = # list of pre-defined methods
prepare(text, pipeline):
   &quot;&quot;&quot;
   a method that cleanup and remove stop words from text input
   &quot;&quot;&quot;
   return # list of clean tokens

# MemoryError! when reaching 50% of cleaning progress
df = df['text'].progress_apply(prepare, pipeline=pipeline)  
</code></pre>
<p>I am trying to solve the issue of <code>MemoryError</code> using <code>progress_apply()</code> but loading data by <strong>chunks</strong>. I have no idea of how I can do this with <code>progress_apply()</code>.
I tried the following:</p>
<pre class=""lang-py prettyprint-override""><code>for i in range(0, df.shape[0], 47):
   df = df['text'][i:i+47].progress_apply(prepare, pipeline=pipeline)  
</code></pre>
<p>What I have tried doesn't same the previous ranges.</p>
",Dataset Preprocessing & Handling,load preprocess dataset chunk large data frame would like apply set function one column using code snippet trying solve issue using loading data chunk idea tried following tried previous range
How to clean non Arabic letters from a text file in python?,"<p>UPDATE-
Very new to python,
How to clean the text from everything but Arabic letters. I used regex function but without success.</p>
<p>This is my code</p>
<pre><code># load text
filename = '/content/drive/MyDrive/Colab Notebooks/ArabicKidsStories.txt'
file = open(filename,'rt')
text = file.read()
file.close()
import re
text = re.sub('([@A-Za-z0-9_]+)|[^\w\s]|#|http\S+', '', text) # cleaning up
print (text)
</code></pre>
<p>This is a sample of the output</p>
<pre><code> تفقدت نظارتي  حين استيقظت صباحا  فلم أجدها في مكانها  وبحثت عنها في كل مكان  دون أن أعثر لها على أثر  يا إلهي  كيف سأخرج اليوم من البيت  وأواجه النهار  
 وتناهى إلي من الخارج  صوت نقار الخشب  فوق جذع شجرة قريبة فأسرعت إلى الباب  وفتحته  وإذا ضوء النهار يبهر بصري  فأغلقت عيني  وهتفت  أيها النقار  أين أنت  
 وحاولت عبثا أن أفتح عيني  وأنا أقول  عفوا  لا أستطيع أن أفتح عيني  إن الضوء يعميني  
 فقال نقار الخشب  هذا طبيعي  يا عزيزتي  فأنت لم تضعي نظارتك الشمسية  
 وتراجعت قليلا  وقلت  لقد اختفت نظارتي  
 فتساءل نقار الخشب  اختفت  ماذا تقولين  
 وبدل أن أجيبه  قلت  أرجوك  ابحث لي عن نظارتي  إنني لا أستطيع الخروج من دونها  
 ولاذ نقار الخشب لحظة  ثم قال  حسن  ابقي أنت في البيت  وسأبحث لك أنا عنها  
 ومضى نقار الخشب  فأغلقت الباب والنافذة  وقبعت في الظلام  يا للغرابة  إنني أرى في الليل أيضا  أوه  كلا  إنني أحب النهار  وأحبذ أن أطير دوما في النور مع رفاقي  إنني لا أحب الليل  ولا أريد أن يكون الظلام عالمي  ترى أين اختفت هذه النظارة اللعينة  
    
 ـــــــــــــ 
 عاد نقار الخشب متعبا  قبل المساء  وقال لي  آسف  يا عزيزي  سألت عن نظارتك الطيور جميعا  لكن أحدا منهم لم يرها  
 فأطرقت برأسي برهة  ثم قلت  أشكرك  يا عزيزي  سأبحث عنها بنفسي ليلا  
 واتسعت عينا نقار الخشب دهشة  وقال  ليلا  
 وقبل أن أجيبه  مضى على عجل  وهو يقول  عفوا  صغاري ينتظرونني الآن  إلى اللقاء  
</code></pre>
<p>Any help will be appreciated.
Thanks in advance.</p>
",Dataset Preprocessing & Handling,clean non arabic letter text file python update new python clean text everything arabic letter used regex function without success code sample output help appreciated thanks advance
how to Read data from a text file in java to extract data using StanfordNLP rather than reading text from a simple String,"<p>i tried using
Annotation document = new Annotation(&quot;this is a simple string&quot;);
and also tried
CoreDocument coreDocument = new CoreDocument(text);
stanfordCoreNLP.annotate(coreDocument);
but not able to solve it to read from a text file</p>
",Dataset Preprocessing & Handling,read data text file java extract data using stanfordnlp rather reading text simple string tried using annotation document new annotation simple string also tried coredocument coredocument new coredocument text stanfordcorenlp annotate coredocument able solve read text file
Is it possible to tokenize feature on the fly while training?,"<p>I have CSV file with feature column (string) and multiple label columns (multi-label classification). Dataset is too big to fit into memory, so I have to load this using <a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset"" rel=""nofollow noreferrer"">make_csv_dataset</a> with specified batch size. The issue is that I don't know how to tokenize the feature column without overloading the memory, can I implement a DataProvider which will tokenize the data on the fly while training? Then I can tokenize it batch by batch so memory is not an issue.</p>
",Dataset Preprocessing & Handling,possible tokenize feature fly training csv file feature column string multiple label column multi label classification dataset big fit memory load using make csv dataset specified batch size issue know tokenize feature column without overloading memory implement dataprovider tokenize data fly training tokenize batch batch memory issue
Extracting information from multiple resumes all in PDF format,"<p>I have a data set with a column which has google drive link for resumes, I have 5000 rows so there are 5000 links , I am trying to extract information like years of experience and salary from these resumes in 2 separate columns. so far I've seen so many examples mentioned here on SO.</p>
<p>For example: the code mentioned below can only read the data from one file , how do I replicate this to multiple rows ?</p>
<p>Please help me with this , else I will have to manually go through 500 resumes and fill in the data</p>
<p>Hoping that I'll get a solution for this painful problem that I have.</p>
<pre><code>pdf_file = open('sample.pdf', 'rb')
read_pdf = PyPDF2.PdfFileReader(pdf_file)
number_of_pages = read_pdf.getNumPages()
page = read_pdf.getPage(0)
page_content = page.extractText()
print page_content.encode('utf-8')

#to extract salary , experience using regular expressions
import re

prog = re.compile(&quot;\s*(Name|name|nick).*&quot;)
result = prog.match(&quot;Name: Bob Exampleson&quot;)

if result:
    print result.group(0)

result = prog.match(&quot;University: MIT&quot;)

if result:
    print result.group(0)
</code></pre>
",Dataset Preprocessing & Handling,extracting information multiple resume pdf format data set column ha google drive link resume row link trying extract information like year experience salary resume separate column far seen many example mentioned example code mentioned read data one file replicate multiple row please help else manually go resume fill data hoping get solution painful problem
Rank the row based on the similar text using python?,"<p>How to rank the data frame based on the row value. i.e I have a row that contains text data want to provide the rank based on the similarity?</p>
<p><a href=""https://i.sstatic.net/v8T7q.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/v8T7q.png"" alt=""Input"" /></a></p>
<p>Expected output</p>
<p><a href=""https://i.sstatic.net/wsGxG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wsGxG.png"" alt=""expected output"" /></a></p>
<p>i have tried with the levistian distance but not sure how can i do for the whole table</p>
<pre><code>def bow(x=None):
    x = x.lower()
    words = x.split(' ')
    words.sort()
    x = ' '.join(words)
    
    exclude = set('{}{}'.format(string.punctuation, string.digits))
    x = ''.join(ch for ch in x if ch not in exclude)
    x = '{} '.format(x.strip())
    return x

#intents = load_intents(export=True)
df['bow'] = df['name'].apply(lambda x: bow(x))

df.sort_values(by='bow',ascending=True,inplace=True)

last_bow = ''
recs = []
for idx,row in df.iterrows():
    
    record = { 
        'name': row['name'],
        'bow': row['bow'],
        'lev_distance': ed.eval(last_bow,row['bow'])
    }
    recs.append(record)
    last_bow = row['bow']

intents = pd.DataFrame(recs,columns=['name','bow','lev_distance'])

l = intents[intents['lev_distance'] &lt;= lev_distance_range]

r = []
for x in l.index.values:
    r.append(x - 1)
    r.append(x)

r = list(set(r))
    
l = intents.iloc[r,:]
</code></pre>
",Dataset Preprocessing & Handling,rank row based similar text using python rank data frame based row value e row contains text data want provide rank based similarity expected output tried levistian distance sure whole table
Fuzzy fix column based correct values in a list,"<p>I have a dirty dataframe as shown below</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Region</th>
</tr>
</thead>
<tbody>
<tr>
<td>Berlin</td>
</tr>
<tr>
<td>Munich</td>
</tr>
<tr>
<td>Berlin-Spandau</td>
</tr>
<tr>
<td>Spandau-Berlin</td>
</tr>
<tr>
<td>Shop-Munich</td>
</tr>
<tr>
<td>munich-rest</td>
</tr>
<tr>
<td>Frankfurt</td>
</tr>
</tbody>
</table>
</div>
<p>I also have list with the clean information</p>
<p>city = ['Berlin','Munich','Frankfurt']</p>
<p>I need help creating a new column in the data frame with clean cities as shown</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Region</th>
<th>Clean Region</th>
</tr>
</thead>
<tbody>
<tr>
<td>Berlin</td>
<td>Berlin</td>
</tr>
<tr>
<td>Munich</td>
<td>Munich</td>
</tr>
<tr>
<td>Berlin-Spandau</td>
<td>Berlin</td>
</tr>
<tr>
<td>Spandau-Berlin</td>
<td>Berlin</td>
</tr>
<tr>
<td>Shop-Munich</td>
<td>Munich</td>
</tr>
<tr>
<td>munich-rest</td>
<td>Munich</td>
</tr>
<tr>
<td>Frankfurt-pla</td>
<td>Frankfurt</td>
</tr>
</tbody>
</table>
</div>
<p>I am not sure how to create this column. Need help in python</p>
",Dataset Preprocessing & Handling,fuzzy fix column based correct value list dirty dataframe shown region berlin munich berlin spandau spandau berlin shop munich munich rest frankfurt also list clean information city berlin munich frankfurt need help creating new column data frame clean city shown region clean region berlin berlin munich munich berlin spandau berlin spandau berlin berlin shop munich munich munich rest munich frankfurt pla frankfurt sure create column need help python
How to automatically label a cluster of words using semantics?,"<p>The context is : I already have clusters of words (phrases actually) resulting from kmeans applied to internet search queries and using common urls in the results of the search engine as a distance (co-occurrence of urls rather than words if I simplify a lot).</p>

<p>I would like to automatically label the clusters using semantics, in other words I'd like to extract the main concept surrounding a group of phrases considered together. </p>

<p>For example - sorry for the subject of my example - if I have the following bunch of queries : ['my husband attacked me','he was arrested by the police','the trial is still going on','my husband can go to jail for harrassing me ?','free lawyer']
My study deals with domestic violence, but clearly this cluster is focused on the legal aspect of the problem so the label could be ""legal"" for example.</p>

<p>I am new to NPL but I have to precise that I don't want to extract words using POS tagging (or at least this is not the expected final outcome but maybe a necessary preliminary step).</p>

<p>I read about Wordnet for sense desambiguation and I think that might be a good track, but I don't want to calculate similarity between two queries (since the clusters are the input) nor obtain the definition of one selected word thanks to the context provided by the whole bunch of words (which word to select in this case ?). I want to use the whole bunch of words to provide a context (maybe using synsets or categorization with the xml structure of the wordnet) and then summarize the context in one or few words.</p>

<p>Any ideas ? I can use R or python, I read a little about nltk but I don't find a way to use it in my context.</p>
",Dataset Preprocessing & Handling,automatically label cluster word using semantics context already cluster word phrase actually resulting kmeans applied internet search query using common url result search engine distance co occurrence url rather word simplify lot would like automatically label cluster using semantics word like extract main concept surrounding group phrase considered together example sorry subject example following bunch query husband attacked wa arrested police trial still going husband go jail harrassing free lawyer study deal domestic violence clearly cluster focused legal aspect problem label could legal example new npl precise want extract word using po tagging least expected final outcome maybe necessary preliminary step read wordnet sense desambiguation think might good track want calculate similarity two query since cluster input obtain definition one selected word thanks context provided whole bunch word word select case want use whole bunch word provide context maybe using synset categorization xml structure wordnet summarize context one word idea use r python read little nltk find way use context
How do I put my scraped data into a data frame,"<p>Please i need help, am having trouble trying to put my scraped data into a data frame that has 3 columns i.e. date, source and keywords extracted from each scraped website for further text analysis, my code is borrowed from <a href=""https://stackoverflow.com/users/12229253/foreverlearning"">https://stackoverflow.com/users/12229253/foreverlearning</a> and is given below:</p>
<pre><code>from newspaper import Article
import nltk
nltk.download('punkt')
urls = ['https://dailypost.ng/2022/02/02/securing-nigeria-duty-of-all-tiers-of-government-oyo-senator-balogun/', 'https://guardian.ng/news/declare-bandits-as-terrorists-senate-tells-buhari/', 'https://www.thisdaylive.com/index.php/2021/10/24/when-will-fg-declare-bandits-as-terrorists/', 'https://punchng.com/rep-wants-buhari-to-name-lawmaker-sponsoring-terrorism/', 'https://punchng.com/national-assembly-plans-to-meet-us-congress-over-875m-weapons-deal-stoppage/']
results = {}
for url in urls:
    article = Article(url)
    article.download()
    article.parse()
    article.nlp()
    results[url] = article
for url in urls:
    print(url)
    article = results[url]
    print(article.authors)
    print(article.publish_date)
    print(article.keywords)
</code></pre>
",Dataset Preprocessing & Handling,put scraped data data frame please need help trouble trying put scraped data data frame ha column e date source keywords extracted scraped website text analysis code borrowed href given p
How to compare rows in pandas data frames?,"<p>I have a pandas df with two columns. One of the columns contains strings of words, the one column contains single words. I need to compare the two columns to see if strings from 'Col_1' contain words from 'Col_2' and then create another column with an index of this element.</p>
<p>This is what I have:</p>
<pre><code>data = {'Col_1':  ['A B C D', 'A B C', 'A B C'], 'Col_2': ['D', 'B', 'Z']}
df = pd.DataFrame(data)
    
print (df)
</code></pre>
<p>This is what I need:</p>
<pre><code>data = {'Col_1':  ['A B C D', 'A B C', 'A B C'], 'Col_2': ['D', 'B', 'C'], 'Col_2': ['3', '1', '2']}
df = pd.DataFrame(data)
    
print (df)
</code></pre>
<p>I have been trying to iterate through columns using .iteritems() but it doesn't really help as it seems that I can't access elements in the string in 'Col_2'</p>
",Dataset Preprocessing & Handling,compare row panda data frame panda df two column one column contains string word one column contains single word need compare two column see string col contain word col create another column index element need trying iterate column using iteritems really help seems access element string col
How to create a document term incidence matrix from long format text data?,"<p>I've got data that look like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>word</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>blue</td>
</tr>
<tr>
<td>1</td>
<td>red</td>
</tr>
<tr>
<td>1</td>
<td>green</td>
</tr>
<tr>
<td>1</td>
<td>yellow</td>
</tr>
<tr>
<td>2</td>
<td>blue</td>
</tr>
<tr>
<td>2</td>
<td>purple</td>
</tr>
<tr>
<td>2</td>
<td>orange</td>
</tr>
<tr>
<td>2</td>
<td>green</td>
</tr>
</tbody>
</table>
</div>
<p>But I want to transform them into a binary incidence matrix denoting whether or not a word appears within a certain document ID. In other words, I'd like to create a matrix that looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>blue</th>
<th>red</th>
<th>green</th>
<th>yellow</th>
<th>purple</th>
<th>orange</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>Is there a way to do this with the tm package? I thought maybe using DocumentTermMatrix() would work since I don't think that any words in my corpus have multiple incidences within a single document, but everything I've tried has returned error messages about the incompatibility of the function with object class data.frame</p>
",Dataset Preprocessing & Handling,create document term incidence matrix long format text data got data look like id word blue red green yellow blue purple orange green want transform binary incidence matrix denoting whether word appears within certain document id word like create matrix look like id blue red green yellow purple orange way tm package thought maybe using documenttermmatrix would work since think word corpus multiple incidence within single document everything tried ha returned error message incompatibility function object class data frame
How to parse and encode Chinese Characters in Jupyter Notebook?,"<p>I want to train a really basic NLP model but using Chinese characters. Read_csv doesn't really work.</p>
<p>I was also wondering if there is any way to extract the different parts of the character, like for example, I would like to write an if function that works something like this:</p>
<pre><code>x = input()

if x contains 女 [as part of the word]:
   female = 1
</code></pre>
<p>So if i typed in 媛， then female=1</p>
<p>The bottomline is to train a Naive Bayes model on Chinese characters. If anyone can direct me to resources that can help me to do this will be appreciated!</p>
<p>Thanks</p>
",Dataset Preprocessing & Handling,parse encode chinese character jupyter notebook want train really basic nlp model using chinese character read csv really work wa also wondering way extract different part character like example would like write function work something like typed female bottomline train naive bayes model chinese character anyone direct resource help appreciated thanks
Biospans from String and Dictionary of Start and End Tokens for NLP Preprocessing,"<p>Suppose I have a simple sentence and a dictionary of which has 2 lists as start and end where start has the starting token and end has the ending token for each BIO Span and I want to create the BIO tags for the sentence where B denotes Beginning, I Inside and O Outside which is a pretty well used concept for data preprocessing in NLP how can I do it?</p>
<p>For instance suppose the input sentence is <code>&quot;I like to play soccer while he likes to run&quot;</code> and the dictionary of tokens is <code>{'start': [0, 6], 'end': [3, 9]}</code> then the expected output is <code>B I I I O O B I I I</code></p>
<p>You can assume the spans don't overlap</p>
",Dataset Preprocessing & Handling,biospans string dictionary start end token nlp preprocessing suppose simple sentence dictionary ha list start end start ha starting token end ha ending token bio span want create bio tag sentence b denotes beginning inside outside pretty well used concept data preprocessing nlp instance suppose input sentence dictionary token expected output assume span overlap
Pipeline fill-mask error with custom Roberta tokenizer,"<p>I’m using <a href=""https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=4keFBUjQFOD1"" rel=""nofollow noreferrer"">boilerplate code</a> to train a Roberta model on my text corpus.</p>
<p>Everything seems to be going okay until I try to load the pretrained tokenizer into the pipeline. I think it’s looking for a config.json file in the tokenizer folder but the BPE tokenizer is only outputting vocab.json and merges.txt files. what am I missing here?</p>
<p><a href=""https://gist.github.com/enpassanty/4bac3d1ed5d8995ac3c48050b0c2aca1"" rel=""nofollow noreferrer"">link to notebook gist</a></p>
",Dataset Preprocessing & Handling,pipeline fill mask error custom roberta tokenizer using boilerplate code train roberta model text corpus everything seems going okay try load pretrained tokenizer pipeline think looking config json file tokenizer folder bpe tokenizer outputting vocab json merges txt file missing link notebook gist
What algorithm should I use for the following sequential dataset training?,"<p>I have a dataset that contains opcode sequences of malware files. I read a paper where the author tried to implement a RNN algorithm like LSTM, but he specified a preprocessing step where he creates a word-bag and uses Word2Vec to convert everything into vectorized format.
I am stuck at this place. Any help would be appreciated.</p>
<pre><code>model = gensim.models.Word2Vec()
model.build_vocab(sequence_text, progress_per=1000)
model.train(sequence_text, total_examples=model.corpus_count, epochs=model.epochs)
</code></pre>
<p>I will also put a screenshot of the CSV file.
<a href=""https://i.sstatic.net/EKEnz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EKEnz.png"" alt=""Dataset"" /></a></p>
<p>Ultimate Goal: I need to identify if a sequence belongs to malware class or not.</p>
",Dataset Preprocessing & Handling,algorithm use following sequential dataset training dataset contains opcode sequence malware file read paper author tried implement rnn algorithm like lstm specified preprocessing step creates word bag us word vec convert everything vectorized format stuck place help would appreciated also put screenshot csv file ultimate goal need identify sequence belongs malware class
Computing relative frequencies based on dictionary,"<p>I'd like to examine the Psychological Capital (a construct consisting of four dimensions, namely hope, optimism, efficacy and resiliency) of founders using computer-aided text analysis in R. So far I have pulled tweets from various users into R. The data frame contains of 2130 tweets from 5 different users in different periods. The dataframe is called before_failure.
<a href=""https://i.sstatic.net/SvHqB.png"" rel=""nofollow noreferrer"">Picture of original data frame</a></p>
<p>I have then used the quanteda package to  create a corpus, perfomed tokenization on it and removed redundant punctuatio/numbers/symbols:</p>
<pre><code>#Creating a corpus
before_failure_corpus &lt;- corpus(before_failure, text_field = &quot;text&quot;)

#Tokenization, removing punctuation and numbers
tok_before_failure &lt;- before_failure_corpus %&gt;%
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %&gt;% 
  tokens_tolower()
</code></pre>
<p>After that I created a dictionary also using the quanteda package (The dictionary itself has been created by other authors examining Psychological capital):</p>
<pre><code>
#Creating Dictionary with quanteda
dict &lt;- dictionary(list(hope = c(&quot;Accomplishments&quot;, &quot;Achievements&quot;, &quot;Approach&quot;, &quot;Aspiration&quot;, &quot;Aspire&quot;, &quot;Aspired&quot;,
                                 &quot;Aspirer&quot;, &quot;Aspires&quot;, &quot;Aspiring&quot;, &quot;Aspiringly&quot;, &quot;Assurance&quot;, &quot;Assurances&quot;, &quot;Assure&quot;,
                                 &quot;Assured&quot;, &quot;Assuredly&quot;, &quot;Assuredness&quot;, &quot;Assuring&quot;, &quot;Assuringly&quot;, &quot;Assuringness&quot;, &quot;Belief&quot;,
                                 &quot;Believe&quot;, &quot;Believed&quot;, &quot;Believes&quot;, &quot;Believing&quot;, &quot;Breakthrough&quot;, &quot;Certain&quot;, &quot;Certainly&quot;,
                                 &quot;Certainty&quot;, &quot;Committed&quot;, &quot;Concept&quot;, &quot;Confidence&quot;, &quot;Confident&quot;, &quot;Confidently&quot;,
                                 &quot;Convinced&quot;, &quot;Dare say&quot;, &quot;Deduce&quot;, &quot;Deduced&quot;, &quot;Deduces&quot;, &quot;Deducing&quot;, &quot;Desire&quot;,
                                 &quot;Desired&quot;, &quot;Desires&quot;, &quot;Desiring&quot;, &quot;Doubt not&quot;, &quot;Energy&quot;, &quot;Engage&quot;, &quot;Engagement&quot;,
                                 &quot;Expectancy&quot;, &quot;Faith&quot;, &quot;Foresaw&quot;, &quot;Foresee&quot;, &quot;Foreseeing&quot;, &quot;Foreseen&quot;, &quot;Foresees&quot;, &quot;Goal&quot;,
                                 &quot;Goals&quot;, &quot;Hearten&quot;, &quot;Heartened&quot;, &quot;Heartening&quot;, &quot;Hearteningly&quot;, &quot;Heartens&quot;, &quot;Hope&quot;,
                                 &quot;Hoped&quot;, &quot;Hopeful&quot;, &quot;Hopefully&quot;, &quot;Hopefulness&quot;, &quot;Hoper&quot;, &quot;Hopes&quot;, &quot;Hoping&quot;, &quot;Idea&quot;,
                                 &quot;Innovation&quot;, &quot;Innovative&quot;, &quot;Ongoing&quot;, &quot;Opportunity&quot;, &quot;Promise&quot;, &quot;Promising&quot;,
                                 &quot;Propitious&quot;, &quot;Propitiously&quot;, &quot;Propitiousness&quot;, &quot;Solution&quot;, &quot;Solutions&quot;, &quot;Upbeat&quot;,
                                 &quot;Wishes&quot;, &quot;Wishing&quot;, &quot;Yearn&quot;, &quot;Yearn for&quot;, &quot;Yearning&quot;, &quot;Yearning for&quot;, &quot;Yearns for&quot;),
                       efficacy = c(&quot;Ability&quot;, &quot;Accomplish&quot;, &quot;Accomplished&quot;, &quot;Accomplishes&quot;, &quot;Accomplishing&quot;,
                                    &quot;Accomplishments&quot;, &quot;Achievements&quot;, &quot;Achieving&quot;, &quot;Adept&quot;, &quot;Adeptly&quot;, &quot;Adeptness&quot;,
                                    &quot;Adroitly&quot;, &quot;Adroitness&quot;, &quot;All-in&quot;, &quot;Aplomb&quot;, &quot;Arrogance&quot;, &quot;Arrogant&quot;, &quot;Arrogantly&quot;,
                                    &quot;Assurance&quot;, &quot;Assured&quot;, &quot;Assuredly&quot;, &quot;Assuredness&quot;, &quot;Backbone&quot;, &quot;Bandwidth&quot;, &quot;Belief&quot;,
                                    &quot;Capable&quot;, &quot;Capableness&quot;, &quot;Capably&quot;, &quot;Certain&quot;, &quot;Certainly&quot;, &quot;Certainness&quot;, &quot;Certainty&quot;,
                                    &quot;Certitude&quot;, &quot;Cocksurely&quot;, &quot;Cocksureness&quot;, &quot;Cocky&quot;, &quot;Commitment&quot;, &quot;Commitments&quot;,
                                    &quot;Committed&quot;, &quot;Compelling&quot;, &quot;Competence&quot;, &quot;Competency&quot;, &quot;Competent&quot;, &quot;Competently&quot;,
                                    &quot;Confidence&quot;, &quot;Confident&quot;, &quot;Confidently&quot;, &quot;Conviction&quot;, &quot;Effective&quot;, &quot;Effectively&quot;,
                                    &quot;Effectiveness&quot;, &quot;Effectual&quot;, &quot;Effectually&quot;, &quot;Effectualness&quot;, &quot;Efficacious&quot;, &quot;Efficaciously&quot;,
                                    &quot;Efficaciousness&quot;, &quot;Efficacy&quot;, &quot;Equanimity&quot;, &quot;Equanimous&quot;, &quot;Equanimously&quot;, &quot;Expertise&quot;,
                                    &quot;Expertly&quot;, &quot;Fortitude&quot;, &quot;Fortitudinous&quot;, &quot;Forward&quot;, &quot;Forwardness&quot;, &quot;Know-how&quot;,
                                    &quot;Knowledgability&quot;, &quot;Knowledgeable&quot;, &quot;Knowledgably&quot;, &quot;Masterful&quot;, &quot;Masterfully&quot;, &quot;Masterfulness&quot;,
                                    &quot;Masterly&quot;, &quot;Mastery&quot;, &quot;Overconfidence&quot;, &quot;Overconfident&quot;, &quot;Overconfidently&quot;,
                                    &quot;Persuasion&quot;, &quot;Power&quot;, &quot;Powerful&quot;, &quot;Powerfully&quot;, &quot;Powerfulness&quot;, &quot;Prevailed&quot;,
                                    &quot;Prevailing&quot;, &quot;Prevails&quot;, &quot;Prevalence&quot;, &quot;Prevalent&quot;, &quot;Reassurance&quot;, &quot;Reassure&quot;, &quot;Reassured&quot;,
                                    &quot;Reassures&quot;, &quot;Reassuring&quot;, &quot;Self-assurance&quot;, &quot;Self-assured&quot;, &quot;Self-assuring&quot;, &quot;Selfconfidence&quot;,
                                    &quot;Self-confident&quot;, &quot;Self-dependence&quot;, &quot;Self-dependent&quot;, &quot;Self-reliance&quot;,
                                    &quot;Self-reliant&quot;, &quot;Stamina&quot;, &quot;Steadily&quot;, &quot;Steadiness&quot;, &quot;Steady&quot;, &quot;Strength&quot;, &quot;Strong&quot;, &quot;Stronger&quot;,
                                    &quot;Strongish&quot;, &quot;Strongly&quot;, &quot;Strongness&quot;, &quot;Superior&quot;, &quot;Superiority&quot;, &quot;Sure&quot;, &quot;Surely&quot;, &quot;Sureness&quot;,
                                    &quot;Unblinking&quot;, &quot;Unblinkingly&quot;, &quot;Undoubtedly&quot;, &quot;Undoubting&quot;, &quot;Unflappability&quot;, &quot;Unflappable&quot;,
                                    &quot;Unflinching&quot;, &quot;Unflinchingly&quot;, &quot;Unhesitating&quot;, &quot;Unhesitatingly&quot;, &quot;Unwavering&quot;,
                                    &quot;Unwaveringly&quot;),
                       resiliency = c(&quot;Adamant&quot;, &quot;Adamantly&quot;, &quot;Assiduous&quot;, &quot;Assiduously&quot;, &quot;Assiduousness&quot;, &quot;Backbone&quot;,
                                      &quot;Bandwidth&quot;, &quot;Bears up&quot;, &quot;Bounce&quot;, &quot;Bounced&quot;, &quot;Bounces&quot;, &quot;Bouncing&quot;, &quot;Buoyant&quot;,
                                      &quot;Commitment&quot;, &quot;Commitments&quot;, &quot;Committed&quot;, &quot;Consistent&quot;, &quot;Determination&quot;,
                                      &quot;Determined&quot;, &quot;Determinedly&quot;, &quot;Determinedness&quot;, &quot;Devoted&quot;, &quot;Devotedly&quot;,
                                      &quot;Devotedness&quot;, &quot;Devotion&quot;, &quot;Die trying&quot;, &quot;Died trying&quot;, &quot;Dies trying&quot;, &quot;Disciplined&quot;,
                                      &quot;Dogged&quot;, &quot;Doggedly&quot;, &quot;Doggedness&quot;, &quot;Drudge&quot;, &quot;Drudged&quot;, &quot;Drudges&quot;, &quot;Endurance&quot;,
                                      &quot;Endure&quot;, &quot;Endured&quot;, &quot;Endures&quot;, &quot;Enduring&quot;, &quot;Grit&quot;, &quot;Hammer away&quot;, &quot;Hammered away&quot;,
                                      &quot;Hammering away&quot;, &quot;Hammers away&quot;, &quot;Held fast&quot;, &quot;Held good&quot;, &quot;Held up&quot;, &quot;Hold fast&quot;,
                                      &quot;Holding fast&quot;, &quot;Holding up&quot;, &quot;Holds fast&quot;, &quot;Holds good&quot;, &quot;Immovability&quot;, &quot;Immovable&quot;,
                                      &quot;Immovably&quot;, &quot;Indefatigable&quot;, &quot;Indefatigableness&quot;, &quot;Indefatigably&quot;, &quot;Indestructibility&quot;,
                                      &quot;Indestructible&quot;, &quot;Indestructibleness&quot;, &quot;Indestructibly&quot;, &quot;Intransigence&quot;, &quot;Intransigency&quot;,
                                      &quot;Intransigent&quot;, &quot;Keep at&quot;, &quot;Keep going&quot;, &quot;Keep on&quot;, &quot;Keeping at&quot;, &quot;Keeping going&quot;,
                                      &quot;Keeping on&quot;, &quot;Keeps at&quot;, &quot;Keeps going&quot;, &quot;Keeps on&quot;, &quot;Kept at&quot;, &quot;Kept going&quot;, &quot;Kept on&quot;,
                                      &quot;Labored&quot;, &quot;Laboring&quot;, &quot;Never-tiring&quot;, &quot;Never-wearying&quot;, &quot;Perdure&quot;, &quot;Perdured&quot;, &quot;Perduring&quot;,
                                      &quot;Perseverance&quot;, &quot;Persevere&quot;, &quot;Persevered&quot;, &quot;Persevering&quot;, &quot;Persist&quot;, &quot;Persisted&quot;,
                                      &quot;Persistence&quot;, &quot;Persistent&quot;, &quot;Persisting&quot;, &quot;Pertinacious&quot;, &quot;Pertinaciously&quot;, &quot;Pertinacity&quot;,
                                      &quot;Rebound&quot;, &quot;Rebounded&quot;, &quot;Rebounding&quot;, &quot;Rebounds&quot;, &quot;Relentlessness&quot;, &quot;Remain&quot;,
                                      &quot;Remained&quot;, &quot;Remaining&quot;, &quot;Remains&quot;, &quot;Resilience&quot;, &quot;Resiliency&quot;, &quot;Resilient&quot;, &quot;Resolute&quot;,
                                      &quot;Resolutely&quot;, &quot;Resoluteness&quot;, &quot;Resolve&quot;, &quot;Resolved&quot;, &quot;Resolves&quot;, &quot;Resolving&quot;, &quot;Robust&quot;,
                                      &quot;Sedulity&quot;, &quot;Sedulous&quot;, &quot;Sedulously&quot;, &quot;Sedulousness&quot;, &quot;Snap back&quot;, &quot;Snapped back&quot;,
                                      &quot;Snapping back&quot;, &quot;Snaps back&quot;, &quot;Spring back&quot;, &quot;Springing back&quot;, &quot;Springs&quot;, &quot;Springs back&quot;,
                                      &quot;Sprung back&quot;, &quot;Stalwart&quot;, &quot;Stalwartly&quot;, &quot;Stalwartness&quot;, &quot;Stand fast&quot;, &quot;Stand firm&quot;, &quot;Standingfast&quot;,
                                      &quot;Standing firm&quot;, &quot;Stands fast&quot;, &quot;Stands firm&quot;, &quot;Stay&quot;, &quot;Steadfast&quot;, &quot;Steadfastly&quot;,
                                      &quot;Steadfastness&quot;, &quot;Stood fast&quot;, &quot;Stood firm&quot;, &quot;Strove&quot;, &quot;Survive&quot;, &quot;Surviving&quot;, &quot;Surviving&quot;,
                                      &quot;Tenacious&quot;, &quot;Tenaciously&quot;, &quot;Tenaciousness&quot;, &quot;Tenacity&quot;, &quot;Tough&quot;, &quot;Uncompromising&quot;,
                                      &quot;Uncompromisingly&quot;, &quot;Uncompromisingness&quot;, &quot;Unfaltering&quot;, &quot;Unfalteringly&quot;, &quot;Unflagging&quot;,
                                      &quot;Unrelenting&quot;, &quot;Unrelentingly&quot;, &quot;Unrelentingness&quot;, &quot;Unshakable&quot;, &quot;Unshakablely&quot;,
                                      &quot;Unshakeable&quot;, &quot;Unshaken&quot;, &quot;Unshaking&quot;, &quot;Unswervable&quot;, &quot;Unswerved&quot;, &quot;Unswerving&quot;,
                                      &quot;Unswervingly&quot;, &quot;Unswervingness&quot;, &quot;Untiring&quot;, &quot;Unwavered&quot;, &quot;Unwavering&quot;, &quot;Unweariedness&quot;,
                                      &quot;Unyielding&quot;, &quot;Unyieldingly&quot;, &quot;Unyieldingness&quot;, &quot;Upheld&quot;, &quot;Uphold&quot;, &quot;Upholding&quot;,
                                      &quot;Upholds&quot;, &quot;Zeal&quot;, &quot;Zealous&quot;, &quot;Zealously&quot;, &quot;Zealousness&quot;),
                       optimism = c(&quot;Aspire&quot;, &quot;Aspirer&quot;, &quot;Aspires&quot;, &quot;Aspiring&quot;, &quot;Aspiringly&quot;, &quot;Assurance&quot;, &quot;Assured&quot;, &quot;Assuredly&quot;,
                                    &quot;Assuredness&quot;, &quot;Assuring&quot;, &quot;Auspicious&quot;, &quot;Auspiciously&quot;, &quot;Auspiciousness&quot;, &quot;Bank on&quot;,
                                    &quot;Beamish&quot;, &quot;Believe&quot;, &quot;Believed&quot;, &quot;Believes&quot;, &quot;Believing&quot;, &quot;Bullish&quot;, &quot;Bullishly&quot;, &quot;Bullishness&quot;,
                                    &quot;Confidence&quot;, &quot;Confident&quot;, &quot;Confidently&quot;, &quot;Encourage&quot;, &quot;Encouraged&quot;, &quot;Encourages&quot;,
                                    &quot;Encouraging&quot;, &quot;Encouragingly&quot;, &quot;Ensuring&quot;, &quot;Expectancy&quot;, &quot;Expectant&quot;, &quot;Expectation&quot;,
                                    &quot;Expectations&quot;, &quot;Expected&quot;, &quot;Expecting&quot;, &quot;Faith&quot;, &quot;Good omen&quot;, &quot;Hearten&quot;, &quot;Heartened&quot;,
                                    &quot;Heartener&quot;, &quot;Heartening&quot;, &quot;Hearteningly&quot;, &quot;Heartens&quot;, &quot;Hope&quot;, &quot;Hoped&quot;, &quot;Hopeful&quot;,
                                    &quot;Hopefully&quot;, &quot;Hopefulness&quot;, &quot;Hoper&quot;, &quot;Hopes&quot;, &quot;Hoping&quot;, &quot;Ideal&quot;, &quot;Idealist&quot;, &quot;Idealistic&quot;,
                                    &quot;Idealistically&quot;, &quot;Ideally&quot;, &quot;Looking up&quot;, &quot;Looks up&quot;, &quot;Optimism&quot;, &quot;Optimist&quot;, &quot;Optimistic&quot;,
                                    &quot;Optimistical&quot;, &quot;Optimistically&quot;, &quot;Outlook&quot;, &quot;Positive&quot;, &quot;Positively&quot;, &quot;Positiveness&quot;,
                                    &quot;Positivity&quot;, &quot;Promising&quot;, &quot;Propitious&quot;, &quot;Propitiously&quot;, &quot;Propitiousness&quot;, &quot;Reassure&quot;,
                                    &quot;Reassured&quot;, &quot;Reassures&quot;, &quot;Reassuring&quot;, &quot;Roseate&quot;, &quot;Rosy&quot;, &quot;Sanguine&quot;, &quot;Sanguinely&quot;,
                                    &quot;Sanguineness&quot;, &quot;Sanguinity&quot;, &quot;Sunniness&quot;, &quot;Sunny&quot;)))

</code></pre>
<p>Now i would like to compute the relative frequency by dividing the number of words used in the tweets that reflect the four dimensions of Psycap trough the total number of words in the corpus. Unfortunately I got stuck at this point. In the end I would like to have a table that looks like this (values are made up):</p>
<pre><code> dimensions Frequency
1       hope      0.36
2   optimism      0.50
3   Efficacy      0.22
4 Resiliency      0.10
</code></pre>
<p>I hope my explanations are sufficient, if not do not hesitate to ask.
Thank you</p>
",Dataset Preprocessing & Handling,computing relative frequency based dictionary like examine psychological capital construct consisting four dimension namely hope optimism efficacy resiliency founder using computer aided text analysis r far pulled tweet various user r data frame contains tweet different user different period dataframe called failure picture original data frame used quanteda package create corpus perfomed tokenization removed redundant punctuatio number symbol created dictionary also using quanteda package dictionary ha created author examining psychological capital would like compute relative frequency dividing number word used tweet reflect four dimension psycap trough total number word corpus unfortunately got stuck point end would like table look like value made hope explanation sufficient hesitate ask thank
how to replace the command tfds.load for imdb reviews with download dataset file?,"<p>tfds.load(name=&quot;imdb_reviews&quot;, data_dir=direc, split=&quot;train&quot;, with_info=True, as_supervised=True)</p>
<p>i have download the dataset , it has downloads and imdb_reviews directories, in the imdb_reviews directory, it has plain_text directory and inside it, exists a directory named 1.0.0 and there are some files inside that. let me say the path to train is: '/content/drive/MyDrive/datasets/packt/imdb/imdb_reviews/plain_text/1.0.0/imdb_reviews-train.tfrecord-00000-of-00001' and the path to test is '/content/drive/MyDrive/datasets/packt/imdb/imdb_reviews/plain_text/1.0.0/imdb_reviews-test.tfrecord-00000-of-00001' , there are also dataset_info.json and features.json and labels.labels.txt and  an unsupervised file, how can I replace the command so that it does not cause other problems. I want to tokenize and encode it with a function</p>
<p>bert_train= [bert_encoder(r) for r,l in imdb_train]</p>
<p>and there is</p>
<p>encoded= tokenizer.encode_plus(text, add_special_tokens=True, max_length=150, pad_to_max_length=True,truncation=True,return_attention_mask=True, return_token_type_ids=True )</p>
<p>inside that encoding function.
thank you</p>
",Dataset Preprocessing & Handling,replace command tfds load imdb review download dataset file tfds load name imdb review data dir direc split train info true supervised true download dataset ha downloads imdb review directory imdb review directory ha plain text directory inside exists directory named file inside let say path train content drive mydrive datasets packt imdb imdb review plain text imdb review train tfrecord path test content drive mydrive datasets packt imdb imdb review plain text imdb review test tfrecord also dataset info json feature json label label txt unsupervised file replace command doe cause problem want tokenize encode function bert train bert encoder r r l imdb train encoded tokenizer encode plus text add special token true max length pad max length true truncation true return attention mask true return token type id true inside encoding function thank
Alternative to loading large file from s3,"<p>I have been trying to load a large language model (&gt; 5 GB) hosted on S3 for use in a Lambda function, but have been so far unsuccessful. The function just continuously times out after a few minutes, even when set on 10240 MB memory.</p>
<p>I assume this is because of the limits on the Lambda function, as well as the streaming of such a large file from S3.</p>
<p>For my implementation, my function needs to be able to load the language model fairly quickly ( ~5-10 seconds).</p>
<p>Being quite new to AWS, is there a better way of doing this?</p>
",Dataset Preprocessing & Handling,alternative loading large file trying load large language model gb hosted use lambda function far unsuccessful function continuously time minute even set mb memory assume limit lambda function well streaming large file implementation function need able load language model fairly quickly second quite new aws better way
R Function for Return New Column to Dataset,"<p>I'm currently working with a dataset with different speakers and am trying to extract the amount of words in a utterance. I am also trying to count the number of backchannels (utterances with three or less words). These metrics would be used for further analysis of the dataset. Please see a slice of the data below.</p>
<pre><code>speaker &lt;- c(&quot;P6&quot;, &quot;P4&quot;, &quot;P5&quot;, &quot;P6&quot;, &quot;P6&quot;)
utterance &lt;- c(&quot;Alright&quot;, &quot;So this is a social talk right? So we’re only supposed to talk about work only&quot;, &quot;yeah&quot;, &quot;And that’s the thing, so it’s not clear to me we need to work or just to be social.&quot;, &quot;But a bit and a bit&quot;)
df &lt;- data.frame(speaker, utterance)
</code></pre>
<p>These are the different functions I've tried out. The issue is that I would like to store the results in a new column in the same dataframe, but this is something I haven't been able to do yet (I'm a beginner with R). I can see with the code below that the first function works as intended, but I am having some issues with the second one. Ideally I would like both functions to just accept the generic column name rather than the dataframe.</p>
<pre><code>#function for utterance length
utterance_length &lt;- function(df){
  df &lt;- df %&gt;%
  mutate(utterance_length = str_count(df$utterance,&quot;\\S+&quot;))
  return(df)}


#function for backchannelling 
backchannelling &lt;- function(df){
  df$backchannelling &lt;- ifelse(df$utterance_length &gt; 3, 0, 1)
  return(df)
}
</code></pre>
<p>How can I: 1) save the new utterance_length column to the data frame (same goes to the backchannelling function); 2) only input column names in the function rather than the dataframe.</p>
",Dataset Preprocessing & Handling,r function return new column dataset currently working dataset different speaker trying extract amount word utterance also trying count number backchannels utterance three le word metric would used analysis dataset please see slice data different function tried issue would like store result new column dataframe something able yet beginner r see code first function work intended issue second one ideally would like function accept generic column name rather dataframe save new utterance length column data frame go backchannelling function input column name function rather dataframe
Python Beginner: How to select key and values from JSON file in Python,"<p>I am working with an API generated json file which contains dictionaries. In the &quot;element dictionary&quot; (inside json file) there is &quot;Path&quot; key and it has a value. Some of the &quot;Path&quot; values are duplicates but I want to make them unique for example by adding /random_alphabets or /random_numbers at the end of the &quot;path&quot; values.
I am using pandas and json_normalize to do this but after making the data frame when I normalize the &quot;elements dictionary&quot; to get the paths, it only displays 50 &quot;paths&quot; (there are 243 paths in the elements dictionary in json file).</p>
<p>Any help would be appreciated. Thanks!</p>
<p>Following is my code:</p>
<pre><code>import pandas as pd
from pandas.io.json import json_normalize
import json

with open('structuredData-15.json') as json_data:
    text = json.load(json_data)
text

test = json_normalize(text)
test

ele = test['elements'][0]
elements = json_normalize(ele)
elements
elements['Path']
</code></pre>
<p>Following is the link to the json file I am using:
<a href=""https://drive.google.com/drive/folders/1G4eaHRa8IPyGrtQKEa_rbU0CupJih7_4?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/1G4eaHRa8IPyGrtQKEa_rbU0CupJih7_4?usp=sharing</a></p>
<p>I also did this but it is still printing 50 paths rather than 243</p>
<pre><code>with open('structuredData-15.json',&quot;r&quot;) as json_data:
     text = json.load(json_data)
test = json_normalize(text)
for item in test['elements']:    
     print(test['elements'][0]) #this is printing all the elements including all the paths(243 paths)
for item in test['elements'][0]:    
     print(item['Path']) #But when I select Path, it just prints 50
</code></pre>
",Dataset Preprocessing & Handling,python beginner select key value json file python working api generated json file contains dictionary element dictionary inside json file path key ha value path value duplicate want make unique example adding random alphabet random number end path value using panda json normalize making data frame normalize element dictionary get path display path path element dictionary json file help would appreciated thanks following code following link json file using also still printing path rather
Spell Correction with Python (pyspellchecker),"<p>I wanna build a spell correction using python and I try to use pyspellchecker, because I have to build my own dictionary and I think pyspellchecker is easy to use with our own model or dictionary. My problem is, how to load and return my word with case_sensitive is On?
I have tried this:</p>

<p><code>spell = SpellChecker(language=None, case_sensitive=True)</code></p>

<p>but when I load my file contains many text like 'Hello' with this code:</p>

<p><code>spell.word_frequency.load_text_file('myfile.txt')</code></p>

<p>and when I start to spell with <code>spell.correction('Hello')</code> its return <code>'hello'</code> (lower case).
Do you know how to build our own model or dictionary with our letters not diminished or it stays uppercase? </p>

<p>Or if you have a recommendation for spell-checking with our own model please let me know, Thank you!</p>
",Dataset Preprocessing & Handling,spell correction python pyspellchecker wan na build spell correction using python try use pyspellchecker build dictionary think pyspellchecker easy use model dictionary problem load return word case sensitive tried load file contains many text like hello code start spell return lower case know build model dictionary letter diminished stay uppercase recommendation spell checking model please let know thank
Chinese Whispers for NLP How to implement for my corpus file,"<p>I'm new to Python, I want to make this code to implement my corpus from (.csv)files Can anybody help me how to implement it like in the picture below.</p>
<pre class=""lang-py prettyprint-override""><code>from chinese_whispers import __version__ as cw_version
from networkx import __version__ as nx_version
from matplotlib import __version__ as plt_version
print('Chinese Whispers {}'.format(cw_version))
print('NetworkX {}'.format(nx_version))
print('matplotlib {}'.format(plt_version))
import networkx as nx
from chinese_whispers import chinese_whispers, aggregate_clusters
G = nx.karate_club_graph()
# Perform clustering of G, parameters weighting and seed can be omitted
chinese_whispers(G, weighting='top', seed=10000) 

# Print the clusters in the descending order of size
print('ID\tCluster\n')

for label, cluster in sorted(aggregate_clusters(G).items(), key=lambda e: len(e[1]), reverse=True):
    print('{}\t{}\n'.format(label, cluster))
import matplotlib.pyplot as plt
colors = [1. / G.nodes[node]['label'] for node in G.nodes()]

nx.draw_networkx(G, cmap=plt.get_cmap('jet'), node_color=colors, font_color='white')
</code></pre>
<p><a href=""https://i.sstatic.net/F7CnL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/F7CnL.jpg"" alt=""wanted output"" /></a></p>
",Dataset Preprocessing & Handling,chinese whisper nlp implement corpus file new python want make code implement corpus csv file anybody help implement like picture
how to load a huge json file with Pycharm in windows 10?,"<p>I am a prospective graduate student studying deep learning.<br>
I ran the code below on my macbook. The result was success. All json files are loaded and processed well.<br>
I figured it would be faster if I processed the data with a lab computer with a bigger 16gb ram.<br>
However, it was not executed in Windows as a <code>Memory Error</code> occurred.<br>
I set Windows virtual memory paging size and Pycharm vm size  following advice on the internet. but error was not solved.</p>
<p><a href=""https://i.sstatic.net/JW6rU.png"" rel=""nofollow noreferrer"">virtual memory paging file size setting</a></p>
<p>how to run the code below in Windows Desktop? and why the code does not run in the Desktop though it has bigger RAM?<br></p>
<h1>Settings</h1>
<p>My computer settings are as follows:</p>
<p><strong>Individual Notebook</strong></p>
<ul>
<li>m1 silicon Macbook Air</li>
<li>8gb ram</li>
<li>256gb storage
</li>
</ul>
<p><strong>Desktop in Lab</strong></p>
<ul>
<li>Windows 10 Education</li>
<li>16gb ram</li>
<li>256gb ssd, 1TB hdd storage
</li>
</ul>
<p><strong>requirements.txt</strong></p>
<ul>
<li>python=3.8.12</li>
<li>pysimdjson=4.0.3</li>
<li>pandas=1.3.5</li>
</ul>
<p><strong>json file</strong></p>
<ul>
<li>NXLS~.json (2.39gb)</li>
<li>SXLS~.json (1.06gb)</li>
</ul>
<h1>Code</h1>
<p>my codes are as follows:</p>
<pre><code>import simdjson
import pandas as pd
from tqdm import trange


if __name__ == '__main__':
    # json key names corresponding to column names
    mapping_key = ['doc_index', 'begin', 'end', 'word_id', 'word', 'pos', 'sense_id']         
    # dataframe column name
    columns = ['sentence', 'target_index_start', 'target_index_end', 'target_word_id', 'target_word', 'target_pos', 'sense_id']
    corpus = pd.DataFrame(columns=columns)

    # read .json dataset.
    data_path = './dataset'
    NXLS = 'NXLS2002104060.json'
    SXLS = &quot;SXLS2002104060.json&quot;

    with open(data_path + '/' + NXLS, &quot;rb&quot;) as f:
        nxls_doc = simdjson.loads(f.read())


    with open(data_path + '/' + SXLS, &quot;rb&quot;) as f:
        sxls_doc = simdjson.loads(f.read())

    # parsing
    sentences = []
    WSDs = []
    for i in trange(len(nxls_doc[&quot;document&quot;])):
        for j in range(len(nxls_doc[&quot;document&quot;][i][&quot;sentence&quot;])):
            sentence = nxls_doc[&quot;document&quot;][i][&quot;sentence&quot;][j][&quot;form&quot;]
            sentences.append(sentence)
            wsd = nxls_doc[&quot;document&quot;][i][&quot;sentence&quot;][j][&quot;WSD&quot;]
            for k in range(len(wsd)):
                wsd[k][&quot;doc_index&quot;] = j
            # wsd[(k for k in range(len(wsd)))][&quot;doc_index&quot;] = j
            WSDs.append(wsd)

    for i in trange(len(sxls_doc[&quot;document&quot;])):
        for j in range(len(sxls_doc[&quot;document&quot;][i][&quot;sentence&quot;])):
            sentence = sxls_doc[&quot;document&quot;][i][&quot;sentence&quot;][j][&quot;form&quot;]
            sentences.append(sentence)
            wsd = sxls_doc[&quot;document&quot;][i][&quot;sentence&quot;][j][&quot;WSD&quot;]
            for k in range(len(wsd)):
                wsd[k][&quot;doc_index&quot;] = j
            # wsd[(k for k in range(len(wsd)))][&quot;doc_index&quot;] = j
            WSDs.append(wsd)

    # list -&gt; Dataframe (restructure)
    for i in trange(len(WSDs)):
        wsd = WSDs[i]
        for j in range(len(wsd)):
            row = []
            doc_index = wsd[j][&quot;doc_index&quot;]
            row.append(sentences[doc_index])
            #for k in range(len(columns)-1):
            row.extend([wsd[j][mapping_key[k+1]] for k in range(len(columns)-1)])
            #corpus[columns[k+1]] = wsd[j][mapping_key[k+1]]
            corpus.loc[len(corpus)] = row
</code></pre>
<h1>Result</h1>
<p><strong>Mac</strong>
<br>
<a href=""https://i.sstatic.net/thuoy.png"" rel=""nofollow noreferrer"">result1 in m1 silicon</a></p>
<p><a href=""https://i.sstatic.net/2zYkg.png"" rel=""nofollow noreferrer"">result2 in m1 silicon. because of the speed of processing, I stopped it.</a></p>
<p><a href=""https://i.sstatic.net/8vw8K.png"" rel=""nofollow noreferrer"">json successfully loaded in m1 silicon</a></p>
<p><strong>windows</strong>
<br>
<a href=""https://i.sstatic.net/GU0EP.png"" rel=""nofollow noreferrer"">result in Windows 10</a></p>
",Dataset Preprocessing & Handling,load huge json file pycharm window prospective graduate student studying deep learning ran code macbook result wa success json file loaded processed well figured would faster processed data lab computer bigger gb ram however wa executed window occurred set window virtual memory paging size pycharm vm size following advice internet error wa solved virtual memory paging file size setting run code window desktop code doe run desktop though ha bigger ram setting computer setting follows individual notebook silicon macbook air gb ram gb storage desktop lab window education gb ram gb ssd tb hdd storage requirement txt python pysimdjson panda json file nxls json gb sxls json gb code code follows result mac result silicon result silicon speed processing stopped json successfully loaded silicon window result window
Is there an easy way to read a .md (markdown) file as a character vector into R,"<p>I have a raw markdown file that I have taken from online, and I want to read the text into R as a character vector. Is there a good way of doing this?</p>
<p>Thanks.</p>
",Dataset Preprocessing & Handling,easy way read md markdown file character vector r raw markdown file taken online want read text r character vector good way thanks
I was looking for how to unload the obtained data into a .txt or csv file,"<p>I was looking for how to unload the obtained data into a .txt or csv file, but I could not find a simple and understandable solution with my level of understanding of this process.</p>
<p>I need to sort words by frequency and highlight the top 100 words. I did it (I know not in the best way, I did everything in the Google collaboratori)</p>
<pre><code>from collections import Counter

Counter(&quot; &quot;.join(test_data['body']).split()).most_common(100)

DATA= Counter(&quot; &quot;.join(test_data['body']).split()).most_common(100)

DATA
</code></pre>
<p>Question:<br />
how to save the result from these top 100 words to a text file .csv or .txt, and possibly an Excel version.(or in three versions at once)</p>
<p>I'm just learning and don't know a lot, trying to figure it out and understand.</p>
<p>Here is a link to the collab, for me the problem is that the words are Russian, and all the practices are for English texts, and it's easier than working with Russian text.
<a href=""https://colab.research.google.com/drive/1LZ3RHPTjTib8lUjzKGcCJgzYnODSjewL?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1LZ3RHPTjTib8lUjzKGcCJgzYnODSjewL?usp=sharing</a></p>
",Dataset Preprocessing & Handling,wa looking unload obtained data txt csv file wa looking unload obtained data txt csv file could find simple understandable solution level understanding process need sort word frequency highlight top word know best way everything google collaboratori question save result top word text file csv txt possibly excel version three version learning know lot trying figure understand link collab problem word russian practice english text easier working russian text
Assigning True/False if a token is present in a data-frame,"<p>My current data-frame is:</p>
<pre><code>     |articleID | keywords                                               | 
     |:-------- |:------------------------------------------------------:| 
0    |58b61d1d  | ['Second Avenue (Manhattan, NY)']                      |     
1    |58b6393b  | ['Crossword Puzzles']                                  |          
2    |58b6556e  | ['Workplace Hazards and Violations', 'Trump, Donald J']|            
3    |58b657fa  | ['Trump, Donald J', 'Speeches and Statements'].        |  
</code></pre>
<p>I want a data-frame similar to the following, where a column is added based on whether a Trump token, 'Trump, Donald J' is mentioned in the keywords and if so then it is assigned True :</p>
<pre><code>     |articleID | keywords                                               | trumpMention |
     |:-------- |:------------------------------------------------------:| ------------:|
0    |58b61d1d  | ['Second Avenue (Manhattan, NY)']                      | False        |      
1    |58b6393b  | ['Crossword Puzzles']                                  | False        |          
2    |58b6556e  | ['Workplace Hazards and Violations', 'Trump, Donald J']| True         |           
3    |58b657fa  | ['Trump, Donald J', 'Speeches and Statements'].        | True         |       
</code></pre>
<p>I have tried multiple ways using df functions. But cannot achieve my wanted results. Some of the ways I've tried are:</p>
<pre><code>df['trumpMention'] = np.where(any(df['keywords']) == 'Trump, Donald J', True, False) 
</code></pre>
<p>or</p>
<pre><code>df['trumpMention'] = df['keywords'].apply(lambda x: any(token == 'Trump, Donald J') for token in x) 
</code></pre>
<p>or</p>
<pre><code>lst = ['Trump, Donald J']  
df['trumpMention'] = df['keywords'].apply(lambda x: ([ True for token in x if any(token in lst)]))   
</code></pre>
<p>Raw input:</p>
<pre><code>df = pd.DataFrame({'articleID': ['58b61d1d', '58b6393b', '58b6556e', '58b657fa'],
                   'keywords': [['Second Avenue (Manhattan, NY)'],
                                ['Crossword Puzzles'],
                                ['Workplace Hazards and Violations', 'Trump, Donald J'],
                                ['Trump, Donald J', 'Speeches and Statements']],
                   'trumpMention': [False, False, True, True]})
</code></pre>
",Dataset Preprocessing & Handling,assigning true false token present data frame current data frame want data frame similar following column added based whether trump token trump donald j mentioned keywords assigned true tried multiple way using df function achieve wanted result way tried raw input
Weights of pre-trained BERT model not initialized,"<p>I am using the <a href=""https://github.com/pair-code/lit"" rel=""nofollow noreferrer"">Language Interpretability Toolkit</a> (LIT) to load and analyze a BERT model that I pre-trained on an NER task.</p>
<p>However, when I'm starting the LIT script with the path to my pre-trained model passed to it, it fails to initialize the weights and tells me:</p>
<pre><code>    modeling_utils.py:648] loading weights file bert_remote/examples/token-classification/Data/Models/results_21_03_04_cleaned_annotations/04.03._8_16_5e-5_cleaned_annotations/04-03-2021 (15.22.23)/pytorch_model.bin
    modeling_utils.py:739] Weights of BertForTokenClassification not initialized from pretrained model: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']
    modeling_utils.py:745] Weights from pretrained model not used in BertForTokenClassification: ['bert.embeddings.position_ids']

</code></pre>
<p>It then simply uses the <code>bert-base-german-cased</code> version of BERT, which of course doesn't have my custom labels and thus fails to predict anything. I think it might have to do with PyTorch, but I can't find the error.</p>
<p>If relevant, here is how I load my dataset into CoNLL 2003 format (modification of the dataloader scripts found <a href=""https://github.com/PAIR-code/lit/tree/main/lit_nlp/examples/datasets"" rel=""nofollow noreferrer"">here</a>):</p>
<pre><code>    def __init__(self):

        # Read ConLL Test Files

        self._examples = []

        data_path = &quot;lit_remote/lit_nlp/examples/datasets/NER_Data&quot;
        with open(os.path.join(data_path, &quot;test.txt&quot;), &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
            lines = f.readlines()

        for line in lines[:2000]:
            if line != &quot;\n&quot;:
                token, label = line.split(&quot; &quot;)
                self._examples.append({
                    'token': token,
                    'label': label,
                })
            else:
                self._examples.append({
                    'token': &quot;\n&quot;,
                    'label': &quot;O&quot;
                })

    def spec(self):
        return {
            'token': lit_types.Tokens(),
            'label': lit_types.SequenceTags(align=&quot;token&quot;),
        }
</code></pre>
<p>And this is how I initialize the model and start the LIT server (modification of the <code>simple_pytorch_demo.py</code> script found <a href=""https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/simple_pytorch_demo.py"" rel=""nofollow noreferrer"">here</a>):</p>
<pre><code>    def __init__(self, model_name_or_path):
        self.tokenizer = transformers.AutoTokenizer.from_pretrained(
            model_name_or_path)
        model_config = transformers.AutoConfig.from_pretrained(
            model_name_or_path,
            num_labels=15,  # FIXME CHANGE
            output_hidden_states=True,
            output_attentions=True,
        )
        # This is a just a regular PyTorch model.
        self.model = _from_pretrained(
            transformers.AutoModelForTokenClassification,
            model_name_or_path,
            config=model_config)
        self.model.eval()

## Some omitted snippets here

    def input_spec(self) -&gt; lit_types.Spec:
        return {
            &quot;token&quot;: lit_types.Tokens(),
            &quot;label&quot;: lit_types.SequenceTags(align=&quot;token&quot;)
        }

    def output_spec(self) -&gt; lit_types.Spec:
        return {
            &quot;tokens&quot;: lit_types.Tokens(),
            &quot;probas&quot;: lit_types.MulticlassPreds(parent=&quot;label&quot;, vocab=self.LABELS),
            &quot;cls_emb&quot;: lit_types.Embeddings()
</code></pre>
",Dataset Preprocessing & Handling,weight pre trained bert model initialized using language interpretability toolkit lit load analyze bert model pre trained ner task however starting lit script path pre trained model passed fails initialize weight tell simply us version bert course custom label thus fails predict anything think might pytorch find error relevant load dataset conll format modification dataloader script found initialize model start lit server modification script found
Fundamental problem - Word frequencies/histogram in R,"<p>I've spent an entire day on this and I cannot seem to find my mistake. I'm sure someone else will be able to spot it in a second (for which I am thankful).</p>
<p>I have a few text files that I need to analyze; it doesn't matter what's in them (use lorem ipsum for a reproducible example). I am at the EDA step. My code looks like this:</p>
<pre><code>lorem &lt;- readLines(&quot;lorem.txt&quot;, skipNul = TRUE) # Read in data; this works

# stringi functions
library(stringi)

# Count words in each entry; this also works fine
lorem_wc &lt;- data.frame(stri_count_words(lorem))
</code></pre>
<p>This produces a data frame with one column with the number of words in each line - so far, this works great. Now I'd like to create a histogram (i.e., see a distribution of word-count frequencies); the number of words would be on the horizontal axis and the frequency of each would be on the vertical. My thought was this:</p>
<pre><code>lorem_df &lt;- table(lorem_wc)
</code></pre>
<p>This produces something I don't want (I'm not actually sure what this is, but it's clearly not what I want - it's a single row of information, but I don't understand what R computed. If I try this:</p>
<pre><code>lorem_df &lt;- data.frame(table(stri_count_words(lorem)))
</code></pre>
<p>I get the same information, but it's now in a column instead of a row. The contents are still not what I'd expect. So I created a test file and did the same thing:</p>
<pre><code>testfile &lt;- readLines(&quot;test.txt&quot;, skipNul = TRUE)
testtab &lt;- data.frame(table(stri_count_words(testfile)))
</code></pre>
<p>When I print <code>testtab</code>, it looks like a two-column data frame; the first column is called <code>Var1</code> and looks like it's the number of words per line in the text file called &quot;test.txt&quot;. The second column is called <code>Freq</code> and looks like it's the frequency that goes with the number of words in the first column.</p>
<p>Now I use ggplot to generate a histogram:</p>
<pre><code>g &lt;- ggplot(testtab, aes(x = Var1))
g &lt;- g + geom_histogram()
g
</code></pre>
<p>... which throws an error: <code>Error: StatBin requires a continuous x variable: the x variable is discrete. Perhaps you want stat=&quot;count&quot;?</code></p>
<p>I'm sure this is trivial, but I've apparently gone into vapor lock.</p>
<p>Thanks in advance.</p>
",Dataset Preprocessing & Handling,fundamental problem word frequency histogram r spent entire day seem find mistake sure someone else able spot second thankful text file need analyze matter use lorem ipsum reproducible example eda step code look like produce data frame one column number word line far work great like create histogram e see distribution word count frequency number word would horizontal axis frequency would vertical thought wa produce something want actually sure clearly want single row information understand r computed try get information column instead row content still expect created test file thing print look like two column data frame first column called look like number word per line text file called test txt second column called look like frequency go number word first column use ggplot generate histogram throw error sure trivial apparently gone vapor lock thanks advance
NLP algorithm and data analysis,"<p>I am very new to analyzing text data and extracting information out of it. I need some suggestion help from the communnity. The dataset is from the job website where 'id' denotes the job id, 'abstract' denotes title, 'content' denotes the body and so on.</p>
<p>This data have the columns</p>
<pre><code>Index(['id', 'title', 'abstract', 'content', 'metadata'], dtype='object')
</code></pre>
<p>Head of the data looks like the following. (1)</p>
<pre><code>    id  title   abstract    content     metadata    clean_content
0   38915469    Recruitment Consultant  We are looking for someone to focus purely on ...   &lt;HTML&gt;&lt;p&gt;Are you looking to join a thriving bu...   {'standout': {'bullet1': 'Join a Sector that i...   Are you looking to join a thriving business th...
1   38934839    Computers Salesperson - Coburg  Passionate about exceptional customer service?...   &lt;HTML&gt;&lt;p&gt;&amp;middot;&amp;nbsp;&amp;nbsp;Casual hours as r...   {'additionalSalaryText': 'Attractive Commissio...   middotnbspnbspCasual hours as required transit...
2   38946054    Senior Developer | SA   Readifarians are known for discovering the lat...   &lt;HTML&gt;&lt;p&gt;Readify helps organizations innovate ...   {'standout': {'bullet1': 'Design, develop, tes...   Readify helps organizations innovate with tech...
3   38833950    Senior Commercial Property Manager | Leading T...   ~ Rare opportunity for a Senior PM to step int...   &lt;HTML&gt;&lt;p&gt;&lt;strong&gt;WayPoint Recruitment&amp;nbsp;&lt;/s...   {'additionalSalaryText': '$140k + Car Park - C...   WayPoint Recruitmentnbsphave partnered up with...
4   38856271    Technology Manager | Travel Industry    Rare opportunity for an experienced Technology...   &lt;HTML&gt;This is a key role within a market leadi...   {'standout': {'bullet1': 'Lead overarching str...   This is a key role within a market leading wi...
</code></pre>
<p>The second set of data related to the above is given as follows, where it has job id, , job viewing platform and I feel most important column 'kind' has two labels 'A' for applied and 'V' for view.</p>
<p>(2)</p>
<pre><code>    event_datetime  resume_id   job_id  event_platform  kind
0   2021-05-01T08:42:05     158655  38820886    IOS_APP     V
1   2021-05-01T08:42:05     158655  38901970    IOS_APP     V
2   2021-05-01T08:42:05     158655  38919645    IOS_APP     A
3   2021-05-01T08:42:05     158655  38928402    IOS_APP     V
4   2021-05-01T08:42:05     158655  38847632    IOS_APP     A
</code></pre>
<p>The first thing I did groupby job_id and kind</p>
<pre><code>ndf = df.groupby([&quot;job_id&quot;,&quot;kind&quot;]).count()['resume_id'].reset_index().pivot(index='job_id', columns = 'kind')
(3)

</code></pre>
<p>The above code returns a data frame that is indexed by job_id and two columns one representing A and the other V.</p>
<p>There are more information about the job in the 'metadata' columns like 'location' etc, I could not use it as I don't now how to extract. I have only use 'content' column to make the prediction.</p>
<p>I have done some textual analysis using nltk, my the main aim is to give a model to predict given a job if it's applied to the view. Is there exist some similar notebook I can follow or some suggestions.</p>
<p>What other interesting facts I can get out of the data? Or any other model I can predict out of the data? I never use the event platform data that is the data mentioning job applicant using IOS_APP or Anroid_APP or web. Any discussion is very welcome.</p>
",Dataset Preprocessing & Handling,nlp algorithm data analysis new analyzing text data extracting information need suggestion help communnity dataset job website id denotes job id abstract denotes title content denotes body data column head data look like following second set data related given follows ha job id job viewing platform feel important column kind ha two label applied v view first thing groupby job id kind code return data frame indexed job id two column one representing v information job metadata column like location etc could use extract use content column make prediction done textual analysis using nltk main aim give model predict given job applied view exist similar notebook follow suggestion interesting fact get data model predict data never use event platform data data mentioning job applicant using io app anroid app web discussion welcome
run my python script easily on any other laptop,"<p>I have written a script (around 2k lines) for processing text.</p>
<p>It reads the input form my text file, and print the output in another file.</p>
<p>But, I want <strong>it can be run on any other laptop</strong> (with Python installed) <strong>easily</strong> as well. For example,
other people can run it without installing additional libraries (that I had imported in the script).</p>
<p>How can I realize my purpose? By packaging my script in a library or what else I can do? Please provide any hint.</p>
<p>I tried to use the <em>pyinstaller</em> or the <em>py2exe</em>, but I always have a problem of over recursion limit,
and since I have several huge sized libraries being imported, so I guess even I can finally make a .exe file,
it would be in a huge size, so I stopped to using that way. Anyone has a comment on it?</p>
",Dataset Preprocessing & Handling,run python script easily laptop written script around k line processing text read input form text file print output another file want run laptop python installed easily well example people run without installing additional library imported script realize purpose packaging script library else please provide hint tried use pyinstaller py exe always problem recursion limit since several huge sized library imported guess even finally make exe file would huge size stopped using way anyone ha comment
table.remove removes certain elements but not all,"<p>I'm trying to remove elements from a list (list is stored in a pandas dataframe) with <code>.remove()</code>. The base idea is that i iterate through all the rows in the dataframe then every element in the row (=list), and check whether that particular element is a keeper or a &quot;goner&quot;</p>
<pre><code>data=dict()
data=pd.read_csv('raw_output_v2.csv', names=['ID','Body'])
data['Body']=data['Body'].apply(eval)  
keyword_dict={}
for row in tqdm(data['Body'], desc=&quot;building dict&quot;):
    for word in row:
        if word in keyword_dict:
            keyword_dict[word]+=1
        else:
            keyword_dict[word]=1 

new_df=remove_sparse_words_from_df(data, keyword_dict, cutoff=1_000_000)
</code></pre>
<p>And here is the important stuff:</p>
<pre><code>def remove_sparse_words_from_df(df, term_freq, cutoff=1):
    i=0
    for row in tqdm(df['Body'],desc=&quot;cleaning df&quot;):
        for word in row:
            if term_freq[word]&lt;=cutoff:
                row.remove(word)
            else:
                continue
    return df
</code></pre>
<p>I've uploaded a short example csv to be used here: <a href=""https://pastebin.com/g25bHCC7"" rel=""nofollow noreferrer"">https://pastebin.com/g25bHCC7</a>.</p>
<p>My problem is: the <code>remove_sparse_words_from_df</code> function removes some occurances of the words that fall below cutoff, but not all. Example: the word &quot;clean&quot; occurs ~10k in the original dataframe (data), after running <code>remove_sparse_words_from_df</code> about 2k still remains. Same with other words.</p>
<p>What am I missing?</p>
",Dataset Preprocessing & Handling,table remove remove certain element trying remove element list list stored panda dataframe base idea iterate row dataframe every element row list check whether particular element keeper goner important stuff uploaded short example csv used problem function remove occurances word fall cutoff example word clean occurs k original dataframe data running k still remains word missing
join rows with same index and keep other rows unchanged,"<p>I have this data frame</p>
<pre><code>df=

ID    join        Chapter  ParaIndex      text 
 0     NaN         1         0            I am test 
 1     NaN         2         1            it is easy 
 2     1           3         2            but not so
 3     1           3         3            much easy

</code></pre>
<p>I want to get this<br />
(merge the column &quot;text&quot; with the same index in column &quot;join&quot; and reindex &quot;ID&quot; and &quot;ParaIndex&quot;, rest without change)</p>
<pre><code>dfEdited=

ID    join        Chapter  ParaIndex      text 
 0     NaN         1         0            I am test 
 1     NaN         2         1            it is easy 
 2     1           3         2            but not so much easy
</code></pre>
<p>I used this command</p>
<pre><code>dfedited=df.groupby(['join'])['text'].apply(lambda x: ' '.join(x.astype(str))).reset_index()
</code></pre>
<p>it only merges the row with the numerical index in column join and exclude row with non index</p>
<p>so I changed to this</p>
<pre><code>dfedited=df.groupby(['join'],dropna=False)['text'].apply(lambda x: ' '.join(x.astype(str))).reset_index()
</code></pre>
<p>here it merges all rows based on index join but it considers row with index NaN as one group therefore join them also to be group! however, I do not want to join them   ...any idea? many thanks</p>
<p>I also used this</p>
<pre><code>dfedited=df.groupby(['join', &quot;ParaIndex&quot;, &quot;Chapter&quot;],dropna=False  )['text'].apply(lambda x: ' '.join(x.astype(str) )).reset_index()
</code></pre>
<p>it looks better as it has all columns, but no changes!!</p>
",Dataset Preprocessing & Handling,join row index keep row unchanged data frame want get merge column text index column join reindex id paraindex rest without change used command merges row numerical index column join exclude row non index changed merges row based index join considers row index nan one group therefore join also group however want join idea many thanks also used look better ha column change
NLP model - Arabic diacritized_text [Errno 22] Invalid argument,"<p>i am trying to read  diacritized_text  from pdf file by using this code:</p>
<pre><code>import PyPDF2 
import pdfplumber.utils
import pdfminer.pdftypes
import arabic_reshaper
from pdfplumber.pdf import PDF
from bidi.algorithm import get_display
from PyPDF2 import PdfFileReader, PdfFileWriter
#import pyPdf
import codecs
input_filepath = &quot;D:\Arabic research\input.pdf&quot; file path
output_filepath = &quot;D:\Arabic research\output.txt&quot;#output text file path
output_file = open(r&quot;D:\Arabic research\output.txt&quot;, &quot;wb&quot;)#open output file
pdf = PyPDF2.PdfFileReader(codecs.open(r&quot;D:\Arabic research\input.pdf&quot;, &quot;rb&quot;, encoding='utf-8'))#read PDF
for page in PyPDF2.pages:#loop through pages
    page_text = page.extractText()#get text from page
    page_text = page_text.decode(encoding='utf-8')#decode 
    print(page_text)
    output_file.write(page_text)#write to file
output_file.close()#close
</code></pre>
<p>but i have the following error:[Errno 22] Invalid argument</p>
",Dataset Preprocessing & Handling,nlp model arabic diacritized text errno invalid argument trying read diacritized text pdf file using code following error errno invalid argument
Clustering a feature matrix using sklearn (Python),"<p>I have a dataframe of size 9x100 with tf-idf scores of 100 words that exist in documents 0 to 8, the dataframe can be seen here:</p>
<p><a href=""https://i.sstatic.net/XN860.png"" rel=""nofollow noreferrer"">My Dataframe</a></p>
<p>I then convert this dataframe to a matrix X using:
<code>X= df.values</code></p>
<p>I am trying to cluster these 100 words into 50 clusters (where no empty cluster is allowed) using the <code>sklearn.cluster.AgglomerativeClustering</code> package but I'm really not too sure how to implement this method on my sample set as I still want to keep the word labels from the dataframe in the data output, could any help out?</p>
",Dataset Preprocessing & Handling,clustering feature matrix using sklearn python dataframe size x tf idf score word exist document dataframe seen dataframe convert dataframe matrix x using trying cluster word cluster empty cluster allowed using package really sure implement method sample set still want keep word label dataframe data output could help
Creating a TF IDF matrix in Python,"<p>I have a list of lists in the form:</p>
<pre><code>[['alice', 'in', 'wonderland',....], ['the', 'final', 'showdown',....],.............]
</code></pre>
<p>Where each element corresponds to the word tokens of a specific document (that I have processed)</p>
<p>I want to create a term frequency and inverse document frequency matrix, but I'm not sure how to go about doing this.</p>
<p>I'm thinking of using a pandas dataframe to store the data for this but not really sure how to iterate over it to get the TF and IDF (I know nltk might have some tools)</p>
<p>Any help would be appreciated!</p>
",Dataset Preprocessing & Handling,creating tf idf matrix python list list form element corresponds word token specific document processed want create term frequency inverse document frequency matrix sure go thinking using panda dataframe store data really sure iterate get tf idf know nltk might tool help would appreciated
My training data contains line breaks; how can I work with Gensim&#39;s LineSentence format for the corpus_file parameter?,"<p>Per Gensim's <a href=""https://radimrehurek.com/gensim_3.8.3/models/doc2vec.html"" rel=""nofollow noreferrer"">documentation</a>, <a href=""https://github.com/RaRe-Technologies/gensim/blob/7e4965ee6c9d4e200dae6fb089b46c2ebc27e159/CHANGELOG.md#star2-new-features"" rel=""nofollow noreferrer"">changelog</a>, and <a href=""https://stackoverflow.com/a/57532194/4189676"">previous StackOverflow answers</a>, I know that passing training data in the <a href=""https://radimrehurek.com/gensim_3.8.3/models/word2vec.html#gensim.models.word2vec.LineSentence"" rel=""nofollow noreferrer"">LineSentence</a> format to the <code>corpus_data</code> parameter can dramatically speed up Any2Vec training.</p>
<p>Documentation on the <a href=""https://radimrehurek.com/gensim_3.8.3/models/word2vec.html#gensim.models.word2vec.LineSentence"" rel=""nofollow noreferrer"">LineSentence</a> format reads as follows:</p>
<blockquote>
<p>Iterate over a file that contains sentences: one line = one sentence. Words must be already preprocessed and separated by whitespace.</p>
</blockquote>
<p>My training data is comprised of tens of millions (and potentially 1xx million) of sentences extracted from plaintext files using spaCy. A sample sentence quite often contains one or more line break characters (<code>\n</code>).</p>
<p>How can I make these samples compatible with the <a href=""https://radimrehurek.com/gensim_3.8.3/models/word2vec.html#gensim.models.word2vec.LineSentence"" rel=""nofollow noreferrer"">LineSentence</a> format? As far as I understand, these samples should be &quot;understood&quot; in the context of their linebreaks, as these breaks are present in the target text (data not trained upon). That means I can't just remove them from the training data.</p>
<p>Do I escape the newline characters with <code>\\n</code>? Is there a way to pass a custom delimiter?</p>
<p>I appreciate any guidance. Thanks in advance.</p>
",Dataset Preprocessing & Handling,training data contains line break work gensim linesentence format corpus file parameter per gensim documentation changelog linesentence format parameter dramatically speed vec training documentation linesentence format read follows iterate file contains sentence one line one sentence word must already preprocessed separated whitespace training data comprised ten million potentially xx million sentence extracted plaintext file using spacy sample sentence quite often contains one line break character make sample compatible linesentence format far understand sample understood context linebreaks break present target text data trained upon mean remove training data escape newline character way pas custom delimiter appreciate guidance thanks advance
How to load a spark-nlp pre-trained model from disk,"<p>From the <code>spark-nlp</code> Github <a href=""https://github.com/JohnSnowLabs/spark-nlp#downloading-models-for-offline-use"" rel=""nofollow noreferrer"">page</a> I downloaded a <code>.zip</code> file containing a pre-trained NerCRFModel. The zip contains three folders: embeddings, fields, and metadata.</p>
<p>How do I load that into a Scala <code>NerCrfModel</code> so that I can use it? Do I have to drop it into HDFS or the host where I launch my Spark Shell? How do I reference it?</p>
",Dataset Preprocessing & Handling,load spark nlp pre trained model disk github page downloaded file containing pre trained nercrfmodel zip contains three folder embeddings field metadata load scala use drop hdfs host launch spark shell reference
I have R code to extract information from one document. How do I loop that for all the documents in my folder?,"<p>I have a folder of txt files, and I want to extract specific texts from them and arrange them separate columns into a new data frame. I did the code for one file, but I can't seem to edit it into a loop that will run across all the documents in my folder.</p>
<p>This is my code for the one txt file:</p>
<pre><code>    clean_text &lt;- as.data.frame(strsplit(text$text, '\\*' ), col.names = &quot;text&quot;) %&gt;% 
mutate(text = str_replace_all(text, &quot;\n&quot;, &quot; &quot;),
         text = str_replace_all(text, &quot;- &quot;, &quot;&quot;), 
         text = str_replace_all(text,&quot;^\\s&quot;, &quot;&quot;)) %&gt;% 
  
  filter(!text == &quot; &quot;) %&gt;% 
  
  mutate(paragraphs = ifelse(grepl(&quot;^[[:digit:]]&quot;, text) == T, text, NA)) %&gt;% 
  
  rename(category = text) %&gt;% 
  mutate(category = ifelse(grepl(&quot;^[[:digit:]]&quot;, category) == T, NA, category)) %&gt;% 
  fill(category) %&gt;% 
  filter(!is.na(paragraphs)) %&gt;% 
  
  mutate(paragraphs = strsplit(paragraphs, '^[[:digit:]]{1,3}\\.|\\t\\s[[:digit:]]{1,3}\\.')) %&gt;% 
  unnest(paragraphs) %&gt;% 
  mutate(paragraphs = strsplit(paragraphs, 'Download as PDF')) %&gt;%
  unnest(paragraphs) %&gt;% 
  mutate(paragraphs = str_replace_all(paragraphs, &quot;\t&quot;, &quot;&quot;)) %&gt;% 
  mutate(paragraphs = ifelse(grepl(&quot;javascript&quot;, paragraphs), &quot;&quot;, paragraphs)) %&gt;%
  mutate(paragraphs = str_replace_all(paragraphs, &quot;^\\s+&quot;, &quot;&quot;)) %&gt;%
  filter(!paragraphs == &quot;&quot;) 
</code></pre>
<p>How do I make this into a loop? I realise there are similar questions, but none of the solutions have worked for me. Thanks in advance for the help!</p>
",Dataset Preprocessing & Handling,r code extract information one document loop document folder folder txt file want extract specific text arrange separate column new data frame code one file seem edit loop run across document folder code one txt file make loop realise similar question none solution worked thanks advance help
Tables not detected with tabula and camelot,"<p>I tried to extract tables from PDFs that are not in proper format that I think. The tables in these PDFs have a table format but not enclosed properly with verical borders.<a href=""https://i.sstatic.net/A3KrZ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/A3KrZ.jpg"" alt=""enter image description here"" /></a> I'll attach the sample pdf and output with both libraries. When I tried to use tabula for table detection, a blank datadrame is returned on all the pages in pdf.</p>
<blockquote>
<p>enter 0 for single pages, 1 for all, 2 for specific page: 2
enter page number: 25
no tables found on this page by tabula.</p>
</blockquote>
<p>And when I use camelot there is same no response when i use <code>flovor='lattice'</code></p>
<blockquote>
<p>enter 0 for single pages, 1 for all pages, 2 for pages in tables are detected by tabula, 3 for specific pages: 3
enter 0 for lattice or 1 for stream: 0
enter page number: 25
no tables found on this page by camelot.</p>
</blockquote>
<p>and when I use <code>flovor='stream'</code>, I get a dataframe that has each line read line by line with tab separated data, but it will include normal text as well in that dataframe.</p>
<blockquote>
<p>enter 0 for single pages, 1 for all pages, 2 for pages in tables are detected by tabula, 3 for specific pages: 3
enter 0 for lattice or 1 for stream: 1
enter page number: 25
<a href=""https://i.sstatic.net/L1GMN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/L1GMN.png"" alt=""enter image description here"" /></a></p>
</blockquote>
<p>I just need an efficient way to detect table and extract the same data if vertical enclosing table lines are not present. Both tabula and camelot libraries are working fine if table is in proper format enclosed by vertical and horizontal lines.</p>
",Dataset Preprocessing & Handling,table detected tabula camelot tried extract table pdfs proper format think table pdfs table format enclosed properly verical border attach sample pdf output library tried use tabula table detection blank datadrame returned page pdf enter single page specific page enter page number table found page tabula use camelot response use enter single page page page table detected tabula specific page enter lattice stream enter page number table found page camelot use get dataframe ha line read line line tab separated data include normal text well dataframe enter single page page page table detected tabula specific page enter lattice stream enter page number need efficient way detect table extract data vertical enclosing table line present tabula camelot library working fine table proper format enclosed vertical horizontal line
Python | Jupyter Notebook - NLTK function slow when checking for words,"<p>I'm trying to clean up my dataset by using NLTK, but I'm having some trouble, as it is taking ages to complete. I do have a very large dataset of over 20000+ rows of text.</p>
<p>The code that I'm running looks like this:</p>
<pre><code>from nltk.corpus import words
nltk.download('words')
gibberishBody = []

for x in bodyStopWords:
    if x in words.words():
        gibberishBody.append(x)
print(gibberishBody)
</code></pre>
<p>bodyStopWords is pandas.core.series.Series datatype.</p>
<p>Does anyone have any suggestions to optimize the script for speed?</p>
",Dataset Preprocessing & Handling,python jupyter notebook nltk function slow checking word trying clean dataset using nltk trouble taking age complete large dataset row text code running look like bodystopwords panda core series series datatype doe anyone suggestion optimize script speed
Checking the similarities between 2 texts and in a dataframe of 100 rows (thus more than 2 sentences),"<p>I have a dataframe of 100 rows and i would like to compare the similarities between the column OriginalTxt and the SummarizedTxt</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>OriginalTxt</th>
<th>SummarizedTxt</th>
</tr>
</thead>
<tbody>
<tr>
<td>Due to the horizontal pusher system, the highest possible operational security is guaranteed. So that the load can be pushed off with the lowest possible friction values, the agroliner pusher trailer has a stainless steel base as standard and the pusher plate is coated with VA sheet steel plates. The side panels are designed to be torsion-resistant so that the seal, particularly with bulk cargo such as rapeseed and grain, is guaranteed to be long-lasting. The standard large volume tailgate effectively increases the loading volume.</td>
<td>The agroliner pusher trailer has a stainless steel base as standard and the pusher plate is coated with VA sheet steel plates . The side panels are designed to be torsion-resistant so that the seal, particularly with bulk cargo such as rapeseed and grain, is guaranteed to be long-lasting .</td>
</tr>
<tr>
<td>Calf-Tel Pro hutches are designed to allow calves to seek their ideal level of comfort in any of the various micro environments. The hutch has the same size and design as the DELUXE but has a fully opened entrance. Calf-Tec offers the lowest depreciation, longest life.</td>
<td>With calves isolated from each other, there is little or no transfer of disease between calves . Indestructable High Density Polyethylene our hutches are easy to sanitise and move . This results in a system that will not transfer disease from one group to the next .</td>
</tr>
</tbody>
</table>
</div>
<p>So i tried to use different librairies like sentence-similarity 1.0.0 or the Levenhstein one but it works for 2 sentences at the time but not as a function for the whole dataframe :</p>
<p><code> compare('Head Wrap The Head Wrap is innovative in its ability to provide uninterrupted therapy throughout a variety of therapies. It offers an innovative, non-invasive method of controlling core body temperature. Plus, it is a great alternative to ice or ice packs since its versatile design provides therapy in a wide variety of clinical situations. Benefits Unique design allows for easy access to the head and neck area Head cooling is a useful adjunct treatment to whole body hypothermia in regulating temperature Helps maintain normothermia Features One size fits most Adjustable Velcro® Straps Soft material for patient comfort Specifications Model No. - Sizes: 600 - One size fits most','The Head Wrap is an innovative non-invasive method of controlling core body temperature. Unique design allows for easy access to the head and neck area. Helps maintain normothermia in regulating temperature and is a useful adjunct to whole body hypothermia treatment.')</code></p>
<p>It works when comparing one sentence to another <code> 0.6137419245698322</code></p>
<p>I tried this: <code> compare(df.IDPTexteEn, df.Résumé)</code></p>
<p>and this:</p>
<pre><code>def compare_row_wise(row):
    return compare(row['summaries'], row['abstracts'])

df.apply(compare_row_wise, axis=1)
</code></pre>
<p>but it doesn't works and gives me this :</p>
<p><code> compare() missing 1 required positional argument: 'string_j'</code></p>
<p>Thank you in advance if you guys have any idea to help ^^&quot;</p>
",Dataset Preprocessing & Handling,checking similarity text dataframe row thus sentence dataframe row would like compare similarity column originaltxt summarizedtxt originaltxt summarizedtxt due horizontal pusher system highest possible operational security guaranteed load pushed lowest possible friction value agroliner pusher trailer ha stainless steel base standard pusher plate coated va sheet steel plate side panel designed torsion resistant seal particularly bulk cargo rapeseed grain guaranteed long lasting standard large volume tailgate effectively increase loading volume agroliner pusher trailer ha stainless steel base standard pusher plate coated va sheet steel plate side panel designed torsion resistant seal particularly bulk cargo rapeseed grain guaranteed long lasting calf tel pro hutch designed allow calf seek ideal level various micro environment hutch ha size design deluxe ha fully opened entrance calf tec offer lowest depreciation longest life calf isolated little transfer disease calf indestructable high density polyethylene hutch easy sanitise move result system transfer disease one group next tried use different librairies like sentence similarity levenhstein one work sentence time function whole dataframe work comparing one sentence another tried work give thank advance guy idea help
The label tag mixed with comment in sentiment analysis data frame,"<p>I have below data frame and my label column as u can see is part of sentence row and its separated by \ character. My question is how can I delete these zero and ones or replace them with &quot; &quot; character and transition them to new Label column beside this comment column?</p>
<p>thanks for you're help.</p>
<p><a href=""https://i.sstatic.net/t4hnW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/t4hnW.png"" alt=""enter image description here"" /></a></p>
",Dataset Preprocessing & Handling,label tag mixed comment sentiment analysis data frame data frame label column u see part sentence row separated character question delete zero one replace character transition new label column beside comment column thanks help
Matrix of distances between words in R,"<p>I need a connection data frame with intensity for connection using words. The data looks like this:</p>
<pre><code>df1 &lt;- c(&quot;test&quot;, &quot;example&quot;, &quot;random word&quot;, &quot;another&quot;)
df2 &lt;- c(&quot;word2&quot;, &quot;word3&quot;, &quot;test&quot;)
df3 &lt;- c(&quot;word2&quot;, &quot;test&quot;, &quot;question&quot;, &quot;stack&quot;, &quot;overflow&quot;)
df4 &lt;- c(&quot;word2&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;vector&quot;)
</code></pre>
<p>Ideally, I should get something like this:</p>
<pre><code>links &lt;- data.frame(
  source=c(&quot;df1&quot;,&quot;df2&quot;, &quot;df3&quot;, &quot;df4&quot;), 
  target=c(&quot;df1&quot;,&quot;df2&quot;, &quot;df3&quot;, &quot;df4&quot;), 
  value=c(1,2, 2, 1)
  )
</code></pre>
<p>The idea is to create a sankey diagram as explained here (<a href=""https://www.r-graph-gallery.com/321-introduction-to-interactive-sankey-diagram-2.html"" rel=""nofollow noreferrer"">https://www.r-graph-gallery.com/321-introduction-to-interactive-sankey-diagram-2.html</a>) based on the similarity between datasets. However, I do not figure out:</p>
<p>(1) How to calculate the similarity between word vectors across several datasets
(2) How to create a distance matrix based on this similarity with the result of each dataset-pair distance</p>
<p>The problem is not so much about how to calculate distances, but about how to do it among different datasets (the example only has 4, but I have more than 70) and store the results in a single matrix.</p>
",Dataset Preprocessing & Handling,matrix distance word r need connection data frame intensity connection using word data look like ideally get something like idea create sankey diagram explained based similarity datasets however figure calculate similarity word vector across several datasets create distance matrix based similarity result dataset pair distance problem much calculate distance among different datasets example ha store result single matrix
Fuzzy String Matching in Python using weightings,"<p>I have Salesforce Records that I want to dedupe using fuzzy string matching techniques with weighting across different fields.</p>
<p>I want to set up scenarios such as weightings on specific columns in the row that increase or decrease the overall similarity metric. Essentially changing the weighting allow me to prioritize my columns at different levels.</p>
<p>I describe scenarios as a set of rules for how I want to compare records.</p>
<p>Below is an example data set:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">First</th>
<th style=""text-align: left;"">Last Name</th>
<th>Email</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">Matt</td>
<td style=""text-align: left;"">Metro</td>
<td>name@example.com</td>
</tr>
<tr>
<td style=""text-align: left;"">Alex</td>
<td style=""text-align: left;"">Two</td>
<td>Three</td>
</tr>
<tr>
<td style=""text-align: left;"">Matthew</td>
<td style=""text-align: left;"">Meos</td>
<td>name@example.com</td>
</tr>
</tbody>
</table>
</div>
<p>In this scenario we have 3 features for each row of data.</p>
<p>Each Feature has a weight of 10, this giving me a total score of 30</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Feature</th>
<th style=""text-align: left;"">Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">Fist Name</td>
<td style=""text-align: left;"">10</td>
</tr>
<tr>
<td style=""text-align: left;"">Last Name</td>
<td style=""text-align: left;"">10</td>
</tr>
<tr>
<td style=""text-align: left;"">Email</td>
<td style=""text-align: left;"">10</td>
</tr>
<tr>
<td style=""text-align: left;"">TOTAL</td>
<td style=""text-align: left;"">30</td>
</tr>
</tbody>
</table>
</div>
<p>Thus an exact matching across all three fields would yield a 30 / 30 (i.e. 100% similarity score)</p>
<p>Now lets say I want to the weighting of email to be 3 times more powerful. The model should look like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Feature</th>
<th style=""text-align: left;"">Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">Fist Name</td>
<td style=""text-align: left;"">10</td>
</tr>
<tr>
<td style=""text-align: left;"">Last Name</td>
<td style=""text-align: left;"">10</td>
</tr>
<tr>
<td style=""text-align: left;"">Email</td>
<td style=""text-align: left;"">30</td>
</tr>
<tr>
<td style=""text-align: left;"">TOTAL</td>
<td style=""text-align: left;"">50</td>
</tr>
</tbody>
</table>
</div>
<p>Thus, an exact match with email, would hold significantly more weight in the similarity score.</p>
<p>I am trying to figure out the Python Packages that can help me achieve this dynamic weighting and the algorithm I should use for similarity.</p>
<p>For the Algorithm, I was thinking of using</p>
<ul>
<li>Levenshtein Distance</li>
</ul>
<p>For the Weightings of different fields between records:</p>
<ul>
<li>Python Data Frame</li>
</ul>
<p>What is the most performant way for achieving this this type of fuzzy string matching in Python?</p>
",Dataset Preprocessing & Handling,fuzzy string matching python using weighting salesforce record want dedupe using fuzzy string matching technique weighting across different field want set scenario weighting specific column row increase decrease overall similarity metric essentially changing weighting allow prioritize column different level describe scenario set rule want compare record example data set first last name email matt metro name example com alex two three matthew meos name example com scenario feature row data feature ha weight giving total score feature score fist name last name email total thus exact matching across three field would yield e similarity score let say want weighting email time powerful model look like feature score fist name last name email total thus exact match email would hold significantly weight similarity score trying figure python package help achieve dynamic weighting algorithm use similarity algorithm wa thinking using levenshtein distance weighting different field record python data frame performant way achieving type fuzzy string matching python
Applying function to pandas dataframe: is there a more efficient way of doing this?,"<p>I have a dataframe that has a small number of columns but many rows (about 900K right now, and it's going to get bigger as I collect more data). It looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;""></th>
<th style=""text-align: left;"">Author</th>
<th style=""text-align: left;"">Title</th>
<th style=""text-align: left;"">Date</th>
<th style=""text-align: left;"">Category</th>
<th style=""text-align: left;"">Text</th>
<th style=""text-align: left;"">url</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">0</td>
<td style=""text-align: left;"">Amira Charfeddine</td>
<td style=""text-align: left;"">Wild Fadhila 01</td>
<td style=""text-align: left;"">2019-01-01</td>
<td style=""text-align: left;"">novel</td>
<td style=""text-align: left;"">الكتاب هذا نهديه لكل تونسي حس إلي الكتاب يحكي ...</td>
<td style=""text-align: left;"">NaN</td>
</tr>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">Amira Charfeddine</td>
<td style=""text-align: left;"">Wild Fadhila 02</td>
<td style=""text-align: left;"">2019-01-01</td>
<td style=""text-align: left;"">novel</td>
<td style=""text-align: left;"">في التزغريت، والعياط و الزمامر، ليوم نتيجة الب...</td>
<td style=""text-align: left;"">NaN</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: left;"">253826</td>
<td style=""text-align: left;"">1515368_7636953</td>
<td style=""text-align: left;"">2010-12-28</td>
<td style=""text-align: left;"">/forums/forums/91/</td>
<td style=""text-align: left;"">هذا ما ينص عليه إدوستور التونسي لا رئاسة مدى ا...</td>
<td style=""text-align: left;""><a href=""https://www.tunisia-sat.com/forums/threads/151.."" rel=""nofollow noreferrer"">https://www.tunisia-sat.com/forums/threads/151..</a>.</td>
</tr>
<tr>
<td style=""text-align: left;"">3</td>
<td style=""text-align: left;"">250442</td>
<td style=""text-align: left;"">1504416_7580403</td>
<td style=""text-align: left;"">2010-12-21</td>
<td style=""text-align: left;"">/forums/sports/</td>
<td style=""text-align: left;"">\n\n\n\n\n\nاعلنت الجامعة التونسية لكرة اليد ا...</td>
<td style=""text-align: left;""><a href=""https://www.tunisia-sat.com/forums/threads/150.."" rel=""nofollow noreferrer"">https://www.tunisia-sat.com/forums/threads/150..</a>.</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td style=""text-align: left;"">312628</td>
<td style=""text-align: left;"">1504416_7580433</td>
<td style=""text-align: left;"">2010-12-21</td>
<td style=""text-align: left;"">/forums/sports/</td>
<td style=""text-align: left;"">quel est le résultat final\n,,,,????</td>
<td style=""text-align: left;""><a href=""https://www.tunisia-sat.com/forums/threads/150.."" rel=""nofollow noreferrer"">https://www.tunisia-sat.com/forums/threads/150..</a>.</td>
</tr>
</tbody>
</table>
</div>
<p>The &quot;Text&quot; Column has a string of text that may be just a few words (in the case of a forum post) or it may a portion of a novel and have tens of thousands of words (as in the two first rows above).</p>
<p>I have code that constructs the dataframe from various corpus files (.txt and .json), then cleans the text and saves the cleaned dataframe as a pickle file.</p>
<p>I'm trying to run the following code to analyze how variable the spelling of different words are in the corpus. The functions seem simple enough: One counts the occurrence of a particular spelling variable in each Text row; the other takes a list of such frequencies and computes a Gini Coefficient for each lemma (which is just a numerical measure of how heterogenous the spelling is). It references a spelling_var dictionary that has a lemma as its key and the various ways of spelling that lemma as values. (like {'color': ['color', 'colour']} except not in English.)</p>
<p>This code works, but it uses a lot of CPU time. I'm not sure how much, but I use PythonAnywhere for my coding and this code sends me into the tarpit (in other words, it makes me exceed my daily allowance of CPU seconds).</p>
<p>Is there a way to do this so that it's less CPU intensive? Preferably without me having to learn another package (I've spent the past several weeks learning Pandas and am liking it, and need to just get on with my analysis). Once I have the code and have finished collecting the corpus, I'll only run it a few times; I won't be running it everyday or anything (in case that matters).</p>
<p>Here's the code:</p>
<pre><code>import pickle
import pandas as pd
import re

with open('1_raw_df.pkl', 'rb') as pickle_file:
    df = pickle.load(pickle_file)

spelling_var = {
    'illi': [&quot;الي&quot;, &quot;اللي&quot;],
    'besh': [&quot;باش&quot;, &quot;بش&quot;],
    ...
    }

spelling_df = df.copy()

def count_word(df, word):
    pattern = r&quot;\b&quot; + re.escape(word) + r&quot;\b&quot;
    return df['Text'].str.count(pattern)

def compute_gini(freq_list):
    proportions = [f/sum(freq_list) for f in freq_list]
    squared = [p**2 for p in proportions]
    return 1-sum(squared)

for w, var in spelling_var.items():
    count_list = []
    for v in var:
        count_list.append(count_word(spelling_df, v))
        gini = compute_gini(count_list)
    spelling_df[w] = gini
</code></pre>
",Dataset Preprocessing & Handling,applying function panda dataframe efficient way dataframe ha small number column many row k right going get bigger collect data look like author title date category text url amira charfeddine wild fadhila novel nan amira charfeddine wild fadhila novel nan forum forum forum sport n n n n n n forum sport quel est le r sultat final n text column ha string text may word case forum post may portion novel ten thousand word two first row code construct dataframe various corpus file txt json clean text save cleaned dataframe pickle file trying run following code analyze variable spelling different word corpus function seem simple enough one count occurrence particular spelling variable text row take list frequency computes gini coefficient lemma numerical measure heterogenous spelling reference spelling var dictionary ha lemma key various way spelling lemma value like color color colour except english code work us lot cpu time sure much use pythonanywhere coding code sends tarpit word make exceed daily allowance cpu second way le cpu intensive preferably without learn another package spent past several week learning panda liking need get analysis code finished collecting corpus run time running everyday anything case matter code
Passing Term-Document Matrix to Gensim LDA Model,"<p>My term-document matrix is in a numpy matrix format, and I have a dictionary to represent the  of the term-document matrix.</p>

<p>Is there any way I can easily pass these two into Gensim's LDA model?</p>

<pre><code>tdMatrix = np.load('tdmatrix.npy')
dictionary = cPickle.load(open('dictionary.p', 'r')) # stores term represented by each column
</code></pre>

<p>Can I pass this somewhow to gensim.models.ldamodel.LDA?</p>
",Dataset Preprocessing & Handling,passing term document matrix gensim lda model term document matrix numpy matrix format dictionary represent term document matrix way easily pas two gensim lda model pas somewhow gensim model ldamodel lda
How to check term similarity within a pandas column with similarity.jarowinkler,"<p>I would need to check if two or more words in a list are similar. 
To do this, I am using the Jaro Wrinkler distance as follows:</p>

<pre><code>from similarity.jarowinkler import JaroWinkler

word1='sweet chili'
word2='sriracha chilli'

jarowinkler = JaroWinkler()
print(jarowinkler.similarity(word1, word2))
</code></pre>

<p>It seems to be able to detect the similarity between words, but I would need to set a threshold to select only words that are similar at 80%. 
My difficulties, however, are in checking all the words within a data frame's column: </p>

<pre><code>Words

sweet chili
sriracha chilli
tomato
mayonnaise 
water
milk
still water
sparkling water
wine
chicken 
beef
...
</code></pre>

<p>What I would like to do is: 
- starting with the first element, check the similarity between this one and the others; if the similarity is greater than a threshold (80%), save it in a new array;
- check the second element (sriracha chilli) as above; 
- and so on. </p>

<p>Could you please tell me how to run such a similar loop?</p>
",Dataset Preprocessing & Handling,check term similarity within panda column similarity jarowinkler would need check two word list similar using jaro wrinkler distance follows seems able detect similarity word would need set threshold select word similar difficulty however checking word within data frame column would like starting first element check similarity one others similarity greater threshold save new array check second element sriracha chilli could please tell run similar loop
What does it mean when we combine text features and feed it to a Neural Network?,"<p>I am reading this paper -<a href=""https://link.springer.com/chapter/10.1007/978-3-030-19823-7_28"" rel=""nofollow noreferrer"">&quot;Review Spam Detection Using Word Embeddings and Deep Neural Networks&quot; - paywall link</a> and here they talk about how they combined ngram and skip-gram features of text before feeding it to the to feed-forward network.Here is the architecture<a href=""https://i.sstatic.net/hcIiV.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Some brief description of the dataset:-</p>
<ul>
<li>no. of documents=1600</li>
<li>dimension of skip-gram model=500</li>
<li>no. of the n-gram features(uni,bi,trigram)=2000</li>
</ul>
<p>For example:-The pictures show that skip-gram and n-gram models were combined before they were sent as an input to the feed-forward network.
Let's suppose the dimension of skip-gram is (no. of documents, dimension of skip-gram) and the dimension of the n-gram model is (no. of documents, no. of n-gram features)
My question is what does it mean when you combine two different features like skip-gram and n-gram. Does it mean concatenation i.e how do you combine two features? Along which axis do you combine those features?</p>
<blockquote>
<p>The size of the word vectors (embeddings) was set to 500 and context size c = 5 [7]
to generate a complex representation. The average values of the vector were used to
represent each review. Thus, the input attributes (features) for the subsequent supervised
learning included 2000 n-grams and 500 embeddings.
Deep feedforward neural network (DNN) was used to classify reviews into
spam/legitimate categories.</p>
</blockquote>
<p>Hope I have explained it well this time</p>
",Dataset Preprocessing & Handling,doe mean combine text feature feed neural network reading paper review spam detection using word embeddings deep neural network paywall link talk combined ngram skip gram feature text feeding feed forward network architectureenter image description brief description dataset document dimension skip gram model n gram feature uni bi trigram example picture show skip gram n gram model combined sent input feed forward network let suppose dimension skip gram document dimension skip gram dimension n gram model document n gram feature question doe mean combine two different feature like skip gram n gram doe mean concatenation e combine two feature along axis combine feature size word vector embeddings wa set context size c generate complex representation average value vector used represent review thus input attribute feature subsequent supervised learning included n gram embeddings deep feedforward neural network dnn wa used classify review spam legitimate category hope explained well time
Convert pandas data frame to JSON with strings separated,"<p><strong>I have a pandas.dataframe named 'df' with the following format:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>group_name</th>
<th>Positive_Sentiment</th>
<th>Negative_Sentiment</th>
</tr>
</thead>
<tbody>
<tr>
<td>group1</td>
<td>helpful, great support</td>
<td>slow customer service, weak interface, bad management</td>
</tr>
</tbody>
</table>
</div>
<p><strong>I would like to convert this dataframe to a JSON file with the following format:</strong></p>
<pre><code>[{
&quot;Group Name&quot;: &quot;group1&quot;,
&quot;Postive Sentiment&quot;: [
&quot;helpful&quot;,
&quot;great support&quot;
],
&quot;Negative Sentiment&quot;: [
&quot;slow customer service&quot;,
&quot;weak interface&quot;,
&quot;bad management&quot;
]
}
]
</code></pre>
<p><strong>So far I have used this:</strong></p>
<pre><code>    import json
    b = []
    for i in range(len(df)):
        x={}
        x['Group Name']=df.iloc[i]['group_name']
        x['Positive Sentiment']= [df.iloc[i]['Positive_Sentiment']]
        x['Negative Sentiment']= [df.iloc[i]['Negative_Sentiment']]
        b.append(x)
    
    ##Export
    with open('AnalysisResults.json', 'w') as f:
        json.dump(b, f, indent = 2)
</code></pre>
<p><strong>This results in:</strong></p>
<pre><code>[{
&quot;Group Name&quot;: &quot;group1&quot;,
&quot;Postive Sentiment&quot;: [
&quot;helpful,
great support&quot;
],
&quot;Negative Sentiment&quot;: [
&quot;slow customer service,
weak interface,
bad UX&quot;
]
}
]
</code></pre>
<p><strong>You can see it is quite close. The crucial difference is the double-quotes around the ENTIRE contents of each row (e.g., &quot;helpful, great support&quot;) instead of each comma-separated string in the row (e.g., &quot;helpful&quot;, &quot;great support&quot;). I would like double-quotes around each string.</strong></p>
",Dataset Preprocessing & Handling,convert panda data frame json string separated panda dataframe named df following format group name positive sentiment negative sentiment group helpful great support slow customer service weak interface bad management would like convert dataframe json file following format far used result see quite close crucial difference double quote around entire content row e g helpful great support instead comma separated string row e g helpful great support would like double quote around string
Data PreProcessing for BERT (base-german),"<p>I am working on a sentiment analysis solution with BERT to analyze tweets in german. My training dataset of is a class of 1000 tweets, which have been manually annotated into the classes neutral, positive and negative.</p>
<p>The dataset with 10.000 tweets is quite unevenly distributed:</p>
<p>approx.
3000 positive
2000 negative
5000 neutral</p>
<p>the tweets contain formulations with @names, https links, numbers, punctuation marks, smileys like :3 :D :) etc..</p>
<p>The interesting thing is, if I remove them with the following code during Data Cleaning, the F1 score gets worse. Only the removal of https links (if I do it alone) leads to a small improvement.</p>
<pre><code># removing the punctuation and numbers
def remove_punct(text):
    text = re.sub(r'http\S+', '', text)                                         # removing links
    text = re.sub(r'@\S+', '', text)                                            # removing referencing on usernames with @
    text = re.sub(r':\S+', '', text)                                            # removing smileys with : (like :),:D,:( etc) 
    text  = &quot;&quot;.join([char for char in text if char not in string.punctuation])
    text = re.sub('[0-9]+', '', text)
    return text

data['Tweet_clean'] = data['Tweet'].apply(lambda x: remove_punct(x))            # extending the dataset with the column tweet_clean
data.head(40)
</code></pre>
<p>also steps like stop words removal or lemmitazation lead more to a deterioration. Is this because I do something wrong or can the model BERT actually handle such values?</p>
<p>A second question is:</p>
<p>I found other records that were also manually annotated, but these are not tweets and the structure of the sentences and language use is different. Would you still recommend to add these records to my original?</p>
<p>There are about 3000 records in German.</p>
<p>My last question:</p>
<p>Should I reduce the class sizes to the size of the smallest unit and thus balance?</p>
",Dataset Preprocessing & Handling,data preprocessing bert base german working sentiment analysis solution bert analyze tweet german training dataset class tweet manually annotated class neutral positive negative dataset tweet quite unevenly distributed approx positive negative neutral tweet contain formulation name link number punctuation mark smiley like etc interesting thing remove following code data cleaning f score get worse removal link alone lead small improvement also step like stop word removal lemmitazation lead deterioration something wrong model bert actually handle value second question found record also manually annotated tweet structure sentence language use different would still recommend add record original record german last question reduce class size size smallest unit thus balance
How can I prevent words with hyphens from being tokenized when using scikit-learn`s term document matrix?,"<p>I am currently working with a large corpus of articles (around 205 thousand), which require the construction of a term document matrix.</p>
<p>I have looked around and it seems that sklearn offers an efficient way to construct it. However, when applying the proposed code to a small list of documents (as a test), I find out that words containing hyphens are divided, with the hyphens as delimiters. This is not desirable, as I am working with documents in Portuguese, in which hyphens are very common due to the large number of compound nouns. I would like to find out how I can generate a term document matrix that simply contains, as coluns, all tokens of my corpus, in which only empty spaces are used as delimiters between tokens (if a word contains a hyphen, it should be considered as a single token).</p>
<p>Here is the code:</p>
<pre><code>index=['doc 1','doc 2','doc 3','doc 4']
docs=['como você está', 'guarda-chuva!','covid-19 piorou','teto-de-gastos do tesouro']

df = pd.DataFrame(list(zip(index, docs)))
df.columns = ['index', 'docs']

from sklearn.feature_extraction.text import CountVectorizer
vect = CountVectorizer()  
vects = vect.fit_transform(df.docs)
td = pd.DataFrame(vects.todense()).iloc[:len(df)]  
td.columns = vect.get_feature_names()
term_document_matrix = td.T
term_document_matrix.columns = ['Doc '+str(i) for i in range(1, len(df)+1)]
term_document_matrix['total_count'] = term_document_matrix.sum(axis=1)
</code></pre>
<p>When printing the matrix, I find that “teto-de-gastos” was transformed into “teto”,”de”,”gastos”, which I do not want. Any suggestions on how to fix this hyphen issue?</p>
",Dataset Preprocessing & Handling,prevent word hyphen tokenized using scikit learn term document matrix currently working large corpus article around thousand require construction term document matrix looked around seems sklearn offer efficient way construct however applying proposed code small list document test find word containing hyphen divided hyphen delimiters desirable working document portuguese hyphen common due large number compound noun would like find generate term document matrix simply contains coluns token corpus empty space used delimiters token word contains hyphen considered single token code printing matrix find teto de gastos wa transformed teto de gastos want suggestion fix hyphen issue
does gensim.corpora wikiCorpus work only with bz2 file?,"<p>I'm trying to load a wiki dump (.gz) and use it in gensim word2vec. I convert it into bz2 using bzip2 in terminal but Wikicorpus class seems to refuse the file. Can someone please explain me how to get the text from a wiki dump in a easy way?
thanks</p>
",Dataset Preprocessing & Handling,doe gensim corpus wikicorpus work bz file trying load wiki dump gz use gensim word vec convert bz using bzip terminal wikicorpus class seems refuse file someone please explain get text wiki dump easy way thanks
How to load Bert pretrained model with SentenceTransformers from local path?,"<p>I am using the SentenceTransformer library to use Bert pre-trained model
I download the file in google Colabs and saved it with these commands:</p>
<pre><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('bert-large-uncased-whole-word-masking')
model.save('/content/drive/MyDrive/Pizza/Bert_Model')
</code></pre>
<p>these give me a folder with my model in it, then I download the folder and wanna use it in my local file with this code:</p>
<pre><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('Bert_Model')
</code></pre>
<p>but now it doesn't work properly and I got this error that '<strong>NameError: name 'model' is not defined</strong>'
How can I load the model that I have saved to a path?</p>
",Dataset Preprocessing & Handling,load bert pretrained model sentencetransformers local path using sentencetransformer library use bert pre trained model download file google colabs saved command give folder model download folder wan na use local file code work properly got error nameerror name model defined load model saved path
Understanding TypeError: &#39;&lt;&#39; not supported between instances of &#39;Example&#39; and &#39;Example&#39;,"<p>I am working on a project of text simplification using a multi-head attention transformer model. For the same, I am using torchtext for tokenisation and numericalization. The dataset contains two aligned files for training and two aligned files for testing. In the training files, one file contains the complex sentences while the other contains the corresponding simplified sentences.</p>

<p>I read the files as such:</p>

<pre><code>training_sentences = open(path + ""train.en"" , encoding = ""utf-8"").read().split(""\n"")
target_sentences = open(path + ""train.sen"" , encoding = ""utf-8"").read().split(""\n"")
</code></pre>

<p>Next, I tokenised them as such:</p>

<pre><code>complicated = spacy.load('en')
simple = spacy.load('en')

def tokenize_complicated(sentence):
   return [tok.text for tok in complicated.tokenizer(sentence)]

def tokenize_simple(sentence):
    return [tok.text for tok in simple.tokenizer(sentence)]

C_TEXT = Field(tokenize=tokenize_complicated, fix_length = 100)
S_TEXT = Field(tokenize=tokenize_simple, fix_length = 100, init_token = ""&lt;sos&gt;"", eos_token = ""&lt;eos&gt;"")
</code></pre>

<p>I then converted into TabularDataset object of torchtext.</p>

<pre><code>import pandas as pd
raw_data = {'Complicated' : [line for line in training_sentences], 
        'Simple': [line for line in target_sentences]}

df = pd.DataFrame(raw_data, columns=[""Complicated"", ""Simple""])

df.to_csv(""df.csv"", index=False)
data_fields = [('Complicated', C_TEXT), ('Simple', S_TEXT)]

train = torchtext.data.TabularDataset.splits(path='./', train = ""df.csv"", format='csv', fields=data_fields, skip_header = True)
</code></pre>

<p>And then created vocabulary</p>

<pre><code>C_TEXT.build_vocab(train)
S_TEXT.build_vocab(train)
</code></pre>

<p>However, on doing so I got this error: </p>

<blockquote>
  <p>TypeError: '&lt;' not supported between instances of 'Example' and
  'Example'</p>
</blockquote>

<p>On searching, I came across this solution <a href=""https://github.com/pytorch/text/issues/474#issuecomment-454385601"" rel=""nofollow noreferrer"">here</a> and the error disappeared. However, I am not understanding whether this makes the model take only one instance or it takes all of the dataset?
I would like to know the significance of the index <code>[0]</code> so that I can manipulate it effectively for my model.</p>
",Dataset Preprocessing & Handling,understanding typeerror supported instance example example working project text simplification using multi head attention transformer model using torchtext tokenisation numericalization dataset contains two aligned file training two aligned file testing training file one file contains complex sentence contains corresponding simplified sentence read file next tokenised converted tabulardataset object torchtext created vocabulary however got error typeerror supported instance example example searching came across solution error disappeared however understanding whether make model take one instance take dataset would like know significance index manipulate effectively model
python cannot load en_core_web_lg module in azure app service with docker image,"<p>I have a flask python app that uses a spacy model (md or lg).  I am running in a docker container in VSCode and all work correctly on my laptop.</p>
<p>When I push the image to my azure container registry the app restarts but it doesn't seem to get past this line in the log:</p>
<blockquote>
<p>Initiating warmup request to the container.</p>
</blockquote>
<p>If I comment out the line <code>nlp = spacy.load('en_core_web_lg')</code>, the website loads fine (of course it doesn't work as expected).</p>
<p>I am installing the model in the docker file after installing the requirements.txt:
<code>RUN python -m spacy download en_core_web_lg</code>.</p>
<p>Docker file:</p>
<pre><code>FROM python:3.6
EXPOSE 5000

# Keeps Python from generating .pyc files in the container
ENV PYTHONDONTWRITEBYTECODE 1

# Turns off buffering for easier container logging
ENV PYTHONUNBUFFERED 1

# steps needed for scipy
RUN apt-get update -y
RUN apt-get install -y python-pip python-dev libc-dev build-essential
RUN pip install -U pip

# Install pip requirements
ADD requirements.txt.
RUN python -m pip install -r requirements.txt
RUN python -m spacy download en_core_web_md

WORKDIR /app
ADD . /app

# During debugging, this entry point will be overridden. For more information, refer to https://aka.ms/vscode-docker-python-debug
CMD [&quot;gunicorn&quot;, &quot;--bind&quot;, &quot;0.0.0.0:5000&quot;, &quot;Application.webapp:app&quot;]
</code></pre>
",Dataset Preprocessing & Handling,python load en core web lg module azure app service docker image flask python app us spacy model md lg running docker container vscode work correctly laptop push image azure container registry app restarts seem get past line log initiating warmup request container comment line website load fine course work expected installing model docker file installing requirement txt docker file
How to assign new observations to cluster using distance matrix and kmedoids?,"<p>I have a dataframe that holds the Word Mover's Distance between each document in my dataframe. I am running kmediods on this to generate clusters.</p>
<pre><code>       1      2     3      4      5   
  1  0.00   0.05  0.07   0.04   0.05
  2  0.05   0.00  0.06   0.04   0.05
  3. 0.07   0.06  0.00   0.06   0.06
  4  0.04   0.04. 0.06   0.00   0.04
  5  0.05   0.05  0.06   0.04   0.00

  kmed = KMedoids(n_clusters= 3, random_state=123, method  ='pam').fit(distance)
</code></pre>
<p>After running on this initial matrix and generating clusters, I want to add new points to be clustered. After adding a new document to the distance matrix I end up with:</p>
<pre><code>       1      2     3      4      5      6
  1  0.00   0.05  0.07   0.04   0.05   0.12
  2  0.05   0.00  0.06   0.04   0.05   0.21 
  3. 0.07   0.06  0.00   0.06   0.06   0.01
  4  0.04   0.04. 0.06   0.00   0.04   0.05
  5  0.05   0.05  0.06   0.04   0.00   0.12
  6. 0.12   0.21  0.01   0.05   0.12   0.00
</code></pre>
<p>I have tried using kmed.predict on the new row.</p>
<pre><code>kmed.predict(new_distance.loc[-1: ])
</code></pre>
<p>However, this gives me an error of incompatible dimensions <code>X.shape[1] == 6</code> while <code>Y.shape[1] == 5</code>.</p>
<p>How can I use this distance of the new document to determine which cluster it should be a part of? Is this even possible, or do I have to recompute clusters every time? Thanks!</p>
",Dataset Preprocessing & Handling,assign new observation cluster using distance matrix kmedoids dataframe hold word mover distance document dataframe running kmediods generate cluster running initial matrix generating cluster want add new point clustered adding new document distance matrix end tried using kmed predict new row however give error incompatible dimension use distance new document determine cluster part even possible recompute cluster every time thanks
Permission error in load_dataset() from datasets library,"<p>I'm trying to get Wikisql dataset using the following code snippet.</p>
<pre><code>from datasets import load_dataset
import random, warnings
warnings.filterwarnings(&quot;ignore&quot;)

valid_dataset = load_dataset('wikisql', split='validation')

valid_dataset[0]
</code></pre>
<p>But it throws following error. I am not the administrator, but this error occurs in certain datasets only.</p>
<pre><code>PermissionError: [WinError 5] Access is denied: 'C:\\Users\\XXXXXX\\.cache\\huggingface\\datasets\\wiki_sql\\default\\0.1.0\\2e98053891fd8f9b2c4348bba609ce40cc0a4d7f621191cebcd7cb558b5f8a70.incomplete' -&gt; 'C:\\Users\\XXXXXX\\.cache\\huggingface\\datasets\\wiki_sql\\default\\0.1.0\\2e98053891fd8f9b2c4348bba609ce40cc0a4d7f621191cebcd7cb558b5f8a70'
</code></pre>
<p>As this appeared to be permission error in C drive, tried changing the cache directory, but still same error coming from that location.</p>
<pre><code>dataset = load_dataset('wikisql', split='validation', cache_dir=&quot;PATH/TO/MY/CACHE/DIR&quot;)
</code></pre>
<p>Any help would be highly appriciated.
I am using datasets==1.4.1</p>
",Dataset Preprocessing & Handling,permission error load dataset datasets library trying get wikisql dataset using following code snippet throw following error administrator error occurs certain datasets appeared permission error c drive tried changing cache directory still error coming location help would highly appriciated using datasets
"How to search for words with asterisks and wildcards (e.g., exampl*) in R (word appearance in a data frame)","<p>I wrote a code to count the appearance of words in a data frame:</p>
<pre><code>Items &lt;-  c('decid*','head', 'heads')
df1&lt;-data.frame(Items)
words&lt;- c('head', 'heads', 'decided', 'decides', 'top', 'undecided')
df_main&lt;-data.frame(words)
item &lt;- vector() 
count &lt;- vector()
for (i in 1:length(unique(Items))){ 
item[i] &lt;- Items[i] 
count[i]&lt;- sum(df_main$words  == item[i])} 
word_freq &lt;- data.frame(cbind(item, count))
word_freq
</code></pre>
<p>However, the results are like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>item</th>
<th>count</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>decid*</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>head</td>
<td>1</td>
</tr>
<tr>
<td>3</td>
<td>heads</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>As you see, it does not correctly count for &quot;decid*&quot;. The actual results I expect should be like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>item</th>
<th>count</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>decid*</td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td>head</td>
<td>1</td>
</tr>
<tr>
<td>3</td>
<td>heads</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>I think I need to change the item word (decid*) format, however, I could not figure it out. Any help is much appreciated!</p>
",Dataset Preprocessing & Handling,search word asterisk wildcards e g exampl r word appearance data frame wrote code count appearance word data frame however result like item count decid head head see doe correctly count decid actual result expect like item count decid head head think need change item word decid format however could figure help much appreciated
Why is the result different for same dataset in torchtext.legecy.text when i change the position of data in the csv file?,"<p>I am trying to learn PyTorch NLP basic text classification and following Lazy Programmer's Tutorial and I got a different result from the tutorial and when I tried to change the data, I encountered a strange change in the output.</p>
<pre class=""lang-py prettyprint-override""><code>
import torchtext.legacy.data as ttd
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime


data = {
    'label':[0, 1,1 ],
    'data':[   'ham and eggs or just  morning',
            'I like eggs and ham.',
            'Eggs I like!',
          
           
    ]
}

df = pd.DataFrame(data)
df.to_csv('thedata.csv', index=False)
TEXT = ttd.Field(
    sequential =True,
    batch_first =True,
    lower = True,
    tokenize ='spacy',
    pad_first = True
)
LABEL = ttd.Field(
    sequential=False,
    use_vocab=False,
    is_target  =True
)

dataset = ttd.TabularDataset(
    path = 'thedata.csv',
    format ='csv',
    skip_header=True,
    fields = [
              ('label', LABEL),
              ('data',TEXT)
    ]
)
train_dataset, test_dataset = dataset.split()
TEXT.build_vocab(train_dataset,)
vocab = TEXT.vocab
vocab.stoi

</code></pre>
<p>This is my first type of code and in the data, if you see i have used &quot;'ham and eggs or just  morning',&quot; in index 1. So after running the code, at last when i run vocab.stoi, I get the following output.
<a href=""https://i.sstatic.net/wquff.png"" rel=""nofollow noreferrer"">The output for the code.</a></p>
<pre class=""lang-py prettyprint-override""><code>
import torchtext.legacy.data as ttd
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime


data = {
    'label':[0, 1,1 ],
    'data':[   
            'I like eggs and ham.',
            'Eggs I like!',
'ham and eggs or just  morning',
          
           
    ]
}

df = pd.DataFrame(data)
df.to_csv('thedata.csv', index=False)
TEXT = ttd.Field(
    sequential =True,
    batch_first =True,
    lower = True,
    tokenize ='spacy',
    pad_first = True
)
LABEL = ttd.Field(
    sequential=False,
    use_vocab=False,
    is_target  =True
)

dataset = ttd.TabularDataset(
    path = 'thedata.csv',
    format ='csv',
    skip_header=True,
    fields = [
              ('label', LABEL),
              ('data',TEXT)
    ]
)
train_dataset, test_dataset = dataset.split()
TEXT.build_vocab(train_dataset,)
vocab = TEXT.vocab
vocab.stoi

</code></pre>
<p>Now In the second code, I have change the index of data &quot;'ham and eggs or just  morning',&quot; in third index, now if I run the code then I get different output for vocab.stoi
<a href=""https://i.sstatic.net/86g5m.png"" rel=""nofollow noreferrer"">output for the second code</a>.
I want to know the reason for this and how vocab_build works in PyTorch.
Plus, this is my first question, if the question is not clear please let me know.</p>
",Dataset Preprocessing & Handling,result different dataset torchtext legecy text change position data csv file trying learn pytorch nlp basic text classification following lazy programmer tutorial got different result tutorial tried change data encountered strange change output first type code data see used ham egg morning index running code last run vocab stoi get following output output code second code change index data ham egg morning third index run code get different output vocab stoi output second code want know reason vocab build work pytorch plus first question question clear please let know
K-mer words in R,"<p>I am still new to R programming and I just have no idea how to write this same code below from python to R.</p>
<p>human_data is dataframe from CSV file. the word includes sequence of letters. Basically, I want to convert my 'word' column sequence of string into all possible k-mer words of length 6.</p>
<pre><code>def getKmers(sequence, size=6):
    return [sequence[x:x+size] for x in range(len(sequence) - size + 1)]

human_data['words'] = human_data.apply(lambda x: getKmers(x['sequence']), axis=1)
</code></pre>
",Dataset Preprocessing & Handling,k mer word r still new r programming idea write code python r human data dataframe csv file word includes sequence letter basically want convert word column sequence string possible k mer word length
Loop text data based on column value in data frame in python,"<p>I have a dataset called data_set_tweets.csv as below</p>
<blockquote>
<p>created_at,tweet,retweet_count<br>
7/29/2021 2:40,Great Sunny day for Cricket at London,3<br>
7/29/2021 10:40,Great Score put on by England batting,0<br>
7/29/2021 11:50,England won the match,1<br></p>
</blockquote>
<p>And what I was trying to do is to get the below output in to a data frame.<br>
Which means I want to iterate the text in the <strong>tweet</strong> column based on the <strong>retweet_count</strong> value with same created_at values on that particular tweet
<br><em><strong>Below is the expected output for my dataset</strong></em></p>
<blockquote>
<p>created_at,tweet<br>7/29/2021 2:40,Great Sunny day for Cricket at London<br>
7/29/2021 2:40,Great Sunny day for Cricket at London<br>
7/29/2021 2:40,Great Sunny day for Cricket at London<br>
7/29/2021 2:40,Great Sunny day for Cricket at London<br>
7/29/2021 10:40,Great Score put on by England batting<br>
7/29/2021 11:50,England won the match<br>
7/29/2021 11:50,England won the match<br></p>
</blockquote>
<br>
Below is how I started my approach
<pre><code>import pandas as pd

def iterateTweets():
tweets = pd.read_csv(r'data_set_tweets.csv')
df = pd.DataFrame(tweets, columns=['created_at', 'tweet', 'retweet_count'])
df['created_at'] = pd.to_datetime(df['created_at'])
df['tweet'] = df['tweet'].apply(lambda x: str(x))
df['retweet_count'] = df['retweet_count'].apply(lambda x: str(x))

# print(df)
return df

if __name__ == '__main__':

print(iterateTweets())
</code></pre>
<p>I'm beginner to data frame and python can someone help me out?</p>
",Dataset Preprocessing & Handling,loop text data based column value data frame python dataset called data set tweet csv created tweet retweet count great sunny day cricket london great score put england batting england match wa trying get output data frame mean want iterate text tweet column based retweet count value created value particular tweet expected output dataset created tweet great sunny day cricket london great sunny day cricket london great sunny day cricket london great sunny day cricket london great score put england batting england match england match started approach beginner data frame python someone help
How to find most important 300 words from set of 3000 documents using TF-ID?,"<p>I have a set of 3000 documents and I want to select the top 300 important words from all the documents. I used TF-ID Implementation to get the scores for words across all the documents using TF-ID Vectorizer.</p>
<p>But I dont know how to use this Scorematrix to finally get most important 300 words ?</p>
<p>Is it a good approach to take the mean of the score Matrix along The column axis and then choose those words which has the highest mean TF-ID Score ?</p>
",Dataset Preprocessing & Handling,find important word set document using tf id set document want select top important word document used tf id implementation get score word across document using tf id vectorizer dont know use scorematrix finally get important word good approach take mean score matrix along column axis choose word ha highest mean tf id score
How to get sentiment score for a word in a given dataset,"<p>I have a sentiment analysis dataset that is labeled in three categories: positive, negative, and neutral. I also have a list of words (mostly nouns), for which I want to calculate the sentiment value, to understand &quot;how&quot; (positively or negatively) these entities were talked about in the dataset. I have read some online resources like blogs and thought about a couple of approaches for calculating the sentiment score for a particular word X.</p>
<ol>
<li><p>Calculate how many data instances (sentences) which have the word X in those, have &quot;positive&quot; labels, have &quot;negative&quot; labels, and &quot;neutral&quot; labels. Then, calculate the weighted average sentiment for that word.</p>
</li>
<li><p>Take a generic untrained BERT architecture, and then train it using the dataset. Then, pass each word from the list to that trained model to get the sentiment scores for the word.</p>
</li>
</ol>
<p>Does any of these approaches make sense? If so, can you suggest some related works that I can look at?
If these approaches don't make sense, could you please advise how I can calculate the sentiment score for a word, in a given dataset?</p>
",Dataset Preprocessing & Handling,get sentiment score word given dataset sentiment analysis dataset labeled three category positive negative neutral also list word mostly noun want calculate sentiment value understand positively negatively entity talked dataset read online resource like blog thought couple approach calculating sentiment score particular word x calculate many data instance sentence word x positive label negative label neutral label calculate weighted average sentiment word take generic untrained bert architecture train using dataset pas word list trained model get sentiment score word doe approach make sense suggest related work look approach make sense could please advise calculate sentiment score word given dataset
Different sample rate SR for same wav file between librosa and tensorflow,"<p>I have one wav file which I resampled to 16.000 kHz with Audacity.
Now I am trying to load the file with python with 2 different ways.</p>
<pre><code>import tensorflow as tf
import librosa

f = &quot;path/to/wav/file/xxxx.wav&quot;

raw = tf.io.read_file(f)
audio, sr = tf.audio.decode_wav(raw, desired_channels=1)
print(&quot;Sample Rate TF: &quot;,sr.numpy())

y, sr2 = librosa.load(f)
print(&quot;Sample Rate librosa: &quot;,sr2)


#Sample Rate TF:  16000
#Sample Ratelibrosa:  22050
</code></pre>
<p>Why is the sample rate so different for the same file?
Which library I can trust more?</p>
",Dataset Preprocessing & Handling,different sample rate sr wav file librosa tensorflow one wav file resampled khz audacity trying load file python different way sample rate different file library trust
Spacy Model load error from local directory,"<p>I am trying to find a way to load the downloaded <code>en_core_web_lg ==2.3.1</code> for <code>Spacy == 2.3.2</code>.</p>
<p>Steps:</p>
<ol>
<li>Downloaded the <code>tar</code> file</li>
<li>extracted it to <code>path</code></li>
</ol>
<p>Code:</p>
<pre><code>import spacy
nlp=spacy.load(&quot;path/en_core_web_lg&quot;)
</code></pre>
<p>Error:</p>
<pre><code>OSERROR: [E053] Could not read meta.json from en_core_web_lg/meta.json
</code></pre>
<p>Any suggestions will be helpful</p>
",Dataset Preprocessing & Handling,spacy model load error local directory trying find way load downloaded step downloaded file extracted code error suggestion helpful
Searching over a list of individual sentences by a specific term in Python,"<p>I have a list of terms in Python that look like this.</p>
<pre><code>Fruit
apple
banana
grape
orange
</code></pre>
<p>As well as a list of individual sentences that may contain the name of that fruit in a data frame. Something similar to this:</p>
<pre><code>Customer     Review
1            ['the banana was delicious','he called the firetruck','I had only half an orange']
2            ['I liked the banana','there was a worm in my apple','Cantaloupes are better then melons']
3            ['It could use some more cheese','the grape and orange was sour']
</code></pre>
<p>And I want to take the sentences in the review column, match them with the fruit mentioned in the text and print out a data frame of that as a final result. So, something like this:</p>
<pre><code>Fruit     Review
apple     ['the banana was delicious','I liked the banana']
banana    ['there was a worm in my apple']
grape     ['the grape and orange was sour']
orange    ['the grape and orange was sour','I had only half an orange']
</code></pre>
<p>Hoe could I go about doing this?</p>
",Dataset Preprocessing & Handling,searching list individual sentence specific term python list term python look like well list individual sentence may contain name fruit data frame something similar want take sentence review column match fruit mentioned text print data frame final result something like hoe could go
Use latent semantic analysis to understand if a document is about a topic,"<p>This is an example of the use of latent semantic analysis. For simplicity I have considered 4 documents and 2 topics. The code I used is the following:</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import pandas as pd

body = [
    'the quick brown fox',
    'the slow brown dog',
    'the quick red dog',
    'the lazy yellow fox'
]

vectorizer = TfidfVectorizer(use_idf=False, norm='l1')
bag_of_words = vectorizer.fit_transform(body)

svd = TruncatedSVD(n_components=2)
lsa = svd.fit_transform(bag_of_words)

topic_encoded_df = pd.DataFrame(lsa, index=['text_1', 'text_2', 'text_3', 'text_4'], columns=['topic_1', 'topic_2'])
</code></pre>
<p><code>topic_encoded_df</code> is the data frame</p>
<pre><code>        topic_1     topic_2
text_1  0.423726    0.074881
text_2  0.378963    -0.192278
text_3  0.378963    -0.192278
text_4  0.316547    0.360146
</code></pre>
<p>Again, this is a trivial case to understand what I did.
Is there a way to coherently say if, for example, <code>text_1</code> is about <code>topic_2</code> in a meaningful way? Or if <code>text_ 2</code> is about <code>topic_2</code>? I was thinking of something like Elbow method on column values (sorted in descending order), but I'm afraid negative signs may give wrong indications. Anyone have another idea?</p>
",Dataset Preprocessing & Handling,use latent semantic analysis understand document topic example use latent semantic analysis simplicity considered document topic code used following data frame trivial case understand way coherently say example meaningful way wa thinking something like elbow method column value sorted descending order afraid negative sign may give wrong indication anyone another idea
how to see the class name of one hot encoded?,"<p>I have a CSV file that includes two columns: a 'Text' of a tweet and its &quot;label'. each tweet could belong to one of these 4 categories: Hate, Neutral, CounterHate and Non-Asian Aggression.
I did <strong>One Hot Encode Y values</strong> for train and test vectors by the following code in Python:</p>
<pre><code>encoder = LabelEncoder()
y_train = encoder.fit_transform(train['Label'].values)
y_train = to_categorical(y_train) 
y_test = encoder.fit_transform(test['Label'].values)
y_test = to_categorical(y_test)
</code></pre>
<p>which if you print the first index:</p>
<pre><code>print(y_train[0])
</code></pre>
<p>The answer is:</p>
<pre><code>[0. 1. 0. 0.]
</code></pre>
<p>We know that each Label is converted to a vector of length 4, where each position corresponds to a Label class. How can I find the position of each class?</p>
<p>For example: Hate=0, Counterhate=1,...</p>
",Dataset Preprocessing & Handling,see class name one hot encoded csv file includes two column text tweet label tweet could belong one category hate neutral counterhate non asian aggression one hot encode value train test vector following code python print first index answer know label converted vector length position corresponds label class find position class example hate counterhate
Parsing long string in R,"<p>I have a data frame which contains rows by date. One column, <code>text</code> contains a long string (text of municipal council motions) with the following format:</p>
<p>-&gt; Number. (eg 1.)</p>
<p>-&gt; string (eg Motion to get squirrels banned)</p>
<p>-&gt; the word Councilor or Mayor + a Name (eg Councilor Obama)</p>
<p>-&gt; Number. (eg 2.)</p>
<p>-&gt; string (eg Action on standardizing calendars)</p>
<p>-&gt; the word Councilor or Mayor + a Name (eg Mayor Biden)</p>
<p>etc</p>
<p>Such as in the below reprex:</p>
<pre><code>n &lt;- 3
dat &lt;- data.frame(id=c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;), 
                  date=c(&quot;2020-12-26&quot;,&quot;2020-12-31&quot;,&quot;2021-01-31&quot;),
                  text=c(&quot;1. Increasing Public Access to Information About False Creek South  Councillor Dennings submitted a notice of Council Members motion on the above -noted matter. The motion may be placed on the Council meeting agenda of March 30, 2021, as a Council Members Motion.  2. Turning Construction Hoard ing into a Canvass for Public Art in Vancouver  Councillor Pikachu submitted a notice of Council Members motion on the above -noted matter. The motion may be placed on the Council meeting agenda of March 30, 2021, as a Council Members Motion.  Council Meeting Minutes, March 9, 2021 34  3. A Pl an to Significantly Reduce the Citys Permitting Backlog  Mayor Dominato submitted a notice of Council Members motion on the above -noted matter. The motion may be placed on the Council meeting agenda of March 30, 2021, as a Council Members Motion. &quot;,
                         &quot;1. Doing something good for health  Councillor Horse submitted a notice of Council Members motion on the above -noted matter. The motion may be placed on the Council meeting agenda of March 30, 2021, as a Council Members Motion.  2. Doing something for the good of the kids  Mayor Bowser submitted a notice of Council Members motion on the above -noted matter. The motion may be placed on the Council meeting agenda of March 30, 2021, as a Council Members Motion. &quot;, 
                         &quot;1. Finding a way to make an impact  Councillor Nimby bmitted a notice of Council Members motion on the above -noted matter. The motion may be placed on the Council meeting agenda of March 30, 2021, as a Council Members Motion.  2. Doing something for farmers  Councillor Biscoff submitted a notice of Council Members motion on the above -noted matter. The motion may be placed on the Council meeting agenda of March 30, 2021, as a Council Members Motion. 3. Funding a park for puppies  Councillor AnimalLover submitted a notice of Council Members motion on the above -noted matter. The motion may be placed on the Council meeting agenda of March 30, 2021, as a Council Members Motion. 4. Providing tea for all students  Councillor Shumba submitted a notice of Council Members motion on the above -noted matter. The motion may be placed on the Council meeting agenda of March 30, 2021, as a Council Members Motion.&quot;))
</code></pre>
<p>I'd like to parse the text column so that for each row, the number and the sentence following it is extracted and put into a separate column, and the Councilor or Mayor that submitted the motion is saved into a third column, and so on for each number afterwards. The data frame would look like this reprex:</p>
<pre><code>n &lt;- 3
dat &lt;- data.frame(id=c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;), 
                  date=c(&quot;2020-12-26&quot;,&quot;2020-12-31&quot;,&quot;2021-01-31&quot;),
                  text=c(&quot;1. Increasing Public Access to Information About False Creek South  Councillor Dennings submitted a notice of Council Members motion on the above -noted matter. The motion may be placed on the Council meeting agenda of March 30, 2021, as a Council Members Motion.  2. Turning Construction Hoard ing into a Canvass for Public Art in Vancouver  Councillor Pikachu &quot;,
                         &quot;1. Doing something good for health  Councillor Horse submitted a notice of Council Members motion on the above -noted matter. The motion may be placed on the Council meeting agenda of March 30, 2021, as a Council Members Motion. &quot;, 
                         &quot;1. Finding a way to make an impact  Councillor Nimby bmitted a notice of Council Members motion on the above -noted matter. The motion may be placed on the Council meeting agenda of March 30, 2021, as a Council Members Motion.  2. Doing something for farmers  Councillor Biscoff submitted a notice of Council Members motion on the above -noted matter. The motion may be placed on the Council meeting agenda of March 30, 2021, as a Council Members Motion. 3. Funding a park for puppies  Councillor AnimalLover submitted a notice of Council Members motion on the above -noted matter. The motion may be placed on the Council meeting agenda of March 30, 2021, as a Council Members Motion. &quot;),
                  Motion1= c('1. Increasing Public Access to Information About False Creek South',
                  '1. Doing something good for health', 
                  '1. Finding a way to make an impact'),
                  Motion1_Submitter= c('Councillor Dennings',
                                       'Councillor Horse',
                                       'Councillor Nimby'),
                  Motion2 = c('2. Turning Construction Hoard ing into a Canvass for Public Art in Vancouver',
                  'NA',
                  '2. Doing something for farmers'),
                  Motion2_Submitter = c('Councillor Pikachu',
                                        'NA',
                                        ' Councillor Biscoff'),
                  Motion3= c('NA', 'NA', '3. Funding a park for puppies.'),
                  Motion3_Submitter = c('NA', 'NA', '  Councillor AnimalLover'))
</code></pre>
<p>Is there a package or way to do this in R? Would it be easier to do in Python?</p>
<p>TIA!</p>
",Dataset Preprocessing & Handling,parsing long string r data frame contains row date one column contains long string text municipal council motion following format number eg string eg motion get squirrel banned word councilor mayor name eg councilor obama number eg string eg action standardizing calendar word councilor mayor name eg mayor biden etc reprex like parse text column row number sentence following extracted put separate column councilor mayor submitted motion saved third column number afterwards data frame would look like reprex package way r would easier python tia
"In text analysis, what does the *density of coefficients* of a classifier model mean?","<p>I'm reading through <a href=""https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html"" rel=""nofollow noreferrer"">this very good text classifier example</a> from sklearn that takes a collection of documents (newspaper articles), vectorises the corpus, then runs the resulting <code>n x p</code> matrix through a series of classifiers and grabs some comparison metrics including the following code:</p>
<pre><code>clf = SomeClassifier(parameters)
if hasattr(clf, 'coef_'):
    print(f&quot;dimensionality: {clf.coef_.shape[1]}&quot;)
    print(f&quot;density: {density(clf.coef_)}&quot;)
</code></pre>
<p>I know that the n x p sparse matrix has <code>n</code> rows (number of newspaper articles) and <code>p</code> predictor columns (number of tokens passed by the vectoriser, in my case individual words). I know that the attribute <code>coef_</code> only exists for models that form some kind of line or boundary, and that it has shape <code>c x p</code> where <code>c</code> is the number of classes from the target variable and uses the same <code>p</code> number of parameters passed to it by the vectoriser, which is <code>clf.coef_.shape[1]</code> in the example code. Unfortunately my matrix maths is very rusty, and I'm still learning NLP text analysis so I'm stuck with this last bit</p>
<p>Questions:</p>
<h4>i) What does the function <code>density()</code> tell us about a matrix?</h4>
<p>The example models  ~0.001-1.0 for the various model, so I assume this is something about the proportion of null/0 values in the array? I can't seem to find what library this function is even coming from, and my web searches keep returning density graphs or physics articles, which I don't think are relevant.</p>
<h4>ii) What does the density of a <code>class c x p token predictor</code> matrix tell us about the text classifiers?</h4>
<p>If density is the proportion of null values, I assume this tells us if the model is selecting relevant parameters on a class by class basis, like in LASSO / L1 normalisation. If this is the case, I assume that a very low value (density = ~0.001) means that most of the parameters are dropped, and a value of 1 means that the model is using every parameter to predict every class. Am I off the beaten track here? If I'm right, can anyone link to any articles or write examples to help me understand this?</p>
",Dataset Preprocessing & Handling,text analysis doe density coefficient classifier model mean reading good text classifier example sklearn take collection document newspaper article vectorises corpus run resulting matrix series classifier grab comparison metric including following code know n x p sparse matrix ha row number newspaper article predictor column number token passed vectoriser case individual word know attribute exists model form kind line boundary ha shape number class target variable us number parameter passed vectoriser example code unfortunately matrix math rusty still learning nlp text analysis stuck last bit question doe function tell u matrix example model various model assume something proportion null value array seem find library function even coming web search keep returning density graph physic article think relevant ii doe density matrix tell u text classifier density proportion null value assume tell u model selecting relevant parameter class class basis like lasso l normalisation case assume low value density mean parameter dropped value mean model using every parameter predict every class beaten track right anyone link article write example help understand
How to detect protected cells in Excel file using Python?,"<p>Given that an Excel file contains some cells protected with passwords, I want to detect these protected cells to choose whether to include them in the inputs or skip them.</p>
<p>I have tried <code>pandas</code></p>
<pre><code>df = pd.read_excel('test.xlsx', engine='openpyxl')
</code></pre>
<p>and <code>openpyxl</code></p>
<pre><code>wb = openpyxl.load_workbook('test.xlsx')
sheet = wb['Sheet1']
# B4 is a protected cell with a specific password so I cannot change its value in
# the Excel file without the password
print(sheet['B4'].value)
&gt;&gt; 8
sheet['B4'].value = 7
print(sheet['B4'].value)
&gt;&gt; 7
</code></pre>
<p>However, the protected cells are read normally like other unprotected cells and could be easily changed.</p>
<p>So the question is, how could I detect these protected cells?</p>
<p>For example, is there any way to read the attributes of the Excel file that indicate if the cell is protected or not?</p>
",Dataset Preprocessing & Handling,detect protected cell excel file using python given excel file contains cell protected password want detect protected cell choose whether include input skip tried however protected cell read normally like unprotected cell could easily changed question could detect protected cell example way read attribute excel file indicate cell protected
How many times a character appears in a text of a dataframe form,"<p>I am a begginer in NLP and I have a dataframe which has the following form</p>
<pre><code>text                         label 
----                        -----
This is he # first text     first label
This is the # second text   second label 
....                         ....
</code></pre>
<p>and I want to cound how many times the character '#' appread in this data frame. Could you please help me? I am looking for a generalised code which I cound count either '#' or another character or a word.</p>
",Dataset Preprocessing & Handling,many time character appears text dataframe form begginer nlp dataframe ha following form want cound many time character appread data frame could please help looking generalised code cound count either another character word
How to read PDFs with Hindi text in R?,"<p>I have multiple typed PDFs with Hindi text. I need to create a data frame using all the PDFs for further analysis. I am using R for this.</p>
<pre><code>&gt; sessionInfo()
R version 4.0.3 (2020-10-10)
Platform: x86_64-apple-darwin17.0 (64-bit)
Running under: macOS Mojave 10.14.6

Matrix products: default
BLAS:   /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib

locale:
[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] pdftools_3.0.1  textreadr_1.0.2 tm_0.7-8        NLP_0.2-1      

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.7        pillar_1.6.1      compiler_4.0.3    tools_4.0.3       lfe_2.8-6         lifecycle_1.0.0  
 [7] tibble_3.1.2      lattice_0.20-41   pkgconfig_2.0.3   rlang_0.4.11      Matrix_1.2-18     DBI_1.1.1        
[13] parallel_4.0.3    xfun_0.23         dplyr_1.0.6       xml2_1.3.2        generics_0.1.0    vctrs_0.3.8      
[19] askpass_1.1       grid_4.0.3        tidyselect_1.1.0  data.table_1.13.2 glue_1.4.2        qpdf_1.1         
[25] R6_2.5.0          fansi_0.4.1       sp_1.4-4          Formula_1.2-4     purrr_0.3.4       magrittr_2.0.1   
[31] ellipsis_0.3.2    assertthat_0.2.1  xtable_1.8-4      sandwich_3.0-1    utf8_1.1.4        tinytex_0.32     
[37] slam_0.1-48       crayon_1.4.1      zoo_1.8-9    
</code></pre>
<p>I have tried using the following :</p>
<pre><code>library(tm)
library(pdftools)

file &lt;- 'pdf_file.pdf'

dat = readPDF(control=list(text=&quot;-layout&quot;))(elem=list(uri=file), 
                                            language=&quot;UTF-8&quot;, id=&quot;id1&quot;) 

library(textreadr)

pdf_dat &lt;- read_pdf(system.file(&quot;pdf_file.pdf&quot;, package = &quot;textreadr&quot;))

pdf_text(file)
</code></pre>
<p>However, the result I get looks like :</p>
<pre><code>   27 ªF³F½FSXeX XY2012\n\n                                                                                                                                                                                       ¸FûQe ³FZ d½FIYFÀF °Fû dIY¹FF W` : IYFa¦FiZÀF\n                                                                                                                                                                                         w ¦F„þSF°F IYFa¦FiZÀF IZY d½FÄFF´F³F ¸FZÔ dQJe          ¦F„þSF°Fe IYû d½FIYFÀF ´F„÷Y¿F ¶F°FF°FZ W„E SFª¹F IZY        IYFa¦FiÀZ F ³FZ ¹FWX ÀFaQVZ F QZ³FZ IYe IYFZdVFVF þøYSX            QþÊ³F C°ÀF½FûÔ IYF ¶FûÓF OXF»F³FZ IYF AFSû´F\n                                                                                                                                                                                                                                                                                                                                                                                                                                                    ³F¦FSX ÀFaÀIYSX¯F\n   
</code></pre>
<p>An example PDF for this purpose can be downloaded from <a href=""https://vk.com/doc562341376_609941034?hash=fb68b749b7aebfdd87&amp;dl=0b94ca877de3492686"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Thank You!</p>
",Dataset Preprocessing & Handling,read pdfs hindi text r multiple typed pdfs hindi text need create data frame using pdfs analysis using r tried using following however result get look like example pdf purpose downloaded thank
How to save parameters just related to classifier layer of pretrained bert model due to the memory concerns?,"<p>I fine tuned the pretrained model <a href=""https://huggingface.co/dbmdz/bert-base-turkish-cased"" rel=""noreferrer"">here</a> by freezing all layers except the classifier layers. And I saved weight file with using pytorch as .bin format.</p>
<p>Now instead of loading the 400mb pre-trained model, is there a way to load the parameters of the just Classifier layer I retrained it? By the way, I know that I have to load the original pretrained model, I just don't want to load the entire fine tuned model. due to memory concerns.</p>
<p>I can access the last layer's parameters from state_dict as below, but how can I save them in a separate file to use them later for less memory usage?</p>
<pre><code>model = PosTaggingModel(num_pos_tag=num_pos_tag)
state_dict = torch.load(&quot;model.bin&quot;)
print(&quot;state dictionary:&quot;,state_dict)
with torch.no_grad():
    model.out_pos_tag.weight.copy_(state_dict['out_pos_tag.weight'])
    model.out_pos_tag.bias.copy_(state_dict['out_pos_tag.bias'])
</code></pre>
<p>Here is the model class:</p>
<pre><code>class PosTaggingModel(nn.Module):
    def __init__(self, num_pos_tag):
        super(PosTaggingModel, self).__init__()
        self.num_pos_tag = num_pos_tag
        self.model = AutoModel.from_pretrained(&quot;dbmdz/bert-base-turkish-cased&quot;)
        for name, param in self.model.named_parameters():
            if 'classifier' not in name: # classifier layer
                param.requires_grad = False
        self.bert_drop = nn.Dropout(0.3)
        self.out_pos_tag = nn.Linear(768, self.num_pos_tag)
        
    def forward(self, ids, mask, token_type_ids, target_pos_tag):
        o1, _ = self.model(ids, attention_mask = mask, token_type_ids = token_type_ids)
        
        bo_pos_tag = self.bert_drop(o1)
        pos_tag = self.out_pos_tag(bo_pos_tag)

        loss = loss_fn(pos_tag, target_pos_tag, mask, self.num_pos_tag)
        return pos_tag, loss
</code></pre>
<p>I don't know if this is possible but I'm just looking for a way to save and reuse the last layer's parameters, without the need for parameters of frozen layers. I couldn't find it in the <a href=""https://pytorch.org/tutorials/beginner/saving_loading_models.html?highlight=save"" rel=""noreferrer"">documentation</a>.
Thanks in advance to those who will help.</p>
",Dataset Preprocessing & Handling,save parameter related classifier layer pretrained bert model due memory concern fine tuned pretrained model freezing layer except classifier layer saved weight file using pytorch bin format instead loading mb pre trained model way load parameter classifier layer retrained way know load original pretrained model want load entire fine tuned model due memory concern access last layer parameter state dict save separate file use later le memory usage model class know possible looking way save reuse last layer parameter without need parameter frozen layer find documentation thanks advance help
How to read multiple ann files (from brat annotation) within a folder into one pandas dataframe?,"<p>I can read one ann file into pandas dataframe as follows:</p>
<pre><code>df = pd.read_csv('something/something.ann', sep='^([^\s]*)\s', engine='python', header=None).drop(0, axis=1)
df.head()
</code></pre>
<p>But I don't know how to read multiple ann files into one pandas dataframe. I tried to use <code>concat</code>, but the result is not what I expected.</p>
<p>How can I read many ann files into one pandas dataframe?</p>
",Dataset Preprocessing & Handling,read multiple ann file brat annotation within folder one panda dataframe read one ann file panda dataframe follows know read multiple ann file one panda dataframe tried use result expected read many ann file one panda dataframe
How to use Spacy with Pyspark?,"<p>I'm trying to use Spacy with Pyspark. Since Spacy models are not serializable, they cannot be broadcasted. The solution I have found is to load the models on worker-side. But loading models for each task can be time consuming. The solution I have found is to use a global variable in which I store the loaded model. And since &quot;spark.python.worker.reuse&quot; is True by default, the model will be loaded only once in each worker.</p>
<pre><code>accu = spark.sparkContext.accumulator(0)
spmodel = None
def get_sp():
    global spmodel
    if not spmodel:
        accu.add(1)
        spmodel = spacy.load(&quot;ja_core_news_sm&quot;)
    return spmodel

def compute_ner(s_text:pd.Series) -&gt; pd.DataFrame:
    nlp = get_sp()
    docs = nlp.pipe(s_text)
    ents=[
         [(ent.text, ent.label_) for ent in doc.ents]
         for doc in docs
         ]
    preds = []
    # Some Instructions .. 
    return pd.DataFrame(preds)
udf_calculate = F.pandas_udf(compute_ner, returnType=return_types)
ner_df = df.repartition(16).withColumn('ner', udf_calculate('text'))
</code></pre>
<p>I have used an accumulator to see how many times the model is loaded. I have tested the application on local mode with 8 dedicated cores. The DataFrame I have used is divided on 16 partitions. I was surprised to discover that the accumulator value is 16. So spmodel is always None.
I'm a little bit confused, can someone explain what's happening, or maybe I am missing something.</p>
",Dataset Preprocessing & Handling,use spacy pyspark trying use spacy pyspark since spacy model serializable broadcasted solution found load model worker side loading model task time consuming solution found use global variable store loaded model since spark python worker reuse true default model loaded worker used accumulator see many time model loaded tested application local mode dedicated core dataframe used divided partition wa surprised discover accumulator value spmodel always none little bit confused someone explain happening maybe missing something
Measuring co-occurence patterns in media articles over time with Quanteda,"<p>I am trying to measure the number of times that different words co-occur with a particular term in collections of Chinese newspaper articles from each quarter of a year. To do this, I have been using Quanteda and written several R functions to run on each group of articles. My work steps are:</p>
<ol>
<li>Group the articles by quarter.</li>
<li>Produce a frequency co-occurence matrix (FCM) for the articles in each quarter (Function 1).</li>
<li>Take the column from this matrix for the 'term' I am interested in and convert this to a data.frame (Function 2)</li>
<li>Merge the data.frames for each quarter together, then produce a large csv file with a column for each quarter and a row for each co-occurring term.</li>
</ol>
<p>This seems to work okay. But I wondered if anybody more skilled in R might be able to check what I am doing is correct, or might suggest a more efficient way of doing it?</p>
<p>Thanks for any help!</p>
<pre class=""lang-r prettyprint-override""><code>#Function 1 to produce the FCM

get_fcm &lt;- function(data) {
  ch_stop &lt;- stopwords(&quot;zh&quot;, source = &quot;misc&quot;)
  corp = corpus(data)
  toks = tokens(corp, remove_punct = TRUE) %&gt;% tokens_remove(ch_stop)  
  fcm = fcm(toks, context = &quot;window&quot;, window = 1, tri = FALSE)
  return(fcm)
}

&gt;fcm_14q4 &lt;- get_fcm(data_14q4)
&gt;fcm_15q1 &lt;- get_fcm(data_15q1)

#Function 2 to select the column for the 'term' of interest (such as China 中国) and make a data.frame

convert2df &lt;- function(matrix, term){
  mat_term = matrix[,term]
  df = convert(mat_term, to = &quot;data.frame&quot;)
  colnames(df)[1] = &quot;Term&quot;
  colnames(df)[2] = &quot;Freq&quot;
  x = df[order(-df$Freq),]
  return(x)
}

&gt;CH14 &lt;- convert2df(fcm_14q4, &quot;中国&quot;)
&gt;CH15 &lt;- convert2df(fcm_15q1, &quot;中国&quot;)

#Merging the data.frames

df &lt;- merge(x=CH14q4, y=CH15q1, by=&quot;Term&quot;, all.x=TRUE, all.y=TRUE)
df &lt;- merge(x=df, y=CH15q2, by=&quot;Term&quot;, all.x=TRUE, all.y=TRUE) #etc for all the dataframes... 

</code></pre>
<p>UPDATE: Following Ken's advice in the comments below, I have tried doing it a different way, using the window function of tokens_select() and then a document feature matrix. After labelling the corpus documents according to their quarter, the following R function should take the tokenized corpus <code>toks</code> and then produce a data.frame of the number of times words co-occur within a specified <code>window</code> of a <code>term</code>.</p>
<pre class=""lang-r prettyprint-override""><code>COOCdfm &lt;- function(toks, term, window){
  ch_stop = stopwords(&quot;zh&quot;, source = &quot;misc&quot;)
  cooc_toks = tokens_select(toks, term, window = window)
  cooc_toks2 = tokens(cooc_toks, remove_punct = TRUE)
  cooc_toks3 = tokens_remove(cooc_toks2, ch_stop)
  dfmat = dfm(cooc_toks3)
  dfmat_grouped = dfm_group(dfmat, groups = &quot;quarter&quot;)
  counts = convert(t(dfmat_grouped), to = &quot;data.frame&quot;)
  colnames(counts)[1] &lt;- &quot;Feature&quot;
  return(counts)
} 
</code></pre>
",Dataset Preprocessing & Handling,measuring co occurence pattern medium article time quanteda trying measure number time different word co occur particular term collection chinese newspaper article quarter year using quanteda written several r function run group article work step group article quarter produce frequency co occurence matrix fcm article quarter function take column matrix term interested convert data frame function merge data frame quarter together produce large csv file column quarter row co occurring term seems work okay wondered anybody skilled r might able check correct might suggest efficient way thanks help update following ken advice comment tried different way using window function token select document feature matrix labelling corpus document according quarter following r function take tokenized corpus produce data frame number time word co occur within specified
Can&#39;t figure out where this error is coming from,"<p>I'm working on a sentiment analysis program for an NLP class. I've imported a folder with a lot of different files and my goal is to merge all of the files text together to analyze it as one big pool of text. So far I've been able to import all of the files using:</p>
<pre><code>path = &quot;noblespeeches&quot; 
nobel_speeches = os.listdir(path)

files = sorted([file for file in nobel_speeches if file.endswith('.txt')])
</code></pre>
<p>...where  &quot;nobelspeeches&quot; is the path to the appropriate file on my computer. I've tested this part and it seems to work fine.  I'm having trouble creating a function to read the files so I can merge all of the text together.</p>
<pre class=""lang-py prettyprint-override""><code>def read_file(file_name):
  with open(file_name, 'r+', encoding='utf-8') as file:
    file_text = file.read()
  return file_text
</code></pre>
<p>This  function is what I've been working with, and I cannot seem to get it to work for the life of me. I'm sure the answer is quite simple, but I'm fairly new to Python and very new to NLP. I've been researching different possible solutions to no avail.</p>
<p>The error code is: <code>FileNotFoundError: [Errno 2] No such file or directory:</code></p>
<p>The machine is producing an error that states that the first file in my folder doesn't exist even though it will acknowledge the file in the earlier code.</p>
",Dataset Preprocessing & Handling,figure error coming working sentiment analysis program nlp class imported folder lot different file goal merge file text together analyze one big pool text far able import file using nobelspeeches path appropriate file computer tested part seems work fine trouble creating function read file merge text together function working seem get work life sure answer quite simple fairly new python new nlp researching different possible solution avail error code machine producing error state first file folder exist even though acknowledge file earlier code
How do I access beanstalk application venv?,"<p>this last week I have been trying to upload a flask app using AWS Beanstalk.</p>
<p>The main problem for me was loading a very heavy library as part of the bundle (there is a 500mb limit for uploading the bundle code).
Instead, I tried to use requirements.txt file so it would download the library directly to the server.<br>
Unfortunately, every time I tried to include the library name in the requirements file, it failed to load it (torch library).</p>
<p>on pythonanywhere server there is a console which allows you to access the virtual environment and simply type</p>
<blockquote>
<p>pip install torch</p>
</blockquote>
<p>which was very useful and comfortable.
I am looking for something similar in AWS beanstalk, so that I could install the library directly instead of relying on the requirements.txt file.</p>
<p>I have been at it for a few days now and can't make any progress.<br>
your help would be much appreciated.</p>
<p>another question,
is it possible to load the venv to Amazon-S3 and then access the folder from the beanstalk environment?</p>
",Dataset Preprocessing & Handling,access beanstalk application venv last week trying upload flask app using aws beanstalk main problem wa loading heavy library part bundle mb limit uploading bundle code instead tried use requirement txt file would download library directly server unfortunately every time tried include library name requirement file failed load torch library pythonanywhere server console allows access virtual environment simply type pip install torch wa useful comfortable looking something similar aws beanstalk could install library directly instead relying requirement txt file day make progress help would much appreciated another question possible load venv amazon access folder beanstalk environment
How to install Specific version of Spacy,"<p>I am trying to install spacy version 2.0.0 it starts downloading, but can not install and give this error.</p>
<pre><code>Found existing installation: cymem 2.0.5
Uninstalling cymem-2.0.5:
  Successfully uninstalled cymem-2.0.5
Running setup.py install for cymem ... error
ERROR: Command errored out with exit status 1:
 command: 'c:\users\taqi\appdata\local\programs\python\python39\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'C:\\Users\\TAQI\\AppData\\Local\\Temp\\pip-install-o890sm67\\cymem_134557a3656d414d8545e96d491823df\\setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'C:\\Users\\TAQI\\AppData\\Local\\Temp\\pip-install-o890sm67\\cymem_134557a3656d414d8545e96d491823df\\setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record 'C:\Users\TAQI\AppData\Local\Temp\pip-record-oa_pr0r9\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\users\taqi\appdata\local\programs\python\python39\Include\cymem'
     cwd: C:\Users\TAQI\AppData\Local\Temp\pip-install-o890sm67\cymem_134557a3656d414d8545e96d491823df\
Complete output (17 lines):
running install
running build
running build_py
creating build
creating build\lib.win-amd64-3.9
creating build\lib.win-amd64-3.9\cymem
copying cymem\about.py -&gt; build\lib.win-amd64-3.9\cymem
copying cymem\__init__.py -&gt; build\lib.win-amd64-3.9\cymem
package init file 'cymem\tests\__init__.py' not found (or not a regular file)
creating build\lib.win-amd64-3.9\cymem\tests
copying cymem\tests\test_import.py -&gt; build\lib.win-amd64-3.9\cymem\tests
copying cymem\cymem.pyx -&gt; build\lib.win-amd64-3.9\cymem
copying cymem\cymem.pxd -&gt; build\lib.win-amd64-3.9\cymem
copying cymem\__init__.pxd -&gt; build\lib.win-amd64-3.9\cymem
running build_ext
building 'cymem.cymem' extension
error: Microsoft Visual C++ 14.0 or greater is required. Get it with &quot;Microsoft C++ Build Tools&quot;: https://visualstudio.microsoft.com/visual-cpp-build-tools/
----------------------------------------
</code></pre>
<p>Rolling back uninstall of cymem
Moving to c:\users\taqi\appdata\local\programs\python\python39\lib\site-packages\cymem-2.0.5.dist-info<br />
from C:\Users\TAQI\AppData\Local\Programs\Python\Python39\Lib\site-packages~ymem-2.0.5.dist-info
Moving to c:\users\taqi\appdata\local\programs\python\python39\lib\site-packages\cymem<br />
from C:\Users\TAQI\AppData\Local\Programs\Python\Python39\Lib\site-packages~ymem
ERROR: Command errored out with exit status 1: 'c:\users\taqi\appdata\local\programs\python\python39\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'C:\Users\TAQI\AppData\Local\Temp\pip-install-o890sm67\cymem_134557a3656d414d8545e96d491823df\setup.py'&quot;'&quot;'; <strong>file</strong>='&quot;'&quot;'C:\Users\TAQI\AppData\Local\Temp\pip-install-o890sm67\cymem_134557a3656d414d8545e96d491823df\setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(<strong>file</strong>) if os.path.exists(<strong>file</strong>) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, <strong>file</strong>, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record 'C:\Users\TAQI\AppData\Local\Temp\pip-record-oa_pr0r9\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\users\taqi\appdata\local\programs\python\python39\Include\cymem' Check the logs for full command output.</p>
",Dataset Preprocessing & Handling,install specific version spacy trying install spacy version start downloading install give error rolling back uninstall cymem moving c user taqi appdata local program python python lib site package cymem dist info c user taqi appdata local program python python lib site package ymem dist info moving c user taqi appdata local program python python lib site package cymem c user taqi appdata local program python python lib site package ymem error command errored exit status c user taqi appdata local program python python python exe u c import io sys setuptools tokenize sys c user taqi appdata local temp pip install sm cymem e df setup py file c user taqi appdata local temp pip install sm cymem e df setup py f tokenize open open file path exists file else io stringio setuptools import setup setup code f read replace r n n f close exec compile code file exec install record c user taqi appdata local temp pip record oa pr r install record txt single version externally managed compile install header c user taqi appdata local program python python include cymem check log full command output
Load custom trained spaCy model,"<p>I am trying to load a spaCy text classification model that I trained previously. After training, the model was saved into the <code>en_textcat_demo-0.0.0.tar.gz</code> file.</p>
<p>I want to use this model in a jupyter notebook, but when I do</p>
<pre><code>import spacy
spacy.load(&quot;spacy_files/en_textcat_demo-0.0.0.tar.gz&quot;)
</code></pre>
<p>I get</p>
<pre><code>OSError: [E053] Could not read meta.json from spacy_files/en_textcat_demo-0.0.0.tar.gz
</code></pre>
<p>What is the correct way to load my model here?</p>
",Dataset Preprocessing & Handling,load custom trained spacy model trying load spacy text classification model trained previously training model wa saved file want use model jupyter notebook get correct way load model
Greek alphabet being converted to unicode in dataframe? any packages to support use of different alphabets?,"<p>I created a column containing Greek words, e.g:</p>
<p><code>vocab&lt;-c(&quot;να&quot;, &quot;το&quot;,&quot;δεν&quot;, &quot;είναι&quot;)</code>
however, when shown in a data frame these words are shown as:</p>
<p>&quot;㯚, tο, deν, eί㯚ι &quot; respectivley.</p>
<p>Are there any packages that help R cope with the use of foreign alphabets and treat them the same as they would the standard English one? I don't really understand why they don't automatically treat these symbols as normal English letters.</p>
<p>Thanks for your help!</p>
",Dataset Preprocessing & Handling,greek alphabet converted unicode dataframe package support use different alphabet created column containing greek word e g however shown data frame word shown de e respectivley package help r cope use foreign alphabet treat would standard english one really understand automatically treat symbol normal english letter thanks help
What is the correct way of encoding a large batch of documents with sentence transformers/pytorch?,"<p>I am having issues encoding a large number of documents (more than a million) with the <a href=""https://www.sbert.net/index.html"" rel=""nofollow noreferrer"">sentence_transformers</a> library.</p>
<p>Given a very similar <a href=""https://rentry.co/zt4a2"" rel=""nofollow noreferrer"">corpus</a> list of strings. When I do:</p>
<pre><code>from sentence_transformers import SentenceTransformer
embedder = SentenceTransformer('msmarco-distilbert-base-v2')
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=False)
    
</code></pre>
<p>After some hours, the process seems to be stuck, as it never finishes and when checking the process viewer nothing is running.</p>
<p>As I am suspicious that this is a ram issue (the GPU board doesn't have enough memory to fit everything in a single step) I tried to split the corpus into batches, transform them into NumPy arrays, and concat them into a single matrix as follows:</p>
<pre><code>from itertools import zip_longest
from sentence_transformers import SentenceTransformer, util
import torch
from loguru import logger
import glob
from natsort import natsorted



def grouper(iterable, n, fillvalue=np.nan):
    args = [iter(iterable)] * n
    return zip_longest(*args, fillvalue=fillvalue)

embedder = SentenceTransformer('msmarco-distilbert-base-v2')

for j, e in enumerate(list(grouper(corpus, 3))):
    try:
#         print('------------------')
        for i  in filter(lambda v: v==v, e):
            corpus_embeddings=embedder.encode(i, convert_to_tensor=False)
            torch.save(corpus_embeddings, f'/Users/user/Downloads/embeddings_part_{j}.npy')
    except TypeError:
        print(j, e)
        logger.debug(&quot;TypeError in batch {batch_num}&quot;, batch_num=j)

l = []
for e in natsorted(glob.glob(&quot;/Users/user/Downloads/*.npy&quot;)):
    l.append(torch.load(e))
    corpus_embeddings = np.vstack(l)
corpus_embeddings
</code></pre>
<p>Nevertheless, the above procedure doesn't seem to work. The reason is that when I try with a small sample of the corpus with and without the batch approach the matrices I get are different for example:</p>
<p>Without batch approach:</p>
<pre><code>array([[-0.6828216 , -0.26541945,  0.31026787, ...,  0.19941986,
         0.02366139,  0.4489861 ],
       [-0.45781   , -0.02955275,  1.0897563 , ..., -0.20077021,
        -0.37821707,  0.2248317 ],
       [ 0.8532193 , -0.13642257, -0.8872398 , ..., -0.57482916,
         0.12760726, -0.66986346],
       ...,
       [-0.04036704,  0.06745373, -0.6010259 , ..., -0.08174597,
        -0.18513843, -0.64744204],
       [-0.30782765, -0.04935509, -0.11624689, ...,  0.10423593,
        -0.14073376, -0.09206307],
       [-0.77139395, -0.08119706,  0.43753916, ...,  0.1653319 ,
         0.06861683, -0.16276269]], dtype=float32)
</code></pre>
<p>With batch approach:</p>
<pre><code>array([[ 0.8532191 , -0.13642241, -0.8872397 , ..., -0.5748289 ,
         0.12760736, -0.6698637 ],
       [ 0.3679317 , -0.21968201,  0.9932826 , ..., -0.86282325,
        -0.04683857,  0.18995859],
       [ 0.23026675,  0.69587034, -0.8116473 , ...,  0.23903558,
         0.413471  , -0.23438476],
       ...,
       [ 0.923319  ,  0.4152724 , -0.3153545 , ..., -0.6863369 ,
         0.01149149, -0.51300013],
       [-0.30782777, -0.04935484, -0.11624689, ...,  0.10423636,
        -0.1407339 , -0.09206269],
       [-0.77139413, -0.08119693,  0.43753892, ...,  0.16533189,
         0.06861652, -0.16276267]], dtype=float32)
</code></pre>
<p>What is the correct way of doing the above batch procedure?</p>
<p><strong>UPDATE</strong></p>
<p>After inspecting the above batch procedure, I found that I was able to get the same matrix output with and without the batching when I set to <code>1</code> the batch size of the above code <code>(enumerate(list(grouper(corpus, 1))))</code>. Therefore, my question is, what is the correct way of applying the encoder to a large set of documents?</p>
",Dataset Preprocessing & Handling,correct way encoding large batch document sentence transformer pytorch issue encoding large number document million sentence transformer library given similar corpus list string hour process seems stuck never finish checking process viewer nothing running suspicious ram issue gpu board enough memory fit everything single step tried split corpus batch transform numpy array concat single matrix follows nevertheless procedure seem work reason try small sample corpus without batch approach matrix get different example without batch approach batch approach correct way batch procedure update inspecting batch procedure found wa able get matrix output without batching set batch size code therefore question correct way applying encoder large set document
"Creating a pandas dataframe using a dataset with hindi, or indic languages","<p>I have the following  [dataset] <a href=""https://www.kaggle.com/disisbig/hindi-text-short-summarization-corpus"" rel=""nofollow noreferrer"">1</a>, I wish to create a dataframe out of it using Pandas(python). How to do it?</p>
<p>I am getting the following error</p>
<pre><code>ParserError                               Traceback (most recent call last)
</code></pre>
<p> in ()
2 import pandas as pd
3
----&gt; 4 df = pd.read_csv(r&quot;/content/drive/MyDrive/DRDO project documents/datasets/article_summary_heading/test.csv&quot;,encoding=&quot;latin-1&quot;)</p>
<p>3 frames
/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py in read(self, nrows)
2155     def read(self, nrows=None):
2156         try:
-&gt; 2157             data = self._reader.read(nrows)
2158         except StopIteration:
2159             if self._first_chunk:</p>
<p>pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.read()</p>
<p>pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._read_low_memory()</p>
<p>pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._read_rows()</p>
<p>pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows()</p>
<p>pandas/_libs/parsers.pyx in pandas._libs.parsers.raise_parser_error()</p>
<p>ParserError: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.</p>
",Dataset Preprocessing & Handling,creating panda dataframe using dataset hindi indic language following dataset wish create dataframe using panda python getting following error import panda pd df pd read csv r content drive mydrive drdo project document datasets article summary heading test csv encoding latin frame usr local lib python dist package panda io parser py read self nrows def read self nrows none try data self reader read nrows except stopiteration self first chunk panda libs parser pyx panda libs parser textreader read panda libs parser pyx panda libs parser textreader read low memory panda libs parser pyx panda libs parser textreader read row panda libs parser pyx panda libs parser textreader tokenize row panda libs parser pyx panda libs parser raise parser error parsererror error tokenizing data c error buffer overflow caught possible malformed input file
Merging Two CSV Files Using Spacy nlp,"<p>I have two CSV files. They have a same column but each of rows in the same column are not unique, like this:</p>
<pre><code>gpo_full.csv:
    Date           hearing_sub_type   topic      Specific_Date
    January,1997   Oversight          weather    January 12,1997
    June,2000      General            life       June 5,2000
    January,1997   General            forest     January 1,1997
    April,2001     Oversight          people     NaN 
    June,2000      Oversight          depressed  June 6,2000
    January,1997   General            weather    January 1,1997
    June,2000      Oversight          depressed  June 5,2000

CAP_cols.csv:
    majortopic   id     Chamber   topic           Date           Specific_Date
    21           79846  1         many forest     January,1997   January 1,1997
    4            79847  2         emotion         June,2000      June 6,2000
    13           79848  1         NaN             May,2001       NaN
    7            79849  2         good life       June,2000      June 5,2000
    21           79850  1         windy           January,1997   January 1,1997
    25           79851  1         rain &amp; cloudy   January,1997   January 12,1997
    6            79852  2         sad &amp; depressed June,2000      June 5,2000
</code></pre>
<p>I want to use three criteria to match these data: Specific_Date, Date, and topic. <br />
First, I want to use &quot;Date&quot; column to group these data. Next, I try to use &quot;Specific_Date&quot; column to narrow down the scope since some data are lost in this column. Finally, I want to use &quot;topic&quot; column by similar words like Spacy and nlp to make sure which rows in gpo_full can be corresponding with a unique row in CAP_cols. <br />
I have tried to use &quot;Date&quot; column to group the data and merge them into JSON file. However, I am trapped in achieving the next step to narrow down the scope by specific date and topic. <br />
My thought for this output would be like:</p>
<pre><code>{
&quot;Date&quot;: &quot;January,1997&quot;,
&quot;Specific_Date&quot;: &quot;January 12,1997&quot;
&quot;similarity&quot;: 0.9
&quot;GPO&quot;: {
    &quot;hearing_sub_type&quot;: &quot;Oversight&quot;,
    and other columns
}
&quot;CAP&quot;: {
    &quot;majortopic&quot;: &quot;25&quot;,
    &quot;id&quot;: &quot;79851&quot;,
    &quot;Chamber&quot;: &quot;1&quot;
}
},
{
&quot;Date&quot;: &quot;January,1997&quot;,
&quot;Specific_Date&quot;: &quot;January 1,1997&quot;
&quot;similarity&quot;: 0.89
&quot;GPO&quot;: {
    &quot;hearing_sub_type&quot;: &quot;General&quot;,
    and other columns
}
&quot;CAP&quot;: {
    &quot;majortopic&quot;: &quot;21&quot;,
    &quot;id&quot;: &quot;79846&quot;,
    &quot;Chamber&quot;: &quot;1&quot;
}
and similar for others}
</code></pre>
<p>My specific question is how to use spacy to calculate similarity within the same date and to match the most similar row.\</p>
<p>My code I wrote is:</p>
<pre><code>from pprint import pprint
import spacy

with open('gpo_full.csv') as gpo_file:
    gpo_reader = csv.DictReader(gpo_file)
    
    output = {}
    for gpo_row in gpo_reader:
        date = gpo_row[&quot;Date&quot;]
        held_date = gpo_row[&quot;held_date&quot;]
        
        if date not in output:
            output[date] = {&quot;Date&quot;: date, &quot;Specific_Date&quot;: held_date, &quot;Similarity&quot;: [], &quot;GPO&quot;: [], &quot;CAP&quot;: []}
        
        output_gpo = [gpo_row[&quot;title&quot;]]
        for sentence_gpo in output_gpo:
            gpo = nlp(sentence_gpo)
            
            with open(&quot;cap_colsNew.csv&quot;) as cap_file:
                cap_reader = csv.DictReader(cap_file)
                    
                for cap_row in cap_reader:
                    if date == cap_row[&quot;Date&quot;]:
                        del cap_row[&quot;Date&quot;]
                    if held_date == cap_row[&quot;held_date&quot;]:
                        del cap_row[&quot;held_date&quot;]
                        
                    output_cap = [cap_row[&quot;description&quot;]] 
                    for sentence_cap in output_cap:
                        cap = nlp(sentence_cap)
                        
                        if gpo.similarity(cap) &gt; 0.8:
                            output[date][&quot;Similarity&quot;].append(gpo.similarity(cap))
                            
                            output[date][&quot;CAP&quot;].append(cap_row)
                    
            if not output[date][&quot;CAP&quot;]:
                cap_row = {fieldname: None for fieldname in gpo_reader.fieldnames
                          if fieldname != &quot;Date&quot;}
                output[date][&quot;CAP&quot;].append(cap_row)
        
        if date in output:
            del gpo_row[&quot;Date&quot;]
            if gpo.similarity(cap) &gt; 0.8:
                output[date][&quot;GPO&quot;].append(gpo_row)
                
output = output.values()
pprint(output, sort_dicts = 0)
</code></pre>
<p>Could any find out the problem of my code? Any help would be appreciated!</p>
",Dataset Preprocessing & Handling,merging two csv file using spacy nlp two csv file column row column unique like want use three criterion match data specific date date topic first want use date column group data next try use specific date column narrow scope since data lost column finally want use topic column similar word like spacy nlp make sure row gpo full corresponding unique row cap col tried use date column group data merge json file however trapped achieving next step narrow scope specific date topic thought output would like specific question use spacy calculate similarity within date match similar row code wrote could find problem code help would appreciated
opening PDF from a webpage in R,"<p>I'm trying to practice text analysis with the Fed FOMC minutes.</p>
<p>I was able to obtain all links to the appropriate pdf files from the link below.
<a href=""https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm"" rel=""nofollow noreferrer"">https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm</a></p>
<p>I tried download.file(<a href=""https://www.federalreserve.gov/monetarypolicy/files/fomcminutes20160316.pdf,%221.pdf%22"" rel=""nofollow noreferrer"">https://www.federalreserve.gov/monetarypolicy/files/fomcminutes20160316.pdf,&quot;1.pdf&quot;</a>).</p>
<p>The download was successful; however, when I click on the downloaded file, it outputs &quot;There was an error opening this document. The file is damaged and could not be repaired.&quot;
What are some ways to fix this? Is this a way of preventing web scraping on Fed's side?</p>
<p>I have 44 links(pdf files) to download and read in R. Is there a way to do this without physically downloading the files?</p>
",Dataset Preprocessing & Handling,opening pdf webpage r trying practice text analysis fed fomc minute wa able obtain link appropriate pdf file link tried download file download wa successful however click downloaded file output wa error opening document file damaged could repaired way fix way preventing web scraping fed side link pdf file download read r way without physically downloading file
Pre-load python libraries in Nodejs,"<p>I have a web app with JavaScript front end and python back end. The user writes a phrase, clicks a button, python parses the sentence (natural language processing) and sends data back to the client. I use SpaCy for natural language processing. SpaCy takes a long time to load, so I wanted to know if I could pre-load SpaCy in NodeJS when I start the server (vs. importing SpaCy every time I spawn the python file - which is what I do now, See code below).
Thanks!</p>
<p>JavaScript code:</p>
<pre><code>io.on('connection', (socket) =&gt; {
    socket.on('run_command_request', async (data) =&gt; {
        let output_str = await run_python_command(data);
        socket.emit('run_command_complete');
    });
});


async function run_python_command(data) {
    var spawn = require('child_process').spawn;
    var py = spawn(python_executable, ['./run_command.py']); //PYTHON FILE CALLED EVERY TIME A USER CLICKS A BUTTON
    var python_output_string ='';
    py.stdin.write(JSON.stringify(data));
    py.stdin.end();
    return new Promise((res, rej) =&gt; {
        py.stdout.on('end', function() {
            res(python_output_string);
        });
    });
}
</code></pre>
<p>Python code:</p>
<pre><code>import spacy #############   THIS STEP TAKES FOREVER    #############   
nlp = spacy.load(&quot;en_core_web_sm&quot;) 
doc = nlp(tc)
verb = [token.lemma_ for token in doc if token.pos_ == &quot;VERB&quot;]
print(verb)
</code></pre>
",Dataset Preprocessing & Handling,pre load python library nodejs web app javascript front end python back end user writes phrase click button python par sentence natural language processing sends data back client use spacy natural language processing spacy take long time load wanted know could pre load spacy nodejs start server v importing spacy every time spawn python file see code thanks javascript code python code
How to save Farsi text in csv file using python?,"<p>I was trying to save my dataset in a CSV file with the following script:</p>
<pre><code>with open(data_path+'Furough.csv', 'w',encoding=&quot;utf-8&quot;) as f0:
    df = pd.DataFrame(columns=['title','poem','year'])
    for f in onlyfiles:
        poem=[]
        title=&quot;&quot;
        year=0
        with open(mypath+f,&quot;r&quot;,encoding=&quot;utf-8&quot;) as f1:
            for line in f1:
                if line.__contains__(&quot;TIMESTAMP&quot;):
                    year=int(line[12:15])
                    continue
                if line.__contains__('TITLE'):
                    title=line[7:]
                if line!=&quot;&quot;:
                    poem.append(line)
            df = df.append({
                            'title': title,
                            'poem':poem,
                            'year': int(float(year))
                            }, ignore_index=True)
            df.to_csv(f0, index=False,encoding='utf-8-sig')
</code></pre>
<p>but the result is confusing, write some unknown chars to CSV file instead of Farsi chars:
Can anyone help me?</p>
<p>I want to write all these files in a CSV:
<a href=""https://i.sstatic.net/txBIH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/txBIH.png"" alt=""enter image description here"" /></a>
example of what I have in one of them and want to write:</p>
<pre><code>[V_START] بر پرده‌های درهم امیال سرکشم [HEM]
نقش عجیب چهرۀ یک ناشناس بود [V_END]
[V_START] نقشی ز چهره‌ای که چو می‌جستمش به شوق [HEM]
پیوسته می‌رمید و بمن رخ نمی‌نمود [V_END]

[V_START] یک شب نگاه خستۀ مردی به روی من [HEM]
لغزید و سست گشت و همان ‌جا خموش ماند [V_END]
[V_START] تا خواستم که بگسلم این رشتۀ نگاه [HEM]
قلبم تپید و باز مرا سوی او کشاند [V_END]
</code></pre>
<p>but result:</p>
<p><a href=""https://i.sstatic.net/eNdpI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eNdpI.png"" alt=""enter image description here"" /></a></p>
",Dataset Preprocessing & Handling,save farsi text csv file using python wa trying save dataset csv file following script result confusing write unknown char csv file instead farsi char anyone help want write file csv example one want write result
Extract specific keywords from a sentence using NLP python (I have dictionary of approx. 4 million keywords),"<p>I am learning NLP and have basic knowledge.</p>
<p>I have a CSV file which contains approx. million rows which has 4 important columns.</p>
<p><strong>Dataset(Dummy data, not the actual set):</strong></p>
<p><a href=""https://i.sstatic.net/Mv3Zv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Mv3Zv.png"" alt=""enter image description here"" /></a></p>
<pre><code>Sentence: BLA BLA ABC44, FUNNY CAT TEST COLOR BE SAFE, STAY HOME
Answer: Style_no: ABC44, Style_name: FUNNY CAT, Color_no: 200, Color_name: TEST COLOR
</code></pre>
<p>In the dataset each row is unique. In my example, 3 properties are there in the sentence, It is possible we may have 2 properties, in that case it should search and return all the columns.</p>
<p>I am trying to build this with Spacy, I am looking for guidance/algorithm/code.
Feel free to suggest me tutorial as well.</p>
",Dataset Preprocessing & Handling,extract specific keywords sentence using nlp python dictionary approx million keywords learning nlp basic knowledge csv file contains approx million row ha important column dataset dummy data actual set dataset row unique example property sentence possible may property case search return column trying build spacy looking guidance algorithm code feel free suggest tutorial well
Is there a way to count the number of words for each row in a specific excel column?,"<p>I have a .csv file saved as an object called &quot;ProjectTwo&quot; that has the following columns: &quot;song&quot;, &quot;artist&quot;, &quot;year&quot;, &quot;lyrics&quot;, with lyrics being the actual lyrics to the song. Here's a snapshot of what the dataset looks like:
<a href=""https://i.sstatic.net/BakK0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BakK0.png"" alt=""enter image description here"" /></a></p>
<p>I have taken the lyrics of every song and lemmatized the words to seperate them using nltk.</p>
<pre><code>lyrics_all = ProjectTwo[&quot;lyrics&quot;]
str1 = ''.join(lyrics_all)
nltk_tokenized = nltk.word_tokenize(str1)
lemmatizer = nltk.stem.WordNetLemmatizer()
lemmatizer_results = [lemmatizer.lemmatize(t) for t in nltk_tokenized]
print(lemmatizer_results)
</code></pre>
<p>Some results:</p>
<p>['I', &quot;'m&quot;, 'crazy', 'for', 'my', 'baby', 'But', 'my', 'baby', 'she', 'do', &quot;n't&quot;, 'love', 'me', 'I', &quot;'m&quot;, 'so', 'lonely', 'babe', 'I', &quot;'m&quot;, 'so', 'lonely', 'babe', 'No', 'I', 'ca', &quot;n't&quot;, 'live', 'without', 'you', 'babe', 'I', &quot;'m&quot;, 'so', 'lonely', 'babe', 'I', 'woke', 'up', 'last', ...]</p>
<p>I want to find a way to count every word to each song so that I can plot the length of songs (number of words) as a function of year and then add a regression line, but I can't figure out how to count the words of each song.</p>
",Dataset Preprocessing & Handling,way count number word row specific excel column csv file saved object called projecttwo ha following column song artist year lyric lyric actual lyric song snapshot dataset look like taken lyric every song lemmatized word seperate using nltk result crazy baby baby n love lonely babe lonely babe ca n live without babe lonely babe woke last want find way count every word song plot length song number word function year add regression line figure count word song
division by zero in calculating TF-IDF algorithm for keyword-extraction,"<p>I wrote a code based on the TF-IDF algorithm to extract keywords from a very large text.
The problem is that I keep getting the division by zero error. When I debug my code, everything is working perfectly. As soon as I make the text shorter to contains the word that causes the problem, it works. So, I assume that it's a memory problem.</p>
<p>I thought maybe I could read the big text file in chunks (1KB) instead of reading the whole document in the first place. Unfortunately, it does not work. what should I do?
(I am using pycharm on windows)</p>
<p>I am a beginner in programming, python, and NLP domain. Therefore, I really appreciate it if you could help me here.</p>
<pre><code>if __name__ == &quot;__main__&quot;:
 with open('spli.txt') as f:
    for piece in read_in_chunks(f):
        #print(piece)
        piece = piece.lower()
        no_punc_words, all_words = text_split(piece)
        no_punc_words, all_words = rm_stop_word(no_punc_words, all_words)
        no_punc_words_freq, all_words_freq = calc_freq(no_punc_words, all_words)
        tf_score = calc_tf_score(no_punc_words_freq)
        idf_score = calc_idf_score(no_punc_words_freq, all_words_freq, piece)
        tf_idf_score = {}
        for k in tf_score:
           tf_idf_score[k] = tf_score[k] * idf_score[k]
           #print(final_score)
    final_tf_idf = {}
    for scores in tf_idf_score:
        final_tf_idf += tf_idf_score
        print(final_tf_idf)
</code></pre>
",Dataset Preprocessing & Handling,division zero calculating tf idf algorithm keyword extraction wrote code based tf idf algorithm extract keywords large text problem keep getting division zero error debug code everything working perfectly soon make text shorter contains word cause problem work assume memory problem thought maybe could read big text file chunk kb instead reading whole document first place unfortunately doe work using pycharm window beginner programming python nlp domain therefore really appreciate could help
Problem analyzing a doc column in a df with spaCy nlp,"<p>After using a amazon review scraper to build this data frame, I called on nlp in order to tokenize and create a new column containing the processed reviews as 'docs'</p>
<p>However, now I am trying to create a pattern in order to analyzing the reviews in the doc column, but I keep getting know matches, which makes me thinking I'm missing one more pre-processing step, or perhaps not pointing the matcher in the right direction.</p>
<p>While the following code executes without any errors, I receive a matches list with 0 - even though I know the word exists in the doc column. The docs for spaCy are still a tad slim, and I'm not too sure the matcher.add is correct, as the one specific in the tutorial</p>
<pre><code>matcher.add(&quot;Name_of_List&quot;, None, pattern)
</code></pre>
<p>returns an error saying that only 2 arguments are required for this class.</p>
<ul>
<li>source -- <a href=""https://course.spacy.io/en/chapter1"" rel=""nofollow noreferrer"">https://course.spacy.io/en/chapter1</a></li>
</ul>
<p>Question: What do I need to change to accurately analyze the df doc column for the pattern created?</p>
<p>Thanks!</p>
<p>Full code:</p>
<pre><code>import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

import spacy
from spacy.matcher import Matcher

nlp = spacy.load('en_core_web_md')


df = pd.read_csv('paper_towel_US.csv')

#calling on NLP to return processed doc for each review
df['doc'] = [nlp(body) for body in df.body]


# Sum the number of tokens in each Doc
df['num_tokens'] = [len(token) for token in df.doc]



#calling matcher to create pattern
matcher = Matcher(nlp.vocab)


pattern =[{&quot;LEMMA&quot;: &quot;love&quot;},
          {&quot;OP&quot;:&quot;+&quot;}
          
          ]
matcher.add(&quot;QUALITY_PATTERN&quot;, [pattern])


def find_matches(doc):
    spans = [doc[start:end] for _, start, end in matcher(doc)]
    for span in spacy.util.filter_spans(spans):
        return ((span.start, span.end, span.text))
    
    
df['doc'].apply(find_matches)
    
</code></pre>
<p>df sample for reproduction via <code>df.iloc[596:600, :].to_clipboard(sep=',')</code></p>
<pre><code>,product,title,rating,body,doc,num_tokens
596,Amazon.com: Customer reviews: Bamboo Towels - Heavy Duty Machine Washable Reusable Rayon Towels - One roll replaces 6 months of towels! 1 Pack,Awesome!,5,Great towels!,Great towels!,3
597,Amazon.com: Customer reviews: Bamboo Towels - Heavy Duty Machine Washable Reusable Rayon Towels - One roll replaces 6 months of towels! 1 Pack,Good buy!,5,Love these,Love these,2
598,Amazon.com: Customer reviews: Bamboo Towels - Heavy Duty Machine Washable Reusable Rayon Towels - One roll replaces 6 months of towels! 1 Pack,Meh,3,&quot;Does not clean countertop messes well. Towels leave a large residue. They are durable, though&quot;,&quot;Does not clean countertop messes well. Towels leave a large residue. They are durable, though&quot;,18
599,Amazon.com: Customer reviews: Bamboo Towels - Heavy Duty Machine Washable Reusable Rayon Towels - One roll replaces 6 months of towels! 1 Pack,Exactly as Described. Packaged Well and Mailed Promptly,4,Exactly as Described. Packaged Well and Mailed Promptly,Exactly as Described. Packaged Well and Mailed Promptly,9
</code></pre>
",Dataset Preprocessing & Handling,problem analyzing doc column df spacy nlp using amazon review scraper build data frame called nlp order tokenize create new column containing processed review doc however trying create pattern order analyzing review doc column keep getting know match make thinking missing one pre processing step perhaps pointing matcher right direction following code executes without error receive match list even though know word exists doc column doc spacy still tad slim sure matcher add correct one specific tutorial return error saying argument required class source question need change accurately analyze df doc column pattern created thanks full code df sample reproduction via
How encode text can be converted to main text (without special character created by encoding),"<p>I am going to extract text from a series of PDF files to do Topic Modeling. After extracting text from PdF files, I am going to save the text of each PDF file in a .txt file or .doc file. To do this, I had an error that I should add .encode('utf-8') for saving extracted text in a .txt file. So, I added <code>txt = str(txt.encode('utf-8'))</code>. The problem is reading the .txt files, when I read the .txt files, they have special characters due to UTF-8, I don't know how I can have the main text without that characters. I applied to decode but it didn't work.</p>
<p>I applied another approach to avoid saving in .txt format, I was going to save the extracted text in a data frame, but I found that the few first pages were saved in data frame!</p>
<p>I would appreciate it if you could share your solutions to read from the .txt file and removing characters relating to encoding ('utf-8') and how I can save the extracted text in a data frame.</p>
<pre><code>import pdfplumber
import pandas as pd
import  codecs

txt = ''

with pdfplumber.open(r'C:\Users\thmag\3rdPaperLDA\A1.pdf') as pdf:
    pages = pdf.pages
    for i, pg in enumerate (pages):
            txt += pages [i].extract_text()
        
print (txt)

data = {'text': [txt]}
df = pd.DataFrame(data)


####write in .txt file
text_file = open(&quot;Test.txt&quot;, &quot;wt&quot;)
txt = str(txt.encode('utf-8'))
n = text_file.write(txt)
text_file.close()

####read from .txt file
with codecs.open('Test.txt', 'r', 'utf-8') as f:
    for line in f:
        print (line)
</code></pre>
",Dataset Preprocessing & Handling,encode text converted main text without special character created encoding going extract text series pdf file topic modeling extracting text pdf file going save text pdf file txt file doc file error add encode utf saving extracted text txt file added problem reading txt file read txt file special character due utf know main text without character applied decode work applied another approach avoid saving txt format wa going save extracted text data frame found first page saved data frame would appreciate could share solution read txt file removing character relating encoding utf save extracted text data frame
How to get verb forms of a verb in Spanish language in Python?,"<p>I am working on a project where I have to plug in verb forms of different Spanish verbs into a text. Is there any library that will allow me to do this?
Or is there any CSV file or pdf which I can read into my code and get the verb forms from there? Any help would be appreciated, thanks.</p>
",Dataset Preprocessing & Handling,get verb form verb spanish language python working project plug verb form different spanish verb text library allow csv file pdf read code get verb form help would appreciated thanks
Removing from a text sequences of items from a list,"<p>I'm conducting research on code-switching. I have a collection of bilingual Polish text messages with English code-switches (say, corpus A) as well as an English dictionary (also a list, corpus B). I wanted to extract from corpus A all instances of words from corpus B - this way I could see which English words appeared in the bilingual corpus. This is the code I used to create a list of these common words (it's far from elegant, but I'm a novice, so don't be too harsh on me haha):</p>
<pre><code>
intersection=common.intersection(corpusB)

commonlist=list(intersection)

with open(&quot;commonlist.txt&quot;,&quot;w&quot;) as z:

    print(commonlist, file=z)
</code></pre>
<p>However, I noticed that a large portion of my data is skewed because it contains words that are irrelevant to my research. For example, some text messages had large blocks of English text (e.g. copy-pasted paragraphs of English articles - so not really code-switching but quotations). So I'd like to get rid of all of these large blocks of English text from corpus A.</p>
<p>What I thought I should do was locate and delete any text messages that include, say, five English words in a sequence (these would be my big chunks of English text). In other words, I want to scan corpus A for messages that include four adjacent words from corpus B. How can I go about doing that?</p>
<p>(I also have .csv files of both corpora, might be more useful perhaps?)</p>
",Dataset Preprocessing & Handling,removing text sequence item list conducting research code switching collection bilingual polish text message english code switch say corpus well english dictionary also list corpus b wanted extract corpus instance word corpus b way could see english word appeared bilingual corpus code used create list common word far elegant novice harsh haha however noticed large portion data skewed contains word irrelevant research example text message large block english text e g copy pasted paragraph english article really code switching quotation like get rid large block english text corpus thought wa locate delete text message include say five english word sequence would big chunk english text word want scan corpus message include four adjacent word corpus b go also csv file corpus might useful perhaps
How to store data consisting of articles and dates and other attributes for use in python,"<p>I am working with large textual data  (articles containing words, symbols,escape characters,line breaks etc). Each article also consists of attributes like date  , author etc.</p>
<p>It will be used in python for NLP purposes . What is the best way to store this data such that it can be read efficiently from disk.</p>
<h2>EDIT :</h2>
<p>I have loaded the data as a pandas dataframe in python</p>
<p>Storing as a csv results in corruption due to line breaks(\n) in the text making the data unusable.</p>
<p>storing as JSON is not working due to encoding problems.</p>
",Dataset Preprocessing & Handling,store data consisting article date attribute use python working large textual data article containing word symbol escape character line break etc article also consists attribute like date author etc used python nlp purpose best way store data read efficiently disk edit loaded data panda dataframe python storing csv result corruption due line break n text making data unusable storing json working due encoding problem
Building text-similarity time series in a corpus of tweets,"<p>I want to measure the evolution of text similarity over time. My data frame consists on a column for tweet identifiers ( <code>id</code>) a column for dates with a daily frequency (<code>date</code>) and a column with the tidy text of the tweets (<code>clean_text</code>).</p>
<p>Here is a brief <code>repex</code> with some actual cleaned tweets:</p>
<pre><code>final &lt;- data.frame(id=1:5,Date=c(as.Date(&quot;2020-12-26&quot;),as.Date(&quot;2020-12-26&quot;),as.Date(&quot;2020-12-27&quot;),
                    as.Date(&quot;2020-12-27&quot;),as.Date(&quot;2020-12-27&quot;)),
              clean_text = c(&quot;americans died covid nfebruary people couple days&quot;,
                                        &quot;cops crush peoples necks death eric garner&quot;,  
                                        &quot;video clip tells george floyd resist arrest earlier claimed police officer&quot;, 
                                        &quot;black americans terrible daily dangers outdoor spaces subjected unwarranted suspicion&quot;, 
                                         &quot;announcement waiting minneapolis police officer derek chauvin charged manslaug&quot;))
</code></pre>
<p>Since I want something that permits me to measure how similar/disimilar text became with time, I thought about using some similarity measures (i.e frequency, cosine similarity...), via <code>textstat_simil</code> from <code>quanteda</code>.</p>
<p>Here is my try:</p>
<pre><code>require(quanteda)            
start &lt;- as.Date(&quot;2020-12-26&quot;,format=&quot;%Y-%m-%d&quot;)
    end   &lt;- as.Date(&quot;2020-12-27&quot;,format=&quot;%Y-%m-%d&quot;)
    
    theDate &lt;- start
    i=1
    
    similarity&lt;-data.frame(matrix(NA, nrow = as.numeric(end+1-start), ncol = 1))
    #value&lt;-vector(mode=&quot;numeric&quot;)
    colnames(similarity)&lt;-c(&quot;value&quot;)
    while (theDate &lt;= end){
      if (nrow(subset(final,final$Date==theDate))&gt;1){
        corp &lt;- corpus(subset(final,final$Date==theDate),
                       docid_field = &quot;id&quot;,
                       text_field = &quot;clean_text&quot;)
        a&lt;-as.matrix(dfm(corp) %&gt;%
                       textstat_simil())
        
            similarity$value[i]&lt;-sum(a[lower.tri(a)])/length(corp)
      }else{
        similarity$value[i]&lt;-0
      } 
      i&lt;-i+1
      theDate &lt;- theDate + 1 
      
    }
</code></pre>
<p>In this code, I am summing over the lower triangular elements of the correlation matrix <code>a</code> and dividing by the number of tweets to get a &quot;mean&quot; of the correlation over the tweets. I believe this is the problematic step, since, when I apply this function to my corpus of tweets the similarity drops precisely in the moments where tweets are very related.</p>
<p>Also I must say that my goal is to obtain a time series with a column for a measure of similarity and another for dates so it can be plotted. Maybe there is a simpler npl method to track this?</p>
<p>edit: Just realised that the correlation entries in matrix <code>a</code> are negative. Not sure how to interpret that or whether there is an error in the coding then?</p>
",Dataset Preprocessing & Handling,building text similarity time series corpus tweet want measure evolution text similarity time data frame consists column tweet identifier column date daily frequency column tidy text tweet brief actual cleaned tweet since want something permit measure similar disimilar text became time thought using similarity measure e frequency cosine similarity via try code summing lower triangular element correlation matrix dividing number tweet get mean correlation tweet believe problematic step since apply function corpus tweet similarity drop precisely moment tweet related also must say goal obtain time series column measure similarity another date plotted maybe simpler npl method track edit realised correlation entry matrix negative sure interpret whether error coding
error &quot;Unable to load vocabulary from file&quot; when loading spacy &quot;fr_dep_news_trf&quot; model,"<p>I can't load the fr_dep_news_trf model with spacy.</p>
<pre><code>nlp = spacy.load('fr_dep_news_trf')
</code></pre>
<p>It raises a &quot;Unable to load vocabulary from file&quot; error.</p>
<p>The model has been installed with the command</p>
<pre><code>python -m spacy download fr_dep_news_trf
</code></pre>
<p>and install was successful</p>
<p>Version info :
spaCy version : 3.0.6 / Python version   3.7.8 / Pipelines fr_dep_news_trf (3.0.0)</p>
",Dataset Preprocessing & Handling,error unable load vocabulary file loading spacy fr dep news trf model load fr dep news trf model spacy raise unable load vocabulary file error model ha installed command install wa successful version info spacy version python version pipeline fr dep news trf
spaCy: Is it possible to convert json format (with BILUO scheme) files to list format that is used for training in Python?,"<p>I would like to do some evaluation of spaCy's pretrained models with the wikiner datasets. However, these datasets are in json format, using the BILUO annotation scheme. I know that I can do evaluation in the command-line interface, but I would like to do it in the Python interpreter instead, which requires a different data format, as shown below. </p>

<pre><code>TRAIN_DATA = [(""Dogs are loyal"", {'entities': [(0, 4, 'ANIMAL)]})]
</code></pre>

<p>I wonder if there is a way to convert the BILUO scheme json formatted data into the format below. OR alternatively, is it possible to directly evaluate data that is in json format (e.g., I could read json files into the Python interpreter.) </p>

<p>Thanks!</p>

<p><strong>EDIT</strong>: Added sample json data set</p>

<pre><code>[
  {
    ""id"":0,
    ""paragraphs"":[
      {
        ""sentences"":[
          {
            ""tokens"":[
              {
                ""orth"":""Zum"",
                ""tag"":""-"",
                ""ner"":""O""
              },
              {
                ""orth"":""1."",
                ""tag"":""-"",
                ""ner"":""O""
              },
              {
                ""orth"":""Januar"",
                ""tag"":""-"",
                ""ner"":""O""
              },
              {
                ""orth"":""1994"",
                ""tag"":""-"",
                ""ner"":""O""
              },
              {
                ""orth"":""wird"",
                ""tag"":""-"",
                ""ner"":""O""
              },
              {
                ""orth"":""Ruppendorf"",
                ""tag"":""-"",
                ""ner"":""U-LOC""
              },
              {
                ""orth"":""nach"",
                ""tag"":""-"",
                ""ner"":""O""
              },
              {
                ""orth"":""H\u00f6ckendorf"",
                ""tag"":""-"",
                ""ner"":""U-LOC""
              },
              {
                ""orth"":""eingemeindet"",
                ""tag"":""-"",
                ""ner"":""O""
              },
              {
                ""orth"":""."",
                ""tag"":""-"",
                ""ner"":""O""
              }
            ]
          }
        ]
      }
    ]
  },
</code></pre>
",Dataset Preprocessing & Handling,spacy possible convert json format biluo scheme file list format used training python would like evaluation spacy pretrained model wikiner datasets however datasets json format using biluo annotation scheme know evaluation command line interface would like python interpreter instead requires different data format shown wonder way convert biluo scheme json formatted data format alternatively possible directly evaluate data json format e g could read json file python interpreter thanks edit added sample json data set
Extracting abstract nouns and adjectives from a string in Python,"<p>I am working on a data frame that has one of the columns containing feedback text. It has been cleaned.
All I need to know is how to extract the abstract nouns and adjectives from the string.</p>
<p>Here's a sample of the cleaned text that I have:</p>
<p><a href=""https://i.sstatic.net/wtkY6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wtkY6.png"" alt=""enter image description here"" /></a></p>
<p>The output must contain only the abstract nouns and the adjectives from each feedback.</p>
<p>For instance, the feedback is:</p>
<pre><code>&quot;smells good also nice taste in love with it&quot;
</code></pre>
<p>The output should be:</p>
<pre><code>good nice love
</code></pre>
<p>I tried using nltk pos tagger and the textblob lexicon. I could extract all the adjectives using textblob but in the case of nouns, it is tagging all the nouns. I am not able to separate out just the abstract nouns, like 'love' in the example mentioned.</p>
",Dataset Preprocessing & Handling,extracting abstract noun adjective string python working data frame ha one column containing feedback text ha cleaned need know extract abstract noun adjective string sample cleaned text output must contain abstract noun adjective feedback instance feedback output tried using nltk po tagger textblob lexicon could extract adjective using textblob case noun tagging noun able separate abstract noun like love example mentioned
Counting only &quot;longest&quot; occurrence of an N-gram using countVectorizer,"<p>I want to use a pre-defined vocabulary/lexicon from Picault &amp; Renault (link to public CSV file <a href=""https://stackoverflow.com"">here</a>) and fit it to sklearn's countVectorizer to eventually transform my own corpus of words into a count matrix. However, as the lexicon includes up to 9 n-grams, I only want CountVectorizer to count the highest occurrence of the sequence of n-grams. As can be seen in the CSV file, it contains the sequential n-grams and a column with the <em>n</em> of the corresponding n-gram, but no grouping variable that groups the set of n-grams together.</p>
<p>So as a min. ReprEx, I define the vocabulary and my corpus as follows:</p>
<pre><code>my_vocab = [
'ab',
'ab market',
'ab market enhanc',
'ab market enhanc function',
'about',
'about econom']

my_corpus = ['ab market enhanc','ab','about econom']
</code></pre>
<p>Then define the countVectorizer and use it to transform my corpus:</p>
<pre><code>my_vectorizer = CountVectorizer(vocabulary=my_vocab, ngram_range=(1,4))

my_transformed_corpus = my_vectorizer.transform(my_corpus)
print(my_transformed_corpus)
</code></pre>
<p>Current output:</p>
<pre><code> (0, 0) 1
 (0, 1) 1
 (0, 2) 1
 (1, 0) 1
 (2, 4) 1
 (2, 5) 1
</code></pre>
<p>Desired output:</p>
<pre><code> (0, 0) 1
 (0, 1) 0
 (0, 2) 1
 (1, 0) 1
 (2, 4) 0
 (2, 5) 1
</code></pre>
<p>As my actual corpus and resulting sparse matrix is quite large, I figure I'd ideally need a solution that doesn't require iterating over the sparse matrix. But since it is a one-off computation only, I wouldn't mind if that is the only/easiest solution.</p>
<p><strong>Important note:</strong> <em>not every sequence of n-grams in the lexicon has a length of 9, but may be shorter (but may only be up to and including 9)</em></p>
",Dataset Preprocessing & Handling,counting longest occurrence n gram using countvectorizer want use pre defined vocabulary lexicon picault renault link public csv file href fit sklearn countvectorizer eventually transform corpus word count matrix however lexicon includes n gram want countvectorizer count highest occurrence sequence n gram seen csv file contains sequential n gram column em n corresponding n gram grouping variable group set n gram together min reprex define vocabulary corpus follows define countvectorizer use transform corpus current output desired output actual corpus resulting sparse matrix quite large figure ideally need solution require iterating sparse matrix since one computation mind easiest solution important note every sequence n gram lexicon ha length may shorter may including
How do I convert this print statement into a data frame? Python NLP LSA topics,"<p>I need to add these LSA topics to each corresponding topic in my data frame. How can I get this print statement output in a data frame?</p>
<p>--&gt; I am trying to get a data frame with the topic numbers and their corresponding keywords in a different column.</p>
<pre><code># most important words for each topic
vocab = vect.get_feature_names()

for i, comp in enumerate(lsa_model.components_):
    vocab_comp = zip(vocab, comp)
    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:3]
    print(&quot;Topic &quot;+str(i)+&quot;: &quot;)
    for t in sorted_words:
        print(t[0],end=&quot; &quot;)
    print(&quot;\n&quot;)
</code></pre>
<p>topic 1:
xxx yyy zzz
.
.
.
Topic 8:
fddd dddd dsdsd</p>
<p>Topic 9:
akah ahkha ahkha</p>
",Dataset Preprocessing & Handling,convert print statement data frame python nlp lsa topic need add lsa topic corresponding topic data frame get print statement output data frame trying get data frame topic number corresponding keywords different column topic xxx zzz topic fddd dddd dsdsd topic akah ahkha ahkha
How to parse fixed free text using R,"<p>I have the following free text:</p>
<pre><code>#################################################################
# If you used AutoDock Vina in your work, please cite:          #
#                                                               #
# O. Trott, A. J. Olson,                                        #
# AutoDock Vina: improving the speed and accuracy of docking    #
# with a new scoring function, efficient optimization and       #
# multithreading, Journal of Computational Chemistry 31 (2010)  #
# 455-461                                                       #
#                                                               #
# DOI 10.1002/jcc.21334                                         #
#                                                               #
# Please see http://vina.scripps.edu for more information.      #
#################################################################

WARNING: The search space volume &gt; 27000 Angstrom^3 (See FAQ)
Detected 8 CPUs
Reading input ... done.
Setting up the scoring function ... done.
Analyzing the binding site ... done.
Using random seed: -1553787135
Performing search ... done.
Refining results ... done.

mode |   affinity | dist from best mode
     | (kcal/mol) | rmsd l.b.| rmsd u.b.
-----+------------+----------+----------
   1         -5.9      0.000      0.000
   2         -5.7     22.945     25.492
   3         -5.5      1.426      2.046
   4         -5.5     23.669     25.616
   5         -5.4     25.783     29.152
   6         -5.3     21.146     23.357
   7         -5.2     20.323     22.545
   8         -5.2     23.864     26.064
   9         -5.1     23.422     26.585
Writing output ... done.
</code></pre>
<p>The text is fixed only the value under the table above changed.
What I want to do is to extract the table and keep it as a data frame.</p>
<pre><code>mode |   affinity | dist from best mode
     | (kcal/mol) | rmsd l.b.| rmsd u.b.
-----+------------+----------+----------
   1         -5.9      0.000      0.000
   2         -5.7     22.945     25.492
   3         -5.5      1.426      2.046
   4         -5.5     23.669     25.616
   5         -5.4     25.783     29.152
   6         -5.3     21.146     23.357
   7         -5.2     20.323     22.545
   8         -5.2     23.864     26.064
   9         -5.1     23.422     26.585
</code></pre>
<p>I have hundreds of such files which I need to parse.
Is there a way or package in R that can do that?</p>
",Dataset Preprocessing & Handling,parse fixed free text using r following free text text fixed value table changed want extract table keep data frame hundred file need parse way package r
How to find NLP words count and plot it?,"<p>I am doing some NLP work</p>
<p>my original dataframe is <code>df_all</code></p>
<pre><code>Index    Text
1        Hi, Hello, this is mike, I saw your son playing in the garden...
2        Besides that, sometimes my son studies math for fun...
3        I cannot believe she said that. she always says such things...
</code></pre>
<p>I converted my texts to BOW data frame</p>
<p>so my dataframe <code>df_BOW</code> looks like this now</p>
<pre><code>Index    Hi   This   my   son   play   garden ...
1        3    6      3    0     2       4
2        0    2      4    4     3       1
3        0    2      0    7     3       0
</code></pre>
<p>I want to find how many times each word appeared in the corpus</p>
<pre><code>cnt_pro = df_all['Text'].value_counts()
plt.figure(figsize=(12,4))
sns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Word', fontsize=12)
plt.xticks(rotation=90)
plt.show();
</code></pre>
<p>to get top words like this</p>
<p><a href=""https://i.sstatic.net/pKzpd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pKzpd.png"" alt=""enter image description here"" /></a></p>
<p>but I get this chart that shows no info</p>
<p><a href=""https://i.sstatic.net/y42oj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/y42oj.png"" alt=""I w"" /></a></p>
<p>How can I fix that?</p>
",Dataset Preprocessing & Handling,find nlp word count plot nlp work original dataframe converted text bow data frame dataframe look like want find many time word appeared corpus get top word like get chart show info fix
Preprocessing .txt files for NLP,"<p>I have a upwards of a hundred unstructured .txt files (articles) that I need preprocess for NLP, must I convert the .txt files to .csv files first? Or can I start scrubbing with the raw text file? If so, can anybody help me with batch file type conversion with Python? </p>
",Dataset Preprocessing & Handling,preprocessing txt file nlp upwards hundred unstructured txt file article need preprocess nlp must convert txt file csv file first start scrubbing raw text file anybody help batch file type conversion python
Cannot open local .txt file to read in NLTK,"<p>I'm not an experienced programmer and I am solely trying to use it for some data analysis with NLTK. I'm following the online book, but the following code does not open to read the file.</p>
<pre><code>#download nltk

import nltk
nltk.download()

#import everything from nltk
from nltk.book import *

import io
open('ALL FEMALES1.txt', encoding=&quot;ISO-8859-1&quot;)

</code></pre>
<p>The output:
&lt;_io.TextIOWrapper name='ALL FEMALES1.txt' mode='r' encoding='ISO-8859-1'&gt;</p>
<p>I've tried to use</p>
<pre><code>f = open('ALL FEMALES1.txt')
&gt;&gt;&gt; raw = f.read()
</code></pre>
<p>But this gives me the following error:
'utf-8' codec can't decode byte 0xd5 in position 18929: invalid continuation byte</p>
<p>Any advice? I'm using Jupyter Notebook if that matters.</p>
",Dataset Preprocessing & Handling,open local txt file read nltk experienced programmer solely trying use data analysis nltk following online book following code doe open read file output io textiowrapper name female txt mode r encoding iso tried use give following error utf codec decode byte xd position invalid continuation byte advice using jupyter notebook matter
How to write a method that returns cosine similarity between two documents,"<p>I am writing a method that returns cosine similarity between two documents. Using sklearn CountVectorizer()
I have tried</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def doc_cos_similar(doc1:str, doc2:str) -&gt; float:
  vectorizer= CountVectorizer()
  doc1=&quot;Good morning&quot;
  doc2=&quot;Good evening&quot;
  documents = [doc1, doc2]
  count_vectorizer = CountVectorizer()
  sparse_matrix = count_vectorizer.fit_transform(documents)
  doc_term_matrix = sparse_matrix.todense()
  return doc_term_matrix
</code></pre>
<p>#input</p>
<pre><code>doc1=&quot;Good morning&quot;
doc2=&quot;Good afternoon&quot;
</code></pre>
<p>the
output should be 0.60(something alike)</p>
<p>But the output is a</p>
<blockquote>
<p>matrix([[0, 1, 1],
[1, 1, 0]])</p>
</blockquote>
",Dataset Preprocessing & Handling,write method return cosine similarity two document writing method return cosine similarity two document using sklearn countvectorizer tried input output something alike output matrix
How to fix this code and make my own POS-tagger? (PYTHON),"<p>My program need to read a file with sentences and produce an output like that:</p>
<p>input: Ixé Maria.
output: Ixé\PRON Maria\N-PR.</p>
<p>Until now, I wrote this, but the outfile gives me an empty textfile. (please, give me suggestions):</p>
<pre class=""lang-py prettyprint-override""><code>infile = open('corpus_test.txt', 'r', encoding='utf-8').read()
outfile = open('tag_test.txt', 'w', encoding='utf-8')

dicionario = {'mimbira': 'N',
             'anama-itá': 'N-PL',
             'Maria': 'N-PR',
             'sumuara-kunhã': 'N-FEM',
             'sumuara-kunhã-itá': 'N-FEM-PL',
             'sapukaia-apigaua': 'N-MASC',
             'sapukaia-apigaua-itá': 'N-MASC-PL',
             'nhaã': 'DEM',
             'nhaã-itá': 'DEM-PL',
             'ne': 'POS',
             'mukuĩ': 'NUM',
             'muíri': 'QUANT',
             'iepé': 'INDF',
             'pirasua': 'A1',
             'pusé': 'A2',
             'ixé': 'PRON1',
             'se': 'PRON2',
             '. ;': 'PUNCT'
             }

np_words = dicionario.keys()
np_tags = dicionario.values()

for line in infile.splitlines():
   list_of_words = line.split()
   if np_words in list_of_words:
       tag_word = list_of_words.index(np_words)+1
       word_tagged = list_of_words.insert(tag_word, f'\{np_tags}') 
       word_tagged = &quot; &quot;.join(word_tagged)
       print(word_tagged, file=outfile)

outfile.close()
</code></pre>
",Dataset Preprocessing & Handling,fix code make po tagger python program need read file sentence produce output like input ix maria output ix pron maria n pr wrote outfile give empty textfile please give suggestion
Text (cosine) similarity,"<p>I have followed the explanation of Fred Foo in this stack overflow question: <a href=""https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents"">How to compute the similarity between two text documents?</a></p>
<p>I have run the following piece of code that he wrote:</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [&quot;I'd like an apple&quot;,
          &quot;An apple a day keeps the doctor away&quot;,
          &quot;Never compare an apple to an orange&quot;,
          &quot;I prefer scikit-learn to Orange&quot;,
          &quot;The scikit-learn docs are Orange and Blue&quot;]
vect = TfidfVectorizer(min_df=1, stop_words=&quot;english&quot;)
tfidf = vect.fit_transform(corpus)
pairwise_similarity = tfidf * tfidf.T
print(pairwise_similarity.toarray())
</code></pre>
<p>And the result is:</p>
<pre><code>[[1.         0.17668795 0.27056873 0.         0.        ]
 [0.17668795 1.         0.15439436 0.         0.        ]
 [0.27056873 0.15439436 1.         0.19635649 0.16815247]
 [0.         0.         0.19635649 1.         0.54499756]
 [0.         0.         0.16815247 0.54499756 1.        ]]
</code></pre>
<p>But what I noticed is that when I set corpus to be:</p>
<pre><code>corpus = [&quot;I'd like an apple&quot;,
          &quot;An apple a day keeps the doctor away&quot;]
</code></pre>
<p>and run the same code again, I get the matrix:</p>
<pre><code>[[1.         0.19431434]
 [0.19431434 1.        ]]
</code></pre>
<p>Thus their similarity changes (in the first matrix, their similarity is 0.17668795). Why is that the case? I am really confused.
Thank you in advance!</p>
",Dataset Preprocessing & Handling,text cosine similarity followed explanation fred foo stack overflow question href compute similarity two text document run following piece code wrote result noticed set corpus run code get matrix thus similarity change first matrix similarity case really confused thank advance
How to create inverted index for a column in a dataframe?,"<p>I created a data frame for my <strong>scraped</strong> data and removed punctuations, stopwords and tokenized it.
How do I create inverted index for the columns name and brand ?</p>
<pre><code>import nltk
import string
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
import pandas as pd
</code></pre>
<pre><code>tokens = RegexpTokenizer(r'\w+')
macys_df['name'] = macys_df['name'].apply(lambda x: tokens.tokenize(x.lower()))
macys_df.head()
</code></pre>
<pre><code>stop_words = set(stopwords.words('english')) 
stop_words = stop_words.union(&quot;,&quot;,&quot;(&quot;,&quot;)&quot;,&quot;[&quot;,&quot;]&quot;,&quot;{&quot;,&quot;}&quot;,&quot;#&quot;,&quot;@&quot;,&quot;!&quot;,&quot;:&quot;,&quot;;&quot;,&quot;.&quot;,&quot;?&quot;)

macys_df['name'] = macys_df['name'].apply(lambda x: [item for item in x if item not in stop_words])
</code></pre>
<pre><code>Output - 
macys_df['name'].head()
0    [versa, 2, black, elastomer, strap, touchscree...
1    [men, digital, black, resin, strap, watch, 50,...
2      [versa, lite, white, strap, smart, watch, 39mm]
3    [access, mkgo, black, silicone, strap, touchsc...
4    [inspire, black, strap, activity, tracker, 19,...
</code></pre>
",Dataset Preprocessing & Handling,create inverted index column dataframe created data frame scraped data removed punctuation stopwords tokenized create inverted index column name brand
How do I make train and test set the same size using bag of words,"<p>I want to use bag of words to train a regression model. But, I do not want to have information leak between my train and test sets. So, that means I need to create a word vector on the train set separate from my test set. I ran this code</p>
<pre><code>def bow (tokens, data):
    tokens = tokens.apply(nltk.word_tokenize)
    cvec = CountVectorizer(min_df = .01, max_df = .95, ngram_range=(1,2), tokenizer=lambda doc:doc, lowercase=False)
    cvec.fit(tokens)
    cvec_counts = cvec.transform(tokens)
    cvec_counts_bow = cvec_counts.toarray()
    vocab = cvec.get_feature_names()
    bow_model = pd.DataFrame(cvec_counts_bow, columns=vocab)
    return bow_model

X_train = bow(train['clean_text'], train)
X_test = bow(test['clean_text'], test)

vocab = list(X_train.columns)

</code></pre>
<p>But the shape of my data frames are X_train: (300, 730) and X_test (35, 1661)
There are more unique words in my test set than my train set -- because the train set is so small --- and the words do not match. There are words in X_train that are not in X_test and vice versa. I thought to create a vocab list from X_train then keep only the columns in X_test but that doesn't seem right.</p>
<p>How do I fit the vocabulary from the X_train onto X_test?</p>
",Dataset Preprocessing & Handling,make train test set size using bag word want use bag word train regression model want information leak train test set mean need create word vector train set separate test set ran code shape data frame x train x test unique word test set train set train set small word match word x train x test vice versa thought create vocab list x train keep column x test seem right fit vocabulary x train onto x test
"load_dataset(&#39;multi_nli&#39;) from dataset is not working, getting import error","<p>I am currently working on an NLI project, and about a week ago, load_dataset('multi_nli') was working just fine. However, when I was about to import it again and test a different model, an import error is showing up.</p>
<p>ImportError: cannot import name 'setup_logging' from 'fsspec.utils' (/opt/conda/lib/python3.7/site-packages/fsspec/utils.py)</p>
<p>Everything should be installed, like datasets and such but it is not working.</p>
",Dataset Preprocessing & Handling,load dataset multi nli dataset working getting import error currently working nli project week ago load dataset multi nli wa working fine however wa import test different model import error showing importerror import name setup logging fsspec utils opt conda lib python site package fsspec utils py everything installed like datasets working
How to predict sentiment of a text by using word2Vec and deep learning model?,"<p>I am trying to detect sentiment of tweets by using word2vec and some deep learning algorithms. Here is my model training codes :</p>
<pre><code>word2VecValues = pd.read_csv(allVectorValuesPath, sep=&quot;,&quot;, skipinitialspace=True)
word2VecValues = convertLabelToFloat(word2VecValues)
word2VecValues = prepareDataSet(word2VecValues)

X = word2VecValues.iloc[:, :-1]
Y = word2VecValues.iloc[:, -1]

model = Sequential()
model.add(Dense(12, input_dim=100, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, Y, epochs=25, batch_size=10)

saveModel(model, &quot;Word2Vec&quot;)

_, accuracy = model.evaluate(X, Y)
print('Accuracy: %.2f' % (accuracy * 100))
</code></pre>
<p>As you see, I have a csv file which contains word2vec vector values of each word of my corpus. I  am using it as data frame which is &quot;word2VecValues&quot;.</p>
<p>I save my word2vec model after model training finished.
I want to load the model and predict sentiment of one text. However, I have little bit confusing about how I can get vector values of new text. Can you help me ?</p>
",Dataset Preprocessing & Handling,predict sentiment text using word vec deep learning model trying detect sentiment tweet using word vec deep learning algorithm model training code see csv file contains word vec vector value word corpus using data frame word vecvalues save word vec model model training finished want load model predict sentiment one text however little bit confusing get vector value new text help
tokenizing on a pdf for quantitative analysis,"<p>I ran into an issue using the unnest_tokens function on a data_frame. I am working with pdf files I want to compare.</p>
<pre><code>text_path &lt;- &quot;c:/.../text1.pdf&quot;
text_raw &lt;- pdf_text(&quot;c:/.../text1.pdf&quot;)
text1df&lt;- data_frame(Zeile = 1:25, 
                      text_raw)
</code></pre>
<p>So far so good. But here comes my problemo:</p>
<pre><code>  unnest_tokens(output = token, input = content) -&gt; text1_long
</code></pre>
<p>Error: Must extract column with a single valid subscript.
x Subscript <code>var</code> has the wrong type <code>function</code>.
i It must be numeric or character.</p>
<p>I want to tokenize my pdf files so I can analyse the word frequencies and maybe compare multiple pdf files on wordclouds.</p>
",Dataset Preprocessing & Handling,tokenizing pdf quantitative analysis ran issue using unnest token function data frame working pdf file want compare far good come problemo error must extract column single valid subscript x subscript ha wrong type must numeric character want tokenize pdf file analyse word frequency maybe compare multiple pdf file wordclouds
How to coerce a corpus to be assessed on the same terms as another one,"<h1>Problem</h1>
<p>I need to have a corpus of text, applied the same terms as another, so I can get a term document matrix with the same values.
What I am attempting, is to classify different corpus of texts between 2 groups, using a logistic regression, but I need both of these corpus to have the same variables from the function <code>DocumentTermMatrix()</code>.</p>
<h1>Current attempt and explanation with code</h1>
<p>I can't wrap my head about how to approach the issue, for example, this gives me a first matrix of terms with their frequency:</p>
<pre><code>data(&quot;crude&quot;)
crude_1 &lt;- crude[1:10]
dtm_1 &lt;- DocumentTermMatrix(crude_1)
dtm_1$dimnames$Terms
#  [1] &quot;...&quot;               &quot;\&quot;(it)&quot;            &quot;\&quot;demand&quot;          &quot;\&quot;for&quot;            
#  [5] &quot;\&quot;growth&quot;          &quot;\&quot;if&quot;              &quot;\&quot;is&quot;              &quot;\&quot;may&quot; ...
</code></pre>
<p>How can I use the same terms for the second part of the <code>crude</code> dataset, because so far the Terms are different:</p>
<pre><code>crude_2 &lt;- crude[11:20]
dtm_2 &lt;- DocumentTermMatrix(crude_2)
dtm_2$dimnames$Terms
#  [1] &quot;...&quot;             &quot;\&quot;expansion&quot;     &quot;\&quot;is&quot;            &quot;\&quot;may&quot;          
#  [5] &quot;\&quot;none&quot;          &quot;\&quot;this&quot;          &quot;\&quot;we&quot;            &quot;\&quot;will&quot; ...
</code></pre>
<p>I could try to run a frequency on the same terms on <code>crude_2</code>. However, it would be expensive in terms of computation, and you might know a practical solution to this problem.</p>
<h1>Question</h1>
<p>I would like to coerce <code>dtm_2</code> to have the same terms as in <code>dtm_1</code>. Only with the frequency of the <code>crude_2</code> dataset. Is there a practical way to do this in R ?</p>
<p><strong>Or more easily for example: Say I want to find out, how many times <code>zebra</code> or <code>girafe</code> appears in these text, and I want to do it explicitly, how can I proceed ?</strong></p>
<p>Libraries used: <code>library(tm)</code></p>
",Dataset Preprocessing & Handling,coerce corpus assessed term another one problem need corpus text applied term another get term document matrix value attempting classify different corpus text group using logistic regression need corpus variable function current attempt explanation code wrap head approach issue example give first matrix term frequency use term second part dataset far term different could try run frequency term however would expensive term computation might know practical solution problem question would like coerce term frequency dataset practical way r easily example say want find many time appears text want explicitly proceed library used
R: Convert a &quot;Term Document Matrix&quot; to a &quot;Corpus&quot;,"<p>I am using the R programming language. I am trying to follow the instructions from this tutorial over here (<a href=""https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html"" rel=""nofollow noreferrer"">https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html</a>) and learn how to convert a &quot;term document matrix&quot; into a &quot;corpus&quot;. However, the explanations provided in this tutorial are unclear to me, and I am not sure how to do this.</p>
<p>Using publicly available Shakespeare Plays, I created the term document matrix as follows:</p>
<pre><code>#load libraries
library(dplyr)
library(pdftools)
library(tidytext)
library(textrank)
library(tm)

#1st document
url &lt;- &quot;https://shakespeare.folger.edu/downloads/pdf/hamlet_PDF_FolgerShakespeare.pdf&quot;

article &lt;- pdf_text(url)
article_sentences &lt;- tibble(text = article) %&gt;%
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_id = row_number()) %&gt;%
  select(sentence_id, sentence)


article_words &lt;- article_sentences %&gt;%
  unnest_tokens(word, sentence)


article_words_1 &lt;- article_words %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)

#2nd document
url &lt;- &quot;https://shakespeare.folger.edu/downloads/pdf/macbeth_PDF_FolgerShakespeare.pdf&quot;

article &lt;- pdf_text(url)
article_sentences &lt;- tibble(text = article) %&gt;%
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_id = row_number()) %&gt;%
  select(sentence_id, sentence)


article_words &lt;- article_sentences %&gt;%
  unnest_tokens(word, sentence)


article_words_2&lt;- article_words %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)


#3rd document
url &lt;- &quot;https://shakespeare.folger.edu/downloads/pdf/othello_PDF_FolgerShakespeare.pdf&quot;

article &lt;- pdf_text(url)
article_sentences &lt;- tibble(text = article) %&gt;%
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_id = row_number()) %&gt;%
  select(sentence_id, sentence)


article_words &lt;- article_sentences %&gt;%
  unnest_tokens(word, sentence)


article_words_3 &lt;- article_words %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)
</code></pre>
<p>From here, I create the actual &quot;term document matrix&quot;:</p>
<pre><code>library(tm)

#create term document matrix
tdm &lt;- TermDocumentMatrix(Corpus(VectorSource(rbind(article_words_1, article_words_2, article_words_3))))

#inspect the &quot;term document matrix&quot; (I don't know why this is producing an error)
inspect(tdm)
</code></pre>
<p>Now, I am unsure how to use the instructions from this tutorial (<a href=""https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html"" rel=""nofollow noreferrer"">https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html</a>) and convert the &quot;Term Document Matrix&quot; into a &quot;Corpus&quot;.</p>
<p>Is this how it is supposed to be done?</p>
<pre><code>library(quanteda)
d &lt;- quanteda::dfm(tdm, verbose = FALSE)
</code></pre>
<p>Can someone please show me how to solve this problem?</p>
<p>Thanks</p>
",Dataset Preprocessing & Handling,r convert term document matrix corpus using r programming language trying follow instruction tutorial learn convert term document matrix corpus however explanation provided tutorial unclear sure using publicly available shakespeare play created term document matrix follows create actual term document matrix unsure use instruction tutorial convert term document matrix corpus supposed done someone please show solve problem thanks
R Error: Only works with Character Objects,"<p>I am using the R programming language. I am trying to replicate the previous stackoverflow post over here  <a href=""https://stackoverflow.com/questions/55276570/r-about-stopwords-in-documenttermmatrix"">(R) About stopwords in DocumentTermMatrix</a> , for the purpose of &quot;tokenizing&quot; and removing &quot;stop words&quot;.</p>
<p>Using some publicly available Shakespeare Plays, I created a &quot;term document matrix&quot; using 3 plays:</p>
<pre><code>#load libraries
library(dplyr)
library(pdftools)
library(tidytext)
library(textrank)
library(tm)

#1st document
url &lt;- &quot;https://shakespeare.folger.edu/downloads/pdf/hamlet_PDF_FolgerShakespeare.pdf&quot;

article &lt;- pdf_text(url)
article_sentences &lt;- tibble(text = article) %&gt;%
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_id = row_number()) %&gt;%
  select(sentence_id, sentence)


article_words &lt;- article_sentences %&gt;%
  unnest_tokens(word, sentence)


article_words_1 &lt;- article_words %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)

#2nd document
url &lt;- &quot;https://shakespeare.folger.edu/downloads/pdf/macbeth_PDF_FolgerShakespeare.pdf&quot;

article &lt;- pdf_text(url)
article_sentences &lt;- tibble(text = article) %&gt;%
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_id = row_number()) %&gt;%
  select(sentence_id, sentence)


article_words &lt;- article_sentences %&gt;%
  unnest_tokens(word, sentence)


article_words_2&lt;- article_words %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)


#3rd document
url &lt;- &quot;https://shakespeare.folger.edu/downloads/pdf/othello_PDF_FolgerShakespeare.pdf&quot;

article &lt;- pdf_text(url)
article_sentences &lt;- tibble(text = article) %&gt;%
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_id = row_number()) %&gt;%
  select(sentence_id, sentence)


article_words &lt;- article_sentences %&gt;%
  unnest_tokens(word, sentence)


article_words_3 &lt;- article_words %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)
</code></pre>
<p>From here, I create the actual &quot;term document matrix&quot;:</p>
<pre><code>library(tm)

#create term document matrix
tdm &lt;- TermDocumentMatrix(Corpus(VectorSource(rbind(article_words_1, article_words_2, article_words_3))))

#inspect the &quot;term document matrix&quot; (I don't know why this is producing an error)
inspect(tdm)
</code></pre>
<p>After this, I am trying to perform &quot;tokenization&quot; and remove &quot;stop words&quot; using two different methods (source: <a href=""https://stackoverflow.com/questions/55276570/r-about-stopwords-in-documenttermmatrix"">(R) About stopwords in DocumentTermMatrix</a> )</p>
<pre><code>library(quanteda)

#first method:

first_method &lt;- tokens(tdm) %&gt;%
  tokens_remove(stopwords(&quot;en&quot;), pad = TRUE)

Error: tokens() only works on character, corpus, list, tokens objects.


#second method:

second_method &lt;- dfm(text, remove_punct = TRUE) %&gt;%
  dfm_remove(stopwords(&quot;en&quot;))

Error: dfm() only works on character, corpus, list, tokens objects.
</code></pre>
<p>Both of these steps result an errors, indicating that these functions only work on &quot;character, corpus, list or token&quot; objects. Is there some way to use these functions on the term document matrix I created?</p>
<p>Thanks</p>
",Dataset Preprocessing & Handling,r error work character object using r programming language trying replicate previous stackoverflow post step result error indicating function work character corpus list token object way use function term document matrix created thanks
How to set time slices - Dynamic Topic Model,"<p><strong>Intro</strong></p>

<p>Currently I am using Gensim in combination with pandas and numpy to run document NLP computation.  I'd like to build a LDA seqential model to track how our topics change over time but am running into errors with the corpus format.</p>

<p>I am trying to figure out how to set time slices for dynamic topic models.  I am using <a href=""https://radimrehurek.com/gensim/models/ldaseqmodel.html"" rel=""nofollow noreferrer"">LdaSeqModel</a> which requires an integer time slice. </p>

<p><strong>The Data</strong></p>

<p>It's a csv:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>data = pd.read_csv('CGA Jan17 - Mar19 Time Slice.csv', encoding = ""ISO-8859-1"");
documents = data[['TextForTopics']]
documents['index'] = documents.index</code></pre>
</div>
</div>
</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>	       Month	Year	Begin Date	TextForTopics	                                      time_slice
0	march	2017	3/23/2017	request: the caller is requesting an appointme...	1</code></pre>
</div>
</div>
</p>

<p>This is then converted into an array of tuples called the bow_corpus:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>[[(12, 2), (25, 1), (30, 1)], [(33, 1), (136, 1), (159, 1), (161, 1)], [(165, 1), (247, 2)], (326, 1), (354, 1), (755, 1), (821, 1)]]</code></pre>
</div>
</div>
</p>

<p><strong>Desired Output</strong></p>

<p>It should print one topic allocation for each time slice. If I entered 3 topics and two time slices I should get three topics printed twice showing how the topics evolved over time.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>[(0,
  '0.165*""enrol"" + 0.108*""medicar"" + 0.051*""form""),
(1,
  '0.303*""caller"" + 0.290*""inform"" + 0.031*""abl""),
(2,
  '0.208*""date"" + 0.140*""effect"" + 0.060*""medicaid""')]
[(0,
  '0.165*""enrol"" + 0.108*""cats"" + 0.051*""form""),
(1,
  '0.303*""caller"" + 0.290*""puppies"" + 0.031*""abl""),
(2,
  '0.208*""date"" + 0.140*""elephants"" + 0.060*""medicaid""')]</code></pre>
</div>
</div>
</p>

<p><strong>What I've tried</strong></p>

<p>This is the function - the bow corpus is an array of tuples</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>ldaseq = LdaSeqModel(corpus=bow_corpus, time_slice=[], num_topics=15, chunksize=1)</code></pre>
</div>
</div>
</p>

<p>I've tried every version of integer inputs for those time_slices and they all produce errors.  The premise was that the time_slice would represent the number of indicies/rows/documents in each time slice.  For example my data has 1.8 million rows if I wanted two time slices I would order my data by time and enter an integer cutoff like time_slice = [489234, 1310766].  All inputs produce this error:</p>

<p><strong>The Error</strong>
<div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-5-e58059a7fb6f&gt; in &lt;module&gt;
----&gt; 1 ldaseq = LdaSeqModel(corpus=bow_corpus, time_slice=[], num_topics=15, chunksize=1)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in __init__(self, corpus, time_slice, id2word, alphas, num_topics, initialize, sstats, lda_model, obs_variance, chain_variance, passes, random_state, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)
    186 
    187             # fit DTM
--&gt; 188             self.fit_lda_seq(corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)
    189 
    190     def init_ldaseq_ss(self, topic_chain_variance, topic_obs_variance, alpha, init_suffstats):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in fit_lda_seq(self, corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)
    275             # seq model and find the evidence lower bound. This is the E - Step
    276             bound, gammas = \
--&gt; 277                 self.lda_seq_infer(corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)
    278             self.gammas = gammas
    279 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in lda_seq_infer(self, corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)
    351             bound, gammas = self.inferDTMseq(
    352                 corpus, topic_suffstats, gammas, lhoods, lda,
--&gt; 353                 ldapost, iter_, bound, lda_inference_max_iter, chunksize
    354             )
    355         elif model == ""DIM"":

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in inferDTMseq(self, corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)
    401         time = 0  # current time-slice
    402         doc_num = 0  # doc-index in current time-slice
--&gt; 403         lda = self.make_lda_seq_slice(lda, time)  # create lda_seq slice
    404 
    405         time_slice = np.cumsum(np.array(self.time_slice))

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in make_lda_seq_slice(self, lda, time)
    459         """"""
    460         for k in range(self.num_topics):
--&gt; 461             lda.topics[:, k] = self.topic_chains[k].e_log_prob[:, time]
    462 
    463         lda.alpha = np.copy(self.alphas)

IndexError: index 0 is out of bounds for axis 1 with size 0</code></pre>
</div>
</div>
</p>

<p><strong>Solutions</strong></p>

<p>I tried going back to the documentation and looking at the format of the common_corpus used as an example and the format of my bow_corpus is the same.  I also tried running the code in the documentation to see how it worked but it also produced the same error.  I'm not sure if the problem is my code anymore but I hope it is.</p>

<p>I've also tried messing with the file format by manually dividing my csv into 9 csvs containing my time_slices and creating an iterated corpus out of those, but that didn't work.  I've considered converting each row of my csv into txt files and then creating a corpus out of that like David Beil does, but that sounds pointlessly tedious as I already have an iterated corpus.</p>
",Dataset Preprocessing & Handling,set time slice dynamic topic model intro currently using gensim combination panda numpy run document nlp computation like build lda seqential model track topic change time running error corpus format trying figure set time slice dynamic topic model using ldaseqmodel requires integer time slice data csv converted array tuples called bow corpus desired output print one topic allocation time slice entered topic two time slice get three topic printed twice showing topic evolved time tried function bow corpus array tuples tried every version integer input time slice produce error premise wa time slice would represent number indicies row document time slice example data ha million row wanted two time slice would order data time enter integer cutoff like time slice input produce error error solution tried going back documentation looking format common corpus used example format bow corpus also tried running code documentation see worked also produced error sure problem code anymore hope also tried messing file format manually dividing csv csvs containing time slice creating iterated corpus work considered converting row csv txt file creating corpus like david beil doe sound pointlessly tedious already iterated corpus
Is there a way to stop scikit learn from truncating my (large) output data in a document term matrix?,"<p>I am trying to build and store a document term matrix (DTM) from a large corpus of text data. Everything seems to be working except that when I store the to .tsv it takes only saves the first and last 24  entries like this:</p>
<p>(0, 256)  2</p>
<p>(0, 272)  1</p>
<p>(0, 286)  1</p>
<p>: :</p>
<p>(0, 12351)    4</p>
<p>(0, 12353)    5</p>
<p>(0, 12357)    2</p>
<p>I want it to save every entry and not give the : : instead.</p>
<p>I am writing each text entry to a <em>scipy.sparse.csr.csr_matrix</em> object like this:</p>
<pre><code>briefVector = briefVectorizer.transform(briefInstance)
</code></pre>
<p>Where briefVectorizer is the features matrix and briefInstance is the raw text data. I am then appending all of these objects to a list like this:</p>
<pre><code>briefVectorsList.append(briefVector)
</code></pre>
<p>Then making that list into a DataFrame column and saving the DataFrame to .tsv.</p>
<p>Can anyone help me?</p>
",Dataset Preprocessing & Handling,way stop scikit learn truncating large output data document term matrix trying build store document term matrix dtm large corpus text data everything seems working except store tsv take save first last entry like want save every entry give instead writing text entry scipy sparse csr csr matrix object like briefvectorizer feature matrix briefinstance raw text data appending object list like making list dataframe column saving dataframe tsv anyone help
float&#39; object is not iterable,"<p><a href=""https://i.sstatic.net/65Pc8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/65Pc8.png"" alt=""enter image description here"" /></a><a href=""https://i.sstatic.net/1hkSv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1hkSv.png"" alt=""enter image description here"" /></a>I'm trying to clean a column of abstract in a dataset</p>
<p>this is the cleaning function I'm using</p>
<pre><code>#remove punctuations
def remove_punctuation (text):
    no_punct=&quot;&quot;.join([c for c in text if c not in string.punctuation])
    return no_punct
</code></pre>
<p>when I write the line</p>
<pre><code>df['abstract'] = df['abstract'].apply(lambda x: remove_punctuation(x))
</code></pre>
<p>I get: 'float' object is not iterable</p>
<p>When column abstract is of type string.</p>
<p>Can anybody explain why it does not work?</p>
",Dataset Preprocessing & Handling,float object iterable trying clean column abstract dataset cleaning function using write line get float object iterable column abstract type string anybody explain doe work
Read and Write large text file python too slow,"<p>This code goes over a large 5.1GB text file and checks if there are words that appear less than 100 times . Then rewrites the 5.1GB into an output text file and replaces those words with unk. The main problem is that the creation of output.txt is taking a very long time.
I suspect that the method write_text() is causing an issue by the way its opening the dataset file and the output file.</p>
<p>The goal behind this script : I have a prebuilt vocab and i have a text. The text might have new words that are not in my vocab so i would like to add them to my vocab. But I only want to add new words that are relevant (appears more than 100 times). The new words that appear in the text less than 100 times are disposable and not important so i would like to change them to &quot;unk&quot;.</p>
<pre><code>
from collections import Counter

extra_words = []
new_words = []
add_words = []


def get_vocab():
    vocab = set()
    with open('vocab.txt', 'r', encoding='utf-8') as rd:
        lines = rd.readlines()

    for line in lines:
        tokens = line.split(' ')
        word = tokens[0]
        vocab.add(word)

    return vocab


def _count(text):

    vocab = get_vocab()

    with open(text, 'r', encoding='utf-8') as fd:

        for line in fd.readlines():

            for token in line.split():

                if token not in vocab:
                    extra_words.append(token)

    word_count = Counter(extra_words)

    # add del word_count[punctuation] to remove it from list

    #del word_count['&quot;']

    for word in word_count:

        if word_count[word] &lt; 100:
            new_words.append(word)

        else:
            add_words.append(word)

    write_text()

    #return len(new_words), word_count.most_common()[0]


def write_text():

    with open('dataset', 'r', encoding='utf-8') as fd:

        f = fd.readlines()

    with open('output.txt', 'w', encoding='utf-8') as rd:
        new_text = []
        for line in f:
            new_line = []
            for token in line.split():

                

                if token in new_words:

                    new_line.append('&lt;unk&gt;')

                else:

                    new_line.append(token)

            new_text.append(' '.join(new_line))
        print('\n'.join(new_text), file=rd)
            #print(' '.join(new_line), file=rd)


def add_vocab():

    ln = len(get_vocab())

    with open('vocab.txt', 'w', encoding='utf-8') as fd:

        for idx, word in add_words:

            print(f'{word} {ln + idx + 1}\n', file=fd)

    pass


print(_count('dataset'))
add_vocab()
</code></pre>
",Dataset Preprocessing & Handling,read write large text file python slow code go large gb text file check word appear le time rewrite gb output text file replaces word unk main problem creation output txt taking long time suspect method write text causing issue way opening dataset file output file goal behind script prebuilt vocab text text might new word vocab would like add vocab want add new word relevant appears time new word appear text le time disposable important would like change unk
UnicodeError when I load dataset with nlp package,"<p>I encounter a problem when I use <code>nlp.load_dataset('eli5')</code>. I don't why my system always tells me:</p>
<pre><code>UnicodeDecodeError: 'gbk' codec can't decode byte 0x80 in position 2178: illegal multibyte sequence
</code></pre>
<p>This is my code:</p>
<pre><code>import nlp
eli5 = nlp.load_dataset('eli5')
</code></pre>
<p>But when I write this codes on colab and ubuntu, I don't receive the error.</p>
<p>My python version is 3.8.10.
My system is win 10.</p>
<p>What should I do to work around this error?</p>
",Dataset Preprocessing & Handling,unicodeerror load dataset nlp package encounter problem use system always tell code write code colab ubuntu receive error python version system win work around error
how to calculate the sentiment score for the dataframe using SpacyTextBlob?,"<p>I am finding sentiment scores for reviews using SpacyTextblob. I can able to calculate the polarity and subjectivity score for individual text but unable to calculate for the column</p>
<p>My data frame called 'data' which has reviews called reviews.text column</p>
<pre><code>
import spacy
from spacytextblob.spacytextblob import SpacyTextBlob

nlp = spacy.load('en_core_web_sm')
nlp.add_pipe(&quot;spacytextblob&quot;)
doc = nlp(data['reviews.text'][0])

print('Polarity:', doc._.polarity)
print('Sujectivity:', doc._.subjectivity)

</code></pre>
<p>The above code is working for each review separately and I can calculate the polarity and subjectivity. However, I cannot ale to calculate for the entire column</p>
<pre><code>
polarity = lambda x: nlp(x)._.polarity
subjectivity = lambda x: nlp(x)._.subjectivity
data['polarity'] = data['reviews.text'].apply(polarity)
df['subjectivity'] = data['reviews.text'].apply(subjectivity)
</code></pre>
<p>The above code is not working, can anyone tell me how to calculate it?</p>
",Dataset Preprocessing & Handling,calculate sentiment score dataframe using spacytextblob finding sentiment score review using spacytextblob able calculate polarity subjectivity score individual text unable calculate column data frame called data ha review called review text column code working review separately calculate polarity subjectivity however ale calculate entire column code working anyone tell calculate
Remove text lines only from first 2 lines if matches to other column&#39;s text,"<p>I have a Pandas data frame (Python 3.x) like the following:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">title</th>
<th style=""text-align: center;"">type</th>
<th style=""text-align: right;"">content</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">Mango</td>
<td style=""text-align: center;"">Paragraph</td>
<td style=""text-align: right;"">Mango is called the king of fruits. It is a stone fruit produced from numerous species of tropical trees.....</td>
</tr>
<tr>
<td style=""text-align: left;"">Dates</td>
<td style=""text-align: center;"">Paragraph</td>
<td style=""text-align: right;"">[Dates] /n [Paragraph] /n Dates have been a staple food of the Middle East and the Indus Valley for thousands of years......</td>
</tr>
<tr>
<td style=""text-align: left;"">Mango</td>
<td style=""text-align: center;"">Essay</td>
<td style=""text-align: right;"">&quot;Mango&quot; &quot;Essay&quot; Mango is called the king of fruits. It is a stone fruit produced from numerous species of tropical trees.....</td>
</tr>
<tr>
<td style=""text-align: left;"">Apple</td>
<td style=""text-align: center;"">Article</td>
<td style=""text-align: right;"">Apple /n Article /n An apple is an edible fruit produced by an apple tree.....</td>
</tr>
<tr>
<td style=""text-align: left;"">......</td>
<td style=""text-align: center;"">......</td>
<td style=""text-align: right;"">...................................................................................</td>
</tr>
</tbody>
</table>
</div>
<p>In some rows, the text of &quot;title&quot; &amp; &quot;type&quot; column is also present in &quot;content&quot; column. I need to remove the whole line if &quot;title&quot; &amp; &quot;type&quot; is present in the &quot;content&quot; column.
I tried so many times, but failed to remove the whole line containing same text of other columns.
Please help!!</p>
",Dataset Preprocessing & Handling,remove text line first line match column text panda data frame python x like following title type content mango paragraph mango called king fruit stone fruit produced numerous specie tropical tree date paragraph date n paragraph n date staple food middle east indus valley thousand year mango essay mango essay mango called king fruit stone fruit produced numerous specie tropical tree apple article apple n article n apple edible fruit produced apple tree row text title type column also present content column need remove whole line title type present content column tried many time failed remove whole line containing text column please help
Pipe Operators returning two rows for one comment,"<p>I am attempting to obtain sentiment scores on comments in a data frame with two columns, Author and Comment. I used the command</p>
<pre><code>data %&gt;%
  get_sentences() %&gt;%
  sentiment() -&gt; data_senti
</code></pre>
<p>to put the result into a variable. The result is more than doubled as each comment is broken down into n amount of rows for n amount of sentences in a comment. How do I keep a comment intact, as in, return only one row for each comment and not break it down to n rows for n sentences in a comment?</p>
<p>=&gt; input example:</p>
<p><a href=""https://i.sstatic.net/yxWtM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yxWtM.png"" alt=""Author  Comment
Bob     &quot;I love it. I do not hate it&quot;"" /></a></p>
<p>=&gt; preferred output example</p>
<p><a href=""https://i.sstatic.net/VWMeD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VWMeD.png"" alt=""Author  Comment                          Sentiment
Bob     &quot;I love it. I do not hate it&quot;    0.02"" /></a></p>
<p>what I actually get:</p>
<p><a href=""https://i.sstatic.net/etfNF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/etfNF.png"" alt=""Author     Comment           Sentiment
Bob       &quot;I love it&quot;         1
Bob       &quot;I do not hate it&quot;  .8"" /></a></p>
<p>Full code so far:</p>
<pre><code># load proper libraries; ensure they are already installed
install.packages(c('syuzhet', 'sentimentr', 'tidyverse', 'magrittr', 'dplyr'))
library(syuzhet)
library(sentimentr)
library(tidyverse)

# set working dir
setwd(&quot;C:/Users/user3/OneDrive/Desktop/Sentiment_Analysis/Data/Output/Clean Sets&quot;)

# import file
data &lt;- read.csv(&quot;C:/Users/user3/OneDrive/Desktop/Sentiment_Analysis/Data/Output/Clean Sets/all_comments.csv&quot;)

data %&gt;%
  get_sentences() %&gt;%
  sentiment() -&gt; data_senti
</code></pre>
<p>UPDATE: dput(head(data))</p>
<blockquote>
<blockquote>
<p>structure(list(Primary.Key = c(&quot;Google_e1&quot;, &quot;Google_e3&quot;, &quot;Google_e98&quot;
), Original.Text = c(&quot;I feel as though awards are fairly awarded, but the time frame in which awards are given is sometimes  significantly late due at times to admin.  Not providing awards to personnel in a timely fashion can give the  perception that the command does not care about members&quot;,
&quot;Our all white, all male, all 40 or 50-something leadership is horrible to women and minorities. Every single  person that has had difficulties in the workplace has been a woman, a minority, or both. On the other hand, the  white men (mickey, donald, and others) are allowed to be abusive, dismissive, and unprofessional.  white women (Minnie, and Cardi B and Lovely Jun) are allowed to make a sport out of  gossiping about people and essentially run Mordor like a remake of the Mean Girls movie. I honestly find it hard  to believe that the chain of command has NEVER NOTICED that the people they force out of their jobs are  Latina women, black men, and other minorities, and likewise don't notice that the people they give uncontested  promotions to are white men (Lebron) and white women (Meghan The Horse). Honestly, for an analytical organization,  it's hard to believe that nobody has ever seen it through the eyes of a minority. This is why representation  matters!!!&quot;,
&quot;I have not observed clear indications of &quot;better treatment&quot; based off of Race, Religion, Sex or sexual prefrence.&quot;
)), row.names = c(NA, 3L), class = &quot;data.frame&quot;)</p>
</blockquote>
</blockquote>
",Dataset Preprocessing & Handling,pipe operator returning two row one comment attempting obtain sentiment score comment data frame two column author comment used command put result variable result doubled comment broken n amount row n amount sentence comment keep comment intact return one row comment break n row n sentence comment input example preferred output example actually get full code far update dput head data structure list primary key c google e google e google e original text c feel though award fairly awarded time frame award given sometimes significantly late due time admin providing award personnel timely fashion give perception command doe care member white male something leadership horrible woman minority every single person ha difficulty workplace ha woman minority hand white men mickey donald others allowed abusive dismissive unprofessional white woman minnie cardi b lovely jun allowed make sport gossiping people essentially run mordor like remake mean girl movie honestly find hard believe chain command ha never noticed people force job latina woman black men minority likewise notice people give uncontested white men lebron white woman meghan horse honestly analytical organization hard believe nobody ha ever seen eye minority representation matter observed clear indication better treatment based race religion sex sexual prefrence row name c na l class data frame
Annotated Dataset for Evaluating Text Coherence,"<p>I need a human-annotated dataset for evaluating text coherence methods. I read many papers but none of them provides such a dataset (except for the recent neural network-based models).</p>
<p>Is there any publicly available dataset where the coherence of text is scored by experts?</p>
<p>Thanks in advance.</p>
",Dataset Preprocessing & Handling,annotated dataset evaluating text coherence need human annotated dataset evaluating text coherence method read many paper none provides dataset except recent neural network based model publicly available dataset coherence text scored expert thanks advance
writing comma separated strings to single cell in csv without any packages,"<p>I'm new to python and trying to solve the following problem:
writing below list as single column to csv file</p>
<pre class=""lang-py prettyprint-override""><code>mylist=['emilysmom718',
 'okiemama18',
 'Dave Melton',
 'Anonymous',
 'Anonymous',
 'The Mazatlan, Post',
 '',
 '',
 '',
 'bripat9643',
 'Jeff Vorva',
 'Karen Caffarini',
 'USA TODAY',
 'Proud Boys organizer, rioter who attacked Capitol officer arrested',
 'Proud Boys organizer, rioter who attacked, Capitol officer arrested']

with open('test.csv','w',encoding='utf-8') as f:
  for s in mylist:
    f.write(&quot;%s\n&quot; % s)
</code></pre>
<p>However, with above code values with a comma are pushed to the next cell of the following column.</p>
<p>P.S: I'm new to python sorry if the query is naive.</p>
",Dataset Preprocessing & Handling,writing comma separated string single cell csv without package new python trying solve following problem writing list single column csv file however code value comma pushed next cell following column p new python sorry query naive
Unable to write content to csv in single row,"<p>I'm trying to write a list to csv format in python and facing a unique challenge with specific row which is in the format:</p>
<p>[('c6',
'',
'2021-01-21',
'Pak vs SA: Security  them play and that\u2019s almost like a generation and that\u2019s missed seeing them play.\n\u201cPakistan fans are very proud of their team, like any other nation, but it is when you tour the subcontinent, you realise people here are very passionate about cricket. They go to every match and support their team however they can. Singing, chanting, there\u2019s energy the whole day,\&quot; the Proteas player cherished. 2017 tour with the ICC World XI\nReflecting on the 2017 tour with the ICC World XI, du Plessis said: \u201cI think [the ICC World XI\u2019s tour of Pakistan] was really important in the context of bringing cricket back into Pakistan. There was zero cricket happening in Pakistan at that stage. Read more: Pak vs SA: Pacer Tabish Khan determined after finally being named in national squad\nThe second Test between the two teams will be played at the Pindi Cricket Stadium, Rawalpindi, from February 4, before they travel to Lahore for three T20Is on 11, 13, and 14 February at the Gaddafi Stadium.\nDu Plessis has played seven Tests against Pakistan in which he has scored 246 runs \u2013 including a century \u2013 at an average of 27.33. Two of those seven games were played in the United Arab Emirates, where Pakistan hosted their home cricket from 2010 until 2019, before the complete restoration of international and top-flight cricket in the country. More From Sports:')]</p>
<p>When I try to write it to a csv with following code:</p>
<pre><code>out = open('out.csv', 'w')
for row in l:
   out.write( '&quot;' + '&quot;,&quot;'.join(row) + '&quot;\n' )
   out = open('out.csv', 'w')
</code></pre>
<p>The output is splitting at exactly &quot;ICC World XI&quot; i.e. text before ICC World XI is in one cell and the rest text is pushed to the next cell in the same row. However, when I reduce the content (randomly) in the original text, the output looks fine. I'm unable to understand the reason behind it. Can anyone please explain. (I'm bit new to python)</p>
",Dataset Preprocessing & Handling,unable write content csv single row trying write list csv format python facing unique challenge specific row format c pak v sa security play u almost like generation u missed seeing play n u cpakistan fan proud team like nation tour subcontinent realise people passionate cricket go every match support team however singing chanting u energy whole day protea player cherished tour icc world xi nreflecting tour icc world xi du plessis said u ci think icc world xi u tour pakistan wa really important context bringing cricket back pakistan wa zero cricket happening pakistan stage read pak v sa pacer tabish khan determined finally named national squad second test two team played pindi cricket stadium rawalpindi february travel lahore three february gaddafi stadium ndu plessis ha played seven test pakistan ha scored run u including century u average two seven game played united arab emirate pakistan hosted home cricket complete restoration international top flight cricket country sport try write csv following code output splitting exactly icc world xi e text icc world xi one cell rest text pushed next cell row however reduce content randomly original text output look fine unable understand reason behind anyone please explain bit new python
How to extract text within flagged tags?,"<p>I have the following document and I would like to extract all categories flags.</p>
<p><strong>Input</strong>: Should be a variable has unstructured text named <code>doc</code>.</p>
<pre class=""lang-py prettyprint-override""><code>doc = &quot;Like APC , &lt;category=&quot;Modifier&quot;&gt;APC2&lt;/category&gt; regulates the formation of active betacatenin-Tcf 
       complexes , as demonstrated using transient transcriptional activation assays in APC - / - 
       &lt;category=&quot;Modifier&quot;&gt;colon carcinoma&lt;/category&gt; cells. Human APC2 maps to chromosome 19p13 . 3 . 
       APC and APC2 may therefore have comparable functions in development 
       and &lt;category=&quot;SpecificDisease&quot;&gt;cancer&lt;/category&gt;&quot;
</code></pre>
<p><strong>Output</strong>: Should be as follows:</p>
<pre class=""lang-py prettyprint-override""><code>{
'Modifier': ['APC2', 'colon carcinoma'],
'SpecificDisease': ['cancer']
}
</code></pre>
<p>This should be automated to be able to extract all category tags in a corpus.</p>
<hr />
<p>I tried the following code:</p>
<pre class=""lang-py prettyprint-override""><code>soup = BeautifulSoup(doc)
contents = soup.find_all('category')
</code></pre>
<p>But didn't know how to extract each flag.</p>
",Dataset Preprocessing & Handling,extract text within flagged tag following document would like extract category flag input variable ha unstructured text named output follows automated able extract category tag corpus tried following code know extract flag
How to run the R RAKE function in udpipe across individual groups,"<p><strong>Given the following sample data frame:</strong></p>
<pre><code>Question &lt;- c(&quot;Q1&quot;, &quot;Q1&quot;, &quot;Q1&quot;,&quot;Q1&quot;,&quot;Q2&quot;, &quot;Q2&quot;, &quot;Q2&quot;,&quot;Q2&quot;)
Answer &lt;- c(&quot;I like to be creative when I cook with crock pots.&quot;,&quot;I like to be creative when I cook with crock pots.&quot;,
            &quot;I like to be creative when I cook with crock pots.&quot;,&quot;I like to be unique when I cook with a skillet.&quot;,
            &quot;I like to be creative when I cook with crock pots.&quot;,&quot;I like to be unique when I cook with a skillet.&quot;,
            &quot;I like to be unique when I cook with a skillet.&quot;,&quot;I like to be unique when I cook with a skillet.&quot;)
QAID &lt;- c(&quot;Q11&quot;, &quot;Q12&quot;, &quot;Q13&quot;,&quot;Q14&quot;,&quot;Q21&quot;, &quot;Q22&quot;, &quot;Q23&quot;,&quot;Q24&quot;)

v &lt;- data.frame(Question, Answer, QAID)
</code></pre>
<p><strong>Given the following code:</strong></p>
<pre><code>library(dplyr)
library(udpipe)

#Download your own instance of the english model to call here
udmodel_english &lt;- udpipe_load_model(file = &quot;english-ewt-ud-2.4-190531.udpipe&quot;)

t &lt;- udpipe_annotate(udmodel_english, v$Answer, doc_id = paste0(v$QAID,'~',v$Question))
x &lt;- data.frame(t)

x &lt;- x %&gt;%
  mutate(Question = sub(&quot;.*~&quot;, &quot;&quot;, doc_id),
         ID = sub(&quot;~.*&quot;, &quot;&quot;, doc_id))

stats &lt;- keywords_rake(x = x, term = &quot;lemma&quot;, group = &quot;Question&quot;, 
                       relevant = x$upos %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;))

x$term &lt;- txt_recode_ngram(x$lemma, compound = stats$keyword, ngram = stats$ngram)
x$term &lt;- ifelse(!x$term %in% stats$keyword, NA, x$term)

x &lt;- x %&gt;%
  left_join(stats, by = c(&quot;term&quot; = &quot;keyword&quot;)) %&gt;%
  filter(!is.na(term))
</code></pre>
<p><strong>I would expect the following output:</strong></p>
<p><a href=""https://i.sstatic.net/kdeS3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kdeS3.png"" alt=""enter image description here"" /></a></p>
<p>I would expect this output as I am trying to group the RAKE output by the question, not across both questions:</p>
<pre><code>keywords_rake(x = x, term = &quot;lemma&quot;, group = &quot;Question&quot;, 
                       relevant = x$upos %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;))
</code></pre>
<p><strong>However, my output looks like this:</strong></p>
<p><a href=""https://i.sstatic.net/U3m5k.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/U3m5k.png"" alt=""enter image description here"" /></a></p>
<p>Even though the keyword Crock Pot is used only once within the group Q2, and 3 times within the group Q1, I get the same rake score, and a freq of 4.</p>
<p>Checking the notes for the <code>group</code> argument within the <code>keywords_rake</code> function turns up the following:</p>
<blockquote>
<p>a character vector with 1 or several columns from x which indicates
for example a document id or a sentence id. Keywords will be computed
within this group in order not to find keywords across sentences or
documents for example.</p>
</blockquote>
<p><strong>My Question:</strong></p>
<p>Am I using the <code>group</code> argument incorrectly? How should I use the RAKE algorithm to get a rake score for a keyword within a single question, not across all questions? I know I could loop through questions, but before I add that overhead, I want to check to see if there is a built in way to handle this. Am I thinking about this function incorrectly?</p>
",Dataset Preprocessing & Handling,run r rake function udpipe across individual group given following sample data frame given following code would expect following output would expect output trying group rake output question across question however output look like even though keyword crock pot used within group q time within group q get rake score freq checking note argument within function turn following character vector several column x indicates example document id sentence id keywords computed within group order find keywords across sentence document example question using argument incorrectly use rake algorithm get rake score keyword within single question across question know could loop question add overhead want check see built way handle thinking function incorrectly
R: Extracting Individual &quot;Terms&quot; from a Matrix,"<p>I am using the R programming language. Using the following 3 &quot;articles&quot; (Shakespeare's plays), I created a &quot;term document matrix&quot; (a R &quot;object&quot; used for text analytics).</p>
<p>First, I create these 3 articles:</p>
<pre><code>#load libraries
library(pdftools)
library(tidytext)
library(textrank)
library(tm)

#1st document
url &lt;- &quot;https://shakespeare.folger.edu/downloads/pdf/hamlet_PDF_FolgerShakespeare.pdf&quot;

article &lt;- pdf_text(url)
article_sentences &lt;- tibble(text = article) %&gt;%
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_id = row_number()) %&gt;%
  select(sentence_id, sentence)


article_words &lt;- article_sentences %&gt;%
  unnest_tokens(word, sentence)


article_words_1 &lt;- article_words %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)

#2nd document
url &lt;- &quot;https://shakespeare.folger.edu/downloads/pdf/macbeth_PDF_FolgerShakespeare.pdf&quot;

article &lt;- pdf_text(url)
article_sentences &lt;- tibble(text = article) %&gt;%
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_id = row_number()) %&gt;%
  select(sentence_id, sentence)


article_words &lt;- article_sentences %&gt;%
  unnest_tokens(word, sentence)


article_words_2&lt;- article_words %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)


#3rd document
url &lt;- &quot;https://shakespeare.folger.edu/downloads/pdf/othello_PDF_FolgerShakespeare.pdf&quot;

article &lt;- pdf_text(url)
article_sentences &lt;- tibble(text = article) %&gt;%
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_id = row_number()) %&gt;%
  select(sentence_id, sentence)


article_words &lt;- article_sentences %&gt;%
  unnest_tokens(word, sentence)


article_words_3 &lt;- article_words %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)
</code></pre>
<p>From here, I create the actual &quot;term document matrix&quot;:</p>
<pre><code>library(tm)

tdm &lt;- TermDocumentMatrix(Corpus(VectorSource(rbind(article_words_1, article_words_2, article_words_3))))
inspect(tdm)
</code></pre>
<p>My question: From this &quot;term document matrix&quot; (the &quot;tdm&quot; object that is created in the above step), is it possible to &quot;extract&quot; (i.e. ungroup) each of these 3 articles? Can you go back and forth between the 3 individual articles and the term document matrix? If I save this term document matrix as an &quot;RDS&quot; file (e.g. tdm.RDS) , close R studio and then re-import this file (tdm.RDS) back into R - will I be able to separate &quot;tdm.RDS&quot; back into &quot;article_words_1&quot;, &quot;article_words_2&quot; and &quot;article_words_3&quot;?</p>
<p>I found some related stackoverflow questions, but they do not seem to specifically answer this question (e.g.  <a href=""https://stackoverflow.com/questions/27364356/inspect-specific-document-from-documenttermmatrix-for-specific-terms"">inspect specific document from DocumentTermMatrix for specific terms</a> , <a href=""https://stackoverflow.com/questions/47328799/extract-top-features-by-frequency-per-document-from-a-dtm-in-r"">Extract top features by frequency per document from a dtm in R</a>).</p>
<p>Thanks</p>
",Dataset Preprocessing & Handling,r extracting individual term matrix using r programming language using following article shakespeare play created term document matrix r object used text analytics first create article create actual term document matrix question term document matrix tdm object created step possible extract e ungroup article go back forth individual article term document matrix save term document matrix rds file e g tdm rds close r studio import file tdm rds back r able separate tdm rds back article word article word article word found related stackoverflow question seem specifically answer question e g thanks
I have written this Python code to make HTTP Post to NLP and then save the result,"<p>This is working fine when paste the content individually.
However, I many text files to read. Need help on how can I make it to read content from text files and then save the result.</p>
<pre class=""lang-py prettyprint-override""><code>import requests
    
url = 'https://language.googleapis.com/v1beta2/documents:annotateText?key=***************'
myobj = {
    &quot;document&quot;: {
        &quot;type&quot;: 1,&quot;language&quot;: &quot;EN&quot;,
        &quot;content&quot;: &quot;INSERT TEXT HERE&quot;
    },
    &quot;features&quot;: {
        &quot;extractSyntax&quot;: &quot;FALSE&quot;,
        &quot;extractEntities&quot;: &quot;TRUE&quot;,
        &quot;extractDocumentSentiment&quot;: &quot;FALSE&quot;,
        &quot;extractEntitySentiment&quot;: &quot;FALSE&quot;,
        &quot;classifyText&quot;: &quot;TRUE&quot;
    }
}
    
def hitGoogleAPI(url, myobj):
    req = requests.post(url,json=myobj)
    response= req.text
    print(response)
    
hitGoogleAPI(url, myobj)
</code></pre>
",Dataset Preprocessing & Handling,written python code make http post nlp save result working fine paste content individually however many text file read need help make read content text file save result
"Using NLTK, how to search for concepts in a text","<p>I'm novice to both Python and NLTK. So, I'm trying to see the representation of some concepts in text using NLTK. I have a CSV file which looks like this <a href=""https://i.sstatic.net/F8hPg.png"" rel=""nofollow noreferrer"">image</a></p>
<p>And I want to see how frequent, e.g., Freedom, Courage, and all other concepts are. I also want to know how to make sure the code looks for bi and trigrams. However, the code I have below only allows me to look for a <strong>single</strong> list of words in a text (Preps.txt <a href=""https://i.sstatic.net/dOQJe.png"" rel=""nofollow noreferrer"">like this</a> ).</p>
<p>The output I expect is something like:
Concept = Frequency in text, i.e., Freedom = 10, Courage = 20</p>
<pre><code>import nltk
from nltk.corpus import PlaintextCorpusReader
corpus_root = '/Users/Muhsa/Myfolder/Concepts' #this is where the texts I want to study are located
Concepts= PlaintextCorpusReader(corpus_root, '.*')
Concepts.fileids()
for fileid in Concepts.fileids():
    text3 = Concepts.words(fileid)
from nltk import word_tokenize
from nltk import FreqDist
text3 = Concepts.words(fileid)
preps = open('preps.txt', encoding=&quot;utf-8&quot;)
rawpreps = preps.read() #preps refer to the file that has the list of words
tokens = word_tokenize(rawpreps)
texty = nltk.Text(tokens)
fdist = nltk.FreqDist(w.lower() for w in text3)
for m in texty:
    print(m + ':', fdist[m], end=' ')  
</code></pre>
",Dataset Preprocessing & Handling,using nltk search concept text novice python nltk trying see representation concept text using nltk csv file look like image want see frequent e g freedom courage concept also want know make sure code look bi trigram however code allows look single list word text prep txt like output expect something like concept frequency text e freedom courage
R: Converting Tibbles to a Term Document Matrix,"<p>I am using the R programming language. I learned how to take pdf files from the internet and load them into R. For example, below I load 3 different books by Shakespeare into R:</p>
<pre><code>library(pdftools)
library(tidytext)
library(textrank)
library(tm)

#1st document
url &lt;- &quot;https://shakespeare.folger.edu/downloads/pdf/hamlet_PDF_FolgerShakespeare.pdf&quot;

article &lt;- pdf_text(url)
article_sentences &lt;- tibble(text = article) %&gt;%
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_id = row_number()) %&gt;%
  select(sentence_id, sentence)


article_words &lt;- article_sentences %&gt;%
  unnest_tokens(word, sentence)


article_words_1 &lt;- article_words %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)

#2nd document
url &lt;- &quot;https://shakespeare.folger.edu/downloads/pdf/macbeth_PDF_FolgerShakespeare.pdf&quot;

article &lt;- pdf_text(url)
article_sentences &lt;- tibble(text = article) %&gt;%
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_id = row_number()) %&gt;%
  select(sentence_id, sentence)


article_words &lt;- article_sentences %&gt;%
  unnest_tokens(word, sentence)


article_words_2&lt;- article_words %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)


#3rd document
url &lt;- &quot;https://shakespeare.folger.edu/downloads/pdf/othello_PDF_FolgerShakespeare.pdf&quot;

article &lt;- pdf_text(url)
article_sentences &lt;- tibble(text = article) %&gt;%
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_id = row_number()) %&gt;%
  select(sentence_id, sentence)


article_words &lt;- article_sentences %&gt;%
  unnest_tokens(word, sentence)


article_words_3 &lt;- article_words %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)
</code></pre>
<p>Each one of these files (e.g. article_words_1) is now a &quot;tibble&quot; file. From here, I want to convert these into a &quot;document term matrix&quot; so that I can perform text mining and NLP on these :</p>
<pre><code>#convert to document term matrix
myCorpus &lt;- Corpus(VectorSource(article_words_1, article_words_2, article_words_3))
tdm &lt;- TermDocumentMatrix(myCorpus)
inspect(tdm)
</code></pre>
<p>But this seems to result in an error:</p>
<pre><code>Error in VectorSource(article_words_1, article_words_2, article_words_3) : 
  unused arguments (article_words_2, article_words_3)
</code></pre>
<p>Can someone please show me what I am doing wrong?</p>
<p>Thanks</p>
",Dataset Preprocessing & Handling,r converting tibbles term document matrix using r programming language learned take pdf file internet load r example load different book shakespeare r one file e g article word tibble file want convert document term matrix perform text mining nlp seems result error someone please show wrong thanks
Unable to tokenise data in python,"<p>This is my code, I want to import a CSV file and only tokenize texts from one column. the column is named 'tweet'. I'm unable to get the output for this code</p>
<pre><code>import nltk
import pandas as pd
import numpy

from nltk import sent_tokenize
from nltk import word_tokenize
from nltk import pos_tag


data = pd.read_csv('/Users/yoshithKotla/Desktop/dingdang/finaldid.csv')

Texts = list(data['tweet'].values)

tokenData = [nltk.word_tokenize(tweet) for tweet in Texts]
</code></pre>
",Dataset Preprocessing & Handling,unable tokenise data python code want import csv file tokenize text one column column named tweet unable get output code
how to make multiple line text file as dataframe in python?,"<p>have the mess text file, need to convert it to csv file format, how to do that for example:</p>
<p>hello this is first line.
this is both lines are one raw.</p>
<p>hello this is second line.
this is both lines are one raw.</p>
<p>how to convert multiple lines of text like a paragraph convert them into data frame or cv format</p>
",Dataset Preprocessing & Handling,make multiple line text file dataframe python mess text file need convert csv file format example hello first line line one raw hello second line line one raw convert multiple line text like paragraph convert data frame cv format
Python: Importing a large function only when needed,"<p>I want to use <code>spacy</code> in my module for some natural language processing. I can load the function <code>nlp</code> as</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_lg&quot;) 
</code></pre>
<p>which takes a long time because the database is nearly 1gb. Suppose the above code is in the file <code>nlp_file.py</code>. If I want to use the function <code>nlp</code> in my module importing using <code>from nlp_file import nlp</code>, I can do one of a few things:</p>
<ol>
<li>Load it when my module is imported in the <code>__init__.py</code> file.</li>
<li>Load it at the beginning of each file that uses it.</li>
<li>Load only when I need to call <code>nlp</code>.</li>
</ol>
<p>I think that 3 is the best option here, but goes against all my python instincts to put import statements at the top of files. If I put it in  <code>__init__.py</code> then everytime I want to use part of my module that doesn't require nlp I will take a massive performance hit. If I load it at the beginning of each file that uses it, I will have the same problem. Therefore it makes most sense to import at the beginning of functions and classes that require nlp.</p>
<p>Am I correct in believing this? Is there a better way to use <code>nlp</code> in different parts of my module? Thank you in advance!</p>
<p>Similar question: <a href=""https://stackoverflow.com/questions/53359891/is-it-good-practice-to-load-large-objects-in-one-file-and-import-them-in-other-p"">Is it good practice to load large objects in one file and import them in other Python files?</a></p>
",Dataset Preprocessing & Handling,python importing large function needed want use module natural language processing load function take long time database nearly gb suppose code file want use function module importing using one thing load module imported file load beginning file us load need call think best option go python instinct put import statement top file put everytime want use part module require nlp take massive performance hit load beginning file us problem therefore make sense import beginning function class require nlp correct believing better way use different part module thank advance similar question href good practice load large object one file import python file
PEGASUS pre-training for summarisation tasks,"<p>I am unsure of how the evaluation for large document summarisation is conducted for the recently introduced <a href=""http://scholar.google.com.sg/scholar_url?url=http://proceedings.mlr.press/v119/zhang20ae/zhang20ae.pdf&amp;hl=en&amp;sa=X&amp;ei=It5iYPzDPKuM6rQP-6OsuAc&amp;scisig=AAGBfm3TMc7EP3WzoI9nibV1Z0HLzzeb2w&amp;nossl=1&amp;oi=scholarr"" rel=""nofollow noreferrer"">PEGASUS model</a> for single document summarisation.</p>
<p>The author's show evaluation against large document datasets like Big Patent, PubMed etc with document lengths exceeding that of the input size to the transformer models.</p>
<p>To quote from the paper, they did talk about this but didn't really elaborate further.</p>
<blockquote>
<p>CNN/DailyMail, Multi-News, arXiv, PubMed, BIG- PATENT datasets contain input documents longer than the maximum input length (<code>L_input = 512 tokens</code>) in pre- training. This would present a problem for position em- beddings which would never be updated for longer input lengths, but we confirm the postulation that sinusoidal po- sitional encodings (Vaswani et al., 2017) generalize well when fine-tuning PEGASUSLARGE beyond the input lengths observed in training up to <code>L_input = 1024 tokens</code>. Since average input length in BIGPATENT, arXiv, PubMed and Multi-News are well beyond 1024 tokens, further scaling up <code>L_input</code> or applying a two-stage approach (Liu et al., 2018) may improve performance even more, although this is out- side the scope of this work.</p>
</blockquote>
<p>They did mention that the input length is up till 1024 tokens. In the PEGASUS Large model on huggingface the max input tokens is also 1024.</p>
<p>I am not sure how they managed to extend their document summarisations for more than 1024 tokens.</p>
<p>I would also like to do similar for my own long document summarisations that I want to try.</p>
",Dataset Preprocessing & Handling,pegasus pre training summarisation task unsure evaluation large document summarisation conducted recently introduced pegasus model single document summarisation author show evaluation large document datasets like big patent pubmed etc document length exceeding input size transformer model quote paper talk really elaborate cnn dailymail multi news arxiv pubmed big patent datasets contain input document longer maximum input length pre training would present problem position em bedding would never updated longer input length confirm postulation sinusoidal po sitional encoding vaswani et al generalize well fine tuning pegasuslarge beyond input length observed training since average input length bigpatent arxiv pubmed multi news well beyond token scaling applying two stage approach liu et al may improve performance even although side scope work mention input length till token pegasus large model huggingface max input token also sure managed extend document summarisation token would also like similar long document summarisation want try
ValueError: setting an array element with a sequence (Assigning a list to a new created column),"<p>I have this code, which basically reads a csv file, founds similar words found in one of the columns of the csv file (which is a string text) and a dictionary of keywords I created, and then returns the words as a list.</p>
<pre><code>import pandas as pd
import re
from nltk.tokenize.treebank import TreebankWordDetokenizer
from langdetect import detect
from sentiment_analysis_spanish import sentiment_analysis
from textblob import TextBlob
import unidecode

df1=pd.read_csv('TFG1.csv', encoding = 'utf8')

def find_all_words(words, sentence):
    all_words = re.findall(r'\w+', sentence)
    words_found = []
    for word in words:

        if word in all_words:
            words_found.append(word)
    return &quot;Words found:&quot;, words_found.__len__(), &quot; The words are:&quot;, words_found


english_dic=['sage', 'selection']
spanish_dic=['grupo', 'bien']

df1[&quot;Reescribe aquí / Rewrite here&quot;].apply(unidecode.unidecode) #para quitar acentos
TreebankWordDetokenizer().detokenize(df1[&quot;Reescribe aquí / Rewrite here&quot;])

i=1
f=0

df1[&quot;Words count&quot;]=0
df1[&quot;Words found&quot;] = None

for rows in [x.lower() for x in df1[&quot;Reescribe aquí / Rewrite here&quot;]]:

    if detect(rows)=='en':
        df1[&quot;Words count&quot;].iloc[f]=find_all_words(english_dic, rows)[1]
        df1[&quot;Words found&quot;].iloc[f]=find_all_words(english_dic, rows)[3]
        print(i, &quot;-&quot;, rows, find_all_words(english_dic, rows))

    elif detect(rows)=='es':
        df1[&quot;Words count&quot;].iloc[f]=find_all_words(spanish_dic, rows)[1]
        df1[&quot;Words found&quot;].iloc[f]=find_all_words(spanish_dic, rows)[3]
        print(i, &quot;-&quot;, rows, find_all_words(spanish_dic, rows))

    f+=1
    i+=1
</code></pre>
<p>The function find_all_words() returns 2 things; the number of words found, for example, if the dictionary I have <code>english_dic=['sage', 'selection']</code> and the text is; <strong>sage said that the selection is good</strong> it will return:
<strong>Words found: 2
The words are: [sage, selection]</strong></p>
<p>However, it gives me an error when I try to assign the list to a new column.</p>
<pre><code>        df1[&quot;Words found&quot;].iloc[f]=find_all_words(english_dic, rows)[3]
</code></pre>
<p>It returns this error: <strong>ValueError: setting an array element with a sequence</strong></p>
<p>Do you have any solution for this?</p>
",Dataset Preprocessing & Handling,valueerror setting array element sequence assigning list new created column code basically read csv file found similar word found one column csv file string text dictionary keywords created return word list function find word return thing number word found example dictionary text sage said selection good return word found word sage selection however give error try assign list new column return error valueerror setting array element sequence solution
How should i process the data in a json/dataframe format so that is suitable for rasa chatbots,"<p>I'm new with NLP and the rasa api. I'm trying to prepare the data so that it can be used as training data for intent recognition. The function that I'm trying to use is:</p>
<pre><code>from rasa_nlu.training_data import load_data   #Import function
train_data_rasa=load_data('/content/data_file.json') #Json file
</code></pre>
<p>However the next error pop ups:</p>
<pre><code>AttributeError: 'str' object has no attribute 'get'
</code></pre>
<p>The json file is the result of using pandas.to_json() function. The original dataset, is the ATIS flight intent dataframe in which there are two columns: The text and the intent.
Here is a preview of the json file:</p>
<pre><code>{&quot;Intent&quot;:{&quot;0&quot;:&quot;atis_flight&quot;,&quot;1&quot;:&quot;atis_flight_time&quot;,&quot;2&quot;:&quot;atis_airfare&quot;,&quot;3&quot;:&quot;atis_airfare&quot;,&quot;4&quot;:&quot;atis_flight&quot;,&quot;5&quot;:&quot;atis_aircraft&quot;,&quot;6&quot; ........
</code></pre>
<p>I don't really know what is going on as the dataset seems to be clean. I have also tried multiple alternatives such as markdown (md) type of file but it does not seem to work.</p>
<p>Thank you in advance !!</p>
",Dataset Preprocessing & Handling,process data json dataframe format suitable rasa chatbots new nlp rasa api trying prepare data used training data intent recognition function trying use however next error pop ups json file result using panda json function original dataset atis flight intent dataframe two column text intent preview json file really know going dataset seems clean also tried multiple alternative markdown md type file doe seem work thank advance
How to use fine-tuned BERT model for sentence encoding?,"<p>I fine-tuned the BERT base model on my own dataset following the script here:</p>
<p><a href=""https://github.com/cedrickchee/pytorch-pretrained-BERT/tree/master/examples/lm_finetuning"" rel=""nofollow noreferrer"">https://github.com/cedrickchee/pytorch-pretrained-BERT/tree/master/examples/lm_finetuning</a></p>
<p>I saved the model as a <code>.pt</code> file and I want to use it now for a sentence similarity task. Unfortunately, it is not clear to me, how to load the fine-tuned model. I tried the following:</p>
<pre><code>model = BertModel.from_pretrained('trained_model.pt')
model.eval()
</code></pre>
<p>This doesn't work. It says:</p>
<pre><code>ReadError: not a gzip file
</code></pre>
<p>So apparently, loading a <code>.pt</code> file with the <code>from_pretrained</code> method is not possible. Can anyone help me out here? Thank's a lot!! :)</p>
<p>Edit: I saved the model in a s3 bucket as follows:</p>
<pre><code># Convert model to buffer
buffer = io.BytesIO()
torch.save(model, buffer)
# Save in s3 bucket
output_model_file = output_folder + &quot;trained_model.pt&quot;
s3_.put_object(Bucket=&quot;power-plant-embeddings&quot;, Key=output_model_file, Body=buffer.getvalue())
</code></pre>
",Dataset Preprocessing & Handling,use fine tuned bert model sentence encoding fine tuned bert base model dataset following script saved model file want use sentence similarity task unfortunately clear load fine tuned model tried following work say apparently loading file method possible anyone help thank lot edit saved model bucket follows
Using GPU for NLP data preprocessing,"<p>I am trying to preprocess my dataset and I am facing problem like it takes ages to Tokenize and Parts of Speech tagging. I am wondering that can I use GPU for faster processing or I have some issues in my code.
Code I am using for word tokenization is:</p>
<pre class=""lang-py prettyprint-override""><code>def word_tokenizer(text):
    out_result = &quot; &quot;
    for words in text:
        final_result = nltk.tokenize.TreebankWordTokenizer().tokenize(words)
    return final_result
    
    train_df['review'] = train_df['review'].apply(lambda z: word_tokenizer(z))
</code></pre>
<p>and my code for parts of speech tagging is:</p>
<pre class=""lang-py prettyprint-override""><code>def pos_tagging(text):
    #data = nltk.pos_tag()
    out_result = &quot; &quot;
    for words in text:
        out_result = nltk.pos_tag_sents(words)
    return out_result
    train_df['review'] = train_df['review'].apply(lambda z: pos_tagging(z))
</code></pre>
<p>Note: Initially I was using <code>nltk.pos_tag</code> but I it was also very slow. I read somewhere that instead of using that use <code>nltk.pos_tag_sents</code> but still it is too slow as my dataset is pretty much large.</p>
<p>A help would be greatly appreciated.</p>
",Dataset Preprocessing & Handling,using gpu nlp data preprocessing trying preprocess dataset facing problem like take age tokenize part speech tagging wondering use gpu faster processing issue code code using word tokenization code part speech tagging note initially wa using wa also slow read somewhere instead using use still slow dataset pretty much large help would greatly appreciated
Restore original text from Keras’s imdb dataset,"<p>Restore original text from Keras’s imdb dataset</p>

<p>I want to restore imdb’s original text from Keras’s imdb dataset.</p>

<p>First, when I load Keras’s imdb dataset, it returned sequence of word index.</p>

<p>

<pre><code>&gt;&gt;&gt; (X_train, y_train), (X_test, y_test) = imdb.load_data()
&gt;&gt;&gt; X_train[0]
[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]
</code></pre>

<p>I found imdb.get_word_index method(), it returns word index dictionary like {‘create’: 984, ‘make’: 94,…}. For converting, I create index word dictionary.


<pre><code>&gt;&gt;&gt; word_index = imdb.get_word_index()
&gt;&gt;&gt; index_word = {v:k for k,v in word_index.items()}
</code></pre>

<p>Then, I tried to restore original text like following.</p>

<p>

<pre><code>&gt;&gt;&gt; ' '.join(index_word.get(w) for w in X_train[5])
""the effort still been that usually makes for of finished sucking ended cbc's an because before if just though something know novel female i i slowly lot of above freshened with connect in of script their that out end his deceptively i i""
</code></pre>

<p>I’m not good at English, but I know this sentence is something strange.</p>

<p>Why is this happened? How can I restore original text?</p>
",Dataset Preprocessing & Handling,restore original text kera imdb dataset restore original text kera imdb dataset want restore imdb original text kera imdb dataset first load kera imdb dataset returned sequence word index found imdb get word index method return word index dictionary like create make converting create index word dictionary tried restore original text like following good english know sentence something strange happened restore original text
How to identify the reviews where the sentiment of review text does not match rating,"<p>can anyone help me with the problem, Im trying to build NLP model where i collect the user reviews and ratings of the app. There are times where user writes a good review but also give low rating. How do i identify those reviews.
CSV file contains user reviews, rating and name column</p>
",Dataset Preprocessing & Handling,identify review sentiment review text doe match rating anyone help problem im trying build nlp model collect user review rating app time user writes good review also give low rating identify review csv file contains user review rating name column
how to assign labels to each token in my datafrmae?,"<p>I want to change the structure of my data frame, I want to convert a data frame,</p>
<p><a href=""https://i.sstatic.net/obWky.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/obWky.png"" alt=""enter image description here"" /></a></p>
<p>e.g first row is so</p>
<pre><code>[Yes, it's annoying and cumbersome to separate your rubbish properly all the time., Three different bin bags stink away in the kitchen and have to be sorted into different wheelie bins., But still Germany produces way too much rubbish, and too many resources are lost when what actually should be separated and recycled is burnt., We Berliners should take the chance and become pioneers in waste separation!]
</code></pre>
<p>as you see in each row, column paragraph, I have a couple of sentences and in the column Label, I have corrpsp[edn label, I want to break assign to each token of sentences each  it is a label and it is Pos tags like this</p>
<pre><code>[(Yes, UH, 0), (,, ,, 0), (it, PRP, 0), ('s, VBZ, 0), (annoying, JJ, 0), (and, CC, 0), (cumbersome, JJ, 0), (to, TO, 0), (separate, VB, 0), (your, PRP$, 0), (rubbish, NN, 0), (properly, RB, 0), (all, PDT, I0), (the, DT, 0), (time, NN, 0), (., ., 0),...]

</code></pre>
<p>i have kind  of done  to tokenized and assign pos tag using spacy</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_sm&quot;)
def mapTokenPos(l):
    tokenlist=[]
    for x in l:
        doc=nlp(x)
        a=[(token.text,token.tag_) for token in doc]
        tokenlist.append(a)
    return tokenlist
df[&quot;IOB&quot;]=df[&quot;Paragraph&quot;].apply(mapTokenPos)
</code></pre>
<p>but I do not know how to assign the labels to</p>
",Dataset Preprocessing & Handling,assign label token datafrmae want change structure data frame want convert data frame e g first row see row column paragraph couple sentence column label corrpsp edn label want break assign token sentence label po tag like kind done tokenized assign po tag using spacy know assign label
ValueError: can&#39;t extend empty axis 0 using modes other than &#39;constant&#39; or &#39;empty&#39; when extracting features using MFCC,"<p>I am working on speech recognition system ,but I am having a problem when I extract the signal features</p>
<p>here is the preprocessing block where i read the audio files and labels and extract the features:</p>
<pre><code>audio_path = r&quot;C:\Users\Salma\Downloads\TORGO&quot;
files = os.listdir(audio_path)
all_wave = []
all_label = []
mfcc_features = []
ignored = {&quot;Notes&quot;, &quot;phn_arrayMic&quot;, &quot;amps&quot;, &quot;rawpos&quot;, &quot;wavall&quot;, &quot;pos&quot;, &quot;alignment&quot;, &quot;Cpcmd&quot;, &quot;EMA&quot;, &quot;VIDEO&quot;, &quot;wav_headMic&quot;, &quot;normpos&quot;, &quot;log&quot;}

for file in files:
    for subfile in os.listdir(audio_path + '/'):
        
        for session in [s for s in os.listdir(audio_path + '/' + subfile +'/')if s not in ignored]:
            for subsession in [x for x in os.listdir(audio_path + '/'+ subfile +'/'+ session + '/')if x not in ignored]:
                for data in [y for y in os.listdir(audio_path + '/'+ subfile +'/'+ session + '/' +subsession +'/') if y not in ignored]:
                    
                    waves = [f for f in os.listdir(audio_path + '/' + subfile +'/'+ session +'/' + subsession +'/' + data + '/')if f.endswith('.wav')]
                    labels = [L for L in os.listdir(audio_path + '/' + subfile +'/'+ session +'/' + subsession + '/' + data + '/')if L.endswith('.txt')]

                    for wav in waves:
                        samples, sample_rate = librosa.load(audio_path + '/' + subfile +'/'+ session + '/'+subsession +'/'+ data +'/'+ wav, sr = 16000)
                        samples = librosa.resample(samples, sample_rate, 16000)
                        mfcc = librosa.feature.mfcc(samples, sample_rate, n_mfcc=13)
                        mfcc = mfcc.T                       
                        all_wave.append(samples)
                        mfcc_features.append(mfcc.tolist())
                    for label in labels:
                        file_path = audio_path + '/' + subfile + '/' + session + '/' + subsession + '/' + data +'/'+ label
                        l = open(file_path, &quot;r&quot;)
                        for target in l:
                        
                            all_label.append(target.strip())
</code></pre>
<p>the code working fine if i removed the mfcc = librosa.feature.mfcc</p>
<p>and here is the return error :</p>
<pre><code>
ValueError                                Traceback (most recent call last)
&lt;ipython-input-13-fd45bf295b2c&gt; in &lt;module&gt;
     12                         samples, sample_rate = librosa.load(audio_path + '/' + subfile +'/'+ session + '/'+subsession +'/'+ data +'/'+ wav, sr = 16000)
     13                         samples = librosa.resample(samples, sample_rate, 16000)
---&gt; 14                         mfcc = librosa.feature.mfcc(samples, sample_rate, n_mfcc=13)
     15                         mfcc = mfcc.T
     16                         all_wave.append(samples)

C:\Users\Salma\Anaconda3\lib\site-packages\librosa\feature\spectral.py in mfcc(y, sr, S, n_mfcc, dct_type, norm, lifter, **kwargs)
   1850 
   1851     if S is None:
-&gt; 1852         S = power_to_db(melspectrogram(y=y, sr=sr, **kwargs))
   1853 
   1854     M = scipy.fftpack.dct(S, axis=0, type=dct_type, norm=norm)[:n_mfcc]

C:\Users\Salma\Anaconda3\lib\site-packages\librosa\feature\spectral.py in melspectrogram(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)
   2003         window=window,
   2004         center=center,
-&gt; 2005         pad_mode=pad_mode,
   2006     )
   2007 

C:\Users\Salma\Anaconda3\lib\site-packages\librosa\core\spectrum.py in _spectrogram(y, S, n_fft, hop_length, power, win_length, window, center, pad_mode)
   2517                     center=center,
   2518                     window=window,
-&gt; 2519                     pad_mode=pad_mode,
   2520                 )
   2521             )

C:\Users\Salma\Anaconda3\lib\site-packages\librosa\core\spectrum.py in stft(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode)
    226             )
    227 
--&gt; 228         y = np.pad(y, int(n_fft // 2), mode=pad_mode)
    229 
    230     elif n_fft &gt; y.shape[-1]:

&lt;__array_function__ internals&gt; in pad(*args, **kwargs)

C:\Users\Salma\Anaconda3\lib\site-packages\numpy\lib\arraypad.py in pad(array, pad_width, mode, **kwargs)
    817                 raise ValueError(
    818                     &quot;can't extend empty axis {} using modes other than &quot;
--&gt; 819                     &quot;'constant' or 'empty'&quot;.format(axis)
    820                 )
    821         # passed, don't need to do anything more as _pad_simple already

ValueError: can't extend empty axis 0 using modes other than 'constant' or 'empty'

</code></pre>
<p>so where is could be the problem here and thanks in advance</p>
",Dataset Preprocessing & Handling,valueerror extend empty axis using mode constant empty extracting feature using mfcc working speech recognition system problem extract signal feature preprocessing block read audio file label extract feature code working fine removed mfcc librosa feature mfcc return error could problem thanks advance
Keywords search in text column of data frame using dictionary,"<p>I am new to python and their is very specific requirement on which I got stuck due to limited knowledge, I will appreciate if someone can help with this</p>
<p>I have generated a dictionary using excel which look like this</p>
<pre><code>dict = {'Fruit' : {'Comb Words' : ['yellow',
                                   'elongated',
                                   'cooking'],
                   'Mandatory Word' : ['banana',
                                       'banana',
                                       'banana']},
       'Animal' : {'Comb Words' : ['mammal',
                                   'white'
                                   'domestic'],
                  'Mandatory Word' : ['cat',
                                      'cat',
                                      'cat']}}
</code></pre>
<p>Now, I have a dataframe which has a text column and I want to match keywords from this dictionary with that column. For example:</p>
<pre><code>            Text                     Mandatory      Comb            Final
A white domestic cat is playing        cat       domestic,white     Animal
yellow banana is not available        banana       yellow           Fruit

</code></pre>
<p>This dictionary is just an idea, I can change it since it is an input from excel. So any other format or way which can result in above output is the only aim here.</p>
",Dataset Preprocessing & Handling,keywords search text column data frame using dictionary new python specific requirement got stuck due limited knowledge appreciate someone help generated dictionary using excel look like dataframe ha text column want match keywords dictionary column example dictionary idea change since input excel format way result output aim
I&#39;ve downloaded bert pretrained model &#39;bert-base-cased&#39;. I&#39;m unable to load the model with help of BertTokenizer,"<p>I've downloaded bert pretrained model 'bert-base-cased. I'm unable to load the model with help of BertTokenizer. I'm trying for bert tokenizer. In the bert-pretrained-model folder I have config.json and pytorch_model.bin.</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained(r'C:\Downloads\bert-pretrained-model')
</code></pre>
<p>I'm facing error like</p>
<pre><code>OSError                                   Traceback (most recent call last)
&lt;ipython-input-17-bd4c0051c48e&gt; in &lt;module&gt;
----&gt; 1 tokenizer = BertTokenizer.from_pretrained(r'\Downloads\bert-pretrained-model')

~\sentiment_analysis\lib\site-packages\transformers\tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
   1775                 f&quot;- or '{pretrained_model_name_or_path}' is the correct path to a directory containing relevant tokenizer files\n\n&quot;
   1776             )
-&gt; 1777             raise EnvironmentError(msg)
   1778 
   1779         for file_id, file_path in vocab_files.items():

OSError: Can't load tokenizer for 'C:\Downloads\bert-pretrained-model'. Make sure that:

- 'C:\Downloads\bert-pretrained-model' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'C:\Downloads\bert-pretrained-model' is the correct path to a directory containing relevant tokenizer files
</code></pre>
<p><strong>When I'm trying load with BertModel, it's loading. But when i'm trying with BertTokenizer it's not loading.</strong></p>
",Dataset Preprocessing & Handling,downloaded bert pretrained model bert base cased unable load model help berttokenizer downloaded bert pretrained model bert base cased unable load model help berttokenizer trying bert tokenizer bert pretrained model folder config json pytorch model bin facing error like trying load bertmodel loading trying berttokenizer loading
Dimension does not match when using `keras.Model.fit` in `BERT` of tensorflow,"<p>I follow the instruction of <a href=""https://www.tensorflow.org/official_models/fine_tuning_bert?hl=en"" rel=""nofollow noreferrer"">Fine-tuning BERT</a> to build a model with my own dataset(It is kind of large, and greater than 20G),  then take steps to re-cdoe my data and load them from <code>tf_record</code> files.
The <code>training_dataset</code> I create has the same signature as that in the instruction</p>
<pre><code>training_dataset.element_spec

({'input_word_ids': TensorSpec(shape=(32, 1024), dtype=tf.int32, name=None), 
'input_mask': TensorSpec(shape=(32, 1024), dtype=tf.int32, name=None), 
'input_type_ids': TensorSpec(shape=(32, 1024), dtype=tf.int32, name=None)}, 
TensorSpec(shape=(32,), dtype=tf.int32, name=None))
</code></pre>
<p>where <code>batch_size</code> is 32, <code>max_seq_length</code> is 1024.
As the instruction suggestes,</p>
<pre><code>The resulting tf.data.Datasets return (features, labels) pairs, as expected by keras.Model.fit
</code></pre>
<p>It semms that everything works as expected,(the instruction does not show how to use <code>training_dataset</code> though ) However, the following code</p>
<pre><code>bert_classifier.fit(
    x = training_dataset, 
    validation_data=test_dataset, # has the same signature just as training_dataset
    batch_size=32,
    epochs=epochs,
    verbose=1,
)
</code></pre>
<p>encouters an error that seems weird to me,</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main
    &quot;__main__&quot;, mod_spec)
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;/home/captain/project/dataload/train.py&quot;, line 81, in &lt;module&gt;
    verbose=1,
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py&quot;, line 1100, in fit
    tmp_logs = self.train_function(iterator)
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 828, in __call__
    result = self._call(*args, **kwds)
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 871, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 726, in _initialize
    *args, **kwds))
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 2969, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 3361, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 3206, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 990, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 634, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 977, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /home/captain/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *
        return step_function(self, iterator)
    /home/captain/.local/lib/python3.7/site-packages/official/nlp/keras_nlp/layers/position_embedding.py:88 call  *
        return tf.broadcast_to(position_embeddings, input_shape)
    /home/captain/.local/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:845 broadcast_to  **
        &quot;BroadcastTo&quot;, input=input, shape=shape, name=name)
    /home/captain/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper
        attrs=attr_protos, op_def=op_def)
    /home/captain/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:592 _create_op_internal
        compute_device)
    /home/captain/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3536 _create_op_internal
        op_def=op_def)
    /home/captain/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:2016 __init__
        control_input_ops, op_def)
    /home/captain/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1856 _create_c_op
        raise ValueError(str(e))

    ValueError: Dimensions must be equal, but are 512 and 1024 for '{{node bert_classifier/bert_encoder_1/position_embedding/BroadcastTo}} = 
BroadcastTo[T=DT_FLOAT, Tidx=DT_INT32](bert_classifier/bert_encoder_1/position_embedding/strided_slice_1, bert_classifier/bert_encoder_1/position_embedding/Shape)' 
with input shapes: [512,768], [3] and with input tensors computed as partial shapes: input[1] = [32,1024,768].
</code></pre>
<p>There is nothing to do with 512, and I didn't use 512 thorough my code. So where is wrong with my code and how to fix that?</p>
",Dataset Preprocessing & Handling,dimension doe match using tensorflow follow instruction fine tuning bert build model dataset kind large greater g take step cdoe data load file create ha signature instruction instruction suggestes semms everything work expected instruction doe show use though however following code encouters error seems weird nothing use thorough code wrong code fix
How to remove irrelevant text data from a large dataset,"<p>I am working on a ML project where data were coming from a social media, and the topic about the data should be depression under Covid-19. However, when I read some of the data retrieved, I noticed that even though the text (around 1-5 %) mentioned some covid-related keywords, the context of those texts are not actually about the pandemic, they are telling a life story (from 5-year-old to 27-year-old) instead of how covid affects their lives.
The data I want to use and am looking for is some texts that tell people how covid makes depression worse and what not.
Is there a general way to clean those irrelevant data whose contexts are not covid-related (or outliers )?
Or is it ok to keep them in the dataset since they only count for 1-5% ?</p>
",Dataset Preprocessing & Handling,remove irrelevant text data large dataset working ml project data coming social medium topic data depression covid however read data retrieved noticed even though text around mentioned covid related keywords context text actually pandemic telling life story year old year old instead covid affect life data want use looking text tell people covid make depression worse general way clean irrelevant data whose context covid related outlier ok keep dataset since count
AllenNLP 2.0: Using `allennlp predict` with MultiTaskDatasetReader leads to RuntimeError,"<p>I trained a multitask model using allennlp 2.0 and now want to predict on new examples using the <code>allennlp predict</code> command.</p>
<p><strong>Problem/Error:</strong>
I am using the following command: <code>allennlp predict results/model.tar.gz new_instances.jsonl --include-package mtl_sd --predictor mtlsd_predictor --use-dataset-reader --dataset-reader-choice validation</code></p>
<p>This gives me the following error:</p>
<pre><code>Traceback (most recent call last):
File &quot;.../mtl_sd_venv/bin/allennlp&quot;, line 10, in &lt;module&gt;
sys.exit(run())
File &quot;.../mtl_sd_venv/lib/python3.7/site-packages/allennlp/__main__.py&quot;, line 34, in run
main(prog=&quot;allennlp&quot;)
File &quot;.../mtl_sd_venv/lib/python3.7/site-packages/allennlp/commands/__init__.py&quot;, line 119, in main
args.func(args)
File &quot;.../mtl_sd_venv/lib/python3.7/site-packages/allennlp/commands/predict.py&quot;, line 220, in _predict
manager.run()
File &quot;.../mtl_sd_venv/lib/python3.7/site-packages/allennlp/commands/predict.py&quot;, line 186, in run
for batch in lazy_groups_of(self._get_instance_data(), self._batch_size):
File &quot;.../mtl_sd_venv/lib/python3.7/site-packages/allennlp/common/util.py&quot;, line 139, in lazy_groups_of
s = list(islice(iterator, group_size))
File &quot;.../mtl_sd_venv/lib/python3.7/site-packages/allennlp/commands/predict.py&quot;, line 180, in _get_instance_data
yield from self._dataset_reader.read(self._input_file)
File &quot;.../mtl_sd_venv/lib/python3.7/site-packages/allennlp/data/dataset_readers/multitask.py&quot;, line 31, in read
raise RuntimeError(&quot;This class is not designed to be called like this&quot;)
RuntimeError: This class is not designed to be called like this
</code></pre>
<p><strong>As far as I understand, that's what's going on:</strong></p>
<p>This <a href=""https://github.com/allenai/allennlp/blob/d2ae540d489336ba05f15479d3c55530b0bd6949/allennlp/data/dataset_readers/multitask.py#L30"" rel=""nofollow noreferrer"">RuntimeError is raised by the MultiTaskDatasetReader</a> because the <code>read()</code>-method of the MultiTaskDatasetReader should not be called  itself. The <code>read()</code>-method should only be called for specific DatasetReaders in <code>MultiTaskDatasetReader.readers</code>.</p>
<p>The read()-method of the MultiTaskDatasetReader is called because in the jsonnet-config I have specified the DatasetsReaders as follows:</p>
<pre class=""lang-json prettyprint-override""><code>&quot;dataset_reader&quot;: {
    &quot;type&quot;: &quot;multitask&quot;,
    &quot;readers&quot;: {
        &quot;SemEval2016&quot;: {
            &quot;type&quot;: &quot;SemEval2016&quot;,
            &quot;max_sequence_length&quot;: 509,
            &quot;token_indexers&quot;: {
                &quot;bert&quot;: {
                    &quot;type&quot;: &quot;pretrained_transformer&quot;,
                    &quot;model_name&quot;: &quot;bert-base-cased&quot;
                }
            },
            &quot;tokenizer&quot;: {
                &quot;type&quot;: &quot;pretrained_transformer&quot;,
                &quot;model_name&quot;: &quot;bert-base-cased&quot;
            }
        }, ...
    }
}
</code></pre>
<p>Usually the <code>type</code> of dataset_reader indicates the dataset-reader class to be instanciated for prediction. But in this case the <code>type</code> just points MultiTaskDatasetReader, which has no <code>read()</code>-method implemented and contains multiple DatasetReaders.</p>
<p>As far as I understand, when using <code>allennlp predict</code> I need to specify somehow which of the multiple DatasetReaders should be used.</p>
<p><strong>The questions is:</strong></p>
<p>How can I specify which specific DatasetReader (of the multiple DatasetReaders in <code>MultiTaskDatasetReader.readers</code>) should be used when executing <code>allennlp predict</code>? Or more generally: How can I get <code>allennlp predict</code> to run with a MultiTaskDatasetReader?</p>
<p><strong>Additional code, for the sake of completeness:</strong>
The predictor:</p>
<pre class=""lang-py prettyprint-override""><code>@Predictor.register('mtlsd_predictor')
class MTLSDPredictor(Predictor):

    def predict(self, sentence: str) -&gt; JsonDict:
        return self.predict_json({'sentence': sentence})

    @overrides
    def _json_to_instance(self, json_dict: JsonDict) -&gt; Instance:
        target = json_dict['text1']
        claim = json_dict['text2']
        return self._dataset_reader.text_to_instance(target, claim)
</code></pre>
",Dataset Preprocessing & Handling,allennlp using multitaskdatasetreader lead runtimeerror trained multitask model using allennlp want predict new example using command problem error using following command give following error far understand going runtimeerror raised multitaskdatasetreader method multitaskdatasetreader called method called specific datasetreaders read method multitaskdatasetreader called jsonnet config specified datasetsreaders follows usually dataset reader indicates dataset reader class instanciated prediction case point multitaskdatasetreader ha method implemented contains multiple datasetreaders far understand using need specify somehow multiple datasetreaders used question specify specific datasetreader multiple datasetreaders used executing generally get run multitaskdatasetreader additional code sake completeness predictor
how to make a pandas take values from multiple lists within a list,"<p>I have a folder with multiple html files.I want the code to go through each and every file and pick the subject verb object triplets using nlp.  I then want pandas to list all of them under the headings of subject verb object for all the files together in one data frame. The problem I face is panda lists only the the subject verb object from the last file and not the first two. When I print the sub_verb_obj in loop it shows 3 lists within a list. But pandas does not pick the 3 lists triplets. Can someone tell me what mistake am I doing?</p>
<pre><code>
sub_verb_obj=[]
folder_path = 'C:/Users/user3/.ipynb_checkpoints/xyz/xyz_2018'
for filename in glob.glob(os.path.join(folder_path, '*.html')):
  with open(filename, 'r',encoding='utf-8') as f:
    pat = f.read()
    
    doc=nlp(text)
    text_ext = textacy.extract.subject_verb_object_triples(doc)
    sub_verb_obj=list(text_ext)
    
sao=pd.DataFrame(sub_verb_obj)
sao.columns=['subject','verb','object']
sao=sao.set_index('subject')
print(sao)```

how can I make sure the pandas lists all the subject verb object from all the files in a folder in a single dataframe?
</code></pre>
",Dataset Preprocessing & Handling,make panda take value multiple list within list folder multiple html file want code go every file pick subject verb object triplet using nlp want panda list heading subject verb object file together one data frame problem face panda list subject verb object last file first two print sub verb obj loop show list within list panda doe pick list triplet someone tell mistake
Plotting KMeans Clustering of Text Data in Python,"<p>I have code that cleans some text data, vectorizes it with TfidfVectorizer, and is run through a KMeans Model. Everything is working ok, with the exception of actually plotting the clusters.</p>
<p>I am not totally understanding the output of TfidVectorizer</p>
<p>For example:</p>
<pre><code>vectorizer = TfidfVectorizer()

X = vectorizer.fit_transform(df['column 1'].values.astype('U'))

print(X)

(0, 36021)  0.17081171474660714

(0, 36020)  0.17081171474660714

(0, 36011)  0.13668653157547714
</code></pre>
<p>Can someone help me understand how to actually plot the clusters? I'm a little stuck on where to go for here. Or is there a better vectorizer to use for KMeans?</p>
<p>Also, when I look at the cluster centers I am seeing weird output, it ends up with a few thousand columns like below... Its a relatively small dataset of about 3000 records of text</p>
<pre><code>print(kmeans.cluster_centers_)

[[8.71020045e-05 8.71020045e-05 8.71020045e-05 ... 1.34902052e-05
  1.34902052e-05 1.34902052e-05]
</code></pre>
<p>Here is some sample code for the clustering as recommended:</p>
<pre><code>df = pd.read_csv('----------------.csv')

vectorizer = TfidfVectorizer()

X = vectorizer.fit_transform(df['column 1'].values.astype('U'))

true_k = 10
model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)

model.fit(X)

print('Top Terms Per Cluster:')
order_centroids = model.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names()
for i in range(true_k):
    print('cluster %d' % i),
    for ind in order_centroids[i, :10]:
        print(' %s' % terms[ind]),
    print

print(model.cluster_centers_)
print(X)
</code></pre>
",Dataset Preprocessing & Handling,plotting kmeans clustering text data python code clean text data vectorizes tfidfvectorizer run kmeans model everything working ok exception actually plotting cluster totally understanding output tfidvectorizer example someone help understand actually plot cluster little stuck go better vectorizer use kmeans also look cluster center seeing weird output end thousand column like relatively small dataset record text sample code clustering recommended
How i can extract only text without tables inside a pdf file using PDFplumber?,"<p>I want to process some pdf files using a NLP module, then I want to clean those files from all existing tables.</p>
<p>this is the code for extracting tables using pdfplumber</p>
<pre><code>import pdfplumber
pdf = pdfplumber.open(&quot;file.pdf&quot;)
page = pdf.pages[1]
table=page.extract_table()
</code></pre>
<p>but I want to inverse the operation to extract text only</p>
",Dataset Preprocessing & Handling,extract text without table inside pdf file using pdfplumber want process pdf file using nlp module want clean file existing table code extracting table using pdfplumber want inverse operation extract text
Difference between a string read from a text file and a string in a variable in Python,"<p>I am writing some python code which will read in textual data from text files and look for the three words to the left and right of a word or a phrase contained in a python list.  I wrote the initial code using a sentence stored as a python string and then converted it to reading from a text file.  This is the original code.</p>
<pre><code>work_list = [&quot;for all good&quot;, &quot;to the aid&quot;, &quot;for all practical purposes&quot;, &quot;all&quot;, &quot;to&quot;, &quot;aid&quot;, &quot;Now&quot;]

txt = &quot;Now is the time to aid and for all practical purposes for all good men to come to the aid of the party&quot;

for element in work_list:
 if element in txt: # was string in earlier version
  s = txt.split()
  # print(f'&quot;{element}&quot; found in file {txt}')
  # was string
  element_split = element.split()
  before = element_split[0] #get first word
  after = element_split[-1] #get last word

  three_before = &quot;&quot; # circumvents warning message 'variable can be initially undefined'
  for i, w in enumerate(s):  # it gives a list items a number
   if w in before: # if the first word in the phrase/word is in the string
    three_before = s[(i - 3):i] if i &gt; 1 else ''
    three_before = ' '.join(word for word in three_before)

         # get the three words after the last word in the phrase/string
   if w in after:  # if the last word in the phrase/string is in the string
      three_after = s[(i+1):(i + 4)] if i  &lt; len(s) else ''
      three_after = ' '.join(word for word in three_after)
      print(&quot;%s &lt;%s&gt; %s&quot; % (three_before, element, three_after))
</code></pre>
<p>Which produces the following output.</p>
<pre><code>is the time &lt;to the aid&gt; and for all
men to come &lt;to the aid&gt; of the party
to aid and &lt;for all practical purposes&gt; for all good
aid and for &lt;all&gt; practical purposes for
practical purposes for &lt;all&gt; good men to
is the time &lt;to&gt; aid and for
all good men &lt;to&gt; come to the
men to come &lt;to&gt; the aid of
the time to &lt;aid&gt; and for all
come to the &lt;aid&gt; of the party
 &lt;Now&gt; is the time
</code></pre>
<p>The code to read from a text file looks like this</p>
<pre><code>work_list = [&quot;for all good&quot;, &quot;to the aid&quot;, &quot;for all practical purposes&quot;, &quot;all&quot;, &quot;to&quot;, &quot;aid&quot;, &quot;Now&quot;]


path = 'D:/Testing10'


context_d = {}
for filename in glob.glob(os.path.join(path, '*.txt')):
    if filename.endswith('.txt'):
        f = open(filename)
        txt = f.read()
        txt = txt.lower()
        s = txt
        s = [item.replace('May', '') for item in s]  # locate and replace all months of May before lowering
        s = [item.replace('\n', '') for item in s]
        s = [item.replace('\\n', '') for item in s]
       # s = [item.replace('\\', '') for item in txt]
       # s = [item.replace('\\s', '') for item in txt]

        for element in work_list:
         if element in txt: # was string in earlier version
          # print(f'&quot;{element}&quot; found in file {string}')
           # was string
          s= txt.split()

          element_split = element.split()
          before = element_split[0] #get first word
          after = element_split[-1] #get last word

          three_before = &quot;&quot; # circumvents warning message 'variable can be initially undefined'
          for i, w in enumerate(s):  # it gives to the list items numbers
           if w in before: # if the first word in the phrase/word is in the string
            three_before = s[(i - 3):i] if i &gt; 1 else ''
            three_before = ' '.join(word for word in three_before)

           #    get the three words after the last word in the phrase/string
           if w in after:  # if the last word in the phrase/string is in the string
            three_after = s[(i+1):(i + 4)] if i  &lt; len(s) else ''
            three_after = ' '.join(word for word in three_after)
            print(&quot;%s &lt;%s&gt; %s&quot; % (three_before, element, three_after))
</code></pre>
<p>Which produces the following output</p>
<pre><code>all practical purposes &lt;for all good&gt; men to come
to aid for &lt;all&gt; practical purposes for
practical purposes for &lt;all&gt; good men to
is the time &lt;to&gt; aid for all
all good men &lt;to&gt; come to the
men to come &lt;to&gt; the aid of
the time to &lt;aid&gt; for all practical
come to the &lt;aid&gt; of the party
</code></pre>
<p>The code reading the text fie containing the sentence is not detecting the phrases &quot;for all practical purposes&quot; and &quot;to the aid&quot;.  Does anyone have an idea why this is happening and if there is a better way to find the words three spaces left and right of the word/phrase (it is very important that this algorithm can pick up all phrases)</p>
<p>Thanks in advance</p>
",Dataset Preprocessing & Handling,difference string read text file string variable python writing python code read textual data text file look three word left right word phrase contained python list wrote initial code using sentence stored python string converted reading text file original code produce following output code read text file look like produce following output code reading text fie containing sentence detecting phrase practical purpose aid doe anyone idea happening better way find word three space left right word phrase important algorithm pick phrase thanks advance
How remove special characters in csv-file and keeping umlaut with python,"<p>Hej, I´m new in the python-world and try to clean data of a twitter-corpus for an linguistic-assignment. The tweets are stored in a csv-file and I want to remove the special characters. The german Umlaut is coded in special characters in the csv-file. So I lose all the umlauts when I remove the special characters.
How can I remove the special charakters and at the same time keep the umlaut?
It would be great if somebody could help me.</p>
<p>That is, what I have so far:</p>
<pre><code>import pandas as pd
data = pd.read_csv(&quot;test_newtest90.csv&quot;) 
data = pd.read_csv(&quot;train_newtest90.csv&quot;) 
import re
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns
import string
import nltk
import warnings 
warnings.filterwarnings(&quot;ignore&quot;, category=DeprecationWarning)

%matplotlib inline

train  = pd.read_csv(&quot;train_newtest90.csv&quot;)
test = pd.read_csv(&quot;test_newtest90.csv&quot;)

combi = train.append(test, ignore_index=True)

def remove_pattern(input_txt, pattern):
    r = re.findall(pattern, input_txt)
    for i in r:
        input_txt = re.sub(i, '', input_txt)
        
    return input_txt    

#Removing Twitter Handles (@user)
combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], &quot;@[\w]*&quot;)

#Removing Short Words: bis len 3
combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)&gt;3]))

#Removing Punctuations, Numbers, and Special Characters
combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(&quot;[^a-zA-Z#]&quot;, &quot; &quot;)
</code></pre>
",Dataset Preprocessing & Handling,remove special character csv file keeping umlaut python hej new python world try clean data twitter corpus linguistic assignment tweet stored csv file want remove special character german umlaut coded special character csv file lose umlaut remove special character remove special charakters time keep umlaut would great somebody could help far
Web scraping python for Arabic text,"<p>I am trying to web Scrape the website: &quot;http://norumors.net/?post_type=rumors?post_type=rumors&quot; to get only the heading news and put them in a CSV file using Beautifulsoup and python, This is the code I am using after i look into the HTML source code &quot;view-source:<a href=""http://norumors.net/?post_type=rumors?post_type=rumors%22"" rel=""nofollow noreferrer"">http://norumors.net/?post_type=rumors?post_type=rumors&quot;</a></p>
<pre><code>import urllib.request,sys,time
from bs4 import BeautifulSoup
import requests
import pandas as pd

pagesToGet= 1

upperframe=[]  
for page in range(1,pagesToGet+1):
    print('processing page :', page)
    url = 'http://norumors.net/?post_type=rumors/?page='+str(page)
    print(url)
    
    #an exception might be thrown, so the code should be in a try-except block
    try:
        #use the browser to get the url. This is suspicious command that might blow up.
        page=requests.get(url)                             # this might throw an exception if something goes wrong.
    
    except Exception as e:                                   # this describes what to do if an exception is thrown
        error_type, error_obj, error_info = sys.exc_info()      # get the exception information
        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem
        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception
        continue                                              #ignore this page. Abandon this and go back.
    time.sleep(2)   
    soup=BeautifulSoup(page.text,'html.parser')
    frame=[]
    links=soup.find_all('li',attrs={'class':'o-listicle__item'})
    print(len(links))
    filename=&quot;NEWS.csv&quot;
    f=open(filename,&quot;w&quot;, encoding = 'utf-8')
    headers=&quot;Statement,Link\n&quot;
    f.write(headers)
    
    for j in links:
        Statement = j.find(&quot;div&quot;,attrs={'class':'row d-flex'}).text.strip()
       # Link = &quot;http://norumors.net/&quot;
        Link += j.find(&quot;div&quot;,attrs={'class':'col-lg-4 col-md-4 col-sm-6 col-xs-6'}).find('a')['href'].strip()
        frame.append((Statement,Link))
        f.write(Statement.replace(&quot;,&quot;,&quot;^&quot;)+&quot;,&quot;+Link+&quot;,&quot;+Date.replace(&quot;,&quot;,&quot;^&quot;)+&quot;,&quot;+Source.replace(&quot;,&quot;,&quot;^&quot;)+&quot;,&quot;+Label.replace(&quot;,&quot;,&quot;^&quot;)+&quot;\n&quot;)
    upperframe.extend(frame)
f.close()
data=pd.DataFrame(upperframe, columns=['Statement','Link'])
data.head()
</code></pre>
<p>but After I run the code I am getting the pandas data frame and CSV file empty, any suggestion why is that? knowing that i want to get the text between  tags.</p>
",Dataset Preprocessing & Handling,web scraping python arabic text trying web scrape website get heading news put csv file using beautifulsoup python code using look html source code view source run code getting panda data frame csv file empty suggestion knowing want get text tag
Binary Classification using the N-Grams,"<p>I want to extract the ngrams of the tweets, from two groups of users (0/1), to make a CSV file as follows for a binary classifier.</p>
<pre><code>user_tweets, ngram1, ngram2, ngram3, ..., label
1, 0.0, 0.0, 0.0, ..., 0
2, 0.0, 0.0, 0.0, ..., 1
..
</code></pre>
<p>My question is whether I should first extract the important ngrams of the two groups, and then score each ngram that I found in the user's tweets? or is there an easier way to do this?</p>
",Dataset Preprocessing & Handling,binary classification using n gram want extract ngrams tweet two group user make csv file follows binary classifier question whether first extract important ngrams two group score ngram found user tweet easier way
How to print comma seperated output?,"<pre><code>from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators = 200)
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

preds=le.inverse_transform(y_pred)
datatocsv=pd.DataFrame({'id':range(1,len(preds)+1),'taste':preds})
datatocsv.to_csv('prediction.csv',index=False)
</code></pre>
<p>Suppose I  have saved the prediction output in a csv file named prediction.csv
and this csv file has two columns 'id' and 'taste' column</p>
<p><img src=""https://i.sstatic.net/gO88P.jpg"" alt=""output_format image"" /></p>
<p>But I want to print the prediction output in the format specified in the image.
Please guide</p>
",Dataset Preprocessing & Handling,print comma seperated output suppose saved prediction output csv file named prediction csv csv file ha two column id taste column want print prediction output format specified image please guide
Underline in RTF reporting in R,"<p>Hi I want to create following RTF from a data frame in R-</p>

<pre><code>df &lt;-

Country         N1   Mean1   SD1   N2   Mean2    SD1 
----------------------------------------------------
Bangladesh      52   25.03   0.02  43   22.31   0.08
Germany         42   95.01   1.02  53   9.31    0.09
Italy            2   20.22   0.00  11   8.09    1.11
---
</code></pre>

<p>I want the report as:</p>

<pre><code>                    Treatment A         Treatment B 
Country          N    Mean    SD     N    Mean     SD 
------------------------------------------------------
Bangladesh      52   25.03   0.02    43   22.31   0.08
Germany         42   95.01   1.02    53   9.31    0.09
Italy            2   20.22   0.00    11   8.09    1.11
---
</code></pre>

<p>with two underlines under texts ""Treatment A"" and ""Treatment B"".
Like-</p>

<pre><code>        Treatment A
------------------------
N        Mean         SD
</code></pre>

<p>Can any one please help me.</p>
",Dataset Preprocessing & Handling,underline rtf reporting r hi want create following rtf data frame r want report two underline text treatment treatment b like one please help
What does documents_columns paramter in Sparse2Corpus do?,"<p>I searched gensim.matutils.Dense2Corpus documentation but I do not find what does True/False value for documents_columns do.
Ex: gensim.matutils.Dense2Corpus(input, documents_columns=True)</p>
",Dataset Preprocessing & Handling,doe document column paramter sparse corpus searched gensim matutils dense corpus documentation find doe true false value document column ex gensim matutils dense corpus input document column true
Is there a simple way to reshape a token object to documents in quanteda?,"<p>I am trying to clean some text data, and after tokenising and e.g. removing punctuation, I want my transform the token object into a vector/dataframe/corpus.</p>
<p>My current approach is:</p>
<pre><code>library(quanteda)
library(dplyr)

raw &lt;- c(&quot;This is text #1.&quot;, &quot;And a second document...&quot;)
tokens &lt;- raw %&gt;% tokens(remove_punct = T)
docs &lt;- lapply(tokens, toString) %&gt;% gsub(pattern = &quot;,&quot;, replacement = &quot;&quot;)
</code></pre>
<p>Is there a more &quot;quanteda&quot; or at least a simpler way to do this?</p>
",Dataset Preprocessing & Handling,simple way reshape token object document quanteda trying clean text data tokenising e g removing punctuation want transform token object vector dataframe corpus current approach quanteda least simpler way
how to load rdf file by virtuoso or other graph database?,"<p>I want to load freebase rdf file. but no documents. so I want to know how to load rdf file to virtuoso graph database, or other graph database.</p>
",Dataset Preprocessing & Handling,load rdf file virtuoso graph database want load freebase rdf file document want know load rdf file virtuoso graph database graph database
BERT embeddings for abstractive text summarisation in Keras using encoder-decoder model,"<p>I am working on a text summarization task using encoder-decoder architecture in Keras. I would like to test the model's performance using different word embeddings such as GloVe and BERT. I already tested it out with GloVe embeddings but could not find an appropriate example for BERT embeddings in seq2seq models using Keras. This is an excerpt of my code:</p>

<pre><code>&lt;...&gt;
# splitting the data

from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data['clean_texts'], data['clean_summaries'], 
                                            test_size=0.2,shuffle=True,random_state=0)
# prepare a tokenizer for inputs

tokenizer = Tokenizer()
tokenizer.fit_on_texts(Xtrain) 

X_train = tokenizer.texts_to_sequences(Xtrain)
X_test = tokenizer.texts_to_sequences(Xtest)

X_train = pad_sequences(X_train, maxlen= MAX_TEXT_LENGTH, padding='post')
X_test = pad_sequences(X_test, maxlen= MAX_TEXT_LENGTH, padding='post')

# prepare a tokenizer for outputs

y_tokenizer = Tokenizer()
y_tokenizer.fit_on_texts(ytrain) 

y_train = y_tokenizer.texts_to_sequences(ytrain)
y_test = y_tokenizer.texts_to_sequences(ytest)

y_train = pad_sequences(y_train, maxlen= MAX_SUM_LENGTH, padding='post')
y_test = pad_sequences(y_test, maxlen= MAX_SUM_LENGTH, padding='post')

Textvocab_size   =  len(tokenizer.word_index) + 1
Sumvocab_size  =   len(y_tokenizer.word_index) + 1 

# Encoder 

encoder_inputs = Input(shape=(MAX_TEXT,))
encoder_embedding = Embedding(Textvocab_size, LATENT_DIMENSION,trainable=True)(encoder_inputs) 

encoderlstm1 = Bidirectional(LSTM(LATENT_DIMENSION,return_sequences=True, return_state=True))
encoder_output1, forward_h1, forward_c1, backward_h1, backward_c1 = encoderlstm1(encoder_embedding)
state_h1 = Concatenate()([forward_h1, backward_h1])
state_c1 = Concatenate()([forward_c1, backward_c1])
encoder_states1 = [state_h1, state_c1]

&lt;...&gt;
</code></pre>

<p>How to add BERT word embeddings to such a model? I tried <a href=""https://pypi.org/project/bert-embedding/"" rel=""nofollow noreferrer"">this implementation</a> on my data frame before tokenization but I ran into an error:</p>

<pre><code>AttributeError: 'str' object has no attribute 'device_typeid'
</code></pre>

<p>I could not find a solution to it. Are there any other ways how to simply add these word embeddings as GloVe? </p>
",Dataset Preprocessing & Handling,bert embeddings abstractive text summarisation kera using encoder decoder model working text summarization task using encoder decoder architecture kera would like test model performance using different word embeddings glove bert already tested glove embeddings could find appropriate example bert embeddings seq seq model using kera excerpt code add bert word embeddings model tried implementation data frame tokenization ran error could find solution way simply add word embeddings glove
Error in tolower(txt) non character argument in R (for textmining),"<p>So as a beginner, I'm trying to do simple text-mining (NLP) using R language.</p>
<p>I preprocessed my data using <code>tm_map</code> function and inspected it and all the punctuations, numbers were removed.
I also converted the text document in lower case using <code>tolower()</code> function.
It worked great.</p>
<p>But while creating a document matrix, I'm encountering an issue where the error is:</p>
<blockquote>
<p>error in tolower(txt): non character argument</p>
</blockquote>
<p>What is this error about and how to go ahead with this?
Is this something related to UTF8?
Any leads would be appreciated.</p>
<pre><code>docs &lt;- tm_map(docs, removePunctuation) 
inspect(docs[1]) 

for(j in seq(docs)) { 
  docs[[j]] &lt;- gsub(&quot;\n&quot;, &quot; &quot;, docs[[j]]) 
} 

docs &lt;- tm_map(docs, removeNumbers) 
docs &lt;- tm_map(docs, content_transformer(tolower)) 
docs &lt;- tm_map(docs, removeWords, stopwords(&quot;english&quot;)) 
docs &lt;- tm_map(docs, stripWhitespace) 
</code></pre>
<p>This all worked just fine and my text document (which is simply an ebook) got converted into lower case, with no white spaces, numbers, etc. just fine and the next step returns the error.</p>
<pre><code># returns the above error. 
dtm &lt;- DocumentTermMatrix(docs)
</code></pre>
",Dataset Preprocessing & Handling,error tolower txt non character argument r textmining beginner trying simple text mining nlp using r language preprocessed data using function inspected punctuation number removed also converted text document lower case using function worked great creating document matrix encountering issue error error tolower txt non character argument error go ahead something related utf lead would appreciated worked fine text document simply ebook got converted lower case white space number etc fine next step return error
Unable to get polarity scores from Vader Sentiment Analyzer,"<p>I am trying to add these new words and their corresponding polarity scores from a CSV file into Vader Sentiment Lexicon</p>
<p><a href=""https://i.sstatic.net/Y7JAx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Y7JAx.png"" alt=""New words csv here"" /></a></p>
<p>It also reflects in the vadersentiment object when it is updated:</p>
<p><a href=""https://i.sstatic.net/hACL4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hACL4.png"" alt=""enter image description here"" /></a></p>
<p>But as soon as I try to get the polarity scores for the newly added words, it throws an error:</p>
<p><a href=""https://i.sstatic.net/0SC5q.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0SC5q.png"" alt=""enter image description here"" /></a></p>
<p>I am confused as to what is happening even though that the word is present in the Vader dictionary:</p>
<p><a href=""https://i.sstatic.net/clc5t.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/clc5t.png"" alt=""enter image description here"" /></a></p>
<p>Does anyone know why is it happening?</p>
",Dataset Preprocessing & Handling,unable get polarity score vader sentiment analyzer trying add new word corresponding polarity score csv file vader sentiment lexicon also reflects vadersentiment object updated soon try get polarity score newly added word throw error confused happening even though word present vader dictionary doe anyone know happening
Restricting the term-document matrix to most frequent unigrams,"<p>The code below as an example for analyzing massive corpus. I want to restrict the term-document matrix to 1000 most frequent unigrams, but changing the <code>max-features</code> parameter to <code>n</code> only return the first <code>n</code> unigrams. Any suggestion?</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer 
import pandas as pd

corpus = ['Hi my name is Joe.', 'Hi my name is Donald.'] 
vectorizer = TfidfVectorizer(max_features=3) 
X = vectorizer.fit_transform(corpus).todense()
    
df = pd.DataFrame(X, columns=vectorizer.get_feature_names()) 
df.to_csv('test.csv')
</code></pre>
",Dataset Preprocessing & Handling,restricting term document matrix frequent unigrams code example analyzing massive corpus want restrict term document matrix frequent unigrams changing parameter return first unigrams suggestion
How do I leverage Spark&#39;s pipelines to find phrases in strings then add feature category?,"<p>I would like to search my text column in a pyspark data frame for phrases. Here is an example to show you what I mean.</p>
<pre><code>sentenceData = spark.createDataFrame([
(0, &quot;Hi I heard about Spark&quot;),
(4, &quot;I wish Java could use case classes&quot;),
(11, &quot;Logistic regression models are neat&quot;)], 
[&quot;id&quot;, &quot;sentence&quot;])
</code></pre>
<p>If the sentence contains &quot;heard about spark&quot; then categorySpark=1 and categoryHeard=1.</p>
<p>If the sentence contains &quot;java OR regression&quot; then categoryCool=1.</p>
<p>I have about 28 booleans (or maybe better if I use regex) to check for.</p>
<pre><code>sentenceData.withColumn('categoryCool',sentenceData['sentence'].rlike('Java | regression')).show()
</code></pre>
<p>returns:</p>
<pre><code>+---+--------------------+------------+
| id|            sentence|categoryCool|
+---+--------------------+------------+
|  0|Hi I heard about ...|       false|
|  4|I wish Java could...|        true|
| 11|Logistic regressi...|        true|
+---+--------------------+------------+
</code></pre>
<p>This is what I want, but I'd like to add it to a pipeline as a transformation step.</p>
",Dataset Preprocessing & Handling,leverage spark pipeline find phrase string add feature category would like search text column pyspark data frame phrase example show mean sentence contains heard spark categoryspark categoryheard sentence contains java regression categorycool booleans maybe better use regex check return want like add pipeline transformation step
Colab OSError: [Errno 36] File name too long when reading a docx2text file,"<p>I am studying NLP techniques and while I have some experience with .txt files, using .docx has been troublesome. I am trying to use regex on strings, and since I am using a word document, this is my approach:</p>
<p>I will use textract to get a docx to txt and get the bytes to strings:</p>
<pre><code>import textract
my_text = textract.process(&quot;1337.docx&quot;)
my_text = text.decode(&quot;utf-8&quot;)
</code></pre>
<p>I read the file:</p>
<pre><code>def load_doc(filename):

  # open the file as read only
  file = open(filename, 'r')

  # read all text
  text = file.read()

  # close the file
  file.close()

  return text
</code></pre>
<p>I then try and do some regexs such as remove all numbers and etc, and when executing it in the main:</p>
<pre><code>def regextest(doc):

...

...
text = load_doc(my_text)
tokens = regextest(text)
print(tokens)
</code></pre>
<p>I get the exception:</p>
<pre><code>OSError: [Errno 36] File name too long: Are you buying a Tesla?\n\n\n\n - I believe the pricing is...(and more text from te file)
</code></pre>
<p>I know I am transforming my docx file to a text file and then, when I read the &quot;filename&quot;, it is actually the whole text. How can I preserve the file and make it work? How would you guys approach this?</p>
",Dataset Preprocessing & Handling,colab oserror errno file name long reading docx text file studying nlp technique experience txt file using docx ha troublesome trying use regex string since using word document approach use textract get docx txt get byte string read file try regexs remove number etc executing main get exception know transforming docx file text file read filename actually whole text preserve file make work would guy approach
Object class change in quanteda::corpus,"<p>I am having a problem with running a script that I wrote a few weeks ago and it was running with no errors then. The script was to read a few thousands of files and create a corpus from them. But now, I don't know why, the same code returns a different class of object at the beginning of script and as a result, following sections of the code break.</p>
<p>The documents I want to read are international treaties. Before, when I applied quanteda::corpus, it returned S3 corpus list objects under &quot;Data&quot; in the global environment. But now, when I apply the function, I get corpus class character objects under &quot;Values&quot;.</p>
<p>Here is a sample from the code aimed at reading only one treaty to illustrate the problem:</p>
<pre><code>&gt; #This is the link to the file to be read: 
&gt; [https://investmentpolicy.unctad.org/international-investment-agreements/treaty-files/5908/download][1]

&gt; mar_jp_txt&lt;- readtext(&quot;/Desktop/mar_jp.pdf&quot;)
&gt; corpus_txt_mjp &lt;- corpus(mar_jp_txt) %&gt;%
  corpus_reshape(to = &quot;sentences&quot;)

&gt; class(corpus_txt_mjp)
[1] &quot;corpus&quot;    &quot;character&quot;
</code></pre>
<p>However, I have a previously saved workspace image and there I have the <code>corpus_txt_mjp</code> object too. When I check its class, it is different:</p>
<pre><code>&gt; class(corpus_txt_mjp)
[1] &quot;corpus&quot; &quot;list&quot;  
</code></pre>
<p>Considering that I started this project the last October, I should be using the same version of quanteda since then. Yet, even though I have changed nothing in the script, I get a different outcome now. Back then I was running the script on small samples to see whether it works and when I try to run it on the entire sample now, I get this problem. What could be the reason for this and can someone please offer me a solution?</p>
<p>Many many thanks in advance!</p>
<p><strong>EDIT</strong></p>
<p>I guess I realized the issue. I don't know why and how, but last month I was using an older version of quanteda. When I installed the recent version, the code stopped working. So in this case I should either install an older version or change the script according to the new version.</p>
",Dataset Preprocessing & Handling,object class change quanteda corpus problem running script wrote week ago wa running error script wa read thousand file create corpus know code return different class object beginning script result following section code break document want read international treaty applied quanteda corpus returned corpus list object data global environment apply function get corpus class character object value sample code aimed reading one treaty illustrate problem however previously saved workspace image object check class different considering started project last october using version quanteda since yet even though changed nothing script get different outcome back wa running script small sample see whether work try run entire sample get problem could reason someone please offer solution many many thanks advance edit guess realized issue know last month wa using older version quanteda installed recent version code stopped working case either install older version change script according new version
How to process similar notations with Python?,"<p>I have list with keywords and their corresponding searchvolumes as CSV file. Keywords are in german language. Some keywords are unique, other keywords have slightly different notations, like in this example:</p>
<pre><code>+--------------------------------------+-----+
| verkehrsrechtsschutz rückwirkend     | 50  |
+--------------------------------------+-----+
| verkehrs-rechtsschutz rückwirkend    | 50  |
+--------------------------------------+-----+
| familien rechtsschutzversicherung    | 100 |
+--------------------------------------+-----+
| familienrechtsschutzversicherung     | 100 |
+--------------------------------------+-----+
| privat rechtsschutz ohne wartezeit   | 20  |
+--------------------------------------+-----+
| privater rechtsschutz ohne wartezeit | 20  |
+--------------------------------------+-----+
| rechtsschutzversicherung strafrecht  | 80  |
+--------------------------------------+-----+
| strafrechtsschutz                    | 80  |
+--------------------------------------+-----+
| rechtsschutzversicherung gewerbe     | 200 |
+--------------------------------------+-----+
| rechtsschutzversicherung gewerblich  | 200 |
+--------------------------------------+-----+
| fahrer rechtsschutz                  | 160 |
+--------------------------------------+-----+
| fahrerrechtsschutz                   | 160 |
+--------------------------------------+-----+
| fahrer-rechtsschutz                  | 160 |
+--------------------------------------+-----+
</code></pre>
<p>Similar noted keywords often have same suchvolumes - but not always.</p>
<p>I'm looking a way to move all keywords with similar notation into another file.</p>
<p>I guess, it could be done with Python, but don't know, what module, package or library has such special language processing capability to recognize similar notations and to decide about relation of keywords between each other.</p>
<p>Please point me into the right direction.</p>
<p><strong>Update</strong>: Solutions, which calculate similarity ratio will deliver very high amount of false positives and negatives - because of german language structure. I think rather about a tool, which &quot;knows&quot; german linguistics and works with a language and not with string differences. Maybe something like <a href=""https://pypi.org/project/textblob-de/"" rel=""nofollow noreferrer"">https://pypi.org/project/textblob-de/</a>, <a href=""https://spacy.io/models/de"" rel=""nofollow noreferrer"">https://spacy.io/models/de</a> or something from <a href=""https://github.com/adbar/German-NLP"" rel=""nofollow noreferrer"">https://github.com/adbar/German-NLP</a></p>
<p>I was already trying some tools, which calculate string differences - some VBA and Google App scripts, they fail miserably.</p>
",Dataset Preprocessing & Handling,process similar notation python list keywords corresponding searchvolumes csv file keywords german language keywords unique keywords slightly different notation like example similar noted keywords often suchvolumes always looking way move keywords similar notation another file guess could done python know module package library ha special language processing capability recognize similar notation decide relation keywords please point right direction update solution calculate similarity ratio deliver high amount false positive negative german language structure think rather tool know german linguistics work language string difference maybe something like something wa already trying tool calculate string difference vba google app script fail miserably
Why would a scaled SVD run so much slower than an unscaled SVD in a Random Forest model?,"<p>I'm studying machine learning and NLP in Python by recreating the common &quot;predict spam messages&quot; project. I did all the preliminary steps of cleanup and preprocessing until I got a TF-IDF document-term matrix of 2,000 terms. I then performed SVD to reduce it to 300 terms (or components) and scaled the results so that I could run a quick logistic classifier to get a benchmark for later models.</p>
<p>Later in the project, while building random forests, I realized I had forgotten to comment out the scaler below and was building the forests with the scaled SVD, which is totally unnecessary. However, I did not realize this would slow down the random forests compared to the unscaled SVD, and worse, sensitivity was about 10% lower as well.</p>
<p>Can anyone help me understand <strong>why this is so</strong>?</p>
<p>Here are results of the grid search with the best (highest sensitivity) unscaled SVD:</p>
<pre><code>Elapsed: 1348 s
Best params: {'max_depth': 20, 'max_features': 250, 'min_samples_split': 10, 'n_estimators': 200}
Confusion matrix on validation set:
     pred_neg  pred_pos
neg       844         2
pos         5       124
Evaluation metrics:
accuracy: 0.9928
sensitivity: 0.9612
specificity: 0.9976
</code></pre>
<p>Here are the results of the grid search with the best (highest sensitivity) scaled SVD:</p>
<pre><code>Elapsed: 5297 s
Best params: {'max_depth': 5, 'max_features': 250, 'min_samples_split': 5, 'n_estimators': 200}
Confusion matrix on validation set:
     pred_neg  pred_pos
neg       838         8
pos        18       111
Evaluation metrics:
accuracy: 0.9733
sensitivity: 0.8605
specificity: 0.9905
</code></pre>
<p>Here's the culprit:</p>
<pre><code>from scipy.sparse.linalg import svds
from sklearn.utils.extmath import svd_flip
from sklearn.preprocessing import MaxAbsScaler

def perform_SVD(X, n_components=300):

    # transpose to a term-document matrix
    U, Sigma, VT = svds(X.asfptype().T, 
                        k=n_components)
    # reverse outputs
    Sigma = Sigma[::-1]
    U, VT = svd_flip(U[:, ::-1], VT[::-1])
    
    # transpose to get V
    V = VT.T
    
    # scale for logistic classifier only
    # can't take log of negative numbers
    # ends up predicting ham base rate
    # comment out for random forests!
    scaler = MaxAbsScaler()
    X_scaled = scaler.fit_transform(V) 
    
    return X_scaled
</code></pre>
",Dataset Preprocessing & Handling,would scaled svd run much slower unscaled svd random forest model studying machine learning nlp python recreating common predict spam message project preliminary step cleanup preprocessing got tf idf document term matrix term performed svd reduce term component scaled result could run quick logistic classifier get benchmark later model later project building random forest realized forgotten comment scaler wa building forest scaled svd totally unnecessary however realize would slow random forest compared unscaled svd worse sensitivity wa lower well anyone help understand result grid search best highest sensitivity unscaled svd result grid search best highest sensitivity scaled svd culprit
How to cluster a large text corpus (e.g. list of job titles) using Python or R?,"<p>I have a text corpus - the list of job tiles extracted from the web. The list is pretty clean and stored as one column CSV file where titles are listed in rows.</p>
<p>I have tried approaches using TF-IDF and Affinity Propagation, but this runs into memory issues. I tried to do this using <code>word2vec</code> and then applying a clustering algorithm, but it's not showing decent results. What could be the most effective way to cluster the dataset of around 75k job titles?</p>
",Dataset Preprocessing & Handling,cluster large text corpus e g list job title using python r text corpus list job tile extracted web list pretty clean stored one column csv file title listed row tried approach using tf idf affinity propagation run memory issue tried using applying clustering algorithm showing decent result could effective way cluster dataset around k job title
Extract specific lines of text in r,"<p>I have a <strong>.txt</strong> file with thousands of lines. In this file, I have a meta information about research articles. Every paper has information about Published year (PY), Title (TI), DOI number (DI), Publishing Type (PT) and Abstract (AB). So, the information of almost 300 papers exist in the text file. The format of information about first two article is as follows.</p>
<pre><code>PT J
AU Filieri, Raffaele
   Acikgoz, Fulya
   Ndou, Valentina
   Dwivedi, Yogesh
TI Is TripAdvisor still relevant? The influence of review credibility,
   review usefulness, and ease of use on consumers' continuance intention
SO INTERNATIONAL JOURNAL OF CONTEMPORARY HOSPITALITY MANAGEMENT
DI 10.1108/IJCHM-05-2020-0402
EA NOV 2020
PY 2020
AB Purpose - Recent figures show that users are discontinuing their usage
   of TripAdvisor, the leading user-generated content (UGC) platform in the
   tourism sector. Hence, it is relevant to study the factors that
   influence travelers' continued use of TripAdvisor.
   Design/methodology/approach - The authors have integrated constructs
   from the technology acceptance model, information systems (IS)
   continuance model and electronic word of mouth literature. They used
   PLS-SEM (smartPLS V.3.2.8) to test the hypotheses using data from 297
   users of TripAdvisor recruited through Prolific.
   Findings - Findings reveal that perceived ease of use, online consumer
   review (OCR) credibility and OCR usefulness have a positive impact on
   customer satisfaction, which ultimately leads to continuance intention
   of UGC platforms. Customer satisfaction mediates the effect of the
   independent variables on continuance intention.
   Practical implications - Managers of UGC platforms (i.e. TripAdvisor)
   can benefit from the findings of this study. Specifically, they should
   improve the ease of use of their platforms by facilitating travelers'
   information searches. Moreover, they should use signals to make credible
   and helpful content stand out from the crowd of reviews.
   Originality/value - This is the first study that adopts the IS
   continuance model in the travel and tourism literature to research the
   factors influencing consumers' continued use of travel-based UGC
   platforms. Moreover, the authors have extended this model by including
   new constructs that are particularly relevant to UGC platforms, such as
   performance heuristics and OCR credibility.
ZR 0
ZA 0
Z8 0
ZS 0
TC 0
ZB 0
Z9 0
SN 0959-6119
EI 1757-1049
UT WOS:000592516500001
ER

PT J
AU Li, Yelin
   Bu, Hui
   Li, Jiahong
   Wu, Junjie
TI The role of text-extracted investor sentiment in Chinese stock price
   prediction with the enhancement of deep learning
SO INTERNATIONAL JOURNAL OF FORECASTING
VL 36
IS 4
BP 1541
EP 1562
DI 10.1016/j.ijforecast.2020.05.001
PD OCT-DEC 2020
PY 2020
AB Whether investor sentiment affects stock prices is an issue of
   long-standing interest for economists. We conduct a comprehensive study
   of the predictability of investor sentiment, which is measured directly
   by extracting expectations from online user-generated content (UGC) on
   the stock message board of Eastmoney.com in the Chinese stock market. We
   consider the influential factors in prediction, including the selections
   of different text classification algorithms, price forecasting models,
   time horizons, and information update schemes. Using comparisons of the
   long short-term memory (LSTM) model, logistic regression, support vector
   machine, and Naive Bayes model, the results show that daily investor
   sentiment contains predictive information only for open prices, while
   the hourly sentiment has two hours of leading predictability for closing
   prices. Investors do update their expectations during trading hours.
   Moreover, our results reveal that advanced models, such as LSTM, can
   provide more predictive power with investor sentiment only if the inputs
   of a model contain predictive information. (C) 2020 International
   Institute of Forecasters. Published by Elsevier B.V. All rights
   reserved.
CT 14th International Conference on Services Systems and Services
   Management (ICSSSM)
CY JUN 16-18, 2017
CL Dongbei Univ Finance &amp; Econ, Sch Management Sci &amp; Engn, Dalian, PEOPLES
   R CHINA
HO Dongbei Univ Finance &amp; Econ, Sch Management Sci &amp; Engn
SP Tsinghua Univ; Chinese Univ Hong Kong; IEEE Syst Man &amp; Cybernet Soc
ZA 0
TC 0
ZB 0
ZS 0
Z8 0
ZR 0
Z9 0
SN 0169-2070
EI 1872-8200
UT WOS:000570797300025
ER
</code></pre>
<p>Now, I want to extract the abstract of each article and store it in the data frame. To extract the abstract I have the following code, which gives me the first match of abstract.</p>
<pre><code>f = readLines(&quot;sample.txt&quot;)
#extract first match....
pattern &lt;- &quot;AB\\s*(.*?)\\s*ZR&quot;
result &lt;- regmatches(as.String(f), regexec(pattern, as.String(f)))
result[[1]][2]
[1] &quot;Purpose - Recent figures show that users are discontinuing their usage\n   of TripAdvisor, the leading user-generated content (UGC) platform in the\n   tourism sector. Hence, it is relevant to study the factors that\n   influence travelers' continued use of TripAdvisor.\n   Design/methodology/approach - The authors have integrated constructs\n   from the technology acceptance model, information systems (IS)\n   continuance model and electronic word of mouth literature. They used\n   PLS-SEM (smartPLS V.3.2.8) to test the hypotheses using data from 297\n   users of TripAdvisor recruited through Prolific.\n   Findings - Findings reveal that perceived ease of use, online consumer\n   review (OCR) credibility and OCR usefulness have a positive impact on\n   customer satisfaction, which ultimately leads to continuance intention\n   of UGC platforms. Customer satisfaction mediates the effect of the\n   independent variables on continuance intention.\n   Practical implications - Managers of UGC platforms (i.e. TripAdvisor)\n   can benefit from the findings of this study. Specifically, they should\n   improve the ease of use of their platforms by facilitating travelers'\n   information searches. Moreover, they should use signals to make credible\n   and helpful content stand out from the crowd of reviews.\n   Originality/value - This is the first study that adopts the IS\n   continuance model in the travel and tourism literature to research the\n   factors influencing consumers' continued use of travel-based UGC\n   platforms. Moreover, the authors have extended this model by including\n   new constructs that are particularly relevant to UGC platforms, such as\n   performance heuristics and OCR credibility.&quot;
</code></pre>
<p>The problem is, I want to extract all the abstracts but the pattern would be different for most of the abstracts. So the specific pattern for all the abstract is that I should extract text starting from <strong>AB</strong> and every next line having space in the front. Any body can help me in this regard?</p>
",Dataset Preprocessing & Handling,extract specific line text r txt file thousand line file meta information research article every paper ha information published year py title ti doi number di publishing type pt abstract ab information almost paper exist text file format information first two article follows want extract abstract article store data frame extract abstract following code give first match abstract problem want extract abstract pattern would different abstract specific pattern abstract extract text starting ab every next line space front body help regard
How to get the wikipedia corpus text with punctuation by using gensim wikicorpus?,"<p>I'm trying to get the text with its punctuation as it is important to consider the latter in my doc2vec model.  However, the wikicorpus retrieve only the text. After searching the web I found these pages:</p>

<ol>
<li>Page from gensim github issues section. It was a question by someone where the answer was to subclass WikiCorpus (answered by Piskvorky). Luckily, in the same page, there was a code representing the suggested 'subclass' solution. The code was provided by Rhazegh. (<a href=""https://github.com/RaRe-Technologies/gensim/issues/552"" rel=""noreferrer"">link</a>)</li>
<li>Page from stackoverflow with a title: ""Disabling Gensim's removal of punctuation etc. when parsing a wiki corpus"". However, no clear answer was provided and was treated in the context of spaCy. (<a href=""https://stackoverflow.com/questions/43500996/disabling-gensims-removal-of-punctuation-etc-when-parsing-a-wiki-corpus"">link</a>)</li>
</ol>

<p>I decided to use the code provided in page 1. My current code (mywikicorpus.py):</p>

<pre><code>import sys
import os
sys.path.append('C:\\Users\\Ghaliamus\\Anaconda2\\envs\\wiki\\Lib\\site-packages\\gensim\\corpora\\')

from wikicorpus import *

def tokenize(content):
    # override original method in wikicorpus.py
    return [token.encode('utf8') for token in utils.tokenize(content, lower=True, errors='ignore')
        if len(token) &lt;= 15 and not token.startswith('_')]

def process_article(args):
   # override original method in wikicorpus.py
    text, lemmatize, title, pageid = args
    text = filter_wiki(text)
    if lemmatize:
        result = utils.lemmatize(text)
    else:
        result = tokenize(text)
    return result, title, pageid


class MyWikiCorpus(WikiCorpus):
def __init__(self, fname, processes=None, lemmatize=utils.has_pattern(), dictionary=None, filter_namespaces=('0',)):
    WikiCorpus.__init__(self, fname, processes, lemmatize, dictionary, filter_namespaces)

    def get_texts(self):
        articles, articles_all = 0, 0
        positions, positions_all = 0, 0
        texts = ((text, self.lemmatize, title, pageid) for title, text, pageid in extract_pages(bz2.BZ2File(self.fname), self.filter_namespaces))
        pool = multiprocessing.Pool(self.processes)
        for group in utils.chunkize(texts, chunksize=10 * self.processes, maxsize=1):
            for tokens, title, pageid in pool.imap(process_article, group):  # chunksize=10):
                articles_all += 1
                positions_all += len(tokens)
            if len(tokens) &lt; ARTICLE_MIN_WORDS or any(title.startswith(ignore + ':') for ignore in IGNORED_NAMESPACES):
                continue
            articles += 1
            positions += len(tokens)
            if self.metadata:
                yield (tokens, (pageid, title))
            else:
                yield tokens
    pool.terminate()

    logger.info(
        ""finished iterating over Wikipedia corpus of %i documents with %i positions""
        "" (total %i articles, %i positions before pruning articles shorter than %i words)"",
        articles, positions, articles_all, positions_all, ARTICLE_MIN_WORDS)
    self.length = articles  # cache corpus length
</code></pre>

<p>And then, I used another code by Pan Yang (<a href=""https://textminingonline.com/training-word2vec-model-on-english-wikipedia-by-gensim"" rel=""noreferrer"">link</a>). This code initiates WikiCorpus object and retrieve the text. The only change in my current code is initiating MyWikiCorpus instead of WikiCorpus. The code (process_wiki.py):</p>

<pre><code>from __future__ import print_function
import logging
import os.path
import six
import sys
import mywikicorpus as myModule



if __name__ == '__main__':
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
    logging.root.setLevel(level=logging.INFO)
    logger.info(""running %s"" % ' '.join(sys.argv))

    # check and process input arguments
    if len(sys.argv) != 3:
        print(""Using: python process_wiki.py enwiki-20180601-pages-    articles.xml.bz2 wiki.en.text"")
        sys.exit(1)
    inp, outp = sys.argv[1:3]
    space = "" ""
    i = 0

    output = open(outp, 'w')
    wiki = myModule.MyWikiCorpus(inp, lemmatize=False, dictionary={})
    for text in wiki.get_texts():
        if six.PY3:
            output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\n')
        else:
            output.write(space.join(text) + ""\n"")
        i = i + 1
        if (i % 10000 == 0):
            logger.info(""Saved "" + str(i) + "" articles"")

    output.close()
    logger.info(""Finished Saved "" + str(i) + "" articles"")
</code></pre>

<p>Through command line I ran the process_wiki.py code. I got text of the corpus with the last line in the command prompt: </p>

<p>(2018-06-05 09:18:16,480: INFO: Finished Saved 4526191 articles)</p>

<p>When I read the file in python, I checked the first article and it was without punctuation. Example:</p>

<p>(anarchism is a political philosophy that advocates self governed societies based on voluntary institutions these are often described as stateless societies although several authors have defined them more specifically as institutions based on non hierarchical or free associations anarchism holds the state to be undesirable unnecessary and harmful while opposition to the state is central anarchism specifically entails opposing authority or hierarchical)</p>

<p>My two relevant questions, and I wish you can help me with them, please:</p>

<ol>
<li>is there any thing wrong in my reported pipeline above?</li>
<li>regardless such pipeline, if I opened the gensim  wikicorpus python code (<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/corpora/wikicorpus.py"" rel=""noreferrer"">wikicorpus.py</a>) and wanted to edit it, what is the line that I should add it or remove it or update it (with what if possible) to get the same results but with punctuation?</li>
</ol>

<p>Many thanks for your time reading this long post. </p>

<p>Best wishes,</p>

<p>Ghaliamus </p>
",Dataset Preprocessing & Handling,get wikipedia corpus text punctuation using gensim wikicorpus trying get text punctuation important consider latter doc vec model however wikicorpus retrieve text searching web found page page gensim github issue section wa question someone answer wa subclass wikicorpus answered piskvorky luckily page wa code representing suggested subclass solution code wa provided rhazegh link page stackoverflow title disabling gensim removal punctuation etc parsing wiki corpus however clear answer wa provided wa treated context spacy link code initiate wikicorpus object retrieve text change current code initiating mywikicorpus instead wikicorpus code process wiki py command line ran process wiki py code got text corpus last line command prompt info finished saved article read file python checked first article wa without punctuation example anarchism political philosophy advocate self governed society based voluntary institution often described stateless society although several author defined specifically institution based non hierarchical free association anarchism hold state undesirable unnecessary harmful opposition state central anarchism specifically entail opposing authority hierarchical two relevant question wish help please thing wrong reported pipeline regardless pipeline opened gensim wikicorpus python code wikicorpus py wanted edit line add remove update possible get result punctuation many thanks time reading long post best wish ghaliamus
How to use python to convert a Japanese PDF or HTML file into unicode,"<p>I am trying to use python to read a Japanese PDF or HTML file as input, and I want to get each Japanese characters' unicode in the file.</p>
<p>Someone suggests that I can use 'tika' library to read a PDF file. I ran the following code and got a series of garbled text as below.</p>
<pre><code>import tika
from tika import parser
parsed = parser.from_file('jpn.pdf')
print(parsed[&quot;content&quot;])
</code></pre>
<p>result:</p>
<p>��������������������������������</p>
<p>�1948.12.10
������</p>
<p>����������������</p>
<p>�������������� �!&quot;#$%&amp;'()#�&amp;*+,-.#/01�(</p>
<p>)#2345678(9:3;&lt;=&gt;$?�@A&amp;B(�&amp;3</p>
<p>�-�CD=&gt;EFG3���HI/JK6LMNOPQR/SNTU3VW=&gt;XY</p>
<p>�9:GZ8T[3]=&gt;^_�+,45�`aG3Yc��d�ef�gh#U�</p>
<p>iVj[N�&amp;3</p>
<p>�kGlm#no#6p�(eq�rs#U�tu6vw()#G+,xy6�(Nz</p>
<p>623{�|}6xM��-/~��()#G��&amp;B(�&amp;3</p>
<p>k��������/���()#G��&amp;B(�&amp;3</p>
<p>���67,�3@���-3�k�!&quot;=&gt;���&gt;6��</p>
<p>��-6�,��X�/��1U3��3Y��*+9:�y�&amp;���� #¡¢£</p>
<p>¤�¥¦#/���()#/§¨UN�&amp;3</p>
<p>���#«¬U�3�-=&gt;@��9:�­®�+!¯=&gt;°±���/</p>
<p>²��()#/³´UN�&amp;3</p>
<p>)[T�-.=&gt;9:6p�(µ¶�·¸23)�³´/¹º6�(Nz6SM#S¯</p>
<p>�&amp;B(�&amp;3</p>
<p>���23 63</p>
<p>9Á�»¼�=&gt;»½�G3)�45�-iV/¾6�¿6À*+GT3©ª</p>
<p>�ÃÄÅ6B(ÆÇ����k6S3)[T�-.#9:</p>
<p>#�!¯/ÈÉ=&gt;ÊË6xM����()#�&gt;6Ì[T�­®�ÍÀ6xM��~</p>
<p>#G²���*µ¶�@¤#U�����#����</p>
<p>�3)��-iV/ÏÐ�(Ñ</p>
<p>Is there any recommended Python library or code to deal with the aforesaid problem ?
This is my first time to ask question on this platform. Please help......</p>
",Dataset Preprocessing & Handling,use python convert japanese pdf html file unicode trying use python read japanese pdf html file input want get japanese character unicode file someone suggests use tika library read pdf file ran following code got series garbled text result b cd efg hi jk lmnopqr sntu vw xy gz ag yc ef gh u ivj n kglm p eq r u tu vw g xy nz xm g b k g b k x u un u un p nz sm b g iv gt b k xm xm g u iv recommended python library code deal aforesaid problem first time ask question platform please help
How to extract text from a docx file and store in a text file,"<p>I have been trying to read a .docx file and copy its text to a .txt file</p>

<p>I started off by writing this piece of script for achieving the above results.</p>

<pre><code>if extension == 'docx' :

   document = Document(filepath)
      for para in document.paragraphs:
         with open(""C:/Users/prasu/Desktop/PySumm-resource/CodeSamples/output.txt"",""w"") as file:
            file.writelines(para.text)
</code></pre>

<p>The error occurred is as follows :</p>

<pre><code>Traceback (most recent call last):
  File ""input_script.py"", line 27, in &lt;module&gt;
    file.writelines(para.text)
  File ""C:\Python\lib\encodings\cp1252.py"", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode character '\u2265' in 
position 0: character maps to &lt;undefined&gt;
</code></pre>

<p>I tried printing ""para.text"" with the help of print(), it works.
Now, I want to write ""para.text"" to a .txt file.</p>
",Dataset Preprocessing & Handling,extract text docx file store text file trying read docx file copy text txt file started writing piece script achieving result error occurred follows tried printing para text help print work want write para text txt file
"Why does my code return random letter tokens, instead of word tokens?","<p>I'm an absolute beginner with Python, and I am very stuck at this part. I tried creating a function to preprocess my texts/data for topic modeling, and it works perfectly when I ran it as an individual code, but when it does not return anything when I ran it as a function. I would appreciate any help!</p>
<ul>
<li>The codes I'm using are very basic, and probably inefficient, but it's for my basic class, so really basic ways is the way to go for me!</li>
</ul>
<p>codes:</p>
<pre class=""lang-py prettyprint-override""><code>def clean (data):
    data_prep = []
    for data in data:
        tokenized_words = nltk.word_tokenize (data)
        text_words = [token.lower() for token in tokenized_words if token.isalnum()]
        text_words = [word for word in text_words if word not in stop_words]
        text_joined = &quot; &quot;.join(textwords)
        data_prep.append(text_joined)
        
    return data_prep
</code></pre>
<p>the outputs are really random like &quot;j&quot;, &quot;,&quot;, &quot;i&quot;. I was using a .txt file as my data, converted from a .csv file.</p>
<p>edit:</p>
<p>I've adjusted my codes from pointed mistakes and it is now</p>
<pre><code>def clean (data):
    data_prep = []
    for row in data:
        tokenized_words = nltk.word_tokenize (data)
        text_words = [token.lower() for token in tokenized_words if token.isalnum()]
        text_words = [word for word in text_words if word not in stop_words]
        text_joined = &quot; &quot;.join(text_words)
        data_prep.append(text_joined)
    return data_prep
</code></pre>
<p>results: it now returns tokenized sentences and seemingly on loop.</p>
<p>what is my mistake this time?</p>
<p><a href=""https://i.sstatic.net/uUHtL.png"" rel=""nofollow noreferrer"">see image</a></p>
",Dataset Preprocessing & Handling,doe code return random letter token instead word token absolute beginner python stuck part tried creating function preprocess text data topic modeling work perfectly ran individual code doe return anything ran function would appreciate help code using basic probably inefficient basic class really basic way way go code output really random like j wa using txt file data converted csv file edit adjusted code pointed mistake result return tokenized sentence seemingly loop mistake time see image
Extract multi-word phrases with surrounding context,"<p>I have written some code which looks for the three words before and after certain key words which are contained in multiple lists which are subsequently joined into a larger list (list &quot;words&quot; in the code). It writes the output to a .csv file and works fine for single words but not phrases. To illustrate there are a series of lists containing words such as</p>
<pre class=""lang-py prettyprint-override""><code>approx = [&quot;approximately&quot;]
could = [&quot;could&quot;]
</code></pre>
<p>and phrases such as</p>
<pre class=""lang-py prettyprint-override""><code>can_be = [&quot;can be&quot;]
shouldbeso = [&quot;should be so&quot;]
</code></pre>
<p>These are concatenated into one larger list which we're calling &quot;words&quot;.  The program can find all 3 words to the left  and to the right of &quot;approximately&quot;, &quot;could&quot; or similar words, but misses out completely the phrases.</p>
<p>Is there a way to modify the code to detect the three words before and after specific phrases?  My code is as follows.</p>
<pre class=""lang-py prettyprint-override""><code>path = 'D:/Testing'

context_d = {}
for filename in glob.glob(os.path.join(path, '*.txt')):
    if filename.endswith('.txt'):
        f = open(filename)
        file = f.read()
       # txt = file.lower()
        txt = file.split()
        txt = [item.replace('May', '') for item in txt]  # locate and replace all months of May before lowering
        # txt = list([[word.lower() for word in line.split()] for line in txt])
        txt = (list(map(lambda x: x.lower(), txt)))
        for j in range(len(txt)):
          if (j + 3) &lt; len(txt):
           if txt[j] in words:
            if txt[j] in context_d:
             context_d[txt[j]] += txt[(j - 3):j]
             context_d[txt[j]] += txt[(j + 1):(j + 3)]
            else:
             context_d[txt[j]] = txt[(j - 3):j]
             context_d[txt[j]] += txt[(j + 1):(j + 3)]
</code></pre>
",Dataset Preprocessing & Handling,extract multi word phrase surrounding context written code look three word certain key word contained multiple list subsequently joined larger list list word code writes output csv file work fine single word phrase illustrate series list containing word phrase concatenated one larger list calling word program find word left right approximately could similar word miss completely phrase way modify code detect three word specific phrase code follows
spaCy CLI debug shows 0 train/dev docs in CLI-formatted JSON converted by spacy.gold.docs_to_json,"<h1>Issue</h1>

<p>I am trying to run the spaCy CLI but my training data and dev data seem somehow to be incorrect as seen when I run debug:</p>

<pre><code>| =&gt; python3 -m spacy debug-data en 

./CLI_train_randsplit_anno191022.json ./CLI_dev_randsplit_anno191022.json --pipeline ner --verbose 




=========================== Data format validation =========================== 

✔ Corpus is loadable 




=============================== Training stats =============================== 

Training pipeline: ner 

Starting with blank model 'en' 

0 training docs 

0 evaluation docs 

✔ No overlap between training and evaluation data 

✘ Low number of examples to train from a blank model (0) 

It's recommended to use at least 2000 examples (minimum 100) 




============================== Vocab &amp; Vectors ============================== 

ℹ 0 total words in the data (0 unique) 

10 most common words: 

ℹ No word vectors present in the model 




========================== Named Entity Recognition ========================== 

ℹ 0 new labels, 0 existing labels 

0 missing values (tokens with '-' label) 

✔ Good amount of examples for all labels 

✔ Examples without occurrences available for all labels 

✔ No entities consisting of or starting/ending with whitespace 




================================== Summary ================================== 

✔ 5 checks passed 

✘ 1 error 
</code></pre>

<p>Trying to train anyway yields:</p>

<pre><code>| =&gt; python3 -m spacy train en ./models/CLI_1 ./CLI_train_randsplit_anno191022.json ./CLI_dev_randsplit_anno191022.json -n 150 -p 'ner' --verbose 

dropout_from = 0.2 by default 

dropout_to = 0.2 by default 

dropout_decay = 0.0 by default 

batch_from = 100.0 by default 

batch_to = 1000.0 by default 

batch_compound = 1.001 by default 

Training pipeline: ['ner'] 

Starting with blank model 'en' 

beam_width = 1 by default 

beam_density = 0.0 by default 

beam_update_prob = 1.0 by default 

Counting training words (limit=0) 

learn_rate = 0.001 by default 

optimizer_B1 = 0.9 by default 

optimizer_B2 = 0.999 by default 

optimizer_eps = 1e-08 by default 

L2_penalty = 1e-06 by default 

grad_norm_clip = 1.0 by default 

parser_hidden_depth = 1 by default 

subword_features = True by default 

conv_depth = 4 by default 

bilstm_depth = 0 by default 

parser_maxout_pieces = 2 by default 

token_vector_width = 96 by default 

hidden_width = 64 by default 

embed_size = 2000 by default 




Itn  NER Loss   NER P   NER R   NER F   Token %  CPU WPS 

---  ---------  ------  ------  ------  -------  ------- 

✔ Saved model to output directory 

models/CLI_1/model-final 




Traceback (most recent call last): 

  File ""/usr/local/lib/python3.7/site-packages/spacy/cli/train.py"", line 389, in train 

    scorer = nlp_loaded.evaluate(dev_docs, verbose=verbose) 

  File ""/usr/local/lib/python3.7/site-packages/spacy/language.py"", line 673, in evaluate 

    docs, golds = zip(*docs_golds) 

ValueError: not enough values to unpack (expected 2, got 0) 




During handling of the above exception, another exception occurred: 




Traceback (most recent call last): 

  File ""/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py"", line 193, in _run_module_as_main 

    ""__main__"", mod_spec) 

  File ""/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py"", line 85, in _run_code 

    exec(code, run_globals) 

  File ""/usr/local/lib/python3.7/site-packages/spacy/__main__.py"", line 35, in &lt;module&gt; 

    plac.call(commands[command], sys.argv[1:]) 

  File ""/usr/local/lib/python3.7/site-packages/plac_core.py"", line 328, in call 

    cmd, result = parser.consume(arglist) 

  File ""/usr/local/lib/python3.7/site-packages/plac_core.py"", line 207, in consume 

    return cmd, self.func(*(args + varargs + extraopts), **kwargs) 

  File ""/usr/local/lib/python3.7/site-packages/spacy/cli/train.py"", line 486, in train 

    best_model_path = _collate_best_model(meta, output_path, nlp.pipe_names) 

  File ""/usr/local/lib/python3.7/site-packages/spacy/cli/train.py"", line 548, in _collate_best_model 

    bests[component] = _find_best(output_path, component) 

  File ""/usr/local/lib/python3.7/site-packages/spacy/cli/train.py"", line 567, in _find_best 

    accs = srsly.read_json(epoch_model / ""accuracy.json"") 

  File ""/usr/local/lib/python3.7/site-packages/srsly/_json_api.py"", line 50, in read_json 

    file_path = force_path(location) 

  File ""/usr/local/lib/python3.7/site-packages/srsly/util.py"", line 21, in force_path 

    raise ValueError(""Can't read file: {}"".format(location)) 

ValueError: Can't read file: models/CLI_1/model0/accuracy.json 
</code></pre>

<p>My training and dev docs were generated using spacy.gold.docs_to_json(), saved as json files using the function: </p>

<pre><code>def make_CLI_json(mock_docs, CLI_out_file_path): 
    CLI_json = docs_to_json(mock_docs) 
    with open(CLI_out_file_path, 'w') as json_file: 
        json.dump(CLI_json, json_file) 
</code></pre>

<p>I verified them both to be valid json at <a href=""http://www.jsonlint.com"" rel=""nofollow noreferrer"">http://www.jsonlint.com</a>.</p>

<p>I created the docs from which these json originated using the function:  </p>

<pre><code>def import_from_doccano(jx_in_file_path, view=True): 
    annotations = load_jsonl(jx_in_file_path)     
    mock_nlp = English()     
    sentencizer = mock_nlp.create_pipe(""sentencizer"")     
    unlabeled = 0     
    DATA = []     
    mock_docs = [] 

    for anno in annotations:                      
        # get DATA (as used in spacy inline training)     
        if ""label"" in anno.keys():     
            ents = [tuple([label[0], label[1], label[2]])     
                               for label in anno[""labels""]] 
        else: 
            ents = [] 

        DATUM = (anno[""text""], {""entities"": ents}) 
        DATA.append(DATUM) 

        # mock a doc for viz in displacy 
        mock_doc = mock_nlp(anno[""text""]) 
        if ""labels"" in anno.keys():     
            entities = anno[""labels""]     
            if not entities:     
                unlabeled += 1     
            ents = [(e[0], e[1], e[2]) for e in entities]     
            spans = [mock_doc.char_span(s, e, label=L) for s, e, L in ents]     
            mock_doc.ents = _cleanup_spans(spans)     
            sentencizer(mock_doc) 

            if view:     
                displacy.render(mock_doc, style='ent') 

        mock_docs.append(mock_doc)                      
    print(f'Unlabeled: {unlabeled}')                      
    return DATA, mock_docs 
</code></pre>

<p>I wrote the function above to return the examples in both the format required for inline training (e.g. as shown at <a href=""https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py</a>) as well as to form these kind of “mock” docs so that I can use displacy and/or the CLI. For the latter purpose, I followed the code shown at <a href=""https://github.com/explosion/spaCy/blob/master/spacy/cli/converters/jsonl2json.py"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/master/spacy/cli/converters/jsonl2json.py</a> with a couple of notable differences. The _cleanup_spans() function is identical to the one in the example. I did not use the minibatch() but made a separate doc for each of my labeled annotations. Also, (perhaps critically?) I found that using the sentencizer ruined many of my annotations, possibly because the spans get shifted in a way that the _cleanup_spans() function fails to repair properly. Removing the sentencizer causes the docs_to_json() function to throw an error. In my function (unlike in the linked example) I therefore run the sentencizer on each doc <em>after</em> the entities are written to them, which preserves my annotations properly and allows the docs_to_json() function to run without complaints.  </p>

<p>The function load_jsonl called within import_from_doccano() is defined as:</p>

<pre><code>def load_jsonl(input_path):     
    data = []     
    with open(input_path, 'r', encoding='utf-8') as f:     
        for line in f:     
            data.append(json.loads(line.replace('\n|\r',''), strict=False))     
    print('Loaded {} records from {}'.format(len(data), input_path))     
    print()     
    return data 
</code></pre>

<p>My annotations are each of length ~10000 characters or less. They are exported from doccano </p>

<p>(<a href=""https://doccano.herokuapp.com/"" rel=""nofollow noreferrer"">https://doccano.herokuapp.com/</a>) as JSONL using the format:</p>

<pre><code>{""id"": 1, ""text"": ""EU rejects ..."", ""labels"": [[0,2,""ORG""], [11,17, ""MISC""], [34,41,""ORG""]]}
{""id"": 2, ""text"": ""Peter Blackburn"", ""labels"": [[0, 15, ""PERSON""]]}
{""id"": 3, ""text"": ""President Obama"", ""labels"": [[10, 15, ""PERSON""]]}
...
</code></pre>

<p>The data are split into train and test sets using the function:</p>

<pre><code>def test_train_split(DATA, mock_docs, n_train):
    L = list(zip(DATA, mock_docs))
    random.shuffle(L)
    DATA, mock_docs = zip(*L)
    DATA = [i for i in DATA]
    mock_docs = [i for i in mock_docs]
    TRAIN_DATA = DATA[:n_train]
    train_docs = mock_docs[:n_train]
    TEST_DATA = DATA[n_train:] 
    test_docs = mock_docs[n_train:]
    return TRAIN_DATA, TEST_DATA, train_docs, test_docs
</code></pre>

<p>And finally each is written to json using the following function:</p>

<pre><code>def make_CLI_json(mock_docs, CLI_out_file_path):
    CLI_json = docs_to_json(mock_docs)
    with open(CLI_out_file_path, 'w') as json_file:
        json.dump(CLI_json, json_file)
</code></pre>

<p>I do not understand why the debug shows 0 training docs and 0 development docs, or why the train command fails. The JSON look correct as far as I can tell. <strong>Is my data formatted incorrectly, or is there something else going on?</strong> Any help or insights would be greatly appreciated.</p>

<p><em>This is my first question on SE- apologies in advance if I've failed to follow some or other guideline. There are a lot of components involved so I'm not sure how I might produce a minimal code example that would replicate my problem.</em></p>

<h1>Environment</h1>

<p>Mac OS 10.15 Catalina
Everything is pip3 installed into user path 
No virtual environment</p>

<pre><code>| =&gt; python3 -m spacy info --markdown

## Info about spaCy

* **spaCy version:** 2.2.1
* **Platform:** Darwin-19.0.0-x86_64-i386-64bit
* **Python version:** 3.7.4
</code></pre>
",Dataset Preprocessing & Handling,spacy cli debug show train dev doc cli formatted json converted spacy gold doc json issue trying run spacy cli training data dev data seem somehow incorrect seen run debug trying train anyway yield training dev doc generated using spacy gold doc json saved json file using function verified valid json created doc json originated using function wrote function return example format required inline training e g shown well form kind mock doc use displacy cli latter purpose followed code shown couple notable difference cleanup span function identical one example use minibatch made separate doc labeled annotation also perhaps critically found using sentencizer ruined many annotation possibly span get shifted way cleanup span function fails repair properly removing sentencizer cause doc json function throw error function unlike linked example therefore run sentencizer doc entity written preserve annotation properly allows doc json function run without complaint function load jsonl called within import doccano defined annotation length character le exported doccano jsonl using format data split train test set using function finally written json using following function understand debug show training doc development doc train command fails json look correct far tell data formatted incorrectly something else going help insight would greatly appreciated first question se apology advance failed follow guideline lot component involved sure might produce minimal code example would replicate problem environment mac catalina everything pip installed user path virtual environment
Calculating term frequencies in a big corpus efficiently regardless of document boundaries,"<p>I have a corpus of almost 2m documents. I want to calculate the term frequencies of the terms in the whole corpus, regardless of document boundaries.</p>
<p>A naive approach would be combining all the documents into one very big document and vectorising it.</p>
<p>An elaborate approach is building a full-blow TDM with <code>tm</code> or any tool, where one would have the term frequencies of each term in each document, and thus in the whole corpus. Here is how I do it:</p>
<pre><code># Build a TDM from the 'corpus' tibble using a filtered 'texts' column:

htgs = VCorpus(VectorSource(subset(x = corpus,
                                   subset = condition)$texts))
# Some preprocessing
htgs = preprocess(htgs)

# Consider terms whose length is between 2 and Inf in the TDM (the default is 3 to Inf):
dtm_htgs = TermDocumentMatrix(htgs,
                              control = list(wordLengths=c(2, Inf)))
</code></pre>
<pre><code>&gt; dtm_htgs
&lt;&lt;TermDocumentMatrix (terms: 495679, documents: 1983567)&gt;&gt;
Non-/sparse entries: 5361931/983207145062
Sparsity           : 100%
Maximal term length: 170
Weighting          : term frequency (tf)
</code></pre>
<p>However, trying to unpack the sparse matrix that results from this attempt is, expectedly, uttering out the memory error:</p>
<pre><code>&gt; m = as.matrix(dtm_htgs)

Error: cannot allocate vector of size 7325.5 Gb
</code></pre>
<p>How to disregard per-document counts and suffice with the global term frequencies in the corpus as a whole, which can save up a lot of memory?</p>
",Dataset Preprocessing & Handling,calculating term frequency big corpus efficiently regardless document boundary corpus almost document want calculate term frequency term whole corpus regardless document boundary naive approach would combining document one big document vectorising elaborate approach building full blow tdm tool one would term frequency term document thus whole corpus however trying unpack sparse matrix result attempt expectedly uttering memory error disregard per document count suffice global term frequency corpus whole save lot memory
Writing out a list of phrases to a csv file,"<p>Following on from an earlier <a href=""https://stackoverflow.com/questions/65295940/how-do-i-count-all-occurrences-of-a-phrase-in-a-text-file-using-regular-expressi"">post</a>, I have written some Python code to calculate the frequency of occurrences of certain phrases (contained in the &quot;word_list&quot; variable with three examples listed but will have many more) in a large number of text files.  The code I've written below requires me to take each element of the list and insert it into a string for comparison to each text file.  However the current code is only writing the frequencies for the last phrase in the list rather than all of them to the relevant columns in a spreadsheet.  Is this just an indent issue, not placing the writerow in the correct position or is there a logic flaw in my code.  Also  is there any way to avoid using a list to string assignment in order to compare the phrases to those in the text files?</p>
<pre><code>word_list = ['in the event of', 'frankly speaking', 'on the other hand']
S = {}
p = 0
k = 0

with open(file_path, 'w+', newline='') as csv_file:
    writer = csv.writer(csv_file)
    writer.writerow([&quot;Fohone-K&quot;] + word_list)

    for filename in glob.glob(os.path.join(path, '*.txt')):
     if filename.endswith('.txt'):
        f = open(filename)
        Fohone-K = filename[8:]
        data = f.read()
        # new code section from scratch file
        l = len(word_list)
        for s in range(l):
         phrase = word_list[s]
         S = data.count((phrase))
         if S:
          #k = k + 1
          print(&quot;'{}' match&quot;.format(Fohone-K), S)
         else:
          print(&quot;'{} no match&quot;.format(Fohone-K))
          print(&quot;\n&quot;)

          # for m in word_list:
     if S &gt;= 0:
      print([Fohone-K] + [S])
     writer.writerow([Fohone-K] + [S])
</code></pre>
<p>The output currently looks like this.</p>
<p><a href=""https://i.sstatic.net/LMReU.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>When it needs to look like this.</p>
<p><a href=""https://i.sstatic.net/RXB05.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",Dataset Preprocessing & Handling,writing list phrase csv file following earlier enter image description need look like enter image description
NLP using NLTK take join between Tokenized list and csv file,"<pre><code>mast_dict = pd.read_csv('C:/Users/ZAM/Desktop/LoughranMcDonald_MasterDictionary_2018.csv')
stop_words = set(stopwords.words('english')) 
#print(stopwords.words('english'))

filtered_text = []
with open('C:/Users/ZAM/Downloads/HEURO/file0.txt') as fin:
    tokens = word_tokenize(fin.read()) 
for r in words:  
    if not r in stop_words:  
        filtered_text.append(r)
filtered_text = [''.join(c for c in s if c not in string.punctuation) for s in filtered_text]
filtered_text = [x for x in filtered_text if not (x.isdigit())]
print(filtered_text)
</code></pre>
<pre><code>Output
['BEGIN', 'PRIVACYENHANCED', 'MESSAGE', 'ProcType', '2001MICCLEAR', 'OriginatorName', 'webmasterwwwsecgov', 'OriginatorKeyAsymmetric', 'MFgwCgYEVQgBAQICAf8DSgAwRwJAW2sNKK9AVtBzYZmr6aGjlWyK3XmZv3dTINen', 'TWSM7vrzLADbmYQaionwg5sDW3P6oaM5D3tdezXMm7z1TBtwIDAQAB', 'MICInfo', 'RSAMD5RSA', 'RHiO0hkbOslk2iB2eQM8lSgcXyjFKRha0FtFBz3xsf7Z6kKaWhrhStjKFZUvWZf', 'eDg67b8ZljTBWxC8ybQfg', ...]
</code></pre>
<p>The <code>mast_dict</code> variable contains around 86K English words with positive scores, negative scores, polarity scores, etc.
I want to make a joint between the <code>filtered_text</code>list which contains tokenized words as shown in the <code>mast_dict</code> CSV file so I can get the scores of the tokenized words in the <code>filtered_text</code> list.</p>
<p>Let me know. Thanks!</p>
",Dataset Preprocessing & Handling,nlp using nltk take join tokenized list csv file variable contains around k english word positive score negative score polarity score etc want make joint list contains tokenized word shown csv file get score tokenized word list let know thanks
How can I Convert a dataset to glove or word2vec format?,"<p>I have my twitter archive downloaded and wanted to run word2vec to experiment most similar words, analogies etc on it.</p>
<p>But I am stuck at first step - how to convert a given dataset / csv / document so that it can be input to word2vec? i.e. what is the process to convert data to glove/word2vec format?</p>
",Dataset Preprocessing & Handling,convert dataset glove word vec format twitter archive downloaded wanted run word vec experiment similar word analogy etc stuck first step convert given dataset csv document input word vec e process convert data glove word vec format
remove String row in pandas data frame when number of words is less than N,"<p>I am pre-processing dataset for NLP classification task, i want to drop the sentences with less than 3 words, the code i tried drop the words with less than 3 letters:</p>
<pre><code>import re
text = &quot;The quick brown fox jumps over the lazy dog.&quot;
# remove words between 1 and 3
shortword = re.compile(r'\W*\b\w{1,3}\b')
print(shortword.sub('', text))
</code></pre>
<p>how to do this in python?</p>
",Dataset Preprocessing & Handling,remove string row panda data frame number word le n pre processing dataset nlp classification task want drop sentence le word code tried drop word le letter python
"In R, how can I count specific words in a corpus?","<p>I need to count the frequency of particular words. Lots of words. I know how to do this by putting all words in one group (see below), but I would like to get the count for each specific word.</p>
<p>This is what I have at the moment:</p>
<pre><code>library(quanteda)
#function to count 
strcount &lt;- function(x, pattern, split){unlist(lapply(strsplit(x, split),function(z) na.omit(length(grep(pattern, z)))))}
txt &lt;- &quot;Forty-four Americans have now taken the presidential oath. The words have been spoken during rising tides of prosperity and the still waters of peace. Yet, every so often the oath is taken amidst gathering clouds and raging storms. At these moments, America has carried on not simply because of the skill or vision of those in high office, but because We the People have remained faithful to the ideals of our forbearers, and true to our founding documents.&quot;
df&lt;-data.frame(txt)
mydict&lt;-dictionary(list(all_terms=c(&quot;clouds&quot;,&quot;storms&quot;)))
corp &lt;- corpus(df, text_field = 'txt')
#count terms and save output to &quot;overview&quot;
overview&lt;-dfm(corp,dictionary = mydict)
overview&lt;-convert(overview, to ='data.frame')
</code></pre>
<p>As you can see, the counts for &quot;clouds&quot; and &quot;storms&quot; are in the &quot;all_terms&quot; category in the resulting data.frame. Is there an easy way to get the count for all terms in &quot;mydict&quot; in individual columns, without writing the code for each individual term?</p>
<pre><code>E.g.
clouds, storms
1, 1

Rather than 
all_terms
2
</code></pre>
",Dataset Preprocessing & Handling,r count specific word corpus need count frequency particular word lot word know putting word one group see would like get count specific word moment see count cloud storm term category resulting data frame easy way get count term mydict individual column without writing code individual term
How to store the output of case folding into data frame or list in Python?,"<p>So I built up the function for Case Folding and Stemming, but the output is not what I wanted. This is the case of Text pre-Processing in Bahasa Indonesia, but you don't need to understand the language, my question is the format of the output. The code is shown below :</p>
<pre><code>import re 
import string

#import StemmerFactory class
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

#create stemmer
factory = StemmerFactory()
stemmer = factory.create_stemmer()

def case_fold(data, title = None):
        text = &quot; &quot;.join(words for words in data)
        text = text.lower() #lowercase
        text = re.sub(r&quot;\d+&quot;, &quot;&quot;, text) #remove numbers
        text = text.translate(str.maketrans(&quot;&quot;,&quot;&quot;,string.punctuation)) #remove punctuation
        text = text.strip() #remove whitepace
        text = stemmer.stem(text) #stemming
        return text
</code></pre>
<p>I input the data like this</p>
<pre><code>import pandas as pd

cars = {'Brand': ['Ini &amp;Adalah [contoh]56 kalimat 57?','dengan} tanda. baca?!','seharusnya sih bagus tapi jelek !!!','Audi A4'],
        'Price': [22000,25000,27000,35000]
        }
df = pd.DataFrame(cars, columns = ['Brand', 'Price'])

print(case_fold(df['Brand']))
</code></pre>
<p>I got the output like this :</p>
<pre><code>ini adalah contoh kalimat dengan tanda baca harus sih bagus tapi jelek audi a
</code></pre>
<p>While the output that I need is data frame like this :</p>
<pre><code>                        Brand  
0   ini adalah contoh kalimat 
1           dengan tanda baca  
2  harus sih bagus tapi jelek  
3                      audi a  
</code></pre>
<p>or list like this :</p>
<pre><code>[ini adalah contoh kalimat',
 'dengan tanda baca',
 'harus sih bagus tapi jelek',
 'audi a']
</code></pre>
<p>Thank you so much for helping me, I work on this for hours, Because I not often build function by myself.</p>
",Dataset Preprocessing & Handling,store output case folding data frame list python built function case folding stemming output wanted case text pre processing bahasa indonesia need understand language question format output code shown input data like got output like output need data frame like list like thank much helping work hour often build function
unable to retrieve any candidates with kb.get_candidates,"<p>I created a csv file like this:</p>
<pre><code>&quot;CAMERA&quot;, &quot;Camera&quot;, &quot;kamera&quot;, &quot;cam&quot;, &quot;Kamera&quot;
&quot;PICTURE&quot;, &quot;Picture&quot;, &quot;bild&quot;, &quot;photograph&quot;
</code></pre>
<p>and used it somewhat like this:</p>
<pre><code>nlp = de_core_news_sm.load()
text = &quot;Cam is not good&quot;
doc = nlp(text)

name_dict, desc_dict = load_entities()

kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=96)

for qid, desc in desc_dict.items():
    desc_doc = nlp(desc)
    desc_enc = desc_doc.vector
    kb.add_entity(entity=qid, entity_vector=desc_enc, freq=342)  # 342 is an arbitrary value here

for qid, name in name_dict.items():
        kb.add_alias(alias=name, entities=[qid], probabilities=[1])  # 100% prior probability P(entity|alias)
</code></pre>
<p>Printing values like this:</p>
<pre><code>print(f&quot;Entities in the KB: {kb.get_entity_strings()}&quot;)
print(f&quot;Aliases in the KB: {kb.get_alias_strings()}&quot;)
</code></pre>
<p>gives me:</p>
<pre><code>Entities in the KB: ['PICTURE', 'CAMERA']
Aliases in the KB: [' &quot;Camera&quot;', ' &quot;Picture&quot;']
</code></pre>
<p>However, if I try to check for candidates, I only get an empty list:</p>
<pre><code>candidates = kb.get_candidates(&quot;Camera&quot;)
print(candidates)
for c in candidates:
    print(&quot; &quot;, c.entity_, c.prior_prob, c.entity_vector)
</code></pre>
",Dataset Preprocessing & Handling,unable retrieve candidate kb get candidate created csv file like used somewhat like printing value like give however try check candidate get empty list
packaging public data with R,"<p>I'm a college student who is interested in participating a hackathon for packaging public data with R.</p>
<p>This is my first time with R, packaging and public data. So I have few concerns that I'd like to be advised.
In order for me to participate the activity, I have to prepare a packaging idea with a simple business logic referring to how the package can do good to other businesses. My friend and I are currently working on developing a plan for a package that organizes csv files concerning bicycle roads and relevant gadget stations the government is providing access to individuals.</p>
<ol>
<li><p>How many functions should we devise for the plan? As I am new to packaging, other than few (about 4) simple functions like returning the number and the location of outdated gadgets if received the name of the region, I can't really imagine what to do with it.</p>
</li>
<li><p>Has anybody made a package for natural language processing with R using public data? I would like if this hackathon experience relate to my study of nlp but considering the nature of the language R, I'm thinking maybe this isn't the right project to plan for this time.</p>
</li>
</ol>
<p>Thank you in advance for your comment :)</p>
",Dataset Preprocessing & Handling,packaging public data r college student interested participating hackathon packaging public data r first time r packaging public data concern like advised order activity prepare packaging idea simple business logic referring package good business friend currently working developing plan package organizes csv file concerning bicycle road relevant gadget station government providing access individual many function devise plan new packaging simple function like returning number location outdated gadget received name region really imagine ha anybody made package natural language processing r using public data would like hackathon experience relate study nlp considering nature language r thinking maybe right project plan time thank advance comment
How do you save textnets (python) to gml / gexf or access dataframe of graph?,"<p>I have been using textnets (python) to analyse a corpus. I need to export the resulting graph for further analysis / layout editing in Gephi. Having read the docs I am still confused on how to either save the resulting igraph Graph in the appropriate format or to access the pandas dataframe which could then be exported. For example using the tutorial from docs, if using:</p>
<pre><code>from textnets import Corpus, Textnet
from textnets import examples
corpus = Corpus(examples.moon_landing)
tn = Textnet(corpus.tokenized(), min_docs=1)

print(tn)
</code></pre>
<p>I had thought I could either return a pandas data frame by calling 'tn' though this returns a 'Textnet' object.</p>
<p>I had also thought I could return an igraph.Graph object and then subsequently use Graph.write_gml() using something like  <code>tn.project(node_type='doc').write_gml('test.gml') </code> to save the file in an appropriate format but this returns a ProjectedTextnet.</p>
<p>Any advise would be most welcome.</p>
",Dataset Preprocessing & Handling,save textnets python gml gexf access dataframe graph using textnets python analyse corpus need export resulting graph analysis layout editing gephi read doc still confused either save resulting igraph graph appropriate format access panda dataframe could exported example using tutorial doc using thought could either return panda data frame calling tn though return textnet object also thought could return igraph graph object subsequently use graph write gml using something like save file appropriate format return projectedtextnet advise would welcome
Testing on Existing NLP Model,"<p>I'm creating an <code>NLP</code> model in which I use <code>tokenizing</code></p>
<pre><code>num_words = 5000
tokenizer = Tokenizer(num_words=num_words)
tokenizer.fit_on_texts(data)
</code></pre>
<p>Then I convert the texts to <code>sequences</code>, calculate <code>max_tokens</code> to determine the input dimension, and <code>pad</code> them:</p>
<pre><code>X_train_tokens = tokenizer.texts_to_sequences(X_train)
X_test_tokens = tokenizer.texts_to_sequences(X_test)

num_tokens = [len(tokens) for tokens in X_train_tokens + X_test_tokens]
num_tokens = np.array(num_tokens)

max_tokens = np.mean(num_tokens) + (2 * np.std(num_tokens))
max_tokens = int(max_tokens)

X_train_pad = pad_sequences(X_train_tokens, maxlen=max_tokens)
X_test_pad = pad_sequences(X_test_tokens, maxlen=max_tokens)
</code></pre>
<p>Then I build a Keras model and save it.</p>
<p>Then, I load the model I trained with above information. However, this time there is no tokenizer to prepare my text input, and I don't know what the input_dim is, since these are two different Python classes.</p>
<p>How can I use the information which I used in training, in test? What is the correct methodology for NLP testing? How can I use the same fitted tokenizer and calculated num_words on a separate Python file?</p>
",Dataset Preprocessing & Handling,testing existing nlp model creating model use convert text calculate determine input dimension build kera model save load model trained information however time tokenizer prepare text input know input dim since two different python class use information used training test correct methodology nlp testing use fitted tokenizer calculated num word separate python file
unable to load NER pipeline with nlp.from_disk(),"<p>I am trying to load a pre-trained pipeline into my code like this:</p>
<pre><code>nlp = de_core_news_sm.load()
nlp = nlp.from_disk('./TRAINED/Background/')
</code></pre>
<p>but I get a versos error saying:</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-4-1f41fefa6daa&gt; in &lt;module&gt;
      1 nlp = de_core_news_sm.load()
----&gt; 2 nlp = nlp.from_disk('./TRAINED/Background/')
      3 print(nlp)

/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py in from_disk(self, path, exclude, disable)
    972             # Convert to list here in case exclude is (default) tuple
    973             exclude = list(exclude) + [&quot;vocab&quot;]
--&gt; 974         util.from_disk(path, deserializers, exclude)
    975         self._path = path
    976         return self

/opt/anaconda3/lib/python3.8/site-packages/spacy/util.py in from_disk(path, readers, exclude)
    688         # Split to support file names like meta.json
    689         if key.split(&quot;.&quot;)[0] not in exclude:
--&gt; 690             reader(path / key)
    691     return path
    692 

/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py in deserialize_vocab(path)
    948         def deserialize_vocab(path):
    949             if path.exists():
--&gt; 950                 self.vocab.from_disk(path)
    951             _fix_pretrained_vectors_name(self)
    952 

vocab.pyx in spacy.vocab.Vocab.from_disk()

strings.pyx in spacy.strings.StringStore.from_disk()

/opt/anaconda3/lib/python3.8/site-packages/srsly/_json_api.py in read_json(location)
     48         data = sys.stdin.read()
     49         return ujson.loads(data)
---&gt; 50     file_path = force_path(location)
     51     with file_path.open(&quot;r&quot;, encoding=&quot;utf8&quot;) as f:
     52         return ujson.load(f)

/opt/anaconda3/lib/python3.8/site-packages/srsly/util.py in force_path(location, require_exists)
     19         location = Path(location)
     20     if require_exists and not location.exists():
---&gt; 21         raise ValueError(&quot;Can't read file: {}&quot;.format(location))
     22     return location
     23 

ValueError: Can't read file: TRAINED/Background/vocab/strings.json
</code></pre>
<p>If I open the Vocab folder on my macOS, there's no string.json file. Just a few exec files. What can I do to properly read the model?</p>
",Dataset Preprocessing & Handling,unable load ner pipeline nlp disk trying load pre trained pipeline code like get verso error saying open vocab folder macos string json file exec file properly read model
How do you combine multiple documents into a single document with topicmodels in r?,"<p>I am currently trying to combine multiple documents of a corpus into a single document using the topicmodels package. I initially imported my data through multiple csvs, each with multiple lines of text. When I import each csv, however, each line of the csv is treated as a document, and each csv is treated as a corpus. What I would like to do is merge each of the documents/lines for each csv into a single document, and then each of the csvs would represent one document in my corpus. I'm not sure if this possible--perhaps it would be easier to somehow read in all of the lines of the csv as a single text file when initially importing and then create the docs and corpus, but I don't know how to do that either. Below is the code that I have used to import my csvs:</p>
<pre><code>file &lt;- read.csv(&quot;file.csv&quot;)
fileCorp &lt;- VCorpus(VectorSource(file$text))
</code></pre>
<p>The rows in the csv look something like this (where each / represents a line break): 'I walked' / 'the dog' / 'at the' / 'park last night'</p>
<p>I would like to combine each of those lines into a single line of text that will serve as a single document in my corpus.</p>
<p>Thanks for the help!</p>
",Dataset Preprocessing & Handling,combine multiple document single document topicmodels r currently trying combine multiple document corpus single document using topicmodels package initially imported data multiple csvs multiple line text import csv however line csv treated document csv treated corpus would like merge document line csv single document csvs would represent one document corpus sure possible perhaps would easier somehow read line csv single text file initially importing create doc corpus know either code used import csvs row csv look something like represents line break walked dog park last night would like combine line single line text serve single document corpus thanks help
How to convert document term matrix to tf-idf matrix in python,"<p><a href=""https://i.sstatic.net/BxnwE.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BxnwE.jpg"" alt=""enter image description here"" /></a></p>
<p>Given this dataframe, how can I convert it to a tf-idf matrix?</p>
",Dataset Preprocessing & Handling,convert document term matrix tf idf matrix python given dataframe convert tf idf matrix
Load Amazon Sagemaker NTM model locally for inference,"<p>I have trained a Sagemaker <a href=""https://aws.amazon.com/blogs/machine-learning/introduction-to-the-amazon-sagemaker-neural-topic-model/"" rel=""nofollow noreferrer"">NTM</a> model which is a neural topic model, directly on the AWS sagemaker platform. Once training is complete you are able to download the <code>mxnet</code> model files. Once unpacked the files contain:</p>
<ul>
<li>params</li>
<li>symbol.json</li>
<li>meta.json</li>
</ul>
<p>I have followed the docs on mxnet to load the model and have the following code:</p>
<pre class=""lang-py prettyprint-override""><code>sym, arg_params, aux_params = mx.model.load_checkpoint('model_algo-1', 0)
module_model = mx.mod.Module(symbol=sym, label_names=None, context=mx.cpu())

module_model.bind(
    for_training=False,
    data_shapes=[('data', (1, VOCAB_SIZE))]
)

module_model.set_params(arg_params=arg_params, aux_params=aux_params, allow_missing=True) # must set allow missing true here or receive an error for a missing n_epoch var
</code></pre>
<p>I now try and use the model for inference using:</p>
<pre class=""lang-py prettyprint-override""><code>module_model.predict(x) # where x is a numpy array of size (1, VOCAB_SIZE)
</code></pre>
<p>The code runs, but the result is just a single value, where I expect a distribution over topics:</p>
<pre><code>[11.060672]
&lt;NDArray 1 @cpu(0)&gt;
</code></pre>
<p>EDIT:</p>
<p>I have tried to load it using the Symbol API, but still no luck:</p>
<pre class=""lang-py prettyprint-override""><code>import warnings
with warnings.catch_warnings():
    warnings.simplefilter('ignore')
    deserialized_net = gluon.nn.SymbolBlock.imports('model_algo-1-symbol.json', ['data'], 'model_algo-1-0000.params', ctx=mx.cpu())
</code></pre>
<p>Error:</p>
<pre><code>AssertionError: Parameter 'n_epoch' is missing in file: model_algo-1-0000.params, which contains parameters: 'logsigma_bias', 'enc_0_bias', 'projection_bias', ..., 'enc_1_weight', 'enc_0_weight', 'mean_bias', 'logsigma_weight'. Please make sure source and target networks have the same prefix.
</code></pre>
<p>Any help would be great!</p>
",Dataset Preprocessing & Handling,load amazon sagemaker ntm model locally inference trained sagemaker ntm model neural topic model directly aws sagemaker platform training complete able download model file unpacked file contain params symbol json meta json followed doc mxnet load model following code try use model inference using code run result single value expect distribution topic edit tried load using symbol api still luck error help would great
pandas: Split and convert series of alphanumeric texts to columns and rows,"<p><strong>Current data frame:</strong> I have a pandas data frame where each employee has a text code(all codes start with T) and an associated frequency right next to the code. All text codes have 8 characters.</p>
<pre><code>+----------+-------------------------------------------------------------+
|  emp_id  |   text                                                      |
+----------+-------------------------------------------------------------+
|   E0001  | [T0431516,-8,T0401531,-12,T0517519,12]                      |
|   E0002  | [T0701540,-1,T0431516,-2]                                   |
|   E0003  | [T0517519,-1,T0421531,-7,T0516319,9,T0500371,-6,T0309711,-3]|
|   E0004  | [T0516319,-3]                                               |
|   E0005  | [T0431516,2]                                                |
+----------+-------------------------------------------------------------+
</code></pre>
<p><strong>Expected data frame:</strong> I am trying to make the text codes present in the data frame as individual columns and if an employee has a frequency for that code then populate frequency else 0.</p>
<pre><code>+----------+----------------------------------------------------------------------------------------+
|  emp_id  | T0431516 | T0401531 | T0517519 | T0701540 | T0421531 |  T0516319 | T0500371 | T0309711 |                                      
+----------+----------------------------------------------------------------------------------------+
|   E0001  | -8       | -12      | 12       | 0        | 0        | 0         | 0        | 0        |
|   E0002  | -2       | 0        | 0        | -1       | 0        | 0         | 0        | 0        |
|   E0003  | 0        | 0        | -1       | 0        | -7       | 9         | -6       | -3       |
|   E0004  | 0        | 0        | 0        | 0        | 0        | -3        | 0        | 0        |
|   E0005  | 2        | 0        | 0        | 0        | 0        | 0         | 0        | 0        |
+----------+----------------------------------------------------------------------------------------+
</code></pre>
<p><strong>Sample data</strong>:</p>
<pre><code>pd.DataFrame({'emp_id' : {0: 'E0001', 1: 'E0002', 2: 'E0003', 3: 'E0004', 4: 'E0005'},
                'text' :  {0: '[T0431516,-8,T0401531,-12,T0517519,12]', 1: '[T0701540,-1,T0431516,-2]', 2: '[T0517519,-1,T0421531,-7,T0516319,9,T0500371,-6,T0309711,-3]', 3: '[T0516319,-3]', 4: '[T0431516,2]'}
                })
</code></pre>
<p>So, far my attempts were unsuccessful. Any pointers/help is much appreciated!</p>
",Dataset Preprocessing & Handling,panda split convert series alphanumeric text column row current data frame panda data frame employee ha text code code start associated frequency right next code text code character expected data frame trying make text code present data frame individual column employee ha frequency code populate frequency else sample data far attempt unsuccessful pointer help much appreciated
how to get stylometric feature from text( for task of author attribution),"<p>I try to get text features for stylometry task in identifying the author of the given text.<br />
I check for :</p>
<ul>
<li>text length by word</li>
<li>text length by character</li>
<li>punctuation count</li>
<li>unique word count<br />
etc...
but in classifying this feature give unreasonable results, so:<br />
i want to check more features...
I have two questions:</li>
</ul>
<ol>
<li>Is there any good feature that I forget to extract from text to help classifying result</li>
<li>I have a data frame like this:<br />
text ,                  author ,    pos<br />
i go to school   ,        x  ,       [N,V,...]<br />
..<br />
we are good   ,     y      ,    [N,V,ADj]</li>
</ol>
<p>my question is that how can I get good feature from pos column? for example the ratio of &quot;N&quot; to other? or some thing like this...<br />
how to improve this multi_input single_lable multi_class classification result? with which feature? is there any good source?</p>
",Dataset Preprocessing & Handling,get stylometric feature text task author attribution try get text feature stylometry task identifying author given text check text length word text length character punctuation count unique word count etc classifying feature give unreasonable result want check feature two question good feature forget extract text help classifying result data frame like text author po go school x n v good n v adj question get good feature po column example ratio n thing like improve multi input single lable multi class classification result feature good source
how can I read ann file provided by brat annotation toll and convert them to dataframe in python?,"<p>I am working on the sequence tagging classification based IOB scheme,</p>
<p>firstly, I  want to kind of read my corpus and their labels, but the corpus has been saved in kind of format  called .ann file  that I have never worked as you here. it annotated using
<a href=""https://brat.nlplab.org/"" rel=""nofollow noreferrer"">https://brat.nlplab.org/</a>
when I oped it i see this</p>
<pre><code>T1  Claim 78 140    competition can effectively promote the development of economy
A1  Stance T1 Against
T2  MajorClaim 503 550  we should attach more importance to cooperation
T3  Premise 142 283 In order to survive in the competition, companies continue to improve their products and service, and as a result, the whole society prospers
T4  Claim 591 714   through cooperation, children can learn about interpersonal skills which are significant in the future life of all students
A2  Stance T4 For
T5  Premise 716 851 What we acquired from team work is not only how to achieve the same goal with others but more importantly, how to get along with others
T6  Premise 853 1086    During the process of cooperation, children can learn about how to listen to opinions of others, how to communicate with others, how to think comprehensively, and even how to compromise with other team members when conflicts occurred
T7  Premise 1088 1191   All of these skills help them to get on well with other people and will benefit them for the whole life
T8  Claim 1332 1376 competition makes the society more effective
A3  Stance T8 Against
T9  Premise 1212 1301   the significance of competition is that how to become more excellence to gain the victory
T10 Premise 1387 1492   when we consider about the question that how to win the game, we always find that we need the cooperation
T11 Premise 1549 1846   Take Olympic games which is a form of competition for instance, it is hard to imagine how an athlete could win the game without the training of his or her coach, and the help of other professional staffs such as the people who take care of his diet, and those who are in charge of the medical care
T12 Premise 1848 1915   The winner is the athlete but the success belongs to the whole team
T13 Claim 1927 1992 without the cooperation, there would be no victory of competition
A4  Stance T13 For
T14 Claim 2154 2231 a more cooperative attitudes towards life is more profitable in one's success
A5  Stance T14 For
R1  supports Arg1:T3 Arg2:T1    
R2  attacks Arg1:T1 Arg2:T2 
R3  supports Arg1:T5 Arg2:T4    
R4  supports Arg1:T6 Arg2:T4    
R5  supports Arg1:T7 Arg2:T4    
R6  supports Arg1:T9 Arg2:T8    
R7  supports Arg1:T11 Arg2:T12  
R8  supports Arg1:T12 Arg2:T13  
R9  supports Arg1:T10 Arg2:T13  
R10 supports Arg1:T4 Arg2:T2    
R11 attacks Arg1:T8 Arg2:T2 
R12 supports Arg1:T13 Arg2:T2   
R13 supports Arg1:T14 Arg2:T2   

</code></pre>
<p>I want to easily decode that, and saved my data as dataframe  in this format:</p>
<p>sentence with their labels  ( claim or Premise or MAJORCLAIM  , as you see in the text)</p>
<p>something similar this format</p>
<p><a href=""https://i.sstatic.net/EhhNh.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EhhNh.jpg"" alt=""enter image description here"" /></a></p>
<p>sentences with their labels</p>
<p>I have tried to read  .txt file using this function</p>
<pre><code>myList = []                #read the whole text from 
for root, dirs, files in os.walk(path):
    for file in files:
        if file.endswith('.txt'):
            with open(os.path.join(root, file), 'r', encoding=&quot;utf-8&quot;) as f:
                text = f.read()
                myList.append(text)
</code></pre>
<pre><code>df = pd.DataFrame(np.array(myList),index=list(range(1,len(myList)+1)),columns=[&quot;Paragraph&quot;])

</code></pre>
<p>but for this ann file provided by brat, I have no idea</p>
",Dataset Preprocessing & Handling,read ann file provided brat annotation toll convert dataframe python working sequence tagging classification based iob scheme firstly want kind read corpus label corpus ha saved kind format called ann file never worked annotated using oped see want easily decode saved data dataframe format sentence label claim premise majorclaim see text something similar format sentence label tried read txt file using function ann file provided brat idea
Standard way to make a CSV file for classification with n-grams,"<p>I want to make a CSV file from n-grams of the tweets with 0/1 labels for binary classification. What is the best and standard structure of the CSV file to do that?</p>
<p>Do you agree with the below example structure?</p>
<pre><code>pos_tweet = 'I love you'
neg_tweet = 'I hate you'
</code></pre>
<p>CSV file:</p>
<pre><code>bigram, label
I love, 1
love you, 1
I hate, 0
hate you, 0
</code></pre>
",Dataset Preprocessing & Handling,standard way make csv file classification n gram want make csv file n gram tweet label binary classification best standard structure csv file agree example structure csv file
how to merge two consecutive records in a Data Frame in Python Based on Condition,"<p>I have a data frame like below:</p>
<p><a href=""https://i.sstatic.net/uUHMt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uUHMt.png"" alt=""enter image description here"" /></a></p>
<p>i want to merge all consecutive clauses in clauses calumn which has False value in the column['clauses_last_word_chk'], then delete the row after merging,and keep the row which have True in the column['clauses_last_word_chk']</p>
<p>For example :
the first three rows have False values to be merged, then delete second and third rows.
the 4th, 5th,6th will remain the same without merging.
the 7th and 8th rows to be merged, then delete the 8th row as below:</p>
<p><a href=""https://i.sstatic.net/YV0lg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YV0lg.png"" alt=""enter image description here"" /></a></p>
<p>below is my code:</p>
<pre><code>for i in range (0,len(general_df)):
    if (general_df['clauses_last_word_chk'][i]==False) and i&lt;len(general_df):
        
        general_df['clauses_merged'][i]=str(general_df['clauses'][i])+','+str(general_df['clauses'][i+1])
        general_df.drop( general_df['clauses_merged'][i+1], inplace=True)
    else:
        general_df['clauses_merged'][i]=general_df['clauses'][i]
</code></pre>
",Dataset Preprocessing & Handling,merge two consecutive record data frame python based condition data frame like want merge consecutive clause clause calumn ha false value column clause last word chk delete row merging keep row true column clause last word chk example first three row false value merged delete second third row th th th remain without merging th th row merged delete th row code
Python: Find method to match similar but not identical names from two data frames,"<p>I have two data frames, one contains service requests opened by customers and the second contains orders by customers.
I want to merge these two data frames on customer name, but the problem is that the names are not identical because they came from two different sources.
Just a fictive example: data frame 1 can have &quot;Facebook, Inc.&quot; and data frame 2 can be &quot;facebook social media&quot;.</p>
<p>I tried using SequenceMatcher (from difflib import SequenceMatcher) and take the result with the highest ratio per customer in data frame 1 but the accuracy was not good enough.</p>
<p>I would be happy to hear ideas from people that encountered the same issue.</p>
<p>Thanks.</p>
",Dataset Preprocessing & Handling,python find method match similar identical name two data frame two data frame one contains service request opened customer second contains order customer want merge two data frame customer name problem name identical came two different source fictive example data frame facebook inc data frame facebook social medium tried using sequencematcher difflib import sequencematcher take result highest ratio per customer data frame accuracy wa good enough would happy hear idea people encountered issue thanks
Document Similarity runtime exceeds using Spacy,"<p>I have written a function in Python to compute the similarity between PDF pages to return the most similar page mapping.</p>
<p>Function gets input the file and a list which has dictionary entries as:
thumbnail=[{'page': 1, 'text' : 'strin1'}, {'page': 2, 'text' : 'strin2'},...]</p>
<p>The function:</p>
<pre><code>import PyPDF2
import spacy

filename2=&quot;file.pdf&quot;
nlp = spacy.load('en_core_web_lg')


def checker(filename2, thumbnail):
    object = PyPDF2.PdfFileReader(filename2)
    NumPages = object.getNumPages()
    
    specialCharacters = {ord(c): &quot; &quot; for c in &quot;!@#$%^&amp;*()[]{};:,./&lt;&gt;?\|`~-=_+&quot;}
    
    # extract text and do the search
    output=[]
    for i in range(0, NumPages):
        temp_dict={}
        Text = object.getPage(i).extractText().translate(specialCharacters)
        Text=Text.replace('\n','')
        
        for item in thumbnail:
            sim= nlp(Text).similarity(nlp(item['text']))
            if sim&gt;0.98:
                temp_dict['page_thumbnail'] = item['page']
                temp_dict['page_file']=i+1
                temp_dict['sim'] = sim
                output.append(temp_dict)
    return output
</code></pre>
<p>This is taking a really long time for a PDF with 38 pages matched with a list of 38 entries using Spacy.
Any suggestion on how to make it scalable? Also the primary goal is to return the page number of the document (i) and the matched page for which the similarity score is the highest in the thumbnail (item['page']).</p>
",Dataset Preprocessing & Handling,document similarity runtime exceeds using spacy written function python compute similarity pdf page return similar page mapping function get input file list ha dictionary entry thumbnail page text strin page text strin function taking really long time pdf page matched list entry using spacy suggestion make scalable also primary goal return page number document matched page similarity score highest thumbnail item page
how to clean text for nlp containg &#39;[]&#39;,"<p>I have this dataframe 'data_clean' for nlp where on 'louis' you can see the transcripts has [rock music playing][audience cheering]. How can i remove the paranthesis '[]' to clean the text?</p>
<p><a href=""https://i.sstatic.net/idbYl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/idbYl.png"" alt=""ss"" /></a></p>
",Dataset Preprocessing & Handling,clean text nlp containg dataframe data clean nlp louis see transcript ha rock music playing audience cheering remove paranthesis clean text
Reading data from txt file and storing in dataframe,"<p>I have a txt file with has data like this:</p>
<pre><code>[{&quot;text&quot; : &quot;Random Text&quot; , &quot;location&quot; : &quot;place&quot; , &quot;date&quot; : &quot;01 Jan,2012&quot;}, 
 {&quot;text&quot; : &quot;Similar texts&quot; , &quot;location&quot; : &quot;xyz&quot; , &quot;date&quot;: &quot;02 Jan, 2020&quot; },
...}]
</code></pre>
<p>How do I read this txt file in python and store the data in a dataframe?
Please help.<br />
I tried using various delimiters but the results are pretty messed up.</p>
",Dataset Preprocessing & Handling,reading data txt file storing dataframe txt file ha data like read txt file python store data dataframe please help tried using various delimiters result pretty messed
How can I read english.pickle file from nltk module?,"<p>I am trying to figure out why I cant read the contents of the english.pickle file downloaded from nltk module.</p>
<p>I first downloaded the nltk file using this code:</p>
<pre><code>import nltk
nltk.download('punkt')
</code></pre>
<p>I then looked for inside the punkt file that I have on my home directory and found english.pickle file. I used the following code to read the file in python:</p>
<pre><code>import pickle
with open('english.pickle', 'rb') as file:
    x = pickle.load(file)
</code></pre>
<p>It all seemed fine, however, when I am running the variable x (which should be storing the pickled data) i am unable to retrieve the data from as I would from any other pickled file.</p>
<p>Instead I am only getting the object name and the id:</p>
<pre><code>&lt;nltk.tokenize.punkt.PunktParameters at 0x7f86cf6c0cd0&gt;
</code></pre>
<p>The problem is I need to access the content of the file and I cant iterate through as it is not iterable.</p>
<p>Has anyone encountered the same problem?</p>
",Dataset Preprocessing & Handling,read english pickle file nltk module trying figure cant read content english pickle file downloaded nltk module first downloaded nltk file using code looked inside punkt file home directory found english pickle file used following code read file python seemed fine however running variable x storing pickled data unable retrieve data would pickled file instead getting object name id problem need access content file cant iterate iterable ha anyone encountered problem
Pythonic way of reducing the subclasses,"<p>background: so, I am working on an NLP problem. where I need to extract different types of features based on different types of text documents. and I currently have a setup where there is a FeatureExtractor base class, which is subclassed multiple times depending on the different types of docs and all of them calculate a different set of features and return a pandas data frame as output.</p>
<p>all these subclasses are further called by one wrapper type class called FeatureExtractionRunner which calls all the subclasses and calculates the features on all docs and returns the output for all types of docs.</p>
<p>Problem: this pattern of calculating features leads to lots of subclasses. currently, I have like 14 subclasses, since I have 14 types of docs.it might expand further. and this is too many classes to maintain. Is there an alternative way of doing this? with less subclassing</p>
<p>here is some sample representative code of what i explained:</p>
<pre><code>from abc import ABCMeta, abstractmethod

class FeatureExtractor(metaclass=ABCMeta):
    #base feature extractor class
    def __init__(self, document):
        self.document = document
        
        
    @abstractmethod
    def doc_to_features(self):
        return NotImplemented
    
    
class ExtractorTypeA(FeatureExtractor):
    #do some feature calculations.....
    
    def _calculate_shape_features(self):
        return None
    
    def _calculate_size_features(self):
        return None
    
    def doc_to_features(self):
        #calls all the fancy feature calculation methods like 
        f1 = self._calculate_shape_features(self.document)
        f2 = self._calculate_size_features(self.document)
        #do some calculations on the document and return a pandas dataframe by merging them  (merge f1, f2....etc)
        data = &quot;dataframe-1&quot;
        return data
    
    
class ExtractorTypeB(FeatureExtractor):
    #do some feature calculations.....
    
    def _calculate_some_fancy_features(self):
        return None
    
    def _calculate_some_more_fancy_features(self):
        return None
    
    def doc_to_features(self):
        #calls all the fancy feature calculation methods
        f1 = self._calculate_some_fancy_features(self.document)
        f2 = self._calculate_some_more_fancy_features(self.document)
        #do some calculations on the document and return a pandas dataframe (merge f1, f2 etc)
        data = &quot;dataframe-2&quot;
        return data
    
class ExtractorTypeC(FeatureExtractor):
    #do some feature calculations.....
    
    def doc_to_features(self):
        #do some calculations on the document and return a pandas dataframe
        data = &quot;dataframe-3&quot;
        return data

class FeatureExtractionRunner:
    #a class to call all types of feature extractors 
    def __init__(self, document, *args, **kwargs):
        self.document = document
        self.type_a = ExtractorTypeA(self.document)
        self.type_b = ExtractorTypeB(self.document)
        self.type_c = ExtractorTypeC(self.document)
        #more of these extractors would be there
        
    def call_all_type_of_extractors(self):
        type_a_features = self.type_a.doc_to_features()
        type_b_features = self.type_b.doc_to_features()
        type_c_features = self.type_c.doc_to_features()
        #more such extractors would be there....
        
        return [type_a_features, type_b_features, type_c_features]
        
        
all_type_of_features = FeatureExtractionRunner(&quot;some document&quot;).call_all_type_of_extractors()
</code></pre>
",Dataset Preprocessing & Handling,pythonic way reducing subclass background working nlp problem need extract different type feature based different type text document currently setup featureextractor base class subclassed multiple time depending different type doc calculate different set feature return panda data frame output subclass called one wrapper type class called featureextractionrunner call subclass calculates feature doc return output type doc problem pattern calculating feature lead lot subclass currently like subclass since type doc might expand many class maintain alternative way le subclassing sample representative code explained
Quanteda convert fcm output to data.frame,"<p>I am trying to use the fantastic Quanteda to look at co-occurance of terms in news articles.</p>
<p>I can find the features which co-occur with &quot;美国&quot; (the United States) as follows:</p>
<pre><code>ch14_corp &lt;- corpus(data_14)
ch14_toks &lt;- tokens(ch14_corp, remove_punct = TRUE) %&gt;%
+ tokens_remove(ch_stop)
ch14_fcm &lt;- fcm(ch14_toks, context = &quot;window&quot;)

</code></pre>
<p>and then get the features that co-occur most frequently</p>
<pre><code>
topfeatures(ch14_fcm[&quot;美国&quot;, ], n=50)

朝鲜     美国     日本     中国     韩国     问题       马     政府     国家     报道 
     881      804      555      552      297      288      270      254      253      243 
      奥     总统       称     战略     表示       韩     关系     政策     认为     进行 
     238      238      234      227      214      174      173      169      162      160 
      中       核     亚太 国家安全     经济     安全       局     世界     发言   国务院 
     157      153      148      137      136      136      136      135      132      129 
      美       国     访问   俄罗斯     军事     国际     官员     媒体     公民     人权 
     126      122      121      120      120      118      118      114      114      114 
    联合     一个       名     地区     安倍     平衡     导弹     国防       斯     克里 
     112      112      112      111      110      110      107      105      104      102
</code></pre>
<p>Could anybody tell me how convert this to a 'data.frame'? Or a table with the 'feature' in column A and then the number of times it co-occurs with '美国' in column B?</p>
<p>I guess the other way might be to not use 'topfeatures' but to get just the row (or column?) of the matrix which has all the terms that co-occur with '美国', then to sort these based on the number of times they co-occur?</p>
",Dataset Preprocessing & Handling,quanteda convert fcm output data frame trying use fantastic quanteda look co occurance term news article find feature co occur united state follows get feature co occur frequently could anybody tell convert data frame table feature column number time co occurs column b guess way might use topfeatures get row column matrix ha term co occur sort based number time co occur
&#39;KeyError:&#39; when iterating over pandas data frame,"<p>I have a Dataframe df having two columns : 'label' and 'review'. As a data cleaning process, I have dropped all the null values. Now I want to remove all the stopwords and punctuations from review column.</p>
<p><a href=""https://i.sstatic.net/ONm7J.jpg"" rel=""nofollow noreferrer"">dataframe</a></p>
<p>I'm getting keyerror when I tried this code.</p>
<pre><code>    stemmer = PorterStemmer()
    for i in range(len(df)):
        review = re.sub('[^a-zA-Z]', ' ',df['review'][i] )
        review = review.lower()
        review = review.split()
        review = [ stemmer.stem(word) for word in review if word not in stopwords.words('english')]
        df['review'][i] = &quot; &quot;.join(review)
    
</code></pre>
<p><a href=""https://i.sstatic.net/cq9h7.jpg"" rel=""nofollow noreferrer"">code</a></p>
<pre><code>     KeyError                                  Traceback (most recent call last)
    &lt;ipython-input-44-91ef309cd900&gt; in &lt;module&gt;
          2 
          3 for i in range(len(df)):
     ----&gt; 4     review = re.sub('[^a-zA-Z]', ' ',df['review'][i] )
          5     review = review.lower()
          6     review = review.split()

    ~\Anaconda3\lib\site-packages\pandas\core\series.py in __getitem__(self, key)
        866         key = com.apply_if_callable(key, self)
        867         try:
    --&gt; 868             result = self.index.get_value(self, key)
        869 
        870             if not is_scalar(result):

    ~\Anaconda3\lib\site-packages\pandas\core\indexes\base.py in get_value(self, series, key)
       4373         try:
       4374             return self._engine.get_value(s, k,
     -&gt; 4375                                           tz=getattr(series.dtype, 'tz', None))
       4376         except KeyError as e1:
       4377             if len(self) &gt; 0 and (self.holds_integer() or self.is_boolean()):

    pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_value()

    pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_value()

    pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

    pandas/_libs/hashtable_class_helper.pxi in 
    pandas._libs.hashtable.Int64HashTable.get_item()

    pandas/_libs/hashtable_class_helper.pxi in 
    pandas._libs.hashtable.Int64HashTable.get_item()

    KeyError: 140
</code></pre>
<p>Please help me out.</p>
",Dataset Preprocessing & Handling,keyerror iterating panda data frame dataframe df two column label review data cleaning process dropped null value want remove stopwords punctuation review column dataframe getting keyerror tried code code please help
Encoding two different languages using pd.read_csv problem,"<p>I am building a Neural Machine Automatic Translator from German to Arabic. I am reading a CSV file containing German sentences and their corresponding Arabic translations. I want to read both languages at the same time using <code>pd.read_csv</code>. I have tried all the codes for all languages in this <a href=""https://docs.python.org/3/library/codecs.html#standard-encodings"" rel=""nofollow noreferrer"">Python documentation</a> but none of them worked.</p>
<p><a href=""https://i.sstatic.net/2JBdP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2JBdP.png"" alt=""Aliases encoding for all languages"" /></a></p>
<p>The only thing that worked best for me is this:</p>
<pre><code>df = pd.read_csv(&quot;DLATS.csv&quot;, encoding ='windows-1256')
</code></pre>
<p>'windows-1256' is the encoding Alias for the Arabic language. But the problem is that it doesn't catch the German special characters like (ä) but it converts them into question marks (?). So the word drängte became dr?ngte.</p>
<p>So, can anyone please help me to solve this problem or how to work around it? I have thought of separating the German and Arabic sentences in separate CSV files so that each CSV file contains one row only, and then maybe I will try to mix them in the Python code. But it seems that <code>pd.read_csv</code> requires at least two columns in the CSV file to work.</p>
<p>Update: I have noticed that the original csv file contains these problems as well for the German language. So, I have finally managed to solve my problem by reading excel directly instead of csv since the original file is in Excel, so I used pd.read_excel without any encoding attribute and it worked well. I didn't know before that pandas has pd.read_excel.</p>
",Dataset Preprocessing & Handling,encoding two different language using pd read csv problem building neural machine automatic translator german arabic reading csv file containing german sentence corresponding arabic translation want read language time using tried code language python documentation none worked thing worked best window encoding alias arabic language problem catch german special character like convert question mark word dr ngte became dr ngte anyone please help solve problem work around thought separating german arabic sentence separate csv file csv file contains one row maybe try mix python code seems requires least two column csv file work update noticed original csv file contains problem well german language finally managed solve problem reading excel directly instead csv since original file excel used pd read excel without encoding attribute worked well know panda ha pd read excel
how to iterate pandas frame from csv and apply text summarisation,"<p>Im working on text summarization extraction on long text data. I have multiple users text data in the input csv file. But current code is appending all the text column data in to sentences and then apply logic. How do I apply the code for each row instead of merging all the column values? Any Help will appreciated.</p>
<p><strong>Input.csv (^ delimited)</strong></p>
<pre><code>uid^name^text
36d73f013aa7^Don Howard^The Irvine Foundation has entered into a partnership with College Futures Foundation that starts a new chapter in our support of postsecondary success in California.To achieve Irvine’s singular goal. 
36d73f013aa8^Simon Haris^That’s why we have long provided funding to expand postsecondary success. Now with our focus on low-wage workers, we have decided to split our postsecondary funding into two parts:. Strengthening and expanding work-ready credentialing programs (which we will do directly, primarily as part of our Better Careers initiative). 
36d73f013aa8^David^Accelerating and expanding the attainment of bachelor’s degrees (which we will fund through our partnership with College Futures). We believe that College Futures is in a stronger position than we are to make grants to support improvements in how the CSUs and the California Community Colleges can better serve students.  
</code></pre>
<p><strong>pseudo code</strong></p>
<pre><code>Loop each record
     apply below logic to text column to get summary
</code></pre>
<p><strong>Code : Text Summarization Code</strong></p>
<pre><code>import numpy as np
import pandas as pd
import nltk
from nltk.tokenize import sent_tokenize
nltk.download('punkt') # one time execution
from nltk.corpus import stopwords
import re
# Read the CSV file
import io


df = pd.read_csv('/home/sshuser/textsummerisation/input.csv',sep='^')
# split the the text in the articles into sentences
sentences = []
for s in df['text']:
  sentences.append(sent_tokenize(s))
  
# flatten the list
sentences = [y for x in sentences for y in x]

# remove punctuations, numbers and special characters
clean_sentences = pd.Series(sentences).str.replace(&quot;[^a-zA-Z]&quot;, &quot; &quot;)

# make alphabets lowercase
clean_sentences = [s.lower() for s in clean_sentences]

nltk.download('stopwords')# one time execution

stop_words = stopwords.words('english')

def remove_stopwords(sen):
  sen_new = &quot; &quot;.join([i for i in sen if i not in stop_words])
  return sen_new
  
# remove stopwords from the sentences
clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]

# Extract word vectors
word_embeddings = {}
fopen = open('/home/sshuser/textsummerisation/glove.6B.100d.txt', encoding='utf-8')
for line in fopen:
    values = line.split()
    word = values[0]
    print(values)
    print(word) 
    coefs = np.asarray(values[1:], dtype='float32')
    word_embeddings[word] = coefs
fopen.close()

sentence_vectors = []
for i in clean_sentences:
  if len(i) != 0:
    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)
  else:
    v = np.zeros((100,))
  sentence_vectors.append(v)

len(sentence_vectors)

# similarity matrix
sim_mat = np.zeros([len(sentences), len(sentences)])

for i in range(len(sentences)):
  for j in range(len(sentences)):
    if i != j:
      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]
      
import networkx as nx

nx_graph = nx.from_numpy_array(sim_mat)
scores = nx.pagerank(nx_graph)

ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)

# Specify number of sentences to form the summary
sn = 10

# Generate summary
for i in range(sn):
  print(ranked_sentences[i][1])  
</code></pre>
<p><strong>Expected output</strong>: Output of the above code should come in summary column for each record</p>
<pre><code>uid^name^text^summary
</code></pre>
",Dataset Preprocessing & Handling,iterate panda frame csv apply text summarisation im working text summarization extraction long text data multiple user text data input csv file current code appending text column data sentence apply logic apply code row instead merging column value help appreciated input csv delimited pseudo code code text summarization code expected output output code come summary column record
Replace dot product for loop Numpy,"<p>I am trying to replace the dot product for loop using something faster like NumPy</p>
<p>I did research on dot product and kind of understand and can get it working with toy data in a few ways in but not 100% when it comes to implementing it for actual use with a data frame.</p>
<p>I looked at these and other SO threads to no luck <a href=""https://stackoverflow.com/questions/27006176/dot-product-of-subarrays-without-for-loop"">avoide loop dot product, matlab</a> and <a href=""https://stackoverflow.com/questions/27006176/dot-product-of-subarrays-without-for-loop"">dot product subarrays without for loop</a> and <a href=""https://stackoverflow.com/questions/24090889/multiple-numpy-dot-products-without-a-loop"">multiple numpy dot products without a loop</a></p>
<p><strong>looking to do something like this which works with toy numbers in np array</strong></p>
<pre><code>u1 =np.array([1,2,3])
u2 =np.array([2,3,4])
v1.dot(v2)
20
</code></pre>
<pre><code>
u1 =np.array([1,2,3])
u2 =np.array([2,3,4])
(u1 * u2).sum()
20

</code></pre>
<pre><code>u1 =np.array([1,2,3])
u2 =np.array([2,3,4])
sum([x1*x2 for x1, x2 in zip (u1, u2)])
20

</code></pre>
<p><strong>this is the current working get dot product</strong></p>
<p>I would like to do this with out the for loop</p>
<pre><code>def get_dot_product(self, courseid1, courseid2, unit_vectors):
    u1 = unit_vectors[courseid1]
    u2 = unit_vectors[courseid2]
    dot_product = 0.0
    for dimension in u1:
        if dimension in u2:
            dot_product += u1[dimension] * u2[dimension]
    return dot_product
</code></pre>
<p>** code**</p>
<pre><code>

    #!/usr/bin/env python
    # coding: utf-8
    
    
    
    class SearchRecommendationSystem:
    
        def __init__(self):
            pass
    
 
    def get_bag_of_words(self, titles_lines):
        bag_of_words = {}
        for index, row in titles_lines.iterrows():
            courseid, course_bag_of_words = self.get_course_bag_of_words(row)
            for word in course_bag_of_words:
                word = str(word).strip()  # added
                if word not in bag_of_words:
                    bag_of_words[word] = course_bag_of_words[word]
                else:
                    bag_of_words[word] += course_bag_of_words[word]
        return bag_of_words

    def get_course_bag_of_words(self, line):
        course_bag_of_words = {}
        courseid = line['courseid']
        title = line['title'].lower()
        description = line['description'].lower()
        wordlist = title.split() + description.split()
        if len(wordlist) &gt;= 10:
            for word in wordlist:
                word = str(word).strip()  # added
                if word not in course_bag_of_words:
                    course_bag_of_words[word] = 1
                else:
                    course_bag_of_words[word] += 1
        return courseid, course_bag_of_words

    def get_sorted_results(self, d):
        kv_list = d.items()
        vk_list = []
        for kv in kv_list:
            k, v = kv
            vk = v, k
            vk_list.append(vk)
        vk_list.sort()
        vk_list.reverse()
        k_list = []
        for vk in vk_list[:10]:
            v, k = vk
            k_list.append(k)
        return k_list

    def get_keywords(self, titles_lines, bag_of_words):
        n = sum(bag_of_words.values())
        keywords = {}
        for index, row in titles_lines.iterrows():
            courseid, course_bag_of_words = self.get_course_bag_of_words(row)
            term_importance = {}
            for word in course_bag_of_words:
                word = str(word).strip()  # extra
                tf_course = (float(course_bag_of_words[word]) / sum(course_bag_of_words.values()))
                tf_overall = float(bag_of_words[word]) / n
                term_importance[word] = tf_course / tf_overall
            keywords[str(courseid)] = self.get_sorted_results(term_importance)
        return keywords

    def get_inverted_index(self, keywords):
        inverted_index = {}
        for courseid in keywords:
            for keyword in keywords[courseid]:
                if keyword not in inverted_index:
                    keyword = str(keyword).strip()  # added
                    inverted_index[keyword] = []
                inverted_index[keyword].append(courseid)
        return inverted_index

    def get_search_results(self, query_terms, keywords, inverted_index):
        search_results = {}
        for term in query_terms:
            term = str(term).strip()
            if term in inverted_index:
                for courseid in inverted_index[term]:
                    if courseid not in search_results:
                        search_results[courseid] = 0.0
                    search_results[courseid] += (
                            1 / float(keywords[courseid].index(term) + 1) *
                            1 / float(query_terms.index(term) + 1)
                    )
        sorted_results = self.get_sorted_results(search_results)
        return sorted_results

    def get_titles(self, titles_lines):
        titles = {}
        for index, row in titles_lines.iterrows():
            titles[row['courseid']] = row['title'][:60]
        return titles
    
        def get_unit_vectors(self, keywords, categories_lines):
            norm = 1.884
            cat = {}
            subcat = {}
            for line in categories_lines[1:]:
                courseid_, category, subcategory = line.split('\t')
                cat[courseid_] = category.strip()
                subcat[courseid_] = subcategory.strip()
            unit_vectors = {}
            for courseid in keywords:
                u = {}
                if courseid in cat:
                    u[cat[courseid]] = 1 / norm
                    u[subcat[courseid]] = 1 / norm
                for keyword in keywords[courseid]:
                    u[keyword] = (1 / float(keywords[courseid].index(keyword) + 1) / norm)
                unit_vectors[courseid] = u
            return unit_vectors
    
        def get_dot_product(self, courseid1, courseid2, unit_vectors):
            u1 = unit_vectors[courseid1]
            u2 = unit_vectors[courseid2]
            dot_product = 0.0
            for dimension in u1:
                if dimension in u2:
                    dot_product += u1[dimension] * u2[dimension]
            return dot_product
    
        def get_recommendation_results(self, seed_courseid, keywords, inverted_index, unit_vectors):
            courseids = []
            seed_courseid = str(seed_courseid).strip()
            for keyword in keywords[seed_courseid]:
                for courseid in inverted_index[keyword]:
                    if courseid not in courseids and courseid != seed_courseid:
                        courseids.append(courseid)
    
            dot_products = {}
            for courseid in courseids:
                dot_products[courseid] = self.get_dot_product(seed_courseid, courseid, unit_vectors)
            sorted_results = self.get_sorted_results(dot_products)
            return sorted_results
    
    
        def Final(self):
            print(&quot;Reading Title file.......&quot;)
            titles_lines = open('s2-titles.txt', encoding=&quot;utf8&quot;).readlines()
            print(&quot;Reading Category file.......&quot;)
            categories_lines = open('s2-categories.tsv', encoding = &quot;utf8&quot;).readlines()
            print(&quot;Getting Supported Functions Data&quot;)
            bag_of_words = self.get_bag_of_words(titles_lines)
            keywords = self.get_keywords(titles_lines, bag_of_words)
            inverted_index = self.get_inverted_index(keywords)
            titles = self.get_titles(titles_lines)
    
            print(&quot;Getting Unit Vectors&quot;)
            unit_vectors = self.get_unit_vectors(keywords=keywords, categories_lines=categories_lines)
    
            #Search Part
            print(&quot;\n ############# Started Search Query System ############# \n&quot;)
            query = input('Input your search query: ')
            while query != '':
                query_terms = query.split()
                search_sorted_results = self.get_search_results(query_terms, keywords, inverted_index)
                print(f&quot;==&gt; search results for query: {query.split()}&quot;)
    
                for search_result in search_sorted_results:
                    print(f&quot;{search_result.strip()} - {str(titles[search_result]).strip()}&quot;)
    
                #ask again for query or quit the while loop if no query is given
                query = input('Input your search query [hit return to finish]: ')
    
    
            print(&quot;\n ############# Started Recommendation Algorithm System ############# \n&quot;)
            # Recommendation ALgorithm Part
            seed_courseid = (input('Input your seed courseid: '))
            while seed_courseid != '':
                seed_courseid = str(seed_courseid).strip()
                recom_sorted_results = self.get_recommendation_results(seed_courseid, keywords, inverted_index, unit_vectors)
                print('==&gt; recommendation results:')
                for rec_result in recom_sorted_results:
                    print(f&quot;{rec_result.strip()} - {str(titles[rec_result]).strip()}&quot;)
                    get_dot_product_ = self.get_dot_product(seed_courseid, str(rec_result).strip(), unit_vectors)
                    print(f&quot;Dot Product Value: {get_dot_product_}&quot;)
                seed_courseid = (input('Input seed courseid [hit return to finish]:'))
    
    
    if __name__ == '__main__':
        obj = SearchRecommendationSystem()
        obj.Final()
</code></pre>
<p><strong>s2-categories.tsv</strong></p>
<pre><code>    courseid    category    subcategory
    21526   Design  3D &amp; Animation
    153082  Marketing   Advertising
    225436  Marketing   Affiliate Marketing
    19482   Office Productivity Apple
    33883   Office Productivity Apple
    59526   IT &amp; Software   Operating Systems
    29219   Personal Development    Career Development
    35057   Personal Development    Career Development
    40751   Personal Development    Career Development
    65210   Personal Development    Career Development
    234414  Personal Development    Career Development
</code></pre>
<p><strong>Example of how s2-titles.txt looks</strong></p>
<pre><code>courseidXXXYYYZZZtitleXXXYYYZZZdescription
3586XXXYYYZZZLearning Tools for Mrs  B's Science Classes This is a series of lessons that will introduce students to the learning tools that will be utilized throughout the schoXXXYYYZZZThis is a series of lessons that will introduce students to the learning tools that will be utilized throughout the school year  The use of these tools serves multiple purposes       1  Allow the teacher to give immediate and meaningful feedback on work that is in progress    2  Allow students to have access to content and materials when outside the classroom    3  Provide a variety of methods for students to experience learning materials    4  Provide a variety of methods for students to demonstrate learning    5  Allow for more time sensitive correction  grading and reflections on concepts that are assessed  
</code></pre>
",Dataset Preprocessing & Handling,replace dot product loop numpy trying replace dot product loop using something faster like numpy research dot product kind understand get working toy data way come implementing actual use data frame looked thread luck href numpy dot product without loop looking something like work toy number np array current working get dot product would like loop code category tsv example title txt look
Check if a word in present in multiple sheets,"<p>I have a data frame in the following format:</p>
<p>Sample dataframe:</p>
<pre><code>row1:['efcc', 'fficial', 'billiontwits', 'since', 'covid', 'landed']
row2:['when', 'people', 'say', 'the', 'fatality', 'rate', 'of', 'coronavirus', 'is']
row3:['in', 'the', 'coronavirus-induced', 'crisis', 'people', 'are',  'cyvbwx']
row4:['in', 'the', 'be-induced', 'crisis', 'people', 'are',  'cyvbwx']
</code></pre>
<p>columns2(sheet_retreived_from) has list of place the word is from:</p>
<pre><code>row1:sheet1
row2:sheet2
row3:sheet3
row4:sheet2
</code></pre>
<p>And a words_collection which has list of words through below code:</p>
<pre><code>words_collection=[]
for w in sample.tokenised_text:
   for t in w:
       words_collection.append(t)
</code></pre>
<p>and sheet names from: <code>sheetlist=list(set(sample.sheet.to_list()))</code></p>
<p>However, struggling to find a proper way to iterate over the dataframe to check if a word is present in more than one sheet?</p>
<p>Basically I'm looking for output which shows:</p>
<p><img src=""https://i.sstatic.net/DhRKE.png"" alt=""Sample_output"" /></p>
",Dataset Preprocessing & Handling,check word present multiple sheet data frame following format sample dataframe column sheet retreived ha list place word word collection ha list word code sheet name however struggling find proper way iterate dataframe check word present one sheet basically looking output show
ImportError &quot;text_process&quot; Python library,"<p>I'm taking a look at some CBOW Python implementations.
The owner of the code used a function called &quot;line_processing&quot; from text_process lib.
When I tried to run, I got that error:</p>
<pre><code>ImportError: cannot import name 'line_processing' from 'text_process'
</code></pre>
<p>So I took a look at the lib implementation. There is no function called &quot;line_processing&quot;.</p>
<p>That guy used this function to read each line from a .txt file, and write them in a variable, creating a &quot;big string&quot;:</p>
<pre><code>    text = 'file.txt'
    print(text)
    text = ''
    count = 0
    for i in open(text_file, 'r', encoding='utf-8'):
        text+=line_processing(i)+'\n'
        count += 1
        if count % 10000 == 0: break
</code></pre>
<p>Is there anyone who knows something about &quot;line_processing&quot; function, or about a function/lib I can use instead?</p>
<p>Thank you!</p>
<p>Ps.:</p>
<pre><code>$ python CBOW.py
Building prefix dict from the default dictionary ...
Dumping model to file cache C:\&quot;path_to&quot;\AppData\Local\Temp\jieba.cache
Loading model cost 0.723 seconds.
Prefix dict has been built successfully.
Traceback (most recent call last):
  File &quot;CBOW.py&quot;, line 1, in &lt;module&gt;
    from text_process import line_processing
ImportError: cannot import name 'line_processing' from 'text_process' (C:\&quot;path_to&quot;\miniconda3\lib\site\...\text_process)
</code></pre>
",Dataset Preprocessing & Handling,importerror text process python library taking look cbow python implementation owner code used function called line processing text process lib tried run got error took look lib implementation function called line processing guy used function read line txt file write variable creating big string anyone know something line processing function function lib use instead thank p
build custom corpus with labels from text documents using nltk,"<p>I am following the tutorial to build custom corpus with ntlk. This is an example of my code:</p>
<pre><code>import pandas as pd
import numpy as np
import re
import nltk
import matplotlib.pyplot as plt

pd.options.display.max_colwidth = 200
%matplotlib inline

corpus = ['t_3_0 v_0_17 v_1_20 v_2_78 u_0_1 u_0_2 u_1_2',
          't_3_1 v_0_144 v_1_17 v_2_20 u_0_1 u_0_2 u_1_2',
          't_3_2 v_0_143 v_1_233 v_2_238 u_0_1 u_0_2 u_1_2',
          't_3_3 v_0_20 v_1_253 v_2_275 u_0_1 u_0_2 u_1_2',
          't_3_4 v_0_144 v_1_209 v_2_90 u_0_1 u_0_2 u_1_2',
          't_3_59 v_0_233 v_1_222 v_2_51 v_3_52 u_0_1 u_0_2 u_0_3',
          't_3_60 v_0_238 v_1_11 v_2_137 v_3_143 u_0_1 u_0_2 u_0_3',
          't_3_61 v_0_238 v_1_111 v_2_214 v_3_94 u_0_1 u_0_2 u_0_3',
          't_3_62 v_0_238 v_1_111 v_2_214 v_3_97 u_0_1 u_0_2 u_0_3',
          't_3_63 v_0_238 v_1_137 v_2_214 v_3_51 u_0_1 u_0_2 u_0_3'
             
]
labels = ['block_1', 'block_1', 'block_1', 'block_1', 'block_1',
          'block_2', 'block_2', 'block_2', 'block_2', 'block_2']

corpus = np.array(corpus)
corpus_df = pd.DataFrame({'Document': corpus, 
                          'Category': labels})
corpus_df = corpus_df[['Document', 'Category']]
corpus_df
</code></pre>
<p>The output looks like this:</p>
<pre><code>    Document                                                Category
0   t_3_0 v_0_17 v_1_20 v_2_78 u_0_1 u_0_2 u_1_2            block_1
1   t_3_1 v_0_144 v_1_17 v_2_20 u_0_1 u_0_2 u_1_2           block_1
2   t_3_2 v_0_143 v_1_233 v_2_238 u_0_1 u_0_2 u_1_2         block_1
3   t_3_3 v_0_20 v_1_253 v_2_275 u_0_1 u_0_2 u_1_2          block_1
4   t_3_4 v_0_144 v_1_209 v_2_90 u_0_1 u_0_2 u_1_2          block_1
5   t_3_59 v_0_233 v_1_222 v_2_51 v_3_52 u_0_1 u_0_2 u_0_3  block_2
6   t_3_60 v_0_238 v_1_11 v_2_137 v_3_143 u_0_1 u_0_2 u_0_3 block_2
7   t_3_61 v_0_238 v_1_111 v_2_214 v_3_94 u_0_1 u_0_2 u_0_3 block_2
8   t_3_62 v_0_238 v_1_111 v_2_214 v_3_97 u_0_1 u_0_2 u_0_3 block_2
9   t_3_63 v_0_238 v_1_137 v_2_214 v_3_51 u_0_1 u_0_2 u_0_3 block_2
</code></pre>
<p>What I would like to do is instead of hardcoding corpus and labels, I would prefer to read each line from txt documents and assign labels automatically using the name of the file. For example:</p>
<pre><code>corpus = ['block_1.txt', 'block_2.txt', 'block_3.txt', 'block_4.txt']

labels = ['block_1', 'block_2', 'block_3', 'block_4']

corpus = np.array(corpus)
corpus_df = pd.DataFrame({'Document': corpus, 
                          'Category': labels})
corpus_df = corpus_df[['Document', 'Category']]
corpus_df
</code></pre>
<p>The example of desirable output would look like this:</p>
<pre><code>    Document                                                Category
0   t_3_0 v_0_17 v_1_20 v_2_78 u_0_1 u_0_2 u_1_2            block_1
1   t_3_1 v_0_144 v_1_17 v_2_20 u_0_1 u_0_2 u_1_2           block_1
2   t_3_2 v_0_143 v_1_233 v_2_238 u_0_1 u_0_2 u_1_2         block_1
...
</code></pre>
<p>I would appreciate any suggestions and help!</p>
<p>Thank you!
Olha</p>
",Dataset Preprocessing & Handling,build custom corpus label text document using nltk following tutorial build custom corpus ntlk example code output look like would like instead hardcoding corpus label would prefer read line txt document assign label automatically using name file example example desirable output would look like would appreciate suggestion help thank olha
How can I extract sentences from paragraphs in a pandas.DataFrame and keep the paragraph key?,"<p>I have a <code>pandas.DataFrame</code> with 1604 paragraphs as follows:</p>
<p><a href=""https://i.sstatic.net/s97KL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/s97KL.jpg"" alt=""enter image description here"" /></a></p>
<p>I want to extract all the sentences (even in a NAIVE way using dots) and provide a new data frame which has in each row one sentence and the previous column values especially the paragraph key (mainly index in the first column in the left)</p>
<p>I have worked on that and could provide the chapter column for each sentence as follows:</p>
<pre><code> # Create lists to fill with values
l_col1 = []
l_col2 = []

# iterate over each row and fill our lists
for ix, row in dfAstroNova.iterrows():
    for value in row['sentences']:
        l_col1.append(value)
        l_col2.append(row['chapter'])

# Create new dataframe from the two lists
df= pd.DataFrame({'sentences': l_col1 ,
                         'chapter': l_col2 })
df=df.rename(columns={&quot;sentences&quot;:&quot;sents&quot;});

</code></pre>
<p>which gives me this data frame(dfAstroNova is the name of the original data frame)</p>
<p>as you see I have the chapter key. My question is how to add paragraph key (which is the number of column text in main data frame to a new data frame)</p>
<p>Then I have one other column which shows that this sentence belong to which paragraph in the original data frame or better one additional column which includes for each sentence the corresponded paragraph?</p>
<p><a href=""https://i.sstatic.net/Fx8q3.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Fx8q3.jpg"" alt=""enter image description here"" /></a></p>
",Dataset Preprocessing & Handling,extract sentence paragraph panda dataframe keep paragraph key paragraph follows want extract sentence even naive way using dot provide new data frame ha row one sentence previous column value especially paragraph key mainly index first column left worked could provide chapter column sentence follows give data frame dfastronova name original data frame see chapter key question add paragraph key number column text main data frame new data frame one column show sentence belong paragraph original data frame better one additional column includes sentence corresponded paragraph
How can I bootstrap text readability statistics using quanteda?,"<p>I'm new to both bootstrapping and the quanteda package for text analysis. I have a large corpus of texts organized by document group type that I'd like to obtain readability scores for. I can easily obtain readability scores for each group with the following function: </p>

<pre><code>textstat_readability(texts(mwe, groups = ""document""), ""Flesch"")
</code></pre>

<p>I then want to bootstrap the results to obtain a 95% confidence interval by wrapping a function:</p>

<pre><code>b_readability &lt;- function(x, i, groups = NULL, measure = ""Flesch"")
textstat_readability(texts(x[i], groups = groups), measure) 
n &lt;- 10

groups &lt;- factor(mwe[[""document""]]$document)  
b &lt;- boot(texts(mwe), b_readability, strata = groups, R = n, groups = groups) 
colnames(b$t) &lt;- names(b$t0)
apply(b$t, 2, quantile, c(.025, .5, .975)) 
</code></pre>

<p>But ""b &lt;-"" fails with the error:
    ""Error in t.star[r, ] &lt;- res[[r]] : incorrect number of subscripts on matrix"" </p>

<p>I've wasted two days trying to debug with no luck. What am I doing wrong? Much appreciated for any advice...</p>

<p>MWE:</p>

<pre><code>mwe&lt;-structure(list(document = structure(c(1L, 1L), 
.Label = c(""a"", ""b"", ""c"", ""d"", ""e""), class = ""factor""),  text = c(""Text 1. Text 1.1"", ""Text 2.""), section = structure(2:1, .Label = c(""aa"", ""bb"", ""cc"", ""dd"", ""ee"", ""ff"", ""hh"", ""ii"", ""jj"", ""kk""), class = ""factor""), year = c(1919L, 1944L), preamble = structure(8:9, .Label = c(""1"", ""2"",""3"", "" 4 "", ""5"", ""6  "",  ""7  "",  ""8  "", ""9  "",  ""10 ""), class = ""factor""), articles = c(43L, 47L), pages = c(5.218, 7.666), wordcount = c(3503L, 4929L), mean_articles = c(45, 45)), row.names = 1:2, class = ""data.frame"")

mwe &lt;- corpus(mwe)

b_readability &lt;- function(x, i, groups = NULL, measure = ""Flesch"")
textstat_readability(texts(x[i], groups = groups), measure) 
n &lt;- 10

groups &lt;- factor(mwe[[""document""]]$document)  
b &lt;- boot(texts(mwe), b_readability, strata = groups, R = n, groups = groups) 
colnames(b$t) &lt;- names(b$t0)
apply(b$t, 2, quantile, c(.025, .5, .975)) 
</code></pre>
",Dataset Preprocessing & Handling,bootstrap text readability statistic using quanteda new bootstrapping quanteda package text analysis large corpus text organized document group type like obtain readability score easily obtain readability score group following function want bootstrap result obtain confidence interval wrapping function b fails error error star r r incorrect number subscript matrix wasted two day trying debug luck wrong much appreciated advice mwe
dealing with multiple text files in Python,"<p>It is my first time dealing with more than one unstructured data file, and I need to know if what am doing is the best approach or there is something better.</p>
<p>I have more than 1000 text file stand for different novels have text up to 139965 or more.
and I have read them and save them in a data frame as shown below:</p>
<pre><code>file_list = glob.glob(&quot;C:/.../TextFiles/*.txt&quot;)
data = pd.DataFrame({'Name':[],'Content':[]})

for file in file_list:
    with open(file, 'r',encoding=&quot;utf8&quot;, errors='ignore') as myfile:
        new_name=os.path.splitext(file)[0]
        data=data.append({'Name':re.sub(&quot;.*\\\\&quot;, &quot; &quot;,new_name), 'Content': myfile.read()},ignore_index=True)
     
</code></pre>
<p>then I have started cleaning the texts by going row by row.</p>
<pre><code>data['Name'] = data['Name'].apply(lambda x: &quot; &quot;.join(x.split()))
</code></pre>
<p>do you think this is the best approach to dealing with multi and large text files by saving them in data frame?</p>
<p>my next step will extract specific information from the text and save them in columns.</p>
<p>any advice?</p>
",Dataset Preprocessing & Handling,dealing multiple text file python first time dealing one unstructured data file need know best approach something better text file stand different novel text read save data frame shown started cleaning text going row row think best approach dealing multi large text file saving data frame next step extract specific information text save column advice
What is the best way to extract the body of an article with Python?,"<p><strong>Summary</strong></p>
<p>I am building a text summarizer in Python. The kind of documents that I am mainly targeting are scholarly papers that are usually in pdf format.</p>
<p><strong>What I Want to Achieve</strong></p>
<p>I want to effectively extract the body of the paper (abstract to conclusion), excluding title of the paper, publisher names, images, equations and references.</p>
<p><strong>Issues</strong></p>
<p>I have tried looking for effective ways to do this, but I was not able to find something tangible and useful. The current code I have tries to split the pdf document by sentences and then filters out the entries that have less than average number of characters per sentence. Below is the code:</p>
<pre><code>from pdfminer import high_level

# input: string (path to the file)
# output: list of sentences
def pdf2sentences(pdf): 
    article_text = high_level.extract_text(pdf)
    sents = article_text.split('.') #splitting on '.', roughly splits on every sentence      
    run_ave = 0
    
    for s in sents:
        run_ave += len(s)
    run_ave /= len(sents)
    sents_strip = []
    
    for sent in sents:
        if len(sent.strip()) &gt;= run_ave:
            sents_strip.append(sent)

    return sents_strip
</code></pre>
<p><em>Note: I am using <a href=""http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf"" rel=""nofollow noreferrer"">this</a> article as input.</em></p>
<p>Above code seems to work fine, but I am still not effectively able to filter out thing like title and publisher names that come before the abstract section and things like the references section that come after the conclusion. Moreover, things like images are causing gibberish characters to show up in the text which is messing up the overall quality of the output. Due to the weird unicode characters I am not able to write the output to a txt file.</p>
<p><strong>Appeal</strong></p>
<p>Are there ways I can improve the performance of this parser and make it more consistent?</p>
<p>Thank you for your answers!</p>
",Dataset Preprocessing & Handling,best way extract body article python summary building text summarizer python kind document mainly targeting scholarly paper usually pdf format want achieve want effectively extract body paper abstract conclusion excluding title paper publisher name image equation reference issue tried looking effective way wa able find something useful current code try split pdf document sentence filter entry le average number character per sentence code note using article input code seems work fine still effectively able filter thing like title publisher name come abstract section thing like reference section come conclusion moreover thing like image causing gibberish character show text messing overall quality output due weird unicode character able write output txt file way improve performance parser make consistent thank answer
How can I find and replace all punctuation with space in node js in large text file using streaming?,"<p>I want to find all the punctuation in any size of text file and replace it with spaces using node js. Since the file can be a large text file so I am using read &amp; write stream onto the file to break it into the chunks and then put it into the function which will find and replace the punctuation on that text file. Please help me to find a way to get this task completed.</p>
",Dataset Preprocessing & Handling,find replace punctuation space node j large text file using streaming want find punctuation size text file replace space using node j since file large text file using read write stream onto file break chunk put function find replace punctuation text file please help find way get task completed
How to calculate tf-idf for a single term after getting the tf-idf matrix?,"<p>In the past, I have received help with building a tf-idf for the one of my document and got an output which I wanted (please see below).</p>
<pre class=""lang-r prettyprint-override""><code>TagSet &lt;- data.frame(emoticon = c(&quot;🤔&quot;,&quot;🍺&quot;,&quot;💪&quot;,&quot;🥓&quot;,&quot;😃&quot;),
                     stringsAsFactors = FALSE)

TextSet &lt;- data.frame(tweet = c(&quot;🤔Sharp, adversarial⚔️~pro choice💪~ban Pit Bulls☠️~BSL🕊️~aberant psychology😈~common sense🤔~the Piper will lead us to reason🎵~sealskin woman🐺&quot;,
                                &quot;Blocked by Owen, Adonis. Abbott &amp; many #FBPE😃 Love seaside, historic houses &amp; gardens, family &amp; pets. RTs &amp; likes/ Follows may=interest not agreement 🇬🇧&quot;,
                                &quot;🇺🇸🇺🇸🇺🇸🇺🇸 #healthy #vegetarian #beatchronicillness fix infrastructure&quot;,
                                &quot;LIBERTY-IDENTITARIAN. My bio, photo at Site Info. And kindly add my site to your Daily Favorites bar. Thank you, Eric&quot;,
                                &quot;💙🖤I #BackTheBlue for my son!🖤💙 Facts Over Feelings. Border Security saves lives! #ThankYouICE&quot;,
                                &quot;🤔🇺🇸🇺🇸 I play Pedal Steel @CooderGraw &amp; #CharlieShafter🇺🇸🇺🇸 #GoStars #LiberalismIsAMentalDisorder&quot;,
                                &quot;#Englishman  #Londoner  @Chelseafc  🕵️‍♂️ 🥓🚁 🍺 🏴󠁧󠁢󠁥󠁮󠁧󠁿🇬🇧🇨🇿&quot;,
                                &quot;F*** the Anti-White Agenda #Christian #Traditional #TradThot #TradGirl #European #MAGA #AltRight #Folk #Family #WhitePride&quot;,
                                &quot;🌸🐦❄️Do not dwell in tbaconhe past, do not dream of the future, concentrate the mind on the present moment.🌸🐿️❄️&quot;,
                                &quot;Ordinary girl in a messed up World | Christian | Anti-War | Anti-Zionist | Pro-Life | Pro 🇸🇪 | 👋🏼Hello intro on the Minds Link |&quot;),
                      stringsAsFactors = FALSE)


library(dplyr)
library(quanteda)

tweets_dfm &lt;- dfm(TextSet$tweet)  # convert to document-feature matrix

tweets_dfm %&gt;% 
  dfm_select(TagSet$emoticon) %&gt;% # only leave emoticons in the dfm
  dfm_tfidf() %&gt;%                 # weight with tfidf
  convert(&quot;data.frame&quot;)           # turn into data.frame to display more easily

#     document       🤔             🍺           💪          🥓           😃
# 1     text1      1.39794            1            0            0            0
# 2     text2      0.00000            0            1            0            0
# 3     text3      0.00000            0            0            0            0
# 4     text4      0.00000            0            0            0            0
# 5     text5      0.00000            0            0            0            0
# 6     text6      0.69897            0            0            0            0
# 7     text7      0.00000            0            0            1            1
# 8     text8      0.00000            0            0            0            0
# 9     text9      0.00000            0            0            0            0
# 10   text10      0.00000            0            0            0            0

</code></pre>
<p>But I need a little help with calculating tf-idf per singular term. Meaning, how do I accurately get the tf-idf value for each term from the matrix?</p>
<pre class=""lang-r prettyprint-override""><code># terms      tfidf
# 🤔      #its tfidf the correct way   
# 🍺      #its tfidf the correct way 
# 💪      #its tfidf the correct way 
# 🥓      #its tfidf the correct way 
# 😃      #its tfidf the correct way 
</code></pre>
<p>I am sure, it's not like add all of tf-idf for a term from its matrix column and divide by documents where it appeared. And that would be the value for that term.</p>
<p>I have looked at a few sources such as here, <a href=""https://stats.stackexchange.com/questions/422750/how-to-calculate-tf-idf-for-a-single-term"">https://stats.stackexchange.com/questions/422750/how-to-calculate-tf-idf-for-a-single-term</a>, but the author is asking something else entirely from what I read.</p>
<p>I am currently weak in text-mining/analysis terminology.</p>
",Dataset Preprocessing & Handling,calculate tf idf single term getting tf idf matrix past received help building tf idf one document got output wanted please see need little help calculating tf idf per singular term meaning accurately get tf idf value term matrix sure like add tf idf term matrix column divide document appeared would value term looked source href author asking something else entirely read p currently weak text mining analysis terminology
How to do both Data Augmentation and Cross Validation at the same time in NLP?,"<p>I have read somewhere that you should not use <strong>data augmentation on your validation set, and you should only use it on your training set.</strong></p>
<p>My problem is this:</p>
<p>I have a dataset which has less number of training samples and I want to use data augmentation.
I split the dataset into training and test set and use data augmentation on the training set. I then use StratifiedKfold on the training set, which returns me a train index and a test index, but if I use the X_train[test index] as my validation set, it has some augmented images and I don't want that.</p>
<p>Is there any way to do data augmentation on the training set and do cross-validation ?</p>
<p>Here is my code(I haven't done data augmentation but would love to get a way to separate out the test_index from the augmented training samples.):</p>
<pre><code>kfold = StratifiedKFold(n_splits=5,shuffle=True)
i=1
for train_index , test_index in kfold.split(X_train,y_train):

  dataset_train = tf.data.Dataset.from_tensor_slices((X_train[train_index],                                                        
  y_train.iloc[train_index])).shuffle(len(X_train[train_index]))
  dataset_train = dataset_train.batch(512,drop_remainder=True).repeat()
  dataset_test = tf.data.Dataset.from_tensor_slices((X_train[test_index],                                                     
  y_train.iloc[test_index])).shuffle(len(X_train[test_index]))
  dataset_test = dataset_test.batch(32,drop_remainder=True).take(steps_per_epoch).repeat()

  model_1 = deep_neural()

  print('-------------------------------------------------------------------------------------------- 
  -------------------------------')
  print('\n')
  print(f'Training for fold {i} ...')
  print('Training on {} samples.........Validating on {} samples'.format(len(X_train[train_index]),
                                                                       len(X_train[test_index])))
  checkpoint = tf.keras.callbacks.ModelCheckpoint(get_model_name(i), 
                           monitor='val_loss', verbose=1, 
                           save_best_only=True, mode='min')

  history = model_1.fit(dataset_train,steps_per_epoch = len(X_train[train_index])//BATCH_SIZE,
                      epochs=4,validation_data=dataset_test,
                      validation_steps=1,callbacks=[csv_logger,checkpoint]) 

  scores = model_1.evaluate(X_test,y_test,verbose=0)
  pred_classes = model_1.predict(X_test).argmax(1)
  f1score = f1_score(y_test,pred_classes,average='macro') 

  print('\n')
  print(f'Score for fold {i}: {model_1.metrics_names[0]} of {scores[0]}; {model_1.metrics_names[1]} 
  of {scores[1]*100}; F1 Score of {f1score}%')
  print('\n')            
  acc_per_fold.append(scores[1] * 100)
  loss_per_fold.append(scores[0]) 
  f1score_per_fold.append(f1score)

  tf.keras.backend.clear_session()
  gc.collect()     
  del model_1
  i=i+1
</code></pre>
",Dataset Preprocessing & Handling,data augmentation cross validation time nlp read somewhere use data augmentation validation set use training set problem dataset ha le number training sample want use data augmentation split dataset training test set use data augmentation training set use stratifiedkfold training set return train index test index use x train test index validation set ha augmented image want way data augmentation training set cross validation code done data augmentation would love get way separate test index augmented training sample
Csv file tokenization using Pandas Python,"<p>I can't find any example coding about how to do tokenization with csv file using Pandas Python. Below is my code with the cleaned review. I am going to use this &quot;cleaned_review&quot; to perform tokenization.</p>
<p><a href=""https://i.sstatic.net/c9mLv.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",Dataset Preprocessing & Handling,csv file tokenization using panda python find example coding tokenization csv file using panda python code cleaned review going use cleaned review perform tokenization enter image description
Use http.client to login to the online Farasa lemmatizer and lemmatize arabic text file content,"<p>I am trying to use the code for Farasa lemmatizer into my code to lemmatize text files or csv files  that contain Arabic language. here is the link to the code <a href=""http://alt.qcri.org/farasa/"" rel=""nofollow noreferrer"">http://alt.qcri.org/farasa/</a></p>

<p>I tried to just pass a text for the <strong>payload</strong> variable and it works.
My question  can I pass file  to the code on the website of farasa to start the process of lemmatization. I am trying to understand what is (payload) but I couldnot</p>

<p>Here is what I attempted</p>

<pre><code>import http.client
from IPython.core import payload

conn = http.client.HTTPSConnection(""farasa-api.qcri.org"") 
payload = ""{\""text\"": \""يجب أن يرحلوا و يعودوا إلى الوطن هذا مثال بسيط\""}"".encode(""utf-8"")

#the below line is not working 
#payload = ""{\""file\"":\""F:/AIenv/textAnalysis/testin2.txt\""}"".encode(""utf-8"")

headers = { ""content-type"": ""application/json"", ""cache-control"": ""no-cache"", }

conn.request(""POST"", ""/msa/webapi/lemma"", payload, headers)

# conn.request(""POST"", ""/msa/webapi/lemma"", files = files, headers=headers)
res = conn.getresponse()

data = res.read()

print(data.decode(""utf-8""))
</code></pre>

<p>does anyone have any idea about this problem and how can be fixed ?</p>
",Dataset Preprocessing & Handling,use login online farasa lemmatizer lemmatize arabic text file content trying use code farasa lemmatizer code lemmatize text file csv file contain arabic language link code tried pas text payload variable work question pas file code website farasa start process lemmatization trying understand payload couldnot attempted doe anyone idea problem fixed
Python extracting contents from list,"<p>I am putting together a text analysis script in Python using pyLDAvis, and I am trying to clean up one of the outputs into something cleaner and easier to read. The function to return the top 5 important words for 4 topics is a list that looks like:</p>
<pre><code>    [(0, '0.008*&quot;de&quot; + 0.007*&quot;sas&quot; + 0.004*&quot;la&quot; + 0.003*&quot;et&quot; + 0.003*&quot;see&quot;'),

     (1,
      '0.009*&quot;sas&quot; + 0.004*&quot;de&quot; + 0.003*&quot;les&quot; + 0.003*&quot;recovery&quot; + 0.003*&quot;data&quot;'),

     (2,
      '0.007*&quot;sas&quot; + 0.006*&quot;data&quot; + 0.005*&quot;de&quot; + 0.004*&quot;recovery&quot; + 0.004*&quot;raid&quot;'),

     (3,
      '0.019*&quot;sas&quot; + 0.009*&quot;expensive&quot; + 0.008*&quot;disgustingly&quot; + 0.008*&quot;cool.&quot; + 0.008*&quot;houses&quot;')]
</code></pre>
<p>I ideally want to turn this into a dataframe where the first row contains the first words of each topic, as well as the corresponding score, and the columns represent the word and its score i.e.:</p>
<p>r1col1 is 'de', r1col2 is 0.008, r1col3 is 'sas', r1col4 is 0.009, etc, etc.</p>
<p>Is there a way to extract the contents of the list and separate the values given the format it is in?</p>
",Dataset Preprocessing & Handling,python extracting content list putting together text analysis script python using pyldavis trying clean one output something cleaner easier read function return top important word topic list look like ideally want turn dataframe first row contains first word topic well corresponding score column represent word score e r col de r col r col sa r col etc etc way extract content list separate value given format
Clean corpus using Quanteda,"<p>What's the <strong>Quanteda</strong> way of cleaning a corpus like shown in the example below using <strong>tm</strong> (lowercase, remove punct., remove numbers, stem words)? To be clear, I <strong>don't</strong> want to create a document-feature matrix with <code>dfm()</code>, I just want a clean corpus that I can use for a specific downstream task.</p>
<pre><code># This is what I want to do in quanteda
library(&quot;tm&quot;)
data(&quot;crude&quot;)
crude &lt;- tm_map(crude, content_transformer(tolower))
crude &lt;- tm_map(crude, removePunctuation)
crude &lt;- tm_map(crude, removeNumbers)
crude &lt;- tm_map(crude, stemDocument)
</code></pre>
<p>PS I am aware that I could just do <code>quanteda_corpus &lt;- quanteda::corpus(crude)</code>to get what I want, but I would much prefer being able to do everything in Quanteda.</p>
",Dataset Preprocessing & Handling,clean corpus using quanteda quanteda way cleaning corpus like shown example using tm lowercase remove punct remove number stem word clear want create document feature matrix want clean corpus use specific downstream task p aware could get want would much prefer able everything quanteda
How to extract specific parts of messy PDFs in R?,"<p>I need to extract specific parts of a large corpus of PDF documents. The PDFs are large and messy reports containing all kinds of digital, alphabetic and other information. The files are of different length but have unified content and sections across them. The documents have a Table of Content with the section names in them. For example</p>
<pre><code>Table of Content:

Item 1. Business                                                                            1
Item 1A. Risk Factors                                                                       2
Item 1B. Unresolved Staff Comments                                                          5
Item 2. Properties                                                                          10
Item N........

..........text I do not care about...........

Item 1A. Risk Factors 

.....text I am interested in getting.......

(section ends)

Item 1B. Unresolved Staff Comments

..........text I do not care about...........
</code></pre>
<p>I have no problem reading them in and analyzing them as a whole but I need to pull out only the text between <strong>&quot;Item 1A. Risk Factors&quot;</strong> and <strong>&quot;Item 1B. Unresolved Staff Comments&quot;</strong>.
I used <em>pdftools, tm, quanteda and readtext package</em>
This is the part of code I use to read-in my docs. I created a directory where I placed my PDFs and called it &quot;PDF&quot; and another directory where R will place converted to &quot;.txt&quot; files.</p>
<pre><code>pdf_directory &lt;- paste0(getwd(), &quot;/PDF&quot;)
txt_directory &lt;- paste0(getwd(), &quot;/Texts&quot;)
</code></pre>
<p>Then I create a list of files using &quot;list.files&quot; function.</p>
<pre><code>files &lt;- list.files(pdf_directory, pattern = &quot;.pdf&quot;, recursive = FALSE, 
                    full.names = TRUE)
files
</code></pre>
<p>After that, I go on to create a function that extracts file names.</p>
<pre><code>extract &lt;- function(filename) {
  print(filename)
  try({
    text &lt;- pdf_text(filename)
  })
  f &lt;- gsub(&quot;(.*)/([^/]*).pdf&quot;, &quot;\\2&quot;, filename)
  write(text, file.path(txt_directory, paste0(f, &quot;.txt&quot;)))
}
</code></pre>
<pre><code>
for (file in files) {
  extract(file)
}
</code></pre>
<p>After this step, I get stuck and do not know how to proceed. I am not sure if I should try to extract the section of interest when I read data in, therefore, I suppose, I would have to wrestle with the chunk where I create the function --  <code>f &lt;- gsub(&quot;(.*)/([^/]*).pdf&quot;, &quot;\\2&quot;, filename)</code>? I apologize for such questions but I am self-teaching myself.
I also tried engaging the following code on just one file instead of a corpus:</p>
<pre><code>start &lt;- grep(&quot;^\\*\\*\\* ITEM 1A. RISK FACTORS&quot;, text_df$text) + 1

stop &lt;- grep(&quot;^ITEM 1B. UNRESOLVED STAFF COMMENTS&quot;, text_df$text) - 1

lines &lt;- raw[start:stop]

scd &lt;- paste0(&quot;.*&quot;,start,&quot;(.*)&quot;,&quot;\n&quot;,stop,&quot;.*&quot;)  
gsub(scd,&quot;\\1&quot;, name_of_file)
</code></pre>
<p>but it did not help me in any way.</p>
",Dataset Preprocessing & Handling,extract specific part messy pdfs r need extract specific part large corpus pdf document pdfs large messy report containing kind digital alphabetic information file different length unified content section across document table content section name example problem reading analyzing whole need pull text item risk factor item b unresolved staff comment used pdftools tm quanteda readtext package part code use read doc created directory placed pdfs called pdf another directory r place converted txt file create list file using list file function go create function extract file name step get stuck know proceed sure try extract section interest read data therefore suppose would wrestle chunk create function apologize question self teaching also tried engaging following code one file instead corpus help way
NLTK norpus unable to read text file,"<p>I have a sample python script like this</p>
<pre><code>import nltk

from nltk.collocations import *
bigram_measures = nltk.collocations.BigramAssocMeasures()
finder = BigramCollocationFinder.from_words(
   nltk.corpus.genesis.words('/Users/anonymous/Desktop/text.txt')
)
finder.apply_freq_filter(3)
finder.nbest(bigram_measures.pmi, 5)
</code></pre>
<p>I'm not able to run it because I run into the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;text2tag.py&quot;, line 10, in &lt;module&gt;
    nltk.corpus.genesis.words('/Users/anonymous/Desktop/text.txt')
  File &quot;/Users/anonymous/.virtualenvs/playground/lib/python3.6/site-packages/nltk/collocations.py&quot;, line 178, in from_words
    for window in ngrams(words, window_size, pad_right=True):
  File &quot;/Users/anonymous/.virtualenvs/playground/lib/python3.6/site-packages/nltk/util.py&quot;, line 525, in ngrams
    next_item = next(sequence)
  File &quot;/Users/anonymous/.virtualenvs/playground/lib/python3.6/site-packages/nltk/corpus/reader/util.py&quot;, line 296, in iterate_from
    tokens = self.read_block(self._stream)
  File &quot;/Users/anonymous/.virtualenvs/playground/lib/python3.6/site-packages/nltk/corpus/reader/plaintext.py&quot;, line 134, in _read_word_block
    words.extend(self._word_tokenizer.tokenize(stream.readline()))
  File &quot;/Users/anonymous/.virtualenvs/playground/lib/python3.6/site-packages/nltk/tokenize/regexp.py&quot;, line 133, in tokenize
    return self._regexp.findall(text)
TypeError: cannot use a string pattern on a bytes-like object
</code></pre>
",Dataset Preprocessing & Handling,nltk norpus unable read text file sample python script like able run run following error
uploading arabic files in django not working returning codes,"<p>i have a view in my django project that should be able to read the content of an uploaded .txt file from the input type=&quot;file&quot;, but the thing is that with arabic content it doesn't print the actual text, but a series of codes &quot;\xd9\x88\xd9\x82\xd8\xa7\xd9\x84&quot; and i couldn't find any solution for this since the file is perfectly viewable on my pc and my website it the one exporting that file in &quot;utf-8&quot;. any help here ?</p>
<pre><code>Uploaded_File = request.FILES[&quot;Doc&quot;]
for chunk in Uploaded_File.chunks(chunk_size=None):
    print(chunk)
</code></pre>
",Dataset Preprocessing & Handling,uploading arabic file django working returning code view django project able read content uploaded txt file input type file thing arabic content print actual text series code xd x xd x xd xa xd x find solution since file perfectly viewable pc website one exporting file utf help
Pandas csv not reading Arabic characters,"<p>I have a csv file with mixed columns of numbers and Arabic text. I am trying to open it using pandas csv reader. I tried using the following.</p>
<pre><code>Text= pd.read_csv('text.csv', encoding ='utf-8-sig')
</code></pre>
<p>However, after reading the file, when I am looking at the columns, I am getting instead of Arabic text a bunch of exclamation marks for each row.
what could be the problem?</p>
<p>thanks</p>
",Dataset Preprocessing & Handling,panda csv reading arabic character csv file mixed column number arabic text trying open using panda csv reader tried using following however reading file looking column getting instead arabic text bunch exclamation mark row could problem thanks
Emotional score of sentences using Spacy,"<p>I have a series of 100.000+ sentences and I want to rank how emotional they are.</p>
<p>I am quite new to the NLP world, but this is how I managed to get started (adaptation from <a href=""https://spacy.io/usage/spacy-101"" rel=""nofollow noreferrer"">spacy 101</a>)</p>
<pre><code>import spacy
from spacy.matcher import Matcher

matcher = Matcher(nlp.vocab)

def set_sentiment(matcher, doc, i, matches):
    doc.sentiment += 0.1

myemotionalwordlist = ['you','superb','great','free']

sentence0 = 'You are a superb great free person'
sentence1 = 'You are a great person'
sentence2 = 'Rocks are made o minerals'

sentences = [sentence0,sentence1,sentence2]

pattern2 = [[{&quot;ORTH&quot;: emotionalword, &quot;OP&quot;: &quot;+&quot;}] for emotionalword in myemotionalwordlist]
matcher.add(&quot;Emotional&quot;, set_sentiment, *pattern2)  # Match one or more emotional word

for sentence in sentences:
    doc = nlp(sentence)
    matches = matcher(doc)

    for match_id, start, end in matches:
        string_id = nlp.vocab.strings[match_id]
        span = doc[start:end]
    print(&quot;Sentiment&quot;, doc.sentiment)
</code></pre>
<p>myemotionalwordlist is a list of about 200 words that Ive built manually.</p>
<p>My questions are:</p>
<p>(1-a) Counting the number of emotional words does not seem like the best approach. Anyone has any suggetions of a better way of doing so?</p>
<p>(1-b) In case this approach is good enough, any suggestions on how I can extract emotional words from wordnet?</p>
<p>(2) Whats the best way of escalating this? I am thinking about adding all sentences to a pandas data frame and then applying the match function to each one of them</p>
<p>Thanks in advance!</p>
",Dataset Preprocessing & Handling,emotional score sentence using spacy series sentence want rank emotional quite new nlp world managed get started adaptation spacy myemotionalwordlist list word ive built manually question counting number emotional word doe seem like best approach anyone ha suggetions better way b case approach good enough suggestion extract emotional word wordnet whats best way escalating thinking adding sentence panda data frame applying match function one thanks advance
How should I load a large NLP file into Lambda,"<p>I have a NLP program which uses Spacy to load in a large word2vec file. This file is 3.75 GB and is too large to put directly in a lambda. Currently I have this program running in an extra large EC2 instance, but this option is expensive. The code also only runs on demand around four times a week, so keeping a server running constantly is not what I want.</p>
<ul>
<li>Is there any way I can convert this to serverless?</li>
<li>If I use EFS, aren't I just converting one server to a server plus a lambda?</li>
<li>Is there an API I can talk to which would return the same information that Spacy can, specifically similarity of two sentences?</li>
</ul>
",Dataset Preprocessing & Handling,load large nlp file lambda nlp program us spacy load large word vec file file gb large put directly lambda currently program running extra large ec instance option expensive code also run demand around four time week keeping server running constantly want way convert serverless use efs converting one server server plus lambda api talk would return information spacy specifically similarity two sentence
Is there any way to give an input file to Stanza (stanford corenlp client) rather then one piece of text while calling server?,"<p>I have a .csv file consists of Imdb sentiment analysis data-set. Each instance is a paragraph. I am using Stanza <a href=""https://stanfordnlp.github.io/stanza/client_usage.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanza/client_usage.html</a> for getting parse tree for each instance.</p>
<pre><code>text = &quot;Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.&quot;

with CoreNLPClient(
    annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],
    timeout=30000,
    memory='16G') as client:
ann = client.annotate(text)
</code></pre>
<p>Right now, I have to re-run server for every instance and it is taking a lot of time since I have 50k instances.</p>
<pre><code>1
Starting server with command: java -Xmx16G -cp /home/wahab/treeattention/stanford-corenlp- 
4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 1200000 -threads 
5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-a74576b3341f4cac.props 
-preload parse
2
Starting server with command: java -Xmx16G -cp /home/wahab/treeattention/stanford-corenlp- 
4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 1200000 -threads 
5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-d09e0e04e2534ae6.props 
-preload parse
</code></pre>
<p>Is there any way to pass a file or do batching?</p>
",Dataset Preprocessing & Handling,way give input file stanza stanford corenlp client rather one piece text calling server csv file consists imdb sentiment analysis data set instance paragraph using stanza getting parse tree instance right run server every instance taking lot time since k instance way pas file batching
Why funcion self.spacy_nlp(phrase) doen&#39;t no work,"<p>Forgive me if I say any improper things.
We keep a project developed in python which makes an analysis of the feeling of the customer comments. The steps are simple:</p>
<ol>
<li>Extract source comments and dump them into a .csv</li>
<li>Clean comments and dump clean comments on another csv</li>
<li>Analyze the comments</li>
<li>Up the feeling to the bbd</li>
</ol>
<p>The problem is at point 3 and we don't understand what's going on. The problem is this,
The clean_data_path variable receives the file with the clean comments:</p>
<pre><code>def run(clean_data_path, predictions_data_path):
   
   
    logger.info(&quot;Data analysis started!&quot;)

    logger.info(&quot;Loading clean data...&quot;)
    df = pd.read_csv(clean_data_path)

    logger.info(&quot;Loading comments to analyze...&quot;)
    review_id, _, comments = load(df)

    logger.info(&quot;Sentiment analysis...&quot;)
    domain_dict = load_domain_dict()
    nlp = NLP(
        domain_dict=domain_dict,
        domain_dict_cap=conf.DOMAIN_DICT_CAP,
        max_review_len=conf.MAX_REVIEW_LEN,
    )
    words =  list(map(nlp.phrase_to_words, comments))

When executing the sentence :
    words = list(map(nlp.phrase_to_words, comments))
</code></pre>
<p>It just fails to continue and we don't know how to capture the error. We tried to put try i except but it doesn't come out.
If we look at the nlp.phrase_to_words we see the following code:</p>
<pre><code>
class NLP():
    &quot;&quot;&quot;
    Class designed to carry out all the actions related with text
    manipulation and basic NLP tasks.
    &quot;&quot;&quot;
    def __init__(self, domain_dict, domain_dict_cap, max_review_len):
        &quot;&quot;&quot;
        TODO
        &quot;&quot;&quot;
        self.domain_dict = domain_dict
        self.domain_dict_cap = domain_dict_cap
        self.max_review_len = max_review_len
        self.spacy_nlp = spacy.load('es_core_news_sm')
   …
   …..
def phrase_to_words(self, phrase):
        &quot;&quot;&quot;
        Given a phrase as a string, it returns a list with the words
        in that phrase. 
        
        Args
        ---- 
            phrases : Phrase to tokenize

        Returns
        -------
            List with the words of the phrase
        &quot;&quot;&quot; 
           print(&quot;before&quot;)
           doc = self.spacy_nlp(phrase)
           print(&quot;after&quot;, doc)
           return list(map(str, doc))
</code></pre>
<p>The problem for that is when you call self.spacy_nlp(phrase). Don't run, simply exit.
But the strangest thing is that if I execute the process putting a complete path of the file clean_data_path it works correctly and is able to execute doc = self.spacy_nlp(phrase) (the same file that it uses when it executes the whole process)</p>
<p>The process worked perfectly until a week ago</p>
<p>Someone could give me some guidance or have found something like this</p>
<p>Greetings,</p>
",Dataset Preprocessing & Handling,funcion self spacy nlp phrase doen work forgive say improper thing keep project developed python make analysis feeling customer comment step simple extract source comment dump csv clean comment dump clean comment another csv analyze comment feeling bbd problem point understand going problem clean data path variable receives file clean comment fails continue know capture error tried put try except come look nlp phrase word see following code problem call self spacy nlp phrase run simply exit strangest thing execute process putting complete path file clean data path work correctly able execute doc self spacy nlp phrase file us executes whole process process worked perfectly week ago someone could give guidance found something like greeting
Am i clustering users correctly by using sklearn&#39;s cosine similarity method and K-means algorithm?,"<p>I have a movie dataset with more than 200 movies and more than 100 users. The users rated the movies. A value of 1 for good, 0 for bad and blank if no choice.</p>
<p>I want to cluster similar users based on their reviews with the idea that users who rated similar movies as good might also rate a movie as good which was not rated by any user in the same cluster. I used cosine similarity measure with k-means clustering. The csv file is shown below:</p>
<pre><code>  UserID         M1     M2       M3  ...............  M200                          
  user1          1      0                               0     
  user2          0      1        1                                      
  user3          0      1                               1                                                                         
    .
    .
    .
    .
 user100         1      0        1                                       
</code></pre>
<p>According to the scheme, if an original review was 1 (good) then we put 1 in the cell and -1 in the cell if the review was 0 (bad). For no reviews, we put 0 in the cell. The csv file below explains the scheme. The rows are users and M in the column is movie and C is the choice.</p>
<pre><code> UserID      M1C1   M2C1  M3C1 .  . ..............M200C1                            
  user1       1     -1     0                        -1    
  user2      -1      1     1                         0   
  user3      -1      1     0                         1                                                                       
    .
    .
    .
 user100      1     -1     1                         0       
</code></pre>
<p>I measured cosine similarity and then clustered the users with sklearn's cosine_similarity and kmeans clustering algorithm. The code is:</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans

df = pd.read_csv('input_file.csv', sep=',', encoding='latin-1',  
                  index_col=False)

df = df.set_index('UserID')


pairwise = pd.DataFrame(
         cosine_similarity(df.values),
         columns = df.index.values,
         index = df.index
)

print (pairwise.round(2))

pairs = pairwise.unstack()

pairs.index.rename(['User A', 'User B'], inplace=True)
pairs = pairs.to_frame('cosine distance').reset_index()

A = pairs[
      (pairs['cosine distance'] &lt; 0.00) 
      &amp; (pairs['User A'] != pairs['User B'])
]

print(A)

kmeans = KMeans(n_clusters=2, init ='k-means++', max_iter=50, 
                n_init=5,random_state=0 )
y_kmeans = kmeans.fit_predict(pairwise)

print(y_kmeans)

frame = pd.DataFrame(pairwise)
frame['cluster'] = y_kmeans

print(frame['cluster'])
print(frame['cluster'].value_counts())
</code></pre>
<p>With this code, i am getting the cosine similarity for all the pairs and i can filter the pairs based on the value of cosine similarity. I am also getting a list of clusters for the users. I want to know that am i doing it right ? Is it right to calculate the cosine similarity first and then pass the values to kmeans ? As by default, the sklearn's kmeans function uses euclidean distance.</p>
<p>I will really appreciate some help.</p>
<p>Thanks..</p>
",Dataset Preprocessing & Handling,clustering user correctly using sklearn cosine similarity method k mean algorithm movie dataset movie user user rated movie value good bad blank choice want cluster similar user based review idea user rated similar movie good might also rate movie good wa rated user cluster used cosine similarity measure k mean clustering csv file shown according scheme original review wa good put cell cell review wa bad review put cell csv file explains scheme row user column movie c choice measured cosine similarity clustered user sklearn cosine similarity kmeans clustering algorithm code code getting cosine similarity pair filter pair based value cosine similarity also getting list cluster user want know right right calculate cosine similarity first pas value kmeans default sklearn kmeans function us euclidean distance really appreciate help thanks
Issue with tokenizing words with NLTK in Python. Returning lists of single letters instead of words,"<p>I'm having some trouble with my NLP python program, I am trying to create a dataset of positive and negative tweets however when I run the code it only returns what appears to be tokenized individual letters. I am new to Python and NLP so I apologise if this is basic or if I'm explaining myself poorly. I have added my code below:</p>
<pre><code>import csv
import random
import re
import string
import mysql.connector
from nltk import FreqDist, classify, NaiveBayesClassifier
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize


def remove_noise(tweet_tokens, stop_words=()):
    cleaned_tokens = []
    for token, tag in pos_tag(tweet_tokens):
        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+#]|[!*\(\),]|' \
                  '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)
        token = re.sub(&quot;(@[A-Za-z0-9_]+)&quot;, &quot;&quot;, token)

        if tag.startswith(&quot;NN&quot;):
            pos = 'n'
        elif tag.startswith('VB'):
            pos = 'v'
        else:
            pos = 'a'

        lemmatizer = WordNetLemmatizer()
        token = lemmatizer.lemmatize(token, pos)

        if len(token) &gt; 0 and token not in string.punctuation and token.lower() not in stop_words:
            cleaned_tokens.append(token.lower())
    print(token)
    return cleaned_tokens


def get_all_words(cleaned_tokens_list):
    for tokens in cleaned_tokens_list:
        for token in tokens:
            yield token


def get_tweets_for_model(cleaned_tokens_list):
    for tweet_tokens in cleaned_tokens_list:
        yield dict([token, True] for token in tweet_tokens)


if __name__ == &quot;__main__&quot;:


with open('positive_tweets.csv') as csv_file:
    positive_tweets = csv.reader(csv_file, delimiter=',')
with open('negative_tweets.csv') as csv_file:
    negative_tweets = csv.reader(csv_file, delimiter=',')

stop_words = stopwords.words('english')

positive_tweet_tokens = word_tokenize(positive_tweets)
negative_tweet_tokens = word_tokenize(negative_tweets)

positive_cleaned_tokens_list = []
negative_cleaned_tokens_list = []

for tokens in positive_tweet_tokens:
    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))

for tokens in negative_tweet_tokens:
    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))

all_pos_words = get_all_words(positive_cleaned_tokens_list)
all_neg_words = get_all_words(negative_cleaned_tokens_list)

freq_dist_pos = FreqDist(all_pos_words)
freq_dist_neg = FreqDist(all_neg_words)
print(freq_dist_pos.most_common(10))
print(freq_dist_neg.most_common(10))

positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)
negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)

positive_dataset = [(tweet_dict, 'positive')
                    for tweet_dict in positive_tokens_for_model]

negative_dataset = [(tweet_dict, 'negative')
                    for tweet_dict in negative_tokens_for_model]

dataset = positive_dataset + negative_dataset

random.shuffle(dataset)

train_data = dataset[:7000]
test_data = dataset[7000:]

classifier = NaiveBayesClassifier.train(train_data)

print(&quot;Accuracy is:&quot;, classify.accuracy(classifier, test_data))
</code></pre>
<p>snippet from CSV file for reference:</p>
<pre><code>    &quot;tweetid&quot;,&quot;username&quot;,&quot;created_at&quot;,&quot;tweet&quot;,&quot;location&quot;,&quot;place&quot;,&quot;classification&quot;
&quot;1285666943073161216&quot;,&quot;MeFixerr&quot;,&quot;2020-07-21 20:04:20+00:00&quot;,&quot;Overwhelmed by all the calls, msgs and tweets. I apologize for getting lost without prior notice. Did not expect to be missed with such fervor. 
I am good &amp;amp; taking a break. Lots of love and dua's for everyone of you in #PTIFamily ❤&quot;,&quot;Pakistan, Quetta&quot;,,&quot;positive&quot;
</code></pre>
",Dataset Preprocessing & Handling,issue tokenizing word nltk python returning list single letter instead word trouble nlp python program trying create dataset positive negative tweet however run code return appears tokenized individual letter new python nlp apologise basic explaining poorly added code snippet csv file reference
How to Do Topic Modelling and Classification on Each Sentence Comment in a Data Frame in R?,"<p>Is there a way to do topic modelling and classification on a data frame of comments in R?</p>
<p>I have 10 columns of comments (where each comment is a open ended sentence of a topic related to a question) and I want to classify each of these comments by topic for each column of comments.</p>
<p>I tried to use LDA (Latent Dirichlet Allocation) using the topicmodels package in R (and use DocumentTermMatrix and Corpus before I applied the LDA model). I tried to find the optimal number of topics using the lowest perplexity.</p>
<p>The issue is that I don't know what topic each sentence of a comment is classified by. It does put words into a similar topic but not by sentence. So it's a little confusing.</p>
<p>I don't know where to go from there and need advice on how to do this.</p>
<p>I was able to apply Sentimental Analysis on the same data frame in R using the sentimentr package and it worked but I can't do the same for topic modelling and classification.</p>
<p>How can I do this in R for each sentence of comment in a column (for a total of 10 columns)?</p>
<p><img src=""https://i.sstatic.net/fsAHJ.png"" alt=""image of data"" /></p>
",Dataset Preprocessing & Handling,topic modelling classification sentence comment data frame r way topic modelling classification data frame comment r column comment comment open ended sentence topic related question want classify comment topic column comment tried use lda latent dirichlet allocation using topicmodels package r use documenttermmatrix corpus applied lda model tried find optimal number topic using lowest perplexity issue know topic sentence comment classified doe put word similar topic sentence little confusing know go need advice wa able apply sentimental analysis data frame r using sentimentr package worked topic modelling classification r sentence comment column total column
Text comparison of phrases in two data-frames and getting the output at matching phrases with sequence and index,"<p>Two datasets df and df1 are in columns in row-wise split, but separated by fullstop '.' as complete sentence.
I want to match the dataset phrases which are present in both and and get the dataset at the matching sentences with the index of superset df.</p>
<p>I can only make if the text is plain, but not in the column-wise. If the spaCy or nlp with language model can help to handle this issue?</p>
<pre><code>df:

index ID-0 ID-1 text
0 4 20 This
1 6 8 is 
2 8 6 an 
3 12 15 apple
4 29 9.
5 45 5 The
6 56 8 apple
7 60 10 is 
8 62 15 sweet
9 65 2 .
10 66 1 This 
11 68 2 is
12 70 6 very
13 73 4 good
14 75 1 fruit
15 76 3 .
16 78 1 I 
17 82 0 like
18 90 6 to 
19 95 8 eat
20 99 2 apple
21 100 0 .

df1

idx text
1 The
2 apple
3 is 
4 sweet
5 .
6 I 
7 like
8 to 
9 eat
10 apple
11 .

output:

index ID-0 ID-1 text    
5 45 5 The
6 56 8 apple
7 60 10 is 
8 62 15 sweet
9 65 2 .
16 78 1 I 
17 82 0 like
18 90 6 to 
19 95 8 eat
20 99 2 apple
21 100 0 .
</code></pre>
",Dataset Preprocessing & Handling,text comparison phrase two data frame getting output matching phrase sequence index two datasets df df column row wise split separated fullstop complete sentence want match dataset phrase present get dataset matching sentence index superset df make text plain column wise spacy nlp language model help handle issue
How can I add a feature using torchtext?,"<p><code>torchtext</code> is able to read a file with some columns, each one corresponding to a field. What if I want to create a new column (which I will use as a feature)? For example, imagine the file has two columns, text and target, and I want to extract some information from the text and generate a new feature (e.g. if it contains certain words), can I do this directly with <code>torchtext</code> or do I need to do it in the file before?</p>

<p>Thanks!</p>
",Dataset Preprocessing & Handling,add feature using torchtext able read file column one corresponding field want create new column use feature example imagine file ha two column text target want extract information text generate new feature e g contains certain word directly need file thanks
Encoding problem while training my own Glove model,"<p>I am training a GloVe model with my own corpus and I have troubles to save it/load it in an <code>utf-8</code> format.</p>

<p>Here what I tried: </p>

<pre><code>from glove import Corpus, Glove

#data
lines = [['woman', 'umbrella', 'silhouetted'], ['person', 'black', 'umbrella']]

#GloVe training
corpus = Corpus() 
corpus.fit(lines, window=4)
glove = Glove(no_components=4, learning_rate=0.1)
glove.fit(corpus.matrix, epochs=10, no_threads=8, verbose=True)
glove.add_dictionary(corpus.dictionary)
glove.save('glove.model.txt')
</code></pre>

<p>The saved file <code>glove.model.txt</code> is unreadable and I can't succeed to save it with a <code>utf-8</code> encoding.</p>

<p>When I try to read it, for exemple by converting it in a Word2Vec format:</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
glove2word2vec(glove_input_file=""glove.model.txt"", 
word2vec_output_file=""gensim_glove_vectors.txt"")    

model = KeyedVectors.load_word2vec_format(""gensim_glove_vectors.txt"", binary=False)
</code></pre>

<p>I have the following error:</p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>Any idea on how I could use my own GloVe model ?  </p>
",Dataset Preprocessing & Handling,encoding problem training glove model training glove model corpus trouble save load format tried saved file unreadable succeed save encoding try read exemple converting word vec format following error idea could use glove model
"Using get_word_forms, how do I generate all possible forms of a list of words?","<p>I am struggling with generating all possible forms of the words in the list below</p>

<p>For instance,</p>

<p>from word_forms.word_forms import get_word_forms</p>

<p>text = ['courses', 'unsurpassable', 'alcohol']
a = [get_word_forms(word) for word in text]
print(a)</p>

<p>[{'n': {'coursings', 'course', 'coursing', 'courses'}, 'a': set(), 'v': {'course', 'coursing', 'courses', 'coursed'}, 'r': {'course'}}, {'n': set(), 'a': {'unsurpassable'}, 'v': set(), 'r': set()}, {'n': {'alcohols', 'alcoholics', 'alcoholic', 'alcohol'}, </p>

<p>at the end, I get the result above...
However, I would like to save it into a csv file and I could not find a way to make this happen.</p>

<p>Pls let me know if there is another solution other than the package, 'get_word_forms' to generate all possible forms of a word.</p>
",Dataset Preprocessing & Handling,using get word form generate possible form list word struggling generating possible form word list instance word form word form import get word form text course unsurpassable alcohol get word form word word text print n coursing course coursing course set v course coursing course coursed r course n set unsurpassable v set r set n alcohol alcoholic alcoholic alcohol end get result however would like save csv file could find way make happen pls let know another solution package get word form generate possible form word
spaCy: Scispacy abbreviation large document,"<p>I found this <a href=""https://stackoverflow.com/questions/52570805/how-to-identify-abbreviations-acronyms-and-expand-them-in-spacy"">post</a> looking for a way to identify and clean abbreviations within my dataframe. The code works well for my use case.</p>
<p>However, I'm dealing with a large data set and was wondering if there was a better or proficient way to apply this without dealing with memory issues.</p>
<p>In order for me to run the code snipet, I sampled 10% of the original dataset and it runs perfectly. If I run the full dataset, my laptop locks.</p>
<p>Below is updated version of the original code:</p>
<pre><code>import spacy
from scispacy.abbreviation import AbbreviationDetector

nlp = spacy.load(&quot;en_core_web_sm&quot;)
nlp.max_length = 43793966

abbreviation_pipe = AbbreviationDetector(nlp)
nlp.add_pipe(abbreviation_pipe)

text = [nlp(text, disable = ['ner', 'parser','tagger']) for text in train.text]

text = ' '.join([str(elem) for elem in text]) 


doc = nlp(text)

#Print the Abbreviation and it's definition
print(&quot;Abbreviation&quot;, &quot;\t&quot;, &quot;Definition&quot;)
for abrv in doc._.abbreviations:
    print(f&quot;{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}&quot;)
</code></pre>
",Dataset Preprocessing & Handling,spacy scispacy abbreviation large document found href looking way identify clean abbreviation within dataframe code work well use case p however dealing large data set wa wondering wa better proficient way apply without dealing memory issue order run code snipet sampled original dataset run perfectly run full dataset laptop lock updated version original code
Extracting groups of unstructured text to for later NLP?,"<p>I am new to data mining / text mining so I am not sure that I am using the right terminology. I am attempting to come up with a process to extract groups of related content to later apply NLP and other techniques to extract the meaningful data out of it. I have starting data that looks something like this:</p>

<pre><code>Product Name - $-25- 15
Product Name - $3

Product Bundle $100
-Product 1
-Product 2 Condition
-Product 3 Condition

Product - Version - Condition $100

Product
Extras
Extras

More Info
$20 

Product
Extras
Condition
$15

Product (Condition) 50
Product (Condition) 25
Product (Condition) 10
</code></pre>

<p>The goal is to obtain a list like this with a unique entry for each ""listing"", grouped with relevant meta data:</p>

<pre><code>[Product Name - $-25- 15], [Product Name - $3], [Product Bundle $100 -Product 1 -Product 2 Condition -Product 3 Condition], [Product - Version - Condition $100] 
</code></pre>

<p>The full text is written by many different authors and often switches formats within a single post so I can't detect what format it is in and process the whole document. The one thing all formats have in common is that they have new line breaks rather than a dense paragraph of text. So working with that I have a few ideas on how to approach it:</p>

<p><strong>Option 1: Rudimentary</strong></p>

<ol>
<li>Split the document into an array by new lines (\n)</li>
<li>If there is an extra empty space between entries then group the previous ones</li>
<li>If there is no extra space, detect if there is a price, if so consider it's own group</li>
</ol>

<p>This option is very simple and could work when double spaced. However it fails by using a number as a heuristic to determine if it's a new group as the product names, extras, conditions could contain numbers when single spaced.</p>

<p><strong>Option 2: NLP</strong></p>

<p>This option would attempt to classify each word in the document as Product Name, Condition, Attribute, Price. Then process the document again to group text so that it has a Name and Price and optionally the condition and extra meta data.</p>

<p>The problem with this approach is that the extras and bundles are products as well so classifying them would determine that they are a unique entry with meta data when they belong under a ""parent"" product because of how they are spaced in the document. </p>

<p><strong>Option 3: Something else?</strong></p>

<p>My first thought was to process the document into groups first so that when NLP knows all words in this group are related to the same product. I have a list of all Product Names and a pretty good one of all Conditions. The extras, versions, and other text is unique so it may cause some issues attempting to determine how to group.</p>

<p>It seems like it might need a mix of the two because how the author spaces them is ultimately how everything is bundled. Yet we don't immediately know if the next set of content is related to the first or a new listing without some other process.</p>

<p><strong>INPUT</strong></p>

<pre><code>Mario Party - $10

Party Games Bundle $100
-Super Mario Bros
-Mario World - NEW

Donkey Kong - 2017 Version - Used $10

Wii Sports
Includes Controllers

Also includes memory card
$10 

Grand Theft Auto
San Andreas
Includes poster
Used
$10

Zelda (Unopened box) 10
</code></pre>

<p><strong>OUTPUT (JSON)</strong></p>

<pre><code>{ listings: [
    { name: 'Mario Party', condition: null, version: null, currency: '$', price: 10, includes: null },
    { name: 'Party Games Bundle', condition: null, version: null, currency: '$', price: 100, includes: ['Super Mario Bros', 'Mario World - NEW'] },
    { name: 'Donkey Kong', condition: 'Used', version: '2017 Version', currency: '$', price: 10, includes: null },
    { name: 'Wii Sports', condition: null, version: null, currency: '$', price: 10, includes: ['Includes Controllers', 'Also includes memory card'] },
    { name: 'Grand Theft Auto', condition: 'Used', version: 'San Andreas', currency: '$', price: 10, includes: 'Includes poster' },
    { name: 'Zelda', condition: 'Unopened box', version: null, currency: '$', price: 10, includes: null }
] }
</code></pre>
",Dataset Preprocessing & Handling,extracting group unstructured text later nlp new data mining text mining sure using right terminology attempting come process extract group related content later apply nlp technique extract meaningful data starting data look something like goal obtain list like unique entry listing grouped relevant meta data full text written many different author often switch format within single post detect format process whole document one thing format common new line break rather dense paragraph text working idea approach option rudimentary split document array new line n extra empty space entry group previous one extra space detect price consider group option simple could work double spaced however fails using number heuristic determine new group product name extra condition could contain number single spaced option nlp option would attempt classify word document product name condition attribute price process document group text ha name price optionally condition extra meta data problem approach extra bundle product well classifying would determine unique entry meta data belong parent product spaced document option something else first thought wa process document group first nlp know word group related product list product name pretty good one condition extra version text unique may cause issue attempting determine group seems like might need mix two author space ultimately everything bundled yet immediately know next set content related first new listing without process input output json
Recommendations table sort dataframe result in descending order Pandas,"<p>The recommendation tables output scores are not truly descending, the recommendations don't match the scores of the recommendationTable.</p>
<p>Currently, the input does work and it does give a correct recommendationTable_df.</p>
<pre><code>recommendationTable_df = recommendationTable_df.sort_values(ascending=False)
</code></pre>
<p><strong>Note:</strong> the output is correct  recommendationTable_df.head(6)</p>
<pre><code>13    1.00
20    1.00
6     0.75
1     0.75
25    0.75
8     0.75
</code></pre>
<p><strong>However, when it goes to display the matching results that display id to name in the scored order It does not.</strong></p>
<pre><code>df.loc[df.index.isin(recommendationTable_df.head(6).keys())] #adjust the value of 6 here
</code></pre>
<p><strong>At this point, the order is no longer descending or  correct</strong></p>
<p>but it is instead ordering by maybe the id that i am using to match to the name</p>
<pre><code>    name    herotype    weapons     spells
1   niem    Sorcerer    light crossbow, battleaxe   Necromancy
6   sax     Bard    light crossbow, battleaxe, Dagger, sling, club  Necromancy
8   wuc     Sorcerer    light crossbow, battleaxe   Necromancy
13  Rolf Rylan  Paladin     light crossbow, battleaxe   Necromancy
20  Braak Presley   Paladin     light crossbow, battleaxe, Dagger, sling, club  Necromancy
25  Jantroph    Paladin     light crossbow, battleaxe   Abjuration
</code></pre>
<p>the order of the score should match the output with the id in the descending order of the score.</p>
<p><strong>This is what I am trying to achieve</strong></p>
<pre><code>userInput = [
            {'name':'Rolf Rylan', 'rating':1}   #Their is no rating system is being used thus by default rating is set to 1
         ] 
</code></pre>
<p><strong>recommendationTable_df  with Exspected Results match up this is not a real dataframe</strong></p>
<pre><code>13    1.00    |    13    Rolf Rylan  Paladin     light crossbow, battleaxe   Necromancy
20    1.00    |    20    Braak Presley   Paladin     light crossbow, battleaxe, Dagger, sling, club  Necromancy
6     0.75    |    6     sax     Bard    light crossbow, battleaxe, Dagger, sling, club  Necromancy
1     0.75    |    1     niem    Sorcerer    light crossbow, battleaxe   Necromancy
25    0.75    |    25    Jantroph    Paladin     light crossbow, battleaxe   Abjuration
8     0.75    |    8     wuc     Sorcerer    light crossbow, battleaxe   Necromancy
</code></pre>
<p><strong>I instead get this as the results which is not matching the decending order</strong></p>
<pre><code>13    1.00    |    1   niem    Sorcerer    light crossbow, battleaxe   Necromancy
20    1.00    |    6   sax     Bard    light crossbow, battleaxe, Dagger, sling, club  Necromancy
6     0.75    |    8   wuc     Sorcerer    light crossbow, battleaxe   Necromancy
1     0.75    |    13  Rolf Rylan  Paladin     light crossbow, battleaxe   Necromancy
25    0.75    |    20  Braak Presley   Paladin     light crossbow, battleaxe, Dagger, sling, club  Necromancy
8     0.75    |    25  Jantroph    Paladin     light crossbow, battleaxe   Abjuration
</code></pre>
<p>How would I get it the recommendation data frame to match the order of the recommendation tables output scores.</p>
<p><strong>This is the recommendation tables output scores</strong>
Which are in the correct order</p>
<pre><code>recommendationTable_df.head(6)
</code></pre>
<p><strong>output</strong></p>
<pre><code>13    1.00
20    1.00
6     0.75
1     0.75
25    0.75
8     0.75
dtype: float64
</code></pre>
<p><strong>This is how it Sorts the Score</strong></p>
<pre><code>#Multiply the genres by the weights and then take the weighted average
recommendationTable_df = ((genreTable*userProfile).sum(axis=1))/(userProfile.sum())

#Sort our recommendations in descending order
recommendationTable_df = recommendationTable_df.sort_values(ascending=False)

df.loc[df.index.isin(recommendationTable_df.head(6).keys())] #adjust the value of 6 here
</code></pre>
<p><strong>This is the current recommendations</strong> This order is not correct</p>
<pre><code>    name    herotype    weapons     spells
1   niem    Sorcerer    light crossbow, battleaxe   Necromancy
6   sax     Bard    light crossbow, battleaxe, Dagger, sling, club  Necromancy
8   wuc     Sorcerer    light crossbow, battleaxe   Necromancy
13  Rolf Rylan  Paladin     light crossbow, battleaxe   Necromancy
20  Braak Presley   Paladin     light crossbow, battleaxe, Dagger, sling, club  Necromancy
25  Jantroph    Paladin     light crossbow, battleaxe   Abjuration
</code></pre>
<p><strong>This is the result I was trying or expecting to get</strong></p>
<pre><code>        name    herotype    weapons     spells
    13  Rolf Rylan  Paladin     light crossbow, battleaxe   Necromancy
    20  Braak Presley   Paladin     light crossbow, battleaxe, Dagger, sling, club  Necromancy
    6   sax     Bard    light crossbow, battleaxe, Dagger, sling, club  Necromancy
    1   niem    Sorcerer    light crossbow, battleaxe   Necromancy
    25  Jantroph    Paladin     light crossbow, battleaxe   Abjuration
    8   wuc     Sorcerer    light crossbow, battleaxe   Necromancy

</code></pre>
<p><strong>based on the output</strong></p>
<pre><code>
   13    1.00
   20    1.00
   6     0.75
   1     0.75
   25    0.75
   8     0.75
   dtype: float64
</code></pre>
",Dataset Preprocessing & Handling,recommendation table sort dataframe result descending order panda recommendation table output score truly descending recommendation match score recommendationtable currently input doe work doe give correct recommendationtable df note output correct recommendationtable df head however go display matching result display id name scored order doe point order longer descending correct instead ordering maybe id using match name order score match output id descending order score trying achieve recommendationtable df exspected result match real dataframe instead get result matching decending order would get recommendation data frame match order recommendation table output score recommendation table output score correct order output sort score current recommendation order correct result wa trying expecting get based output
BERT word embbeding on a simple dataset,"<p>I want to learn how can I use BERt for word embbeding. I tried this code:</p>
<pre><code>from bert_serving.client import BertClient
bc = BertClient()
</code></pre>
<p>But the problem is that it took a lot and I finally couldn't run it even after 10 hours. I've read that it's a GPU intensive task. I use my own laptop with 8GB GPU.
my question is, there is a way that I can use BERT for a small dataset on my own laptop?</p>
",Dataset Preprocessing & Handling,bert word embbeding simple dataset want learn use bert word embbeding tried code problem took lot finally run even hour read gpu intensive task use laptop gb gpu question way use bert small dataset laptop
Convert a multi value dictionary to a Dataframe in python,"<p>The dictionary</p>
<pre><code>{'password': ['Input data fields to enter - 1) Username 2) Password 3) Re-enter Password 4) Security question 5) Security answer\nUsername data field specifications - At least one uppercase, \nUsername data field specifications - At least one lowercase.',

  'Password data field specifications - At least 7 characters, \nPassword data field specifications - 1 number, \nPassword data field specifications - 1 uppercase letter, \nPassword data field specifications - 1 lowercase letter, \nPassword data field specifications - one special charactersss.',
  'Password criteria should be displayed to the user when user clicks inside the password data field.',
  &quot;Green check mark next to 'Password' data field should be displayed that indicates to the user after typing password that the entry meets criteria.&quot;,
  &quot;Red cross mark next to 'Password' data field should be displayed that indicates to the user after typing password that the entry does not meet criteria.&quot;,
  &quot;Validate that inputs in 'Password' and 'Re-enter Password' data fields match- Green check mark next to 'Re-enter' password data field should be displayed that indicates to the user after typing if passwords match\n'Password' and 'Re-enter Password' entries should be masked.&quot;,
  'Password entries will show the keystroke typed, and then after two seconds the entered character is masked.',
  'If \'Password\' entry does not match criteria specified and user hits Submit, show error alert &quot;Password entry does not meet criteria&quot;.',
  &quot;If entries in 'Password' and 'Re-enter Password' do not match and user hits Submit, show error alert 'Password entries do not match'.&quot;]}
</code></pre>
<p>Needed to a data-frame by converting these list of values into a DF with same key</p>
",Dataset Preprocessing & Handling,convert multi value dictionary dataframe python dictionary needed data frame converting list value df key
OOV (Out Of Vocabulary) word embeddings for Fasttex in low RAM environments,"<p>Is there a way to obtain the vectors for OOV (Out Of Vocabulary) words using fasttext but without loading all the embeddings into memory?</p>

<p>I normally work in low RAM environments (&lt;10GB of RAM) so loading a 7GB model into memory is just impossible. To use word embeddings without using that much RAM one can read a <code>.vec</code> (which is normally a plain-text) file line by line and store it into a database (which you later access to request a word vector). However to obtain OOV vectors with fasttext you need to use the <code>.bin</code> files and load then into memory. Is there a way you can avoid loading the whole <code>.bin</code> file?</p>
",Dataset Preprocessing & Handling,oov vocabulary word embeddings fasttex low ram environment way obtain vector oov vocabulary word using fasttext without loading embeddings memory normally work low ram environment gb ram loading gb model memory impossible use word embeddings without using much ram one read normally plain text file line line store database later access request word vector however obtain oov vector fasttext need use file load memory way avoid loading whole file
Using pdfminer on screenplay to output to csv,"<p>I am attempting to use pdfminer on a screenplay of a television show. The ideal output would be a csv file with one column being the character names, and the other column the lines that they spoke. </p>

<p>Here is a link to an dialogue excerpt from the screenplay. 
<a href=""https://i.sstatic.net/iOvuo.png"" rel=""nofollow noreferrer"">Dialogue From Screenplay</a></p>

<p>Because of the unique structure of the script, just reading the text from the pdf is not enough to distinguish which parts are the narrations, and which parts are the dialogue. With the below code, I identified individual text objects and printed out their coordinates. </p>

<pre><code>from pdfminer.layout import LAParams, LTTextBox
from pdfminer.pdfpage import PDFPage
from pdfminer.pdfinterp import PDFResourceManager
from pdfminer.pdfinterp import PDFPageInterpreter
from pdfminer.converter import PDFPageAggregator

fp = open('Ozark-Episode-1-01-Sugarwood.pdf', 'rb')
rsrcmgr = PDFResourceManager()
laparams = LAParams()
device = PDFPageAggregator(rsrcmgr, laparams=laparams)
interpreter = PDFPageInterpreter(rsrcmgr, device)
pages = PDFPage.get_pages(fp)

for index, page in enumerate(pages):
    interpreter.process_page(page)
    layout = device.get_result()
    for lobj in layout:
        if isinstance(lobj, LTTextBox):
            x, y, text = lobj.bbox[0], lobj.bbox[3], lobj.get_text()
            print('%r: %s' % ((round(x, 1), round(y,1)), text)
</code></pre>

<p>However, here is an example of what was returned: </p>

<pre><code>(108.0, 579.9): No, it wasn’t--

(194.0, 591.9): MARTY

(333.0, 591.9): BRUCE
Sorry, all right?

(252.0, 555.9): BRUCE

(180.0, 543.9): What then?  A place like this 
validates us.  We’re making money 
hand over fist.  Tell me we don’t 
need the higher rent on our books.

(108.0, 483.9): Marty squares up to the skyline, Bruce behind him.

</code></pre>

<p>As can be seen, the order of the lines are mixed up right now. I was intending to read the text into csv line by line in a specific order but whenever two characters speak at the same time, the pdf reads the objects differently. Additionally, at (333.0, 591.9) where Bruce speaks, the pdf is not able to read the dialogue as a separate line from the character. I predict there will be many such problems in the screenplay format. </p>

<p>I would greatly appreciate any help whatsoever on how the lines can be read in a chronological order, or how to better be able to distinguish between character and dialogue in an easy and orderly manner. </p>

<p>Thank you in advance!!!</p>
",Dataset Preprocessing & Handling,using pdfminer screenplay output csv attempting use pdfminer screenplay television show ideal output would csv file one column character name column line spoke link dialogue excerpt screenplay dialogue screenplay unique structure script reading text pdf enough distinguish part narration part dialogue code identified individual text object printed coordinate however example wa returned seen order line mixed right wa intending read text csv line line specific order whenever two character speak time pdf read object differently additionally bruce speaks pdf able read dialogue separate line character predict many problem screenplay format would greatly appreciate help whatsoever line read chronological order better able distinguish character dialogue easy orderly manner thank advance
How to convert list of tokens (after sentence tokenization) in a paragraph format into a numbered list of sentences or convert it to a dataframe?,"<p>I read a pdf file using <code>PDFMiner</code> and extracted the text from it for <code>NLP analysis</code>. As I will be dealing with research articles, I did light cleaning of texts by converting the paragraphs of texts into list of sentence tokens. My goal is to select sentences that contains intext citations for my further analysis.</p>

<p>for instance, 
the data is in the below format:</p>

<pre><code>['this is my new project' , 'I am very excited about this  (Abbasi, 2015)'] 
</code></pre>

<p>Expected output:</p>

<pre><code>1.This is my new project
2.I am very excited about this (Abbasi, 2015)
</code></pre>

<p>Is this possible to convert this into a dataframe so that I can add labels to each sentences?</p>

<p>Or will it be wise to extract only the sentences with in-text citations?</p>
",Dataset Preprocessing & Handling,convert list token sentence tokenization paragraph format numbered list sentence convert dataframe read pdf file using extracted text dealing research article light cleaning text converting paragraph text list sentence token goal select sentence contains intext citation analysis instance data format expected output possible convert dataframe add label sentence wise extract sentence text citation
Does using a pipeline with CountVectorizer and TfidfTransform convert the input data into a document-term matrix?,"<p>Does the following code actually converts the input into a dtm since it's not using the fit_transform method but only fit in order to learn the vocabulary? Is this model sufficient for learning ?</p>

<pre><code>X_train, X_test, y_train, y_test = train_test_split(df.data, df.target, test_size = 0.2, random_state = 42) 

pipeline = Pipeline(
    [
        ('vect', CountVectorizer()),
        ('tfidf', TfidfTransformer()),
        ('logreg', LogisticRegression()),
    ]
)
pipeline.fit(X_train, y_train)
y_pred= pipeline.predict(X_test)
</code></pre>
",Dataset Preprocessing & Handling,doe using pipeline countvectorizer tfidftransform convert input data document term matrix doe following code actually convert input dtm since using fit transform method fit order learn vocabulary model sufficient learning
Extracting Emails using NLP - Spacy Matcher and then encrypting and decrypting them,"<p><a href=""https://i.sstatic.net/XCo9T.png"" rel=""nofollow noreferrer"">Image of csv file</a> I have a csv file which looks like the image provided. 
I am reading the csv file, defined a pattern and using spacy Matcher. I am iterating through the rows and columns of the CSV file. My end goal is to identify the email Ids and SSN numbers as sensitive information and encrypt and decrypt them. But unfortunately all the information is getting encrypted and decrypted in the process. </p>

<pre><code>import spacy
from spacy.matcher import Matcher
import csv
from cryptography.fernet import Fernet
from spacy.vocab import Vocab
from spacy.tokens import Span
from spacy import displacy
nlp = spacy.load('en_core_web_sm')
# pattern = [{""TEXT"": {""REGEX"": ""[a-zA-z0-9-_.] +@[a-zA-z0-9-_.]+""}, ""OP"":""?""}]
pattern =[{""TEXT"": {""REGEX"": ""[a-z0-9\.\-+_] +@[a-z0-9\.\-+_]+\.[a-z]+""}, ""OP"":""?""}]
matcher = Matcher(nlp.vocab)
matcher.add(""Email"", None, pattern)
some_list = []
count = 0

file = open(""PIIsampleData.csv"")
csv_file = csv.reader(file)
for row in csv_file:
    # print(row)
    for text in row:
        doc = nlp(text)
        # print(doc.ents)
        matches = matcher(doc)
        print(matches)

        ent = [(ent.text,ent.label_) for ent in doc.ents]
        for match_id, start, end in matches:
            span = doc[start:end]
            print(span)
            # print(span.text)
            # print(span.text)[Here is the image of csv file][1]
        #     entity = [(ent.text, ent.label_) for ent in doc.ents]
        if ent:
            some_dict = {}
            b = bytes(doc.text, 'utf-8')
            name = Fernet.generate_key()
            lock = Fernet(name)
            code = lock.encrypt(b)
            original = lock.decrypt(code)
            # print(code, original, doc.text)
            some_dict = {'code': code, 'original': original, 'label': ent}
            some_list.append(some_dict)
            # print(some_list)
            count = count + 1
</code></pre>

<p>I think i am missing something here, not sure if it is EntityRuler or something or some problem with my code. </p>

<pre><code>for match_id, start, end in matches:
            span = doc[start:end]
            print(span)
This span is coming as blank. Ideally this should have fetched me the emails, right? 
The final output when i am printing some_list is all the columns encrypted and decrypted, where as i want only emails and SSN to be identified as sensitive information and encrypted. I know i haven't defined regex for SSN yet, so just help me with emails for now. 
</code></pre>

<p><a href=""https://i.sstatic.net/rLQo3.png"" rel=""nofollow noreferrer"">csv</a></p>

<pre><code>Employee Name,Employee Email,Phone,Personal number,Organisation number,SSN
Price Cummings,lectus.Nullam@tristique.ca,1-509-928-5746,0,364858-2678,795-63-3325
McKenzie G. Rios,ullamcorper.Duis@sempertellus.ca,1-118-309-0368,16680213 -6611,208206-7964,183-91-0062
Scarlet Estrada,ornare@dolorFuscemi.net,163-5585,16330216 -1611,727359-3280,739-89-4031
Virginia Knowles,lorem@dui.co.uk,874-2186,16691013 -3450,497114-4243,382-62-1298
Reed S. Pennington,nunc@Phasellusataugue.edu,358-0513,16930326 -4221,724596-5152,190-00-3181
Mona Nelson,pellentesque.Sed.dictum@luctus.com,1-681-841-0005,16750725 -6951,028041-2412,943-18-8562
</code></pre>
",Dataset Preprocessing & Handling,extracting email using nlp spacy matcher encrypting decrypting image csv file csv file look like image provided reading csv file defined pattern using spacy matcher iterating row column csv file end goal identify email id ssn number sensitive information encrypt decrypt unfortunately information getting encrypted decrypted process think missing something sure entityruler something problem code csv
How to keep the unicode character codes in my csv file?,"<p>I am handling a large number of incoming emails and many of them have various emoticons in them. I am planning to apply an NLP analysis on the user comments and train a classifier to provide relevant answers, instead of having to manually reply to hundreds of these messages. For this as a first step, I parsed all emails and saved their content in a list called <code>userMessages</code> that I wrote in a csv file. I plan to add further columns to the csv for analytic purposes, such as user name, address, date, and time but this is not relevant for this question now. </p>

<p>Here is the code I use to write the <code>userMessages</code> list into a csv file called <code>user-messages.csv</code>:</p>

<pre><code>with open('user-messages.csv', 'wb') as myfile:
        wr = csv.writer(myfile, dialect='excel', encoding='utf-8', quoting=csv.QUOTE_ALL)
        for _msg in userMessages:
            wr.writerow([_msg])
</code></pre>

<p>This doesn't run into an error due to the <code>encoding='utf-8'</code> parameter, however, it removes/recodes the emoticons in such a way that it is no longer retraceable, for instance in the following format: <code>ðŸ˜</code>. Ideally, I would like to have the original unicode codes in the csv file, such as <code>'\U0001f604'</code> (smiling face with open mouth and smiling eyes) and later substitute these codes with their (approximate) meaning for the NLP to better understand the context of the messages, for instance in the case of this character (<code>'\U0001f604'</code>), remove the code and add the words 'smile' or 'happy'. </p>

<p>Can this be achieved? Or am I overcomplicating things? Any advice would be greatly appreciated. Thank you!</p>

<p><strong>Edit:</strong> I am using Windows and I open the csv files in Microsoft Excel 2016.</p>
",Dataset Preprocessing & Handling,keep unicode character code csv file handling large number incoming email many various emoticon planning apply nlp analysis user comment train classifier provide relevant answer instead manually reply hundred message first step parsed email saved content list called wrote csv file plan add column csv analytic purpose user name address date time relevant question code use write list csv file called run error due parameter however remove recodes emoticon way longer retraceable instance following format ideally would like original unicode code csv file smiling face open mouth smiling eye later substitute code approximate meaning nlp better understand context message instance case character remove code add word smile happy achieved overcomplicating thing advice would greatly appreciated thank edit using window open csv file microsoft excel
Read multiple txt files into Dict into Pandas dataframe,"<p>I am trying to load multiple txt files into dataframe. I know how to load urls, csv, and excel, but I couldnt find any reference on how to load multiple txt files into dataframe and match with dictionary or viceversa.</p>

<p>the text file are not comma or tab separated just plain text containing plain text song lyrics.</p>

<p>I checked the pandas documents any assistance welcome.</p>

<p><a href=""https://pandas.pydata.org/pandas-docs/stable/reference/io.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/stable/reference/io.html</a></p>

<p><strong>Ideally the dataframe</strong></p>

<p>the dataframe I hope to achieve would be like this example</p>

<pre><code>                 |                                                        lyrics
    -------------+-----------------------------------------------------------------------------------------
    bonjovi      |    some text from the text files HiHello! WelcomeThank you Thank you for coming.
    -------------+---------------------------------------------------------------------------------------
    lukebryan    |    some other text from the text files.Hi.Hello WelcomeThank you Thank you for coming. 
    -------------+-----------------------------------------------------------------------------------------
    johnprine    |    yet some text from the text files. Hi.Hello WelcomeThank you Thank you for coming. 

</code></pre>

<p><strong>Basic example</strong>
folder structure /lyrics/</p>

<pre><code>urls = 

    'lyrics/bonjovi.txt',
    'lyrics/lukebryan.txt',
    'lyrics/johnprine.txt',
    'lyrics/brunomars.txt',
    'lyrics/methodman.txt',
    'lyrics/bobmarley.txt',
    'lyrics/nickcannon.txt',
    'lyrics/weeknd.txt',
    'lyrics/dojacat.txt',
    'lyrics/ladygaga.txt',
    'lyrics/dualipa.txt',
    'lyrics/justinbieber.txt',]
</code></pre>

<p><strong>muscian names</strong></p>

<pre><code>bands = ['bonjovi', 'lukebryan', 'johnprine', 'brunomars', 'methodman', 'bobmarley', 'nickcannon', 'weeknd', 'dojacat', 'ladygaga', 'dualipa', 'justinbieber']
</code></pre>

<p><strong>Open the text files</strong>
the files are in directory lyrics/ from where I running my Jupyter notebook.</p>

<pre><code>for i, c in enumerate(bands):
     with open(""lyrics/"" + c + "".txt"", ""wb"") as file:
         pickle.dump(lyrics[i], file)
</code></pre>

<h1>Double check to make sure data has been loaded properly</h1>

<pre><code>data.keys()
</code></pre>

<p><strong>hopefully get result like this</strong></p>

<p>dict_keys(['bonjovi', 'lukebryan', 'johnprine', 'brunomars', 'methodman', 'bobmarley', 'nickcannon', 'weeknd', 'dojacat', 'ladygaga', 'dualipa', 'justinbieber'])</p>

<pre><code># Combine it!
data_combined = {key: [combine_text(value)] for (key, value) in data.items()}


# We are going to change this to key: artist, value: string format
def combine_text(list_of_text):
    '''Takes a list of text and combines them into one large chunk of text.'''
    combined_text = ' '.join(list_of_text)
    return combined_text
</code></pre>

<h1>We can either keep it in dictionary format or put it into a pandas dataframe</h1>

<p>import pandas as pd</p>

<pre><code>pd.set_option('max_colwidth',150)

data_df = pd.DataFrame.from_dict(data_combined).transpose()
data_df.columns = ['lyrics']
data_df = data_df.sort_index()
data_df
</code></pre>
",Dataset Preprocessing & Handling,read multiple txt file dict panda dataframe trying load multiple txt file dataframe know load url csv excel couldnt find reference load multiple txt file dataframe match dictionary viceversa text file comma tab separated plain text containing plain text song lyric checked panda document assistance welcome ideally dataframe dataframe hope achieve would like example basic example folder structure lyric muscian name open text file file directory lyric running jupyter notebook double check make sure data ha loaded properly hopefully get result like dict key bonjovi lukebryan johnprine brunomars methodman bobmarley nickcannon weeknd dojacat ladygaga dualipa justinbieber either keep dictionary format put panda dataframe import panda pd
Creating classification labels from unstructured data python,"<p>Is there a way to create labels from a column of strings in a dataset using Python?  The format of the string varies between 1 to ~10 describing words, where most of the time the category will be more than 1 word (example below).</p>

<p>I do not want to pre-populate the list of categories, but I would like the algorithm to create the categories based on common themes.</p>

<p>This is what I would like:</p>

<p><strong>Input:</strong></p>

<pre><code>**RepairName**
windscreen chip repair
windscreen chip repair x2
windscreen chip repairs
x4 tyre replacement
head light globe replacement
headlight bulb replace
headlight globe replacement
headlight replacement
tyre repalcement
tyre replacement
tyre replacement lhr
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>**RepairName**                    **Category**            
windscreen chip repair             windscreen chip repair
windscreen chip repair x2          windscreen chip repair
windscreen chip repairs            windscreen chip repair
x4 tyre replacement                tyre replacement
head light globe replacement       headlight replacement
headlight bulb replace             headlight replacement
headlight globe replacement        headlight replacement
headlight                          headlight replacement
headlight replacement              headlight replacement
tyre repalcement                   tyre replacement
tyre replacement lhr               tyre replacement
tyre replacement                   tyre replacement
</code></pre>

<p>This is what I have tried:</p>

<ol>
<li>Loads of sentiment analysis examples, but I my data is not positive or negative</li>
<li><p>I tried Counter from Collections - but this only counts the number of words in the dataset column - rather than common theme (see example below)</p></li>
<li><p>nltk.FreqDist but that counted 1 per observation (see second example below)</p></li>
<li><p>I have tried wordnet from nltk.corpus which seems to categorise 1 word strings only</p></li>
</ol>

<p><strong>First example - counter from collections</strong> </p>

<pre><code>    from collections import Counter
    categories = Counter("" "".join(df[""RepairName""]).split()).most_common(1000)
</code></pre>

<p><strong>Results:</strong>
<a href=""https://i.sstatic.net/BwUYt.png"" rel=""nofollow noreferrer"">counter from collections results</a></p>

<p><strong>nltk.FreqDist example:</strong></p>

<pre><code>    categories2 = df.groupby(['RepairName']).apply(lambda x: nltk.FreqDist(nltk.tokenize.word_tokenize(' '.join(x))))

</code></pre>

<p><strong>nltk.FreqDist results:</strong></p>

<p><a href=""https://i.sstatic.net/vfDry.png"" rel=""nofollow noreferrer"">nltk.FreqDist results</a></p>

<p><strong>wordnet example</strong></p>

<pre><code>df['C'] = df['RepairName'].apply(wordnet.synsets)
</code></pre>

<p>I understand this is probably quite a complex beast of a subject, but any advice would be greatly appreciated!  I would do it manually but aside from wanting to learn something new, the dataset is around 50K observations</p>
",Dataset Preprocessing & Handling,creating classification label unstructured data python way create label column string dataset using python format string varies describing word time category word example want pre populate list category would like algorithm create category based common theme would like input output tried load sentiment analysis example data positive negative tried counter collection count number word dataset column rather common theme see example nltk freqdist counted per observation see second example tried wordnet nltk corpus seems categorise word string first example counter collection result counter collection result nltk freqdist example nltk freqdist result nltk freqdist result wordnet example understand probably quite complex beast subject advice would greatly appreciated would manually aside wanting learn something new dataset around k observation
Creating document-feature matrix takes very long in R,"<p>I am trying to create a document feature matrix with character-level bigrams in R. The last line of my code takes forever to run and never finishes. The other lines take less than a minute max. I am not sure what to do. Any advice would be appreciated.</p>

<p><strong>Code:</strong></p>

<pre><code>library(quanteda)
#Tokenise corpus by characters
character_level_tokens = quanteda::tokens(corpus, 
                                what = ""character"",
                                remove_punct = T,
                                remove_symbols = T,
                                remove_numbers = T,
                                remove_url = T,
                                remove_separators = T, 
                                split_hyphens = T)

#Convert tokens to characters
character_level_tokens = as.character(character_level_tokens)

#Keep A-Z, a-z letters
character_level_tokens = gsub(""[^A-Za-z]"","""",character_level_tokens)

#Extract character-level bigrams
final_data_char_bigram = char_ngrams(character_level_tokens, n = 2L, concatenator = """")

#Create document-feature matrix (DFM)
dfm.final_data_char_bigram = dfm(final_data_char_bigram)


length(final_data_char_bigram)
[1] 37115571

head(final_data_char_bigram)
[1] ""lo"" ""ov"" ""ve"" ""el"" ""ly"" ""yt""


</code></pre>
",Dataset Preprocessing & Handling,creating document feature matrix take long r trying create document feature matrix character level bigram r last line code take forever run never finish line take le minute max sure advice would appreciated code
Convert Float Obj in Dataframe for CountVectorizer &amp; bow_transformer,"<p>I am trying to load a dataframe into into bag of words and CountVectorizer but I get TypeError: 'float' object is not iterable when going loading from mess equal a test sentence to mess equaling the dataframe I need to use.</p>

<p>the example corpus on <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">scikit learn docs</a> and the course online both loaded from just list of sentences instead of data frame. </p>

<p><strong>I tried Removing integers</strong> </p>

<p><a href=""https://stackoverflow.com/questions/53986123/attributeerror-int-object-has-no-attribute-lower-in-tfidf-and-countvectoriz"">AttributeError: 'int' object has no attribute 'lower' in TFIDF and CountVectorizer</a></p>

<p><strong>I get different error</strong></p>

<p>TypeError: list indices must be integers or slices, not str</p>

<pre><code>mess1 = [item for item in mess if not isinstance(item, int)]
</code></pre>

<p>this is what works </p>

<pre><code>mess = 'Sample message! Notice: it has punctuation.'
</code></pre>

<p><strong>this is the dataframe</strong> 
i need to use instead.</p>

<pre><code>mess.head()
</code></pre>

<pre><code>    |  bios                                                   |  artistName
----+---------------------------------------------------------+-------------------
0   |  Chris Cosentino Biography Chris Cosentino gre...       |  Chris Cosentino
----+---------------------------------------------------------+-------------------
1   |  Magda Biography The DJ known as Magda was bor...       |  Magda
----+---------------------------------------------------------+-------------------
2   |  Jean-Michel Cousteau Biography Since first be...       |  jean michel cousteau
----+---------------------------------------------------------+-------------------
3   |  Kyle Busch Biography The American stock car r...       |  Kyle Busch
----+---------------------------------------------------------+-------------------
4   |  Naughty by Nature Biography Naughty by Nature...       |  Naughty by Nature
----+---------------------------------------------------------+-------------------
</code></pre>

<pre><code>nopunc = [c for c in mess if c not in string.punctuation]

def text_process(mess):

   nopunc = [char for char in mess if char not in string.punctuation]

   nopunc = ''.join(nopunc)

   return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]

</code></pre>

<pre><code>mess['bios'].head(5).apply(text_process)

</code></pre>

<p><strong>Output</strong></p>

<pre><code>0    [Chris, Cosentino, Biography, Chris, Cosentino...
1    [Magda, Biography, DJ, known, Magda, born, rai...
2    [JeanMichel, Cousteau, Biography, Since, first...
3    [Kyle, Busch, Biography, American, stock, car,...
4    [Naughty, Nature, Biography, Naughty, Nature, ...
Name: bios, dtype: object
</code></pre>

<pre><code>mess.dtypes
</code></pre>

<pre><code>bios          object
artistName    object
dtype: object

</code></pre>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
</code></pre>

<p>then run either</p>

<pre><code>bow_transformer = CountVectorizer(analyzer=text_process)
bow_transformer.fit(mess['bios'])
print(len(bow_transformer.vocabulary_))
</code></pre>

<p>or this</p>

<pre><code>bow_transformer = CountVectorizer(analyzer=text_process).fit(mess['bios'])

print(len(bow_transformer.vocabulary))
</code></pre>

<p>I get the error</p>

<p><strong>TypeError: 'float' object is not iterable</strong></p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-148-74d381110eec&gt; in &lt;module&gt;
     1 bow_transformer = CountVectorizer(analyzer=text_process)
----&gt; 2 bow_transformer.fit(mess['bios'])
     3 print(len(bow_transformer.vocabulary_))

~\anaconda3\envs\nlp_course\lib\site-packages\sklearn\feature_extraction\text.py in fit(self, raw_documents, y)
   996         self
   997         """"""
--&gt; 998         self.fit_transform(raw_documents)
   999         return self
  1000 

~\anaconda3\envs\nlp_course\lib\site-packages\sklearn\feature_extraction\text.py in fit_transform(self, raw_documents, y)
  1030 
  1031         vocabulary, X = self._count_vocab(raw_documents,
-&gt; 1032                                           self.fixed_vocabulary_)
  1033 
  1034         if self.binary:

~\anaconda3\envs\nlp_course\lib\site-packages\sklearn\feature_extraction\text.py in _count_vocab(self, raw_documents, fixed_vocab)
   940         for doc in raw_documents:
   941             feature_counter = {}
--&gt; 942             for feature in analyze(doc):
   943                 try:
   944                     feature_idx = vocabulary[feature]

&lt;ipython-input-134-ad1781692b41&gt; in text_process(mess)
     1 def text_process(mess):
     2 
----&gt; 3     nopunc = [char for char in mess if char not in string.punctuation]
     4 
     5     nopunc = ''.join(nopunc)

TypeError: 'float' object is not iterable
</code></pre>
",Dataset Preprocessing & Handling,convert float obj dataframe countvectorizer bow transformer trying load dataframe bag word countvectorizer get typeerror float object iterable going loading mess equal test sentence mess equaling dataframe need use example corpus scikit learn doc course online loaded list sentence instead data frame tried removing integer href int object ha attribute lower tfidf countvectorizer get different error typeerror list index must integer slice str work dataframe need use instead output run either get error typeerror float object iterable
for loop: Select users that have used specific words more than x times in R,"<p>I have a dataframe (df) with user_names and text for each user. I have another data_frame with important words. I want to create a for loop that iterates over each user and counts how often the important words appear in their text.</p>

<p><strong>Data:</strong></p>

<pre><code>important_words = c(""marcus"", ""yesterday"", ""democrat"", ""republican"", ""trump"", ""hillary"")

df$user_names 
[1] ""marc12""
[2] ""jon""
[3] ""67han""
[4] ""XXmark""
[5] ""mark""
[6] ""mark""

df$text
[1] ""hi my name is marcus and i am a republican""
[2] ""i support hillary""
[3] ""go trump!""
[4] ""tomorrow i will vote democrat""
[5] ""i don't think so""
[6] ""yesterday was ok""


</code></pre>
",Dataset Preprocessing & Handling,loop select user used specific word x time r dataframe df user name text user another data frame important word want create loop iterates user count often important word appear text data
Convert categorical features with and without unique seperators using pd.get_dummies in pandas,"<p><strong>Details about the goal</strong></p>

<p>I am trying to use pd.get_dummies in pandas to convert the categorical features to data frames with dummy/indicator variables for each of three different genres, demographics, and prices separately. </p>

<p><strong>Additional details</strong></p>

<p>Two have a separator one a "","" and another a ""| "" and the third there is only one choice it has a comma but that is part of the price not a separator.</p>

<p><strong>Overall goal - beyond this fix</strong></p>

<p>After I am done I would like to run a scaling function returns a numpy array containing the features KNN model from scikit-learn to the data and calculate the nearest neighbors for each distances.</p>

<p><strong>import and load dataset</strong></p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
import seaborn as sns
%matplotlib inline

import warnings
warnings.filterwarnings(""ignore"", category=DeprecationWarning)

artist = pd.read_csv(""artist.csv"")

artist.head()
</code></pre>

<p><strong>This is the current dataframe</strong></p>

<p>I simplified as the real data frame is massive thousand of names, genres, prices points and demographics. </p>

<p><strong>Dataframe:</strong></p>

<pre><code>id |            name              |       genre                              |     price         |     demo                 |      songs     |         bio      |
---+------------------------------+------------------------------------------+-------------------+--------------------------+----------------+------------------+
 1 |           Ace Frehley         |    Classic Rock,Rock Music               |   Call For Fee    |  25-35,35-50,50 +        |
---+------------------------------+------------------------------------------+-------------------+--------------------------+----------------+------------------+
 2 |           Air Supply         | Adult Contemporary, Pop Music            |   Call For Fee    |  35-50, 50 +             |
---+------------------------------+------------------------------------------+-------------------+--------------------------+----------------+------------------+
 3 |           Bebe Rexha         |  Country Music, Hip Hop &amp; Rap, Pop Music |   Call For Fee    |  Undefined               |
---+------------------------------+------------------------------------------+-------------------+--------------------------+----------------+------------------+
 4 |           Blanco Brown       |           Hip Hop &amp; Rap, R&amp;B             |   Call For Fee    |  Undefined               |
---+------------------------------+------------------------------------------+-------------------+--------------------------+----------------+------------------+
 5 |           Cautious Clay      |           Hip Hop &amp; Rap, R&amp;B             |   Call For Fee    |  Undefined               |
---+------------------------------+------------------------------------------+-------------------+--------------------------+----------------+------------------+
 6 |           Andy Samberg       |           Standup Comedy                 |   Call For Fee    |  18-25,25-35,35-50       | 
---+------------------------------+------------------------------------------+-------------------+--------------------------+----------------+------------------+
 7 |           Afrojack           |              DJ's                        |  Under $200,000   |  Under 18,18-25,25-35    |                    
---+------------------------------+------------------------------------------+-------------------+--------------------------+----------------+------------------+
 8 |           Billy Idol         |              Classic Rock                |  Under $200,000   |  25-35,35-50,50 +        |
---+------------------------------+------------------------------------------+-------------------+--------------------------+----------------+------------------+

</code></pre>

<pre><code>
artist.isnull().sum()
</code></pre>

<p><strong>pandas.get_dummies</strong>
I read here and tried a few different things to no luck.</p>

<p><a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html</a></p>

<p><strong>What i tried</strong></p>

<pre><code>artist1[""genre""] = artist1[""genre""].astype(float)


artist1_features = pd.concat([artist1['genre'].str.get_dummies(sep=""| ""),
                              pd.concat([artist1['demo'].str.get_dummies(sep="",""),
                              pd.get_dummies(artist1[['price']]),axis=1)
artist1[""name""] = artist1[""name""].map(lambda name:re.sub(''[^A-Za0-9]+', "" "", name))                              
artist1_features.head()

</code></pre>

<p><strong>I also tried this</strong></p>

<pre><code>artist1[""genre""] = artist1[""genre""].astype(float)
artist1_processed = pd.get_dummies(metadata['genre']).str.get_dummies(sep=""| "")
artist1_concat = pd.concat([artist1_processed, metadata], axis=1)
pd.get_dummies(artists1[[""genre""]]).head()
</code></pre>

<p><strong>errors I got</strong></p>

<p><a href=""https://i.sstatic.net/LQTF7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LQTF7.png"" alt=""enter image description here""></a></p>

<p><strong>goal</strong></p>

<p>Ideally I would like to use <strong>pd.get_dummies</strong>, a pandas method for converting categorical features to data frames with dummy/indicator variables for each genres, demographics, and prices separately.</p>

<p><strong>Genres</strong> 
has a separator like this ""| "" basically - 
ex: Country Music| Hip Hop &amp; Rap| Pop Music</p>

<p><strong>demographics</strong> 
has a separator like this ""| "" basically - 
ex: Under 18,18-25,25-35 </p>

<p><strong>prices</strong> 
does not need a separator but has a comma - 
ex:  Under $200,000</p>

<p>I am applying ideas from a few different movie database recommender systems tutorials into a real project.</p>

<p>which should look like the following once done.</p>

<p><strong>expected results</strong></p>

<p><strong>What I am trying to do:</strong></p>

<p><strong>Genre:</strong></p>

<pre><code>id |            name              | Adult Contemporary | Classic Rock | Country Music | DJ's | Standup Comedy | Pop Music | Rock Music | Hip Hop &amp; Rap | R&amp;B |
---+------------------------------+--------------------+--------------+---------------+------+----------------+-----------+------------+---------------+-----+
 1 |           Ace Frehley         |           0        |        1     |       0       |  0   |       0        |        0  |      1     |         0     |  0  | 
---+------------------------------+--------------------+--------------+---------------+------+----------------+-----------+------------+---------------+-----+
 2 |           Air Supply         |           1        |        0     |       0       |  0   |       0        |        1  |      0     |         0     |  0  | 
---+------------------------------+--------------------+--------------+---------------+------+----------------+-----------+------------+---------------+-----+
 3 |           Bebe Rexha         |           0        |        0     |       1       |  0   |       0        |        1  |      0     |         1     |  0  | 
---+------------------------------+--------------------+--------------+---------------+------+----------------+-----------+------------+---------------+-----+
 4 |           Blanco Brown       |           1        |        0     |       1       |  0   |       0        |        0  |      0     |         1     |  0  | 
---+------------------------------+--------------------+--------------+---------------+------+----------------+-----------+------------+---------------+-----+
 5 |           Cautious Clay      |           0        |        0     |       0       |  0   |       0        |        0  |      0     |         1     |  1  | 
---+------------------------------+--------------------+--------------+---------------+------+----------------+-----------+------------+---------------+-----+
 6 |           Andy Samberg       |           0        |        0     |       1       |  0   |       1        |        0  |      0     |         1     |  0  | 
---+------------------------------+--------------------+--------------+---------------+------+----------------+-----------+------------+---------------+-----+
 7 |           Afrojack           |           0        |        0     |       0       |  1   |       0        |        0  |      0     |         0     |  0  | 
---+------------------------------+--------------------+--------------+---------------+------+----------------+-----------+------------+---------------+-----+
 8 |           Billy Idol         |           0        |        1     |       0       |  0   |       0        |        0  |      0     |         0     |  0  | 
---+------------------------------+--------------------+--------------+---------------+------+----------------+-----------+------------+---------------+-----+
</code></pre>

<p><strong>Demographics:</strong></p>

<pre><code>
id |            name              | Under 18 | 18-25 | 25-35 | 35-50 | 50 + | Undefined |
---+------------------------------+----------+-------+-------+-------+------+-----------+
 1 |        Ace Frehley           |     0    |   0   |   1   |   1   |   1  |    0      |
---+------------------------------+----------+-------+-------+-------+------+-----------+
 2 |        Air Supply            |     0    |   0   |   0   |   1   |   1  |    0      |
---+------------------------------+----------+-------+-------+-------+------+-----------+
 3 |        Bebe Rexha            |     0    |   0   |   0   |   0   |   0  |    1      |    
---+------------------------------+----------+-------+-------+-------+------+-----------+
 4 |            Blanco Brown      |     0    |   0   |   0   |   0   |   0  |    1      | 
---+------------------------------+----------+-------+-------+-------+------+-----------+
 5 |            Cautious Clay     |     0    |   0   |   1   |   1   |   1  |    1      | 
---+------------------------------+----------+-------+-------+-------+------+-----------+
 6 |            Andy Samberg      |     0    |   1   |   1   |   1   |   0  |    0      | 
---+------------------------------+----------+-------+-------+-------+------+-----------+
 7 |            Afrojack          |     1    |   1   |   1   |   0   |   0  |    0      |  
---+------------------------------+----------+-------+-------+-------+------+-----------+
 8 |            Billy Idol        |     0    |   0   |   1   |   1   |   1  |    0      | 
---+------------------------------+----------+-------+-------+-------+------+-----------+
</code></pre>

<p><strong>Price:</strong></p>

<pre><code>id |            name              | Call For Fee | Under $15,000 | Under $25,000 | Under $50,000 | Under $75,000 | Under $100,000 | Under $150,000 | Under $200,000 |
---+------------------------------+--------------+---------------+---------------+---------------+---------------+----------------+----------------+----------------+
 1 |        Ace Frehley           |       1      |       0       |       0       |       0       |       0       |        0       |        0       |        0       |
---+------------------------------+--------------+---------------+---------------+---------------+---------------+----------------+----------------+----------------+
 2 |        Air Supply            |       0      |       0       |       0       |       0       |       0       |        1       |        0       |        0       |
---+------------------------------+--------------+---------------+---------------+---------------+---------------+----------------+----------------+----------------+
 3 |        Bebe Rexha            |       1      |       0       |       0       |       0       |       0       |        0       |        0       |        0       |
---+------------------------------+--------------+---------------+---------------+---------------+---------------+----------------+----------------+----------------+
 4 |            Blanco Brown      |       1      |       0       |       0       |       0       |       0       |        0       |        0       |        0       | 
---+------------------------------+--------------+---------------+---------------+---------------+---------------+----------------+----------------+----------------+
 5 |            Cautious Clay     |       1      |       0       |       0       |       0       |       0       |        0       |        0       |        0       |
---+------------------------------+--------------+---------------+---------------+---------------+---------------+----------------+----------------+----------------+
 6 |            Andy Samberg      |       1      |       0       |       0       |       0       |       0       |        0       |        0       |        0       |
---+------------------------------+--------------+---------------+---------------+---------------+---------------+----------------+----------------+----------------+
 7 |            Afrojack          |       0      |       0       |       0       |       0       |       0       |        0       |        0       |        1       | 
---+------------------------------+--------------+---------------+---------------+---------------+---------------+----------------+----------------+----------------+
 8 |            Billy Idol        |       0      |       0       |       0       |       0       |       0       |        0       |        0       |        1       | 
---+------------------------------+--------------+---------------+---------------+---------------+---------------+----------------+----------------+----------------+
</code></pre>

<p>After I am done I would like to run a scaling function returns a numpy array containing the features KNN model from scikit-learn to the data and calculate the nearest neighbors for each distances.</p>
",Dataset Preprocessing & Handling,convert categorical feature without unique seperators using pd get dummy panda detail goal trying use pd get dummy panda convert categorical feature data frame dummy indicator variable three different genre demographic price separately additional detail two separator one another third one choice ha comma part price separator overall goal beyond fix done would like run scaling function return numpy array containing feature knn model scikit learn data calculate nearest neighbor distance import load dataset current dataframe simplified real data frame massive thousand name genre price point demographic dataframe panda get dummy read tried different thing luck tried also tried error got goal ideally would like use pd get dummy panda method converting categorical feature data frame dummy indicator variable genre demographic price separately genre ha separator like basically ex country music hip hop rap pop music demographic ha separator like basically ex price doe need separator ha comma ex applying idea different movie database recommender system tutorial real project look like following done expected result trying genre demographic price done would like run scaling function return numpy array containing feature knn model scikit learn data calculate nearest neighbor distance
Python - multiprocessing with large strings slower,"<p>I am working on a textual analysis of a large sample of 10Ks (about 150,000) and desperately trying to speed up my program with multiprocessing. The relevant function loads the txt files, parses them with some RegExp and saves them as ""clean"":</p>

<pre><code>def plain_10k(f):
input_text = open(ipath + ""\\"" + f, errors = ""ignore"").read()

# REGEXP

output_file = open(opath + ""\\"" + f, ""w"", errors = ""ignore"")
output_file.write(input_text)
output_file.close()
</code></pre>

<p>I try to perform this function over a list of file names as follows:</p>

<pre><code>with Pool(processes = 8) as pool, tqdm(total = len(files_10k)) as pbar:
for d in pool.imap_unordered(plain_10k, files_10k):
    pbar.update()
</code></pre>

<p>Unfortunately, the program seems to be stuck as it is not returning (i.e. saving clean txt files) anything. Even with a small list of 10 files, nothing happens.</p>

<p>What is the problem here?</p>

<p>If it is relevant: the size of the input txt files ranges from 10kb to 10mb with the majority beeing smaller than 1mb.</p>

<p>I am quite new to Python, so the code above is the result of hours of googling and certainly not very good. I am happy about any comments and suggestions.</p>

<p>Thank you very much in advance!</p>
",Dataset Preprocessing & Handling,python multiprocessing large string slower working textual analysis large sample k desperately trying speed program multiprocessing relevant function load txt file par regexp save clean try perform function list file name follows unfortunately program seems stuck returning e saving clean txt file anything even small list file nothing happens problem relevant size input txt file range kb mb majority beeing smaller mb quite new python code result hour googling certainly good happy comment suggestion thank much advance
Check if the lines in the dataframe roughly correspond to each other,"<p>I have a data frame with names of cities in Morocco and another one with similar names but that was not well coded. Here's the first one:</p>

<pre><code>&gt;&gt;&gt; df[['new_regiononame']].head()

    new_regiononame
0   Grand Casablanca-Settat
1   Fès-Meknès
2   Souss-Massa
3   Laayoune-Sakia El Hamra
4   Fès-Meknès
</code></pre>

<p>and here's the other one I wanted to change to the names of the first one. At least they know a way to read it correctly:</p>

<pre><code>&gt;&gt;&gt;X_train[['S02Q03A_Region']].head()

    S02Q03A_Region
10918   FÃ¨s-MeknÃ¨s
1892    Rabat-SalÃ©-KÃ©nitra
6671    Casablanca-Settat
4837    Marrakech-Safi
6767    Casablanca-Settat
</code></pre>

<p>How can I check if the lines in the dataframe roughly correspond to each other and, if so, rename <code>X_train</code> rows by <code>df</code> ones?</p>

<p>So far I only know how to extract which rows in <code>X_train</code> have exact equivalents in <code>df</code>:</p>

<pre><code>X_train['S02Q03A_Region'][X_train['S02Q03A_Region'].isin(df['new_regiononame'].unique())]
</code></pre>
",Dataset Preprocessing & Handling,check line dataframe roughly correspond data frame name city morocco another one similar name wa well coded first one one wanted change name first one least know way read correctly check line dataframe roughly correspond rename row one far know extract row exact equivalent
Handle Na without dropping them in Dataframe SpaCy in pandas Dataframe,"<p><strong>Problem</strong></p>

<p>How do I handle the Nan when almost all rows have them making dropna not a valid possibility. I would like to run similarity against a more complete data frame of reviews because currently if I dropna it removes all but 1 row making it not useful.</p>

<p>In past the <strong>original file only had a few nan so dropna worked</strong> but in this case 2800 rows have nan only 1 does not. I assume spaCy will not perform this with nan or 0's</p>

<p>I am trying to run spaCy similarity on a data frame that has several nan fields.</p>

<p><strong>How do I handle the Na without dropping them</strong>
How do you handle NaN </p>

<p><strong>I tried 3 things to no success</strong></p>

<p>dropna, fill with zeros and fill with med</p>

<p>I tried drop na and I also tried replace with fillzero  but all but one review row is complete so it leaves nothing to compare.</p>

<p>New to NLP and got stuck here.</p>

<p>Import</p>

<pre><code>import csv
import pandas as pd
import nltk
import numpy as np
from nltk.tokenize import PunktSentenceTokenizer,RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from tempfile import NamedTemporaryFile
import shutil
import warnings

</code></pre>

<pre><code>import spacy
import en_core_web_sm

nlp = en_core_web_sm.load()

warnings.filterwarnings(""ignore"", category=DeprecationWarning)
</code></pre>

<pre><code>#reading the attributes file
#check into the ""attributes.txt"" file for the proper format
#each attribute has to be listed in a new line.
attributes=list(line.strip() for line in open('attributes.txt'))
attributes="" "".join(attributes)
</code></pre>

<p><strong>attributes.txt</strong></p>

<p>this just load a text file with a list with words see below</p>

<pre><code>Airy
Bright
Child-like
Effervescent
Graceful
Hymn-like
Innocent
Light
Naive
Optimistic
Poignant
Precious
Pure
Rhapsodic
Sacred
Shimmering
Sugary
Sweet
Tender
Thoughtful
Transparent/Translucent
Whimsical

</code></pre>

<pre><code>reviews_df = pd.read_excel('adult_contemporary_reviews(A-B).xlsx',encoding='utf8', errors='ignore')
reviews_df.head()
</code></pre>

<p><strong>The original data-frame</strong></p>

<p><a href=""https://i.sstatic.net/pCzlT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pCzlT.png"" alt=""dataframe""></a></p>

<pre><code>reviews_df.shape
</code></pre>

<p><strong>output (159, 32)</strong></p>

<p>I tried to remove nan by filling with zeros</p>

<pre><code>reviews_df=reviews_df.fillna(0)
reviews_df

</code></pre>

<p><a href=""https://i.sstatic.net/HQeUk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HQeUk.png"" alt=""dropna zero""></a></p>

<p><strong>This is where I got confused only one of these methods worked I believe</strong></p>

<pre><code>reviews_df['similarity'] = -1

# for index, row in reviews_df.iterrows():
# reviews_df.loc[index,'similarity'] = nlp(row[""product_review""]).similarity(nlp(attributes))

for i in reviews_df.columns:
   reviews_df.loc[1,i] = nlp(reviews_df.loc[0,i]).similarity(nlp(attributes))

#     # Iterate over the sequence of column names
# for column in reviews_df:
#     reviews_df.loc[index,'0'] = nlp(column[""aaronneville-reviews""]).similarity(nlp(attributes))

</code></pre>

<p>Type error I assume this is because I replace the Nan with zeros which are ints? Maybe</p>

<p>I get a <strong>TypeError: object of type 'numpy.int64' has no len()</strong></p>

<p><a href=""https://i.sstatic.net/F7pSx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/F7pSx.png"" alt=""type error""></a></p>

<p>then lastly the output which I assume works but not as planned because it dropped everything</p>

<pre><code>#writing to an output file
reviews_df.to_excel(r""C:\Users\Name\nlp\sim\Similarity_output.xlsx"", index=False)
</code></pre>

<p><a href=""https://i.sstatic.net/VflF3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VflF3.png"" alt=""enter image description here""></a></p>

<p>Because the original issue of imported has so many nan in the dataframe it outputted only 1 review and the similarity score it should instead compare all lines by handling the na somehow.</p>

<p><a href=""https://i.sstatic.net/dtT7a.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dtT7a.png"" alt=""outputxslx file""></a></p>
",Dataset Preprocessing & Handling,handle na without dropping dataframe spacy panda dataframe problem handle nan almost row making dropna valid possibility would like run similarity complete data frame review currently dropna remove row making useful past original file nan dropna worked case row nan doe assume spacy perform nan trying run spacy similarity data frame ha several nan field handle na without dropping handle nan tried thing success dropna fill zero fill med tried drop na also tried replace fillzero one review row complete leaf nothing compare new nlp got stuck import attribute txt load text file list word see original data frame output tried remove nan filling zero got confused one method worked believe type error assume replace nan zero ints maybe get typeerror object type numpy int ha len lastly output assume work planned dropped everything original issue imported ha many nan dataframe outputted review similarity score instead compare line handling na somehow
Word cloud based on each row,"<p>My data frame looks like - </p>

<pre><code>state       text
a         good,abc,bad,good,to
b         oil,reach,bad,oil,and
c         milk,oil,milk,milk
</code></pre>

<p>I want to do word cloud based on each state. My code is given below -</p>

<pre><code>import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS

def show_wordcloud(data, title):

    wordcloud = WordCloud(
    background_color='white',
    max_words=200,
    max_font_size=40,
    scale=3,
).generate(str(data))

    fig = plt.figure(1, figsize=(12, 12))
    plt.axis('off')

    if title:
        fig.suptitle(title, fontsize=20)
        fig.subplots_adjust(top=2.3)

    plt.imshow(wordcloud)
    plt.savefig('cluster{}.png'.format(title))
    plt.show()


for row in df.iterrows():
    show_wordcloud(row[text], row[State])
</code></pre>

<p>But not getting any results. Error is - </p>

<pre><code>    TypeError                                 Traceback (most recent call last)
&lt;ipython-input-341-c01a0b1ee783&gt; in &lt;module&gt;
     24 
     25 for row in lead_state_wise1.iterrows():
---&gt; 26     show_wordcloud(row[text], row[State])

TypeError: tuple indices must be integers or slices, not list
</code></pre>

<p>any suggestion?</p>
",Dataset Preprocessing & Handling,word cloud based row data frame look like want word cloud based state code given getting result error suggestion
"Python, pandas and NLP: creating a corpus by dividing text based on value in other column","<p>I'm quite new to ""coding"" in general, and Python in particular, so bear with me! </p>

<p>I have a CSV file that has feedback gathered from a feedback form on a web site (a ""Was this page useful"" feedback form. The CSV has one row per feedback received. There are several columns, but the ones that I'M interested in in this case are:
- 'What's wrong' (populated by a selection from a list by the user - limited number of possible values)
-'Details' (text typed in - the actuations feedback)
- 'Topic' (manually entered by reviewers of the feedback - hoping to eventually automate this through classification, but not there yet - the possible values here are potentially limitless</p>

<p>I made a pandas data frame out of this CSV. </p>

<p>I want to create a corpus to run some NLP algorithms (TF-IDF, for example). I want to stitch together all the text from the ""Details"" column, based the value of the 'What's wrong' and-or the 'Topic' column. Basically, the corpus would have one document per 'value' present in the Topic column, comprised of the text from all rows that have this value. </p>

<p>My initial thinking would be something like this:
- create a dictionary
- iterate over the values in the 'Topic' column, and create a key for each of the value found in that list
- iterate over each row - if value X is in the the 'Topic' column, append the string from the 'Details' cell in the value for key X in the dictionary</p>

<p>I'm not too sure how to code that, or if there would be a better way to do this...</p>

<p>Data looks like this:
<a href=""https://i.sstatic.net/bIFeG.png"" rel=""nofollow noreferrer"">Dataframe with 3 columns: 'Ref number', 'Details', and 'Topic'</a></p>

<p>What I want is to automatically create ""documents"", stitching together all the strings from the Detail column that have the same label in the ""Topic"" column. The list of topics will be changing, so I won't know in advance the number of documents to create (and the exact names of the different topics). </p>

<p>This creates the dictionary:</p>

<pre><code>dict = {}        
for topic in data_cropped['Topic'].unique():
       dict[topic] = []
</code></pre>

<p>And then I'm stuck to try to join the string from 'Details' in dict[topic], if value from 'Topic' is equal to the key. </p>

<p>I tried this:</p>

<pre><code>for text in data_cropped['Details']:
       if data_cropped['Topic'] in dict.keys():
                dict[key] = dict[key].append(text)
</code></pre>

<p>But I get a this error: TypeError: 'Series' objects are mutable, thus they cannot be hashed</p>

<p>Any help would be appreciated!  </p>
",Dataset Preprocessing & Handling,python panda nlp creating corpus dividing text based value column quite new coding general python particular bear csv file ha feedback gathered feedback form web site wa page useful feedback form csv ha one row per feedback received several column one interested case wrong populated selection list user limited number possible value detail text typed actuation feedback topic manually entered reviewer feedback hoping eventually automate classification yet possible value potentially limitless made panda data frame csv want create corpus run nlp algorithm tf idf example want stitch together text detail column based value wrong topic column basically corpus would one document per value present topic column comprised text row value initial thinking would something like create dictionary iterate value topic column create key value found list iterate row value x topic column append string detail cell value key x dictionary sure code would better way data look like dataframe column ref number detail topic want automatically create document stitching together string detail column label topic column list topic changing know advance number document create exact name different topic creates dictionary stuck try join string detail dict topic value topic equal key tried get error typeerror series object mutable thus hashed help would appreciated
search and remove a string if it is part of another string in R,"<p>I have a data frame in R with the following text column</p>

<pre><code>&lt;strings&gt;
sentence
this is
this is a sentence
outlook
microsoft outlook
</code></pre>

<p>I want to search and remove the strings in this column that are a substring of another string. 
My output should be </p>

<pre><code>&lt;strings&gt;
this is a sentence
microsoft outlook
</code></pre>
",Dataset Preprocessing & Handling,search remove string part another string r data frame r following text column want search remove string column substring another string output
How do I perform a regular expression on multiple .txt files in a folder (Python)?,"<p>I'm trying to open up 32 .txt files, extract some text from them (using RegEx) and then save them as individual files again(later on in the project I'm hoping to collate them together). I've tested the RegEx on a single file and it seems to work:</p>

<pre><code>import os
import re
os.chdir(r'C:\Users\garet\OneDrive - University of Exeter\Masters\Year Two\Dissertation planning\Manual scrape\Finished years proper')
with open('1988.txt') as txtfile:
    text= txtfile.read()
#print(len(text)) #sentences in text

start = r'Body\n\n\n'

docs = re.findall(start, text)
print('Found the start of %s documents.' % len(docs))
end = r'Load-Date:'
print('Found the end of %s documents.' % len(docs))
docs = re.findall(end, text)
regex = start+r'(.+?)'+end
articles = re.findall(regex, text, re.S)
print('You have now parsed the 154 articles so only the body of content remains. All metadata has been removed.')
print('Here is an example of a parsed article:', articles[0])
</code></pre>

<p>Now I want to perform the exact same thing on all my .txt files in that folder, but I can't figure out how to. I've been playing around with For loops but with little success. Currently I have this:</p>

<pre><code>import os
import re
finished_years_proper= os.listdir(r'C:\Users\garet\OneDrive - University of Exeter\Masters\Year Two\Dissertation\Manual scrape\Finished years proper')
os.chdir(r'C:\Users\garet\OneDrive - University of Exeter\Masters\Year Two\Dissertation\Manual scrape\Finished years proper')
print('There are %s .txt files in this folder.' % len(finished_years_proper))

if i.endswith("".txt""):
    with open(finished_years_proper + i, 'r') as all_years:
        for line in all_years:
            start = r'Body\n\n\n'
            docs = re.findall(start, all_years)
            end = r'Load-Date:'
            docs = re.findall(end, all_years)
            regex = start+r'(.+?)'+end
            articles = re.findall(regex, all_years, re.S)
</code></pre>

<p>However, I'm returning a type error: </p>

<pre><code> File ""C:\Users\garet\OneDrive - University of Exeter\Masters\Year Two\Dissertation\Method\Python\untitled1.py"", line 15, in &lt;module&gt;
        with open(finished_years_proper + i, 'r') as all_years:

    TypeError: can only concatenate list (not ""str"") to list
</code></pre>

<p>I'm unsure how to proceed... I've seen on other forums that I should convert something into a string, but I'm not sure what to convert or even if this is the right way to proceed. Any help with this would be really appreciated!</p>

<hr>

<p>After taking Benedictanjw's into my codes I've ended up with this:
Hi, this is what I ended up with:</p>

<p>all_years= []
for fyp in finished_years_proper: #fyp is each text file in folder
    with open(fyp, 'r') as year:
        for line in year: #line is each element in each text file in folder
            start = r'Body\n\n\n'
            docs = re.findall(start, line)
            end = r'Load-Date:'
            docs = re.findall(end, line)
            regex = start+r'(.+?)'+end
            articles = re.findall(regex, line, re.S)
            all_years.append(articles) #append strings to reflect RegEx
            parsed_documents= all_years.append(articles) 
            print(parsed_documents) #returns None. Apparently this is okay. </p>

<p>Does the 'None' mean that the parsing of each file is successful (as in it emulates the result I had when I tested the RegEx on a single file)? And if so, how can I visualise my output without returning None.  Many thanks in advance!!</p>
",Dataset Preprocessing & Handling,perform regular expression multiple txt file folder python trying open txt file extract text using regex save individual file later project hoping collate together tested regex single file seems work want perform exact thing txt file folder figure playing around loop little success currently however returning type error unsure proceed seen forum convert something string sure convert even right way proceed help would really appreciated taking benedictanjw code ended hi ended year fyp finished year proper fyp text file folder open fyp r year line year line element text file folder start r body n n n doc findall start line end r load date doc findall end line regex start r end article findall regex line year append article append string reflect regex parsed document year append article print parsed document return none apparently okay doe none mean parsing file successful emulates result tested regex single file visualise output without returning none many thanks advance
how to tokenize multiple columns of a pandas dataframe for NLP,"<p>How to tokenize my data frame with numbers, which is having multiple columns of text data  </p>

<p><img src=""https://i.sstatic.net/27Vx9.png"" alt=""enter image description here""></p>
",Dataset Preprocessing & Handling,tokenize multiple column panda dataframe nlp tokenize data frame number multiple column text data
Custom WordNet in NLTK,"<p>So, I have an xml file that is formatted with as wordNet with Synsets and Synonyms and other tags. I need to use this xml file to get things like word similarites etc. So, I want to use nltk to read and convert this xml to a custom wordNet type of thing. I know I can use the XmlCorpusReader to create corpus out of xml, but I couln't figure out how to use the corpus to create the wornet and use functions like path_similarity etc. Is it possible to do this with nltk, if it is how should I proceed?</p>

<p>Thank you</p>
",Dataset Preprocessing & Handling,custom wordnet nltk xml file formatted wordnet synset synonym tag need use xml file get thing like word similarites etc want use nltk read convert xml custom wordnet type thing know use xmlcorpusreader create corpus xml couln figure use corpus create wornet use function like path similarity etc possible nltk proceed thank
Python - calculate the co-occurrence matrix,"<p>I'm working on an NLP task and I need to calculate the co-occurrence matrix over documents. The basic formulation is as below:</p>

<p>Here I have a matrix with shape <code>(n, length)</code>, where each row represents a sentence composed by <code>length</code> words. So there are <code>n</code> sentences with same length in all. Then with a defined context size, e.g., <code>window_size = 5</code>, I want to calculate the co-occurrence matrix <code>D</code>, where the entry in the <code>cth</code> row and <code>wth</code> column is <code>#(w,c)</code>, which means the number of times that a context word <code>c</code> appears in <code>w</code>'s context.</p>

<p>An example can be referred here. <a href=""https://linguistics.stackexchange.com/questions/3641/how-to-calculate-the-co-occurrence-between-two-words-in-a-window-of-text"">How to calculate the co-occurrence between two words in a window of text?</a></p>

<p>I know it can be calculate by stacking loops, but I want to know if there exits an simple way or simple function? I have find some answers but they cannot work with a window sliding through the sentence. For example:<a href=""https://stackoverflow.com/questions/35562789/word-word-co-occurrence-matrix"">word-word co-occurrence matrix</a></p>

<p>So could anyone tell me is there any function in Python can deal with this problem concisely? Cause I think this task is quite common in NLP things.</p>
",Dataset Preprocessing & Handling,python calculate co occurrence matrix working nlp task need calculate co occurrence matrix document basic formulation matrix shape row represents sentence composed word sentence length defined context size e g want calculate co occurrence matrix entry row column mean number time context word appears context example referred could anyone tell function python deal problem concisely cause think task quite common nlp thing
Reading a Specific JSON Column for Tokenization,"<p>I am planning to tokenize a column within a JSON file with NLTK. The code below reads and slices the JSON file according into different time intervals. </p>

<p>I am however struggling to have the <code>'Main Text'</code> column (within the JSON file) read/tokenized in the final part of the code below. Is there any smart tweak to make this happen?</p>

<pre><code># Loading and reading dataset
file = open(""Glassdoor_A.json"", ""r"")
data = json.load(file)
df = pd.json_normalize(data)
df['Date'] = pd.to_datetime(df['Date'])


# Create an empty dictionary
d = dict()


# Filtering by date
start_date = pd.to_datetime('2009-01-01')
end_date = pd.to_datetime('2009-03-31')
last_end_date = pd.to_datetime('2017-12-31')
mnthBeg = pd.offsets.MonthBegin(3)
mnthEnd = pd.offsets.MonthEnd(3)
while end_date &lt;= last_end_date:
    filtered_dates = df[df.Date.between(start_date, end_date)]
    n = len(filtered_dates.index)
    print(f'Date range: {start_date.strftime(""%Y-%m-%d"")} - {end_date.strftime(""%Y-%m-%d"")},  {n} rows.')
    if n &gt; 0:
        print(filtered_dates)
    start_date += mnthBeg
    end_date += mnthEnd


# NLTK tokenizing
file_content = open('Main Text').read()
tokens = nltk.word_tokenize(file_content)
print(tokens)
</code></pre>
",Dataset Preprocessing & Handling,reading specific json column tokenization planning tokenize column within json file nltk code read slice json file according different time interval however struggling column within json file read tokenized final part code smart tweak make happen
tokenizer.texts_to_sequences Keras Tokenizer gives almost all zeros,"<p>I am working to create a text classification code but I having problems in encoding documents using the tokenizer. </p>

<p>1) I started by fitting a tokenizer on my document as in here: </p>

<pre><code>vocabulary_size = 20000
tokenizer = Tokenizer(num_words= vocabulary_size, filters='')
tokenizer.fit_on_texts(df['data'])
</code></pre>

<p>2) Then I wanted to check if my data is fitted correctly so I converted into sequence as in here: </p>

<pre><code>sequences = tokenizer.texts_to_sequences(df['data'])
data = pad_sequences(sequences, maxlen= num_words) 
print(data) 
</code></pre>

<p>which gave me fine output. i.e. encoded words into numbers </p>

<pre><code>[[ 9628  1743    29 ...   161    52   250]
 [14948     1    70 ...    31   108    78]
 [ 2207  1071   155 ... 37607 37608   215]
 ...
 [  145    74   947 ...     1    76    21]
 [   95 11045  1244 ...   693   693   144]
 [   11   133    61 ...    87    57    24]]
</code></pre>

<p>Now, I wanted to convert a text into a sequence using the same method. 
Like this: </p>

<pre><code>sequences = tokenizer.texts_to_sequences(""physics is nice "")
text = pad_sequences(sequences, maxlen=num_words)
print(text)
</code></pre>

<p>it gave me weird output: </p>

<pre><code>[[   0    0    0    0    0    0    0    0    0  394]
 [   0    0    0    0    0    0    0    0    0 3136]
 [   0    0    0    0    0    0    0    0    0 1383]
 [   0    0    0    0    0    0    0    0    0  507]
 [   0    0    0    0    0    0    0    0    0    1]
 [   0    0    0    0    0    0    0    0    0 1261]
 [   0    0    0    0    0    0    0    0    0    0]
 [   0    0    0    0    0    0    0    0    0 1114]
 [   0    0    0    0    0    0    0    0    0    1]
 [   0    0    0    0    0    0    0    0    0 1261]
 [   0    0    0    0    0    0    0    0    0  753]]
</code></pre>

<p>According to Keras documentation (<a href=""https://faroit.github.io/keras-docs/1.2.2/preprocessing/text/"" rel=""noreferrer"">Keras</a>): </p>

<blockquote>
  <p>texts_to_sequences(texts)</p>
  
  <p>Arguments: texts: list of texts to turn to sequences. </p>
  
  <p>Return: list of
  sequences (one per text input).</p>
</blockquote>

<p>is it not supposed to encode each word to its corresponding number? then pad the text if it shorter than 50 to 50? 
Where is the mistake ? </p>
",Dataset Preprocessing & Handling,tokenizer text sequence kera tokenizer give almost zero working create text classification code problem encoding document using tokenizer started fitting tokenizer document wanted check data fitted correctly converted sequence gave fine output e encoded word number wanted convert text sequence using method like gave weird output according kera documentation kera text sequence text argument text list text turn sequence return list sequence one per text input supposed encode word corresponding number pad text shorter mistake
"Textmining in R using qunateda package, rpart","<p>I just joined stackoverflow to find solution for this error. After running the following code, R throws error (see below). Pls help me how to solve this. I am new to text mining.  </p>

<pre><code>rpart.cv.1 &lt;- train(Include ~ ., data = x, method = ""rpart"", trControl = cv.cntrl, tuneLength = 10).
</code></pre>

<blockquote>
  <p>Error in terms.formula(formula, data = data) :    duplicated name
  'document' in data frame using '.'</p>
</blockquote>
",Dataset Preprocessing & Handling,textmining r using qunateda package rpart joined stackoverflow find solution error running following code r throw error see pls help solve new text mining error term formula formula data data duplicated name document data frame using
Classify web pages with neural net?,"<p>So I am working on a java project and I'm have a webcrawler that retrieves and downloads the html from websites it finds. You input a word, such as the word Java, and my program then searches google for websites and downloads the html data.</p>

<p>So now I am trying to use a Neural Net to predict if the website is what I am looking for, yes I am aware that I am going to need a lot of data and have to manually feed the network. Where I am stuck is with converting the html into numbers for the neural net.</p>

<p>At first I though about just converting the html into plain text and then assign a number to each letter, and just have a document that is made up of a bunch of numbers that make up words. I am unsure though how a neural net would view this, would all these numbers be just one input or would each word be an input to the neural net, which then would cause the problem of having an unspecified number of inputs, and if the net allows this as one input would this be a viable way of classifying if a website is what I want or not?</p>
",Dataset Preprocessing & Handling,classify web page neural net working java project webcrawler retrieves downloads html website find input word word java program search google website downloads html data trying use neural net predict website looking yes aware going need lot data manually feed network stuck converting html number neural net first though converting html plain text assign number letter document made bunch number make word unsure though neural net would view would number one input would word input neural net would cause problem unspecified number input net allows one input would viable way classifying website want
How to turn rows of a dataframe into feature vectors?,"<p>So, I have a dataframe each rows of which represent some low-level user activity on a computer associated with a higher-level business process activity. The high-level business process activity is comprised of sequences of such low-level activities represented by each row. The data frame looks like this:
<a href=""https://i.sstatic.net/bmB5D.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bmB5D.png"" alt=""enter image description here""></a></p>

<p>So, it is a sequence classification problem where each sequence is identified by the case ID and each row represents the data point of the sequences. I need to train a model to predict the Business Process Activity that each sequence represents. </p>

<p>For this I need to transform each row of the dataframe into a feature vector but the problem is that the columns of the dataframe contain different information each row and some the data is numerical and some are textual (For example: the content inside the word document). I need to use of all the data for training. How do I convert these rows into feature vectors for training?</p>
",Dataset Preprocessing & Handling,turn row dataframe feature vector dataframe row represent low level user activity computer associated higher level business process activity high level business process activity comprised sequence low level activity represented row data frame look like sequence classification problem sequence identified case id row represents data point sequence need train model predict business process activity sequence represents need transform row dataframe feature vector problem column dataframe contain different information row data numerical textual example content inside word document need use data training convert row feature vector training
how to iterate through rows within single column of data frame?,"<p>I would like preform a function/iterate through all the rows of a single column of a data frame with 2 columns (id,address). Then write the parsed addresses to a new data-frame WITH the respective id's.</p>

<p>this is what I have thus far:</p>

<pre><code>addresses =pd.read_sql(query, conn)

# Create a list to hold results
results = []
# Go through each address in turn
for rowno,address in addresses.iterrows():
    clean_addresses = pyap.parse(addresses['address'], country='CA')
    results.append(clean_addresses)     
</code></pre>

<p>the example from the library is: </p>

<pre><code>test_address = """"""
   2000 BATH RD KINGSTON ON  
    """"""
addresses = pyap.parse(test_address, country='CA')
for address in addresses:
        # shows found address
        print(address)
        # shows address parts
        print(address.as_dict())
</code></pre>
",Dataset Preprocessing & Handling,iterate row within single column data frame would like preform function iterate row single column data frame column id address write parsed address new data frame respective id thus far example library
How to remove words from a data frame that are not in list in python,"<p>I am a beginner on working on non-English NLP, I want to clean all words in a data frame that are not contained in list <code>kata_dasar</code> :</p>

<p>My code is :</p>

<pre><code>df['tweet']= [' '.join(w for w in p.split() if w in kata_dasar) for p in df['tweet']]
</code></pre>

<p>But it is not working, Please help</p>
",Dataset Preprocessing & Handling,remove word data frame list python beginner working non english nlp want clean word data frame contained list code working please help
How to detect the language of a text (.csv) by its title using Python?,"<p>For a research-purpose work I should:</p>

<ol>
<li>Read a .csv file</li>
<li>Detect the language of the text by the title</li>
<li>Identifying the argument of the text by some keywords
ex. lobotomy --> brain</li>
</ol>

<p>I am trying to do the 2nd and 3rd point using Python with its library NLTK,
Could you give me some tips if you ever did something like it?</p>

<p>Thank you in advance!</p>
",Dataset Preprocessing & Handling,detect language text csv title using python research purpose work read csv file detect language text title identifying argument text keywords ex lobotomy brain trying nd rd point using python library nltk could give tip ever something like thank advance
How to merge sentiment analysis results (dfm) with original readtext object in Quanteda?,"<p>I have been using Quanteda's basic <code>tokens_lookup</code> function with the Young Soroka Sentiment Dictionary to count the number of positive and negative words in Tweets by politicians. </p>

<p>Once I get the results, is there a way I can then add these columns back into the original readtext object with the various docvars? </p>

<pre><code>head(dat)
readtext object consisting of 6 documents and 11 docvars.
# Description: df[,13] [6 × 13]
  doc_id   text       date      username   to      replies retweets favorites geo   mentions   hashtags        id permalink               
* &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;     &lt;int&gt;    &lt;int&gt;     &lt;int&gt; &lt;lgl&gt; &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;                   
1 trump.c… ""\""Sleepy… 2020-05-… realDonal… MZHemi…    5415    13062     39680 NA    @AjitPaiF… """"       1.84e-224 https://twitter.com/rea…
2 trump.c… ""\""He got… 2020-05-… realDonal… mikand…   20406    39081    111370 NA    """"         """"       1.84e-224 https://twitter.com/rea…
3 trump.c… ""\""Thank … 2020-05-… realDonal… mikand…    5733    17293     66992 NA    """"         """"       1.84e-224 https://twitter.com/rea…
4 trump.c… ""\"".@CBS … 2020-05-… realDonal… """"        22215    25834     93625 NA    @CBS @60M… """"       1.83e-224 https://twitter.com/rea…
5 trump.c… ""\""This b… 2020-05-… realDonal… GreggJ…    5379    11403     39869 NA    """"         """"       1.81e-224 https://twitter.com/rea…
6 trump.c… ""\""OBAMAG… 2020-05-… realDonal… """"        55960    89664    320171 NA    """"         """"       1.81e-224 https://twitter.com/rea…
&gt; corp &lt;- corpus(dat)
&gt; toks &lt;- tokens(corp, remove_punct = TRUE)
&gt; toks_lsd &lt;- tokens_lookup(toks, dictionary =  data_dictionary_LSD2015[1:2])
&gt; dfmat_lsd &lt;- dfm(toks_lsd)
&gt; head(dfmat_lsd)
Document-feature matrix of: 6 documents, 2 features (66.7% sparse).
6 x 2 sparse Matrix of class ""dfm""
             features
docs          negative positive
  trump.csv.1        2        0
  trump.csv.2        0        0
  trump.csv.3        0        1
  trump.csv.4        2        1
  trump.csv.5        0        0
  trump.csv.6        0        0
</code></pre>

<p>I've tried taking the required columns from the readtext object and making a new data.frame with them, which works okay, but it'd be great if I could instead merge the dfm results back into the other data.</p>
",Dataset Preprocessing & Handling,merge sentiment analysis result dfm original readtext object quanteda using quanteda basic function young soroka sentiment dictionary count number positive negative word tweet politician get result way add column back original readtext object various docvars tried taking required column readtext object making new data frame work okay great could instead merge dfm result back data
Python Loop: TypeError: string indices must be integers,"<p>For a current research project, I am planning to read the JSON object ""Main_Text"" within a pre-defined time range on basis of Python/Pandas. When running the word-counting loop, the code however yields the error <code>TypeError: string indices must be integers</code> for <code>line = row['Text Main']</code>.</p>

<p><code>Text Main</code> only contains strings/text and no integers. I have alreay been through trouble-shooting threads but not found a solution to this problem yet. Is there any helpful tweak to make this work?</p>

<p>The JSON file has the following structure:</p>

<pre><code>[
{""No"":""121"",""Stock Symbol"":""A"",""Date"":""05/11/2017"",""Text Main"":""Sample text""}
]
</code></pre>

<p>And the relevant code excerpt looks like this:</p>

<pre><code>import string
import json
import csv

import pandas as pd
import datetime

import numpy as np


# Loading and reading dataset
file = open(""Glassdoor_A.json"", ""r"")
data = json.load(file)
df = pd.json_normalize(data)
df['Date'] = pd.to_datetime(df['Date'])


# Create an empty dictionary
d = dict()


# Filtering by date
start_date = ""01/01/2009""
end_date = ""01/01/2015""

after_start_date = df[""Date""] &gt;= start_date
before_end_date = df[""Date""] &lt;= end_date

between_two_dates = after_start_date &amp; before_end_date
filtered_dates = df.loc[between_two_dates]

print(filtered_dates)


# Processing
for row in filtered_dates:
    line = row['Text Main']
</code></pre>
",Dataset Preprocessing & Handling,python loop typeerror string index must integer current research project planning read json object main text within pre defined time range basis python panda running word counting loop code however yield error contains string text integer alreay trouble shooting thread found solution problem yet helpful tweak make work json file ha following structure relevant code excerpt look like
Python: TypeError: unhashable type: &#39;list&#39; indices must be integers,"<p>For a current research project, I am planning to read the JSON object ""Main_Text"" within a pre-defined time range on basis of Python/Pandas. When counting the unique words, the code however yields the error <code>TypeError: unhashable type: 'list' indices must be integers</code> for line <code>if word in d:</code>.</p>

<p>I have alreay been through trouble-shooting threads and, among others, tried to set things up as a tuple (as recommended by some threads), which has overcome the error but led to an empty output. Is there any helpful tweak to make this work?</p>

<p>The JSON file has the following structure:</p>

<pre><code>[
{""No"":""121"",""Stock Symbol"":""A"",""Date"":""05/11/2017"",""Text Main"":""Sample text""}
]
</code></pre>

<p>And the relevant code excerpt looks like this:</p>

<pre><code>import string
import json
import csv

import pandas as pd
import datetime

import numpy as np


# Loading and reading dataset
file = open(""Glassdoor_A.json"", ""r"")
data = json.load(file)
df = pd.json_normalize(data)
df['Date'] = pd.to_datetime(df['Date'])


# Create an empty dictionary
d = dict()


# Filtering by date
start_date = ""01/01/2009""
end_date = ""01/01/2015""

after_start_date = df[""Date""] &gt;= start_date
before_end_date = df[""Date""] &lt;= end_date

between_two_dates = after_start_date &amp; before_end_date
filtered_dates = df.loc[between_two_dates]

print(filtered_dates)


# Processing
for row in filtered_dates:
    line = list(filtered_dates['Text Main'])
    # Remove the leading spaces and newline character

    line = [val.strip() for val in line]

    # Convert the characters in line to
    # lowercase to avoid case mismatch
    line = [val.lower() for val in line]

    # Remove the punctuation marks from the line
    line = [val.translate(val.maketrans("""", """", string.punctuation)) for val in line]

    # Split the line into words
    words = [val.split("" "") for val in line]

    # Iterate over each word in line
    for word in words:
        # Check if the word is already in dictionary
        if word in d:
            # Increment count of word by 1
            d[word] = d[word] + 1
        else:
            # Add the word to dictionary with count 1
            d[word] = 1


</code></pre>
",Dataset Preprocessing & Handling,python typeerror unhashable type list index must integer current research project planning read json object main text within pre defined time range basis python panda counting unique word code however yield error line alreay trouble shooting thread among others tried set thing tuple recommended thread ha overcome error led empty output helpful tweak make work json file ha following structure relevant code excerpt look like
How to find similarity between two text columns in a csv,"<p>I have two text columns in a csv file with 4000 records.I have to perform text similairty between two text columns.How can I do it?</p>
",Dataset Preprocessing & Handling,find similarity two text column csv two text column csv file record perform text similairty two text column
How would you implement model combination when using sub tasks and two different datasets with sklearn,"<p>How could you use model persistence for sub tasks when using two different datasets? I created a copy of the original, and substituted 3 labels in my target column for another label. </p>

<p>For instance, I have a NLP multi-classification problem, where I need to classify the x  as 4 diffferent labels like 1, 2, 3, or 4. 1, 2, 3 labels are related, and their labels can be substituted as 5 so that it's now a binary classification problem. Now, I only need to differentiate between 4 and 5, but I'm still left with the classification between 1, 2, 3, which I'm not too sure how to use the initial classification (4 and 5 binary classification model) to help in the second model (classifying 1,2,3). I can't find any information if sci-kit learn allows this like Keras does. Thanks for any suggestions.</p>

<p>Currently, I'm using SGDClassifier and getting 90% accuracy on classifying between 4 and 5, but not sure how to carry it over for the multi classification of 1, 2, and 3. Is this feature not in sklearn? </p>

<p>I would assume it's similar to Keras. </p>

<hr>

<p><strong>Edit:</strong> 
After further research, I think it I would be using 'stacking models.' However, I'm having conceptual issues understanding how the second model is going to 'know' which labels (1,2,3) are substituted for 5?</p>

<p>One issue I'm running into is that, ensemble learners require that they are trained on the same data set. Mine is the same, except that it's not since I've subbed labels 1,2,3 for 5 so, it only has 4, 5 as labels. What would this approach lead to? </p>

<hr>

<p><strong>Edit2:</strong>
Each sample can only be labelled as one class.
A. The original target column contains (1,2,3, and 4).
B. The adjusted target column contains (4, and 5), where 5 represents 1, 2, and 3</p>

<p>A and B are two different CSV files, and the only difference is what is stated above, and both have the same features. </p>

<p>Fitting <code>SGDClassifier()</code> for each fold (10 folds):</p>

<p><code>X_Train</code> = about (30000, 54) (vertically stacked)
-- <code>y_train</code> = ~30000</p>

<p>Testing on:</p>

<p><code>X_test</code> = (2000 to 4000, 54)
-- <code>y_test</code> = (2000 to 4000)</p>

<p>How it is now:</p>

<pre><code>clf1 = SGDClassifier(max_iter=1000, tol=1e-3)
clf1.fit(X_train, y_train)
predicted = [labels[int(a)] for a in clf1.predict(X_test)]
actual = [labels[int(a)] for a in y_test]
</code></pre>
",Dataset Preprocessing & Handling,would implement model combination using sub task two different datasets sklearn could use model persistence sub task using two different datasets created copy original substituted label target column another label instance nlp multi classification problem need classify x diffferent label like label related label substituted binary classification problem need differentiate still left classification sure use initial classification binary classification model help second model classifying find information sci kit learn allows like kera doe thanks suggestion currently using sgdclassifier getting accuracy classifying sure carry multi classification feature sklearn would assume similar kera edit research think would using stacking model however conceptual issue understanding second model going know label substituted one issue running ensemble learner require trained data set mine except since subbed label ha label would approach lead edit sample labelled one class original target column contains b adjusted target column contains represents b two different csv file difference stated feature fitting fold fold vertically stacked testing
Need helping performing sentiment analysis on customer reviews and for a string of text,"<p>This is a 2 part code question.</p>

<p>1.) Need to perform sentiment analysis on a csv file for customer reviews.</p>

<p>2.) Need to perform sentiment analysis on a harry potter book review saved as a .txt</p>

<p>1.) The name of this Dataframe is ""reviews"" and what I want to do is display the sentiment score for each of these 5 reviews under the ""sent"" column.  Thank you so much!!! If you can provide the code with the ""sent"" column filled  with its sentiment analysis score for each row that would be awesome!!</p>

<p>reviews.head()</p>

<pre><code> ID  Customer Name    Review                                       Sent  

 1   Jack             Beautiful cover up. My only 
                      feedback is that it is a tad larger 
                      than expected, but since it's a cover 
                      up, it doesn't need to be fitted. The 
                      waist tassels also allow you to adjust 
                      to fit your waist which is nice. 
                      Otherwise, its exactly as expected!

 2   Rachel           This tunic is very cute in person. It's 
                      more sheer than I'd like, but I imagine 
                      I'll wear it a ton on vacation.

 3   Ryan             Just got this sweet little dress in 
                      blue. It's a great little dress for a 
                      pool cover up. I can envision myself 
                      wearing it on our winter getaway for 
                      breakfast or on a walk. I'm not sure how 
                      see through it is. I think I could get 
                      away wearing it as a dress. The length 
                      is great, not too short. The quality is 
                      great. I got a size S. Fits true to size. 
                      I am usually a size 2, 34b, 129lb, slim 
                      build. Very happy with this.

 4   Jennifer         Love this hat! Kept the sun off my face 
                      and neck/chest in the intense tropical 
                      sun! Choose white - so I stayed cool.

 5   Alex             What I like about bikinis is that they 
                      always fit you perfectly. You won't 
                      realize how gorgeous they are and how 
                      attractive they make your body look 
                      until you put one on. As for the bra-part 
                      it gives good support and sits well. I 
                      also like the fabric: it stretches well 
                      without losing its shape, the color 
                      doesn't fade. This bikini is no exception. 
                      is far better at making bikinis than 
                      anybody else, I would say! 
</code></pre>

<p>For this string I just to know what the overall sentiment score is... thanks!!</p>

<p>2.) </p>

<p>""Parents need to know that Harry Potter and the Sorcerer's Stone is a thrill-a-minute story, the first in J.K. Rowling's Harry Potter series. It respects kids' intelligence and motivates them to tackle its greater length and complexity, play imaginative games, and try to solve its logic puzzles. It's the lightest in the series, but it still has some scary stuff for sensitive readers: a three-headed dog, an attacking troll, a violent life-size chess board, a hooded figure over a dead and bleeding unicorn, as well as a discussion of how Harry's parents died years ago.""</p>
",Dataset Preprocessing & Handling,need helping performing sentiment analysis customer review string text part code question need perform sentiment analysis csv file customer review need perform sentiment analysis harry potter book review saved txt name dataframe review want display sentiment score review sent column thank much provide code sent column filled sentiment analysis score row would awesome review head string know overall sentiment score thanks parent need know harry potter sorcerer stone thrill minute story first j k rowling harry potter series respect kid intelligence motivates tackle greater length complexity play imaginative game try solve logic puzzle lightest series still ha scary stuff sensitive reader three headed dog attacking troll violent life size chess board hooded figure dead bleeding unicorn well discussion harry parent died year ago
How to clean HTML string to parse it in python using lxml?,"<p>I have a python string that has HTML code in it, coming from JSON that I want to parse using lxml library. The string has several escape characters and other special characters. How to clean this code so that I can extract information from it using lxml? I want to use the XPATH selectros on the string.</p>

<p>String-</p>

<pre><code>&lt;!DOCTYPE html PUBLIC \""-//W3C//DTD XHTML 1.0 Transitional//EN\"" \""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\""&gt;\r\n&lt;html&gt;\r\n\r\n&lt;head&gt;\r\n    &lt;META http-equiv=\""Content-Type\"" content=\""text/html; charset=utf-8\""&gt;\r\n&lt;/head&gt;\r\n\r\n&lt;body&gt;\r\n\r\n&lt;div&gt;\r\n    &lt;table width=\""640\"" align=\""center\"" border=\""0\"" cellspacing=\""0\"" cellpadding=\""0\"" bgcolor=\""#ffffff\"" style=\""font-family:Arial,Helvetica,sans-serif;font-size:14px\""&gt;\r\n        &lt;tr&gt;\r\n            &lt;td align=\""center\""&gt;\r\n\r\n                &lt;table align=\""center\"" border=\""0\"" cellspacing=\""0\"" cellpadding=\""0\"" style=\""max-width:600px;text-align:left\""&gt;\r\n                    &lt;tr&gt;\r\n                        &lt;td width=\""600\""&gt;\r\n                            &lt;table align=\""center\"" border=\""0\"" cellspacing=\""0\"" cellpadding=\""0\"" width=\""600\""&gt;\r\n                                &lt;tr&gt;\r\n                                    &lt;td height=\""10\""&gt;&lt;/td&gt;\r\n                                &lt;/tr&gt;\r\n                                &lt;tr&gt;\r\n                                    &lt;td align=\""center\""&gt;\r\n                                        &lt;a href=\""#0.1_\""&gt;&lt;img src=\""https://ns.yatracdn.com/common/images/emailers/corp-flight-hotel/yatra-logo.png\"" width=\""101\"" height=\""45\"" alt=\""Yatra.com\"" title=\""Yatra.com\"" border=\""0\"" style=\""font-family:Arial,Helvetica,sans-serif;font-size:25px;color:#ea2330\"" vspace=\""0\"" hspace=\""0\"" align=\""center\""&gt;&lt;/a&gt;\r\n                                    &lt;/td&gt;\r\n                                &lt;/tr&gt;\r\n                                &lt;tr&gt;\r\n                                    &lt;td height=\""10\""&gt;&lt;/td&gt;\r\n                                &lt;/tr&gt;\r\n                                &lt;tr&gt;\r\n                                    &lt;td&gt;\r\n                                        &lt;table border=\""0\"" cellspacing=\""0\"" cellpadding=\""0\"" width=\""600\"" style=\""border:1px solid #d8d8d8\""&gt;\r\n                                            &lt;tr&gt;\r\n                                                &lt;td height=\""10\""&gt;&lt;/td&gt;\r\n                                            &lt;/tr&gt;\r\n                                            &lt;tr&gt;\r\n                                                &lt;td width=\""10\""&gt;&lt;/td&gt;\r\n                                                &lt;td colspan=\""3\""&gt;&lt;b&gt;Travel Request Details&lt;/b&gt;&lt;/td&gt;\r\n                                            &lt;/tr&gt;\r\n                                            &lt;tr&gt;\r\n                                                &lt;td height=\""10\""&gt;&lt;/td&gt;\r\n                                            &lt;/tr&gt;\r\n                                            &lt;tr&gt;\r\n                                                &lt;td width=\""10\""&gt;&lt;/td&gt;\r\n                                                &lt;td&gt;\r\n                                                    &lt;table width=\""100%\"" border=\""0\"" cellspacing=\""0\"" cellpadding=\""0\"" bgcolor=\""#ffffff\"" style=\""border:1px solid #d8d8d8\""&gt;\r\n                                                        &lt;tbody&gt;\r\n                                                        &lt;tr&gt;\r\n                                                            &lt;td width=\""10\""&gt;&lt;/td&gt;\r\n                                                            &lt;td&gt;\r\n                                                                &lt;table width=\""100%\"" border=\""0\"" cellspacing=\""0\"" cellpadding=\""0\"" bgcolor=\""#ffffff\""&gt;\r\n                                                                    &lt;tr&gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;\r\n                                                                            Email Verification Date / Time &lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;12 May 2020 17:14&lt;/td&gt;\r\n                                                                    &lt;/tr id='aaaaa'&gt;\r\n                                                                    &lt;tr&gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;\r\n                                                                            Request Submission Date / Time &lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;12 May 2020 17:14&lt;/td&gt;\r\n                                                                    &lt;/tr&gt;\r\n                                                                    &lt;tr&gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;\r\n                                                                            Product &lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;Flight&lt;/td&gt;\r\n                                                                    &lt;/tr&gt;\r\n                                                                    &lt;tr&gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;\r\n                                                                            Journey Type &lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;One way&lt;/td&gt;\r\n                                                                    &lt;/tr&gt;\r\n\r\n                                                                    &lt;tr&gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;\r\n                                                                            Adult &lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;1&lt;/td&gt;\r\n                                                                    &lt;/tr&gt;\r\n                                                                    &lt;tr&gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;\r\n                                                                            Child &lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;0&lt;/td&gt;\r\n                                                                    &lt;/tr&gt;\r\n                                                                    &lt;tr&gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;\r\n                                                                            Infant &lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;0&lt;/td&gt;\r\n                                                                    &lt;/tr&gt;\r\n                                                                    &lt;tr&gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;\r\n                                                                            Flight Class &lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;Travel Class&lt;/td&gt;\r\n                                                                    &lt;/tr&gt;\r\n                                                                    &lt;tr&gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;\r\n                                                                            Preferred Airline &lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;\r\n                                                                            &lt;/td&gt;\r\n                                                                    &lt;/tr&gt;\r\n                                                                    &lt;tr&gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;\r\n                                                                            Non Stop Flight &lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;Preferred Airline&lt;/td&gt;\r\n                                                                    &lt;/tr&gt;\r\n                                                                    &lt;tr&gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;\r\n                                                                            Traveller Email &lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;ankityadav56@demo.com&lt;/td&gt;\r\n                                                                    &lt;/tr&gt;\r\n                                                                    &lt;tr&gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;\r\n                                                                            Traveller Mobile&lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;9971255462&lt;/td&gt;\r\n                                                                    &lt;/tr&gt;\r\n                                                                    &lt;tr&gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;\r\n                                                                            Travel Policy Email&lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;Corporate.traveler@yatra.com&lt;/td&gt;\r\n                                                                    &lt;/tr&gt;\r\n\r\n                                                                    &lt;tr &gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;Origin&lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;New Delhi(DEL)&lt;/td&gt;\r\n                                                                    &lt;/tr&gt;\r\n\r\n                                                                    &lt;tr &gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;Destination&lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;Mumbai(BOM)&lt;/td&gt;\r\n                                                                    &lt;/tr&gt;\r\n\r\n                                                                    &lt;tr &gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;Depart Date&lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;26 Jun 2020&lt;/td&gt;\r\n                                                                    &lt;/tr&gt;\r\n\r\n                                                                    &lt;tr &gt;\r\n                                                                        &lt;td height=\""35\"" valign=\""middle\"" align=\""left\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;Preferred Time From&lt;/td&gt;\r\n                                                                        &lt;td colspan=\""2\"" align=\""right\"" style=\""border-bottom:1px solid #d8d8d8\""&gt;00:23&lt;/td&gt;\r\n                                                                    &lt;/tr&gt;\r\n\r\n                                                                &lt;/table&gt;\r\n                                                            &lt;/td&gt;\r\n                                                            &lt;td width=\""10\""&gt;&lt;/td&gt;\r\n                                                        &lt;/tr&gt;\r\n\r\n                                                        &lt;/tbody&gt;\r\n                                                    &lt;/table&gt;\r\n\r\n                                                &lt;/td&gt;\r\n                                                &lt;td width=\""10\""&gt;&lt;/td&gt;\r\n                                            &lt;/tr&gt;\r\n\r\n                                            &lt;tr&gt;\r\n                                                &lt;td height=\""10\""&gt;&lt;/td&gt;\r\n                                            &lt;/tr&gt;\r\n                                        &lt;/table&gt;\r\n\r\n                                    &lt;/td&gt;\r\n                                &lt;/tr&gt;\r\n                            &lt;/table&gt;\r\n                        &lt;/td&gt;\r\n                    &lt;/tr&gt;\r\n                &lt;/table&gt;\r\n            &lt;/td&gt;\r\n        &lt;/tr&gt;\r\n    &lt;/table&gt;\r\n\r\n&lt;/div&gt;\r\n\r\n&lt;/body&gt;\r\n\r\n&lt;/html&gt;
</code></pre>

<p>With clean string the parser works like this-</p>

<pre><code>&gt;&gt;&gt; broken_html = ""&lt;html&gt;&lt;head&gt;&lt;title&gt;test&lt;body&gt;&lt;h1&gt;page title&lt;/h3&gt;""

&gt;&gt;&gt; parser = etree.HTMLParser()
&gt;&gt;&gt; tree   = etree.parse(StringIO(broken_html), parser)

&gt;&gt;&gt; result = etree.tostring(tree.getroot(),
...                         pretty_print=True, method=""html"")
&gt;&gt;&gt; print(result)
&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;test&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;h1&gt;page title&lt;/h1&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>
",Dataset Preprocessing & Handling,clean html string parse python using lxml python string ha html code coming json want parse using lxml library string ha several escape character special character clean code extract information using lxml want use xpath selectros string string clean string parser work like
How to replace special charachters in Pyspark?,"<p>I am fairly new to Pyspark, and I am trying to do some text pre-processing with Pyspark.
I have a column <code>Name</code> and <code>ZipCode</code> that belongs to a spark data frame <code>new_df</code>. The column 'Name' contains values like <code>WILLY:S MALMÖ, EMPORIA</code> and <code>ZipCode</code> contains values like <code>123 45</code> which is a string too. what I want to do is I want to remove characters like <code>:</code>, <code>,</code> etc and want to remove space between the <code>ZipCode</code>.
I tried the following but nothing seems to work :</p>

<pre><code>new_df = new_df.withColumn('Name', sfn.regexp_replace('Name', r',' , ' '))
new_df = new_df.withColumn('ZipCode', sfn.regexp_replace('ZipCode', r' ' , ''))
</code></pre>

<p>I tried other things too from the SO and other websites. Nothing seems to work.</p>
",Dataset Preprocessing & Handling,replace special charachters pyspark fairly new pyspark trying text pre processing pyspark column belongs spark data frame column name contains value like contains value like string want want remove character like etc want remove space tried following nothing seems work tried thing website nothing seems work
How can I apply histwords to my own corpus of text?,"<p>I recently come across this paper ( <a href=""https://arxiv.org/pdf/1605.09096.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1605.09096.pdf</a> ) and I have been Reading through the GitHub ( <a href=""https://github.com/williamleif/histwords"" rel=""nofollow noreferrer"">https://github.com/williamleif/histwords</a> ) but it is still not so clear to me how I can apply it to my own data. My data comes in the format of the following:</p>

<pre><code>#### 2008
   text_2008 = pd.DataFrame({'dat1': [""I love machine learning in 2008. Its awesome."",
            ""I love coding in Python in 2008"",
            ""I love building chatbots in 2008"",
            ""they chat amagingly well""]})
    ID_2008 = pd.DataFrame({'dat2': [1,2,3,4]})

    my_actual_data_format_2008 = text.join(ID)

#### 2009
   text_2009 = pd.DataFrame({'dat1': [""I love machine learning. Its awesome."",
            ""I love coding in Python"",
            ""I love building chatbots"",
            ""they chat amagingly well""]})
    ID_2009 = pd.DataFrame({'dat2': [1,2,3,4]})

    my_actual_data_format_2009 = text.join(ID)


#### 2010
   text_2010 = pd.DataFrame({'dat1': [""I love machine learning more in 2010. Its awesome."",
            ""I love coding in Python in 2010"",
            ""I love building chatbots in 2010"",
            ""they chat amagingly well""]})
    ID_2010 = pd.DataFrame({'dat2': [1,2,3,4]})

    my_actual_data_format_2010 = text.join(ID)
</code></pre>

<p>So I have multiple pandas data frames with each row containing an <code>ID</code> and <code>text</code> column.</p>

<p>From my understanding, the sgns takes .txt files not pandas data frames. ( <a href=""https://github.com/williamleif/histwords/tree/master/sgns"" rel=""nofollow noreferrer"">https://github.com/williamleif/histwords/tree/master/sgns</a> ) </p>

<p>From the homepage it states ""If you want to learn historical embeddings for new data, the code in the sgns directory is recommended""</p>

<p>If somebody can give me a push in the right direction, that would be awesome! Should I save my pandas rows ""text"" as .txt files?</p>
",Dataset Preprocessing & Handling,apply histwords corpus text recently come across paper reading github still clear apply data data come format following multiple panda data frame row containing column understanding sgns take txt file panda data frame homepage state want learn historical embeddings new data code sgns directory recommended somebody give push right direction would awesome save panda row text txt file
How to extract age and gender of the person from unprocessed text/data?,"<p>i have a CSV file with list of texts(column with rows), and i want to extract the ages of the patients from the each row, i can't do with ""is digit"" cuz there are also some other digits in the texts. how can i do such thing? Thank You</p>

<p>EXTRA: i want to extract the genders too - Patient sometimes is referred as male/female, sometimes as man/woman and sometimes as gentleman/lady.</p>

<p>Is there a method to write the findall for example if the text is 17-year-old print me the number if it is followed by -year-old </p>

<pre><code>re.findall(""[\d]."", '-year-old')
</code></pre>

<p>Sample of lines from text:</p>

<pre><code>This 23-year-old white female presents with...

...pleasant gentleman who is 42 years old...

...The patient is a 10-1/2-year-old born with...

...A 79-year-old Filipino woman...

Patient, 37,...
</code></pre>

<p>How can i have a list of age/gender</p>

<p>i.e.:</p>

<pre><code>Age:

    ['23','42','79','37'...]

Gender:

    ['female','male','male','female','male'...]
</code></pre>
",Dataset Preprocessing & Handling,extract age gender person unprocessed text data csv file list text column row want extract age patient row digit cuz also digit text thing thank extra want extract gender patient sometimes referred male female sometimes man woman sometimes gentleman lady method write findall example text year old print number followed year old sample line text list age gender e
can i use VaderSentiment to calculate polarity and subjectivity on language other than English?,"<p>i am trying to create a nlp project that calculate the polarity and subjectivity for texts that are not English so i can use 2 tools: <strong>Vader</strong> - <strong>Textblob</strong>.</p>

<p>After i done a lot of researches i found that Vader is more efficient and accurate for social media.</p>

<p>My question is : can i add language to vader in order to calculate socres?
 or is their a package for vader like multilanguage?</p>

<p>For the project i read from csv file and import it to dataframe pandas than pre-process and clean the text  than analyse it to extract the sentiments. </p>

<p>i will appreciate any help.</p>
",Dataset Preprocessing & Handling,use vadersentiment calculate polarity subjectivity language english trying create nlp project calculate polarity subjectivity text english use tool vader textblob done lot research found vader efficient accurate social medium question add language vader order calculate socres package vader like multilanguage project read csv file import dataframe panda pre process clean text analyse extract sentiment appreciate help
Cleaning text Data For NLP tasks,"<p>This morning i've been trying to train a chatbot on the Cornell Movie--Dialogs Corpus Dataset  but i'm facing problems cleaning the text data to feed into my Algorithm.
Here is snippet from the text file</p>

<blockquote>
  <p></p>
</blockquote>

<pre><code>L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!
L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!
L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.
L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?
L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.
</code></pre>

<p>L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow
I am only interested in the dialogs at last part of each sentence.
How can i clean this file and make it a csv document?</p>

<p>Dataset Link
<a href=""http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html"" rel=""nofollow noreferrer"">http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html</a></p>
",Dataset Preprocessing & Handling,cleaning text data nlp task morning trying train chatbot cornell movie dialog corpus dataset facing problem cleaning text data feed algorithm snippet text file l u cameron wow interested dialog last part sentence clean file make csv document dataset link
How to count paragraphs from each article from dataframe?,"<p>I want to count paragraphs from data frames. However, it turns out that my result gets zero inside the list. Does anybody know how to fix it? Thank you so much.</p>

<p>Here is my code:</p>

<pre><code>def count_paragraphs(df):
    paragraph_count = []
    linecount = 0
    for i in df.text:
        if i in ('\n','\r\n'):
            if linecount == 0:
                paragraphcount = paragraphcount + 1
    return paragraph_count

count_paragraphs(df)
</code></pre>

<p>df.text</p>

<pre><code>0     On Saturday, September 17 at 8:30 pm EST, an e...
1     Story highlights ""This, though, is certain: to...
2     Critical Counties is a CNN series exploring 11...
3     McCain Criticized Trump for Arpaio’s Pardon… S...
4     Story highlights Obams reaffirms US commitment...
5     Obama weighs in on the debate\n\nPresident Bar...
6     Story highlights Ted Cruz refused to endorse T...
7     Last week I wrote an article titled “Donald Tr...
8     Story highlights Trump has 45%, Clinton 42% an...
9     Less than a day after protests over the police...
10    I woke up this morning to find a variation of ...
11    Thanks in part to the declassification of Defe...
12    The Democrats are using an intimidation tactic...
13    Dolly Kyle has written a scathing “tell all” b...
14    The Haitians in the audience have some newswor...
15    The man arrested Monday in connection with the...
16    Back when the news first broke about the pay-t...
17    Chicago Environmentalist Scumbags\n\nLeftists ...
18    Well THAT’S Weird. If the Birther movement is ...
19    Former President Bill Clinton and his Clinton ...
Name: text, dtype: object
</code></pre>
",Dataset Preprocessing & Handling,count paragraph article dataframe want count paragraph data frame however turn result get zero inside list doe anybody know fix thank much code df text
How to clean up messy &quot;Country&quot; attribute from biopython pubmed extracts?,"<p>I have extracted ~60,000 PubMed abstracts into a data frame using <a href=""https://biopython.org/"" rel=""nofollow noreferrer"">Biopython</a>. The attributes include ""Authors"", ""Title"", ""Year"", ""Journal"", ""Country"", and ""Abstract"". 
The attribute ""Country"" is very messy, with a mixture of countries, cities, names, addresses, free-text items (e.g., ""freelance journalist with interest in Norwegian science""), faculties, etc.
I want to clean up the column only to contain the country - and ""NA"" for those records that are missing the entry, or have a free-text item that does not make sense.</p>

<p>Currently, my clean-up process of this column is very cumbersome:</p>

<pre><code>pub = df['Country']
chicago = pub.str.contains('Chicago')
df['Country'] = np.where(chicago, 'USA', pub.str.replace('-', ' '))
au = pub.str.contains('@edu.au')
df['Country'] = np.where(au, 'Australia', pub.str.replace('-', ' '))
... and so on
</code></pre>

<p>Are you aware of some python libraries, or have some ideas for a more automated way of cleaning up this column? </p>
",Dataset Preprocessing & Handling,clean messy country attribute biopython pubmed extract extracted pubmed abstract data frame using biopython attribute include author title year journal country abstract attribute country messy mixture country city name address free text item e g freelance journalist interest norwegian science faculty etc want clean column contain country na record missing entry free text item doe make sense currently clean process column cumbersome aware python library idea automated way cleaning column
How to determine classes with Keras?,"<p>I am working on a way to classify mail by using Keras. I read the mail that have already been classified, tokenize them to create a dictionary which is link to a folder.</p>

<p>So I created a dataframe with pandas:</p>

<pre><code>data = pd.DataFrame(list(zip(lst, lst2)), columns=['text', 'folder'])
</code></pre>

<p>The text column is where reside all the words present in an email and the folder column is the class (the path) that the email belongs to.</p>

<p>Thanks to that I created my model which gives me those results:</p>

<blockquote>
  <p>3018/3018 [==============================] - 0s 74us/step - loss: 0.0325 - acc: 0.9950 - val_loss: 0.0317 - val_acc: 0.9950</p>
</blockquote>

<p>On 100 Epoch</p>

<p>The evaluation of my model</p>

<blockquote>
  <p>755/755 [==============================] - 0s 28us/step
  Test score: 0.0316697002592071
  Test accuracy: 0.995000006268356</p>
</blockquote>

<p>So the last that I need to do is predict the class of a random mail but the <code>model.predict_classes(numpy.array)</code> call gives me a 2D array full of integer but I still don't know to which ""folder/class"" it belongs. </p>

<p>Here is my code:</p>

<pre><code>#lst contains all the words in the mail
#lst2 the class/path of lst
data = pd.DataFrame(list(zip(lst, lst2)), columns=['text', 'folder'])

train_size = int(len(data) * .8)
train_posts = data['text'][:train_size]
train_tags = data['folder'][:train_size]

test_posts = data['text'][train_size:]
test_tags = data['folder'][train_size:]

num_labels = 200 #The numbers of total classes

#the way I tokenize and encode my data
tokenizer = Tokenizer(num_words=len(lst))
tokenizer.fit_on_texts(pd.concat([train_posts, test_posts], axis = 1))

x_train = tokenizer.texts_to_matrix(train_posts, mode=TOKENISER_MODE)
x_test = tokenizer.texts_to_matrix(test_posts, mode=TOKENISER_MODE)

encoder = preprocessing.LabelBinarizer()
encoder.fit(train_tags)
y_train = encoder.transform(train_tags)
y_test = encoder.transform(test_tags)

#my model, vocab_size = len(lst) = number of the words present in the mails
model = Sequential()
model.add(Dense(16, input_shape=(vocab_size,)))
model.add(Activation('elu'))
model.add(Dropout(0.2))
model.add(Dense(32))
model.add(Activation('elu'))
model.add(Dropout(0.2))
model.add(Dense(16))
model.add(Activation('elu'))
model.add(Dropout(0.2))
model.add(Dense(num_labels))
model.add(Activation('sigmoid'))
model.summary()

#compile training and evaluate
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(x_train, y_train, batch_size=batch_size, epochs=100, verbose=1, validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)
print('Test score:', score[0])
print('Test accuracy:', score[1])

#read the random file
sentences = read_files(""mail.eml"")
sentences = ' '.join(sentences)
sentences = sentences.lower()
salut = unidecode.unidecode(sentences)

#predict
pred = model.predict_classes(salut, batch_size=batch_size, verbose=1)
print(pred)
</code></pre>

<p>The actual output of <code>pred</code>:</p>

<blockquote>
  <p>[125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125
   125 125 125]</p>
</blockquote>

<p>I don't why but the output each time I launch it is always full of the same number. And the output I am looking for is:</p>

<blockquote>
  <p>['medecine/AIDS/', help/, project/classification/]</p>
</blockquote>

<p>sorted by probabilities of being the right one. The <code>read_files</code> call is just a function that read the mail and return a list of all the words present in the mail.</p>

<p>Is there a way to obtain the class of the mail with <code>model.predict_classes()</code> or do I need to use something else?</p>
",Dataset Preprocessing & Handling,determine class kera working way classify mail using kera read mail already classified tokenize create dictionary link folder created dataframe panda text column word present email folder column class path email belongs thanks created model give result u step loss acc val loss val acc epoch evaluation model u step test score test accuracy last need predict class random mail call give array full integer still know folder class belongs code actual output output time launch always full number output looking medecine aid help project classification sorted probability right one call function read mail return list word present mail way obtain class mail need use something else
Problem in coding a Welcome Message along with options in RASA,"<p>I read <a href=""https://forum.rasa.com/t/welcome-message-in-rasa-x/13374/6?u=gadia-aayush"" rel=""nofollow noreferrer"">this answer</a> on How to code a Welcome Message in RASA, accordingly, I did write a custom action but it is not displaying the message as soon as the session starts, instead, it replies after the user has sent a message. <strong>Below is my code for printing just the welcome message. I had put this in my ""actions.py"" file. Please help me to fix this problem.</strong></p>

<p>The image below is an example of How I want my bot to start, It would start up with a general message and then it would give options which the user would be choosing. This is what I am trying to achieve ultimately.</p>

<p><a href=""https://i.sstatic.net/69FPT.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/69FPT.jpg"" alt=""enter image description here""></a></p>

<pre><code>from typing import Text, List, Dict, Any

from rasa_sdk import Action, Tracker
from rasa_sdk.events import SlotSet, SessionStarted, ActionExecuted, EventType
from rasa_sdk.executor import CollectingDispatcher


class ActionSessionStart(Action):
    def name(self) -&gt; Text:
        return ""action_session_start""

    @staticmethod
    def fetch_slots(dispatcher: CollectingDispatcher, tracker: Tracker) -&gt; List[EventType]:
        """"""Collect slots that contain the user's name and phone number.""""""

        slots = []
        return slots        


    async def run(
        self,
        dispatcher: CollectingDispatcher,
        tracker: Tracker,
        domain: Dict[Text, Any],
    ) -&gt; List[EventType]:

        # the session should begin with a `session_started` event
        dispatcher.utter_message(""Hi, I am Aayush Bot !!!"")
        events = [SessionStarted()]

        # any slots that should be carried over should come after the
        # `session_started` event
        events.extend(self.fetch_slots(dispatcher, tracker))

        # an `action_listen` should be added at the end as a user message follows
        events.append(ActionExecuted(""action_listen""))

        return events
</code></pre>
",Dataset Preprocessing & Handling,problem coding welcome message along option rasa read answer code welcome message rasa accordingly write custom action displaying message soon session start instead reply user ha sent message code printing welcome message put action py file please help fix problem image example want bot start would start general message would give option user would choosing trying achieve ultimately
How to save and load google NLP reformer model,"<p>I´m working with the recent NLP model from <a href=""https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html"" rel=""nofollow noreferrer"">Google</a> </p>

<p>I have read a few post but mostly I´m plying over the colab <a href=""https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb#scrollTo=bQbQhqkK0cSH"" rel=""nofollow noreferrer"">example</a> Which have all the model ceations steps and testing function. The problem I have at now is that since the model takes a long time to train even using the google TPUs I need to save the the trained model, my guess is that it works similarly as the GPT-2 model in the sense that the model can be trainined over several sessions since it allows to stop training at any moment:</p>

<pre><code>This will take at least 30 minutes to run to completion, but can safely
# be interrupted by selecting ""Runtime &gt; Interrupt Execution"" 
</code></pre>

<p>But i I have not found an example on how to save and load the model once trained. In case of GPT-2 a new directory was created automatically for each new model, and to use it it was necessary only point to that new directory, but for this one I´m not finding how to load a previously trained model. </p>

<p>EDIT: </p>

<p>In the notebook I saw this code: </p>

<pre><code># Set up a Trainer.
output_dir = os.path.expanduser('~/train_dir/')
!rm -f ~/train_dir/model.pkl  # Remove old model
trainer = trax.supervised.Trainer(
    model=trax.models.ReformerLM,
    loss_fn=trax.layers.CrossEntropyLoss,
    optimizer=trax.optimizers.Adam,
    lr_schedule=trax.lr.MultifactorSchedule,
    inputs=trax.supervised.inputs.Inputs(my_inputs),
    output_dir=output_dir,
    has_weights=True)
</code></pre>

<p>Which is deleteing the previous model, I looked into that directory I found this: </p>

<p><a href=""https://i.sstatic.net/98RQK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/98RQK.png"" alt=""enter image description here""></a></p>

<p>I used pickle to load this model.pkl file, which I also copied to my Gdrive folder: </p>

<pre><code>with open('model.pkl', 'rb') as handle:
    reformer_model = pickle.load(handle)

reformer_model
</code></pre>

<p>But this is just a dictionary with the weigths, not a model to use directly: </p>

<p><a href=""https://i.sstatic.net/Wg65n.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Wg65n.png"" alt=""enter image description here""></a></p>
",Dataset Preprocessing & Handling,save load google nlp reformer model working recent nlp model google read post mostly plying colab example model ceations step testing function problem since model take long time train even using google tpus need save trained model guess work similarly gpt model sense model trainined several session since allows stop training moment found example save load model trained case gpt new directory wa created automatically new model use wa necessary point new directory one finding load previously trained model edit notebook saw code deleteing previous model looked directory found used pickle load model pkl file also copied gdrive folder dictionary weigths model use directly
what is the accurate Twitter sentiment analysis solution with Python?,"<p>I have a CSV file of 20K tweets with all information such as location, username, and date which I want to assign a label positive/neutral/negative to each tweet by Python. 
I used the following Python code from textblob library for <strong>Tweets Sentiment Analysis</strong>. </p>

<pre><code>import csv
from textblob import TextBlob
import sys

# Do some version specific stuff
if sys.version[0] == '3':
    from importlib import reload
    sntTweets = csv.writer(open(""sentimentTweets.csv"", ""w"", newline=''))

if sys.version[0] == '2':
    reload(sys)
    sys.setdefaultencoding(""utf-8"")
    sntTweets = csv.writer(open(""sentimentTweets.csv"", ""w""))

alltweets = csv.reader(open(""Corona.csv"", 'r'))

for row in alltweets:
    blob = TextBlob(row[2])
    print (blob.sentiment.polarity)
    if blob.sentiment.polarity &gt; 0:
        sntTweets.writerow([row[0], row[1], row[2], row[3], blob.sentiment.polarity, ""positive""])
    elif blob.sentiment.polarity &lt; 0:
        sntTweets.writerow([row[0], row[1], row[2], row[3], blob.sentiment.polarity, ""negative""])
    elif blob.sentiment.polarity == 0.0:
        sntTweets.writerow([row[0], row[1], row[2], row[3], blob.sentiment.polarity, ""neutral""])
</code></pre>

<p>this code runs perfect and produces the sentimentTweets.csv file. I like the idea that for each tweet, it gives me two labels: a number between -1 and 1, and also classify tweet to negative/neutral/positive.</p>

<p>but it is not accurate. for example for the following tweet, it assigns positive with the number:0.285714285714285.
<em>""RT @eliyudin: ‚ÄúI‚Äôll have a Corona... hold the virus!‚Äù -a dad on vacation somewhere in Florida right now""</em><br>
but as you can understand, the sentiment of the above tweet should be negative.
How can I make it accurate? and how can I find the accuracy of my output?</p>
",Dataset Preprocessing & Handling,accurate twitter sentiment analysis solution python csv file k tweet information location username date want assign label positive neutral negative tweet python used following python code textblob library tweet sentiment analysis code run perfect produce sentimenttweets csv file like idea tweet give two label number also classify tweet negative neutral positive accurate example following tweet assigns positive number rt eliyudin corona hold virus dad vacation somewhere florida right understand sentiment tweet negative make accurate find accuracy output
Downloading ML annotations in IBM-Watson Knowledge Studio,"<p>I am working in an NLP application with WKS, and after training, got a rather low performing results.</p>

<p>I wonder if there is a way to download annotated documents with its entity classification, both for train and test sets, so I can automatically identify in detail, where are the key differences, so I can fix them.</p>

<p>Those that were annotated by humans, can be downloaded in the section ""Assets"" / ""Documents"" -> Download Document Sets (button on the right side).</p>

<p>The following Python code, lets you look at the data inside it:</p>

<pre><code>import json
import zipfile
with zipfile.ZipFile(&lt;YOUR DOWNLOADED FILE&gt;, ""r"") as zip:
with zip.open('documents.json') as arch:  
    data      = arch.read()  
    documents = json.loads(data)
    print(json.dumps(documents,indent=2,separators=(',',':')))
    df_documentos = pd.DataFrame(None)
i = 0
for documento in documents:
    df_documentos.at[i,'name']         = documento['name']
    df_documentos.at[i,'text']         = documento['text']
    df_documentos.at[i,'status']       = documento['status']
    df_documentos.at[i,'id']           = documento['id']
    df_documentos.at[i,'createdDate']  = '{:14.0f}'.format(documento['createdDate'])
    df_documentos.at[i,'modifiedDate'] = '{:14.0f}'.format(documento['modifiedDate'])
    i += 1
df_documentos

with zipfile.ZipFile(&lt;YOUR DOWNLOADED FILE&gt;, ""r"") as zip:
    with zip.open('sets.json') as arch:  
        data = arch.read()  
        sets = json.loads(data)
        print(json.dumps(sets,indent=2,separators=(',',':')))

df_sets = pd.DataFrame(None)
i = 0
for set in sets:
    df_sets.at[i,'type']         = set['type']
    df_sets.at[i,'name']         = set['name']
    df_sets.at[i,'count']        = '{:6.0f}'.format(set['count'])
    df_sets.at[i,'id']           = set['id']
    df_sets.at[i,'createdDate']  = '{:14.0f}'.format(set['createdDate'])
    df_sets.at[i,'modifiedDate'] = '{:14.0f}'.format(set['modifiedDate'])
    i += 1

df_sets
</code></pre>

<p>Then you can iterate to read each one of the JSON files that come into the ""gt"" folder of the compressed file, and get the detailed sentence splitting, tokenization and annotation.</p>

<p>What I need is being able to download the annotations that resulted from the machine learning model over the TEST documents, which are visible in ""Machine Learning Model"" / ""Performance"" / ""View Decoding Results"".</p>

<p>With this I will be able to identify specific deviations that can lead to revise Type dictionary and annotation criteria.</p>
",Dataset Preprocessing & Handling,downloading ml annotation ibm watson knowledge studio working nlp application wks training got rather low performing result wonder way download annotated document entity classification train test set automatically identify detail key difference fix annotated human downloaded section asset document download document set button right side following python code let look data inside iterate read one json file come gt folder compressed file get detailed sentence splitting tokenization annotation need able download annotation resulted machine learning model test document visible machine learning model performance view decoding result able identify specific deviation lead revise type dictionary annotation criterion
How to calculate semantic similarity of short text corpora?,"<p>What is the proper approach to unsupervised comparison of semantic similarity between two short text corpora? Comparing LDA topic distributions for the two doesn't seem to be a solution, as for short documents the generated topics do not really grasp the semantics very well. Chunking didn't help, because  following tweets don't have to be on the same topic. Is e.g. creating a matrix of cosine similarities between document TF-IDFs in these corpora a good way to go?</p>
",Dataset Preprocessing & Handling,calculate semantic similarity short text corpus proper approach unsupervised comparison semantic similarity two short text corpus comparing lda topic distribution two seem solution short document generated topic really grasp semantics well chunking help following tweet topic e g creating matrix cosine similarity document tf idf corpus good way go
WordNet in offline mode - Python,"<p>I'm creating a program which needs to find synonyms of a given word. I tried Nltk module - <code>Nltk.corpus.wordnet</code> but this is too slow. It's because this is an online version. So I tried to find a way to use it offline - download it's data. </p>

<p>I've tried  this <a href=""https://stackoverflow.com/questions/9684877/synonyms-offline-dictionary-for-a-search-application"">Link to StackOverflow</a> way, downloaded <code>WordNet-2.1.exe</code> from <a href=""http://wordnet.princeton.edu/wordnet/download/current-version/#win"" rel=""nofollow noreferrer"">this link</a>, but it installs only this program: <code>Remove Duplicate Lines &amp; Words In Multiple Text Files Software</code>.</p>

<p>So I tried to download <code>WordNet 3.1 DATABASE FILES ONLY</code> on that page, but it contains some files which didn't help me (maybe I don't know how). </p>

<p>Can somebody give me an advice how to make WordNet offline works (the only lib I need is synonyms)? 
It is possible to make it to work as a Python module or some csv file?</p>
",Dataset Preprocessing & Handling,wordnet offline mode python creating program need find synonym given word tried nltk module slow online version tried find way use offline download data tried link installs program tried download page contains file help maybe know somebody give advice make wordnet offline work lib need synonym possible make work python module csv file
"Spell Correction using TextBlob, autocorrect","<p>I had been trying to implement a function that would correct spellings in multiple documents. I tried two methods viz <code>TextBlob</code> and <code>autocorrect</code>.</p>

<p><strong>Using TextBlob</strong> </p>

<pre><code>def spell_correct(word_list):
    try:
        corrected = []
        for word in word_list:
            w = Word(word)
            corrected.append(w.correct())
        return corrected
    except UnicodeDecodeError:
        return None
</code></pre>

<p><strong>Using autocorrect</strong></p>

<pre><code>def spell_correct(word_list):
     try:
         corrected = []
         for word in word_list:
             corrected.append(spell(word))
         return corrected
     except UnicodeDecodeError:
         return None
</code></pre>

<p>Both of them work quite well on single list of words provided as input. However, when I work with multiple documents in a pandas <code>DataFrame</code> of 13k rows, it is taking so much time that I usually <code>KeyboardInterrupt</code>. Am I impatient or is there a faster method to spell correct?</p>

<p><strong>Update</strong>
This is how I apply these functions on multiple documents in a pandas <code>DataFrame</code>,</p>

<pre><code>df['corrected_words'] = df.words.apply(lambda x: spell_correct(x))
</code></pre>
",Dataset Preprocessing & Handling,spell correction using textblob autocorrect trying implement function would correct spelling multiple document tried two method viz using textblob using autocorrect work quite well single list word provided input however work multiple document panda k row taking much time usually impatient faster method spell correct update apply function multiple document panda
How to separate Parts of Speech tags from Sentences and make them into two separate columns one with the raw sentence and one with only the POS tags,"<p>so I have a Bangla Parts of Speech Data-Set which looks like this:</p>

<pre><code>রপ্তানি\JJ.n.n দ্রব্য\NC.0.0.n.n -\PU তাজা\JJ.n.n ও\CCD.n শুকনা\JJ.n.n ফল\NC.0.0.n.n ,\PU আফিম\NC.0.0.n.n ,\PU পশুচর্ম\NC.0.0.n.n ও\CCD.n পশম\NC.0.0.n.n এবং\CCD.n কার্পেট\NC.0.0.n.n ৷\PU
রাজা\NP.0.0.n.n মহানন্দ\NP.0.0.n.n রাজধানীতে\NC.0.loc.n.n তৈরি\NC.0.0.n.n করেছিল\VM.3.pst.pft.dcl.fin.n.n.n শিব\NP.0.0.n.n মন্দির\NC.0.0.n.n ও\CCD.n বৈষ্ণবদের\NC.0.gen.n.n মন্দির\NC.0.0.n.n ৷\PU
প্রতিটি\JQ.y.n.nnm বৌদ্ধ\JJ.n.n -\PU সন্ন্যাসী\NC.0.0.n.n ,\PU সন্ন্যাসিনী\NC.0.0.n.n বা\CCD.n গৃহস্থ\NC.0.0.n.n -\PU যেই\PRL.sg.0.n.n.y.n হোক\VM.3.prs.sim.sbj.fin.n.n.n না\CX.y কেন\CX.n ,\PU প্রাতে\NC.0.loc.n.n ,\PU দ্বিপ্রহরে\NC.0.loc.n.n ,\PU অপরাহ্নে\NC.0.loc.n.n ,\PU ও\CCD.n সন্ধ্যায়\NC.0.loc.n.n এই\DAB.0.n পবিত্র\JJ.n.n ত্রয়ীকে\NC.0.acc.n.n প্রণাম\NC.0.0.n.n ও\CCD.n ধ্যান\NC.0.0.n.n করে\VM.0.0.0.0.nfn.n.n.n ,\PU তাকে\PPR.sg.3.acc.n.n.n.n জপ\NC.0.0.n.n করে\VM.0.0.0.0.nfn.n.n.n এই\PPR.sg.3.0.n.n.n.n ব'লে\VM.0.0.0.0.nfn.n.n.n -\PU ""\PU আমি\PPR.sg.1.0.n.n.n.n বুদ্ধের\NP.0.gen.n.n শরণাগত\JJ.n.n হলাম\VM.3.pst.sim.dcl.fin.n.n.n ৷\PU
বদাওনী\NP.0.0.n.n যে\CX.n খুব\JQ.n.n.nnm খুশি\JJ.n.n মনে\NC.0.loc.n.n অনুবাদের\NC.0.gen.n.n কাজে\NC.0.loc.n.n আত্মনিয়োগ\NC.0.0.n.n করেছিলেন\VM.3.pst.pft.dcl.fin.n.n.y তা\PPR.sg.3.0.n.n.n.n নয়\VM.3.prs.sim.dcl.fin.n.y.n ,\PU কারণ\CSB.n মহাভারতের\NP.0.gen.n.n ওই\DAB.sg.y অংশের\NC.0.gen.n.n বিষয়বস্তুর\NC.0.gen.n.n সঙ্গে\PP.0.n তাঁর\PPR.sg.3.gen.n.n.n.y গোঁড়া\JJ.n.n ধর্মবিশ্বাসের\NC.0.gen.n.n আদপে\CX.n কোন\JQ.n.n.nnm মিল\NC.0.0.n.n না\CX.y থাকায়\NV.loc.n.n তাঁর\PPR.sg.3.0.n.n.n.y কোনরকম\JQ.n.n.nnm মানসিক\JJ.n.n তৃপ্তি\NC.0.0.n.n হত\VM.3.pst.sim.hab.fin.n.n.n না\CX.y ,\PU সমস্ত\JQ.n.n.nnm পরিশ্রম\NC.0.0.n.n অর্থহীন\JJ.n.n মনে\NC.0.loc.n.n হত\VM.3.pst.sim.hab.fin.n.n.n ৷\PU
</code></pre>

<p>I have read the data-frame using Pandas:</p>

<pre><code>import pandas as pd

df = pd.read_csv('base_dataset.txt', sep='delimiter', encoding ='utf-8', header=None)

df

OUTPUT: 

0   রপ্তানি\JJ.n.n দ্রব্য\NC.0.0.n.n -\PU তাজা\JJ....
1   রাজা\NP.0.0.n.n মহানন্দ\NP.0.0.n.n রাজধানীতে\N...
2   প্রতিটি\JQ.y.n.nnm বৌদ্ধ\JJ.n.n -\PU সন্ন্যাসী...
3   বদাওনী\NP.0.0.n.n যে\CX.n খুব\JQ.n.n.nnm খুশি\...
4   কয়েক\JQ.n.n.nnm বিঘা\CCL.n ধানী\JJ.n.n জমিও\NC...
5   মাটি\NC.0.0.n.n থেকে\PP.0.n বড়জোর\JQ.n.n.nnm চ...
6   তাদের\PPR.pl.3.gen.n.n.n.n চা\NC.0.0.n.n -\PU ...
7   নকল\JJ.n.n ওষুধের\NC.0.gen.n.n কেরামতি\NC.0.0....
</code></pre>

<p>My Query from you guys: 
I want to separate the Parts of Speech Tags from the Sentences and make two different columns.
Column 1  would be the Bangla Sentences and Column 2 would be the corresponding POS Tags so that I could use it to feed it to a Bi-directional LSTM and train</p>

<pre><code>Here is how the output should look like if I printed the First rows of both Columns:
Column 1 Row 1:
রপ্তানি দ্রব্য - তাজা ও শুকনা ফল, আফিম, পশুচর্ম ও পশম এবং কার্পেট ৷

Column 2 Row 1:
JJ.n.n NC.0.0.n.n PU JJ.n.n CCD.n JJ.n.n NC.0.0.n.n PU NC.0.0.n.n PU NC.0.0.n.n CCD.n NC.0.0.n.n CCD.n NC.0.0.n.n PU

</code></pre>

<p>Update: 
If Bangla is not understandable for you can you show me the procedure for doing it in the English Language? 
For example consider a file containing 1000's of english sentences as such:</p>

<pre><code>People/NNS continue/VBP to/TO inquire/VB the/DT reason/NN for/IN the/DT race/NN for/IN outer/JJ space/NN 

Secretariat/NNP is/VBZ expected/VBN to/TO race/VB tomorrow/NN

</code></pre>

<p>What I basically want is to convert the Raw dataset into a data-set containing two columns; Column 1 containing just the plain sentences without the POS tags and Column 2 containing the labels as in the corresponding POS tags of the sentences in column 1.</p>

<p>I would like to do it for all the sentences in the data-set and I have attached the data-set here: 
<a href=""https://drive.google.com/drive/folders/1ipLMjW4LHLK0uSMvo-FKSZQ9r_ppiMob?usp=sharing"" rel=""nofollow noreferrer"">POS Bangla Data-set</a></p>

<p>Please note I want to keep punctuation such as a comma which is denoted with the tag PU since it plays a role in determining the structure of the sentence.</p>

<p>Any help would be highly appreciated.</p>
",Dataset Preprocessing & Handling,separate part speech tag sentence make two separate column one raw sentence one po tag bangla part speech data set look like read data frame using panda query guy want separate part speech tag sentence make two different column column would bangla sentence column would corresponding po tag could use feed bi directional lstm train update bangla understandable show procedure english language example consider file containing english sentence basically want convert raw dataset data set containing two column column containing plain sentence without po tag column containing label corresponding po tag sentence column would like sentence data set attached data set po bangla data set please note want keep punctuation comma denoted tag pu since play role determining structure sentence help would highly appreciated
Applying LSA on term document matrix when number of documents are very less,"<p>I have a term-document matrix (X) of shape <code>(6, 25931)</code>. The first 5 documents are my source documents and the last document is my target document. The column represents counts for different words in the vocabulary set. I want to get the cosine similarity of the last document with each of the other documents.  </p>

<p>But since SVD produces an S of size <code>(min(6, 25931),)</code>, If I used the S to reduce my X, I get a 6 * 6 matrix. But In this case, I feel that I will be losing too much information since I am reducing a vector of size <code>(25931,)</code> to <code>(6,)</code>. </p>

<p>And when you think about it, usually, the number of documents will always be less than number of vocabulary words. In this case, using SVD to reduce dimensionality will always produce vectors that are of size <code>(no documents,)</code>. </p>

<p>According to everything that I have read, when SVD is used like this on a term-document matrix, it's called LSA. </p>

<ol>
<li>Am I implementing LSA correctly? </li>
<li>If this is correct, then is there any other way to reduce the dimensionality and get denser vectors where the size of the compressed vector is greater than <code>(6,)</code>?</li>
</ol>

<p>P.S.: I also tried using <code>fit_transform</code> from <code>sklearn.decomposition.TruncatedSVD</code> which expects the vector to be of the form <code>(n_samples, n_components)</code> which is why the shape of my term-document matrix is <code>(6, 25931)</code> and not <code>(25931, 6)</code>. I kept getting a <code>(6, 6)</code> matrix which initially confused me. But now it makes sense after I remembered the math behind SVD. </p>
",Dataset Preprocessing & Handling,applying lsa term document matrix number document le term document matrix x shape first document source document last document target document column represents count different word vocabulary set want get cosine similarity last document document since svd produce size used reduce x get matrix case feel losing much information since reducing vector size think usually number document always le number vocabulary word case using svd reduce dimensionality always produce vector size according everything read svd used like term document matrix called lsa implementing lsa correctly correct way reduce dimensionality get denser vector size compressed vector greater p also tried using expects vector form shape term document matrix kept getting matrix initially confused make sense remembered math behind svd
Python dictionary creation?,"<p>Hey so currently I've been trying to convert the text containing column of a csv file into a dictionary. From here I would then like to create word embeddings (&amp; potentially embeddings for subparts of words. ie. dictionary => dict - tion - nary) What would be the best way to go about this, what frameworks would work best? I have attached my current code and an example of one row of the database.</p>

<pre><code># First we must input the data, we can use pandas to do this.
import pandas as pd
# Our data does not have headers so we will fabricate ones ourself during the importing
data = pd.read_csv('agr_en_train.csv', header=None, names =['Unique_ID', 'Text', 'Aggression-level'])
# We can now check the data has loaded properly.
data

{0}{facebook_corpus_msr_1723796}    {Well said sonu..you have courage to stand agai...} {OAG}
</code></pre>

<p>Please let me know if you require any other info the answer this question more adeptly. Additionally are there any recommended pre-created dictionaries. How would I utilise them, an answer or direction to any sources that would be helpful are greatly appreciated!</p>
",Dataset Preprocessing & Handling,python dictionary creation hey currently trying convert text containing column csv file dictionary would like create word embeddings potentially embeddings subpart word ie dictionary dict tion nary would best way go framework would work best attached current code example one row database please let know require info answer question adeptly additionally recommended pre created dictionary would utilise answer direction source would helpful greatly appreciated
Collapse a pandas data frame of words into sentences,"<p>My goal is to take a dataframe composed of words and tags, and collapse it into a dataframe composed of sentences and a list of tags.</p>

<p>Sample input:  </p>

<pre><code>df = pd.DataFrame([('Effect', 'O'),
               ('of', 'O'),
               ('ginseng', 'i'),
               ('extract', 'i'),
               ('supplementation', 'i'),
               ('on', 'O'),
               ('testicular', 'o'),
               ('functions', 'o'),
               ('in', 'O'),
               ('diabetic', 'p'),
               ('rats', 'p'),
               ('.', 'p'),
               ('OBJECTIVE', 'O'),
               ('It', 'O'),
               ('was', 'O')],
               columns=('token', 'annotation'))
</code></pre>

<p>Goal output:</p>

<pre><code>df = pd.DataFrame([('Effect of ginseng extract supplementation on testicular functions in diabetic rats.', \ 
                     ['O','O','i','i','i','O','o','o','O','p','p','p','O','O','O']),
                   ('OBJECTIVE It was', ['O','O','O'])],
                   columns=('token', 'annotation'))
</code></pre>

<p>Sorry for the goofy example - that really is the first 15 rows of this dataset!!</p>

<p>Any ideas of how to compress the rows of words into rows of sentences would be much appreciated.</p>
",Dataset Preprocessing & Handling,collapse panda data frame word sentence goal take dataframe composed word tag collapse dataframe composed sentence list tag sample input goal output sorry goofy example really first row dataset idea compress row word row sentence would much appreciated
"I fine tuned a pre-trained BERT for sentence classification, but i cant get it to predict for new sentences","<p>below is the result of my fine-tuning.</p>

<pre><code>Training Loss   Valid. Loss Valid. Accur.   Training Time   Validation Time
epoch                   
1   0.16    0.11    0.96    0:02:11 0:00:05
2   0.07    0.13    0.96    0:02:19 0:00:05
3   0.03    0.14    0.97    0:02:22 0:00:05
4   0.02    0.16    0.96    0:02:21 0:00:05
</code></pre>

<p>next i tried to use the model to predict labels from a csv file. i created a label column, set the type to int64 and run the prediction.</p>

<pre><code>print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))
model.eval()
# Tracking variables 
predictions , true_labels = [], []
# Predict 
for batch in prediction_dataloader:
  # Add batch to GPU
  batch = tuple(t.to(device) for t in batch)

  # Unpack the inputs from our dataloader
  b_input_ids, b_input_mask, b_labels = batch

  # Telling the model not to compute or store gradients, saving memory and 
  # speeding up prediction
  with torch.no_grad():
      # Forward pass, calculate logit predictions
      outputs = model(b_input_ids, token_type_ids=None, 
                      attention_mask=b_input_mask)

  logits = outputs[0]

  # Move logits and labels to CPU
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.to('cpu').numpy()

  # Store predictions and true labels
  predictions.append(logits)
  true_labels.append(label_ids)


</code></pre>

<p>however, while i am able to print out the predictions[4.235, -4.805] etc, and the true_labels[NaN,NaN.....], i am unable to actually get the predicted labels{0 or 1}. Am i missing something here?</p>
",Dataset Preprocessing & Handling,fine tuned pre trained bert sentence classification cant get predict new sentence result fine tuning next tried use model predict label csv file created label column set type int run prediction however able print prediction etc true label nan nan unable actually get predicted label missing something
How to load dataset with pandas dataframes into Spacy?,"<p>I currently have text data with pandas dataframe. I would like to do <em>part of speech</em> tags with Spacy library. But I don't know how to import load data from pandas into Spacy. </p>

<p>Does anybody know how to load data from pandas dataframe into Spacy? Thank you!</p>

<p>Here is my dataset:</p>

<pre><code>
df = pd.concat([jsons_data,label],axis=1)

text    title   authors label
0   On Saturday, September 17 at 8:30 pm EST, an e...   Another Terrorist Attack in NYC…Why Are we STI...   [View All Posts, Leonora Cravotta]  Real
1   Story highlights ""This, though, is certain: to...   Hillary Clinton on police shootings: 'too many...   [Mj Lee, Cnn National Politics Reporter]    Real
2   Critical Counties is a CNN series exploring 11...   Critical counties: Wake County, NC, could put ...   [Joyce Tseng, Eli Watkins]  Real
3   McCain Criticized Trump for Arpaio’s Pardon… S...   NFL Superstar Unleashes 4 Word Bombshell on Re...   []  Real
4   Story highlights Obams reaffirms US commitment...   Obama in NYC: 'We all have a role to play' in ...   [Kevin Liptak, Cnn White House Producer]    Real
5   Obama weighs in on the debate\n\nPresident Bar...   Obama weighs in on the debate   [Brianna Ehley, Jack Shafer]

</code></pre>

<p>This is my code to load data into Spacy:</p>

<pre><code>import spacy
from dframcy import DframCy
nlp = spacy.load('en_core_web_sm')
dframcy = DframCy(nlp)
doc = dframcy.nlp(df)
annotation_dataframe = dframcy.to_dataframe(doc)

TypeError: Argument 'string' has incorrect type (expected str, got DataFrame)
</code></pre>
",Dataset Preprocessing & Handling,load dataset panda dataframes spacy currently text data panda dataframe would like part speech tag spacy library know import load data panda spacy doe anybody know load data panda dataframe spacy thank dataset code load data spacy
"ValueError: Error when checking target: expected dense_22 to have shape (100, 50) but got array with shape (1, 50)","<p>I'm training a neural network to predict the Document-Frequency from a set of documents.</p>

<p>So, the main idea is to map a matrix with 100 documents and 50 tokens to the respective document-frequency array.</p>

<p>X = (n_samples, 100, 50) -> y = (n_samples, 1, 50)</p>

<p>My code is:</p>

<pre><code>model = Sequential()
model.add(Dense(50, activation='sigmoid', input_shape=(100,50)))
model.add(Dense(50))
model.compile(optimizer='rmsprop', loss='mse')
model.fit(X_train, y_train, epochs=50)
</code></pre>

<p>But i got an error:</p>

<pre><code>ValueError: Error when checking target: expected dense_22 to have shape (100, 50) but got array with shape (1, 50)
</code></pre>
",Dataset Preprocessing & Handling,valueerror error checking target expected dense shape got array shape training neural network predict document frequency set document main idea map matrix document token respective document frequency array x n sample n sample code got error
"How to write an R function that reads in a directory of text files, changes them, and saves into the same directory","<p>Have tried looking for an answer elsewhere but no luck, so here goes:</p>

<p>I have a directory full of <code>.txt</code> files containing natural language (legislation). I want to read in the directory, carry out some string manipulations on each file using a new function, and save the file in the same directory with a new filename.</p>

<p>Example data (note the ""Commencement Information"" line at the end):</p>

<pre><code>example &lt;- ""(7) A person who is not a UK national commits an offence under this section if 
            — (a) any part of the arranging or facilitating takes place in the United 
            Kingdom, or (b) the travel consists of arrival in or entry into, departure
            from, or travel within, the United Kingdom. Commencement Information I2 S. 2 
            in force at 31.7.2015 by S.I. 2015/1476, reg. 2(a) (with regs. 6-8)""
</code></pre>

<p>I sorted out reading in a directory (with help from <a href=""https://stackoverflow.com/a/27912952/10734443"">this</a> answer), which works fine:</p>

<pre><code>files &lt;- list.files(path=""../txt_copies/"", pattern=""*.txt"", all.files=T, full.names = T) #filenames
filelist &lt;- lapply(files, read.delim) #read in files from the filenames list
names(filelist) &lt;- paste0(basename(file_path_sans_ext(files))) #name list elements by filenames
list2env(filelist, envir=.GlobalEnv) #move the list elements into the global env as objects
</code></pre>

<p>The issue is my function:</p>

<pre><code>page_cleaner &lt;- function(x) {

  txt &lt;- x

  # clean text and print confirmation
  txt &lt;- str_replace_all(txt, ""(F)(\\d).+?(\\n)"", """")
  txt &lt;- gsub(""Textual Amendments"", """", txt)
  print(""Text cleaned of Textual Amendments"")
  txt &lt;- str_replace_all(txt, ""(Commencement Information).+?(\\d)\\)"", """")
  txt &lt;- str_replace_all(txt, ""(Commencement Information).+?(\\w)\\)"", """")
  print(""Text cleaned of Commencement Information"")

  x &lt;- txt

}

lapply(names(filelist), page_cleaner)
</code></pre>

<p><strong>It should return:</strong></p>

<pre><code>[1] ""(7) A person who is not a UK national commits an offence under this section if
     — (a) any part of the arranging or facilitating takes place in the United Kingdom, 
     or (b) the travel consists of arrival in or entry into, departure from, or travel 
     within, the United Kingdom.
</code></pre>

<p>It seems to work fine on the example when I call the function on its own, e.g. <code>page_cleaner(example)</code> but not on the list of files.</p>

<p>I was fairly certain this would work and I can't figure out where I went wrong. Those string manipulations work fine outside of the function. In the example data, they should delete everything from ""Commencement Information"" to the end of the string.</p>

<p>I already have a method for saving objects to a directory when this is all done, no need for help with that.</p>

<p>Thanks!</p>
",Dataset Preprocessing & Handling,write r function read directory text file change save directory tried looking answer elsewhere luck go directory full file containing natural language legislation want read directory carry string manipulation file using new function save file directory new filename example data note commencement information line end sorted reading directory help already method saving object directory done need help thanks
Not enough memory while using the Tokenizer in keras.preprocessing.text,"<p>I want to build a RNN model using keras to classify sentences.</p>

<p>I tried the following code:</p>

<pre><code>docs = []
with open('all_dga.txt', 'r') as f:
    for line in f.readlines():
        dga_domain, _ = line.split(' ')
        docs.append(dga_domain)

t = Tokenizer()
t.fit_on_texts(docs)
encoded_docs = t.texts_to_matrix(docs, mode='count')
print(encoded_docs)
</code></pre>

<p>but got a MemoryError. It seemed that I couldn't load all data into the memory. This is the output:</p>

<pre><code>Traceback (most recent call last):
  File ""test.py"", line 11, in &lt;module&gt;
    encoded_docs = t.texts_to_matrix(docs, mode='count')
  File ""/home/yurzho/anaconda3/envs/deepdga/lib/python3.6/site-packages/keras/preprocessing/text.py"", line 273, in texts_to_matrix
    return self.sequences_to_matrix(sequences, mode=mode)
  File ""/home/yurzho/anaconda3/envs/deepdga/lib/python3.6/site-packages/keras/preprocessing/text.py"", line 303, in sequences_to_matrix
    x = np.zeros((len(sequences), num_words))
MemoryError
</code></pre>

<p>If anyone familiar with keras, please tell me how to pre-process the dataset.</p>

<p>Thanks in advance!</p>
",Dataset Preprocessing & Handling,enough memory using tokenizer kera preprocessing text want build rnn model using kera classify sentence tried following code got memoryerror seemed load data memory output anyone familiar kera please tell pre process dataset thanks advance
read corpus of text files in spacy,"<p>All the examples that I see for using spacy just read in a single text file (that is small in size).
How does one load a corpus of text files into spacy?</p>

<p>I can do this with textacy by pickling all the text in the corpus:</p>

<pre><code>docs =  textacy.io.spacy.read_spacy_docs('E:/spacy/DICKENS/dick.pkl', lang='en')

for doc in docs:
    print(doc)
</code></pre>

<p>But I am not clear as to how to use this generator object (docs) for further analysis.</p>

<p>Also, I would rather use spacy, not textacy.</p>

<p>spacy also fails to read in a single file that is large (~ 2000000 characters).</p>

<p>Any help is appreciated...</p>

<p>Ravi</p>
",Dataset Preprocessing & Handling,read corpus text file spacy example see using spacy read single text file small size doe one load corpus text file spacy textacy pickling text corpus clear use generator object doc analysis also would rather use spacy textacy spacy also fails read single file large character help appreciated ravi
R - Extract and Parse Each Instance of Multi-line Delimited Text by 2 Strings into Individual Rows (.txt to data.frame),"<p>I believe this is a loop and gregexpr() issue. I'm trying to extract/export multi-line text from i number of standardized instances within i number of standardized .txt forms into a data frame where each instance is a separate row. So far, I can successfully extract the string data (though the algorithm extracts a little more than the stated gregexpr() parameters) but can only export as .txt as a lump sum of text. </p>

<ol>
<li>How can I create a data frame of the extracted txt-files' text where each instance of multi-line text has its own row? (Once the data is in a data.frame format, I know how to export as xlsx from there.)</li>
<li>How can I extract only the data from the parameters I have set?</li>
</ol>

<p>With help (particularly from <a href=""https://stackoverflow.com/questions/21445659/use-r-to-convert-pdf-files-to-text-files-for-text-mining?rq=1"">Ben from the comments of this post</a>), here is what I have so far:</p>

<pre><code># Txt Data Format
txt 1 &lt;-
""A. The First:  abcdefg hijklmnop qrstuv wxyz. B. The Second: abcdefg hijklmnop qrstuv wxyz.
    abcdefg hijklmnop qrstuv wxyz. abcdefg hijklmnop qrstuv wxyz abcdefg hijklmnop qrstuv wxyz.
 C. The Third:  abcdefg hijklmnop qrstuv wxyz. D. The Fourth: abcdefg hijklmnop qrstuv wxyz.
    abcdefg hijklmnop qrstuv wxyz. abcdefg hijklmnop qrstuv wxyz abcdefg hijklmnop qrstuv wxyz.
 A. The First:  abcdefg hijklmnop qrstuv wxyz. B. The Second: abcdefg hijklmnop qrstuv wxyz.
    abcdefg hijklmnop qrstuv wxyz. abcdefg hijklmnop qrstuv wxyz abcdefg hijklmnop qrstuv wxyz.
 C. The Third:  abcdefg hijklmnop qrstuv wxyz. D. The Fourth: abcdefg hijklmnop qrstuv wxyz.""
    abcdefg hijklmnop qrstuv wxyz. abcdefg hijklmnop qrstuv wxyz abcdefg hijklmnop qrstuv wxyz.

txt 2 &lt;-
""A. The First:  abcdefg hijklmnop qrstuv wxyz. B. The Second: abcdefg hijklmnop qrstuv wxyz.
    abcdefg hijklmnop qrstuv wxyz. abcdefg hijklmnop qrstuv wxyz abcdefg hijklmnop qrstuv wxyz.
 C. The Third:  abcdefg hijklmnop qrstuv wxyz. D. The Fourth: abcdefg hijklmnop qrstuv wxyz.
    abcdefg hijklmnop qrstuv wxyz. abcdefg hijklmnop qrstuv wxyz abcdefg hijklmnop qrstuv wxyz.
 A. The First:  abcdefg hijklmnop qrstuv wxyz. B. The Second: abcdefg hijklmnop qrstuv wxyz.
    abcdefg hijklmnop qrstuv wxyz. abcdefg hijklmnop qrstuv wxyz abcdefg hijklmnop qrstuv wxyz.
 C. The Third:  abcdefg hijklmnop qrstuv wxyz. D. The Fourth: abcdefg hijklmnop qrstuv wxyz.""
    abcdefg hijklmnop qrstuv wxyz. abcdefg hijklmnop qrstuv wxyz abcdefg hijklmnop qrstuv wxyz.


#################################
# Directory and Text Extraction #
#################################

dest &lt;- ""C:/~""
docs_text &lt;- list.files(path = dest, pattern = ""txt"",  full.names = TRUE)

## Assumes that all the content I want to extract is between ""A."" and ""C."" in 
## the text while ignoring ""C."" and ""D."" content.

docs_list &lt;- list.files(path = dest, pattern = ""txt"",  full.names = TRUE)
docs_doc &lt;- lapply(docs_list, function(i) {
  j &lt;- paste0(scan(i, what = character()), collapse = "" "")
  regmatches(j, gregexpr(""(?&lt;=A. The First).*?(?=C. The Third)"", j, perl=TRUE))
})

lapply(1:length(docs_doc),  function(i) write.table(docs_doc[i], file=paste(docs_list[i], "" "", 
"" "", sep="".""), quote = FALSE, row.names = FALSE, col.names = FALSE, eol = "" "" ))
</code></pre>

<p>Current output looks like this where all of the text is in one line and captures more than just between ""A."" and ""C."":</p>

<p><a href=""https://i.sstatic.net/D7cqC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/D7cqC.png"" alt=""Current Output""></a></p>

<p>Desired output would look like this where the multi-line text between any instance of ""A."" and ""C."" is extracted and assigned one line each:</p>

<p><a href=""https://i.sstatic.net/IBMbA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IBMbA.png"" alt=""Desired Output""></a></p>

<p>Any help you could provide would be tremendously helpful! </p>

<p>I'm ultimately trying to develop an NLP model that can extract standardized form data from hundreds of large PDFs for a year over year repository. If this post suggests I'm not thinking about how to approach this problem efficiently/effectively, I'm open to direction. </p>

<p>Thanks in advance!</p>
",Dataset Preprocessing & Handling,r extract parse instance multi line delimited text string individual row txt data frame believe loop gregexpr issue trying extract export multi line text number standardized instance within number standardized txt form data frame instance separate row far successfully extract string data though algorithm extract little stated gregexpr parameter export txt lump sum text create data frame extracted txt file text instance multi line text ha row data data frame format know export xlsx extract data parameter set help particularly desired output would look like multi line text instance c extracted assigned one line help could provide would tremendously helpful ultimately trying develop nlp model extract standardized form data hundred large pdfs year year repository post suggests thinking approach problem efficiently effectively open direction thanks advance
"I have to identify paragraphs from multiple text files(.txt) and create a dataframe of [paragraph1, &quot;text of the file1 in paragraphs &quot;]","<pre><code>with open(""/home/xxxx/Downloads/DataEnginner9.txt"", ""r"") as f:
    for line in f:
        print(line)
</code></pre>

<p>when i run this code i am able to get as sentences only,</p>

<p>The above code takes the file and splits into sentences and prints each line, but i want it to identify paragraphs from multiple files and also create a data-frame which contains the file name in the first column and respective entire content in the second column of the same row
i.e..,
example Data-frame : </p>

<p>[file1,content of the file splitted in paragraphs;
file2,content of the file2 splitted in paragraphs
.
.
.
]</p>

<p>Below is the sample output generated by the above script from one file.</p>

<blockquote>
  <p>Job description</p>
  
  <p>Responsibilities </p>
  
  <p>Work collaboratively with a global team to design, develop</p>
  
  <p>scalable, maintainable and reliable services that process very large
  quantities</p>
  
  <p>data using Big Data technologies (100 billion daily indicators, 6
  TB/day before</p>
  
  <p>compression). </p>
  
  <p>Familiar with Object oriented development, with specific experience </p>
  
  <p>in at least one major OO language(knowledge of Java is mandatory and
  if </p>
  
  <p>possible java 8). Nice to have: Knowledge of functional programming. </p>
  
  <p>Perform end-to-end software development life cycle functions </p>
  
  <p>including Design, Development, Performance Analysis &amp; Tuning,
  Optimization,</p>
  
  <p>Testing and Product Maintenance.</p>
</blockquote>
",Dataset Preprocessing & Handling,identify paragraph multiple text file txt create dataframe paragraph text file paragraph run code able get sentence code take file split sentence print line want identify paragraph multiple file also create data frame contains file name first column respective entire content second column row e example data frame file content file splitted paragraph file content file splitted paragraph sample output generated script one file job description responsibility work collaboratively global team design develop scalable maintainable reliable service process large quantity data using big data technology billion daily indicator tb day compression familiar object oriented development specific experience least one major oo language knowledge java mandatory possible java nice knowledge functional programming perform end end software development life cycle function including design development performance analysis tuning optimization testing product maintenance
Split into test and train set before or after generating document term matrix?,"<p>I'm working on simple machine learning problems and I trying to build a classifier that can differentiate between spam and non-spam SMS. I'm confused as to whether I need to generate the document-term matrix before splitting into test and train sets or should I generate the document-term matrix after splitting into test and train? </p>

<p>I tried it both ways and found that the accuracy is slightly higher when the I split the data before generating the document-term matrix. But to me, this makes no sense. Shouldn't the accuracy be the same? Does the order of these operations make any difference?</p>
",Dataset Preprocessing & Handling,split test train set generating document term matrix working simple machine learning problem trying build classifier differentiate spam non spam sm confused whether need generate document term matrix splitting test train set generate document term matrix splitting test train tried way found accuracy slightly higher split data generating document term matrix make sense accuracy doe order operation make difference
R - Export Extracted Text Data (Each Instance as Row) to data.frame Format,"<p>I'm trying to extract/export text from i number of standardized instances within i number of standardized .txt forms into a data frame where each instance is a separate row. I then want to export that data as an .xlsx file. So far, I can successfully extract the data (though the algorithm extracts a little more than the stated gregexpr() parameters) but can only export as .txt as a lump sum of text. </p>

<ol>
<li>How can I create a data frame of the extracted txt-files' text where each instance has its own row?
(Once the data is in a data.frame format, I know how to export as xlsx from there.)</li>
<li>How can I extract only the data from the parameters I have set?</li>
</ol>

<p>With help (particularly from <a href=""https://stackoverflow.com/questions/21445659/use-r-to-convert-pdf-files-to-text-files-for-text-mining?rq=1"">Ben from the comments of this post</a>), here is what I have so far:</p>

<pre><code># Txt Data Format
txt 1 &lt;-
""A. The First:  abcdefg hijklmnop qrstuv wxyz.
 B. The Second: abcdefg hijklmnop qrstuv wxyz.
 C. The Third:  abcdefg hijklmnop qrstuv wxyz.
 D. The Fourth: abcdefg hijklmnop qrstuv wxyz.

 A. The First:  abcdefg hijklmnop qrstuv wxyz.
 B. The Second: abcdefg hijklmnop qrstuv wxyz.
 C. The Third:  abcdefg hijklmnop qrstuv wxyz.
 D. The Fourth: abcdefg hijklmnop qrstuv wxyz.""

txt 2 &lt;-
""A. The First:  abcdefg hijklmnop qrstuv wxyz.
 B. The Second: abcdefg hijklmnop qrstuv wxyz.
 C. The Third:  abcdefg hijklmnop qrstuv wxyz.
 D. The Fourth: abcdefg hijklmnop qrstuv wxyz.

 A. The First:  abcdefg hijklmnop qrstuv wxyz.
 B. The Second: abcdefg hijklmnop qrstuv wxyz.
 C. The Third:  abcdefg hijklmnop qrstuv wxyz.
 D. The Fourth: abcdefg hijklmnop qrstuv wxyz.""


#################################
# Directory and Text Extraction #
#################################

dest &lt;- ""C:/Desktop/""
docs_text &lt;- list.files(path = dest, pattern = ""txt"",  full.names = TRUE)

## Assumes that all the content I want to extract is between ""A."" and ""C."" in 
## the text while ignoring ""C."" and ""D."" content.

docs_list &lt;- list.files(path = dest, pattern = ""txt"",  full.names = TRUE)
docs_doc &lt;- lapply(docs_list, function(i) {
  j &lt;- paste0(scan(i, what = character()), collapse = "" "")
  regmatches(j, gregexpr(""(?&lt;=A. The First).*?(?=C. The Third)"", j, perl=TRUE))
})

lapply(1:length(docs_doc),  function(i) write.table(docs_doc[i], file=paste(docs_list[i], "" "", 
"" "", sep="".""), quote = FALSE, row.names = FALSE, col.names = FALSE, eol = "" "" ))
</code></pre>

<p>Current output looks like this where all of the text is in one line and captures more than just between ""A."" and ""C."":</p>

<p><a href=""https://i.sstatic.net/D7cqC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/D7cqC.png"" alt=""Current Output""></a></p>

<p>Desired output would look like this where multiple lines of text only between ""A."" and ""C."" is captured and where each multi-line capture is assigned one line each instance:</p>

<p><a href=""https://i.sstatic.net/IBMbA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IBMbA.png"" alt=""Desired Output""></a></p>

<p>Any help you could provide would be tremendously helpful! </p>

<p>I'm ultimately trying to develop an NLP model that can extract standardized form data from hundreds of large PDFs for a year over year repository. If this post suggests I'm not thinking about how to approach this problem efficiently/effectively, I'm open to direction. </p>

<p>Thanks in advance!</p>
",Dataset Preprocessing & Handling,r export extracted text data instance row data frame format trying extract export text number standardized instance within number standardized txt form data frame instance separate row want export data xlsx file far successfully extract data though algorithm extract little stated gregexpr parameter export txt lump sum text create data frame extracted txt file text instance ha row data data frame format know export xlsx extract data parameter set help particularly desired output would look like multiple line text c captured multi line capture assigned one line instance help could provide would tremendously helpful ultimately trying develop nlp model extract standardized form data hundred large pdfs year year repository post suggests thinking approach problem efficiently effectively open direction thanks advance
randomly shuffle multiple dataframes,"<p>I have a corpus of conversations (400) between two people as strings (or more precisely as plain text files) A small example of this might be:</p>

<pre><code>my_textfiles = ['john: hello \nmary: hi there \njohn: nice weather \nmary: yes',
            'nancy: hello \nbill: hi there \nnancy: nice weather \nbill: yes',
            'ringo: hello \npaul: hi there \nringo: nice weather \npaul: yes',
            'michael: hello \nbubbles: hi there \nmichael: nice weather \nbubbles: yes',
            'steve: hello \nsally: hi there \nsteve: nice weather \nsally: yes']
</code></pre>

<p>In addition to speaker names, I have also noted each speakers' role in the conversation (as a leader or follower depending on whether they are the first or second speaker). I then have a simple script that converts each conversation into a data-frame by seperating speaker ID from the content:</p>

<pre><code>import pandas as pd
import re
import numpy as np
import random

def convo_tokenize(tf):
    turnTokenize = re.split(r'\n(?=.*:)', tf, flags=re.MULTILINE)
    turnTokenize = [turn.split(':', 1) for turn in turnTokenize]
    dataframe = pd.DataFrame(turnTokenize, columns = ['speaker','turn'])

    return dataframe

df_list = [convo_tokenize(tf) for tf in my_textfiles]
</code></pre>

<p>The corresponding dataframe then forms the basis of a much longer piece of analysis. However, I would now like to be able to shuffle speakers so that I create entirely random (and likely nonsense) conversations. For instance, John, who is having a conversation with Mary in the fist string, might be randomly assigned Paul (the second speaker in the third string). Crucially, I would need to maintain the order of speech within each speaker. It is also important that, when randomly assigning new speakers, I preserve a mix of leader/follower, such that I am not creating conversations from two leaders or two followers.</p>

<p>To begin, my thinking was to create a standardized speaker label (where 1 = leader, 2 = follower), and separate each DF into a sub-DF and store in role_specific df lists</p>

<pre><code>def speaker_role(dataframe):
    leader = dataframe['speaker'].iat[0]
    dataframe['sp_role'] = np.where(dataframe['speaker'].eq(leader), 1, 2)

    return dataframe

df_list = [speaker_role(df) for df in df_list]

leader_df = []
follower_df = []

for df in df_list:
    is_leader = df['sp_role'] == 1
    is_follower = df['sp_role'] != 1

    leader_df.append(df[is_leader])
    follower_df.append(df[is_follower])
</code></pre>

<p>I have worked out that I can now simply shuffle the data-frame of one of the sub-dfs, in this case the follower_df</p>

<pre><code>follower_rand = random.sample(follower_df, len(follower_df)) 
</code></pre>

<p>Having got to this stage I'm not sure where to turn next. I suspect I will need some sort of zip function, but am unsure exactly what. I'm also unsure how I go about merging the turns together such that they form the same dataframe structure I initially had. Assuming Ringo (leader) is randomly assigned to Bubbles (follower) for one of the DFs, I would hope to have something like this...</p>

<pre><code> speaker   |   turn   |   sp_role
------------------------------------
  ringo       hello          1
 bubbles     hi there        2
  ringo    nice weather      1
 bubbles     yes it is       2
</code></pre>
",Dataset Preprocessing & Handling,randomly shuffle multiple dataframes corpus conversation two people string precisely plain text file small example might addition speaker name also noted speaker role conversation leader follower depending whether first second speaker simple script convert conversation data frame seperating speaker id content corresponding dataframe form basis much longer piece analysis however would like able shuffle speaker create entirely random likely nonsense conversation instance john conversation mary fist string might randomly assigned paul second speaker third string crucially would need maintain order speech within speaker also important randomly assigning new speaker preserve mix leader follower creating conversation two leader two follower begin thinking wa create standardized speaker label leader follower separate df sub df store role specific df list worked simply shuffle data frame one sub dfs case follower df got stage sure turn next suspect need sort zip function unsure exactly also unsure go merging turn together form dataframe structure initially assuming ringo leader randomly assigned bubble follower one dfs would hope something like
Read multiple .txt files into R Vector as Individual Elements,"<p>I have a folder on my desktop named ""project"". Inside this folder are multiple .txt files (e.g. 1.txt, 2.txt, 3.txt, etc.). Does anyone know of a solution that would enable me to have a vector where each element in the vector corresponds to the contents of the file? </p>

<p>For example, if 1.txt contains ""You like apples"", 2.txt contains ""I like pears"", and 3.txt contains ""Cats like fish""..... I would like the result to be a vector of length 3 where each element contains the entirety of the text in the .txt file. Essentially ((""I like apples""),(""You like pears""),(""Cats like fish"")). </p>

<p>My current partial-solution can read one text file in the way I would like, but how can I do this for every .txt file in a dir and store it accordingly? </p>

<pre><code>fileName &lt;- '/Users/myname/Desktop/1.txt'
text &lt;- readChar(fileName, file.info(fileName)$size)
</code></pre>

<p>Many thanks! I'm competing in a hackathon and need this to create a tf-idf implementation and each element in the vector will be a document. </p>
",Dataset Preprocessing & Handling,read multiple txt file r vector individual element folder desktop named project inside folder multiple txt file e g txt txt txt etc doe anyone know solution would enable vector element vector corresponds content file example txt contains like apple txt contains like pear txt contains cat like fish would like result vector length element contains entirety text txt file essentially like apple like pear cat like fish current partial solution read one text file way would like every txt file dir store accordingly many thanks competing hackathon need create tf idf implementation element vector document
Unable to Load Model Trained in Gensim- pickle-related error,"<p>When attempting to load a word2vec model trained by Gensim on a Windows machine, I receive the following error:</p>

<p><code>AttributeError: Can't get attribute 'EpochProgress' on &lt;module '__main__'&gt;</code></p>

<p>I've successfully trained numerous models with Gensim in the past on this system. The only variation being this time I split the <code>model.build_vocab()</code> and <code>model.train()</code> phases, adding in saves &amp; time hacks for each epoch. I also used a different iterator for the vocab build and the training phrases, but on the same dataset with the same tokenization pipeline.</p>

<p>Here is how I did the epoch progress tracking/saving:</p>

<pre class=""lang-py prettyprint-override""><code>class EpochProgress(CallbackAny2Vec):
    '''saves the model after each epoch'''

    def __init__(self, path_prefix):
        self.path_prefix = path_prefix
        self.epoch = 0
        self.start_time = time.time()

    def on_epoch_begin(self, model):
        print(""epoch #{} started"".format(self.epoch))

    def on_epoch_end(self, model):
        print(""epoch #{} completed"".format(self.epoch))
        passed = (time.time() - self.start_time)/60/60 # elapsed time since start in HOURS
        print(""{} hours have passed"".format(str(passed)))
        output_path = get_tmpfile('{}_epoch{}.model'.format(self.path_prefix, self.epoch))
        model.save(output_path)
        print(""model saved at: {}"".format(output_path))
        self.epoch +=1
</code></pre>

<p><code>epoch_progress = EpochProgress('E:/jade_prism/embeddings/phrase-embed-over- time/mega_WOS_word2vec/w2v_models/in_progress/')</code></p>

<p>I then load the baseline model with the vocab build and set a few parameters:</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.Word2Vec.load(baseline_models_directory+chosen_name)
model.window = window
model.size = size
model.workers = workers 
model.callbacks = [epoch_progress]
</code></pre>

<p>Then I do the training like this:</p>

<p><code>model.train(corpus, total_examples=model.corpus_count, epochs=epochs)</code></p>

<p>And finally, save the end product like this: </p>

<p><code>model.save('E:/w2v_models/trained/{}'.format(new_model_filename))</code></p>

<p>Training appeared to work properly, and model saved as expected- unfortunately now I can't load it.</p>

<p>Here is the full Debug readout:</p>

<pre><code>&gt; AttributeError                            Traceback (most recent call
&gt; last)
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\word2vec.py
&gt; in load(cls, *args, **kwargs)    1329         try:
&gt; -&gt; 1330             model = super(Word2Vec, cls).load(*args, **kwargs)    1331 
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\base_any2vec.py
&gt; in load(cls, *args, **kwargs)    1243         """"""
&gt; -&gt; 1244         model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)    1245         if not hasattr(model,
&gt; 'ns_exponent'):
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\base_any2vec.py
&gt; in load(cls, fname_or_handle, **kwargs)
&gt;     602         """"""
&gt; --&gt; 603         return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
&gt;     604 
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\utils.py in
&gt; load(cls, fname, mmap)
&gt;     425 
&gt; --&gt; 426         obj = unpickle(fname)
&gt;     427         obj._load_specials(fname, mmap, compress, subname)
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\utils.py in
&gt; unpickle(fname)    1383         if sys.version_info &gt; (3, 0):
&gt; -&gt; 1384             return _pickle.load(f, encoding='latin1')    1385         else:
&gt; 
&gt; AttributeError: Can't get attribute 'EpochProgress' on &lt;module
&gt; '__main__'&gt;
&gt; 
&gt; During handling of the above exception, another exception occurred:
&gt; 
&gt; AttributeError                            Traceback (most recent call
&gt; last) &lt;ipython-input-4-0206f9f8f3ad&gt; in &lt;module&gt;
&gt;       3 
&gt;       4 # Load the model based onthe model name
&gt; ----&gt; 5 model = gensim.models.Word2Vec.load(model_name)
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\word2vec.py
&gt; in load(cls, *args, **kwargs)    1339             logger.info('Model
&gt; saved using code from earlier Gensim Version. Re-loading old model in
&gt; a compatible way.')    1340             from
&gt; gensim.models.deprecated.word2vec import load_old_word2vec
&gt; -&gt; 1341             return load_old_word2vec(*args, **kwargs)    1342     1343 
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\deprecated\word2vec.py
&gt; in load_old_word2vec(*args, **kwargs)
&gt;     170 
&gt;     171 def load_old_word2vec(*args, **kwargs):
&gt; --&gt; 172     old_model = Word2Vec.load(*args, **kwargs)
&gt;     173     vector_size = getattr(old_model, 'vector_size', old_model.layer1_size)
&gt;     174     params = {
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\deprecated\word2vec.py
&gt; in load(cls, *args, **kwargs)    1639     @classmethod    1640     def
&gt; load(cls, *args, **kwargs):
&gt; -&gt; 1641         model = super(Word2Vec, cls).load(*args, **kwargs)    1642         # update older models    1643         if hasattr(model,
&gt; 'table'):
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\deprecated\old_saveload.py
&gt; in load(cls, fname, mmap)
&gt;      85         compress, subname = SaveLoad._adapt_by_suffix(fname)
&gt;      86 
&gt; ---&gt; 87         obj = unpickle(fname)
&gt;      88         obj._load_specials(fname, mmap, compress, subname)
&gt;      89         logger.info(""loaded %s"", fname)
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\deprecated\old_saveload.py
&gt; in unpickle(fname)
&gt;     377             b'gensim.models.wrappers.fasttext', b'gensim.models.deprecated.fasttext_wrapper')
&gt;     378         if sys.version_info &gt; (3, 0):
&gt; --&gt; 379             return _pickle.loads(file_bytes, encoding='latin1')
&gt;     380         else:
&gt;     381             return _pickle.loads(file_bytes)
&gt; 
&gt; AttributeError: Can't get attribute 'EpochProgress' on module '__main__'\&gt;
</code></pre>
",Dataset Preprocessing & Handling,unable load model trained gensim pickle related error attempting load word vec model trained gensim window machine receive following error successfully trained numerous model gensim past system variation time split phase adding save time hack epoch also used different iterator vocab build training phrase dataset tokenization pipeline epoch progress tracking saving load baseline model vocab build set parameter training like finally save end product like training appeared work properly model saved expected unfortunately load full debug readout
"R Caret, how to have the same features in training and applying dataset","<p>I'm working with the <code>caret</code> package, training a model for text classification, but I've faced a problem that bugs me and I'm not finding a proper solution.</p>

<p>I got a <code>data.frame</code> of training like this:</p>

<pre><code>training &lt;- data.frame(x = c(0,0,1),y = c(0,1,0), z = c(1,1,1), result =c('good','good','bad'))
training
  x y z result
1 0 0 1   good
2 0 1 1   good
3 1 0 1    bad
</code></pre>

<p>So I train my model like this:</p>

<pre><code>library(caret)
svm_mod &lt;- train(sent ~ .,df,  method = ""svmLinear"")
# There were 42 warnings (use warnings() to see them)  Some warnings, not the point of the question
</code></pre>

<p>Now let's skip the testing part, let's think that's ok.</p>

<p>Now I've the real work, i.e. predict unknown data. My problem is that the ""applying"" data can have different columns from the <code>training</code> dataset, and predicting is not always permitted:</p>

<pre><code># if the columns are the same, it's ok
applying &lt;- data.frame(x = c(0,0,1),y = c(0,1,0), z = c(0,1,1))
predict(svm_mod, applying)

# if the columns in applying are more than in train, it's ok
applying &lt;- data.frame(x = c(0,0,1),y = c(0,1,0), z = c(0,1,1), k=c(1,1,1))
predict(svm_mod, applying)

# if in applying is missing a column that is in train it does not work:
applying &lt;- data.frame(x = c(0,0,1),y = c(0,1,0))
predict(svm_mod, applying)
# Error in eval(predvars, data, env) : object 'z' not found
</code></pre>

<p>Now the solution should be to add all the missing column in training as 0s:</p>

<pre><code>applying$z &lt;- 0
</code></pre>

<p>in the <code>applying</code> dataset, but I find it not so correct/nice. Are there a correct solution to do this? I've read several question about this (my favourite is <a href=""https://stackoverflow.com/questions/50539633/error-in-evalpredvars-data-env-object-rm-not-found"">this</a>, my question is about finding a workaround about this issue).</p>

<p>My data are phrases, and I'm using document term matrix as inputs, in a production environment, this mean that's going to have newer input, without the columns in train.</p>
",Dataset Preprocessing & Handling,r caret feature training applying dataset working package training model text classification faced problem bug finding proper solution got training like train model like let skip testing part let think ok real work e predict unknown data problem applying data different column dataset predicting always permitted solution add missing column training dataset find correct nice correct solution read several question favourite href question finding workaround issue p data phrase using document term matrix input production environment mean going newer input without column train
Documents-terms matrix dimensionality reduction,"<p>I am working with text documents clustering, with a Hierarchical Clustering approach, in Python.</p>

<p>I have a corpus of 10k documents and have constructed a documents-terms matrix over a dictionary based on a collection of terms classified as 'keyword' for the entire corpus.
The matrix has a shape: [10000 x 2000] and is very sparse. (let's call it <strong>dtm</strong>)</p>

<pre><code>id    0    1    2    4    ...    1998    1999
 0    0    0    0    1    ...       0       0
 1    0    1    0    0    ...       0       1
 2    1    0    0    0    ...       1       0
 ..    ..      ...        ...      ..      ..
9999  0    0    0    0    ...       0       0
</code></pre>

<p>I think that applying some dimensionality reduction techniques could lead to an enhancement in the precision of clustering.
I have tried using some MDS approach like this</p>

<pre><code>def select_n_components(var_ratio, goal_var: float) -&gt; int:
# Set initial variance explained so far
total_variance = 0.0

# Set initial number of features
n_components = 0

# For the explained variance of each feature:
for explained_variance in var_ratio:

    # Add the explained variance to the total
    total_variance += explained_variance

    # Add one to the number of components
    n_components += 1

    # If we reach our goal level of explained variance
    if total_variance &gt;= goal_var:
        # End the loop
        break

# Return the number of components
return n_components


def do_MDS(dtm):
    # scale dtm in range [0:1] to better variance maximization
    scl = MinMaxScaler(feature_range=[0, 1])
    data_rescaled = scl.fit_transform(dtm)

    tsvd = TruncatedSVD(n_components=data_rescaled.shape[1] - 1)
    X_tsvd = tsvd.fit(data_rescaled)

    # List of explained variances
    tsvd_var_ratios = tsvd.explained_variance_ratio_

    optimal_components = select_n_components(tsvd_var_ratios, 0.95)

    from sklearn.manifold import MDS
    mds = MDS(n_components=optimal_components, dissimilarity=""euclidean"", random_state=1)
    pos = mds.fit_transform(dtm.values)

    U_df = pd.DataFrame(pos)
    U_df_transposed = U_df.T  # for consistency with pipeline workflow, export tdm matrix
    return U_df_transposed
</code></pre>

<p>The objective is to automatically detect an optimal number of components and apply the dimensionality reduction. But the output has not shown a tangible enhancement.</p>
",Dataset Preprocessing & Handling,document term matrix dimensionality reduction working text document clustering hierarchical clustering approach python corpus k document constructed document term matrix dictionary based collection term classified keyword entire corpus matrix ha shape x sparse let call dtm think applying dimensionality reduction technique could lead enhancement precision clustering tried using md approach like objective automatically detect optimal number component apply dimensionality reduction output ha shown enhancement
tokenizing a dataframe using unnest_tokens gives me error?,"<p>I am new to R and been trying to tokenize a data.frame using unnest_tokens. My data.frame was obtained from a csv file and has the following column :</p>

<blockquote>
  <p>ID, TITLE, DESCRIPTION, OUTCOMES, DATE .</p>
</blockquote>

<p>I WISH TO TOKENIZE THE TITLE, SO I MADE ANOTHER DATAFRAME :</p>

<pre><code>only_t&lt;-dplyr::select(mydata,title)
up_dat&lt;-data.frame(1:2156, document= only_t)
</code></pre>

<p>Followed by I did :</p>

<pre><code>up_dat %&gt;%
  tidytext::unnest_tokens(word, document)
</code></pre>

<p>When I did this , I get an error stating : </p>

<blockquote>
  <p>Error in check_input(x) :    Input must be a character vector of any
  length or a list of character   vectors, each of which has a length of
  1.</p>
</blockquote>

<p>Would appreciate any help in this. I been stuck here forever .</p>
",Dataset Preprocessing & Handling,tokenizing dataframe using unnest token give error new r trying tokenize data frame using unnest token data frame wa obtained csv file ha following column id title description outcome date wish tokenize title made another dataframe followed get error stating error check input x input must character vector length list character vector ha length would appreciate help stuck forever
Chunking for non-noun phrases in SpaCy,"<p>Sorry if this seems like a silly question, but I am still new to Python and SpaCy.</p>

<p>I have a data frame that contains customer complaints. It looks a bit like this:</p>

<pre><code>df = pd.DataFrame( [[1, 'I was waiting at the bus stop and then suddenly the car mounted the pavement'],
                    [2, 'When we got on the bus, we went upstairs but the bus braked hard and I fell'], 
                    [3, 'The bus was clearly in the wrong lane when it crashed into my car']], 
                    columns = ['ID', 'Text']) 
</code></pre>

<p>If I want to obtain the noun phrases, then I can do this:</p>

<pre><code>def extract_noun_phrases(text):
    return [(chunk.text, chunk.label_) for chunk in nlp(text).noun_chunks]

def add_noun_phrases(df):
    df['noun_phrases'] = df['Text'].apply(extract_noun_phrases)

add_noun_phrases(df)
</code></pre>

<p>What about if I want to extract prepositional phrases from the <code>df</code>? So, specifically trying to extract lines like:</p>

<ul>
<li><code>at the bus stop</code></li>
<li><code>in the wrong lane</code></li>
</ul>

<p>I know I am meant to be using <code>subtree</code> for this, but I don't understand how to apply it to my dataset.</p>
",Dataset Preprocessing & Handling,chunking non noun phrase spacy sorry seems like silly question still new python spacy data frame contains customer complaint look bit like want obtain noun phrase want extract prepositional phrase specifically trying extract line like know meant using understand apply dataset
"Apache OpenNLP, how to train or made new model?","<p>For example <code>en name finder</code> does not find some names, and I want to add new data to existing data model.
I found this <a href=""https://stackoverflow.com/questions/19397291/training-named-entitty-in-opennlp"">click here</a> answer, but I do not understand</p>

<p>what is <code>en-ner-person.train</code> ? is it clone of <code>en-ner-person.bin</code>?</p>

<p>We should load existing model and add new data from file? What type of info should contain <code>txt file</code>?</p>
",Dataset Preprocessing & Handling,apache opennlp train made new model example doe find name want add new data existing data model found href answer understand clone load existing model add new data file type info contain
NLTK: How to create a corpus from csv file,"<p>I have a csv file as</p>

<pre><code>col1         col2      col3

some text    someID    some value
some text    someID    some value
</code></pre>

<p>in each row, col1 corresponds to the text of an entire document. I would like to create a corpus from this csv. my aim is to use sklearn's TfidfVectorizer to compute document similarity and keyword extraction. So consider</p>

<pre><code>tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')
tfs = tfidf.fit_transform(&lt;my corpus here&gt;)
</code></pre>

<p>so then i can use</p>

<pre><code>str = 'here is some text from a new document'
response = tfidf.transform([str])
feature_names = tfidf.get_feature_names()
for col in response.nonzero()[1]:
    print feature_names[col], ' - ', response[0, col]
</code></pre>

<p>how do i create a corpus using nltk?
what form/data structure should the corpus be so that it can be supplied to the transform function?</p>
",Dataset Preprocessing & Handling,nltk create corpus csv file csv file row col corresponds text entire document would like create corpus csv aim use sklearn tfidfvectorizer compute document similarity keyword extraction consider use create corpus using nltk form data structure corpus supplied transform function
how to load word vector model binary file of glove ( cooccurence.bin )?,"<p>I am using the GloVe nlp tools for calculate the cosine distance of two words: word1 and word2 .</p>

<p>I already trained my model and got the cooccurance.bin file by runing: </p>

<pre><code>./demo.sh
</code></pre>

<p>What my question is, how should I load the coocurrence.bin file?</p>

<p>I had search by google there is no any clue about this topic.</p>

<p>Thanks.</p>
",Dataset Preprocessing & Handling,load word vector model binary file glove cooccurence bin using glove nlp tool calculate cosine distance two word word word already trained model got cooccurance bin file runing question load coocurrence bin file search google clue topic thanks
"Looking to calculate the number of items in column 1 that are within a certain date range that is in column 6, using commands","<p>So I have a CSV file with 21 columns.  Column 1 is author_ID and column 6 is publish date.  I am looking to output the total number of authors_IDs that are in the date range of 1/1/2017 0:00 to 12/31/2017 24:00.  So basically the total number of author_IDs in 2017. </p>
",Dataset Preprocessing & Handling,looking calculate number item column within certain date range column using command csv file column column author id column publish date looking output total number author id date range basically total number author id
TypeError while encoding data using label encoder in scikit learn,"<p><strong>I am not able to encode data using label encoder in scikit learn.</strong></p>

<p><code>dataset.csv</code> has two columns text and label
I am <strong>trying to read text from dataset into a list and labels into another list and adding these lists to a dataframe</strong> but it doesn't seem to work.</p>

<pre><code>from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn import decomposition, ensemble
import pandas, xgboost, numpy, string

data = open('dataset.csv').read()
labels = []
texts = []

for i ,line in enumerate(data.split(""\n"")):
    content = line.split(""\"","")
    texts.append(content[0])
    labels.append(content[1:])

trainDF = pandas.DataFrame()
trainDF['text'] = texts
trainDF['label'] = labels

train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'],trainDF['label'],test_size = 0.2,random_state = 0)
encoder = preprocessing.LabelEncoder()
train_y = encoder.fit_transform(train_y)
valid_y = encoder.fit_transform(valid_y)

count_vect = CountVectorizer(analyzer='word', token_pattern=r'\w{1,}')
count_vect.fit(trainDF['texts'])

xtrain_count =  count_vect.transform(train_x)
xvalid_count =  count_vect.transform(valid_x)

tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\w{1,}', max_features=5000)
tfidf_vect.fit(trainDF['texts'])
xtrain_tfidf =  tfidf_vect.transform(train_x)
xvalid_tfidf =  tfidf_vect.transform(valid_x)

accuracy = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)

print(accuracy)
</code></pre>

<p>Error:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/crackthumb/environments/my_env/lib/python3.6/site-packages/sklearn/preprocessing/label.py"", line 105, in _encode
    res = _encode_python(values, uniques, encode)
  File ""/home/crackthumb/environments/my_env/lib/python3.6/site-packages/sklearn/preprocessing/label.py"", line 59, in _encode_python
    uniques = sorted(set(values))
TypeError: unhashable type: 'list'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""Classifier.py"", line 21, in &lt;module&gt;
    train_y = encoder.fit_transform(train_y)
  File ""/home/crackthumb/environments/my_env/lib/python3.6/site-packages/sklearn/preprocessing/label.py"", line 236, in fit_transform
    self.classes_, y = _encode(y, encode=True)
  File ""/home/crackthumb/environments/my_env/lib/python3.6/site-packages/sklearn/preprocessing/label.py"", line 107, in _encode
    raise TypeError(""argument must be a string or number"")
TypeError: argument must be a string or number
</code></pre>
",Dataset Preprocessing & Handling,typeerror encoding data using label encoder scikit learn able encode data using label encoder scikit learn ha two column text label trying read text dataset list label another list adding list dataframe seem work error
python module to remove internet jargon/slang/acronym,"<p>Is there any python module (may be in nltk python) to remove internet slang/ chat slang like ""lol"",""brb"" etc. If not can some one provide me a CSV file comprising of such vast list of slang? </p>

<p>The website <a href=""http://www.netlingo.com/acronyms.php"" rel=""noreferrer"">http://www.netlingo.com/acronyms.php</a> gives the list of acronyms but I am not able to find any CSV files for using them in my program. </p>
",Dataset Preprocessing & Handling,python module remove internet jargon slang acronym python module may nltk python remove internet slang chat slang like lol brb etc one provide csv file comprising vast list slang website give list acronym able find csv file using program
NLP clustering documents,"<p>I am using HDBSCAN algorithm to create clusters from the documents I have. But to create a vector matrix from the words, I am using tf-idf algorithm and want to use GloVe or Word2vec(because tf-idf based on BoW, so it can`t capture semantics). </p>

<p>Which method can I use - GloV, Word2vec or any other methods that will be appropriated for text clusterization?
And how I can implement it?</p>

<p>Any help will be highly appreciated! </p>

<pre><code>nltk.download('stopwords')

title = []
synopses = []
filename = ""twitter-test-dataset.csv""
num_clusters = 10
pkl_file = ""doc_cluster.pkl""
generate_pkl = False

# pre-process data
with open(filename, 'r') as csvfile:
    # creating a csv reader object
    csvreader = csv.reader(csvfile)

    # extracting field names through first row
    fields = csvreader.next()

    # extracting each data row one by one
    duplicates = 0
    for row in csvreader:
        # removes the characters specified
        line = re.sub(r'[.,""!]+', '', row[2], flags=re.MULTILINE)
        line = re.sub(r'^RT[\s]+', '', line, flags=re.MULTILINE)  # removes RT
        line = re.sub(r'https?:\/\/.*[\r\n]*', '',
                    line, flags=re.MULTILINE)  # remove link
        line = re.sub(r'[:]+', '', line, flags=re.MULTILINE)
        line = (re.sub(
            ""(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"", "" "", line, flags=re.MULTILINE))
        line = filter(lambda x: x in string.printable,
                    line)  # filter non-ascii characers
        if line not in synopses:
            synopses.append(line)
            title.append(row[2])
        else:
            duplicates += 1

print(""Removed "" + str(duplicates) + "" rows"")


stopwords = nltk.corpus.stopwords.words('english')
stemmer = SnowballStemmer(""english"")


def tokenize_and_stem(text):
    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token
    tokens = [word for sent in nltk.sent_tokenize(
        text) for word in nltk.word_tokenize(sent)]
    filtered_tokens = []
    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)
    for**strong text** token in tokens:
        if re.search('[a-zA-Z]', token):
            filtered_tokens.append(token)
    stems = [stemmer.stem(t) for t in filtered_tokens]
    return stems


def tokenize_only(text):
    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token
    tokens = [word.lower() for sent in nltk.sent_tokenize(text)
            for word in nltk.word_tokenize(sent)]
    filtered_tokens = []
    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)
    for token in tokens:
        if re.search('[a-zA-Z]', token):
            filtered_tokens.append(token)
    return filtered_tokens


totalvocab_stemmed = []
totalvocab_tokenized = []

for i in synopses:
    # for each item in 'synopses', tokenize/stem
    allwords_stemmed = tokenize_and_stem(i)
    # extend the 'totalvocab_stemmed' list
    totalvocab_stemmed.extend(allwords_stemmed)

    allwords_tokenized = tokenize_only(i)
    totalvocab_tokenized.extend(allwords_tokenized)

vocab_frame = pd.DataFrame(
    {'words': totalvocab_tokenized}, index=totalvocab_stemmed)

# print ""there are "" + str(vocab_frame.shape[0]) + "" items in vocab_frame""


# define vectorizer parameters
tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,
                                min_df=0.0, stop_words='english',
                                use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1, 3))

#CREATE TFIDF MATRIX
tfidf_matrix = tfidf_vectorizer.fit_transform(synopses)
terms = tfidf_vectorizer.get_feature_names()


c = hdbscan.HDBSCAN(min_cluster_size=5)
#PASS TFIDF_MATRIX TO HDBSCAN
c.fit(tfidf_matrix)
print(c.labels_)
sys.exit()

</code></pre>
",Dataset Preprocessing & Handling,nlp clustering document using hdbscan algorithm create cluster document create vector matrix word using tf idf algorithm want use glove word vec tf idf based bow capture semantics method use glov word vec method appropriated text clusterization implement help highly appreciated
&quot;ValueError: multiclass format is not supported&quot; roc_auc_score,"<p>I have been trying to do sentiment analysis on twitter. I am kinda new at coding so I am going paralel with an tutorial. So my csv file basically have two columns. One of them is tweets and the other is sentiments.</p>

<pre><code>from sklearn.model_selection import train_test_split
y = train_tweets.Sentiment
X= train_tweets.Tweets
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(encoding='iso-8859-9').fit(train_X)
train_X_cv = cv.transform(train_X)

from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings(""ignore"", category=FutureWarning)
model = LogisticRegression()
model.fit(train_X_cv, train_y)

from sklearn.metrics import roc_auc_score
prediction = model.predict(cv.transform(val_X))
print('AUC: ', roc_auc_score(val_y, prediction))
</code></pre>

<p>My code looks like this. When I try to run this code I get ""ValueError: multiclass format is not supported"" this error. You have any idea what I should do? </p>

<p>Thank you!</p>
",Dataset Preprocessing & Handling,valueerror multiclass format supported roc auc score trying sentiment analysis twitter kinda new coding going paralel tutorial csv file basically two column one tweet sentiment code look like try run code get valueerror multiclass format supported error idea thank
how to use MultinomialNB model with live data?,"<p>I am new to machine Learning. I was trying to develop a sentimental analysis application for a CRM project, My program model predict follow-up status when user enters follow-up comment. For creating model, I made the program read old comments and status from a input CSV file, and convert 'Comments' to Vector. It works well on test data, But i don't know how to use this in practical case, I mean when I am trying to predict using a another input Value it shows error. 
'ValueError: dimension mismatch'
Can someone guide me how to use this in practical case?</p>

<p>Please see the main code part.</p>

<pre><code>#this program will automatically generate the follow-up status from enquiry followup comment
import numpy as np
import pandas as pd
import string
import nltk
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

#Data preprocessing

follow_ups=pd.read_csv('enquiry_followups.csv')
follow_ups.describe()
follow_ups=follow_ups.dropna()
follow_ups=follow_ups.apply(lambda x: x.astype(str).str.lower())
follow_ups['Status'].loc[(follow_ups['Status'] == 'nuetral')|(follow_ups['Status'] == 'nutral')]='neutral'
follow_ups['Status']=follow_ups['Status'].apply(lambda x: getIndexOfStatus(x))
status_count=follow_ups.groupby('Status').count()
remove_punk_dic= dict((ord(p_value),None) for p_value in string.punctuation)
lemmer=nltk.stem.WordNetLemmatizer()
cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = lem_normalize)
text_counts= cv.fit_transform(follow_ups['Comment'].tolist())#Output status 
#this is how we split the data as train set and test set
X_train, X_test, y_train, y_test = train_test_split(text_counts, follow_ups['Status'].tolist(), test_size=0.2, random_state=4)
#Import scikit-learn metrics module for accuracy calculation
# Model Generation Using Multinomial Naive Bayes
clf = MultinomialNB().fit(X_train, y_train)
predicted= clf.predict(X_test)
print(""MultinomialNB Accuracy:"",metrics.accuracy_score(y_test, predicted))
data_list=['she wont come','he didnt pickup the call']
X_test1= cv.transform(data_list)
predicted2= clf.predict(X_test1)
"""""" Functions """"""
#input status values return index
def getIndexOfStatus(status):
    status_values=['cancelled','negative','neutral','positive','registered']
    return status_values.index(status)

#lemmatize tokens
def lem_tokens(tokens):
    return [lemmer.lemmatize(token_word) for token_word in tokens]

"""""" This function will lemmatize input_corpus
1. converts input_corpus to lower letter
2. remove punctuation
3. lemmatize using lem_tokens
"""""" 
def lem_normalize(input_corpus):
   return lem_tokens(nltk.word_tokenize(input_corpus.lower().translate(remove_punk_dic)))
</code></pre>

<h1>Gettinh output error : ValueError: dimension mismatch</h1>
",Dataset Preprocessing & Handling,use multinomialnb model live data new machine learning wa trying develop sentimental analysis application project program model predict follow status user enters follow comment creating model made program read old comment status input csv file convert comment vector work well test data know use practical case mean trying predict using another input value show error valueerror dimension mismatch someone guide use practical case please see main code part gettinh output error valueerror dimension mismatch
Replace specific text with a redacted version using Python,"<p>I am looking to do the opposite of what has been done here:</p>

<pre><code>import re

text = '1234-5678-9101-1213 1415-1617-1819-hello'

re.sub(r""(\d{4}-){3}(?=\d{4})"", ""XXXX-XXXX-XXXX-"", text)

output = 'XXXX-XXXX-XXXX-1213 1415-1617-1819-hello'
</code></pre>

<p><em><a href=""https://stackoverflow.com/questions/16327590/partial-replacement-with-re-sub"">Partial replacement with re.sub()</a></em></p>

<p>My overall goal is to replace all <code>XXXX</code> within a text using a neural network. <code>XXXX</code> can represent names, places, numbers, dates, etc. that are in a .csv file.</p>

<p>The end result would look like:</p>

<pre><code>XXXX went to XXXX XXXXXX
</code></pre>

<p>Sponge Bob went to Disney World.</p>

<p>In short, I am unmasking text and replacing it with a generated dataset using fuzzy.</p>
",Dataset Preprocessing & Handling,replace specific text redacted version using python looking opposite ha done href replacement sub overall goal replace within text using neural network represent name place number date etc csv file end result would look like sponge bob went disney world short unmasking text replacing generated dataset using fuzzy
Building corpus in Quanteda while keeping track of the ID,"<p>I have a dataset in which I have multiple texts per user. I want to build a corpus of all those documents with Quanteda but without losing the ability to link back the different texts to the corresponding user.</p>

<p>I will give you a sample code to help you understand a little bit more where I am failing.</p>

<pre><code>df &lt;- data.frame('ID'=c(1,1,2), 'Text'=c('I ate apple', ""I don't like fruits"", ""I swim in the dark""), stringsAsFactors = FALSE)
df_corpus &lt;- corpus(df$Text, docnames =df$ID)
corpus_DFM &lt;- dfm(df_corpus, tolower = TRUE, stem = FALSE)
print(corpus_DFM)
</code></pre>

<p>This results in </p>

<pre><code>Document-feature matrix of: 3 documents, 10 features (60.0% sparse).
3 x 10 sparse Matrix of class ""dfm""
     features
docs  i ate apple don't like fruits swim in the dark
  1   1   1     1     0    0      0    0  0   0    0
  1.1 1   0     0     1    1      1    0  0   0    0
  2   1   0     0     0    0      0    1  1   1    1
&gt; 
</code></pre>

<p>But I would like to obtain in dataframe that looks like this in my Document-feature matrix </p>

<pre><code>
Document-feature matrix of: 3 documents, 10 features (60.0% sparse).
3 x 10 sparse Matrix of class ""dfm""
       features
docs    id  i ate apple don't like fruits swim in the dark
  text1 1   1   1     1     0    0      0    0  0   0    0
  text2 1   1   0     0     1    1      1    0  0   0    0
  text3 2   1   0     0     0    0      0    1  1   1    1
&gt; 
</code></pre>

<p>Is there a way to automatize this process using Quanteda.
I would like to modify the the docs column of the dfm object but I do not know how to have access to it.</p>

<p>Any help would be welcome!</p>

<p>Thank you.</p>
",Dataset Preprocessing & Handling,building corpus quanteda keeping track id dataset multiple text per user want build corpus document quanteda without losing ability link back different text corresponding user give sample code help understand little bit failing result would like obtain dataframe look like document feature matrix way automatize process using quanteda would like modify doc column dfm object know access help would welcome thank
Are TF-IDF and BoW techniques incompatible?,"<p>I have studied the <a href=""http://datameetsmedia.com/bag-of-words-tf-idf-explained/"" rel=""nofollow noreferrer"">difference between TF-IDF and BoW</a> methods but I have a big doubt about it. I thought that the two methods could be combined, I will explain better. I have a csv file (<code>MY_DATA</code>) with thousands of comments from a social network, I would like to use this dataset to create my <code>BoW</code> for the creation of a <code>classification model</code> of the sentiment of comments (the sentiment of comments is the other variable of <code>MY_DATA</code> and is of three types: positive, negative and neutral)</p>

<pre><code>tf = TfidfVectorizer()
text_tf = tf.fit_transform(MY_DATA['comments'])
X_train, X_test, y_train, y_test = train_test_split(text_tf, MY_DATA['sentiment'], test_size=0.2)

#Classification model Multinomial Naive Bayes
clf = MultinomialNB().fit(X_train, y_train)
predicted = clf.predict(X_test)
</code></pre>

<p>Now that you have seen my script I would like to know if I am using the TF-IDF method correctly. How could I apply the BoW method in my case? Do the two methods inevitably remain incompatible?</p>
",Dataset Preprocessing & Handling,tf idf bow technique incompatible studied difference tf idf bow method big doubt thought two method could combined explain better csv file thousand comment social network would like use dataset create creation sentiment comment sentiment comment variable three type positive negative neutral seen script would like know using tf idf method correctly could apply bow method case two method inevitably remain incompatible
Change words starting with pattern,"<p>I am analysing political speech and want to standardize some dialect words. I want to change all words starting with ""fra"" so that they start with ""fre"". </p>

<p>Example:</p>

<p><em>""frad walked into a bar""</em> becomes <em>""fred walked into a bar""</em></p>

<p><em>""are you frad""</em> becomes <em>""are you fred""</em></p>

<p><em>""are you afraid""</em> should not change, and stay the same</p>

<p>How do I do this in R?</p>

<p>The speeches are stored in a data frame together with some metadata, where the variable <strong>text</strong> stores speech for each politician within a year.</p>
",Dataset Preprocessing & Handling,change word starting pattern analysing political speech want standardize dialect word want change word starting fra start fre example frad walked bar becomes fred walked bar frad becomes fred afraid change stay r speech stored data frame together metadata variable text store speech politician within year
Unsupervised automatic tagging algorithms?,"<p>I want to build a web application that lets users upload <em>documents</em>, <em>videos</em>, <em>images</em>, <em>music</em>, and then give them an ability to search them. Think of it as <em>Dropbox</em> + Semantic Search.</p>

<p>When user uploads a new file, e.g. <strong>Document1.docx</strong>, how could I automatically generate tags based on the content of the file? In other words no user input is needed to determine what the file is about. If suppose that <strong>Document1.docx</strong> is a research paper on data mining, then when user searches for <em>data mining</em>, or <em>research paper</em>, or <em>document1</em>, that file should be returned in search results, since <em>data mining</em> and <em>research paper</em> will most likely be potential auto-generated tags for that given document.</p>

<p><em><strong>1. Which algorithms would you recommend for this problem?</em></strong></p>

<p><em><strong>2. Is there an natural language library that could do this for me?</em></strong></p>

<p><em><strong>3. Which machine learning techniques should I look into to improve tagging precision?</em></strong></p>

<p><em><strong>4. How could I extend this to video and image automatic tagging?</em></strong></p>

<p>Thanks in advance!</p>
",Dataset Preprocessing & Handling,unsupervised automatic tagging algorithm want build web application let user upload document video image music give ability search think dropbox semantic search user uploads new file e g document docx could automatically generate tag based content file word user input needed determine file suppose document docx research paper data mining user search data mining research paper document file returned search result since data mining research paper likely potential auto generated tag given document algorithm would recommend problem natural language library could machine learning technique look improve tagging precision could extend video image automatic tagging thanks advance
How to calculate tf-idf when working on .txt files in python 3.7?,"<p>I have books in pdf and I want to do NLP tasks such as preprocessing, tf-idf calculation, word2vec, etc on those books. So I converted them into .txt files and was trying to get tf-idf scores. Previously I performed tf-idf on a CSV file, so I made some changes in that code and tried to use it for .txt file. But I am unsuccessful in my attempt.</p>
<p>Below is my code:</p>
<pre><code>import pandas as pd
import numpy as np
from itertools import islice
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
data = open('jungle book.txt', 'r+')
# print(data.read())
cvec = CountVectorizer(stop_words='english', min_df=1, max_df=.5, ngram_range=(1,2))
cvec.fit(data)
list(islice(cvec.vocabulary_.items(), 20))
len(cvec.vocabulary_)
cvec_count = cvec.transform(data)
print('Sparse Matrix Shape : ', cvec_count.shape)
print('Non Zero Count : ', cvec_count.nnz)
print('sparsity: %.2f%%' % (100 * cvec_count.nnz / (cvec_count.shape[0] * cvec_count.shape[1])))

occ = np.asarray(cvec_count.sum(axis=0)).ravel().tolist()
count_df = pd.DataFrame({'term': cvec.get_feature_names(), 'occurrences' : occ})
term_freq = count_df.sort_values(by='occurrences', ascending=False).head(20)
print(term_freq)
transformer = TfidfTransformer()
transformed_weights = transformer.fit_transform(cvec_count)
weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()
weight_df = pd.DataFrame({'term' : cvec.get_feature_names(), 'weight' : weights})
tf_idf = weight_df.sort_values(by='weight', ascending=False).head(20)
print(tf_idf) 
</code></pre>
<p>This code is working until print ('Non Zero Count :', cvec_count.shape) and printing:</p>
<blockquote>
<p>Sparse Matrix Shape :  (0, 7132)</p>
<p>Non Zero Count :  0</p>
</blockquote>
<p>Then it is giving error:</p>
<blockquote>
<p>ZeroDivisionError: division by zero</p>
</blockquote>
<p>Even if I run this code with ignoring ZeroDivisionError, still it is wrong as it is not counting any frequencies.</p>
<p>I have no idea how to work around .txt file. What is the proper way to work on .txt file for NLP tasks?</p>
<p>Thanks in advance!</p>
",Dataset Preprocessing & Handling,calculate tf idf working txt file python book pdf want nlp task preprocessing tf idf calculation word vec etc book converted txt file wa trying get tf idf score previously performed tf idf csv file made change code tried use txt file unsuccessful attempt code code working print non zero count cvec count shape printing sparse matrix shape non zero count giving error zerodivisionerror division zero even run code ignoring zerodivisionerror still wrong counting frequency idea work around txt file proper way work txt file nlp task thanks advance
Merging multiple pandas dataframes by common STRINGS in a column,"<p>I have 6 csv files in which one column is a sentence and the second column is an integer. </p>

<p>The sentences are the same across all csv files, but they are out of key order from file to file. </p>

<p>I want to merge all data frames by sentence, so that I have one column of the sentences and then each integer column associated with that sentence from each csv file. </p>

<p>I've tried various merging and reducing techniques by the common 'sentence' column, but I end up with orders of magnitude more rows than I should have. </p>

<p>For example:</p>

<pre><code>data_frames = [df1, df2, df3, df4, df5, df6]
reduce(lambda x,y: pd.merge(x,y, on='sentence', how='inner'), data_frames)
</code></pre>

<p>results in a dataframe with 12,502,455 rows!! I only have 4,825 rows in each csv file.</p>

<p>even using:</p>

<pre><code>pd.merge(df1,df2, on='sentence', how='inner')
</code></pre>

<p>results in a dataframe with 5295 rows. </p>

<p>I know all the sentences are identical across csv files because I uploaded the same csv file of sentences to mTurk to be labeled. </p>
",Dataset Preprocessing & Handling,merging multiple panda dataframes common string column csv file one column sentence second column integer sentence across csv file key order file file want merge data frame sentence one column sentence integer column associated sentence csv file tried various merging reducing technique common sentence column end order magnitude row example result dataframe row row csv file even using result dataframe row know sentence identical across csv file uploaded csv file sentence mturk labeled
What do the columns of matrix in hidden layer represent in skip-gram model,"<p>I can't find anywhere what do the columns of the matrix represent in the hidden layer in skip-gram model. </p>

<p>If the rows of the matrix represent the words then columns should also represent the words or contexts like documents? Can anyone tell me if the above statement is correct? As most of the papers use the term ""features"" and i am not sure what do they mean by feature.</p>

<p>I have added an image below to clarify the matrix i am talking about.</p>

<p><a href=""https://i.sstatic.net/X4pYI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/X4pYI.png"" alt=""enter image description here""></a></p>
",Dataset Preprocessing & Handling,column matrix hidden layer represent skip gram model find anywhere column matrix represent hidden layer skip gram model row matrix represent word column also represent word context like document anyone tell statement correct paper use term feature sure mean feature added image clarify matrix talking
Best way to store processed text data for streaming to gensim?,"<p>I've got several hundred pandas data frames, each of which has a column of very long strings that need to be processed/sentencized and finally tokenized before modeling with word2vec.</p>

<p>I can store them in any format on the disk, before I build a stream to pass them to gensim's word2vec function. </p>

<p>What format would be best, and why?  The most important criterion would be performance vis-a-vis training (which will take many days), but coherent structure to the filesystem would also be nice.  </p>

<p>Would it be crazy to store several million or maybe even a few billion text files containing one sentence each?  Or perhaps some sort of database?  If this was numerical data I'd use hdf5.  But it's text.  The cleanest would be to store them in the original data frames, but that seems less ideal from an i/o perspective, because I'd have to load each data frame (largish) every epoch.  </p>

<p>What makes the most sense here?</p>
",Dataset Preprocessing & Handling,best way store processed text data streaming gensim got several hundred panda data frame ha column long string need processed sentencized finally tokenized modeling word vec store format disk build stream pas gensim word vec function format would best important criterion would performance vi vi training take many day coherent structure filesystem would also nice would crazy store several million maybe even billion text file containing one sentence perhaps sort database wa numerical data use hdf text would store original data frame seems le ideal perspective load data frame largish every epoch make sense
How to extract Currency Values from Multiple Strings stored in different ways?,"<p>I have multiple types of strings in a dataframe's colume like : </p>

<p>""Dear ... INR 2,36,22,217.29 on Date 2018-12-22 11:33:42 AM ..... No 8356111112294.....""</p>

<p>with the currency being written broadly in INR XX, Rs. XX,Rs.XX,INRXX,RS XX , etc and other ways with/without commas like 2,30,000 and 230000 . </p>

<p>So I basically want to extract the Currency value i.e. 230000 in a new column in the data frame in R. </p>

<p>How can I do it for multiple strings with different sentence structures as well as different ways in which currency is written as explained above. </p>

<p>ALso I am a beginner so please explain the best way. 
Also if It's possible to solve this via ML (which I dont know how to use) then please lemme know.</p>
",Dataset Preprocessing & Handling,extract currency value multiple string stored different way multiple type string dataframe colume like dear inr date currency written broadly inr xx r xx r xx inrxx r xx etc way without comma like basically want extract currency value e new column data frame r multiple string different sentence structure well different way currency written explained also beginner please explain best way also possible solve via ml dont know use please lem know
Processing a Corpus For a word2vec Implementation,"<p>As part of a class project, I'm trying to write a word2vec implementation in Python and train it on a corpus of ~6GB. I'm trying to code a reasonably optimized solution so I don't have to let my PC sit for days.</p>

<p>Going through the C word2vec source code, I notice that there, each thread reads words from a file, and takes the time to look up the index of every word. At the end, it stores a ""sentence"" of word indexes.</p>

<p>Wouldn't it be logical to translate the whole corpus into one containing integer indexes of the appropriate words? That way, time isn't lost during training on hash-table lookups, while the translation process is a one-time expense.</p>

<p>I understand that for extremely large corpuses, you are effectively doubling the amount it takes on disk, which you might want to avoid.</p>

<p>However, if you do have the memory, wouldn't this offer a noticeable increase in efficiency? Or am I just overestimating the impact of a table lookup?</p>
",Dataset Preprocessing & Handling,processing corpus word vec implementation part class project trying write word vec implementation python train corpus gb trying code reasonably optimized solution let pc sit day going c word vec source code notice thread read word file take time look index every word end store sentence word index logical translate whole corpus one containing integer index appropriate word way time lost training hash table lookup translation process one time expense understand extremely large corpus effectively doubling amount take disk might want avoid however memory offer noticeable increase efficiency overestimating impact table lookup
How to do dependency parsing with custom lexicons,"<p>I have a load of medical text. I also have two lexicons. One contains a list of standardised anatomical locations eg <code>anatomy&lt;-c(""oesophagus"", ""stomach"",""duodenum"", ""pylorus"", ""antrum"")</code> as well as a list of events <code>events&lt;-c(""clip"",""inject"",""emr"",""rfa"")</code>.  The lists are actually much longer but I'm simplifying it here.</p>

<p>I would like to do dependency parsing on the medical text using the two lexicons so I can pick up events related to medical sites..for example:</p>

<p><strong>Input text</strong></p>

<p>The pylorus looked indurated. It underwent emr and rfa. The stomach started to bleed so a clip was placed. The bleeding stopped. Finally the oesophagus looked normal but the duodenum required a further clip</p>

<p><strong>Desired output 1</strong> </p>

<p>which should give me:
 pylorus:emr, pylorus:rfa, stomach:clip, duodenum:clip</p>

<p>I think I have understood that I need to train a model using the lexicons and that this model needs to be provided in CONLL-U format (according to the <a href=""https://bnosac.github.io/udpipe/docs/doc3.html"" rel=""nofollow noreferrer""><code>udpipe</code></a> package. documentation). I might be wrong in assuming this however.</p>

<p>I have tried following the instructions but I do not know how to create the CONLL-U file in the first place using my lexicons. If I have misinterpreted how to do my custom dependency parsing please can you direct me to an explanation in r?...</p>

<p>As a further example, I have created a function in R that extracts elements from two lists if they are present in the same sentence, and another function if they are in adjoining functions. I have created this as part of a package I have written but here is the code:</p>

<p><strong>Dependency parsing attempts</strong></p>

<pre><code>#' See if words from two lists co-exist within a sentence 
#'
#' See if words from two lists co-exist within a sentence. Eg site and tissue type.
#' This function only looks in one sentence for the two terms. If you suspect the terms may
#' occur in adjacent sentences then use the EntityPairs_TwoSentence function.
#' @keywords PathPairLookup
#' @param inputText The relevant pathology text column
#' @param list1 First list to refer to
#' @param list2 The second list to look for
#' @importFrom purrr flatten_chr map_chr map map_if
#' @export
#' @family Basic Column mutators 
#' @examples # tbb&lt;-EntityPairs_OneSentence(Mypath$Histology,HistolType(),LocationList())

EntityPairs_OneSentence&lt;-function(inputText,list1,list2){

  #dataframe&lt;-data.frame(dataframe,stringsAsFactors = FALSE)
  list1&lt;-paste0(unlist(list1,use.names=F),collapse=""|"")
  list2&lt;-paste0(unlist(list2,use.names=F),collapse=""|"")

  #text&lt;-textPrep(inputText)
  text&lt;-standardisedTextOutput&lt;-stri_split_boundaries(inputText, type=""sentence"")
  r1 &lt;-lapply(text,function(x) Map(paste, str_extract_all(tolower(x),tolower(list2)), 
                                   str_extract_all(tolower(x),tolower(list1)), MoreArgs = list(sep="":"")))

  r1&lt;-lapply(r1,function(x) unlist(x))
  #Unlist into a single row-This should output a character vector
  out&lt;-lapply(r1,function(x) paste(x,collapse="",""))

  return(out)
}

#' Look for relationships between site and event
#' 
#' This is used to look for relationships between site and event especially for endoscopy events
#' where sentences such as 'The stomach polyp was large. It was removed with a snare' ie the therapy
#' and the site are in two different locations.
#' @keywords Find and replace
#' @param inputString The relevant pathology text column
#' @param list1 The intial list to assess
#' @param list2 The other list to look for
#' @importFrom stringr str_replace_na str_c str_split str_which str_extract_all regex str_subset
#' @importFrom stringi stri_split_boundaries
#' @importFrom rlang is_empty
#' @importFrom purrr flatten_chr map_chr map map_if
#' @export
#' @family Basic Column mutators 
#' @examples # tbb&lt;-EntityPairs_TwoSentence(Myendo$Findings,EventList(),HistolType())

EntityPairs_TwoSentence&lt;-function(inputString,list1,list2){

  #Prepare the text to be back into a tokenised version.
  #text&lt;-textPrep(inputText)
  text&lt;-standardisedTextOutput&lt;-stri_split_boundaries(inputString, type=""sentence"")
  text&lt;-lapply(text,function(x) tolower(x))


  #Some clean up to get rid of white space- all of this prob already covered in the ColumnCleanUp function but for investigation later
  text&lt;-lapply(text,function(x) gsub(""[[:punct:]]+"","" "",x))
  #Prepare the list to use as a lookup:
  tofind &lt;-paste(tolower(list2),collapse=""|"")

  #Prepare the second list to use as a lookup
  EventList&lt;-unique(tolower(unlist(list1,use.names = FALSE)))


  text&lt;-sapply(text,function(x) {

    #Cleaning
    x&lt;-trimws(x)



    #Prepare the text so that all empty text is replaced with NA and 
    #ready for processing
    try(words &lt;-
          x %&gt;%
          unlist() %&gt;%
          str_replace_na()%&gt;%
          str_c(collapse = ' ') %&gt;%
          str_split(' ') %&gt;%
          `[[`(1))


    words&lt;-words[words != """"] 
    x1 &lt;- str_extract_all(tolower(x),tolower(paste(unlist(list1), collapse=""|"")))
    i1 &lt;- which(lengths(x1) &gt; 0)


    try(if(any(i1)) {
      EventList %&gt;%
        map(
          ~words %&gt;%
            str_which(paste0('^.*', .x)) %&gt;%
            map_chr(
              ~words[1:.x] %&gt;%
                str_c(collapse = ' ') %&gt;%

                str_extract_all(regex(tofind, ignore_case = TRUE)) %&gt;%
                map_if(rlang::is_empty, ~ NA_character_) %&gt;%
                flatten_chr()%&gt;%
                `[[`(length(.)) %&gt;%

                .[length(.)]
            ) %&gt;%
            paste0(':', .x)
        ) %&gt;%
        unlist() %&gt;%
        str_subset('.+:')

    } else """")

  }
  )
  return(text)
}
</code></pre>

<p><strong>Lexicons for the functions above</strong></p>

<p>The lexicons I am using for the two functions above are:</p>

<pre><code>EventList &lt;- function() {
  Event &lt;- list(
    ""radiofrequency ablation"" = ""RFA"",
    ""(argon plasma coagulation)|apc"" = ""APC"",
    ""halo| rfa"" = ""RFA"",
    ""dilatation|dilated"" = ""dilat"",
    ""clip"" = ""clip"",
    ""grasp"" = ""grasp"",
    ""iodine"" = ""iodine"",
    ""acetic"" = ""acetic"",
    ""NAC"" = ""NAC"",
    ""Brushings"" = ""brushings"",
    "" cryo"" = ""cryablation""
  )

  # To get the list as a list of values only in a regex use
  # paste0(unlist(Event,use.names=F),collapse=""|"")

  return(Event)
}


HistolType &lt;- function() {

  # First standardise the terms

  tissue &lt;- list(
    ""Resection"" = ""Resection"",
    ""bx|biopsies"" = ""Biopsy"",
    ""(endoscopic mucosal resection)|(endoscopic mucosectomy)"" = ""EMR"",
    ""endoscopic submucosal (dissection|resection)"" = ""ESD"",
    ""nodul"" = ""nodul"",
    ""polyp"" = ""polyp"",
    ""emr"" = ""EMR""
  )


  # To get the list as a list of values only in a regex use
  # paste0(unlist(HistolType(),use.names=F),collapse=""|"")

  return(tissue)
}
</code></pre>

<p><strong>A larger synthetic example dataset is given here:</strong></p>

<pre><code>c(""OESOPHAGUS: tight proximal anastomotic stricture - dilated 20mm\n,STOMACH: minimal non-erosive gastritis - CLO test negative\n,Flumazenil given\n,This couldn't be resolved with the coag graspers so 2 clips were deploted over the vessel and 1:10,000 adrenaline x10ml was injected with haemostasis achieved\n,Oesophagus and stomach normal\n,Stomach- Antrum Gastritis- Nodular\n,Normal duodenum\n,There was loss of pit pattern on surface,STOMACH: significant amount of food residue in the stomach, therefore the procedure was stopped due to risk of aspiration and poor views\n,Examined under white light and NBI\n\n\n"",
 Duodenum treated with HALO RFA Channel at 12J\n,Sliding hiatus hernia 3cm, GOJ biopsies taken\n,Intubation of efferent limb for a length of the scope\n,Linear erosions in oesophagus\n,OESOPHAGUS: Normal to GOJ at 41 cm\n,\n,Treated with HALO 90 RFA at 12J\n\n\n""
    )
</code></pre>

<p><strong>And the desired output 2:</strong></p>

<pre><code>[[1]]
[1] ""Oesophagus:dilat""  ""stomach:clip""   ""stomach:grasp"" 

[[2]]
[1] ""Duodenum:rfa"" ""Oesophagus:rfa""
</code></pre>
",Dataset Preprocessing & Handling,dependency parsing custom lexicon load medical text also two lexicon one contains list standardised anatomical location eg well list event list actually much longer simplifying would like dependency parsing medical text using two lexicon pick event related medical site example input text pylorus looked indurated underwent emr rfa stomach started bleed clip wa placed bleeding stopped finally oesophagus looked normal duodenum required clip desired output give pylorus emr pylorus rfa stomach clip duodenum clip think understood need train model using lexicon model need provided conll u format according package documentation might wrong assuming however tried following instruction know create conll u file first place using lexicon misinterpreted custom dependency parsing please direct explanation r example created function r extract element two list present sentence another function adjoining function created part package written code dependency parsing attempt lexicon function lexicon using two function larger synthetic example dataset given desired output
Extracting all paragraph headings in a text file using Python/NLP,"<p>I have a text file with content shown below. I need to identify paragraph headings and create a csv file column heading from each extracted paragraph heading. The text file looks like the text block below. I was thinking of using a rule like: </p>

<pre><code>if (capitalized) and heading_length &lt;50:
    return heading_text
</code></pre>

<p>Is there something in NLTK or NLP that could help do this without an approximate way of just checking capitalized letters and word length?</p>

<p>This is an old Kaggle competition</p>

<blockquote>
  <p>DUTIES</p>
  
  <p>A 311 Director is responsible for the successful operation and
  expansion of the 311 Call Center in the Information Technology Agency
  (ITA) which answers call from constituents regarding Citywide services
  provided by City departments; works to ensure the efficient and
  effective resolution of any issues that may arise; plans, directs,
  hires, coaches, and coordinates a large staff of professional,
  technical and clerical employees engaged in the implementation,
  administration, and operations of the City's 311 Call Center; applies
  sound supervisor principles and techniques in building and maintaining
  and effective work force; fulfills equal opportunity responsibilities;
  and does related work.</p>
  
  <p>REQUIREMENTS</p>
  
  <ol>
  <li>One year of full-time paid experience as a Senior Management Analyst with the City of Los Angeles or in a class which is at least
  at the level which provides professional experience in supervisory or
  managerial work relating to a call center with at least 50 call agents
  or a call center that receives at least one million calls annually; or</li>
  <li>A Bachelor's degree from a recognized college or university and four years of full-time paid experience in a call center environment
  with at least 50 call agents or a call center that receives at least
  one million calls annually, two years of which must be supervising
  staff working at such a call center; or</li>
  <li>Eight years of full-time paid experience in a call center environment with at least 50 call agents or call center that receives
  at least one million calls annually, two years of which must be
  supervising staff working at such a call center.</li>
  </ol>
  
  <p>NOTES:</p>
  
  <ol>
  <li>In addition to the regular City application, all applicants must complete a 311 Director Qualifications Questionnaire at the time of
  filing.  The 311 Director Qualifications Questionnaire is located
  within the Qualifications Questions section of the City application. 
  Applicants who fail to complete the Qualifications Questionnaire will
  not be considered further in this examination, and their application
  will not be processed.</li>
  <li>Applicants who lack six months or less of the required experience may file for this examination. However, they cannot be appointed until
  the full experience requirement is met.</li>
  <li>Call center experience related to sales and telemarketing is excluded.</li>
  <li>Customer Relations Management (CRM) systems expertise, including implementation, integration, and knowledge base creation is highly
  desired. </li>
  </ol>
  
  <p>WHERE TO APPLY</p>
  
  <p>Applications will only be accepted online. When you are viewing the
  online job bulletin of your choice, simply scroll to the top of the
  page and select the ""Apply"" icon. Online job bulletins are also
  available at <a href=""http://agency.governmentjobs.com/lacity/default.cfm"" rel=""nofollow noreferrer"">http://agency.governmentjobs.com/lacity/default.cfm</a> for
  Open Competitive Examinations and at
  <a href=""http://agency.governmentjobs.com/lacity/default.cfm?promotionaljobs=1"" rel=""nofollow noreferrer"">http://agency.governmentjobs.com/lacity/default.cfm?promotionaljobs=1</a>
  for Promotional Examinations. </p>
  
  <p>NOTE:</p>
  
  <p>Should a large number of qualified candidates file for this
  examination, an expert review committee may be assembled to evaluate
  each candidate's qualifications for the position of 311 Director. In
  this evaluation, the expert review committee will assess each
  applicant's training and experience based upon the information in the
  applicant's City employment application and the Qualifications
  Questionnaire. Those candidates considered by the expert review
  committee as possessing the greatest likelihood of successfully
  performing the duties of a 311 Director, based solely on the
  information presented to the committee, will be invited to participate
  in the interview.</p>
</blockquote>
",Dataset Preprocessing & Handling,extracting paragraph heading text file using python nlp text file content shown need identify paragraph heading create csv file column heading extracted paragraph heading text file look like text block wa thinking using rule like something nltk nlp could help without approximate way checking capitalized letter word length old kaggle competition duty director responsible successful operation expansion call center information technology agency ita answer call constituent regarding citywide service provided city department work ensure efficient effective resolution issue may arise plan directs hire coach coordinate large staff professional technical clerical employee engaged implementation administration operation city call center applies sound supervisor principle technique building maintaining effective work force fulfills equal opportunity responsibility doe related work requirement one year full time paid experience senior management analyst city los angeles class least level provides professional experience supervisory managerial work relating call center least call agent call center receives least one million call annually bachelor degree recognized college university four year full time paid experience call center environment least call agent call center receives least one million call annually two year must supervising staff working call center eight year full time paid experience call center environment least call agent call center receives least one million call annually two year must supervising staff working call center note addition regular city application applicant must complete director qualification questionnaire time filing director qualification questionnaire located within qualification question section city application applicant fail complete qualification questionnaire considered examination application processed applicant lack six month le required experience may file examination however appointed full experience requirement met call center experience related sale telemarketing excluded customer relation management system expertise including implementation integration knowledge base creation highly desired apply application accepted online viewing online job bulletin choice simply scroll top page select apply icon online job bulletin also available open competitive examination promotional examination note large number qualified candidate file examination expert review committee may assembled evaluate candidate qualification position director evaluation expert review committee ass applicant training experience based upon information applicant city employment application qualification questionnaire candidate considered expert review committee possessing greatest likelihood successfully performing duty director based solely information presented committee invited interview
Reading Hong Kong Supplementary Character Set in python 3,"<p>I have a hkscs dataset that I am trying to read in python 3. Below code</p>

<pre><code>encoding = 'big5hkscs'
lines = []
num_errors = 0
for line in open('file.txt'):
    try:
        lines.append(line.decode(encoding))
    except UnicodeDecodeError as e:
        num_errors += 1
</code></pre>

<p>It throws me error <code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0xae in position 0: invalid start byte</code>. Seems like there is a non utf-8 character in the dataset that the code is not able to decode. </p>

<p>I tried adding <code>errors = ignore</code> in this line
<code>lines.append(line.decode(encoding, errors='ignore'))</code></p>

<p>But that does not solve the problem.</p>

<p>Can anyone please suggest?</p>
",Dataset Preprocessing & Handling,reading hong kong supplementary character set python hkscs dataset trying read python code throw error seems like non utf character dataset code able decode tried adding line doe solve problem anyone please suggest
Is it possible to train a model on StanfordNLP for Python and use it in the Java-based CoreNLP?,"<p>StanfordNLP has two main libraries: one for Java and another for Python. For Java, it contains the <a href=""https://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">CoreNLP package</a>. However, I have a dependency parsing model trained in the Python's version of the <a href=""https://stanfordnlp.github.io/stanfordnlp/index.html"" rel=""nofollow noreferrer"">Stanfordnlp library</a>. </p>

<p>My main question: is it possible to load the Python's trained package using CoreNLP?</p>

<p>I checked their documentations and tutorials and found nothing on the compatibility between the two libraries. I know that Python's training is done through neural networks, and although the Java version doesn't necessarily use NNs, StanfordNLP does have a <a href=""https://nlp.stanford.edu/software/nndep.html"" rel=""nofollow noreferrer"">Neural Network implementation</a> for Java. This led me to believe that there could be some way for me to load the Python's model on Java. By the way, the extension of the files are different, Python version outputs a .dt file and for java a .gz is used. I thought about compression the .dt file, but it doesn't work. </p>
",Dataset Preprocessing & Handling,possible train model stanfordnlp python use java based corenlp stanfordnlp ha two main library one java another python java contains corenlp package however dependency parsing model trained python version stanfordnlp library main question possible load python trained package using corenlp checked documentation tutorial found nothing compatibility two library know python training done neural network although java version necessarily use nns stanfordnlp doe neural network implementation java led believe could way load python model java way extension file different python version output dt file java gz used thought compression dt file work
Iterate over each row in pandas data frame and make changes to it,"<p>I'm building a text classification algorithm, for which (in pandas data frame) I need to go through each row of a particular column, each row of the column contains a group of strings. I need to check each word in the row and see if it is present in a pre-defined dictionary. If it is, I will do some operation with that word. Else, I will remove that word from string.
Here's the code I used for it:</p>

<pre><code>onehotmess=[]
for idx,row in df.iterrows():
    sum=np.zeros(dim,dtype=float)
    for acr in row['message']:
        index=np.where(Acr_array==acr)
        if len(index)==0:
            messages=messages.str.replace(acr,"""",case=False)
        else:
            sum=sum+onehot_encoded[index]
            onehotmess.append(sum)
</code></pre>

<p>But it's taking a lot of time, and eventually, my system got crashed. Is it a correct implementation. If so, I need some other efficient method.
If not, please help with the correct code.
The dictionary (Acr_array) contains acronyms:</p>

<blockquote>
  <p>array(['ABN', 'ABV', 'ACC', 'ACCUM', 'ACFT', 'ACR', 'ACT', 'ADJ'...
  'WKDAYS', 'WKEND', 'WND', 'WPT','WSR', 'WTR', 'WX'])</p>
</blockquote>

<p>Sample input:</p>

<blockquote>
  <p>TWY G BTN RWY CC AND RWY LR WIP CONST</p>
</blockquote>

<p>This is present in a row under 'message' column in pandas data frame.</p>

<p>Output:</p>

<blockquote>
  <p>OneHot_encoded[idx(TWY)] OneHot_encoded[idx(BTN)]
  OneHot_encoded[idx(RWY)] OneHot_encoded[idx(AND)]
  OneHot_encoded[idx(RWY)] OneHot_encoded[idx(WIP)]</p>
</blockquote>

<p>Output doesn't have G,CC,LR,CONST as they are not in Acr_array. idx('') is position of that acronym in Acr_array. So the output contains the value at that position in OneHot_encoded.</p>
",Dataset Preprocessing & Handling,iterate row panda data frame make change building text classification algorithm panda data frame need go row particular column row column contains group string need check word row see present pre defined dictionary operation word else remove word string code used taking lot time eventually system got crashed correct implementation need efficient method please help correct code dictionary acr array contains acronym array abn abv acc accum acft acr act adj wkdays wkend wnd wpt wsr wtr wx sample input twy g btn rwy cc rwy lr wip const present row message column panda data frame output onehot encoded idx twy onehot encoded idx btn onehot encoded idx rwy onehot encoded idx onehot encoded idx rwy onehot encoded idx wip output g cc lr const acr array idx position acronym acr array output contains value position onehot encoded
Get filtered co-occurrences from a sparse matrix,"<p>I have a scipy csr sparse matrix (a document-term matrix created with scikit-learn’s CountVectorizer).</p>

<p>The matrix is huge (78M tokens across 9.5M documents):</p>

<pre><code>&lt;9482138x78045191 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
with 394161806 stored elements in Compressed Sparse Row format&gt;
</code></pre>

<p>I want to reduce the matrix so that it contains only some of the tokens (columns). I have these in a list, and can extract them into a new matrix like this:</p>

<pre><code>tokenarrays = []
for tokenid in tokenids_we_want[:3]:
    tokenarray = dtm.getcol(tokenid).toarray()
        tokenarrays.append(tokenarray)
</code></pre>

<p>Maybe there is a better way, using some kind of filtering method.</p>

<p>My end goal is a co-occurrence matrix of the wanted tokens. As a first step towards this, I have created a pandas dataframe using:</p>

<pre><code>pd.DataFrame(zip(tokenids_we_want, tokenarrays))
</code></pre>

<p>This does not result in one pandas column per token in the tokenarray, however.
There is probably a better way altogether to do what I need:</p>

<ul>
<li>Start from a sparse document-term matrix</li>
<li>Keep only some of the terms (tokens)</li>
<li>Get term co-occurrences</li>
</ul>
",Dataset Preprocessing & Handling,get filtered co occurrence sparse matrix scipy csr sparse matrix document term matrix created scikit learn countvectorizer matrix huge token across document want reduce matrix contains token column list extract new matrix like maybe better way using kind filtering method end goal co occurrence matrix wanted token first step towards created panda dataframe using doe result one panda column per token tokenarray however probably better way altogether need start sparse document term matrix keep term token get term co occurrence
Loading Wikipedia XML files into Gensim,"<p>I'm a complete novice to NLP and would like to load a zipped XLM file of the Hungarian Wikipedia corpus (807 MB). I downloaded the dumpfile and started parsing it in Python with Gensim, but after 4 hours my laptop crashed, complaining that I had run out of RAM. I have a fairly old laptop (4GB RAM) and was wondering whether there is any way I could solve this problem by </p>

<ul>
<li>(1) either  tinkering with my code, e.g, by reducing the corpus by taking, say, a 1/10th random sample of it; </li>
<li>(2) or using some cloud platform to enhance my CPU power. I read in <a href=""https://stackoverflow.com/questions/32543235/python-gensim-memory-error"">this SO post</a> that AWS can be used for such puposes, but I am unsure which service I should select (Amazon EC2?). I also checked Google Colab, but got confused that it lists hardware acceleration options (GPU and CPU) in the context of Tensorflow, and I am not sure if that is suitable for NLP. I didn't find any posts about that. </li>
</ul>

<p>Here's my Jupyter Notebook code that I've tried after downloading the wikipedia dumps from <a href=""https://dumps.wikimedia.org/huwiki/latest/huwiki-latest-pages-articles.xml.bz2"" rel=""nofollow noreferrer"">here</a>: </p>

<pre><code>! pip install gensim 
from nltk.stem import SnowballStemmer
from gensim.corpora import WikiCorpus
from gensim.models.word2vec import Word2Vec

hun_stem = SnowballStemmer(language='hungarian')

%%time
hun_wiki = WikiCorpus(r'huwiki-latest-pages-articles.xml.bz2')
hun_articles = list(hun_wiki.get_texts())
len(hun_articles)
</code></pre>

<p>Any guidance would be much appreciated. </p>
",Dataset Preprocessing & Handling,loading wikipedia xml file gensim complete novice nlp would like load zipped xlm file hungarian wikipedia corpus mb downloaded dumpfile started parsing python gensim hour laptop crashed complaining run ram fairly old laptop gb ram wa wondering whether way could solve problem either tinkering code e g reducing corpus taking say th random sample using cloud platform enhance cpu power read guidance would much appreciated
Memory efficiently loading of pretrained word embeddings from fasttext library with gensim,"<p>I would like to load pretrained multilingual word embeddings from the fasttext library with gensim; here the link to the embeddings:</p>

<p><a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a></p>

<p>In particular, I would like to load the following word embeddings: </p>

<ul>
<li>cc.de.300.vec (4.4 GB) </li>
<li>cc.de.300.bin (7 GB)</li>
</ul>

<p>Gensim offers the following two options for loading fasttext files:</p>

<ol>
<li><p><code>gensim.models.fasttext.load_facebook_model(path, encoding='utf-8')</code>    </p>

<blockquote>
  <ul>
  <li><em>Load the input-hidden weight matrix from Facebook’s native fasttext
  .bin output file.</em></li>
  <li><em>load_facebook_model() loads the full model, not just
  word embeddings, and enables you to continue model training.</em></li>
  </ul>
</blockquote></li>
<li><p><code>gensim.models.fasttext.load_facebook_vectors(path, encoding='utf-8')</code></p>

<blockquote>
  <ul>
  <li><em>Load word embeddings from a model saved in Facebook’s native fasttext .bin format.</em></li>
  <li><em>load_facebook_vectors() loads the word embeddings only. Its faster, but does not enable you to continue training.</em></li>
  </ul>
</blockquote></li>
</ol>

<p>Source Gensim documentation: 
<a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model</a></p>

<p>Since my laptop has only 8 GB RAM, I am continuing to get MemoryErrors or the loading takes a very long time (up to several minutes).</p>

<p>Is there an option to load these large models from disk more memory efficient?</p>
",Dataset Preprocessing & Handling,memory efficiently loading pretrained word embeddings fasttext library gensim would like load pretrained multilingual word embeddings fasttext library gensim link embeddings particular would like load following word embeddings cc de vec gb cc de bin gb gensim offer following two option loading fasttext file load input hidden weight matrix facebook native fasttext bin output file load facebook model load full model word embeddings enables continue model training load word embeddings model saved facebook native fasttext bin format load facebook vector load word embeddings faster doe enable continue training source gensim documentation since laptop ha gb ram continuing get memoryerrors loading take long time several minute option load large model disk memory efficient
How to implement incremental learning in NLP,"<p>We are building a system wherein, we would have a initial very small amount of trained data to start with.
The job is to Classify the incoming data(Document, for our case) into 2 categories: Category A &amp; B.
Data is document , so the user needs to classify the Document to belong to Category A or B.
So, with the limited amount of data, we create the trained data set and we  start predicting the Category of next Document using the trained data set. </p>

<p>Now if the prediction is correct user moves to the next Document.
But if the prediction is incorrect , the user inputs the correct Category (Lets Say Category A was predicted by the system, wherein the correct assignment to the data should be Category B). So now the system should use this learning(Category B instead of Category A) to enrich(learn) itself in near real time.</p>

<p>It should train only the added data and not the complete dataset , which is already trained. So it should be incremental learning.
For classification we would be applying Naive Bayes Classification.</p>

<p>Now the question is :</p>

<ul>
<li>How do we implement the incremental training, without training the
whole data set every time? </li>
<li>I know there are incremental learning libraries like Vowpal Wabbit &amp;
creme. Will it be a good solution using these library for my case?</li>
</ul>
",Dataset Preprocessing & Handling,implement incremental learning nlp building system wherein would initial small amount trained data start job classify incoming data document case category category b data document user need classify document belong category b limited amount data create trained data set start predicting category next document using trained data set prediction correct user move next document prediction incorrect user input correct category let say category wa predicted system wherein correct assignment data category b system use learning category b instead category enrich learn near real time train added data complete dataset already trained incremental learning classification would applying naive bayes classification question implement incremental training without training whole data set every time know incremental learning library like vowpal wabbit creme good solution using library case
How to change language of termDocumentmatrix in R text-mining?,"<h2>I need to change language to Turkish in the function of termDocumentmatrix. Could you possibly help me?</h2>
<h1>this code works. I reach result what i want for stemming, stopwords etc for Turkish.</h1>
<pre><code>dat&lt;-&quot;BirGün, Türkiye'de günlük olarak yayımlanan ulusal bir gazete.
Gazetenin yazı işleri müdürü Berkant Gültekin, yayın danışmanı Barış İnce, 
sorumlu müdürü Cansever Uğur ve haber koordinatörü İbrahim Varlı'dır. Yayın 
hayatına 14 Nisan 2004'te başlayan gazetenin sahibi Birgün Yayıncılık ve İletişim Ticaret AŞ'd&quot;

dat%&gt;% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)%&gt;%
  tokens_remove(stopwords(&quot;tr&quot;, source = &quot;stopwords-iso&quot;)) %&gt;%
  tokens_wordstem(language = &quot;turkish&quot;) %&gt;%
  tokens_tolower() 

Result:

[1] &quot;birg&quot;        &quot;türkiye'&quot;    &quot;günlük&quot;      &quot;yayımlana&quot;   &quot;ulusal&quot;      &quot;gaze&quot;        &quot;gazete&quot;      &quot;yaz&quot;         &quot;iş&quot;         
[10] &quot;müdür&quot;       &quot;berkant&quot;     &quot;gültek&quot;      &quot;yay&quot;         &quot;danışma&quot;     &quot;barış&quot;       &quot;ince&quot;        &quot;sorumlu&quot;     &quot;müdür&quot;      
[19] &quot;cansever&quot;    &quot;uğur&quot;        &quot;haber&quot;       &quot;koordinatör&quot; &quot;ibrah&quot;       &quot;varlı'&quot;      &quot;yay&quot;         &quot;hayat&quot;       &quot;nisa&quot;       
[28] &quot;te&quot;          &quot;başlaya&quot;     &quot;gazete&quot;      &quot;sahip&quot;       &quot;birgi&quot;       &quot;yayıncılık&quot;  &quot;iletiş&quot;      &quot;ticaret&quot;     &quot;aş'd&quot;       
</code></pre>
<p><strong>But, i dont integrate these processes into term document matrix which is below that i try to mine pdf file</strong></p>
<pre><code>library(pdftools)
library(tm)
library(SnowballC)
library(dplyr)
library(stringr)
library(tidytext)
library(quanteda)
</code></pre>
<h1>this part is reading pdf file in the working place</h1>
<pre><code>files &lt;- list.files(pattern = &quot;pdf$&quot;)
file&lt;-as.character(files)
opinions &lt;- lapply(files, pdf_text)
length(opinions)
lapply(opinions, length) 
</code></pre>
<h1>creating corpus</h1>
<pre><code>corp &lt;- Corpus(URISource(files),
               readerControl = list(reader = readPDF))

</code></pre>
<h1>in this part, &quot;language=&quot;turkish&quot; does not working, it still uses base language which is English</h1>
<pre><code>opinions.tdm &lt;- TermDocumentMatrix(corp, 
                                   control = 
                                     list(language=&quot;turkish&quot;,
                                          stopwords = TRUE,
                                          removePunctuation = TRUE,
                                          tolower = TRUE,
                                          stemming = TRUE,
                                          removeNumbers = TRUE,
                                          bounds = list(global = c(1, Inf)))) 

inspect(opinions.tdm[1:10,]) 

opinions.tdm &lt;- TermDocumentMatrix(corp, 
                                   control = 
                                     list(language=&quot;turkish&quot;,
                                          stopwords = TRUE,
                                          tolower = TRUE,
                                          stemming = TRUE,
                                          removePunctuation=TRUE,
                                          removeNumbers = TRUE,
                                          bounds = list(global = c(1, Inf))))    
</code></pre>
<pre><code>
findFreqTerms(opinions.tdm, lowfreq = 100, highfreq = Inf)

ft &lt;- findFreqTerms(opinions.tdm, lowfreq = 100, highfreq = Inf)
as.matrix(opinions.tdm[ft,]) 

ft.tdm &lt;- as.matrix(opinions.tdm[ft,])
sort(apply(ft.tdm, 1, sum), decreasing = TRUE)
a&lt;-sort(apply(ft.tdm, 1, sum), decreasing = TRUE)
a&lt;-as.data.frame(a)
a$word&lt;-rownames(a)
aa&lt;-a %&gt;%  filter(a &gt; 200) %&gt;%
  mutate(word = reorder(word, a))
</code></pre>
",Dataset Preprocessing & Handling,change language termdocumentmatrix r text mining need change language turkish function termdocumentmatrix could possibly help code work reach result want stemming stopwords etc turkish dont integrate process term document matrix try mine pdf file part reading pdf file working place creating corpus part language turkish doe working still us base language english
Faster way to apply a function over a column in pandas data frame,"<p>I have to apply some multiple functions to a column to get the list of bigrams but it is painfully slow using the apply function the way I'm currently using. Do you have a way to boost the speed?</p>

<pre><code>def remove_stop_words(text):
    cleantext = text.lower()
    cleantext = ' '.join(re.sub(r'[^\w]', ' ', cleantext).strip().split())
    filtered_sentence= ''
    for w in word_tokenize(cleantext):
        if w not in stop_words: 
            filtered_sentence =  filtered_sentence + ' ' + w
    return  filtered_sentence.strip()

def lemmatize(text):
    lemma_word = []
    for w in word_tokenize(text.lower()):
        word1 = wordnet_lemmatizer.lemmatize(w, pos = ""n"")
        word2 = wordnet_lemmatizer.lemmatize(word1, pos = ""v"")
        word3 = wordnet_lemmatizer.lemmatize(word2, pos = (""a""))
        lemma_word.append(word3)
    return ' '.join(lemma_word)

def get_ngrams(text, n ):
    n_grams = ngrams(word_tokenize(text), n=2)
    return [ ' '.join(grams) for grams in n_grams]


df['bigrams'] = df.headline.apply(lambda x: get_ngrams(lemmatize(remove_stop_words(x)),n=2))
</code></pre>

<p>Edit: (based on comment)
The data frame df contains 2 columns - 1. headline 2. Sentiment score </p>

<p>headline - It's news headline, basically text on which I've to apply the function to get the bigrams of the headline </p>

<p>Sentiment Score - I've to keep the score as well in the df dataframe hence need to get a column called ""bigram"" in the same data frame</p>

<p><a href=""https://i.sstatic.net/o9PZd.png"" rel=""nofollow noreferrer"">Dataframe df</a></p>
",Dataset Preprocessing & Handling,faster way apply function column panda data frame apply multiple function column get list bigram painfully slow using apply function way currently using way boost speed edit based comment data frame df contains column headline sentiment score headline news headline basically text apply function get bigram headline sentiment score keep score well df dataframe hence need get column called bigram data frame dataframe df
getting n-gram counts at id-level for multiple documents per id (tokenize documents then aggregate or tokenizer skip parameter),"<p>I have a dataframe of ids and text, and would like to get n-gram counts per id (e.g. an id-term matrix). </p>

<p>As an example, let's i want bi-grams (2-grams) for the following table</p>

<pre><code>+-----+------------------+
| id  |   text           |
+-----+------------------+
| id1 | quick with fox   |
| id1 | brown fox        |
| id1 | quick squirrel   |
| id2 | yes its great    |
| id2 | fun with fox     |
| id3 | horr time        |
+-----+------------------+
</code></pre>

<p>In this case, the desired 2-gram counts looks like (<em>isn't necessarily binary</em>) </p>

<pre><code>+-----+------------+----------+-----------+----------------+---------+-----------+----------+-----------+
|     | quick with | with fox | brown fox | quick squirrel | yes its | its great | fun with | horr time |
+-----+------------+----------+-----------+----------------+---------+-----------+----------+-----------+
| id1 |          1 |        1 |         1 |              1 |       0 |         0 |        0 |         0 |
| id2 |          0 |        1 |         0 |              0 |       1 |         1 |        1 |         0 |
| id3 |          0 |        0 |         0 |              0 |       0 |         0 |        0 |         1 |
+-----+------------+----------+-----------+----------------+---------+-----------+----------+-----------+
</code></pre>

<p>This has to run efficiently for 1mil rows, 100-300k unique ids, and text that can range from 3 characters to 20+ words (e.g. a paragraph). </p>

<p>Two methods I've tried/proposed are</p>

<ol>
<li>Tokenize at document level then groupBy, then Sum</li>
</ol>

<pre><code>vectorizer = CountVectorizer(min_df = 3, ngram_range=(2,2))
X_bow_sp = vectorizer.fit_transform(df[""text""])
vocab = vectorizer.get_feature_names()

bow_df = pd.DataFrame(X_bow_sp)
bow_ids_df = pd.concat([df['id'], bow_df], axis=1, ignore_index=False, sort=False)
id_term_df = bow_ids_df.groupby('id')[vocab].sum()
</code></pre>

<p>This is way <strong>too slow</strong>. Specifically, the last line sum.</p>

<ol start=""2"">
<li>Concatenate text at group level (e.g. separated by '|'), but how to break apart n-gram? </li>
</ol>

<pre><code>id_text_df = df.groupby(['id'])['text'].apply(lambda x: ' | '.join(x))
</code></pre>

<p>What next? How can I feed a tokenizer option that breaks a potential n-gram apart when it encounters a <code>|</code>?</p>

<p>I'm using python/pandas primarily at the moment.</p>
",Dataset Preprocessing & Handling,getting n gram count id level multiple document per id tokenize document aggregate tokenizer skip parameter dataframe id text would like get n gram count per id e g id term matrix example let want bi gram gram following table case desired gram count look like necessarily binary ha run efficiently mil row k unique id text range character word e g paragraph two method tried proposed tokenize document level groupby sum way slow specifically last line sum concatenate text group level e g separated break apart n gram next feed tokenizer option break potential n gram apart encounter using python panda primarily moment
Is there a way to convert a defined format of CSV file having intent and training phrases to RASA nlu.md file?,"<p>Is there a way to create a CSV file with training phrases in it and convert the same to nlu.md format that is acceptable by Rasa.</p>

<p>This needs to be done for various intents with various training phrases.</p>
",Dataset Preprocessing & Handling,way convert defined format csv file intent training phrase rasa nlu md file way create csv file training phrase convert nlu md format acceptable rasa need done various intent various training phrase
Can I pre-trained BERT model from scratch using tokenized input file and custom vocabulary file for Khmer language,"<p>I would like to know if it's possible for me to use my own tokenized/segmented documents (with my own vocab file as well) as the input file to the <code>create_pretraining_data.py</code> script (git source: <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">https://github.com/google-research/bert</a>).   </p>

<p>The main reason for this question is because the segmentation/tokenization for the Khmer language is different than that of English. </p>

<pre><code>Original:
វា​មាន​មក​ជាមួយ​នូវ

Segmented/Tokenized:
វា មាន មក ជាមួយ នូវ
</code></pre>

<p>I tried something on my own and managed to get some results after running the <code>create_pretraining_data.py</code> and <code>run_pretraining.py</code> script.  However, I'm not sure if what I'm doing can be considered correct.  </p>

<p>I also would like to know the method that I should use to verify my model.  </p>

<p>Any help is highly appreciated!  </p>

<h2>Script Modifications</h2>

<p>The modifications that I did were:</p>

1. Make input file in a list format

<p>Instead of a normal plain text, my input file is from my custom Khmer tokenization output where I then make it into a list format, mimicking the output that I get when running the sample English text.</p>

<pre><code>[[['ដំណាំ', 'សាវម៉ាវ', 'ជា', 'ប្រភេទ', 'ឈើ', 'ហូប', 'ផ្លែ'],  
['វា', 'ផ្តល់', 'ផប្រយោជន៍', 'យ៉ាង', 'ច្រើន', 'ដល់', 'សុខភាព']],  
[['cmt', '$', '270', 'នាំ', 'លាភ', 'នាំ', 'សំណាង', 'ហេង', 'ហេង']]]
</code></pre>

<p><em>* The outer bracket indicates a source file, the first nested bracket indicates a document and the second nested bracket indicates a sentence. Exactly the same structure as the variable <code>all_documents</code> inside the <code>create_training_instances()</code> function</em></p>

2. Vocab file from unique segmented words

<p>This is the part that I'm really really having some serious doubt with. To create my vocab file, all I did was find the unique tokens from the whole documents. I then add the core token requirement <code>[CLS], [SEP], [UNK] and [MASK]</code>. I'm not sure if this the correct way to do it.  </p>

<p>Feedback on this part is highly appreciated!</p>

3. Skip tokenization step inside the create_training_instances() function

<p>Since my input file already matches what the variable <code>all_documents</code> is, I skip line 183 to line 207. I replaced it with reading my input as-is:</p>

<pre><code>  for input_file in input_files:
      with tf.gfile.GFile(input_file, ""r"") as reader:
          lines = reader.read()
      all_documents = ast.literal_eval(lines)
</code></pre>

<h2>Results/Output</h2>

<p>The raw input file (before custom tokenization) is from random web-scraping.  </p>

<p>Some information on the raw and vocab file:</p>

<pre><code>Number of documents/articles: 5
Number of sentences: 78
Number of vocabs: 649 (including [CLS], [SEP] etc.)
</code></pre>

<p>Below is the output (tail end of it) after running the  <code>create_pretraining_data.py</code></p>

<p><a href=""https://i.sstatic.net/eoW3p.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eoW3p.png"" alt=""create_pretraining_data_khmer_output""></a></p>

<p>And this is what I get after running the <code>run_pretraining.py</code></p>

<p><img src=""https://user-images.githubusercontent.com/24283367/69701076-06944b00-1127-11ea-9845-318a0c9520ca.PNG"" alt=""20191119_sample_4""></p>

<p>As shown in the diagram above I'm getting a very low accuracy from this and hence my concern if I'm doing it correctly.</p>
",Dataset Preprocessing & Handling,pre trained bert model scratch using tokenized input file custom vocabulary file khmer language would like know possible use tokenized segmented document vocab file well input file script git source main reason question segmentation tokenization khmer language different english tried something managed get result running script however sure considered correct also would like know method use verify model help highly appreciated script modification modification make input file list format instead normal plain text input file custom khmer tokenization output make list format mimicking output get running sample english text outer bracket indicates source file first nested bracket indicates document second nested bracket indicates sentence exactly structure variable inside function vocab file unique segmented word part really really serious doubt create vocab file wa find unique token whole document add core token requirement sure correct way feedback part highly appreciated skip tokenization step inside create training instance function since input file already match variable skip line line replaced reading input result output raw input file custom tokenization random web scraping information raw vocab file output tail end running get running shown diagram getting low accuracy hence concern correctly
Word2Vec Vocab Similarities,"<p>I ran a word2vec algo on text of about 750k words (before removing some stop words). Using my model, I started looking at the most similar words to particular words of my choosing, and the similarity scores (for model.wv.most_similar method) are all super close to 1. The tenth closest score is still like .998, so I feel like I'm not getting any significant differences between the similarity of words which leads to meaningless similar words.</p>

<p>My constructor for the model is </p>

<pre><code>model = Word2Vec(all_words, size=75, min_count=30, window=10, sg=1)
</code></pre>

<p>I think the problem may lie in how I structure the text to run the neural net on. I store all the words like so:</p>

<pre><code>all_sentences = nltk.sent_tokenize(v)
all_words = [nltk.word_tokenize(sent) for sent in all_sentences]
all_words = [[word for word in all_words[0] if word not in nltk.stopwords('English')]]
</code></pre>

<p>...where v is the result of calling read() on a txt file.</p>
",Dataset Preprocessing & Handling,word vec vocab similarity ran word vec algo text k word removing stop word using model started looking similar word particular word choosing similarity score model wv similar method super close tenth closest score still like feel like getting significant difference similarity word lead meaningless similar word constructor model think problem may lie structure text run neural net store word like v result calling read txt file
Learning n-gram features using CNN model using keras for text data using python step by step,"<p>I am trying to create features for my text corpus using convolution neural network (CNN). Say I want to learn 2-gram, 3-gram and 4-gram features as well ( Can be generalized to n-gram as well). I perform the following steps starting from my documents. I have taken a small document just to convey the idea.</p>
<pre><code>documents = [&quot;The Saudis are preparing a report that will acknowledge that&quot;, 
             &quot;Saudi journalist Jamal Khashoggi's death was the result of an&quot;, 
             &quot;interrogation that went wrong, one that was intended to lead&quot;, 
             &quot;to his abduction from Turkey, according to two sources.&quot;]

Y =[&quot;A&quot;,&quot;B&quot;, &quot;C&quot;,&quot;D&quot;]
</code></pre>
<p>Step 1: All pre-processing step of documents</p>
<p>Step 2: Tokenization</p>
<p>Step 3: Create tf-idf features and represent it as DataFrame ( Document as row and tokens as features)</p>
<p>Step 4: Train CNN network to learn n-gram features</p>
<p>Step 5: Append the learned n-gram features along with previous features to the data frame.</p>
<h3>I am stuck after Step 3</h3>
<p>The code is given below:</p>
<pre><code>def docs_preprocessor(docs):
tokenizer = RegexpTokenizer(r'\w+')
for idx in range(len(docs)):
    docs[idx]= &quot; &quot;.join(w.lower() for w in nltk.wordpunct_tokenize(docs[idx]) if w.lower() in words or not w.isalpha())
    docs[idx]= ' '.join(s for s in docs[idx].split() if not any(c.isdigit() for c in s))
    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.
    
    

# Remove numbers, but not words that contain numbers.
docs = [[token for token in doc if not token.isdigit()] for doc in docs]

# Remove words that are only one character.
docs = [[token for token in doc if len(token) &gt; 3] for doc in docs]

# Lemmatize all words in documents.
lemmatizer = WordNetLemmatizer()
docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]
docs=[ &quot; &quot;.join(doc)  for doc in docs]

return docs
</code></pre>
<p>Perform function on our document.</p>
<p>docs = docs_preprocessor(docs)</p>
<p>Simple pre processing.</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
doc_vec = vectorizer.fit_transform(df.text) # scipy.sparse.csr.csr_matrix

# Now convert to data frame:
df2 = pd.DataFrame(doc_vec.toarray(),columns=vectorizer.get_feature_names())
</code></pre>
<p><code>After this step, I am stuck. How should I train CNN network using Keras to learn the n-gram features and add the features in the dataframe along with previous features. It would be helpful if someone can run on small datasets mentioned above.</code></p>
<p><code>Python 3.6</code></p>
",Dataset Preprocessing & Handling,learning n gram feature using cnn model using kera text data using python step step trying create feature text corpus using convolution neural network cnn say want learn gram gram gram feature well generalized n gram well perform following step starting document taken small document convey idea step pre processing step document step tokenization step create tf idf feature represent dataframe document row token feature step train cnn network learn n gram feature step append learned n gram feature along previous feature data frame stuck step code given perform function document doc doc preprocessor doc simple pre processing
How to map topic to a document after topic modeling is done with LDA?,"<p>Is there any way I can map generated topic from LDA to the list of documents and identify to which topic it belongs to ? I am interested in clustering documents using unsupervised learning and segregating it into appropriate cluster. </p>

<p>Example, I have 10 topics after running LDA model with the best hyperparameter. So, it should return a number of Topic is already defined withe pre-trained LDA model with new sentence or document that user input. </p>

<p>I am waiting you guys good solution. :)</p>

<p>Ps. I am using Gensim for NLP.</p>
",Dataset Preprocessing & Handling,map topic document topic modeling done lda way map generated topic lda list document identify topic belongs interested clustering document using unsupervised learning segregating appropriate cluster example topic running lda model best hyperparameter return number topic already defined withe pre trained lda model new sentence document user input waiting guy good solution p using gensim nlp
How do you include all words from the corpus in a Gensim TF-IDF?,"<p>If I have some documents like this:</p>

<pre><code>doc1 = ""hello hello this is a document""
doc2 = ""this text is very interesting""
documents = [doc1, doc2]
</code></pre>

<p>And I compute a TF-IDF matrix for this in Gensim like this:</p>

<pre><code># create dictionary
dictionary = corpora.Dictionary([simple_preprocess(line) for line in documents])
# create bow corpus
corpus = [dictionary.doc2bow(simple_preprocess(line)) for line in documents]
# create the tf.idf matrix
tfidf = models.TfidfModel(corpus, smartirs='ntc')
</code></pre>

<p>Then for each document, I get a TF-IDF like this:</p>

<pre><code>Doc1: [(""hello"", 0.5), (""a"", 0.25), (""document"", 0.25)]
Doc2: [(""text"", 0.333), (""very"", 0.333), (""interesting"", 0.333)]
</code></pre>

<p>But I want the TF-IDF vector for each document to include words with 0 TF-IDF values (i.e. include every word mentioned in the corpus):</p>

<pre><code>Doc1: [(""hello"", 0.5), (""this"", 0), (""is"", 0), (""a"", 0.25), (""document"", 0.25), (""text"", 0), (""very"", 0), (""interesting"", 0)]
Doc2: [(""hello"", 0), (""this"", 0), (""is"", 0), (""a"", 0), (""document"", 0), (""text"", 0.333), (""very"", 0.333), (""interesting"", 0.333)]
</code></pre>

<p>How can I do this in Gensim? Or maybe there is some other library that can compute a TF-IDF matrix in this fashion (although like Gensim, it needs to be able to handle very large data sets, e.g. I achieved this result in Sci-kit on a small data set, but Sci-kit has memory problems on a large data set).</p>
",Dataset Preprocessing & Handling,include word corpus gensim tf idf document like compute tf idf matrix gensim like document get tf idf like want tf idf vector document include word tf idf value e include every word mentioned corpus gensim maybe library compute tf idf matrix fashion although like gensim need able handle large data set e g achieved result sci kit small data set sci kit ha memory problem large data set
R: how to extract pieces of text from a string based on a pattern,"<p>I have a dataset where each row contains a string of text of this type</p>

<pre><code>1)list(text = \""incredible hosts\"", relevance = 0.87518, count = 1), list(text = \""Japan\"", relevance = 0.675236, count = 1), list(text = \""support\"", relevance = 0.625663, count = 1), list(text = \""result\"", relevance = 0.359757, count = 1)


2)list(text = \""British fleet\"", relevance = 0.912888, count = 1), list(text = \""worst maritime disasters\"", relevance = 0.904047, count = 1), list(text = \""British history\"", relevance = 0.755491, count = 1), list(text = \""Scilly Isles\"", relevance = 0.716508, count = 1), list(text = \""sailors\"", relevance = 0.691141, count = 1), list(text = \""evening\"", relevance = 0.597375, count = 1), list(text = \""Tragedy\"", relevance = 0.577141, count = 1), list(text = \""prize\"", relevance = 0.565035, count = 1), list(text = \""rocks\"", relevance = 0.543257, count = 1), list(text = \""innovation\"", relevance = 0.529463, count = 1), list(text = \""longitude\"", relevance = 0.335207, count = 1)
</code></pre>

<p>basically I would like to extract just the string of text contain between \"" and \""</p>

<p>and obtain something like this</p>

<pre><code>1) ""incredible hosts, Japan, support , result""
2) ""British fleet, worst maritime disasters, British history, scilly Isles, sailors, evening, etc...""
</code></pre>

<p>Moreover I would like to create a data frame that helps le keep track of the relevance score contained in the text for each piece of text (considering that different raws might have different number of pieces of text) so to get something like this:</p>

<pre><code> col1                 col2.   col3.    col4.   col5.     col6.....  colA1    colA2.  .....
 incredible hosts     Japon  support  result    NA.      NA        0.87518.   0.675236....
 british fleet.       worst marit.......
</code></pre>

<p>basically a number of columns that is equal to the maximum number of pieces of text in a row, same for the columns corresponding to the score (each relevance score refers to a piece of text, so they re the same number).</p>

<p>If I can find a way to extract first the pieces of text and separate them by a comma, and then do the same with the relevance scores I think I can easily merge the two in a dataframe. so the problem is mainly extracting this 2 things from that text.</p>

<p>thank you in advance for your help,</p>

<p>Carlo</p>
",Dataset Preprocessing & Handling,r extract piece text string based pattern dataset row contains string text type basically would like extract string text contain obtain something like moreover would like create data frame help le keep track relevance score contained text piece text considering different raw might different number piece text get something like basically number column equal maximum number piece text row column corresponding score relevance score refers piece text number find way extract first piece text separate comma relevance score think easily merge two dataframe problem mainly extracting thing text thank advance help carlo
My text from csv file is being read as raw string. It contains &quot;it\&#39;s&quot; instead of it&#39;s. How do I clean this?,"<p><strong>Sentence</strong>: </p>

<pre><code>'I understood that that morning did not work out for her but I would still like to to make an appointment with her. I mean if she does great lashes and it\'s just this one little hiccup in the beginning it\'s well worth it as far as I\'m concerned.'
</code></pre>

<p>How do I remove escape characters to clean the data?</p>
",Dataset Preprocessing & Handling,text csv file read raw string contains instead clean sentence remove escape character clean data
"IBM Watson NLC - Training with more than 20,000 text examples?","<p>We're currently developing a system that would return an ICD10-CM code (A medical/diagnosis coding system) from a text input. Example</p>

<ul>
<li>input 'Black Eye'</li>
<li>return 'H44 - Disorders of the globe'</li>
</ul>

<p>Problem is, ICD10-CM has 70,000 to 100,000 codes, so it won't let me train the model after I uploaded all those text examples from .csv files.</p>

<p>Is using multiple models a solution or should I switch to Google's AutoML?</p>
",Dataset Preprocessing & Handling,ibm watson nlc training text example currently developing system would return icd cm code medical diagnosis coding system text input example input black eye return h disorder globe problem icd cm ha code let train model uploaded text example csv file using multiple model solution switch google automl
TypeError: can only concatenate list (not &quot;str&quot;),"<p>I'm doing a pos tagging and the algorithm is Baum-Welch algorithm.
I want to send the types and tags in the .csv file but after running the code this error shows</p>

<pre><code>untagged =pd.read_csv('test.csv','UTF-8','r')

print ('Tagging...')

#taggedOutput = doTagging(sent,untagged)

[w for w in sent if w in untagged]

tagged = pd.read_csv(""Tagged_bangla_hmm.csv"",'a',encoding=""utf-8"", 

header=None, delimiter = r'\s+',skip_blank_lines=False, engine='python')

for sentence in tagged:

     a = zip('types', 'tags')

     for word, tag in  a:

         tagged.to_csv( types +'/' + tags + ' ')

         print(tagged)

         print('\n\n')

         tagged.close()

         print ('Finished Tagging')

         i=0
</code></pre>
",Dataset Preprocessing & Handling,typeerror concatenate list str po tagging algorithm baum welch algorithm want send type tag csv file running code error show
Can&#39;t load HDF5 in python,"<p>I am following this tutorial: <a href=""https://github.com/fastai/fastai/tree/master/courses/dl2/imdb_scripts"" rel=""nofollow noreferrer"">https://github.com/fastai/fastai/tree/master/courses/dl2/imdb_scripts</a></p>

<p>I downloaded the pre-trained model in part 3b.
I want to open the .h5 files and look/use the weights. I tried to use python to do this, but it is not opening.</p>

<p>Here’s the code I used:</p>

<pre><code>import tables
import pandas as pd
filename = “…bwd_wt103.h5”
file = tables.open_file(filename)
</code></pre>

<p>Here’s the error:</p>

<pre><code>OSError: HDF5 error back trace
File “C:\ci\hdf5_1525883595717\work\src\H5F.c”, line 511, in H5Fopen
unable to open file
File “C:\ci\hdf5_1525883595717\work\src\H5Fint.c”, line 1604, in H5F_open
unable to read superblock
File “C:\ci\hdf5_1525883595717\work\src\H5Fsuper.c”, line 413, in H5F__super_read
file signature not found

End of HDF5 error back trace

Unable to open/create file 'C:/Users/Rishabh/Documents/School and Work/Classes/8 
Fall2019/Senior Design/ULMFiT/Wiki Data/wt103/models/bwd_wt103.h5'
</code></pre>

<p>I also used The HDF Group HDF Viewer: <a href=""https://support.hdfgroup.org/products/java/release/download.html"" rel=""nofollow noreferrer"">https://support.hdfgroup.org/products/java/release/download.html</a></p>

<p>But that didn’t work either. It gave an error saying “Failed to open the file… Unsupported format”</p>

<p>Is there a way to load the weights in Python? I ultimately want to access the last layer of the stacked LSTMS to create word embeddings.</p>

<p>Thanks in advance.</p>
",Dataset Preprocessing & Handling,load hdf python following tutorial downloaded pre trained model part b want open h file look use weight tried use python opening code used error also used hdf group hdf viewer work either gave error saying failed open file unsupported format way load weight python ultimately want access last layer stacked lstms create word embeddings thanks advance
How to free the memory taken by a pyspark model (JavaModel)?,"<p>As described, I load a trained word2vec model through pyspark. </p>

<pre><code>word2vec_model = Word2VecModel.load(""saving path"")
</code></pre>

<p>After using that, I want to delete it since it will take much memory space on single node (I used the findSynonyms function, and the doc says it should be local used only)
I tried to use</p>

<pre><code>del word2vec_model
gc.collect()
</code></pre>

<p>but it seems that doesn't word. And it's not an rdd file, I can't use .unpersist(). I didn't find any like unload() fuction in the doc.</p>

<p>Anyone could help me or give me some advice?</p>
",Dataset Preprocessing & Handling,free memory taken pyspark model javamodel described load trained word vec model pyspark using want delete since take much memory space single node used findsynonyms function doc say local used tried use seems word rdd file use unpersist find like unload fuction doc anyone could help give advice
How to save an edited .conllu file using conllu python library,"<p>I want to ask something that perhaps I cannot find yet in the internet.</p>

<p>First of all I'm using the Universal Dependencies dataset and want to edit some data on Jupyter Notebook (python 3.6).</p>

<p>I found the conllu library <a href=""https://pypi.org/project/conllu/"" rel=""nofollow noreferrer"">https://pypi.org/project/conllu/</a> and use it to work with .conllu UD dataset. I want to edit one of the data (like changing the lemma). Here's the example:
<a href=""https://i.sstatic.net/lbINf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lbINf.png"" alt=""Example""></a></p>

<p>I've read there's a function in conllu library </p>

<blockquote>
  <p>.serialize()</p>
</blockquote>

<p>that could change back the format to .conllu format instead of string.</p>

<p>But I think it won't save the actual file, it just print it.</p>

<p>All I want is the actual file changed.
Anyone can help me, please? Thank you.</p>
",Dataset Preprocessing & Handling,save edited conllu file using conllu python library want ask something perhaps find yet internet first using universal dependency dataset want edit data jupyter notebook python found conllu library use work conllu ud dataset want edit one data like changing lemma example read function conllu library serialize could change back format conllu format instead string think save actual file print want actual file changed anyone help please thank
Is there a way to load spacy trained model into gensim?,"<p>I want to get the list of similar words. Since Spacy doesn't have a built-in support for this I want to convert the spacy model to gensim word2vec and get the list of similar words.</p>

<p>I have tried to use the below method. But it is time consuming.</p>

<pre class=""lang-py prettyprint-override""><code>def most_similar(word):
    by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)
    return [w.orth_ for w in by_similarity[:10]]
</code></pre>

<pre class=""lang-py prettyprint-override""><code>nlp = spacy.load('en_core_web_md')
nlp.to_disk(filename)
nlp.vocab.vectors.to_disk(filename)
</code></pre>

<p>This does not save the model to a text file. Hence, I am not able to use the following method.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec

glove_file = datapath('test_glove.txt')
tmp_file = get_tmpfile(""test_word2vec.txt"")

_ = glove2word2vec(glove_file, tmp_file)
</code></pre>
",Dataset Preprocessing & Handling,way load spacy trained model gensim want get list similar word since spacy built support want convert spacy model gensim word vec get list similar word tried use method time consuming doe save model text file hence able use following method
Separating English text and non-English Text from a file,"<p>I have a .csv file and I want to separate Non-English Text and English Text in two different files. Below is the code, I tried:</p>

<pre><code>  import string
  def isEnglish(s):
      return s.translate(None, string.punctuation).isalnum()
  file=open('File1.csv','r',encoding='UTF-8')
  outfile1=open('Eng.csv','w', encoding='utf-8')
  outfile2=open('Noneng.csv','w', encoding='utf-8')
  for line in file.readlines():
       r = isEnglish(line)
       if r:
          outfile1.write(line+""\n"")
       else:
          outfile2.write(line+""\n"")
</code></pre>

<p>The code is not producing the desired result. There is repetitive English text in both the files. I have attached a snapshot of one output file.</p>
",Dataset Preprocessing & Handling,separating english text non english text file csv file want separate non english text english text two different file code tried code producing desired result repetitive english text file attached snapshot one output file
"Naive Bayes, Text Analysis, SKLearn","<p>This is from a text analysis exercise using data from Rotten Tomatoes. The data is in critics.csv, imported as a pandas DataFrame, ""critics"".</p>

<p>This piece of the exercise is to</p>

<blockquote>
  <p>Construct the cumulative distribution of document frequencies (df).
  The  𝑥 -axis is a document count  (𝑥𝑖)  and the  𝑦 -axis is the
  percentage of words that appear less than  (𝑥𝑖)  times. For example,
  at  𝑥=5 , plot a point representing the percentage or number of words
  that appear in 5 or fewer documents.</p>
</blockquote>

<p>From a previous exercise, I have a ""Bag of Words""</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()

# build the vocabulary and transform to a ""bag of words""
X = vectorizer.fit_transform(critics.quote)

# Convert matrix to Compressed Sparse Column (CSC) format
X = X.tocsc()  
</code></pre>

<p>Evey sample I've found calculates a matrix of documents per word from that ""bag of words"" matrix in this way:</p>

<pre><code>docs_per_word = X.sum(axis=0) 
</code></pre>

<p>I buy that this works; I've looked at the result. </p>

<p>But I'm confused about what's actually happening and why it works, what is being summed, and how I might have been able to figure out how to do this without needing to look up what other people did.</p>
",Dataset Preprocessing & Handling,naive bayes text analysis sklearn text analysis exercise using data rotten tomato data critic csv imported panda dataframe critic piece exercise construct cumulative distribution document frequency df axis document count axis percentage word appear le time example plot point representing percentage number word appear fewer document previous exercise bag word evey sample found calculates matrix document per word bag word matrix way buy work looked result confused actually happening work summed might able figure without needing look people
&quot;No enough value to unpack&quot; while loading the dataset- Allennlp _read,"<p>I am trying to use Allennlp library to perform NER. The library works perfectly fine with conll2003 and other databases which only have entities and tokens (I had to update _read function for the same).
But the function returns ""ValueError: not enough values to unpack (expected 2, got 1)"" if I try to use my own dataset. I have compared the formatting, special characters, spacing, and even file names but couldn't find any issue.
This is the sample from the dataset which worked,</p>

<pre><code>O   show
O   me
O   films
O   with
B-ACTOR drew
I-ACTOR barrymore
O   from
O   the
B-YEAR  1980s

O   what
O   movies
O   starred
O   both
B-ACTOR al
I-ACTOR pacino
</code></pre>

<p>This is the sample from my dataset which is not working,</p>

<pre><code>O   dated
O   as
O   of
B-STARTDATE February
I-STARTDATE 9
I-STARTDATE ,
L-STARTDATE 2017
O   by
O   and
O   between
O   Allenware
O   Ltd
</code></pre>

<p>I am not able to identify the issue, please help.</p>

<p><strong>Update</strong></p>

<p>adding stderr.log as requested.</p>

<pre><code>0it [00:00, ?it/s]
1it [00:00, 556.72it/s]

0it [00:00, ?it/s]
Traceback (most recent call last):
  File ""/allennlp/bin/allennlp"", line 8, in &lt;module&gt;
    sys.exit(run())
  File ""/allennlp/lib/python3.6/site-packages/allennlp/run.py"", line 18, in run
    main(prog=""allennlp"")
  File ""/allennlp/lib/python3.6/site-packages/allennlp/commands/__init__.py"", line 102, in main
    args.func(args)
  File ""/allennlp/lib/python3.6/site-packages/allennlp/commands/train.py"", line 124, in train_model_from_args
    args.cache_prefix)
  File ""/allennlp/lib/python3.6/site-packages/allennlp/commands/train.py"", line 168, in train_model_from_file
    cache_directory, cache_prefix)
  File ""/allennlp/lib/python3.6/site-packages/allennlp/commands/train.py"", line 226, in train_model
    cache_prefix)
  File ""/allennlp/lib/python3.6/site-packages/allennlp/training/trainer_pieces.py"", line 42, in from_params
    all_datasets = training_util.datasets_from_params(params, cache_directory, cache_prefix)
  File ""/allennlp/lib/python3.6/site-packages/allennlp/training/util.py"", line 185, in datasets_from_params
    validation_data = validation_and_test_dataset_reader.read(validation_data_path)
  File ""/allennlp/lib/python3.6/site-packages/allennlp/data/dataset_readers/dataset_reader.py"", line 134, in read
    instances = [instance for instance in Tqdm.tqdm(instances)]
  File ""/allennlp/lib/python3.6/site-packages/allennlp/data/dataset_readers/dataset_reader.py"", line 134, in &lt;listcomp&gt;
    instances = [instance for instance in Tqdm.tqdm(instances)]
  File ""/allennlp/lib/python3.6/site-packages/tqdm/std.py"", line 1081, in __iter__
    for obj in iterable:
  File ""/allennlp/lib/python3.6/site-packages/allennlp/data/dataset_readers/conll2003.py"", line 119, in _read
    ner_tags,tokens_ = fields
ValueError: not enough values to unpack (expected 2, got 1)
0it [00:00, ?it/s]
</code></pre>

<p>Adding _read and text_to_instance functions</p>

<pre><code>@overrides
    def _read(self, file_path: str) -&gt; Iterable[Instance]:
        # if `file_path` is a URL, redirect to the cache
        file_path = cached_path(file_path)

        with open(file_path, ""r"") as data_file:
            logger.info(""Reading instances from lines in file at: %s"", file_path)

            # Group into alternative divider / sentence chunks.
            for is_divider, lines in itertools.groupby(data_file, _is_divider):
                # Ignore the divider chunks, so that `lines` corresponds to the words
                # of a single sentence.
                if not is_divider:
                    fields = [line.strip().split() for line in lines]
                    # unzipping trick returns tuples, but our Fields need lists
                    fields = [list(field) for field in zip(*fields)]
                    ner_tags,tokens_ = fields
                    # TextField requires ``Token`` objects
                    tokens = [Token(token) for token in tokens_]

                    yield self.text_to_instance(tokens,ner_tags)

    def text_to_instance(  # type: ignore
        self,
        tokens: List[Token],
        ner_tags: List[str] = None,
    ) -&gt; Instance:
        """"""
        We take `pre-tokenized` input here, because we don't have a tokenizer in this class.
        """"""

        sequence = TextField(tokens, self._token_indexers)
        instance_fields: Dict[str, Field] = {""tokens"": sequence}
        instance_fields[""metadata""] = MetadataField({""words"": [x.text for x in tokens]})
        coded_ner=ner_tags
        if 'ner' in self.feature_labels:
            if coded_ner is None:
                raise ConfigurationError(""Dataset reader was specified to use NER tags as ""
                                         "" features. Pass them to text_to_instance."")
            instance_fields['ner_tags'] = SequenceLabelField(coded_ner, sequence, ""ner_tags"")
        if self.tag_label == 'ner' and coded_ner is not None:
            instance_fields['tags'] = SequenceLabelField(coded_ner, sequence,self.label_namespace)
        return Instance(instance_fields)
</code></pre>
",Dataset Preprocessing & Handling,enough value unpack loading dataset allennlp read trying use allennlp library perform ner library work perfectly fine conll database entity token update read function function return valueerror enough value unpack expected got try use dataset compared formatting special character spacing even file name find issue sample dataset worked sample dataset working able identify issue please help update adding log requested adding read text instance function
Converting a list of Counters to sparse Pandas DataFrame,"<p>I am having trouble constructing a pandas DataFrame with sparse dtype. My input is a bunch of feature vectors stored as dicts or Counters. With sparse data like bag-of-words representation of text, it is often inappropriate and infeasible to store the data as a dense document x term matrix, and is necessary to maintain the sparsity of the data structure.</p>

<p>For example, say the input is:</p>

<pre><code>docs = [{'hello': 1}, {'world': 1, '!': 2}]
</code></pre>

<p>Output should be equivalent to:</p>

<pre><code>import pandas as pd
out = pd.DataFrame(docs).astype(pd.SparseDtype(float))
</code></pre>

<p>without creating dense arrays along the way. (We can check <code>out.dtypes</code> and <code>out.sparse.density</code>.)</p>

<p>Attempt 1:</p>

<pre><code>out = pd.DataFrame(dtype=pd.SparseDtype(float))
out.loc[0, 'hello'] = 1
out.loc[1, 'world'] = 1
out.loc[1, '!'] = 2
</code></pre>

<p>But this produces a dense data structure.</p>

<p>Attempt 2:</p>

<pre><code>out = pd.DataFrame({""hello"": pd.SparseArray([]),
                    ""world"": pd.SparseArray([]),
                    ""!"": pd.SparseArray([])})
out.loc[0, 'hello'] = 1
</code></pre>

<p>But this raises <code>TypeError: SparseArray does not support item assignment via setitem</code>.</p>

<p>The solution I eventually found below did not work in earlier versions of Pandas where I tried it.</p>
",Dataset Preprocessing & Handling,converting list counter sparse panda dataframe trouble constructing panda dataframe sparse dtype input bunch feature vector stored dicts counter sparse data like bag word representation text often inappropriate infeasible store data dense document x term matrix necessary maintain sparsity data structure example say input output equivalent without creating dense array along way check attempt produce dense data structure attempt raise solution eventually found work earlier version panda tried
Finding total count for word form when many possible POS tags,"<p>I feel like I have a dumb question, but here goes anyway.. 
I'm trying to go from data that looks something like this:</p>

<pre><code>a word form     lemma    POS                count of occurrance
same word form  lemma    Not the same POS   another count
same word form  lemma    Yet another POS    another count
</code></pre>

<p>to a result that looks like this:</p>

<pre><code>the word form    total count    all possible POS and their individual counts 
</code></pre>

<p>So for example I could have:</p>

<pre><code>ring     total count = 100        noun = 40, verb = 60
</code></pre>

<p>I have my data in a CSV file. I want to do something like this:</p>

<pre><code>for row in all_rows:
    if row[0] is the same as row[0] in the next row, add the values from row[3] together to get the total count
</code></pre>

<p>buuut I can't seem to figure out how to do that. Help? </p>
",Dataset Preprocessing & Handling,finding total count word form many possible po tag feel like dumb question go anyway trying go data look something like result look like example could data csv file want something like buuut seem figure help
Spacy text classification scores,"<p>I'm quite new to NLP text classification and trying to apprehend the basics. It seems that Spacy is more suitable for my tasks and experience. I've read through all the docs and run the example code from <a href=""https://spacy.io/usage/training#example-textcat"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#example-textcat</a> with default plac parameters with my own output folder. Then, I wrote a testing file:</p>

<pre><code>import spacy

output_dir=""train_output_orig""

test_text = [
    ""This movie sucked"",
    ""It's a great one"",
    ""I've watched a lot of films of this kind. A lot of them were more attractive for me"",
    ""This is a great movie"",
    ""This movie is terrible"",
    ""I love this movie"",
    ""This is a bad film"",
    ""So fucking dung!"",
    ""Very involving work with developed characters""
    ]
print(""Loading from"", output_dir)
nlp2 = spacy.load(output_dir)
for text in test_text:
    print(text, nlp2(text).cats)
</code></pre>

<p>and got results:</p>

<pre><code>Loading from train_output_orig
This movie sucked {'POSITIVE': 0.6549780368804932}
It's a great one {'POSITIVE': 0.7863456606864929}
I've watched a lot of films of this kind. A lot of them were more attractive for me {'POSITIVE': 0.7664909958839417}
This is a great movie {'POSITIVE': 0.7897435426712036}
This movie is terrible {'POSITIVE': 0.4777064323425293}
I love this movie {'POSITIVE': 0.7530838847160339}
This is a bad film {'POSITIVE': 0.46895521879196167}
So fucking dung! {'POSITIVE': 0.6296740174293518}
Very involving work with developed characters {'POSITIVE': 0.8538092970848083}
</code></pre>

<p>Is it OK for Spacy model, or have I done something wrong? I mean there's quite narrow frontier between ""positive"" and ""negative"" labels. Even definitive ""This is a bad film"" earned 0.46 of ""positive"" rating. ""I love this movie"" got only 0.75 while ""Very involving work with developed characters"" got 0.83. At the same time, suggested in the original Spacy usage docs phrase ""This movie sucked"" got 0.65 ""positive"" score!</p>

<p>Thank you in advance for your answer</p>
",Dataset Preprocessing & Handling,spacy text classification score quite new nlp text classification trying apprehend basic seems spacy suitable task experience read doc run example code default plac parameter output folder wrote testing file got result ok spacy model done something wrong mean quite narrow frontier positive negative label even definitive bad film earned positive rating love movie got involving work developed character got time suggested original spacy usage doc phrase movie sucked got positive score thank advance answer
How do i write these type of loops in one line using python 3.x,"<p>I'm  novice at python. I'm trying to clean a text file for my NLP project. How can I write these few lines of code into a single line in python. I'm using NLTK for processing my text data. </p>

<pre><code>sentences = [sent_tokenize(token) for token in doc]
words = [word_tokenize(''.join(sentence)) for sentence in sentences]
clean_words=[]
for word in words :
      for token in word :
           if token.isalnum() and token.lower() not in  list(stop_words):
                clean_words.append(token.lower())`
</code></pre>
",Dataset Preprocessing & Handling,write type loop one line using python x novice python trying clean text file nlp project write line code single line python using nltk processing text data
How to split cleaned text data into training and testing datasets except for random sampling,"<p>I have cleaned and de-duplicated text data with a 'count_raw_id' column which implies the number of raw ids that are mapped to one cleaned id 
A clean id represent that it is unique and has some raw ids mapped to it 
Now i don't want to split my cleaned text data('clean_df') randomly 
I need some Criteria based sampling to create two datasets out of this whole cleaned file of about 2k rows one to train the model and one to test the model</p>

<p>I don't want to use train_test_split of sklearn to split my data as it will my data randomly.I want some way out to query my data such that i can use some other sampling technique also i can't use stratified sampling as i don't have actual labels for these records</p>

<pre><code>import pandas as pd
data = {'clean_id': [1,2,3,4],
   'all_terms': [['activation', 'brand', 'admin', 'sale', 'commission', 
                  'administration', 'assistant', 'manager'],
                 ['activation', 'brand', 'group', 'commission', 'mktg', 
                  'marketing', 'manager'],
                 ['activation', 'brand', 'info', 'specialist', 'service', 
                  'manager', 'customer'],
                 ['activation', 'brand', 'lead', 'greece', 'commission', 
                  'mktg', 'mgr', 'marketing']],
   'count_raw_id': [8,2,4,5]}
clean_df = pd.DataFrame(data)
len(clean_df)
#output : 2150
</code></pre>
",Dataset Preprocessing & Handling,split cleaned text data training testing datasets except random sampling cleaned de duplicated text data count raw id column implies number raw id mapped one cleaned id clean id represent unique ha raw id mapped want split cleaned text data clean df randomly need criterion based sampling create two datasets whole cleaned file k row one train model one test model want use train test split sklearn split data data randomly want way query data use sampling technique also use stratified sampling actual label record
Low rank approximation using scipy,"<p>I'm trying to use <strong>low-rank-approximation</strong> for <strong>latent semantic indexing</strong>. I thought that doing low rank approximation reduces matrix dimensions but it contradicts the results I get.</p>

<p>Assume I have my dictionary with 40 000 words and 2000 documents. Then my term-by-document matrix is 40 000 x 2000.
According to wikipedia, I have to do SVD of a matrix and then apply </p>

<p><a href=""https://i.sstatic.net/8TcNQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8TcNQ.png"" alt=""enter image description here""></a></p>

<p>This is the code I use for SVD and low rank approximation (the matrix is sparse):</p>

<pre class=""lang-py prettyprint-override""><code>import scipy
import numpy as np

u, s, vt = scipy.sparse.linalg.svds(search_matrix, k=20)
search_matrix = u @ np.diag(s) @ vt

print('u: ', u.shape) # (40000, 20)
print('s: ', s.shape) # (20, )
print('vt: ', vt.shape) # (20, 2000)
</code></pre>

<p>The result matrix is: (40 000 x 20) * (20 x 20) * (20, 2000) = 40 000 x 2000, which is exactly what I started with. </p>

<p>So... how does the low-rank-approximation reduce the dimensions of the matrix exactly?</p>

<p>Also, I will be doing queries on this approximated matrix to find correlation between user vector and each document (naive search engine). The user vector has dimensions 40 000 x 1 to start with (<strong>bag of words</strong>). According to the same wikipedia page, this is what I should do:</p>

<p><a href=""https://i.sstatic.net/KLOdx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KLOdx.png"" alt=""enter image description here""></a></p>

<p>The code:</p>

<pre class=""lang-py prettyprint-override""><code>user_vec = np.diag((1 / s)) @ u.T @ user_vec
</code></pre>

<p>And it produces a matrix 20 x 1 which is what I expected!
((20 x 20) * (20 x 40 000) * (40 000 x 1) = (20 x 1)). But now, it has dimensions that do not match the search_matrix I want to multiply it with.</p>

<p>So... What am I doing wrong and why?</p>

<p>Sources:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Latent_semantic_analysis"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Latent_semantic_analysis</a></li>
</ul>
",Dataset Preprocessing & Handling,low rank approximation using scipy trying use low rank approximation latent semantic indexing thought low rank approximation reduces matrix dimension contradicts result get assume dictionary word document term document matrix x according wikipedia svd matrix apply code use svd low rank approximation matrix sparse result matrix x x x exactly started doe low rank approximation reduce dimension matrix exactly also query approximated matrix find correlation user vector document naive search engine user vector ha dimension x start bag word according wikipedia page code produce matrix x expected x x x x ha dimension match search matrix want multiply wrong source
aligning sentences to corpus and finding mismatches,"<p>The ideal goal is to correct the output from a speech2text model according to a reference corpus (the actual text). I don't mind using any off the selves tool either in NLP space or ElasticSearch </p>

<p>I have a reference corpus like the following:</p>

<blockquote>
  <p>It is a reliance that has led to a cycle of addiction that has
  destroyed lives it <strong>is a cycle that makes you sick when</strong> you <strong>try to stop
  and potentially takes your</strong> <strong>life when you don't and beyond</strong> its physical
  effects this cycle of addiction also includes constant contact with
  the criminal justice system and not just a cycle of arrests release
  and violation.</p>
</blockquote>

<p>In fact its much longer ...</p>

<p>On the other hand, I have a set of sentences that are recognized from a speech-2-text model in a CSV files</p>

<pre><code>1, is a cycle that makes you dick when
2, try two stops and essentially hates your
3, posses activated
4, lives when who don't and beyond
</code></pre>

<p>As you can see there because the speech2text model is not perfect there are errors, for example</p>

<p>1) With references to the corpus these subsentences are misspelled (e.g. dick instead of sick in number the sentence number 1
2) there are sentences that do not match to the corpus at all - e.g. number 3
3) putting the sentences together does not cover the whole paragraph.</p>

<p>So basically I wonder what is this task called in the NLP topic, then I can do a better googling, and I appreciate if you name specific functions or examples that I can leverage, e.g. in Space or NLTK or any other tool.</p>

<p><em>edit</em> : * I already have experience with nlp (coursera certificate) - therefore, looking for a concrete answer and/or example rather a scientific paper. This is not a general error correction task or the next work recommendation based on sequential models. </p>
",Dataset Preprocessing & Handling,aligning sentence corpus finding mismatch ideal goal correct output speech text model according reference corpus actual text mind using self tool either nlp space elasticsearch reference corpus like following reliance ha led cycle addiction ha destroyed life cycle make sick try stop potentially take life beyond physical effect cycle addiction also includes constant contact criminal system cycle arrest release violation fact much longer hand set sentence recognized speech text model csv file see speech text model perfect error example reference corpus subsentences misspelled e g dick instead sick number sentence number sentence match corpus e g number putting sentence together doe cover whole paragraph basically wonder task called nlp topic better googling appreciate name specific function example leverage e g space nltk tool edit already experience nlp coursera certificate therefore looking concrete answer example rather scientific paper general error correction task next work recommendation based sequential model
Improve efficiency for ranking in Bag of words model,"<p>I am creating a text summarizer and using a basic model to work with using Bag of words approach.<br>
the code i am performing is using the nltk library. 
the file read is a large file with over 2500000 words. 
below is the loop i am working on with but this takes over 2 hours to run and complete. is there a way to optimize this code    </p>

<pre><code>f= open('Complaints.csv', 'r')
raw = f.read()
len(raw)
tokens = nltk.word_tokenize(raw)
len(tokens)
freq = nltk.FreqDist(text)
top_words = [] # blank dictionary 
top_words = freq.most_common(100)
print(top_words)
sentences = sent_tokenize(raw)
print(raw)
ranking = defaultdict(int)
for i, sent in enumerate(raw):
for word in word_tokenize(sent.lower()):
    if word in freq:
        ranking[i]+=freq[word]
top_sentences = nlargest(10, ranking, ranking.get)
print(top_sentences)
</code></pre>

<p>This is only one one file and the actual deployment has more than 10-15 files of similar size. 
How we can improve this.<br>
Please note these are the text from a chat bot and are actual sentences hence there was no requirement to remove whitespaces, stemming and other text pre processing methods</p>
",Dataset Preprocessing & Handling,improve efficiency ranking bag word model creating text summarizer using basic model work using bag word approach code performing using nltk library file read large file word loop working take hour run complete way optimize code one one file actual deployment ha file similar size improve please note text chat bot actual sentence hence wa requirement remove whitespaces stemming text pre processing method
How do I group strings together to create a text &quot;blob&quot; by group?,"<p>I have a data frame with 100,000+ records. Each record is a product with several data points and a free text field that explains a particular issue that a product has. This is an example of a data frame that I have, let's call it: ""<code>df</code>""</p>

<pre><code>product   class     subclass     issue
prod1     R         A1           problem has to do with x, y, and z
prod1     R         A2           this product has something wrong with z
prod2     L         B1           there was no problem with this
prod2     L         B2           part that went bad
prod2     L         B1           this was improperly installed
prod3     R         A1           problem has to with parts a, b, and z
.
.
.
</code></pre>

<p>I've used</p>

<pre><code>df %&gt;% group_by(product) %&gt;% mutate(blob = str_c(df%issue, collapse = "" ""))

</code></pre>

<p>to try and get to:</p>

<pre><code>product     blob
prod1       problem has to do with x, y, and z this product has something wrong with z
prod2       there was no problem with this part that went bad this was improperly installed
prod3       problem has to with parts a, b, and z
</code></pre>

<p>I'm doing more cleaning of the text before I get to this point - removing stopwords, punctuation, lemmatizing etc.</p>

<p>Since I want to group, I've been using <code>dplyr</code>. I've tried using <code>paste</code> to group by and concatenate and I've tried <code>str_c</code> from <code>stringr</code> because I saw it performs better. But I'm not sure if it is a performance issue because I'm trying to group and concatenate over 100,000 strings? </p>

<p>I will also want to look at a blob by class and subclass also, hence why I'm using dplyr.</p>

<p>Once I get this proper grouping, I will be able to look at a number of different things, term frequencies, LDA by group, and so on.</p>

<p>Any help or guidance is greatly appreciated.</p>
",Dataset Preprocessing & Handling,group string together create text blob group data frame record record product several data point free text field explains particular issue product ha example data frame let call used try get cleaning text get point removing stopwords punctuation lemmatizing etc since want group using tried using group concatenate tried saw performs better sure performance issue trying group concatenate string also want look blob class subclass also hence using dplyr get proper grouping able look number different thing term frequency lda group help guidance greatly appreciated
How to set parameter to loop from specific row in excel using python,"<p>The code read data from specific column in excel column ( in my case i used columns = 'profile') 
The result is in dataframe as below:</p>

<pre><code>profile
0  https://scontent-lga3-1.xx.fbcdn.net/v/t1.0-1/...
1  https://scontent-lga3-1.xx.fbcdn.net/v/t1.0-1/...
2  https://scontent-lga3-1.xx.fbcdn.net/v/t1.0-1/...
</code></pre>

<p>So, I try to loop the data in dataframe. My problem is the algorithm includes the header (profile) as well, so it turns error. Below is my work:</p>

<pre><code>results = []

for result in df :
    result = CF.face.detect(result) 
    if result == []:                          
        #do something
    else:
        #do something  
    print(results)
</code></pre>

<p>The error I got from this code is (invalid as it loop the 'profile' as well):</p>

<pre><code>  status_code: 400                                                       
        code: InvalidURL
        code: InvalidURL
        message: Invalid image URL.
</code></pre>

<p>My question is, how to write the code so it will loop all the data within column (excluding the 'profile')? I am not sure if put 'df' in 'for result in df ' is a correct way or vice versa. </p>
",Dataset Preprocessing & Handling,set parameter loop specific row excel using python code read data specific column excel column case used column profile result dataframe try loop data dataframe problem algorithm includes header profile well turn error work error got code invalid loop profile well question write code loop data within column excluding profile sure put df result df correct way vice versa
Preprocessing data in Multi-label classification Python,"<p>My dataset structure:</p>

<pre><code>Text: 'Good service, nice view, location'
Tag: '{SERVICE#GENERAL, positive}, {HOTEL#GENERAL, positive}, {LOCATI
ON#GENERAL, positive}'
</code></pre>

<p>And the point here is that I don't know how can I structure my data frame. If you have any recommendations, these will be really nice to me. Thank you.</p>
",Dataset Preprocessing & Handling,preprocessing data multi label classification python dataset structure point know structure data frame recommendation really nice thank
Unable to retrieve the text data into data frame after cleanup in R,"<p>I'm aware that similar questions have been asked here but I still believe my task is more complex.
I read a CSV file with 3 text columns into a data frame. I used <code>tm</code> package to clean the text data. I used the below code:</p>

<pre><code>con_corpus &lt;- Corpus(VectorSource(my_con$Verba)) 
corpus_clean &lt;- tm_map(con_corpus, tolower)
corpus_clean &lt;- tm_map(corpus_clean, removeNumbers)
corpus_clean &lt;- tm_map(corpus_clean, removeWords, stopwords('english'))
corpus_clean &lt;- tm_map(corpus_clean, removePunctuation)
corpus_clean &lt;- tm_map(corpus_clean, stripWhitespace)
corpus_clean &lt;- tm_map(corpus_clean, trim)
</code></pre>

<p>Now my worry is how to do clean the data in all 3 cloumns at a time.
Second is, how to convert this corpus data into dataframe so that I can see whether all the data that I have imported into R is cleaned up successfully.</p>
",Dataset Preprocessing & Handling,unable retrieve text data data frame cleanup r aware similar question asked still believe task complex read csv file text column data frame used package clean text data used code worry clean data cloumns time second convert corpus data dataframe see whether data imported r cleaned successfully
Why am I getting a list object is not callable error on python?,"<p>I am working on this dataset [<a href=""https://archive.ics.uci.edu/ml/datasets/Reuter_50_50]"" rel=""nofollow noreferrer"">https://archive.ics.uci.edu/ml/datasets/Reuter_50_50]</a> and trying to analyze text features.</p>

<p>I read the files and store it as follows in the documents variable:</p>

<pre><code>documents=author_labels(raw_data_dir)
documents.to_csv(documents_filename,index_label=""document_id"")
documents=pd.read_csv(documents_filename,index_col=""document_id"")
documents.head()
</code></pre>

<p>Subsequently, I am trying to generate tf-idf vectors using sublinear growth and storing it in a variable called vectorizer.</p>

<pre><code>vectorizer = TfidfVectorizer(input=""filename"",tokenizer=tokenizer,stop_words=stopwords_C50)
</code></pre>

<p>Then, I try to generate a matrix, X, of <code>tfidf</code> representations for each document in the corpus, using:</p>

<pre><code>X = vectorizer.fit_transform(documents[""filename""])
</code></pre>

<p>However, I am getting the following error:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-152-8c01204baf0e&gt; in &lt;module&gt;
----&gt; 1 X = vectorizer.fit_transform(documents[""filename""])

~\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py in fit_transform(self, raw_documents, y)
   1611         """"""
   1612         self._check_params()
-&gt; 1613         X = super(TfidfVectorizer, self).fit_transform(raw_documents)
   1614         self._tfidf.fit(X)
   1615         # X is already a transformed view of raw_documents so

~\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py in fit_transform(self, raw_documents, y)
   1029 
   1030         vocabulary, X = self._count_vocab(raw_documents,
-&gt; 1031                                           self.fixed_vocabulary_)
   1032 
   1033         if self.binary:

~\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py in _count_vocab(self, raw_documents, fixed_vocab)
    941         for doc in raw_documents:
    942             feature_counter = {}
--&gt; 943             for feature in analyze(doc):
    944                 try:
    945                     feature_idx = vocabulary[feature]

~\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py in &lt;lambda&gt;(doc)
    327                                                tokenize)
    328             return lambda doc: self._word_ngrams(
--&gt; 329                 tokenize(preprocess(self.decode(doc))), stop_words)
    330 
    331         else:

TypeError: 'list' object is not callable
</code></pre>

<p>How do I resolve this issue? </p>
",Dataset Preprocessing & Handling,getting list object callable error python working dataset trying analyze text feature read file store follows document variable subsequently trying generate tf idf vector using sublinear growth storing variable called vectorizer try generate matrix x representation document corpus using however getting following error resolve issue
Preprocessing text data on many columns from a data frame using python,"<p>I'm looking for an answer <a href=""https://stackoverflow.com/questions/48032578/preprocessing-text-analysis-on-many-columns-from-a-dataframe"">like</a> this but in python. How can I do text preprocessing on multiple columns? I have two text columns see <a href=""https://i.sstatic.net/5WvUp.png"" rel=""nofollow noreferrer"">screenshots</a>. To do the cleaning work, I have to do twice to each column (see my code). Is there any clever way to do a similar task? Thanks!</p>

<pre><code>import requests

from bs4 import BeautifulSoup #html.parser'
df['Summary'] = [BeautifulSoup(text).get_text() for text in df['Summary']]
df['Text'] = [BeautifulSoup(text).get_text() for text in df['Text']]


df.loc[:,""Text""] = df.Text.apply(lambda x : str.lower(x))
df.loc[:,""Summary""] = df.Summary.apply(lambda x : str.lower(x))   

#remove punctuation.
df[""Text""] = df['Text'].str.replace('[^\w\s]','')
df[""Summary""] = df['Summary'].str.replace('[^\w\s]','')
</code></pre>
",Dataset Preprocessing & Handling,preprocessing text data many column data frame using python looking answer screenshots cleaning work twice column see code clever way similar task thanks
Add metadata to VectorSource corpus using &#39;tm&#39; library in R,"<p>I have a csv file and I'm trying to convert it into Corpus to use the tm_map later and the apply some clustering. </p>

<p>I read the file</p>

<pre><code>data &lt;- read.csv(""data.csv"", header = TRUE, sep = "","",stringsAsFactors = FALSE)
</code></pre>

<p>Turn what I need into corpus</p>

<pre><code>corp &lt;- Corpus(VectorSource(data$text)) 
</code></pre>

<p>This is the outcome for the metadata</p>

<pre><code>&gt; meta(corp[[1]])
  author       : character(0)
  datetimestamp: 2019-09-20 20:48:45
  description  : character(0)
  heading      : character(0)
  id           : 1
  language     : en
  origin       : character(0)
</code></pre>

<p>Then I try to add the author info, so I can add the date and title afterwards, like this</p>

<pre><code>&gt; for(i in 1:length(corp)) {
+ corp[[i]]$meta$author == data$author[i]
+ }
</code></pre>

<p>but I keep on getting this</p>

<pre><code>&gt; abstract[[1]]$meta$author
character(0)
&gt; meta(abstract[[1]], tag = 'author')
character(0)
</code></pre>

<p>when</p>

<pre><code>&gt; data$author[1]
[1] ""Juan Vásquez Córdoba""
</code></pre>

<p>How can I add the right metadata info to my Corpus?</p>
",Dataset Preprocessing & Handling,add metadata vectorsource corpus using tm library r csv file trying convert corpus use tm map later apply clustering read file turn need corpus outcome metadata try add author info add date title afterwards like keep getting add right metadata info corpus
FastText - Cannot load model.bin due to C++ extension failed to allocate the memory,"<p>I'm trying to use the FastText Python API <a href=""https://pypi.python.org/pypi/fasttext"" rel=""noreferrer"">https://pypi.python.org/pypi/fasttext</a> Although, from what I've read, this API can't load the newer .bin model files at <a href=""https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"" rel=""noreferrer"">https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md</a> as suggested in <a href=""https://github.com/salestock/fastText.py/issues/115"" rel=""noreferrer"">https://github.com/salestock/fastText.py/issues/115</a></p>

<p>I've tried everything that is suggested at that issue, and furthermore <a href=""https://github.com/Kyubyong/wordvectors"" rel=""noreferrer"">https://github.com/Kyubyong/wordvectors</a> doesn't have the .bin for English, otherwise the problem would be solved. Does anyone know of a work-around for this?</p>
",Dataset Preprocessing & Handling,fasttext load model bin due c extension failed allocate memory trying use fasttext python api although read api load newer bin model file suggested tried everything suggested issue furthermore bin english otherwise problem would solved doe anyone know work around
Creating a new data frame based in minimum frequency,"<p>I am trying to create a new data frame <code>TopWords</code> from an existing one. The original data frame <code>data_to_export</code> has too much many words(<code>bios</code>), and I would like to only keep words (<code>bios</code>)that were used frequently, but I also need to keep the ID numbers associated with each word. </p>

<p>This is what I've come up with, but it doesn't work. It doesn't like the <code>if</code> conditional statement, but I don't know how else to do it. </p>

<p><code>TopWords&lt;- data_to_export if freq_terms(data_to_export$bios2 &gt; 4)</code></p>

<p>I would like to end up with the same data from <code>data_to_export</code>, but only the data for cases that have words that occur fives times or more. </p>

<p>For example, </p>

<pre><code>data_to_export (original data)
ID  bios2
1    i
1    love
1    playing
1    soccer
2    i
2    am
2    a
2    teacher
2    mom
2    grandma
2    sister
3    i
3    think
3    soccer
3    is
3    the 
3    best
4    soccer
4    player
5    i
5    like
5    soccer
5    i
5    could
5    play
5    soccer
5    all
5    day


New data frame:
1   i
1   soccer
2   i
3   i
3   soccer
4   soccer
5   i
5   soccer
5   i
5   soccer


</code></pre>

<p>Any help would be greatly appreciated. Thanks!</p>
",Dataset Preprocessing & Handling,creating new data frame based minimum frequency trying create new data frame existing one original data frame ha much many word would like keep word used frequently also need keep id number associated word come work like conditional statement know else would like end data data case word occur five time example help would greatly appreciated thanks
Text data preprocessing in python,"<p>I am working on extraction of positive, negative &amp; neutral keyword in python.There are 10,000 comments in my comments remarks.txt file(encoded UTF-8).I want to import the text file, read the individual row of comments &amp; extract words(tokenize) from the comments mentioned in column c2 &amp; store it in a next adjacent column. I have written a small program calling get_keywords function in Python.I have created get_keywords() function but facing issues passing each row of the dataframe as argument &amp; calling it using iterations  to provide keywords &amp; store it in adjacent columns.</p>

<p>Codes are not providing expected column ""tokens"" with all the processed words in the df dataframe. </p>

<pre><code>    import nltk
    import pandas as pd
    import re
    import string
    from nltk import sent_tokenize, word_tokenize
    from nltk.corpus import stopwords
    from nltk.stem.porter import PorterStemmer
    remarks = pd.read_csv('/Users/ZKDN0YU/Desktop/comments/New 
    comments/ccomments.txt')
    df = pd.DataFrame(remarks, columns= ['c2'])
    df.head(50)
    df.tail(50)

    filename = 'ccomments.txt'
    file = open(filename, 'rt', encoding=""utf-8"")
    text = file.read()
    file.close()

    def get_keywords(row):     
    # split into tokens by white space
      tokens = text.split(str(row))
    # prepare regex for char filtering
      re_punc = re.compile('[%s]' % re.escape(string.punctuation))
    # remove punctuation from each word
      tokens = [re_punc.sub('', w) for w in tokens]
    # remove remaining tokens that are not alphabetic
      tokens = [word for word in tokens if word.isalpha()]
    # filter out stop words
      stop_words = set(stopwords.words('english'))
      tokens = [w for w in tokens if not w in stop_words]
    # stemming of words
      porter = PorterStemmer()
      stemmed = [porter.stem(word) for word in tokens]
    # filter out short tokens
      tokens = [word for word in tokens if len(word) &gt; 1]
      return tokens
      df['tokens'] = df.c2.apply(lambda row: get_keywords(row['c2']), 
       axis=1)
      for index, row in df.iterrows():
      print(index, row['c2'],""tokens : {}"".format(row['tokens']))
</code></pre>

<p>Expected Output:- A Comments_modified file containing columns 1)index,2) c2(Comments) &amp; 3)tokenized words for all rows of the dataframe having  10,000 comments.</p>
",Dataset Preprocessing & Handling,text data preprocessing python working extraction positive negative neutral keyword python comment comment remark txt file encoded utf want import text file read individual row comment extract word tokenize comment mentioned column c store next adjacent column written small program calling get keywords function python created get keywords function facing issue passing row dataframe argument calling using iteration provide keywords store adjacent column code providing expected column token processed word df dataframe expected output comment modified file containing column index c comment tokenized word row dataframe comment
I am wondering how to make a term document matrix of keywords consisting of several words in R,"<p>Is there a way to make a data frame like this into a term document matrix? Each keyword consists of two or more words.</p>

<h3>Example data</h3>

<p>Data type is a data frame.</p>

<pre><code>doc_id text
1      c('cat dog', 'cat rat')
2      c('cat dog')
3      c('cat rat')
</code></pre>

<h3>Desired result</h3>

<p>I want to get this result. The TermDocumentMatrix function already exists does not reflect a multiword keyword.</p>

<pre><code>         Docs
Terms    1 2 3
cat dog  1 1 0
cat rat  1 0 1
</code></pre>
",Dataset Preprocessing & Handling,wondering make term document matrix keywords consisting several word r way make data frame like term document matrix keyword consists two word example data data type data frame desired result want get result termdocumentmatrix function already exists doe reflect multiword keyword
Building and extending a Knowledge Graph with entity extraction while Neo4j for my database,"<p>My goal is to build an automated Knowledge Graph. I have decided to use Neo4j as my database. I am intending to load a json file from my local directory to Neo4j. The data I will be using are the yelp datasets(the json files are quite large).</p>

<p>I have seen some Neo4j examples with Graphaware and OpenNLP. I read that Neo4j has a good support for JAVA apps. I have also read that Neoj supports python(I am intending to use nltk). Is it advisable to use Neo4j with JAVA maven/gradle and OpenNLP? Or should I use it with py2neo with nltk. </p>

<p>I am really sorry that I don't have any prior experience with these tools. Any advice or recommendation will be greatly appreciated. Thank you so much!</p>
",Dataset Preprocessing & Handling,building extending knowledge graph entity extraction neo j database goal build automated knowledge graph decided use neo j database intending load json file local directory neo j data using yelp datasets json file quite large seen neo j example graphaware opennlp read neo j ha good support java apps also read neoj support python intending use nltk advisable use neo j java maven gradle opennlp use py neo nltk really sorry prior experience tool advice recommendation greatly appreciated thank much
Unable to load Sentihood dataset Json file in Python,"<p>Sentihood Dataset is a dataset for Target Aspect-based Sentiment Analysis. Its Test and Train file are available in Json format. However, when I try loading it using the json module of python, it gives the following error-</p>

<p>JSONDecodeError: Expecting value: line 7 column 1 (char 6)</p>

<p>Is there some other way of loading Json files? I don't have much knowledge of Json and hence would appreciate any help.
Link for Sentihood dataset : <a href=""https://github.com/uclmr/jack/tree/master/data/sentihood"" rel=""nofollow noreferrer"">https://github.com/uclmr/jack/tree/master/data/sentihood</a></p>

<p>My code is simply:</p>

<pre><code>with open(""sentihood-train.json"", ""r"") as read_it: 
    data = json.load(read_it)
</code></pre>
",Dataset Preprocessing & Handling,unable load sentihood dataset json file python sentihood dataset dataset target aspect based sentiment analysis test train file available json format however try loading using json module python give following error jsondecodeerror expecting value line column char way loading json file much knowledge json hence would appreciate help link sentihood dataset code simply
Is there a function that allows me to determine if a text talks about a pre defined topic?,"<p>I want to write topic lists to check whether a review talks about one of the defined topics. It's important for me to write the topic lists myself and not use topic modeling to find possible topics.</p>

<p>I thought this is called dictionary analysis, but I can't find anything.</p>

<p>I have a data frame with reviews from amazon:</p>

<pre><code>df = pd.DataFrame({'User': ['UserA', 'UserB','UserC'], 
'text': ['Example text where he talks about a phone and his charging cable',
 'Example text where he talks about a car with some wheels',
 'Example text where he talks about a plane']})
</code></pre>

<p>Now I want to define topic lists:</p>

<pre><code>phone = ['phone', 'cable', 'charge', 'charging', 'call', 'telephone']
car = ['car', 'wheel','steering', 'seat','roof','other car related words']
plane = ['plane', 'wings', 'turbine', 'fly']
</code></pre>

<p>The result of the method should be 3/12 for the ""phone"" topic of the first review (3 words of the topic list where in the review which has 12 words) and 0 for the other two topics.</p>

<p>The second review would result in 2/11 for the ""car"" topic and 0 for the other topics and for the third review 1/8 for the ""plane"" topic and 0 for the others.</p>

<p>Results as a list:</p>

<pre><code>phone_results = [0.25, 0, 0]
car_results = [0, 0.18181818182, 0]
plane_results = [0, 0, 0.125]
</code></pre>

<p>Of course I would only use lowercase wordstems of the reviews which makes defining topics easier, but this should not be of concern now.</p>

<p>Is there a method for this or do I have to write one?
Thank you in advance!</p>
",Dataset Preprocessing & Handling,function allows determine text talk pre defined topic want write topic list check whether review talk one defined topic important write topic list use topic modeling find possible topic thought called dictionary analysis find anything data frame review amazon want define topic list result method phone topic first review word topic list review ha word two topic second review would result car topic topic third review plane topic others result list course would use lowercase wordstems review make defining topic easier concern method write one thank advance
Automatic labeling of LDA generated topics,"<p>I'm trying to categorize customer feedback and I ran an LDA in python and got the following output for 10 topics:</p>

<pre><code>(0, u'0.559*""delivery"" + 0.124*""area"" + 0.018*""mile"" + 0.016*""option"" + 0.012*""partner"" + 0.011*""traffic"" + 0.011*""hub"" + 0.011*""thanks"" + 0.010*""city"" + 0.009*""way""')
(1, u'0.397*""package"" + 0.073*""address"" + 0.055*""time"" + 0.047*""customer"" + 0.045*""apartment"" + 0.037*""delivery"" + 0.031*""number"" + 0.026*""item"" + 0.021*""support"" + 0.018*""door""')
(2, u'0.190*""time"" + 0.127*""order"" + 0.113*""minute"" + 0.075*""pickup"" + 0.074*""restaurant"" + 0.031*""food"" + 0.027*""support"" + 0.027*""delivery"" + 0.026*""pick"" + 0.018*""min""')
(3, u'0.072*""code"" + 0.067*""gps"" + 0.053*""map"" + 0.050*""street"" + 0.047*""building"" + 0.043*""address"" + 0.042*""navigation"" + 0.039*""access"" + 0.035*""point"" + 0.028*""gate""')
(4, u'0.434*""hour"" + 0.068*""time"" + 0.034*""min"" + 0.032*""amount"" + 0.024*""pay"" + 0.019*""gas"" + 0.018*""road"" + 0.017*""today"" + 0.016*""traffic"" + 0.014*""load""')
(5, u'0.245*""route"" + 0.154*""warehouse"" + 0.043*""minute"" + 0.039*""need"" + 0.039*""today"" + 0.026*""box"" + 0.025*""facility"" + 0.025*""bag"" + 0.022*""end"" + 0.020*""manager""')
(6, u'0.371*""location"" + 0.110*""pick"" + 0.097*""system"" + 0.040*""im"" + 0.038*""employee"" + 0.022*""evening"" + 0.018*""issue"" + 0.015*""request"" + 0.014*""while"" + 0.013*""delivers""')
(7, u'0.182*""schedule"" + 0.181*""please"" + 0.059*""morning"" + 0.050*""application"" + 0.040*""payment"" + 0.026*""change"" + 0.025*""advance"" + 0.025*""slot"" + 0.020*""date"" + 0.020*""tomorrow""')
(8, u'0.138*""stop"" + 0.110*""work"" + 0.062*""name"" + 0.055*""account"" + 0.046*""home"" + 0.043*""guy"" + 0.030*""address"" + 0.026*""city"" + 0.025*""everything"" + 0.025*""feature""') 
</code></pre>

<p>Is there a way to automatically label them? I do have a csv file which has feedbacks manually labeled, but I do not want to supply these labels myself. I want the model to create labels. Is it possible?</p>
",Dataset Preprocessing & Handling,automatic labeling lda generated topic trying categorize customer feedback ran lda python got following output topic way automatically label csv file ha feedback manually labeled want supply label want model create label possible
Directly load spacy model from packaged tar.gz file,"<p>Is it possible to load a packaged spacy model (i.e. <code>foo.tar.gz</code>) directly from the tar file instead of installing it beforehand? I would imagine something like:</p>

<pre><code>import spacy 

nlp = spacy.load(/some/path/foo.tar.gz)
</code></pre>
",Dataset Preprocessing & Handling,directly load spacy model packaged tar gz file possible load packaged spacy model e directly tar file instead installing beforehand would imagine something like
Fuzzy merge in pandas and closest row match,"<p>I have two data frames. Each contains 1 word per row. They are pretty close, but there are misspellings and sometimes one df has one or two words the other doesn't. </p>

<p>As a rule, I want to combine df2.word with df1.metadata. If df2.word and df1.word match, are close in spelling, or are close enough and within 1 row from each other, I want to join df2.word with df1.metadata. If there is no close match directly or within 1 row, I want to drop this row.</p>

<p>I have:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>  </code></pre>
<pre class=""snippet-code-html lang-html prettyprint-override""><code>df1

word      metadata    metadata2

okay        1           A
I           1           A
win         1           A
tree        1           A
apples      1           A
also        0           B
would       0           B
like        0           B
for         0           B 
oranges     0           B

df2

word

OK.         
I          
want        
three       
apples.     
Also,        
I           
would       
like          
four        
oranges.    


What I want is:

word      metadata    metadata2

OK.         1           B
I           1           B
want        1           B
three       1           B
apples.     1           B
Also,       0           B       
would       0           B
like        0           B
four        0           B
oranges.    0           B</code></pre>
</div>
</div>
</p>
",Dataset Preprocessing & Handling,fuzzy merge panda closest row match two data frame contains word per row pretty close misspelling sometimes one df ha one two word rule want combine df word df metadata df word df word match close spelling close enough within row want join df word df metadata close match directly within row want drop row
PolyAnalyst: Is there a way to read in the bookmarks of a PDF file?,"<p>I have a PDF with bookmarks. I tried to read it into PolyAnalyst but looks like it completely ignores the bookmarks. Is there a way to make PA keep the bookmarks of the file?</p>
",Dataset Preprocessing & Handling,polyanalyst way read bookmark pdf file pdf bookmark tried read polyanalyst look like completely ignores bookmark way make pa keep bookmark file
"How to find search words from a table, in another table, and then create new columns of the results?","<p>I'm trying to find specifice words listed in a tibble <code>arbeit</code> in the another tibble <code>rawEng$Text</code>. If a word, or words, were found, I want to create, or mutate, a new data frame <code>iDataArbeit</code> with two new columns, one for the found word/s  <code>wArbeit</code>, and one for the sum of there tf-idf <code>iArbeit</code>scores from <code>arbeit$tfidf</code></p>

<p>My Data: </p>

<p>arbeit:</p>

<pre><code>     X1 feature                   tfidf
  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;
1     0 sick                      0.338
2     2 contract                  0.188
3     3 pay                       0.175
4     4 job                       0.170
5     5 boss                      0.169
6     6 sozialversicherungsnummer 0.169
</code></pre>

<p>rawEng:</p>

<pre><code>Gender Gruppe        Datum               Text                                            
  &lt;chr&gt;  &lt;chr&gt;         &lt;dttm&gt;              &lt;chr&gt;                                           
1 F      Berlin Expats 2017-07-07 00:00:00 Anyone out there who's had to apply for Führung~
2 F      FAB           2018-01-18 00:00:00 Dear FAB, I am in need of a Führungszeugnis no ~
3 M      Free Advice ~ 2017-01-30 00:00:00 Dear Friends, i would like to ask you how can I~
4 M      FAB           2018-04-12 00:00:00 ""Does anyone know why the \""Standesamt Pankow (~
5 F      Berlin Expats 2018-11-12 00:00:00 having trouble finding consistent information a~
6 F      Toytown Berl~ 2017-06-08 00:00:00 ""Hello\r\n\r\nI have a question regarding Airbn~
</code></pre>

<p>I've tried with <code>dplyr::mutate</code>, using this code: </p>

<pre><code>idataEnArbeit &lt;- mutate(rawEng, wArbeit = ifelse((str_count(rawEng$Text, arbeit$feature))&gt;=1,
                                                       arbeit$feature, NA),
                        iArbeit = ifelse((str_count(rawEng$Text, arbeit$feature))&gt;=1,
                                         arbeit$tfidf, NA))
</code></pre>

<p>but all I get is one Word, and it's tf-idf score, in the new columens <code>iDatatArbeit$wArbeit</code>and <code>iDataArbeit$iArbeit</code></p>

<pre><code>Gender Gruppe          Datum               Text                           wArbeit iArbeit
  &lt;chr&gt;  &lt;chr&gt;           &lt;dttm&gt;              &lt;chr&gt;                          &lt;chr&gt;     &lt;dbl&gt;
1 F      Berlin | Girl ~ 2018-09-11 13:22:05 ""11 septembre, 13:21     GGI ~ sick      0.338
2 F      ExpatBabies Be~ 2017-10-19 16:24:23 ""16:24   Babysitter needed! B~ sick      0.338
3 F      Berlin | Girl ~ 2018-06-22 18:24:19 ""gepostet.       Leonor Valen~ sick      0.338
4 F      'Neu in Berlin' 2018-09-18 23:19:51 ""Hello guys, I am working wit~ sick      0.338
5 M      Free Advice Be~ 2018-04-27 08:49:24 ""In need of legal advice: Wha~ sick      0.338
6 F      Free Advice Be~ 2018-07-04 18:33:03 ""Is there somebody I can pay ~ sick      0.338
</code></pre>

<p>In summary: I want all words from <code>arbeit$feature</code> which are found in <code>rawEng$Text</code> to be added in <code>iDataArbeit$wArbeit</code>, and the sum of there tf-idf score to be added in <code>iDataArbeit$iArbeit</code></p>
",Dataset Preprocessing & Handling,find search word table another table create new column result trying find specifice word listed tibble another tibble word word found want create mutate new data frame two new column one found word one sum tf idf score data arbeit raweng tried using code get one word tf idf score new columens summary want word found added sum tf idf score added
Is there a simple way to search for a string from one dataframe in another df and return an associated value?,"<p>I have two data frames (df1 and df2), each with the columns ""Words"" and ""Frequency"". For each word in df1, I want to see if it exists in df2 and then return the ""Frequency"" value so that it can be appended to include the new instances from df1. And if the word does not exist in df2, then add it. </p>

<p>I have found ways of appending dataframes, but I haven't been able to create a functional loop to do what I have described. I was trying to use Pandas and df.query but had no luck. </p>

<p>In the example below I want it to add the words ""This"", ""is"", ""test"", and ""dataframe""
along with their frequency, and I want to append ""a"" in df2 to be the sum of both frequency values (4 + 222 = 226)</p>

<p>[in]</p>

<pre><code>     df1 = pd.DataFrame({'Words': [""this"",""is"",""a"",""test"",""dataframe""], 
           'Frequency': [20,18,4,12,6]})
</code></pre>

<p>[out]</p>

<pre><code>        Words   Frequency
    0   this    20
    1   is  18
    2   a   4
    3   test    12
    4   dataframe   6
</code></pre>

<p>[in]</p>

<pre><code>    df2 = pd.read_csv(""Words.csv"")
</code></pre>

<p>[out]</p>

<pre><code>             Word   Frequency
    0   the 562
    1   to  246
    2   a   222
    3   of  204
    4   and 200
</code></pre>
",Dataset Preprocessing & Handling,simple way search string one dataframe another df return associated value two data frame df df column word frequency word df want see exists df return frequency value appended include new instance df word doe exist df add found way appending dataframes able create functional loop described wa trying use panda df query luck example want add word test dataframe along frequency want append df sum frequency value
How can I plot the quantity of a group in a dataframe?,"<p>I am using a Twitter data set (@realDonaldTrump). On this data set I want to do some NLP stuff. 
Right now I would like to plot the quantity of the column ""Source"".
The data frame looks like that (I cropped it):</p>

<pre><code>    User                Tweet_ID            Source      
0   @realDonaldTrump    1138445389709885445 Twitter for iPhone  
1   @realDonaldTrump    1138444530020245505 Twitter for iPhone
</code></pre>

<p>I already tried to plot all the objects in ""Source"" using <code>groupby().count()</code>:</p>

<pre class=""lang-py prettyprint-override""><code>%matplotlib inline
import pandas as pd
df_trump = pd.read_csv('@realDonaldTrump_tweets.csv')
df = df_trump
df
df.groupby([df.Source]).count().plot(kind='bar')
</code></pre>

<p>I get a nice plot, but I only want one(!) bar for each object. Not all the headers displayed. So in this following picture, the ""Twitter for iPhone"" bar should be one bar with not the data frame headers displayed. </p>

<p><a href=""https://i.sstatic.net/h81Zb.png"" rel=""nofollow noreferrer"">Output: Bar Plot</a></p>
",Dataset Preprocessing & Handling,plot quantity group dataframe using twitter data set realdonaldtrump data set want nlp stuff right would like plot quantity column source data frame look like cropped already tried plot object source using get nice plot want one bar object header displayed following picture twitter iphone bar one bar data frame header displayed output bar plot
How does one choose a column in CSV file for tokenization with python?,"<p>I have loaded a data set (CSV-File) which has a column <code>project description</code>
I need to load the words/sentences in this column for tokenzation. I tried the following code:</p>

<pre><code>token = word_tokenize('school_state')
</code></pre>

<p>But that does not work and <em>Behaviour</em> happens instead.</p>
",Dataset Preprocessing & Handling,doe one choose column csv file tokenization python loaded data set csv file ha column need load word sentence column tokenzation tried following code doe work behaviour happens instead
ResourceExhausted: 429 Quota exceeded for quota metric Natural Language API through Dataflow using Python SDK,"<p>I am building Dataflow pipleline to read fro CSV , perform Sentiment analysis through Google Cloud NLP API and send teh result to BigQuery.</p>

<p>when the function that perform sentiment analysis get the pcollection is gives me the above mentioned error.</p>

<p>What I am thinking about is splitting the Pcollection into small Pcollection in order handle Quote limitation in NLP API.</p>

<pre><code>(p
       | 'ReadData' &gt;&gt; beam.io.textio.ReadFromText(src_path)
       | 'ParseCSV' &gt;&gt; beam.ParDo(Analysis())
       | 'WriteToBigQuery' &gt;&gt; ...
)
</code></pre>
",Dataset Preprocessing & Handling,resourceexhausted quota exceeded quota metric natural language api dataflow using python sdk building dataflow pipleline read fro csv perform sentiment analysis google cloud nlp api send teh result bigquery function perform sentiment analysis get pcollection give mentioned error thinking splitting pcollection small pcollection order handle quote limitation nlp api
Removing Stopwords from Column of Strings in Python,"<p>I'm working on a project to read the text and make a prediction of the outcome. As part of cleaning the data I am trying to remove all of the stopwords. When I try to do this, I need the output to be in a datafram format but I am running into issues there.</p>
<p>So, after much cleaning I got the data to the point where it looks like this.
<a href=""https://i.sstatic.net/z8iMv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/z8iMv.png"" alt=""enter image description here"" /></a></p>
<p>The labels are in a different dataframe that I would have to merge but that is besides the point.</p>
<p>What I am trying to do now is remove all of the stopwords from each string in each row.</p>
<p>After some research the code I am using looks like this:</p>
<pre><code>import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
ht_comments_only_no_stop['All_Comments'] = ht_comments_only_summary['All_Comments'].apply(lambda x: [item for item in x if item not in stop_words])
</code></pre>
<p>The ht_comments_only_summary is basically what you see in the first picture above.</p>
<p>The problem is that now when I try looking at &quot;ht_comments_only_no_stop&quot; I see:</p>
<p><a href=""https://i.sstatic.net/k1tNI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/k1tNI.png"" alt=""enter image description here"" /></a></p>
<p>But what I need is the output to just look like the first picture in dataframe format minus all the stopwords under the &quot;All_Comments&quot; column.</p>
<p>Any help would be greatly appreciated.</p>
",Dataset Preprocessing & Handling,removing stopwords column string python working project read text make prediction outcome part cleaning data trying remove stopwords try need output datafram format running issue much cleaning got data point look like label different dataframe would merge besides point trying remove stopwords string row research code using look like ht comment summary basically see first picture problem try looking ht comment stop see need output look like first picture dataframe format minus stopwords comment column help would greatly appreciated
Textual Entailment on large data corpus,"<p>I'm working on textual entailment recently and I wanted to know the current research projects pertaining to Textual entailment on large datasets.</p>

<p>I have read research papers for the same and wanted to explore if there are any possibility of applying data augmentation to make the limited dataset robust or even applying Hierarchical attention mechanism? </p>

<p>Your valuable thoughts and references are very much appreciable</p>
",Dataset Preprocessing & Handling,textual entailment large data corpus working textual entailment recently wanted know current research project pertaining textual entailment large datasets read research paper wanted explore possibility applying data augmentation make limited dataset robust even applying hierarchical attention mechanism valuable thought reference much appreciable
How to make a matrix of duplicate word in text data,"<p>I want to read some text file and Find out how many times each word is repeated per line?
this is my text file</p>

<ol>
<li>خواب خودرو چگونه محاسبه می گردد؟ برای دریافت آن چه باید كرد؟</li>
<li>مهلت زمانی تامین قطعه پس از درخواست مشتری چند روز است؟</li>
<li>آیا در مراجعه مجدد برای ایرادی كه پس از تعمیرات رفع نشده است باید هزینه ای پرداخت گردد؟ چرا؟</li>
<li>چرا توزیع قطعات در نمایندگی ها مختلف شهر متفاوت است؟</li>
</ol>

<p>and make a output like this</p>

<pre><code>line#       word#1    word#2    word#3 ......
  1            2        0          1 
  2            0        0          2
  .
  .
  .
</code></pre>

<p>i want to create a function to do this , i can't use countvectorizer function for persian language</p>
",Dataset Preprocessing & Handling,make matrix duplicate word text data want read text file find many time word repeated per line text file make output like want create function use countvectorizer function persian language
Best approach to feature engineering in natural language processing?,"<p>I am trying to cluster a large corpus of documents and would like to also subsequently explain what characterises each cluster in terms of the most common shared keywords or key-phrases in each cluster. To make clustering feasible, I will use a dimensionality reduction method like for example LSA (SVD) or doc2vec.</p>

<p>I can see several possible paths to the dimensionality reduced feature matrix:</p>

<ol>
<li><p>POS-tagging, chunking (shallow parsing) for nounal phrases (NPs), TF-IDF, LSA (SVD)</p></li>
<li><p>n-grams, TF-IDF, LSA (SVD)</p></li>
<li><p>doc2vec all the way.</p></li>
</ol>

<p>What are some of the advantages and disadvantages of these and perhaps other approaches? And what is ultimately the best way to go?</p>
",Dataset Preprocessing & Handling,best approach feature engineering natural language processing trying cluster large corpus document would like also subsequently explain characterises cluster term common shared keywords key phrase cluster make clustering feasible use dimensionality reduction method like example lsa svd doc vec see several possible path dimensionality reduced feature matrix po tagging chunking shallow parsing nounal phrase np tf idf lsa svd n gram tf idf lsa svd doc vec way advantage disadvantage perhaps approach ultimately best way go
Extract features and labels from a Tensorflow Dataset,"<p>I have a TextLineDataset that reads lines from a text file.</p>

<p>This dataset reads the file and returns it in a sliding window manner, so for example if my text file contains:</p>

<pre><code>I am going to school
School is far from home
</code></pre>

<p>My dataset returns:</p>

<pre><code>I am going
am going to
going to school
...
</code></pre>

<p>(Assuming I want 3 words at a time, sliding from one word at each step)</p>

<p>I am happy with that.</p>

<p>But now I want, for each sentence returned by the dataset, to extract the first 2 words and say they are my features, and to extract the last word and say it is my label</p>

<p>Of course I want it to be part of the computation graph (like my dataset) and not at running time</p>

<p>Here is my code:</p>

<pre><code>sentences = tf.data.TextLineDataset(""data/train.src"")
words = sentences.map(lambda string: tf.string_split([string]).values)
flat_words = words.flat_map(tf.data.Dataset.from_tensor_slices)

flat_words = flat_words.window(3, 1, 1, True).flat_map(lambda x: x.batch(3)).batch(4)

iterator = flat_words.make_initializable_iterator()
next_element = iterator.get_next()

sess = tf.Session()
sess.run(tf.tables_initializer())
sess.run(iterator.initializer)

print(sess.run(next_element))
</code></pre>

<p>Thanks in advance</p>
",Dataset Preprocessing & Handling,extract feature label tensorflow dataset textlinedataset read line text file dataset read file return sliding window manner example text file contains dataset return assuming want word time sliding one word step happy want sentence returned dataset extract first word say feature extract last word say label course want part computation graph like dataset running time code thanks advance
Generate bigrams BUT only noun and verb combinations,"<p>I have some code below that generates bigrams for my data frame column.</p>

<pre><code>import nltk
import collections
counts = collections.Counter()
for sent in df[""message""]:
    words = nltk.word_tokenize(sent)
    counts.update(nltk.bigrams(words))
counts = {k: v for k, v in counts.items() if v &gt; 25}
</code></pre>

<p>This works great for generating my most common bigrams in the 'message' column of my dataframe, BUT, I want to get bigrams that contain one verb and one noun per pair of bigrams only.  </p>

<p>Any help doing this with spaCy or nltk would be appreciated!</p>
",Dataset Preprocessing & Handling,generate bigram noun verb combination code generates bigram data frame column work great generating common bigram message column dataframe want get bigram contain one verb one noun per pair bigram help spacy nltk would appreciated
Confusion matrix for LDA,"<p>I’m trying to check the performance of my LDA model using a confusion matrix but I have no clue what to do.  I’m hoping someone can maybe just point my in the right direction. </p>

<p>So I ran an LDA model on a corpus filled with short documents. I then calculated the average vector of each document and then proceeded with calculating cosine similarities. </p>

<p>How would I now get a confusion matrix?  Please note that I am very new to the world of NLP. If there is some other/better way of checking the performance of this model please let me know.</p>
",Dataset Preprocessing & Handling,confusion matrix lda trying check performance lda model using confusion matrix clue hoping someone maybe point right direction ran lda model corpus filled short document calculated average vector document proceeded calculating cosine similarity would get confusion matrix please note new world nlp better way checking performance model please let know
Selective text extraction in Python based on certain topics or keywords,"<p>I have a quite long text document describing behaviours of different animals. I want to extract text about a specific animal and haven't figured out how this can be done.</p>

<p>So for example, if the document descibes 15 different animals, I want my alorithm to output all information from the input file that related to lions. Lions described and discussed in several different places of the document - how do I do ""selective extraction"" for text that is only related to lions, does anyone know?</p>

<h1>EDIT - inputs and outputs</h1>

<p>Inputs:
(1) Text file (e.g. ""document.txt"")
(2) Key word(s) (e.g. ""lion"")</p>

<p>Output (example):
""Lions are large felines that are traditionally depicted as the 'king of the jungle.' These big cats once roamed Africa, Asia and Europe. [...] Males are generally larger than females and have a distinctive mane of hair around their heads [...] Asiatic lions eat large animals as well, such as goats, nilgai, chital, sambhar and buffaloes. [...] Females have a gestation period of around four months. She will give birth to her young away from others and hide the cubs for the first six weeks of their lives.""</p>
",Dataset Preprocessing & Handling,selective text extraction python based certain topic keywords quite long text document describing behaviour different animal want extract text specific animal figured done example document descibes different animal want alorithm output information input file related lion lion described discussed several different place document selective extraction text related lion doe anyone know edit input output input text file e g document txt key word e g lion output example lion large feline traditionally depicted king jungle big cat roamed africa asia europe male generally larger female distinctive mane hair around head asiatic lion eat large animal well goat nilgai chital sambhar buffalo female gestation period around four month give birth young away others hide cub first six week life
Extract text from .txt file and save into .csv files with columns and header,"<p>I have approximately 100 text files with clinical notes that consist of 1-2 paragraphs. Each file is named doc_1.txt to doc_179.txt accordingly. I would like to save the text from each file into a .csv file with 2 columns w/ headers (id, text). The  <code>id</code> columns are the name of each files. </p>

<p>For example <code>doc_1</code> is the record file name and will become the id. The text in <code>doc_1</code> will be stored the <code>text column</code>. The desired results is below</p>

<pre><code>
|   id  | text |
|:-----:|:----:|
| doc_1 | abcf |
| doc_2 | efrf |
| doc_3 | gvni |


</code></pre>

<p>So far I am to just viewed the text and have not determine the best practical way to achieve my results. </p>
",Dataset Preprocessing & Handling,extract text txt file save csv file column header approximately text file clinical note consist paragraph file named doc txt doc txt accordingly would like save text file csv file column w header id text column name file example record file name become id text stored desired result far viewed text determine best practical way achieve result
"Given csv file , I need to print those sentence which are most similar based upon the similarity score","<p>For a given <code>.csv</code> file containing 'N'  ID, sentence
Problem is to find most matched sentence in given csv file</p>
",Dataset Preprocessing & Handling,given csv file need print sentence similar based upon similarity score given file containing n id sentence problem find matched sentence given csv file
How to extract all adjectives from a strings of text in a pandas dataframe?,"<p>I am loading a CSV into a pandas data frame. One of the columns in the dataframe is ""reviews"" which contain strings of text. I need to identify all the adjectives in this column in all the rows of the dataframe and then create a new column ""adjectives"" that contains a list of all the adjectives from that review.  </p>

<p>I've tried using TextBlobs and was able to tag the parts of speech for each case using the code posted. </p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from textblob import TextBlob

df=pd.read_csv('./data.csv')

def pos_tag(text):
    try:
        return TextBlob(text).tags
    except:
        return None

df['pos'] = df['reviews'].apply(pos_tag)

df.to_csv('dataadj.csv', index=False)
</code></pre>
",Dataset Preprocessing & Handling,extract adjective string text panda dataframe loading csv panda data frame one column dataframe review contain string text need identify adjective column row dataframe create new column adjective contains list adjective review tried using textblobs wa able tag part speech case using code posted
how to view tf-idf score against each word,"<p>I was trying to know the <code>tf-idf</code> scores of each word in my document. However, it only returns values in the matrix but I see a specific type of representation of <code>tf-idf</code> scores against each word.</p>

<p>I have used processed and the code works however I want to change the way it is presented:</p>

<p>code:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer 
from sklearn.feature_extraction.text import TfidfTransformer

bow_transformer = CountVectorizer(analyzer=text_process).fit(df[""comments""].head())
print(len(bow_transformer.vocabulary_))

tfidf_transformer = CountVectorizer(analyzer=text_process).fit(messages['message'])
bow_transformer.vocabulary_transformer().fit(message_bow)

message_tfidf = tfidf_transformer.transform(message_bow)
</code></pre>

<p>I get the results like this <code>(39028,01),(1393,1672)</code>. However, I expect the results to be like</p>

<pre><code>features    tfidf
fruit       0.00344
excellent   0.00289
</code></pre>
",Dataset Preprocessing & Handling,view tf idf score word wa trying know score word document however return value matrix see specific type representation score word used processed code work however want change way presented code get result like however expect result like
pytorch TabularDataset: can&#39;t load a large file?,"<p>I have this training set of about 1.4 GB, but I can't load it in a TabularDataset.</p>

<pre><code>&gt;&gt;&gt; REF = data.Field(lower=True, tokenize=tokenize_char, init_token='&lt;sos&gt;',eos_token='&lt;eos&gt;')
&gt;&gt;&gt; SRC = data.Field(lower=True, tokenize=tokenize_char)
&gt;&gt;&gt; train = data.TabularDataset('./train.csv', format='csv', fields= [('src', SRC), ('ref', REF)])
Killed
</code></pre>

<p>For small dataset, this code works fine. I don't really understand why the process got killed, since the dataset is not enormous large. Any idea how I can load this file? thanks!</p>
",Dataset Preprocessing & Handling,pytorch tabulardataset load large file training set gb load tabulardataset small dataset code work fine really understand process got killed since dataset enormous large idea load file thanks
"How to write a code for a dataset in which one of the columns contains punctuations, spaces and to delete the corresponding row to it?","<p>I am trying to clean a dataset which contain some chinese characters and get rid of those rows which contain chinese characters.</p>

<p>I have first tried replacing chinese characters with a space and then tried to use regex to fill the dataset with only those rows and columns which don't have the spaces and punctuations.</p>

<pre><code>        df[""reviewer_name""] = df[""reviewer_name""].str.replace(r'[^\x00-\x7F]+','')
        df['comments'] = df['comments'].str.replace(r'[^\x00-\x7F]+', '')
        df = df[df['comments'].str.contains(r'\W+', na=False)]
        df
</code></pre>

<p>The data is like this - </p>

<ul>
<li><p>data -<br>
title_id             date        Reviewer_name           comments</p>

<p>258716           2019-04-21       Heap Chuan        新公寓,很干净,还有管理员接待</p></li>
</ul>

<p>-Expected-
All the rows with chinese character to be gone from dataset</p>
",Dataset Preprocessing & Handling,write code dataset one column contains punctuation space delete corresponding row trying clean dataset contain chinese character get rid row contain chinese character first tried replacing chinese character space tried use regex fill dataset row column space punctuation data like data title id date reviewer name comment heap chuan expected row chinese character gone dataset
Word2Vec: Error received at uploading a pre-trained word2vec file using Gensim,"<blockquote>
  <p>I receive an error when trying to upload a pre-trained word2vec file
  (compiled with fasttext) using Gensim. File has '.vec' extension and
  can be found here:
  <a href=""http://89.38.230.23/word_embeddings/we/corola.300.20.vec.zip"" rel=""nofollow noreferrer"">http://89.38.230.23/word_embeddings/we/corola.300.20.vec.zip</a></p>
  
  <p>What I've tried so far: Option 1: KeyedVectors from gensim.models
  Option 2: FastText wrapper</p>
</blockquote>

<pre><code>#Option 1
    from gensim.models import KeyedVectors
    model = KeyedVectors.load_word2vec_format('Word_embeddings/corola.300.20.vec', binary=True)
######

#Option 2
    from gensim.models.wrappers import FastText
    model = FastText.load_word2vec_format('Word_embeddings/corola.300.20.vec')
</code></pre>

<blockquote>
  <p>Error option 1: UnicodeDecodeError: 'utf-8' codec can't decode byte
  0x9b in position 0: invalid start byte</p>
  
  <p>Deprecation Error option 2: DeprecationWarning: Deprecated. Use
  gensim.models.KeyedVectors.load_word2vec_format instead.</p>
  
  <p>I need the correct method to successfully upload the word2vec file,
  using gensim.</p>
  
  <p>Thank you.</p>
</blockquote>
",Dataset Preprocessing & Handling,word vec error received uploading pre trained word vec file using gensim receive error trying upload pre trained word vec file compiled fasttext using gensim file ha vec extension found tried far option keyedvectors gensim model option fasttext wrapper error option unicodedecodeerror utf codec decode byte x b position invalid start byte deprecation error option deprecationwarning deprecated use gensim model keyedvectors load word vec format instead need correct method successfully upload word vec file using gensim thank
How to load the file use the GCS path without IOError?,"<p>I have met some problem when I run the XLNet code on Google Cloud TPU. When I choose the <code>gs://{model_path}/...</code> as the model path, it turns out the <code>IOError</code>. </p>

<p>Like this:</p>

<pre><code>Traceback (most recent call last):
  File ""run_classifier.py"", line 903, in &lt;module&gt;
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""run_classifier.py"", line 722, in main
    sp.Load(FLAGS.spiece_model_file)
  File ""/usr/local/lib/python2.7/dist-packages/sentencepiece.py"", line 118, in Load
    return _sentencepiece.SentencePieceProcessor_Load(self, filename)
IOError: Not found: ""gs://ykproject/pre-trained/xlnet_cased_L-24_H-1024_A-16/spiece.model"": No such file or directory Error #2
</code></pre>

<p>The original code is:</p>

<pre><code>sp = spm.SentencePieceProcessor()
sp.Load(FLAGS.spiece_model_file)
</code></pre>

<p>I tried to find out the reason. So I decided to load the GCS file in my Python file:</p>

<pre><code>f = open(""gs://ykproject/test.txt"". ""r"")
</code></pre>

<p>The error is still presented:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
IOError: [Errno 2] No such file or directory: 'gs://ykproject/test.txt'
</code></pre>
",Dataset Preprocessing & Handling,load file use gc path without ioerror met problem run xlnet code google cloud tpu choose model path turn like original code tried find reason decided load gc file python file error still presented
How to convert text article with keywords to pandas data frame,"<p>I have similar text files to below, about 5,000 times and I want to extract the text article to one df column and the keywords in a list to another df column. I need this to have more training data.</p>

<p>In below sample, the article I want to extract is everything from 'Addis Abeba' to 'private bank' and the keywords are all keywords after 'SUBJECT' without percentages in brackets.</p>

<p>Sample of the dataset:</p>

<pre><code>

                                 Addis Fortune

                                 February 2011

Declaration? AU Action Needed in Favour of Democracy [opinion]

LENGTH: 692  words


Addis Abeba has been hosting delegates and heads of state for the AU Summit. It
is encouraging to see leaders of Africa discussing issues of continental
importance that accelerate the process of integration and thereby put Africa in
a better bargaining position in its relations with the outside world.

Indeed, ""United We Stand, Divided We Fall.""

It is time that the AU took a bold step to ensure that the leaders of the
continent win the hearts and minds of their citizens. It should ensure the
existence of democratic governments, which, at a minimum, guarantee popular
participation based on an acceptance of political equality among all citizens,
respect for civil liberties, and meaningful checks and balances on the power of
the executive.

This is also indispensable to the realisation of the age-old dream of the
formation of the United States of Africa. Donor countries and organisations also
have moral obligations to extend much needed support in this aspect.

Dawit Haile is a loan officer at a private bank.

SUBJECT: HEADS OF STATE &amp; GOVERNMENT (90%); ELECTIONS (90%); INTERNATIONAL
ASSISTANCE (89%); INTERNATIONAL RELATIONS (73%); GROSS DOMESTIC PRODUCT (70%);
ECONOMIC NEWS (70%); EMBEZZLEMENT (68%); ELECTION FRAUD (68%) Ethiopia;
International Organizations and Africa

GEOGRAPHIC: AFRICA (96%); EGYPT (93%); UNITED STATES (93%); CHINA (92%);
ETHIOPIA (79%); TUNISIA (79%); ISRAEL (79%) Africa

LOAD-DATE: February 8, 2011

LANGUAGE: ENGLISH

PUBLICATION-TYPE: Newspaper


                     Copyright 2011 AllAfrica Global Media.
                              All Rights Reserved


                              2 of 1352 DOCUMENTS


                                 Addis Fortune

                                 February 2011

Gebrekidan Beyene's Prosecutors Repeat Request for 25 Years

BYLINE: Eden Sahle

LENGTH: 815  words


During the appeals hearing last week of Gebrekidan Beyene, a.k.a. Morocco,
general manager and a shareholder of a private limited company by the same name,
prosecutors of the Ethiopian Revenues and Customs Authority (ERCA) requested
almost the same sentence they originally had, in August 2010: a maximum jail
term and confiscation of properties.

However, the lower court's decision to mitigate the sentence was correct and the
Appeals Bench should release Gebrekidan, either as a free man or on parole, the
defence argued. His good behaviour in prison and the investment he had made in
his country should be counted as mitigating circumstances, the lawyer claimed,
also counting the defendant's poor health in mitigation. The case was adjourned
for a verdict until May 2, 2011.

An alleged similar offence involving money laundering and loan sharking against
Ayalew Tesema, board chairman and major shareholder of Ayat Real Estate, is
underway at the Federal High Court.

SUBJECT: LITIGATION (91%); JUSTICE DEPARTMENTS (90%); BANKING &amp; FINANCE (90%);
EXCISE &amp; CUSTOMS (90%); LIMITED LIABILITY COMPANIES (90%); SENTENCING (90%);
APPEALS (89%); LAW COURTS &amp; TRIBUNALS (89%); JAIL SENTENCING (89%); LAWYERS
(89%); VERDICTS (89%); SUPREME COURTS (89%); FINES &amp; PENALTIES (89%);
SETTLEMENTS &amp; DECISIONS (78%); CRIMINAL CONVICTIONS (78%); DECISIONS &amp; RULINGS
(78%); PRISONS (77%); SUITS &amp; CLAIMS (77%); VALUE ADDED TAX (77%); JUDGES (73%);
INCOME TAX (72%); MONEY LAUNDERING (69%); COUNTERFEITING (68%); INTEREST RATES
(55%); ECONOMIC NEWS (55%) Ethiopia; Legal and Judicial Affairs

GEOGRAPHIC: MOROCCO (90%)

LOAD-DATE: March 1, 2011

LANGUAGE: ENGLISH

PUBLICATION-TYPE: Newspaper
</code></pre>

<p>My expected result would be:</p>

<pre><code>df
    content             keywords
1   'string article 1'  [HEADS OF STATE &amp; GOVERNMENT, ELECTIONS, ...]
2   'string article 2'  [LITIGATION, JUSTICE DEPARTMENTS, ...]
</code></pre>
",Dataset Preprocessing & Handling,convert text article keywords panda data frame similar text file time want extract text article one df column keywords list another df column need training data sample article want extract everything addis abeba private bank keywords keywords subject without percentage bracket sample dataset expected result would
How to ignore Null values in a CSV columns with pandas while processing the text?,"<p>I have a CSV file and each word in a sentence is represented in cell, with a null cell between each sentence. </p>

<p><a href=""https://i.sstatic.net/XtrCn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XtrCn.png"" alt=""CSV snippet file""></a></p>

<p>My problem is in <strong>run_id</strong> column, after I load the csv file using pandas I separate each sentence using function ""get sent from df"" but I've a line of assertion that double check that the run_id is unique and =1 but it fails because it take ""Null"" as a ""Null sentence""</p>

<p>Below is a snippet of my code, I hope you can help</p>

<p><strong>Note : I working on T=""test_RE""</strong></p>

<pre><code>def load_dataset(fn,T):

            if T==""test_RE"":
          df = pandas.read_csv(fn,
                         sep= "";"",
                         header=0,
                         keep_default_na=False)
          df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)
          df.word_id = pd.to_numeric(df.word_id, errors='coerce').astype('Int64')
          df.run_id = pd.to_numeric(df.run_id, errors='coerce').astype('Int64')
          df.sent_id = pd.to_numeric(df.sent_id, errors='coerce').astype('Int64')
          df.head_pred_id = pd.to_numeric(df.head_pred_id, errors='coerce').astype('Int64')
      else:
            df = pandas.read_csv(fn,
                         sep= ""\t"",
                         header=0,
                         keep_default_na=False)
      print (df.dtypes)

      if T==""train"":
        encoder.fit(df.label.values)
        print('this is the IF cond')
        print('df.label.values. shape',df.label.values.shape)

      sents = get_sents_from_df(df)

      print('shape of sents 0',sents[0].shape)
      print('sents[0]',sents[0])
      print('shape of sents 1',sents[1].shape)
      print('sents[1]',sents[1])

      #make sure that all sents agree on run_id

                assert(all([len(set(sent.run_id.values)) == 1
                    for sent in sents])) **ERROR HERE**
</code></pre>

<p>the function</p>

<pre><code>def get_sents_from_df( df):

      #Split a data frame by rows accroding to the sentences
      return [df[df.run_id == run_id]
            for run_id
            in sorted(set(df.run_id.values))]
</code></pre>

<p>shape of sent 0 is (10,8) which is correct and the sent[0] is correct</p>

<p>but shape of sent<a href=""https://i.sstatic.net/KvDBt.png"" rel=""nofollow noreferrer"">1</a> is (0,8) and of course sent<a href=""https://i.sstatic.net/KvDBt.png"" rel=""nofollow noreferrer"">1</a> isn't printed because it null, I should have sent<a href=""https://i.sstatic.net/KvDBt.png"" rel=""nofollow noreferrer"">1</a> shape = (6,8) any help ?</p>

<p>Image of Output of print statements:</p>

<p><a href=""https://i.sstatic.net/tXCyC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tXCyC.png"" alt=""Output of print stsatemts""></a></p>
",Dataset Preprocessing & Handling,ignore null value csv column panda processing text csv file word sentence represented cell null cell sentence problem run id column load csv file using panda separate sentence using function get sent df line assertion double check run id unique fails take null null sentence snippet code hope help note working test function shape sent correct sent correct shape sent course sent printed null sent shape help image output print statement
Cumulative Unique words in huge dataframe,"<p>How do I get  cumulative unique words from a dataframe column which has more than 500 words per. Dataframe has ~300,000 rows</p>

<p>I read the csv file in a dataframe with column A having text data.
I have tried creating couple of columns (B &amp; C) by looping through column A and taking unique words from column A as set and appending Column B with unique words and Column C with count</p>

<p>Subsequently, I  take unique words by taking Column A and column B(union) from previous row(set) </p>

<p>This works for small number of rows. But once number of rows exceeds 10,000 performance degrades and kernal eventually dies</p>

<p>Is there any better way of doing this for huge dataframe ?</p>

<p>Tried creating seperate dataframe with just unique words and count, but still have issue</p>

<p>Sample code:</p>

<pre><code>for index, row in DF.iterrows():
      if index = 0:
          result = set(row['Column A'].lower().split()
          DF.at[index, 'Column B'] = result
      else:
          result = set(row['Column A'].lower().split()
          DF.at[index, 'Cloumn B'] = result.union(DF.loc[index -1, 
                                                'Column B'])
DF['Column C'] = DF['Column B'].apply(len)
</code></pre>
",Dataset Preprocessing & Handling,cumulative unique word huge dataframe get cumulative unique word dataframe column ha word per dataframe ha row read csv file dataframe column text data tried creating couple column b c looping column taking unique word column set appending column b unique word column c count subsequently take unique word taking column column b union previous row set work small number row number row exceeds performance degrades kernal eventually dy better way huge dataframe tried creating seperate dataframe unique word count still issue sample code
Group by sparse matrix in scipy and return a matrix,"<p>There are a few questions on SO dealing with using <code>groupby</code> with sparse matrices. However the output seem to be lists, <a href=""https://stackoverflow.com/questions/35410839/group-by-on-scipy-sparse-matrix"">dictionaries</a>, <a href=""http://stackoverflow.duapp.com/questions/30295570/groupby-sum-sparse-matrix-in-pandas-or-scipy-looking-for-performance?rq=1"" rel=""nofollow noreferrer"">dataframes</a> and other objects. </p>

<p>I'm working on an NLP problem and would like to keep all the data in sparse scipy matrices during processing to prevent memory errors.</p>

<p>Here's the context:</p>

<p>I have vectorized some documents (<a href=""http://andrewbrown.ca/data/groupbysparsematrix%20.csv"" rel=""nofollow noreferrer"">sample data here</a>): </p>

<pre><code>import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

df = pd.read_csv('groupbysparsematrix.csv')
docs = df['Text'].tolist()

vectorizer = CountVectorizer()
train_X = vectorizer.fit_transform(docs)

print(""Dimensions of training set: {0}"".format(train_X.shape))
print type(train_X)

Dimensions of training set: (8, 180)
&lt;class 'scipy.sparse.csr.csr_matrix'&gt;
</code></pre>

<p>From the original dataframe I use the date, in a day of the year format, to create the groups I would like to sum over:</p>

<pre><code>from scipy import sparse, hstack    

df['Date'] = pd.to_datetime(df['Date'])
groups = df['Date'].apply(lambda x: x.strftime('%j'))
groups_X = sparse.csr_matrix(groups.astype(float)).T
train_X_all = sparse.hstack((train_X, groups_X))

print(""Dimensions of concatenated set: {0}"".format(train_X_all.shape))

Dimensions of concatenated set: (8, 181)
</code></pre>

<p>Now I'd like to apply <code>groupby</code> (or a similar function) to find the sum of each token per day. I would like the output to be another sparse scipy matrix.   </p>

<p>The output matrix would be 3 x 181 and look something like this:</p>

<pre><code> 1, 1, 1, ..., 2, 1, 3
 2, 1, 3, ..., 1, 1, 4
 0, 0, 0, ..., 1, 2, 5
</code></pre>

<p>Where the columns 1 to 180 represent the tokens and column 181 represents the day of the year.</p>
",Dataset Preprocessing & Handling,group sparse matrix scipy return matrix question dealing using sparse matrix however output seem list dataframes object working nlp problem would like keep data sparse scipy matrix processing prevent memory error context vectorized document sample data original dataframe use date day year format create group would like sum like apply similar function find sum token per day would like output another sparse scipy matrix output matrix would x look something like column represent token column represents day year
How to calculate the similarity measure of text document?,"<p>I have CSV file that looks like:</p>

<pre><code>idx         messages
112  I have a car and it is blue
114  I have a bike and it is red
115  I don't have any car
117  I don't have any bike
</code></pre>

<p>I would like to have the code that reads the file and performs the similarity difference.</p>

<p>I have looked into many posts regarding this such as <a href=""https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents"">1</a> <a href=""https://stackoverflow.com/questions/52090786/how-to-measure-the-similarity-of-two-documents-given-the-similarity-of-each-pa"">2</a> <a href=""https://stackoverflow.com/questions/6069922/measuring-similarity-between-document-sets"">3</a> <a href=""https://stackoverflow.com/questions/9453731/how-to-calculate-distance-similarity-measure-of-given-2-strings"">4</a> but either it is hard for me to understand or not exactly what I want. </p>

<p>based on some posts and webpages that saying ""a simple and effective one is Cosine similarity"" or ""Universal sentence encoder"" or ""Levenshtein distance"". </p>

<p>It would be great if you can provide your help with code that I can run in my side as well. Thanks</p>
",Dataset Preprocessing & Handling,calculate similarity measure text document csv file look like would like code read file performs similarity difference looked many post regarding based post webpage saying simple effective one cosine similarity universal sentence encoder levenshtein distance would great provide help code run side well thanks
Count the occurrences of words in a string row wise based on existing words in other columns,"<p>I have a data frame that has rows of strings. I want to count the occurrence of words in the rows based on what words appear in the column. How can I achieve this with the code below? <strong><em>Can the below code be modified somehow to achieve this or can anyone suggest another piece of code that doesn't require loops</em></strong>? Thanks so much in advance!</p>

<pre><code>df &lt;- data.frame(
  words = c(""I want want to compare each "",
            ""column to the values in"",
            ""If any word from the list any"",
            ""replace the word in the respective the word want""),
  want= c(""want"", ""want"", ""want"", ""want""),
  word= c(""word"", ""word"", ""word"", ""word""),
  any= c(""any"", ""any"", ""any"", ""any""))

#add 1 for match and 0 for no match
for (i in 2:ncol(df))
{
  for (j in 1:nrow(df))
  {                 
    df[j,i] &lt;- ifelse (grepl (df[j,i] , df$words[j]) %in% ""TRUE"", 1, 0)
  }
  print(i)
}

*'data.frame':  4 obs. of  4 variables:
 $ words: chr  ""I want want to compare each "" ""column to the values in "" ""If any word from the words any"" ""replace the word in the respective the word""
 $ want : chr  ""want"" ""want"" ""want"" ""want""
 $ word : chr  ""word"" ""word"" ""word"" ""word""
 $ any  : chr  ""any"" ""any"" ""any"" ""any""*
</code></pre>

<p><strong>The output should look like below:</strong></p>

<pre><code>    words                                                 want word any
1   I want want to compare each                            2    0   0
2   column to the values in                                0    0   0
3   If any word from the list any                          0    1   2
4   replace the word in the respective the word want       1    2   0
</code></pre>

<p><strong>Current output with existing code looks like this:</strong> </p>

<pre><code>    words                                                 want word any
1   I want want to compare each                            1    0   0
2   column to the values in                                0    0   0
3   If any word from the list any                          0    1   1
4   replace the word in the respective the word want       1    1   0
</code></pre>
",Dataset Preprocessing & Handling,count occurrence word string row wise based existing word column data frame ha row string want count occurrence word row based word appear column achieve code code modified somehow achieve anyone suggest another piece code require loop thanks much advance output look like current output existing code look like
How do I train model in Google NLP Sentiment Analysis correctly,"<p>I need to compare to sentiment models trained with different types of content. Google supplies you with a training dataset filled with tweets in a .csv file, As expected training with this went well however, when I decided to train a model using the Stanford NLP's dataset of IMDB reviews, I manage to upload the dataset without issue but when I train it the NLP, for some reason only predicts that the sentiment value is 2, regardless of what I write.  </p>

<p>I figured that the dataset was diluted since, while there were 800-2000 examples of sentiment 0,1,3 and 4, there were 6000 examples of sentiment 2. Although after removing 4000 of these examples, the problem persisted. </p>

<p>I'm expecting my confusion matrix to not simply only have 100% prediction on each sentiment value. It should be distributed over the matrix w</p>
",Dataset Preprocessing & Handling,train model google nlp sentiment analysis correctly need compare sentiment model trained different type content google supply training dataset filled tweet csv file expected training went well however decided train model using stanford nlp dataset imdb review manage upload dataset without issue train nlp reason predicts sentiment value regardless write figured dataset wa diluted since example sentiment example sentiment although removing example problem persisted expecting confusion matrix simply prediction sentiment value distributed matrix w
How do I write a Tensorflow custom op containing a persistent C++ object?,"<p>I'm developing a Tensorflow sequence model that uses a beam search through an OpenFST decoding graph (loaded from a binary file) over the logits output from a Tensorflow sequence model.</p>

<p>I've written a custom op that allows me to perform decoding over the logits, but each time, I'm having the op call fst::Read(BINARY_FILE) before performing the decoding. This might be fine as long as it stays small but I'd like to avoid the I/O overhead.</p>

<p>I've read through the Tensorflow custom op and tried to find similar examples but I'm still lost. Basically, what I want to do in the graph is:</p>

<pre class=""lang-py prettyprint-override""><code>FstDecodingOp.Initialize('BINARY_FILE.bin') #loads the BINARY_FILE.bin into memory
...
for o in output:
    FstDecodingOp.decode(o) # uses BINARY_FILE.bin to decode

</code></pre>

<p>This would of course be straightforward in Python outside of the tensorflow graph, but I need to eventually move this into a vanilla TF-Serving environment, so it needs to get frozen into an export graph. Has anyone encountered a similar problem before? </p>

<p><strong>Solution:</strong></p>

<p>Didn't realize that you could set private attributes using the ""OpKernel(context)"". Just initialized it using that function.</p>

<p><strong>Edit: more detail on how I did it. Have yet to try serving.</strong></p>

<pre class=""lang-cpp prettyprint-override""><code>REGISTER_OP(""FstDecoder"")
    .Input(""log_likelihoods: float"")
    .Attr(""fst_decoder_path: string"")
    ....

...

template &lt;typename Device, typename T&gt;
class FstDecoderOp : public OpKernel {

private:
   fst::Fst&lt;fst::StdArc&gt;* fst_;
   float beam_;

public:
  explicit FstDecoderOp(OpKernelConstruction* context) : OpKernel(context) {
    OP_REQUIRES_OK(context, context-&gt;GetAttr(""beam"", &amp;beam_));

    std::string fst_path;
    OP_REQUIRES_OK(context, context-&gt;GetAttr(""fst_decoder_path"", &amp;fst_path));

    fst_ = fst::Fst&lt;fst::StdArc&gt;::Read(fst_path);
  }

  void Compute(OpKernelContext* context) override {
    // do some compute 
    const Tensor* log_likelihoods;

    OP_REQUIRES_OK(context, context-&gt;input(""log_likelihoods"", 
     &amp;log_likelihoods));

    // simplified 
    compute_op(_fst, log_likelihoods);

  }
};
</code></pre>

<p>In python:</p>

<pre class=""lang-py prettyprint-override""><code>
sess = tf.Session()
mat = tf.placeholder(tf.float32, shape=test_npy.shape)
res_ = decoder_op.fst_decoder(beam=30, fst_decoder_path=""decoder_path.fst"", log_likelihoods=mat)
res = sess.run(res_, {mat : test_npy} )

</code></pre>
",Dataset Preprocessing & Handling,write tensorflow custom op containing persistent c object developing tensorflow sequence model us beam search openfst decoding graph loaded binary file logits output tensorflow sequence model written custom op allows perform decoding logits time op call fst read binary file performing decoding might fine long stay small like avoid overhead read tensorflow custom op tried find similar example still lost basically want graph would course straightforward python outside tensorflow graph need eventually move vanilla tf serving environment need get frozen export graph ha anyone encountered similar problem solution realize could set private attribute using opkernel context initialized using function edit detail yet try serving python
How to speed up this word-tuple finding algorithm?,"<p>I am trying to create a simple model to predict the next word in a sentence. I have a big .txt file that contains sentences seperated by '\n'. I also have a vocabulary file which lists every unique word in my .txt file and a unique ID. I used the vocabulary file to convert the words in my corpus to their corresponding IDs. Now I want to make a simple model which reads the IDs from txt file and find the word pairs and how many times this said word pairs were seen in the corpus. I have managed to write to code below:</p>

<pre><code>tuples = [[]] #array for word tuples to be stored in
data = []   #array for tuple frequencies to be stored in

data.append(0) #tuples array starts with an empty element at the beginning for some reason.
            # Adding zero to the beginning of the frequency array levels the indexes of the two arrays

with open(""markovData.txt"") as f:
    contentData = f.readlines()
    contentData = [x.strip() for x in contentData]
    lineIndex = 0
    for line in contentData:
        tmpArray = line.split() #split line to array of words
        tupleIndex = 0
        tmpArrayIndex = 0
        for tmpArrayIndex in range(len(tmpArray) - 1): #do this for every word except the last one since the last word has no word after it.
            if [tmpArray[tmpArrayIndex], tmpArray[tmpArrayIndex + 1]] in tuples: #if the word pair is was seen before
                data[tuples.index([tmpArray[tmpArrayIndex], tmpArray[tmpArrayIndex + 1]])] += 1 #increment the frequency of said pair
            else:
                tuples.append([tmpArray[tmpArrayIndex], tmpArray[tmpArrayIndex + 1]]) #if the word pair is never seen before
                data.append(1)                                                        #add the pair to list and set frequency to 1.

        #print every 1000th line to check the progress
        lineIndex += 1
        if ((lineIndex % 1000) == 0):
            print(lineIndex)

with open(""markovWindowSize1.txt"", 'a', encoding=""utf8"") as markovWindowSize1File:
    #write tuples to txt file
    for tuple in tuples:
        if (len(tuple) &gt; 0): # if tuple is not epmty
            markovWindowSize1File.write(str(element[0]) + "","" + str(element[1]) + "" "")

    markovWindowSize1File.write(""\n"")
    markovWindowSize1File.write(""\n"")
    #blank spaces between two data

    #write frequencies of the tuples to txt file
    for element in data:
        markovWindowSize1File.write(str(element) + "" "")

    markovWindowSize1File.write(""\n"")
    markovWindowSize1File.write(""\n"")
</code></pre>

<p>This code seems to be working well for the first couple thousands of lines. Then things start to get slower because the tuple list keeps getting bigger and I have to search the whole tuple list to check if the next word pair was seen before or not. I managed to get the data of 50k lines in 30 minutes but I have much bigger corpuses with millions of lines. Is there a way to store and search for the word pairs in a more efficient way? Matrices would probably work a lot faster but my unique word count is about 300.000 words. Which means I have to create a 300k*300k matrix with integers as data type. Even after taking advantage of symmetric matrices, it would require <strong>a lot</strong> more memory than what I have.</p>

<p>I tried using memmap from numpy to store the matrix in disk rather than memory but it required about 500 GB free disk space.</p>

<p>Then I studied the sparse matrices and found out that I can just store the non-zero values and their corresponding row and column numbers. Which is what I did in my code.</p>

<p>Right now, this model works but it is very bad at guessing the next word correctly ( about 8% success rate). I need to train with bigger corpuses to get better results. What can I do to make this word pair finding code more efficient?</p>

<p>Thanks.</p>

<hr>

<p>Edit: Thanks to everyone answered, I am now able to process my corpus of ~500k lines in about 15 seconds. I am adding the final version of the code below for people with similiar problems:</p>

<pre><code>import numpy as np
import time

start = time.time()
myDict = {} # empty dict

with open(""markovData.txt"") as f:
    contentData = f.readlines()
    contentData = [x.strip() for x in contentData]
    lineIndex = 0
    for line in contentData:
        tmpArray = line.split() #split line to array of words
        tmpArrayIndex = 0
        for tmpArrayIndex in range(len(tmpArray) - 1): #do this for every word except the last one since the last word has no word after it.
            if (tmpArray[tmpArrayIndex], tmpArray[tmpArrayIndex + 1]) in myDict: #if the word pair is was seen before
                myDict[tmpArray[tmpArrayIndex], tmpArray[tmpArrayIndex + 1]] += 1  #increment the frequency of said pair
        else:
            myDict[tmpArray[tmpArrayIndex], tmpArray[tmpArrayIndex + 1]] = 1 #if the word pair is never seen before
                                                                              #add the pair to list and set frequency to 1.

    #print every 1000th line to check the progress
    lineIndex += 1
    if ((lineIndex % 1000) == 0):
        print(lineIndex)


end = time.time()
print(end - start)

keyText= """"
valueText = """"

for key1,key2 in myDict:
    keyText += (str(key1) + "","" + str(key2) + "" "")
    valueText += (str(myDict[key1,key2]) + "" "")


with open(""markovPairs.txt"", 'a', encoding=""utf8"") as markovPairsFile:
    markovPairsFile.write(keyText)

with open(""markovFrequency.txt"", 'a', encoding=""utf8"") as markovFrequencyFile:
    markovFrequencyFile.write(valueText)
</code></pre>
",Dataset Preprocessing & Handling,speed word tuple finding algorithm trying create simple model predict next word sentence big txt file contains sentence seperated n also vocabulary file list every unique word txt file unique id used vocabulary file convert word corpus corresponding id want make simple model read id txt file find word pair many time said word pair seen corpus managed write code code seems working well first couple thousand line thing start get slower tuple list keep getting bigger search whole tuple list check next word pair wa seen managed get data k line minute much bigger corpus million line way store search word pair efficient way matrix would probably work lot faster unique word count word mean create k k matrix integer data type even taking advantage symmetric matrix would require lot memory tried using memmap numpy store matrix disk rather memory required gb free disk space studied sparse matrix found store non zero value corresponding row column number code right model work bad guessing next word correctly success rate need train bigger corpus get better result make word pair finding code efficient thanks edit thanks everyone answered able process corpus k line second adding final version code people similiar problem
How to analyze accented characters with Google Cloud Natural Language,"<p>I am trying to use the python client on Python3 (collab) for analyzing text with accented characters. I am setting up the document object with type PLAIN_TEXT.</p>

<pre><code># Run a sentiment analysis request on text
def nlp_analyze_text(text, lang=nlp_def_language):
  client = language.LanguageServiceClient()

  document = types.Document(
      content=text,
      language=lang,      
      type=enums.Document.Type.PLAIN_TEXT)
  entities = client.analyze_entities(document=document, encoding_type='UTF32')
  syntax = client.analyze_syntax(document=document)

  return (entities, syntax)
</code></pre>

<p>As so the input that is feed into the client contains multibyte characters.</p>

<pre><code>text = u""Mi vieja mula ya no es lo que era? Qué era entonces? Era de Bs.As. Saludos!""
nlp_analyze_text(text)
</code></pre>

<p>This I believe is not properly understood by Google Cloud NL.  </p>

<pre><code>sentences {
   text {
     content: ""Qu\303\251 era entonces?""
     begin_offset: -1
   }
 }
</code></pre>

<p>So, how should I setup the code for analyzing text with accented characters.</p>

<p>Thanks</p>
",Dataset Preprocessing & Handling,analyze accented character google cloud natural language trying use python client python collab analyzing text accented character setting document object type plain text input feed client contains multibyte character believe properly understood google cloud nl setup code analyzing text accented character thanks
How to tokenize all the files from all the subfolders,"<p>I have a dataset which consists of 50 subfolders and each of these subfolders has 20-30 files without extension. What I wanted to do is tokenizing the texts in the files for each subfolders, and write it to file with subfolder's name. For example;
Let's say subfolder1 has 25 files and I want to tokenize those 25 files together, and write it to a file named ""subfolder1"". And I want to do it for all the subfolders in the main folder.</p>

<p>I have tried some pieces of this <a href=""https://github.com/ayushoriginal/Multi-Document-Summarization/blob/master/MMR/summary_util/summary_utils.py"" rel=""nofollow noreferrer"">code</a> but it gives PermissionError since it can not read a folder. </p>

<pre><code>    mainfolder=""path\\to\\mainfolder""

    def ls(path):
        return [os.path.join(path, item) for item in os.listdir(path)]


    def load_file_sents(path):
        return [sent.lower()
                for sent in tokenize.sent_tokenize(open(path).read())]


    def load_collection_sents(path):
        sents = []
        for f in ls(path):
            sents.extend(load_file_sents(f))
        return sents


    def get_sentences(path):
        """""" loads sentences from the given path (collection or file) """"""
        sents = []
        try:
            # treat as a single file
            open(path).read()
            sents = load_file_sents(path)
        except IOError:
            # it's a directory!
            sents = load_collection_sents(path)
        return sents


    def get_toks(path):
        return [tokenize.word_tokenize(sent) for sent in get_sentences(path)]

    get_toks(mainfolder)
</code></pre>

<p>This is the error it gives:</p>

<pre><code>PermissionError                           Traceback (most recent call last)
&lt;ipython-input-52-a6f316499b2c&gt; in get_sentences(path)
     37         # treat as a single file
---&gt; 38         open(path).read()
     39         sents = load_file_sents(path)

PermissionError: [Errno 13] Permission denied:
</code></pre>

<p>I have tried merging the first two functions into one, and make sure it will read files, but this time it just returned tokens of the first file of the first subfolder. If you know how to solve this issue or a better way to do it, your help would be greatly appreciated! Thanks.</p>
",Dataset Preprocessing & Handling,tokenize file subfolders dataset consists subfolders subfolders ha file without extension wanted tokenizing text file subfolders write file subfolder name example let say subfolder ha file want tokenize file together write file named subfolder want subfolders main folder tried piece code give permissionerror since read folder error give tried merging first two function one make sure read file time returned token first file first subfolder know solve issue better way help would greatly appreciated thanks
How to compute the perplexity in text classification?,"<p>I'm doing dialect text classification with scikit learn, naive bayes and countvectorizer. So far I'm only doing 3 dialects text classification. I'm going to add a new dialect(or actually, the formal language for those dialects). The problem is, the new text that I'm going to add, shares a lot of words with the other 3 dialects. So I read the following in a research document:</p>

<blockquote>
  <p>We train an n-gram model for each dialect from the collected data. To
  train the MSA model, we select sentences from Arabic UN corpus and
  news collections. All the dialect and MSA models share the same
  vocabulary, thus perplexity can be compared properly. At
  classification time, given an input sentence, the classifier computes
  the perplexity for each dialect type and choose the one with minimum
  perplexity as the label.</p>
</blockquote>

<p>They mean by MSA(Modern Standard Arabic) which is the formal language for those dialects. How are they  calculating the perplexity? Are they just using naive bayes or there's more to it?</p>
",Dataset Preprocessing & Handling,compute perplexity text classification dialect text classification scikit learn naive bayes countvectorizer far dialect text classification going add new dialect actually formal language dialect problem new text going add share lot word dialect read following research document train n gram model dialect collected data train msa model select sentence arabic un corpus news collection dialect msa model share vocabulary thus perplexity compared properly classification time given input sentence classifier computes perplexity dialect type choose one minimum perplexity label mean msa modern standard arabic formal language dialect calculating perplexity using naive bayes
Read text files with paragraphs as one string using VCorpus from tm package in r,"<p>I have a list of text files in my directory, all of which are documents with multiple paragraphs. I want to read those documents and do sentiment analysis.</p>

<p>For example, I have one text document <code>data/hello.txt</code> with text like below:</p>

<pre><code>""Hello world.  
 This is an apple.

 That is an orange""
</code></pre>

<p>I read the document in like below (there can also be multiple documents):</p>

<pre><code>docs &lt;- VCorpus(DirSource('./data/hello.txt'))
</code></pre>

<p>When I look at the document content  <code>docs[[1]]$content</code> It seems like it is character vector.</p>

<pre><code>[1] ""hello  world""        ""this is apple.""      """"                   
[4] ""That is an orange. "" """"  
</code></pre>

<p>My question is how I can read in those documents so that in each document, paragraphs are concatenated into one single character string so that I can use it for sentiment analysis. (VCorpus from tm package)</p>

<p>Thanks a lot.</p>
",Dataset Preprocessing & Handling,read text file paragraph one string using vcorpus tm package r list text file directory document multiple paragraph want read document sentiment analysis example one text document text like read document like also multiple document look document content seems like character vector question read document document paragraph concatenated one single character string use sentiment analysis vcorpus tm package thanks lot
Do I need to adjust for length while comparing cosine similarity across different pairs of documents?,"<p>Assume that I have two documents, A and B, and each document has two versions, 1 and 2. I calculate the cosine similarities for (A1, A2) and (B1, B2). Let Sa = cosine(A1, A2), and Sb = cosine(B1, B2).</p>

<p>If Sa &lt; Sb, can I say that there has been a greater change or update for Document A than Document B? </p>

<p>A paper states that: ""The longer a pair of documents, the more probable a word is included in both documents, leading to a lower likelihood that the documents will differ (see appendix B for an analytical proof)."" The paper can be found <a href=""https://onlinelibrary.wiley.com/doi/full/10.1111/j.1475-679X.2010.00396.x"" rel=""nofollow noreferrer"">HERE</a>. </p>

<p>Is this true? This statement is likely true for the Jaccard similarity since the size of the vocabulary is limited. However, the cosine similarity is an angle, and intuitively the length of the documents shouldn't matter. If this is true,  what is the best way to adjust the similarity scores for length so that I can make a comparison across different pairs of documents. Thank you!   </p>

<p>I run some stats based on 22861 pairs. Indeed, the doc length and the similarity are highly positively correlated. 
Averages: </p>

<pre><code>WC2          4829.637374
WCl          4389.449193
WCA          4609.543283
Cosine         0.750225
CosineR        0.786806
Jaccard        0.606962
JaccardR       0.653031
</code></pre>

<p>Where:</p>

<p>WC2 is the clean word count of version2, excluding stopwords, numbers, and punctuations. 
WCA is the average(WC1, and WC2). 
Cosine is the clean cosine similarity based on the clean words.<br>
CosineR is the cosine similarity based on all tokens(including stopwords, numbers, and punctuations). </p>

<p>Here is the Pearson correlation matrix: </p>

<pre><code>                WC2       WCl       WCA    Cosine   CosineR   Jaccard  JaccardR
WC2       1.000000  0.886743  0.972220  0.121300  0.122131  0.107936  0.123040
WCl       0.886743  1.000000  0.970310  0.238503  0.257914  0.220078  0.254090
WCA       0.972220  0.970310  1.000000  0.184233  0.194497  0.167911  0.193036
Cosine    0.121300  0.238503  0.184233  1.000000  0.978050  0.982066  0.971082
CosineR   0.122131  0.257914  0.194497  0.978050  1.000000  0.948325  0.981853
Jaccard   0.107936  0.220078  0.167911  0.982066  0.948325  1.000000  0.975908
JaccardR  0.123040  0.254090  0.193036  0.971082  0.981853  0.975908  1.000000
</code></pre>
",Dataset Preprocessing & Handling,need adjust length comparing cosine similarity across different pair document assume two document b document ha two version calculate cosine similarity b b let sa cosine sb cosine b b sa sb say ha greater change update document document b paper state longer pair document probable word included document leading lower likelihood document differ see appendix b analytical proof paper found true statement likely true jaccard similarity since size vocabulary limited however cosine similarity angle intuitively length document matter true best way adjust similarity score length make comparison across different pair document thank run stats based pair indeed doc length similarity highly positively correlated average wc clean word count version excluding stopwords number punctuation wca average wc wc cosine clean cosine similarity based clean word cosiner cosine similarity based token including stopwords number punctuation pearson correlation matrix
Encoding with readtext,"<p>I want to do some text analysis based on data stored as a .csv file, but I run into problems regarding the encoding with the <code>readtext</code> package.</p>

<p>To illustrate my problem, I created the following file in Excel, saving it as .csv (UTF-8):</p>

<pre><code>|---------------------|------------------|
|      c_text         |       c_id       |
|---------------------|------------------|
|      München        |        aa        |
|---------------------|------------------|
|       Laïrie        |        bb        |
|---------------------|------------------|
|        Mános        |        cc        |
|---------------------|------------------|
</code></pre>

<p>Then, I load the data in R as follows:</p>

<pre><code>text_raw &lt;- readtext::readtext(""path/test_encoding.csv""),
                   encoding = ""UTF-8"",
                   text_field = ""c_text"")
text_raw
</code></pre>

<p>The output is:</p>

<pre><code>readtext object consisting of 3 documents and 1 docvar.
# Description: data.frame [3 x 3]
  doc_id              text              c_id 
  &lt;chr&gt;               &lt;chr&gt;             &lt;chr&gt;
1 test_encoding.csv.1 ""\""MÃ¼nchen\""..."" aa   
2 test_encoding.csv.2 ""\""LaÃ¯rie\""...""  bb   
3 test_encoding.csv.3 ""\""MÃ¡nos\""...""   cc 
</code></pre>

<p>If I then write the object to a .csv file, the output is once again different. The command <code>write.csv(text_raw, file = ""path"", fileEncoding = ""UTF-8"")</code> yields the following:</p>

<pre><code>MÃƒÂ¼nchen
LaÃƒÂ¯rie
MÃƒÂ¡nos
</code></pre>

<p>Some additional information:</p>

<ul>
<li><p>I am using a Windows machine, and my <code>sys.getLocale()</code> is <code>English_United Kingdom.1252</code> (apparently, this cannot be changed to UTF-8)</p></li>
<li><p>Even if I specify other encodings in the <code>readtext()</code> function, (e.g., ""utf8"", ""Windows-1252"", ""ISO8859-1""), the output doesn't change. However, given that I explicitly save the test file as utf-8, I don't understand what's going on.</p></li>
</ul>

<p>Any help would be greatly appreciated. Thanks.</p>
",Dataset Preprocessing & Handling,encoding readtext want text analysis based data stored csv file run problem regarding encoding package illustrate problem created following file excel saving csv utf load data r follows output write object csv file output different command yield following additional information using window machine apparently changed utf even specify encoding function e g utf window iso output change however given explicitly save test file utf understand going help would greatly appreciated thanks
Sense2vec : os error Could not open binary file b,"<p>after installing sense2vec when i try to load the reddit_vector file using</p>

<p>import sense2vec
model=sense2vec.load(r""C:\Users\Leanda\Downloads\reddit_vectors-1.1.0 second"")</p>

<p>it gives me this error:</p>

<p>OSError: Could not open binary file b'C:\Users\Leanda\Downloads\reddit_vectors-1.1.0 second\vectors.bin'</p>
",Dataset Preprocessing & Handling,sense vec error could open binary file b installing sense vec try load reddit vector file using import sense vec model sense vec load r c user leanda downloads reddit vector second give error oserror could open binary file b c user leanda downloads reddit vector second vector bin
Looking to Regex Strip a predictable chunk of text from data frame,"<p>I have a dataframe of Inspection results &amp; Violations that looks like:</p>

<pre><code>Results                 Violations
Pass w/ Conditions  3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL E

Pass                    36. THERMOMETERS PROVIDED &amp; ACCURATE Comment...
</code></pre>

<p>What I need to do is have python loop through this pandas dataframe specifically in the violations column and identify all scenarios of
'Starts with a number and ends with Comments:'</p>

<p>I was able to use regex to strip the number with this line of code</p>

<pre><code>df_new['Violations'] = df_new['Violations'].map(lambda x: 
    x.lstrip('0123456789.- ').rstrip('[^a-zA-Z]Comments[^a-zA-Z]'))
</code></pre>

<p>As you can see I tried to implement the comments closing end via the rstrip regex command but that does not appear to do anything. Output then looks like this</p>

<pre><code>Results Violations
0   Pass w/ Conditions  MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL EMPL...
1   Pass    THERMOMETERS PROVIDED &amp; ACCURATE - Comments: 4...
</code></pre>

<p>What is the regex command to basically say: Look for a number and delete everything between the number and Comments:</p>

<p>Is there a simple way to do this?</p>
",Dataset Preprocessing & Handling,looking regex strip predictable chunk text data frame dataframe inspection result violation look like need python loop panda dataframe specifically violation column identify scenario start number end comment wa able use regex strip number line code see tried implement comment closing end via rstrip regex command doe appear anything output look like regex command basically say look number delete everything number comment simple way
reading data from a .csv file,"<p>So i have a dataset for a NLP problem which contains data in the following format :
code,body,result</p>

<p>2552272216,Does honey changes black hair into white ?,[Greying Hair]</p>

<p>2552210209,""Hello doctor,my mother was diagnosed with depression at the age of 36 due to over thinking about the family problems. Which caused her depression which caused several other mental problems and made her condition worse which resulted into a brain stroke and she passed away. Now my question iscan it happen with me or to my sister also at some point of."",[Depression]</p>

<p>using pd.read_csv i read these lines using ',' as the delimiter but i want the last column to be read as a list and not string .
Please help!</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import json
# Importing the dataset
dataset = pd.read_csv('case_study_lybrate.csv', delimiter=',',
                     quoting=1, skipinitialspace=True)
</code></pre>
",Dataset Preprocessing & Handling,reading data csv file dataset nlp problem contains data following format code body result doe honey change black hair white greying hair hello doctor mother wa diagnosed depression age due thinking family problem caused depression caused several mental problem made condition worse resulted brain stroke passed away question iscan happen sister also point depression using pd read csv read line using delimiter want last column read list string please help
How to load a folder (with text files) from your computer on Jupyter to be able to run analyses on them together?,"<p>I am trying to load a folder (containing about 1000 .txt files) on my Jupyter notebook (Python 3) from the desktop of my WINDOWS computer; so that I can proceed with my analyses relating to NLP. I am using SPaCY instead of NLTK as advised by one of the Udemy course instructors.</p>
<p>I am a novice in the field and had been trying to read textbooks and udemy online courses but those did not help much.</p>
<h1>Following one of the NLP courses from Udemy, I tried to load the folder &quot;text folder sample&quot; as follows (it did not work):</h1>
<p>gen = os.walk('../text folder sample')</p>
<p>next(gen)</p>
<p>I am seeking your help with lines of codes which will enable my python script to load and proceed with analyzing the files. **Each .txt file is an autobiography, so I am trying to treat each of them as an independent case so that at later stages I can infer which autobiographies are similar (e.g. cluster analyses).</p>
",Dataset Preprocessing & Handling,load folder text file computer jupyter able run analysis together trying load folder containing txt file jupyter notebook python desktop window computer proceed analysis relating nlp using spacy instead nltk advised one udemy course instructor novice field trying read textbook udemy online course help much following one nlp course udemy tried load folder text folder sample follows work gen walk text folder sample next gen seeking help line code enable python script load proceed analyzing file txt file autobiography trying treat independent case later stage infer autobiography similar e g cluster analysis
Removing particular string in python pandas column,"<p>I have a data frame with a column gender. It consists of predictions of the gender. Now the gender column has values such as mostly_male, mostly_female. I want to remove mostly. So I tried<code>df['gender'] = df['gender'].map(lambda x: x.lstrip('mostly_'))</code></p>

<p>But I got a column with values of 'male' corresponding to 'ale'</p>
",Dataset Preprocessing & Handling,removing particular string python panda column data frame column gender consists prediction gender gender column ha value mostly male mostly female want remove mostly tried got column value male corresponding ale
Nothing is being appended(written) to my txt file from my python code,"<p>I have written a code where I am trying to pick out some lines from text files and append them to another text file;</p>

<p>I have a folder : </p>

<blockquote>
  <p>E:\Adhiraj Chattopadhyay\NLG Dataset\FYP DB
  I have several sub-folders in it, each of which contains a <strong>text file</strong>.
  So I have entered this directory in my python intrpreter;</p>
</blockquote>

<pre><code>import os
path = ""E:\\Adhiraj Chattopadhyay\\NLG Dataset\\FYP DB""
os.chdir(path)
</code></pre>

<p>I now created a file with read &amp; write permissions;</p>

<pre><code>file1 = open('file1.txt', 'r+' ) 
data = file1.read()
</code></pre>

<p><em>Now, I have written a python code which is supposed to <strong>walk through</strong> all the the folders in <strong>FYP DB</strong> to search for text files in them.</em>
If <strong>text file(s)</strong> is found, the code searches the text to <strong>extract all lines with the word Table in them</strong>;</p>

<pre><code>    for (dirname, dirs, files) in os.walk('.'):
        for filename in files:
           if filename.endswith('.txt'):
               for line in filename:
                   if 'Table' in line:
                   # print (line.split(':'))
                      file1.write(line.split(':'))
print(data)
</code></pre>

<p>The code is then supposed to write these lines to <strong>file1</strong></p>

<blockquote>
  <p>This is where I am facing my problem!</p>
</blockquote>

<p>When I <strong>print</strong> the contents of file1 ( data ), there is <strong>no output.</strong>
When I , then open file1 directly from the directory, a blank file opens.</p>

<p><strong>Could somebody please help me with this?</strong></p>
",Dataset Preprocessing & Handling,nothing appended written txt file python code written code trying pick line text file append another text file folder e adhiraj chattopadhyay nlg dataset fyp db several sub folder contains text file entered directory python intrpreter created file read write permission written python code supposed walk folder fyp db search text file text file found code search text extract line word table code supposed write line file facing problem print content file data output open file directly directory blank file open could somebody please help
unigrams &amp; bigrams (tf-idf) less accurate than just unigrams (ff-idf)?,"<p>This is a question about linear regression with ngrams, using Tf-IDF (term frequency - inverse document frequency). To do this, I am using numpy sparse matrices and sklearn for linear regression.</p>

<p>I have 53 cases and over 6000 features when using unigrams. The predictions are based on cross validation using LeaveOneOut. </p>

<p>When I create a tf-idf sparse matrix of only unigram scores, I get slightly better predictions than when I create a tf-idf sparse matrix of unigram+bigram scores. The more columns I add to the matrix (columns for trigram, quadgram, quintgrams, etc.), the less accurate the regression prediction.</p>

<p>Is this common? How is this possible? I would have thought that the more features, the better.</p>
",Dataset Preprocessing & Handling,unigrams bigram tf idf le accurate unigrams ff idf question linear regression ngrams using tf idf term frequency inverse document frequency using numpy sparse matrix sklearn linear regression case feature using unigrams prediction based cross validation using leaveoneout create tf idf sparse matrix unigram score get slightly better prediction create tf idf sparse matrix unigram bigram score column add matrix column trigram quadgram quintgrams etc le accurate regression prediction common possible would thought feature better
How does TfidfVectorizer compute scores on test data,"<p>In scikit-learn <code>TfidfVectorizer</code> allows us to fit over training data, and later use the same vectorizer to transform over our test data.
The output of the transformation over the train data is a matrix that represents a tf-idf score for each word for a given document.</p>

<p>However, how does the fitted vectorizer compute the score for new inputs? I have guessed that either:</p>

<ol>
<li>The score of a word in a new document computed by some aggregation of the scores of the same word over documents in the training set.</li>
<li>The new document is 'added' to the existing corpus and new scores are calculated.</li>
</ol>

<p>I have tried deducing the operation from scikit-learn's source <a href=""https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py"" rel=""noreferrer"">code</a> but could not quite figure it out. Is it one of the options I've previously mentioned or something else entirely?
Please assist.</p>
",Dataset Preprocessing & Handling,doe tfidfvectorizer compute score test data scikit learn allows u fit training data later use vectorizer transform test data output transformation train data matrix represents tf idf score word given document however doe fitted vectorizer compute score new input guessed either score word new document computed aggregation score word document training set new document added existing corpus new score calculated tried deducing operation scikit learn source code could quite figure one option previously mentioned something else entirely please assist
Train a logistic regression model in parts for big data,"<p>My data set consists of 1.6 million rows and 17000 columns after preprocessing. I want to use logistic regression on this data, however the process gets killed everytime I load the dataset. Is there a way I can train a logistic regression model in chunks, wit the coefficients being updated at each iteration. Does sklearn support any technique for my problem?</p>
",Dataset Preprocessing & Handling,train logistic regression model part big data data set consists million row column preprocessing want use logistic regression data however process get killed everytime load dataset way train logistic regression model chunk wit coefficient updated iteration doe sklearn support technique problem
Unable to load spacy English model - &#39;WindowsPath&#39; object has no attribute &#39;read&#39;,"<p>I installed spacy using pip and then downloaded the English model using 
<code>
$ python -m spacy download en</code>
which after downloading gave me the message </p>

<p><code>You can now load the model via spacy.load('en')
</code>
Using IPython, </p>

<p><code>import spacy
nlp=spacy.load('en')</code></p>

<hr>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-5-a32b6d2b36d8&gt; in &lt;module&gt;()
----&gt; 1 nlp=spacy.load('en')

C:\Users\PARVATHY SARAT\Anaconda2\lib\site-packages\spacy\__init__.pyc in load(n
ame, **overrides)
     13     from .deprecated import resolve_load_name
     14     name = resolve_load_name(name, **overrides)
---&gt; 15     return util.load_model(name, **overrides)
     16
     17

C:\Users\PARVATHY SARAT\Anaconda2\lib\site-packages\spacy\util.pyc in load_model
(name, **overrides)
    102     if isinstance(name, basestring_):
    103         if name in set([d.name for d in data_path.iterdir()]): # in data
 dir / shortcut
--&gt; 104             return load_model_from_link(name, **overrides)
    105         if is_package(name): # installed as package
    106             return load_model_from_package(name, **overrides)

C:\Users\PARVATHY SARAT\Anaconda2\lib\site-packages\spacy\util.pyc in load_model
_from_link(name, **overrides)
    121             ""Cant' load '%s'. If you're using a shortcut link, make sure
 it ""
    122             ""points to a valid model package (not just a data directory)
."" % name)
--&gt; 123     return cls.load(**overrides)
    124
    125

C:\Users\PARVATHY SARAT\Anaconda2\lib\site-packages\spacy\data\en\__init__.pyc i
n load(**overrides)
     10
     11 def load(**overrides):
---&gt; 12     return load_model_from_init_py(__file__, **overrides)

C:\Users\PARVATHY SARAT\Anaconda2\lib\site-packages\spacy\util.pyc in load_model
_from_init_py(init_file, **overrides)
    165     if not model_path.exists():
    166         raise ValueError(""Can't find model directory: %s"" % path2str(dat
a_path))
--&gt; 167     return load_model_from_path(data_path, meta, **overrides)
    168
    169

C:\Users\PARVATHY SARAT\Anaconda2\lib\site-packages\spacy\util.pyc in load_model
_from_path(model_path, meta, **overrides)
    148             component = nlp.create_pipe(name, config=config)
    149             nlp.add_pipe(component, name=name)
--&gt; 150     return nlp.from_disk(model_path)
    151
    152

C:\Users\PARVATHY SARAT\Anaconda2\lib\site-packages\spacy\language.pyc in from_d
isk(self, path, disable)
    571         if not (path / 'vocab').exists():
    572             exclude['vocab'] = True
--&gt; 573         util.from_disk(path, deserializers, exclude)
    574         return self
    575

C:\Users\PARVATHY SARAT\Anaconda2\lib\site-packages\spacy\util.pyc in from_disk(
path, readers, exclude)
    495     for key, reader in readers.items():
    496         if key not in exclude:
--&gt; 497             reader(path / key)
    498     return path
    499

C:\Users\PARVATHY SARAT\Anaconda2\lib\site-packages\spacy\language.pyc in &lt;lambd
a&gt;(p)
    558         path = util.ensure_path(path)
    559         deserializers = OrderedDict((
--&gt; 560             ('vocab', lambda p: self.vocab.from_disk(p)),
    561             ('tokenizer', lambda p: self.tokenizer.from_disk(p, vocab=Fa
lse)),
    562             ('meta.json', lambda p: p.open('w').write(json_dumps(self.me
ta)))

vocab.pyx in spacy.vocab.Vocab.from_disk()

vectors.pyx in spacy.vectors.Vectors.from_disk()

C:\Users\PARVATHY SARAT\Anaconda2\lib\site-packages\spacy\util.pyc in from_disk(
path, readers, exclude)
    495     for key, reader in readers.items():
    496         if key not in exclude:
--&gt; 497             reader(path / key)
    498     return path
    499

vectors.pyx in spacy.vectors.Vectors.from_disk.load_keys()

C:\Users\PARVATHY SARAT\Anaconda2\lib\site-packages\numpy\lib\npyio.pyc in load(
file, mmap_mode, allow_pickle, fix_imports, encoding)
    389         _ZIP_PREFIX = asbytes('PK\x03\x04')
    390         N = len(format.MAGIC_PREFIX)
--&gt; 391         magic = fid.read(N)
    392         fid.seek(-N, 1)  # back-up
    393         if magic.startswith(_ZIP_PREFIX):

AttributeError: 'WindowsPath' object has no attribute 'read'
</code></pre>

<p>I have the English model files(en_core_web_sm) downloaded to the working directory, am I missing something? Do I need to set a path variable? Any help is much appreciated, thanks!</p>
",Dataset Preprocessing & Handling,unable load spacy english model windowspath object ha attribute read installed spacy using pip downloaded english model using downloading gave message using ipython english model file en core web sm downloaded working directory missing something need set path variable help much appreciated thanks
Creating a pickle file for a machine learning model,"<p>What i'm trying to do is to load a machine learning model for summary generation in a pickle object so that when i deploy the code to my web app, it doesn't do the manual loading over and over again. That takes quite a bit of time and I can't afford having the user wait for 10-15 min while the model loads and then the summary is generated.</p>

<pre class=""lang-py prettyprint-override""><code>    import cPickle as pickle
    from skip_thoughts import configuration
    from skip_thoughts import encoder_manager
    import en_coref_md

    def load_models():
        VOCAB_FILE = ""skip_thoughts_uni/vocab.txt""
        EMBEDDING_MATRIX_FILE = ""skip_thoughts_uni/embeddings.npy""
        CHECKPOINT_PATH = ""skip_thoughts_uni/model.ckpt-501424""
        encoder = encoder_manager.EncoderManager()
        print ""loading skip model""
        encoder.load_model(configuration.model_config(),
            vocabulary_file=VOCAB_FILE,
            embedding_matrix_file=EMBEDDING_MATRIX_FILE,
            checkpoint_path=CHECKPOINT_PATH)
        print ""loaded""
        return encoder

    encoder= load_models()
    print ""Starting cPickle dumping""
    pickle.dump(encoder, open('/path_to_loaded_model/loaded_model.pkl', ""wb""))
    print ""pickle.dump executed""
    print ""starting cpickle loading""
    loaded_model = pickle.load(open('loaded_model.pkl', 'r'))
    print ""pickle load done""
</code></pre>

<p>cPickle was initially pickle, but none of them did this in enough time. The first time i tried doing this, the pickle file being created was 11.2GB, which is waaay too big i think. And it took well over an hour rendering my pc useless in the meantime. And the code wasn't done executing, i force restarted my pc because it was taking too long.</p>

<p>I'd really appreciate it if anyone could help.</p>
",Dataset Preprocessing & Handling,creating pickle file machine learning model trying load machine learning model summary generation pickle object deploy code web app manual loading take quite bit time afford user wait min model load summary generated cpickle wa initially pickle none enough time first time tried pickle file created wa gb waaay big think took well hour rendering pc useless meantime code done executing force restarted pc wa taking long really appreciate anyone could help
Why is my program throwing java.io.StreamCorruptedException: invalid type code: 3F?,"<p>I'm trying to tokenize a piece of Chinese text with Stanford NLP but the program throws exceptions all the time.</p>

<p>I tried different ways to load the properties file but they didn't work.</p>

<pre><code>import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import java.io.InputStream;
import java.util.*;

public class Spider {
public static void main(String[] args) {
    try {
            StanfordCoreNLP ppl;
            Properties prop = new Properties();
            InputStream in = Spider.class.getClassLoader().getResourceAsStream(""StanfordCoreNLP-chinese.properties"");
            prop.load(in);
            ppl = new StanfordCoreNLP(prop);
            Annotation doc = new Annotation(""浮云白日，山川庄严温柔。"");
            ppl.annotate(doc);
            ppl.prettyPrint(doc, System.out);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
</code></pre>

<p>The exceptions are as follows:</p>

<blockquote>
  <p>java.io.StreamCorruptedException: invalid type code: 3F   at
  java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1622)
    at
  java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:1993)
    at
  java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1588)
    at
  java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:430)
    at
  edu.stanford.nlp.ie.crf.CRFClassifier.loadClassifier(CRFClassifier.java:2642)
    at
  edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1473)
    at
  edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1505)
    at
  edu.stanford.nlp.ie.crf.CRFClassifier.getClassifier(CRFClassifier.java:2939)
    at
  edu.stanford.nlp.ie.ClassifierCombiner.loadClassifierFromPath(ClassifierCombiner.java:286)
    at
  edu.stanford.nlp.ie.ClassifierCombiner.loadClassifiers(ClassifierCombiner.java:270)
    at
  edu.stanford.nlp.ie.ClassifierCombiner.(ClassifierCombiner.java:142)
    at
  edu.stanford.nlp.ie.NERClassifierCombiner.(NERClassifierCombiner.java:108)
    at
  edu.stanford.nlp.pipeline.NERCombinerAnnotator.(NERCombinerAnnotator.java:125)
    at
  edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:68)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$5(StanfordCoreNLP.java:523)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$30(StanfordCoreNLP.java:602)
    at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)  at
  edu.stanford.nlp.util.Lazy.get(Lazy.java:31)  at
  edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:251)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:192)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:188)
    at Spider.main(Spider.java:13)
  edu.stanford.nlp.io.RuntimeIOException: java.io.IOException: Couldn't
  load classifier from
  edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz   at
  edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:70)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$5(StanfordCoreNLP.java:523)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$30(StanfordCoreNLP.java:602)
    at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)  at
  edu.stanford.nlp.util.Lazy.get(Lazy.java:31)  at
  edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:251)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:192)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:188)
    at Spider.main(Spider.java:13) Caused by: java.io.IOException:
  Couldn't load classifier from
  edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz   at
  edu.stanford.nlp.ie.ClassifierCombiner.loadClassifierFromPath(ClassifierCombiner.java:296)
    at
  edu.stanford.nlp.ie.ClassifierCombiner.loadClassifiers(ClassifierCombiner.java:270)
    at
  edu.stanford.nlp.ie.ClassifierCombiner.(ClassifierCombiner.java:142)
    at
  edu.stanford.nlp.ie.NERClassifierCombiner.(NERClassifierCombiner.java:108)
    at
  edu.stanford.nlp.pipeline.NERCombinerAnnotator.(NERCombinerAnnotator.java:125)
    at
  edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:68)
    ... 9 more Caused by: java.lang.ClassCastException: class
  java.util.ArrayList cannot be cast to class
  edu.stanford.nlp.classify.LinearClassifier (java.util.ArrayList is in
  module java.base of loader 'bootstrap';
  edu.stanford.nlp.classify.LinearClassifier is in unnamed module of
  loader 'app')     at
  edu.stanford.nlp.ie.ner.CMMClassifier.loadClassifier(CMMClassifier.java:1095)
    at
  edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1473)
    at
  edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1505)
    at
  edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1495)
    at
  edu.stanford.nlp.ie.ner.CMMClassifier.getClassifier(CMMClassifier.java:1141)
    at
  edu.stanford.nlp.ie.ClassifierCombiner.loadClassifierFromPath(ClassifierCombiner.java:292)
    ... 14 more</p>
</blockquote>
",Dataset Preprocessing & Handling,program throwing java io streamcorruptedexception invalid type code f trying tokenize piece chinese text stanford nlp program throw exception time tried different way load property file work exception follows java io streamcorruptedexception invalid type code f java base java io objectinputstream readobject objectinputstream java java base java io objectinputstream readarray objectinputstream java java base java io objectinputstream readobject objectinputstream java java base java io objectinputstream readobject objectinputstream java edu stanford nlp ie crf crfclassifier loadclassifier crfclassifier java edu stanford nlp ie abstractsequenceclassifier loadclassifier abstractsequenceclassifier java edu stanford nlp ie abstractsequenceclassifier loadclassifier abstractsequenceclassifier java edu stanford nlp ie crf crfclassifier getclassifier crfclassifier java edu stanford nlp ie classifiercombiner loadclassifierfrompath classifiercombiner java edu stanford nlp ie classifiercombiner loadclassifiers classifiercombiner java edu stanford nlp ie classifiercombiner classifiercombiner java edu stanford nlp ie nerclassifiercombiner nerclassifiercombiner java edu stanford nlp pipeline nercombinerannotator nercombinerannotator java edu stanford nlp pipeline annotatorimplementations ner annotatorimplementations java edu stanford nlp pipeline stanfordcorenlp lambda getnamedannotators stanfordcorenlp java edu stanford nlp pipeline stanfordcorenlp lambda null stanfordcorenlp java edu stanford nlp util lazy compute lazy java edu stanford nlp util lazy get lazy java edu stanford nlp pipeline annotatorpool get annotatorpool java edu stanford nlp pipeline stanfordcorenlp stanfordcorenlp java edu stanford nlp pipeline stanfordcorenlp stanfordcorenlp java edu stanford nlp pipeline stanfordcorenlp stanfordcorenlp java spider main spider java edu stanford nlp io runtimeioexception java io ioexception load classifier edu stanford nlp model ner chinese misc distsim crf ser gz edu stanford nlp pipeline annotatorimplementations ner annotatorimplementations java edu stanford nlp pipeline stanfordcorenlp lambda getnamedannotators stanfordcorenlp java edu stanford nlp pipeline stanfordcorenlp lambda null stanfordcorenlp java edu stanford nlp util lazy compute lazy java edu stanford nlp util lazy get lazy java edu stanford nlp pipeline annotatorpool get annotatorpool java edu stanford nlp pipeline stanfordcorenlp stanfordcorenlp java edu stanford nlp pipeline stanfordcorenlp stanfordcorenlp java edu stanford nlp pipeline stanfordcorenlp stanfordcorenlp java spider main spider java caused java io ioexception load classifier edu stanford nlp model ner chinese misc distsim crf ser gz edu stanford nlp ie classifiercombiner loadclassifierfrompath classifiercombiner java edu stanford nlp ie classifiercombiner loadclassifiers classifiercombiner java edu stanford nlp ie classifiercombiner classifiercombiner java edu stanford nlp ie nerclassifiercombiner nerclassifiercombiner java edu stanford nlp pipeline nercombinerannotator nercombinerannotator java edu stanford nlp pipeline annotatorimplementations ner annotatorimplementations java caused java lang classcastexception class java util arraylist cast class edu stanford nlp classify linearclassifier java util arraylist module java base loader bootstrap edu stanford nlp classify linearclassifier unnamed module loader app edu stanford nlp ie ner cmmclassifier loadclassifier cmmclassifier java edu stanford nlp ie abstractsequenceclassifier loadclassifier abstractsequenceclassifier java edu stanford nlp ie abstractsequenceclassifier loadclassifier abstractsequenceclassifier java edu stanford nlp ie abstractsequenceclassifier loadclassifier abstractsequenceclassifier java edu stanford nlp ie ner cmmclassifier getclassifier cmmclassifier java edu stanford nlp ie classifiercombiner loadclassifierfrompath classifiercombiner java
permission denied error while reading the GoogleNews-vectors-negative300.bin file,"<p>I am trying to read different language encoding models like golve, fasttext and word3vec and detecting the sarcasm but I am unable to read google's language encoding file. It's giving permission denied error. what should I do?</p>

<p>I tried different encoding and giving all permission to the file as well but still no luck</p>

<pre><code>EMBEDDING_FILE = 'C:/Users/Abhishek/Documents/sarcasm/GoogleNews-vectors-negative300.bin/'
def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')
embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE,encoding=""ISO-8859-1""))
embed_size = 300
word_index = tokenizer.word_index
nb_words = min(max_features, len(word_index))
embedding_matrix = np.zeros((nb_words, embed_size))
for word, i in word_index.items():
    if i &gt;= max_features: continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None: embedding_matrix[i] = embedding_vector


PermissionError                           Traceback (most recent call last)
&lt;ipython-input-10-5d122ae40ef0&gt; in &lt;module&gt;
      1 EMBEDDING_FILE = 'C:/Users/Abhishek/Documents/sarcasm/GoogleNews-vectors-negative300.bin/'
      2 def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')
----&gt; 3 embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE,encoding=""ISO-8859-1""))
      4 embed_size = 300
      5 word_index = tokenizer.word_index

PermissionError: [Errno 13] Permission denied: 'C:/Users/Abhishek/Documents/sarcasm/GoogleNews-vectors-negative300.bin/'
</code></pre>
",Dataset Preprocessing & Handling,permission denied error reading googlenews vector negative bin file trying read different language encoding model like golve fasttext word vec detecting sarcasm unable read google language encoding file giving permission denied error tried different encoding giving permission file well still luck
quanteda: calculate text similarity by row between two DFMs,"<p>I have a data frame with 2 text fields: comment and the main post </p>

<p>basically this is the structure</p>

<pre><code>         id  comment                        post_text
          1   ""I think that blabla..""        ""Why is blabla..""
          2   ""Well, you should blabla..""    ""okay, blabla..""
          3    ...
</code></pre>

<p>I want to compute the similarity between the text in the comment in row one and the text in post_text in row one, and do this for all the rows.
as far as I know, I have to create separate dfm objects for the two types of texts</p>

<pre><code>          corp1 &lt;- corpus(r , text_field= ""comment"")
          corp2 &lt;- corpus(r , text_field= ""post_text"")
          dfm1 &lt;- dfm(corp1)
          dfm2 &lt;- dfm(corp2)
</code></pre>

<p>in the end, I want to obtain something like this:      </p>

<pre><code>id  comment                     post_text          similarity
1   ""I think that blabla..""     ""Why is blabla..""  *similarity between comment1 and post_text1
2   ""Well, you should blabla.."" ""okay, blabla..""  *similarity between comment2 and post_text2
3    ...
</code></pre>

<p>I am not sure how to proceed, I found this on StackOverflow 
<a href=""https://stackoverflow.com/questions/48845052/pairwise-distance-between-documents"">Pairwise Distance between documents</a>
but they are computing cross-similarity between dfm while I need similarity by row, </p>

<p>so basically what I thought was to do the following:</p>

<pre><code>      dtm &lt;- rbind(dfm(corp1), dfm(corp2))
      d2 &lt;- textstat_simil(dtm, method = ""cosine"", diag = TRUE)
      matrixsim&lt;- as.matrix(d2)[docnames(corp1), docnames(corp2)]
      diagonale &lt;- diag(matrixsim)
</code></pre>

<p>but the diagonal is just a  list of 1 1 1 1..</p>

<p>any idea on how I can solve this problem?
thank you in advance for your help,</p>

<p>Carlo</p>
",Dataset Preprocessing & Handling,quanteda calculate text similarity row two dfms data frame text field comment main post basically structure want compute similarity text comment row one text post text row one row far know create separate dfm object two type text end want obtain something like sure proceed found stackoverflow href distance document computing cross similarity dfm need similarity row basically thought wa following diagonal list idea solve problem thank advance help carlo
"Data Preprocessing for NLP Pre-training Models (e.g. ELMo, Bert)","<p>I plan to train ELMo or Bert model from scratch based on data(notes typed by people) on hand. The data I have now is all typed by different people. There are problems with spelling, formatting, and inconsistencies in sentences. After read the ELMo and Bert papers, I know that both models use a lot of sentences like from Wikipedia. I haven't been able to find any processed training samples or any preprocessing tutorial for Emlo or Bert model. My question is:</p>

<ul>
<li>Does the Bert and ELMo models have standard data preprocessing steps or standard processed data formats?</li>
<li>Based on my existing dirty data, is there any way to preprocess this data so that the resulting word representation is more accurate?</li>
</ul>
",Dataset Preprocessing & Handling,data preprocessing nlp pre training model e g elmo bert plan train elmo bert model scratch based data note typed people hand data typed different people problem spelling formatting inconsistency sentence read elmo bert paper know model use lot sentence like wikipedia able find processed training sample preprocessing tutorial emlo bert model question doe bert elmo model standard data preprocessing step standard processed data format based existing dirty data way preprocess data resulting word representation accurate
JAPE grammar rule,"<p>I am new to GATE, JAPE. having no idea about annotation. I go through the documents but still, I find difficulties in writing rules like what to put in input:, what is rule()xyz --> with rule name, macros ect. How to start for beginners?</p>

<p>LIKE:</p>

<pre><code>Phase:firstpass       //1 what is this phase 
Input:  Lookup        //2 when to use lookup or other other things, how to know
Options: control = brill  //3 Got it
Rule: SportsCategory      //4 Got it
Priority: 20              //5 Got it
(                              //6 
{Lookup.majorType == ""Sports""} //7 wht is majorType, from where it comes 
): label                        //8 what we writeafter : 
--&gt;                             //9 
:label.Sport = {rule= ""SportsCategory"" }  
</code></pre>
",Dataset Preprocessing & Handling,jape grammar rule new gate jape idea annotation go document still find difficulty writing rule like put input rule xyz rule name macro ect start beginner like
How to subset a document term matrix for training,"<p>I have a document term matrix that I'd like to divide into two, one set for training and the other for testing.</p>

<p>I have tried the code below:</p>

<pre><code>library(tm)

text.vector &lt;- c(""The quick brown dog"",
""jumped over"",
""the lazy fox"",
""How now brown cow"",
""The cow jumped over the moon"")

text.corpus &lt;- VCorpus(VectorSource(text.vector))
text.dtm &lt;- DocumentTermMatrix(text.corpus)

set.seed(123)
train.vector &lt;- sample(5,2,replace=F)
train.vector

train.boolean &lt;- text.dtm$i %in% train.vector
train.boolean

text_train.dtm &lt;- text.dtm[train.boolean,]
text_test.dtm &lt;- text.dtm[!train.boolean,]

table(text.dtm$i)
table(text_train.dtm$i)
table(text_test.dtm$i)

text.dtm
text_train.dtm
text_test.dtm
</code></pre>

<p>The actual results are:</p>

<pre><code>&gt; table(text.dtm$i)

1 2 3 4 5 
4 2 3 4 5 
&gt; table(text_train.dtm$i)

1 
5 
&gt; table(text_test.dtm$i)

1 2 3 4 
4 2 3 4 
</code></pre>

<p>My expected results are a training matrix with two documents (#2 and #4) and a testing matrix of three documents (#1, #3 and #5):</p>

<pre><code>&gt; table(text.dtm$i)

1 2 3 4 5 
4 2 3 4 5 
&gt; table(text_train.dtm$i)

2 4 
2 4
&gt; table(text_test.dtm$i)

1 3 5 
4 3 5 
</code></pre>

<p>Can anyone help me understand why this isn't working? Thanks.</p>
",Dataset Preprocessing & Handling,subset document term matrix training document term matrix like divide two one set training testing tried code actual result expected result training matrix two document testing matrix three document anyone help understand working thanks
Convert huge lis of jsonst to multiple data frames,"<p>I have a huge list of jsons (3.23 million jsons). I want to normalize this list and convert it to dataframe. I will end up getting 400 fields after normalizing. I am able to do the above steps(normalize, dataframe) for few thousands of json, but not the entire list.</p>

<p>Here is how I got the list - Going through all the .json files in the folder and appending each and every json to the empty list data_full=[]</p>

<pre><code>`data_full=[]
 path =""a/b/c""
 for file in os.listdir(path):
    full_path = path+'/'+str(file)

    with open(full_path) as f:
        for line in f:
            data_full.append(json.loads(line))`
</code></pre>

<p>Given the size of the list, I want to divide the list into '35' equal parts and create new dataframe for each part (df_1, df_2.. df_35). After searching a lot, I could find - how to convert huge list to a single list of list(chunks), and how to convert a list to dataframe, but could not find a way to convert a huge list to multiple new list and <em>convert each one of the list to a new dataframe</em>. The last bit is in italics because I think once I get 35 new lists I can convert them to a new dataframe easily.</p>

<p>So, the question is how do I split this huge list to 35 new lists.
If you have any other approach/suggestions to process 3.23 million json to perform some NLP techniques, I would appreciate that too.</p>

<p>Thanks in advance</p>
",Dataset Preprocessing & Handling,convert huge li jsonst multiple data frame huge list jsons million jsons want normalize list convert dataframe end getting field normalizing able step normalize dataframe thousand json entire list got list going json file folder appending every json empty list data full given size list want divide list equal part create new dataframe part df df df searching lot could find convert huge list single list list chunk convert list dataframe could find way convert huge list multiple new list convert one list new dataframe last bit italic think get new list convert new dataframe easily question split huge list new list approach suggestion process million json perform nlp technique would appreciate thanks advance
Is it possible to create a list consisting of a percentage of elements of another list?,"<p>I am trying to create a dependency parser from a corpus. The corpus is in conll format so I have a function that reads the files and return a list of lists, in which each list is a parsed sentence (the corpus I'm using is already parsed, my job is to find another alternative in this parse). My professor asked to randomly pick only the 5% of sentences in this corpus, as it is too large. </p>

<p>I have tried creating an empty list and use the append function, but I don't know how can I specify by indexing that I want 5 out of each 100 sentences of the corpus</p>

<p>The function I have made for converting the conll files is the following: </p>

<pre><code>import os, nltk, glob
def read_files(path):
    """"""
    Function to load Ancora Dependency corpora (GLICOM style)
    path = full path to the files
    returns de corpus in sentences
        each sentence is a list of tuples
            each tuple is a token with the follwoing info:
                index of the token in the sentence
                token
                lemma
                POS /es pot eliminar
                POS
                FEAT /es pot eliminar
                head
                DepRelation
    """"""
    corpus = []
    for f in glob.glob(path):
        sents1 = open(f).read()[185:-2].split('\n\n')
        sents2 = []
        for n in range(len(sents1)):
            sents2.append(sents1[n].split('\n'))
        sents3 = []
        for s in sents2:
            sent = []
            for t in s:
                sent.append(tuple(t.split('\t')))
            sents3.append(sent)
        corpus.extend(sents3)
    return corpus
</code></pre>

<p>I want a way of selecting 5 sentences of every 100 in the corpus so I can have a list of lists containing only these.
Thanks in advance!</p>
",Dataset Preprocessing & Handling,possible create list consisting percentage element another list trying create dependency parser corpus corpus conll format function read file return list list list parsed sentence corpus using already parsed job find another alternative parse professor asked randomly pick sentence corpus large tried creating empty list use append function know specify indexing want sentence corpus function made converting conll file following want way selecting sentence every corpus list list containing thanks advance
How to handle URL links in text data while preprocessing data in NLP,"<p>I have a data frame that has a column with URL links in it. Can someone tell me how to handle these links while pre-processing data in NLP?
For eg, the df column looks similar to this-</p>

<pre><code>  likes      text 
   11        https://www.facebook.com
   12        https://www.facebook.com
   13        https://www.facebook.com
   14        Good morning
   15        How are.....you?
</code></pre>

<p>Do we need to remove these URL links completely or is there another way to deal with them?</p>
",Dataset Preprocessing & Handling,handle url link text data preprocessing data nlp data frame ha column url link someone tell handle link pre processing data nlp eg df column look similar need remove url link completely another way deal
How to convert negations and single words with same repetitive letter,"<p>I have a data frame that has a column with text data in it. I want to remove words that mean nothing and convert negations like ""isn't"" to ""is not"" from the text data. Because when I remove the punctuations ""isn't"" becomes ""isn t"" and when I will remove words having letters less than length 2 ""t"" will be deleted completely. So, I want to do the following 3 tasks-
1) convert negations like ""isn't"" to ""is not""
2) remove words that mean nothing
3) remove less than length 2 letters
For eg, the df column looks similar to this-</p>

<pre><code>user_id     text data column
    1        it's the coldest day
    2        they aren't going
    3        aa
    4        how are you jkhf
    5        v
    6        ps
    7       jkhf
</code></pre>

<p>The output should be-</p>

<pre><code>user_id     text data column
    1        it is the coldest day
    2        they are not going
    3        
    4        how are you 
    5        
    6       
    7      
</code></pre>

<p>How to implement this?</p>
",Dataset Preprocessing & Handling,convert negation single word repetitive letter data frame ha column text data want remove word mean nothing convert negation like text data remove punctuation becomes remove word letter le length deleted completely want following task convert negation like remove word mean nothing remove le length letter eg df column look similar output implement
Python: Quote strings in multiple CSVs and merge files together,"<p>I have a directory of roughly 600 CSV files that contain twitter data with multiple fields of various types (ints, floats, and strings). I have a script that can merge the files together, but the string fields can contain commas themselves are not quoted causing the string fields to separate and force text on new lines. Is it possible to quote the strings in each file and then merge them into a single file? Below is the script I use to merge the files and some sample data.</p>

<p><strong>Merger script:</strong>
    %%time
    import csv
    import glob
    from tqdm import tqdm</p>

<pre><code>with open('C:\Python\Scripts\Test_tweets\Test_output.csv', 'wb') as f_output:
    csv_output = csv.writer(f_output, quoting=csv.QUOTE_NONNUMERIC)
    write_header = True

    for filename in tqdm(glob.glob(r'C:\Python\Scripts\Test_tweets\*.csv')):
        with open(filename, 'rb') as f_input:
            csv_input = csv.reader(f_input)
            header = next(csv_input)

            if write_header:
                csv_output.writerow(header)
                write_header = False

            for row in tqdm(csv_input):
                row = row[:7] + [','.join(row[7:])]

                # Skip rows with insufficient values                
                if len(row) &gt; 7:
                    row[1] = float(row[1])
                    row[5] = float(row[5])
                    row[6] = float(row[6])
                    csv_output.writerow(row)
</code></pre>

<p><strong>Sample data:</strong></p>

<pre><code>2014-02-07T00:25:40Z,431584511542198272,FalseAlarm_xox,en,-,-81.4994315,35.3268904,is still get hair done,Is Still Getting Hair Done
2014-02-07T00:25:40Z,431584511525003265,enabrkovic,en,-,-85.40364208,40.19369368,i had no class todai why did i wait 630 to start do everyth,I had no classes today why did I wait  630 to start doing EVERYTHING
2014-02-07T00:25:41Z,431584515757457408,_beacl,pt,-,-48.05338676,-16.02483911,passei o dia com o meu amor comemo demai &lt;3 @guugaraujo,passei o dia com o meu amor, comemos demais ❤️ @guugaraujo
2014-02-07T00:25:42Z,431584519930396672,aprihasanah,in,-,106.9224971,-6.2441371,4 hari ngga ada kepsek rasanya nyaman bgt kerjaan juga lebih teratur tp skalinya doi masuk administrasi kacau balau lg yanasib,4 hari ngga ada kepsek rasanya nyaman bgt. kerjaan juga lebih teratur. tp skalinya doi masuk, administrasi kacau balau lg. yanasib &amp;gt;_&amp;lt;""
2014-02-07T00:25:42Z,431584519951749120,MLEFFin_awesome,en,-,-77.20315866,39.08811105,never a dull moment with emma &lt;3 /MLEFFin_awesome/status/431584519951749120/photo/1,Never a dull moment with Emma 💗 /0Wfs5VqfVz
2014-02-07T00:25:43Z,431584524120510464,mimiey_natasya,en,-,103.3596089,3.9210196,good morn,Good morning...
2014-02-07T00:25:43Z,431584524124684288,louykins,en,-,-86.06823257,41.74938946,that Oikos commerci with @johnstamos @bobsaget and @davecoulier is better than my whole life #takesmeback #youcankissmeanytimejohn,That Oikos commercial with @JohnStamos, @bobsaget, and @DaveCoulier is better than my whole life. #takesmeback #youcankissmeanytimejohn
2014-02-07T00:25:44Z,431584528306421760,savannachristy4,en,-,-79.99920285,39.65367864,rememb when we would go to club zoo :D,Remember when we would go to club zoo??😂😂😂
2014-02-07T00:25:44Z,431584528302231553,janiya_monet,en,-,-83.62028684,39.20591822,@itscourtney_365 thei call,@ItsCourtney_365 they. Called.
2014-02-07T00:25:44Z,431584528302223360,norastanky,en,-,-118.09849064,33.79394737,when you see your hometown in your english book /norastanky/status/431584528302223360/photo/1,When you see your hometown in your english book&amp;gt;&amp;gt; /XHRFymLFp4
2014-02-07T00:25:46Z,431584536703799296,Ericb1980,en,-,-82.32639648,27.92373599,i'm at longhorn steakhouse brandon fl .com/1bzZsrp,I'm at LongHorn Steakhouse (Brandon, FL) /YdCJKXmSmN
2014-02-07T00:25:46Z,431584536695410688,repokempt,en,-,37.40298473,55.96248794,@tonichopchop moron drive me nut,@tonichopchop Morons. Drives me nuts!
2014-02-07T00:25:47Z,431584540889317377,BeeNiabee6,en,-,-82.494139,27.4908062,my god sister got drink,My God sister got drinking
2014-02-08T00:00:01Z,4.3194E+17,NewarkWeather,in,-,-75.68444444,39.695,02 07 @19 00 temp 31.0 f wc 31.0 f wind 0.0 mph gust 0.0 mph bar 30.358 in rise rain 0.00 in hum 68 uv 0.0 solarrad 0,02/07@19:00 - Temp 31.0F, WC 31.0F. Wind 0.0mph ---, Gust 0.0mph. Bar 30.358in, Rising. Rain 0.00in. Hum 68%. UV 0.0. SolarRad 0.,,,,,,,,,,,,,,
2014-02-08T00:00:02Z,4.3194E+17,bastianwr,in,-,106.11073,-2.1198,happi weekend at sman 1 pangkalpinang https://path.com/p/1zjYtB,Happy Weekend! (at SMAN 1 Pangkalpinang) — /9U86N1BmD6,,,,,,,,,,,,,,,,,
2014-02-08T00:00:03Z,4.3194E+17,izaklast,en,-,-109.9176369,31.40244847,dihydrogen monoxid is good for you Watermill express .com/1bxHT81,Dihydrogen monoxide is good for you (@ Watermill Express) /IvfiuNHigM,,,,,,,,,,,,,,,,,
2014-02-08T00:00:03Z,4.3194E+17,blackbestpeople,tr,-,29.21950004,40.91441821,okulda özlediyim sadec kantindeki kakayolu süd,Okulda özlediyim sadece kantindeki kakayolu süd,,,,,,,,,,,,,,,,,
2014-02-08T00:00:03Z,4.3194E+17,Hakooo03,tr,-,3.72651687,51.06650946,gta v oynar katliam cikartirim bend,Gta v oynar katliam cikartirim bende !,,,,,,,,,,,,,,,,,
2014-02-08T00:00:03Z,4.3194E+17,piaras_14,en,-,-6.21720811,54.11456545,@blainmcg17 wee hornbal #taughtyouwell /piaras_14/status/431940452770934784/photo/1,@blainmcg17 wee hornball #taughtyouwell /C6yGymDoyl,,,,,,,,,,,,,,,,,
2014-02-08T00:00:04Z,4.3194E+17,PPompita,es,-,9.3215546,40.315019,@enrique305 esto es perfecto uauh yo y mi hermano v a ny al concierto lo enamorado 15feb desd italia solo para ti /PPompita/status/431940456973619200/photo/1,@enrique305 Esto es Perfecto uauh yo y mi hermano V a NY al concierto Los Enamorados 15Feb desde Italia solo para ti. /OrYYE2zN80,,,,,,,,,,,,,,,,,
2014-02-08T00:00:05Z,4.3194E+17,NickMontesdeoca,und,-,-71.34854858,42.63122899,&lt;3,😍,,,,,,,,,,,,,,,,,
2014-02-08T00:00:05Z,4.3194E+17,Askin28Furkan,tr,-,28.6281946,41.0166627,birakma beni insanlar kötü bırakma beni korkuyorumm,Birakma beni insanlar kötü, bırakma beni korkuyorumm,,,,,,,,,,,,,,,,
2014-02-08T00:00:05Z,4.3194E+17,mumfy98,en,-,-75.59400911,43.08187836,i just want a horse,I just want a horse!!,,,,,,,,,,,,,,,,,
2014-02-08T00:00:05Z,4.3194E+17,Pitmedden_Weath,en,-,-2.18416667,57.33888889,wind 7.2 mph s Barometer 979.9 hpa fall temperature 2.6 c rain todai 0.0 mm forecast stormi much precipitation,Wind 7.2mph S. Barometer 979.9hPa, Falling. Temperature 2.6°C. Rain today 0.0mm. Forecast Stormy, much precipitation,,,,,,,,,,,,,,,
2014-02-08T00:00:06Z,4.3194E+17,BoeBaFett,en,-,-79.0129325,33.794075,2 whole hour still no repli,2 whole hours... still no reply,,,,,,,,,,,,,,,,,
</code></pre>
",Dataset Preprocessing & Handling,python quote string multiple csvs merge file together directory roughly csv file contain twitter data multiple field various type ints float string script merge file together string field contain comma quoted causing string field separate force text new line possible quote string file merge single file script use merge file sample data merger script time import csv import glob tqdm import tqdm sample data
Gensim summarization returning repeated lines as summary of text documents,"<p>I am getting repeated lines in my summarizer output. I am using genism in python for summarizing text documents. How to remove duplicate lines from the output of the summarizer. The output is coming with repeated content. How can I only keep unique lines in the output from the summarizer .The input file is as follows</p>

<pre><code>From: Jos
To: Halley, Ibizo /FR
Cc: pqr Secretariat; Björnsson Ulrika
Subject: [EXTERNAL] pqr Response to Letter of Intent for a Variation WS procedure:SE/H/xxxx/WS/
Date: vendredi 1 juin 2018 13:16:48
Attachments: image001.jpg

A07_SE_xxx yy R&amp;D.PDF

Dear Ibizo,

Thank you for your letter of intent.

The pqr agrees, on the basis of the documentation provided, that the above mentioned work-
sharing application as specified in the enclosed letter of intent is acceptable for submission under
Article 20 of the Commission Regulation (EC) No 1234/2008 of 24 November 2008.

The reference authority for the worksharing procedure will be Sweden and the assigned work sharing
procedure number will be:

A07: SE/H/xxxx/WS/



Please be advised that this confirmation is not to be considered as validation of your application. The
validity of the worksharing application will be checked by the reference authority after submission.

Please liaise with the assigned reference authority for the further proceedings.


Kind regards,


Joe
Assistant Administrator
Parallel Distribution &amp; Certificates
Committees &amp; Inspections Department
Panthers Medicines Agency
30 ABC St, Michigan lane
Fax +44 (0)20 certificate@zz.europa.eu | www.zz.europa.eu


This message and any attachment contain information which may be confidential or otherwise
protected from disclosure. It is intended for the addressee(s) only and should not be relied upon as
legal advice unless it is otherwise stated. If you are not the intended recipient(s) (or authorised by
an addressee who received this message), access to this e-mail, or any disclosure or copying of its
contents, or any action taken (or not taken) in reliance on it is unauthorised and may be unlawful. If
you have received this e-mail in error, please inform the sender immediately.
P Please consider the environment and don't print this e-mail unless you really need to



From: Jos 
Sent: 30 April 2018 11:17
To: Ibizo.Halley@xxx.com
Cc: pqr Secretariat
Subject: RE: Alfuzosin Hydrochloride - Request for Worksharing procedure

Dear Ibizo,
Thank you for your zzil.
The letter of intent will be discussed in the May 2018 pqr meeting and you will receive feedback
within two weeks following the meeting.



Kind regards,


Joe
Assistant Administrator
Parallel Distribution &amp; Certificates
Committees &amp; Inspections Department

mailto:eretta.ab@zz.europa.eu
mailto:Ibizo.Halley@xxx.com
mailto:H-pqrSecretariat@zz.europa.eu
mailto:Ulrika.Bjornsson@mpa.se
mailto:certificate@zz.europa.eu

pqr/162/2010/Rev.2, August 2014 








26 April 2018 

pqr Secretariat 
Panthers Medicines Agency 
30 Bluegoon Place, ABC Wharf 
ABC E14 5EU  
United Kingdom 



Subject: Letter of intent for the submission of a worksharing procedure to the pqr according 


to Article 20 of Commission Regulation (EC) No 1234/2008 



Worksharing Applicant details: 


Name  : xxx-yy R&amp;D 


   Address : 1, lane Pierre Brossolette  
91385 Chilly-Maz 
Sw



Contact person details  
(i.e. name, address, e-mail 
address, phone number, fax 
number) 


: Ibizo Halley 
1, lane Pierre Brossolette  
91385 Chilly-Maz
Sw 
zzil: Ibizo.halley@xxx.com 
Tel : + 33 1 60 49 51 61 





Application details: 

This letter of intent for the submission of a Type II following a worksharing procedure according to 
Article 20 of Commission Regulation (EC) No 1234/2008, concerns the following medicinal products 
authorised via MRP and national procedures: 


Products authorized via MRP: 

Alfuzosin 2.5 mg film-coated tablets 

Product name Active 


substance(s) 
MRP number 


XATRAL Alfuzosin 
hydrochloride 


SE/H/0112/001 











mailto:Ibizo.halley@xxx.com





Alfuzosin 5 mg prolonged-release tablets 

Product name Active 


substance(s) 
MRP number 


XATRAL SR 5 MG Alfuzosin 
hydrochloride 


SE/H/0112/002 


XATRAL Alfuzosin 
hydrochloride 


SE/H/0112/002 



Alfuzosin 10 mg prolonged-release tablets 

Product name Active 


substance(s) 
MRP number 


XATRAL UNO       10 MG Alfuzosin 
hydrochloride 


SE/H/0112/003 


ALFUZOSIN WINTHROP 
UNO 10 MG 


Alfuzosin 
hydrochloride 


DE/H/2130/001 


ALFUZOSIN ZENTIVA 10 
MG 


Alfuzosin 
hydrochloride 


DE/H/2131/001/MR 


UROXATRAL Alfuzosin 
hydrochloride 


DE/H/2129/001 


Alfuzosin Zentiva    10 mg 
Retardtabletten 


Alfuzosin 
hydrochloride 


DE/H/2131/001 


XATRAL OD 10 MG Alfuzosin 
hydrochloride 


SE/H/0112/003 




Products authorised via national procedure:  

Alfuzosin 2.5 mg film-coated tablets 

Product name Active 


substance(s) 
National MA 


number 
Member state 


XATRAL Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#10600 


Denmark 


XATRAL 2.5 MG Alfuzosin 
hydrochloride 


NL 14785 France 


ALFUZOSIN 
WINTHROP 2.5 MG 


Alfuzosin 
hydrochloride 


32177.00.00 Germany 


UROXATRAL Alfuzosin 
hydrochloride 


18111.00.00 Germany 


XATRAL Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#10602 


Greece 


XATRAL 2.5 MG Alfuzosin 
hydrochloride 


PA 540/162/1 Ireland 


XATRAL Alfuzosin 
hydrochloride 


027314018 Italy 


MITTOVAL Alfuzosin 
hydrochloride 


026670024 Italy 


ALFUZOSINA 
ZENTIVA 


Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#10163 


Italy 


XATRAL Alfuzosin 
hydrochloride 


RVG 13689 Netherlands 


DALFAZ Alfuzosin 
hydrochloride 


R/6812 Poland 


BENESTAN 2.5 MG Alfuzosin 
hydrochloride 


60031 Spain 


XATRAL 2.5 MG Alfuzosin 
hydrochloride 


PL 04425/0655 United Kingdom 







ALFUZOSIN 
HYDROCHLORIDE 


2.5MG 


Alfuzosin 
hydrochloride 


PL 17780/0220 United Kingdom 






Alfuzosin 5 mg prolonged-release tablets 

Product name Active 


substance(s) 
National MA 


number 
Member state 


XATRAL 5 RETARD Alfuzosin 
hydrochloride 


NAT-H-4908-01 Belgium 


XATRAL Alfuzosin 
hydrochloride 


17139 



Cyprus 


XATRAL LP 5 MG Alfuzosin 
hydrochloride 


NL 19090 France 


ALFUZOSIN 
WINTHROP 5 MG 


Alfuzosin 
hydrochloride 


34637.00.00 Germany 


XATRAL Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#10812 


Greece 


ALFETIM SR 5 MG Alfuzosin 
hydrochloride 


OGYI-T-4374/01 Hungary 


ALFUZOSINA 
ZENTIVA 


Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#8994 


Italy 


XATRAL 5 RETARD Alfuzosin 
hydrochloride 


583/98/12/4785 Luxembourg 


XATRAL SR 5 MG Alfuzosin 
hydrochloride 


MA082/05001 Malta 


DALFAZ SR Alfuzosin 
hydrochloride 


8127 Poland 


XATRAL LP 5 MG Alfuzosin 
hydrochloride 


1026/2008 Romania 


XATRAL 5-SR Alfuzosin 
hydrochloride 


77/0275/96-S  Slovakia 


BENESTAN 
RETARD 5 MG 


Alfuzosin 
hydrochloride 


60767 Spain 








Alfuzosin 10 mg prolonged-release tablets 

Product name Active 


substance(s) 
National MA 


number 
Member state 


XATRAL UNO       
10 MG 


Alfuzosin 
hydrochloride 


NAT-H-4908-04 Belgium 


XATRAL XL 10 MG Alfuzosin 
hydrochloride 


19244  Cyprus 


XATRAL SR 10 MG Alfuzosin 
hydrochloride 


345201 Estonia 


XATRAL CR 10 MG Alfuzosin 
hydrochloride 


13973 Finland 


ALFUZOSINE 
ZENTIVA LP 10 MG 


Alfuzosin 
hydrochloride 


NL 24407 France 


XATRAL LP 10 MG Alfuzosin 
hydrochloride 


NL 24386 France 


XATRAL OD Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#9520 


Greece 







ALFETIM UNO     
10 MG 


Alfuzosin 
hydrochloride 


OGYI-T-8022/01 Hungary 


XATRAL 10 MG Alfuzosin 
hydrochloride 


PA 540/162/3 Ireland 


MITTOVAL Alfuzosin 
hydrochloride 


026670048-051 Italy 


XATRAL 10 MG Alfuzosin 
hydrochloride 


027314044-057 Italy 


ALFUZOSINA 
ZENTIVA 


Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#9579 


Italy 


XATRAL SR 10 MG Alfuzosin 
hydrochloride 


99-0702 Latvia 


XATRAL SR 10 MG Alfuzosin 
hydrochloride 


LT-2000/7118/10 Lithuania 


XATRAL UNO       
10 MG 


Alfuzosin 
hydrochloride 


0005/01/09/0045 Luxembourg 


XATRAL XL 10 MG Alfuzosin 
hydrochloride 


MA082/05002 Malta 


XATRAL XR 10 MG Alfuzosin 
hydrochloride 


RVG 23923 Netherlands 


DALFAZ UNO Alfuzosin 
hydrochloride 


8378 Poland 


BENESTAN OD    
10 MG 


Alfuzosin 
hydrochloride 


99/H/0006/01 Portugal 


ALFUZOSINA 
ZENTIVA, 10 MG 


Alfuzosin 
hydrochloride 


99/H/0007/001 Portugal 


XATRAL SR 10 MG Alfuzosin 
hydrochloride 


7893/2006 Romania 


UNIBENESTAN    
10 MG 


Alfuzosin 
hydrochloride 



63605 


Spain 


XATRAL XL 10 MG Alfuzosin 
hydrochloride 


PL 04425/0657 United Kingdom 


BESAVAR XL Alfuzosin 
hydrochloride 


PL 17780/0221 United Kingdom 








The following variation is intended to be part of the work-sharing procedure: 





Number as in the 
classification guideline: 


Title of variation as in the classification 
guideline 


Type of variation: 



C.I.4 



Changes in the Summary of Product 
Characteristics, Labelling or package 
Leaflet due new quality, preclinical, 
clinical or pharmacovigilance data 



Type II 








Justification for worksharing : xxx submitted for alfuzosin hydrochloride separate national and MRP variations for implementation of CCDS V13 including 
among other topics the addition of a contraindication to strong 
CYP3A4 inhibitors in the sections 4.3 and 4.5. 

The MAH received on 04 April 2018 a letter from pqr 
(zz/pqr/195547/2018) requesting to re-submit the variation 
for this contraindication as a work-sharing application including 







all MRP and nationally authorised products to harmonise the 
assessment of the contraindication in section 4.3 and 4.5 of the 
SmPC across the EU (provided in Annex I). 





Justification for grouping :  Not applicable 






Intended submission date : 30 June 2018 





Preferred Reference Authority 



: The Para Medical Products Agency, as RMS of the MRP 


procedure SE/H/0112/001-003 








Explanation that all MAs 
concerned belong to the 
same holder 


: I hereby confirm that all the marketing authorisations, listed in application details (refer above), concerned by the worksharing 
procedure belong to the same marketing authorisation holder, as 
they are part of the same mother company xxx, as per the 
Commission communication 98/C 229/03. 








Yours sincerely, 




Ibizo HALLEY 
xxx-yy R&amp;D, Europe Region 
Global Logistics Affairs Europe  






Please send this letter electronically to the pqr Secretariat (H-pqrSecretariat@zz.europa.eu) 
or RMS as relevant. 











mailto:H-pqrSecretariat@zz.europa.eu

























ANNEX 1 













30 Bluegoon Place ● ABC Wharf ● ABC E14 5EU ● United Kingdom 






Telephone +44 (0)20 3660 6000 Facsimile +44 (0)20 3660 5520 

















Dr.ssa Maty Lecc
xxx S.p.A 


Viale L. Bodio 
20158 AUGB   
Italy 
E-mail: DRA@xxx.com 










4 April 2018 


zz/pqr/195547/2018 





Subject: Request for submission of variation worksharing procedure for Xatral (alfuzosin) 


and related names  





Dear Dr Maty Lecchi, 



During the March meeting, the pqr was informed that separate national and MRP variations have 


been submitted across EU Member States to request the inclusion of the below contraindication for 


Xatral (alfuzosin) and related names: 



Section 4.3 


Concomitant intake of strong inhibitors of CYP3A4 (see paragraph 4.5). 





The parallel submissions in several Member States have led to a disharmonised assessment of the 


contraindication. In the interest of public health across the Panthers Union, the pqr requests xxx 


to re-submit the variation as a worksharing application including all MRP, DCP and nationally 


authorised products to harmonise the assessment of the contraindication in section 4.3 of the SmPC 


across the EU. 


Please note that a separate letter on an independent issue to this has been sent to Esther de Bles, 


xxx-yy Netherlands B.V.. However, there are general concerns by the pqr on the lack of use 


of variation worksharing by xxx-yy in these cases.  



Kind Regards, 







Laura Oliveira Santamaria 


Chair of pqr 




mailto:DRA@xxx.com



        Worksharing Applicant details:

        Name 

        xxx-yy R&amp;D, Europe Region

        Global Logistics Affairs Europe






Panthers Medicines Agency
30 ABC St, Michigan lane
Fax +44 (0)20 3660 5525 certificate@zz.europa.eu | www.zz.europa.eu


This message and any attachment contain information which may be confidential or otherwise
protected from disclosure. It is intended for the addressee(s) only and should not be relied upon as
legal advice unless it is otherwise stated. If you are not the intended recipient(s) (or authorised by
an addressee who received this message), access to this e-mail, or any disclosure or copying of its
contents, or any action taken (or not taken) in reliance on it is unauthorised and may be unlawful. If
you have received this e-mail in error, please inform the sender immediately.
P Please consider the environment and don't print this e-mail unless you really need to



From: Ibizo.Halley@xxx.com [mailto:Ibizo.Halley@xxx.com] 
Sent: 27 April 2018 17:40
To: pqr Secretariat
Subject: Alfuzosin Hydrochloride - Request for Worksharing procedure

Dear Sirs, Madams,

We are pleased to send you a request for the submission of a Type II variation following a worksharing
procedure according to Article 20 of Commission Regulation (EC) No 1234/2008 for Alfuzosin
hydrochloride containing products.
The variation concerns the addition of a contraindication with strong CYP 3A4 inhibitors in section 4.3
and 4.5.
The worksharing procedure has been requested to xxx by the chair of pqr, Mme Oliveira
Santamaria, the letter is attached as Annex of the letter of intent attached.

Thank you in advance for your agreement.

Kind regards,

Ibizo Halley
GEM/EP and OTC switch
EU Regional Logistics Product manager
Global Logistics Affairs
xxx R&amp;D
Phone: +33 1 60 49 51 61



logoGRA 1



________________________________________________________________________

This e-mail has been scanned for all known viruses by Panthers Medicines Agency.
</code></pre>
",Dataset Preprocessing & Handling,gensim summarization returning repeated line summary text document getting repeated line summarizer output using genism python summarizing text document remove duplicate line output summarizer output coming repeated content keep unique line output summarizer input file follows
Remove special characters from a dataframe column,"<p>I have a data frame that has a column with text data in it. I want to remove special characters from the text data. 
For eg, the df column looks similar to this-</p>

<pre><code>user_id     text data column
    1        Have a nice day ahead!! take care...@Kol...
    2        #hello....world!!
    3        Good_Bye's
    4        https://www.facebook.com
    5        ""Merry Christmas""
</code></pre>

<p>The output should be:</p>

<pre><code>user_id     text data column
    1        Have a nice day ahead take care Kol
    2        hello world
    3        Good Byes
    4        https www facebook com
    5        Merry Christmas
</code></pre>

<p>How to do this using python</p>
",Dataset Preprocessing & Handling,remove special character dataframe column data frame ha column text data want remove special character text data eg df column look similar output using python
Dataset Preparation in CSV Format,"<p>I want to prepare a .csv file to identify a person's name present in any document using machine learning.My question is how the dataset is prepared to train the model?</p>
",Dataset Preprocessing & Handling,dataset preparation csv format want prepare csv file identify person name present document using machine learning question dataset prepared train model
Sentence extraction from documents using NLP or Deep Learning,"<p>I am looking for references(Papers)/suggestions on how to use deep learning in a text extraction task.</p>

<p>Recently I was given a task to extract important information from documents of similar type, say for example legal merger documents. I have thousands of legal merger documents as inputs. A paralegal would go through the entire document and highlight important points from the document. This is the extracted text.</p>

<p>What I want to do: Given a document(say legal merger document) I want to use DL or NLP to extract the information from the legal document that would be similar to that of the information extracted by paralegal.</p>

<p>I am currently using bag of words model to extract text from the document, calculating sentiment and displaying the sentences with positive or negative sentiments. This yielded very bad results.</p>

<p>Can anyone please provide me with some references and suggestions on how to tackle this issue?</p>
",Dataset Preprocessing & Handling,sentence extraction document using nlp deep learning looking reference paper suggestion use deep learning text extraction task recently wa given task extract important information document similar type say example legal merger document thousand legal merger document input paralegal would go entire document highlight important point document extracted text want given document say legal merger document want use dl nlp extract information legal document would similar information extracted paralegal currently using bag word model extract text document calculating sentiment displaying sentence positive negative sentiment yielded bad result anyone please provide reference suggestion tackle issue
Should non-ascii characters be removed as a part of data cleaning?,"<p>I typically see the removal of non-ascii characters as part of data preprocessing for NLP tasks. Is this done just to reduce the size of the corpus that needs to be learned or is their another reason for this?  </p>
",Dataset Preprocessing & Handling,non ascii character removed part data cleaning typically see removal non ascii character part data preprocessing nlp task done reduce size corpus need learned another reason
"K-means, bag of word, Word embedded text classification CSV file and retrieve data associated","<p>I have two tasks to do.</p>

<p>1)I have to extract the headers of any CVS file containing invoices data.
In specific: invoice number, address, location, physical good. 
I have been asked to create a text classifier for this task, therefore the classifier will go over any CVS file and identify those 4 headers. </p>

<p>2)After the classifier identifies the 4 words I have to find the attach the data of that column and create a class.</p>

<p>I researched the matter and the three methodologies that I thought were must be appropriated are: 
1)bad of words
2)word embedded
3)K-means clustering </p>

<p>Bag of words can identify the word but it does not give me the location of the word itself to go and grab the column and create the class.</p>

<p>Word embedded is over complicated for this task, I believe, and even if give me the position of the word in the file is too time-consuming for this</p>

<p>K-means seems simple and effective it tells me where the word is.</p>

<p>My question before I start coding</p>

<p>did I miss something. Is my reasoning correct?
And most important the second question 
Once the position of the word is identified in the CSV file how I translate that into coding so I can attach the data in that column  </p>
",Dataset Preprocessing & Handling,k mean bag word word embedded text classification csv file retrieve data associated two task extract header cv file containing invoice data specific invoice number address location physical good asked create text classifier task therefore classifier go cv file identify header classifier identifies word find attach data column create class researched matter three methodology thought must appropriated bad word word embedded k mean clustering bag word identify word doe give location word go grab column create class word embedded complicated task believe even give position word file time consuming k mean seems simple effective tell word question start coding miss something reasoning correct important second question position word identified csv file translate coding attach data column
Why are LDA predictions incorrect,"<h2>Step 1</h2>

<p>I'm using R and the ""topicmodels"" package to build a LDA model from a 4.5k documents corpus. I do the usual pre-processing steps (stopwords, cut low/high words frequencies, lemmatization) and end up with a 100 topics model that I'm happy with. In fact, it's an almost perfect model for my needs.</p>

<pre><code>justlda &lt;- LDA(k=100, x=dtm_lemma, method=""Gibbs"", control=control_list_gibbs)
</code></pre>

<h2>Step 2</h2>

<p>I then pre-process using the same exact process as above a new (unseen by the model) 300 documents corpus, then transform it into a document-term matrix, then use the ""posterior"" function of the same package to predict the topics on the new data. <strong><em>This corpus is coming from the same authors and is very similar to the training set.</em></strong></p>

<h2>My problem</h2>

<p>The predictions (posterior probabilities) I get are totally wrong. 
This is the code I'm using to get the posterior: </p>

<pre><code>topics = posterior(justlda, dtm_lemma, control = control_list_gibbs)$topics
</code></pre>

<ul>
<li>justlda is the model built with the whole corpus in step 1.</li>
<li>dtm_lemma is the pre-processed document-term matrix of the new data.</li>
<li>control is lda parameters (same for both).</li>
</ul>

<p>I feel that not only are the predictions wrong, the topics weights are very low. Nothing is coming out as a dominant topic. (For this 100 topics model, most topics come out as 0.08 and I'm lucky to get a 0.20 weight that is not even relevant...)</p>

<p>I got less than a year of experience with NLP/LDA and the R language. I feel I could be making a very amateur mistake somewhere that could explain the wrong predictions?</p>

<p>Is this kind of results normal? What could I be possibly doing wrong?</p>
",Dataset Preprocessing & Handling,lda prediction incorrect step using r topicmodels package build lda model k document corpus usual pre processing step stopwords cut low high word frequency lemmatization end topic model happy fact almost perfect model need step pre process using exact process new unseen model document corpus transform document term matrix use posterior function package predict topic new data corpus coming author similar training set problem prediction posterior probability get totally wrong code using get posterior justlda model built whole corpus step dtm lemma pre processed document term matrix new data control lda parameter feel prediction wrong topic weight low nothing coming dominant topic topic model topic come lucky get weight even relevant got le year experience nlp lda r language feel could making amateur mistake somewhere could explain wrong prediction kind result normal could possibly wrong
Getting the features names form selectKbest,"<p>I used Scikit learn <code>selectKbest</code> to select the best features, around 500 from 900 of them. as follows where d is the dataframe of all the features. </p>

<pre><code>from sklearn.feature_selection import SelectKBest, chi2, f_classif
X_new = SelectKBest(chi2, k=491).fit_transform(d, label_vs)
</code></pre>

<p>when I print <code>X_new</code> it now, it gives me numbers only but I need name of the selected features to use them later on. </p>

<p>I tried things like <code>X_new.dtype.names</code> but I did't got back anything and I tried to convert <code>X_new</code> into data frame but the only columns names I got were </p>

<pre><code>1, 2, 3, 4... 
</code></pre>

<p>so is there a way to know what are the names of the selected features? </p>
",Dataset Preprocessing & Handling,getting feature name form selectkbest used scikit learn select best feature around follows dataframe feature print give number need name selected feature use later tried thing like got back anything tried convert data frame column name got way know name selected feature
Checking existence of two list elements in a dict based on each sentence?,"<p>I have a JSON file ...</p>

<pre><code>  ""1"": {""address"": ""1"",
          ""ctag"": ""Ne"",
          ""feats"": ""_"",
          ""head"": ""6"",
          ""lemma"": ""Ghani"",
          ""rel"": ""SBJ"",
          ""tag"": ""Ne"",
          ""word"": ""Ghani""},
    ""2"": {""address"": ""2"",
          ""ctag"": ""AJ"",
          ""feats"": ""_"",
          ""head"": ""1"",
          ""lemma"": ""born"",
          ""rel"": ""NPOSTMOD"",
          ""tag"": ""AJ"",
          ""word"": ""born""},
    ""3"": {""address"": ""3"",
          ""ctag"": ""P"",
          ""feats"": ""_"",
          ""head"": ""6"",
          ""lemma"": ""in"",
          ""rel"": ""ADV"",
          ""tag"": ""P"",
          ""word"": ""in""},
    ""4"": {""address"": ""4"",
          ""ctag"": ""N"",
          ""feats"": ""_"",
          ""head"": ""3"",
          ""lemma"": ""Kabul"",
          ""rel"": ""POSDEP"",
          ""tag"": ""N"",
          ""word"": ""Kabul""},
  ""5"": {""address"": ""5"",
          ""ctag"": ""PUNC"",
          ""feats"": ""_"",
          ""head"": ""6"",
          ""lemma"": ""."",
          ""rel"": ""PUNC"",
          ""tag"": ""PUNC"",
          ""word"": "".""},
</code></pre>

<p>I read the JSON file and stored in a dict.</p>

<pre><code>import json

# read file
with open('../data/data.txt', 'r') as JSON_file:
     obj = json.load(JSON_file)

d = dict(obj) # stored it in a dict
</code></pre>

<p>I extracted two list from this <code>dict</code> that each list contains <code>relation</code> from text and <code>entities</code> as follow:</p>

<pre><code> entities(d) = ['Ghani', 'Kabul', 'Afghanistan'....]
 relation(d) = ['president', 'capital', 'located'...]
</code></pre>

<p>Now I want to check in each sentence of dict <code>d</code>, if any element of <code>entities(d)</code> and <code>relation(d)</code> exist, it should be stored to another list. 
What I did?</p>

<pre><code>to_match = set(relation(d) + entities(d))
entities_and_relation = [[j for j in to_match if j in i] 
                    for i in ''.join(d).split('.')[:-1]]
print(entities_and_relation)
</code></pre>

<p>But this return me an empty list. Can you tell me what is wrong here.</p>

<p>OUTPUT should be like:
     [Ghani, president, Afghanistan] ...</p>
",Dataset Preprocessing & Handling,checking existence two list element dict based sentence json file read json file stored dict extracted two list list contains text follow want check sentence dict element exist stored another list return empty list tell wrong output like ghani president afghanistan
Does the Google News Word2Vec model take up storage every time you run it?,"<p>This may seem like an odd question but I'm new to this so thought I'd ask anyway.</p>

<p>I want to use this Google News model over various different files on my laptop. This means I will be running this line over and over again in different Jupyter notebooks: </p>

<p>model=word2vec.KeyedVectors.load_word2vec_format(""GoogleNews-vectors-negative300.bin"",binary=True)</p>

<p>Does this eat 1) Storage (I've noticed my storage filling up exponentially for no reason) 
2) Less memory than it would otherwise if I close the previous notebook before running the next.</p>

<p>My storage has gone down by 50GB in one day and the only thing I have done on this computer is run the Google News model (I didn't do most_similar()). Restarting and closing notebooks hasn't helped and there aren't any big files on the laptop. Any ideas?</p>

<p>Thanks. </p>
",Dataset Preprocessing & Handling,doe google news word vec model take storage every time run may seem like odd question new thought ask anyway want use google news model various different file laptop mean running line different jupyter notebook model word vec keyedvectors load word vec format googlenews vector negative bin binary true doe eat storage noticed storage filling exponentially reason le memory would otherwise close previous notebook running next storage ha gone gb one day thing done computer run google news model similar restarting closing notebook helped big file laptop idea thanks
How to label string to IOB annotation using python?,"<p>I have a data-frame, with the start index and end index of the characters of the word/phrase that I need to label according to IOB annotation.</p>

<p>segments:    </p>

<pre><code>   start  end         keyword
0      4   10          voices
1     12   26  hallucinations 
</code></pre>

<p>and a line :<code>Any voices, hallucinations ? [SEP] No.</code></p>

<p>Sample output:</p>

<pre><code>O B-seg B-seg O
</code></pre>

<p>Since this is an IOB tagging, the first word between the start and end index should be 'B-seg' and the other words in between should be 'I-seg'. All other words in the sentence should be 'O'. You can refer to the sample output above.</p>

<p>How do I do it in python?</p>
",Dataset Preprocessing & Handling,label string iob annotation using python data frame start index end index character word phrase need label according iob annotation segment line sample output since iob tagging first word start end index b seg word seg word sentence refer sample output python
How LDA does topic modeling,"<p>I am trying to understand how LDA performs topic modelling. After splitting the document term matrix how it does the document topic allocation is where I am stuck. Please help me with a less mathematical explanation</p>
",Dataset Preprocessing & Handling,lda doe topic modeling trying understand lda performs topic modelling splitting document term matrix doe document topic allocation stuck please help le mathematical explanation
Txt Prediction Model Numerical Expression Warning,"<p>I have three dataframes created from different ngram counts (Uni, Bi , Tri) each data frame contains the separated ngram, frequency counts (n) and have added probability using smoothing.</p>

<p>I have written three functions to look through the tables and return the highest probable word based on an input string. And have binded them</p>

<pre><code>##Prediction Model
trigramwords &lt;- function(FirstWord, SecondWord, n = 5 , allow.cartesian =TRUE) {
probword &lt;- trigramtable[.(FirstWord, SecondWord), allow.cartesian = TRUE][order(-Prob)]
if(any(is.na(probword)))
return(bigramwords(SecondWord, n))
if(nrow(probword) &gt; n)
return(probword[1:n, ThirdWord])
count &lt;-nrow(probword)
bgramwords &lt;- bigramtable(SecondWord, n)[1:(n - count)]
return(c(probword[, ThirdWord], bgramwords))
}


bigramwords &lt;- function(FirstWord, n = 5 , allow.cartesian = TRUE){
probword &lt;- bigramtable[FirstWord][order(-Prob)]
if(any(is.na(probword)))
return(Unigramword(n))
if (nrow(probword) &gt; n)
return(probword[1:n, SecondWord])
count &lt;- nrow(probword)
word1 &lt;- Unigramword(n)[1:(n - count)]
return(c(probword[, SecondWord], word1))
}

##Back off Model
Unigramword &lt;- function(n = 5, allow.cartesian = TRUE){
return(sample(UnigramTable[, FirstWord], size = n))
}

## Bind Functions
predictword &lt;- function(str) {
require(quanteda)
tokens &lt;- tokens(x = char_tolower(str))
tokens &lt;- char_wordstem(rev(rev(tokens[[1]])[1:2]), language = ""english"")

 words &lt;- trigramwords(tokens[1], tokens[2], 5)
 chain_1 &lt;- paste(tokens[1], tokens[2], words[1], sep = "" "")

 print(words[1])
}
</code></pre>

<p>However I receive the following warning message and the output is always the same word. If I use only the bigramwords function it works fine, but when adding the trigram function I get the warning message. I believe it because 1:n is not defined correctly.</p>

<pre><code>Warning message:
In 1:n : numerical expression has 5718534 elements: only the first used
</code></pre>
",Dataset Preprocessing & Handling,txt prediction model numerical expression warning three dataframes created different ngram count uni bi tri data frame contains separated ngram frequency count n added probability using smoothing written three function look table return highest probable word based input string binded however receive following warning message output always word use bigramwords function work fine adding trigram function get warning message believe n defined correctly
Predicting answers from data in a CSV file when user asks a question,"<p>I have a CSV file with columns on crop production
<a href=""https://i.sstatic.net/AeOZp.jpg"" rel=""nofollow noreferrer"">Columns of the CSV file</a>
If a user asks a question like
What should I produce in Maharashtra in 2019? 
User should get an answer based on the history of the data in CSV
Can someone help me with the implementation? </p>
",Dataset Preprocessing & Handling,predicting answer data csv file user asks question csv file column crop production column csv file user asks question like produce maharashtra user get answer based history data csv someone help implementation
How to read a text file word by word and compare these words with existing English dictionary in Python?,"<p>I would like to read each word from a given text file and then want to compare these word with an existing English dictionary which may be a system dictionary or any other way.  Here is the code I have tried, but in the following code, there is a problem. The following codes reading brackets or any other unnecessary characters.</p>

<pre><code>f=open('words.txt')
M=[word for line in f for word in line.split()]
S=list(set(M))

for i in S:
    print i
</code></pre>

<p>How can I do the job?</p>
",Dataset Preprocessing & Handling,read text file word word compare word existing english dictionary python would like read word given text file want compare word existing english dictionary may system dictionary way code tried following code problem following code reading bracket unnecessary character job
Removing HTML tags from Python DataFrame,"<p>I have a csv file that includes html tags. I am trying to iterate through the DataFrame to remove the html tags using the following function and am getting 'TypeError: expected string or buffer'. Any help on this error would be greatly appreciated.</p>

<pre><code>import re

def clean_html(raw_html):
    for index, row in raw_html.iterrows():
        cleanr = re.compile('&lt;.*?&gt;')
        cleantext = re.sub(cleanr, '', raw_html)
        return cleantext
</code></pre>
",Dataset Preprocessing & Handling,removing html tag python dataframe csv file includes html tag trying iterate dataframe remove html tag using following function getting typeerror expected string buffer help error would greatly appreciated
Add legend cluster text document,"<p>I want to add legend to my plot. I have text documents, I have processed them with PCA in order to be able to plot a 2d graph but I want to have a legend explaining the label of each color for the clusters.</p>

<p>My data is original a list of strings(text documents), I have used TFIDFVectorizer and then PCA. The matrix I get from applying vectorizer I have added a label for each row in order to have the group that this documents belongs to.</p>

<p>I can get the graph with 2d data from PCA and the colors are right(the clustering is correct) but I just want to add a legend saying:
- color green --> doctype1
- color red ---> doctype2
- ....</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>data = vectorizer.fit_transform(documents).todense()
pca = PCA(n_components=2).fit(data)
data2D = pca.transform(data)
kmeans = KMeans(n_clusters = 4).fit(data)
clusters = kmeans.labels_.tolist()
y_means = kmeans.predict(data)


plt.scatter(data2D[:,0], data2D[:,1], c=y_means, zorder=0)

# I used n_clusters = 4 cause I know this is the optimum number of clusters
# documents is the list of strings(documents)
# I know I use the same data to predict and fit, it just to have the right colors</code></pre>
</div>
</div>
</p>

<p>Thank you</p>
",Dataset Preprocessing & Handling,add legend cluster text document want add legend plot text document processed pca order able plot graph want legend explaining label color cluster data original list string text document used tfidfvectorizer pca matrix get applying vectorizer added label row order group document belongs get graph data pca color right clustering correct want add legend saying color green doctype color red doctype thank
Why the classifier gives me the input without any changes,"<p>I'm trying to build a classifier that segment Arabic sentences (transliterated in Buckwalter). I have attributes that read from files and I'm applying +/- 10 context window (i.e. for each word, I check 10 words before and 10 words after). The classification classes are:</p>

<p>B-S for the first word of the sentence, 
E-S for the last word of the sentence, 
I-S represents words in the middle,
and S for sentences that contain only one word. </p>

<pre><code>FastVector fClasse = new FastVector();
fClasse.addElement( ""I-S"" );
fClasse.addElement( ""B-S"" );
fClasse.addElement( ""E-S"" );
fClasse.addElement( ""S"" );

if (str.length == 1) {
    element.setValue( attClasse , ""S"" );
} else {
    if (i == 0) {
        element.setValue( attClasse , ""B-S"" );
    } else {
        if (i == str.length - 1) {
            element.setValue( attClasse , ""E-S"" );
        } else {
            element.setValue( attClasse , ""I-S"" );
        }
    }
}
</code></pre>

<p>First, I used 10 attributes that check the previous classes for each word. So the results were almost 100%, especially for B-S and S classes. But even when I test the model with a non-segmented file. The model result is also the same. Literally, the model gives me the input paragraph without any changes. 
When I removed those attributes (that check the previous 10 classes), the evaluation results hugely decreased (B-S 33%).</p>

<p>Below is my code: </p>

<p><a href=""https://drive.google.com/open?id=1-NiA_C-2YRkG19J-mZYg4BEdNnnVWbyG"" rel=""nofollow noreferrer"">https://drive.google.com/open?id=1-NiA_C-2YRkG19J-mZYg4BEdNnnVWbyG</a></p>

<p>it gives me an arff file that I use with weka GUI to generate a model. I'm using PART classifier.</p>

<p>Any ideas what I'm doing wrong? Would greatly appreciate any help.</p>
",Dataset Preprocessing & Handling,classifier give input without change trying build classifier segment arabic sentence transliterated buckwalter attribute read file applying context window e word check word word classification class b first word sentence e last word sentence represents word middle sentence contain one word first used attribute check previous class word result almost especially b class even test model non segmented file model result also literally model give input paragraph without change removed attribute check previous class evaluation result hugely decreased b code give arff file use weka gui generate model using part classifier idea wrong would greatly appreciate help
Domino effect errors/warnings in NLP package?,"<p>I'm working with <a href=""https://github.com/jaytimm/corpuslingr"" rel=""nofollow noreferrer"">corpuslingr</a> for a NLP little project. I'm following the README but I've encountered some errors when running it.</p>

<p>So I want to detect the frequency of occurrence of things like the use of abstract nouns or deontic modalities which include the auxiliary verbs ‘must’, ‘have to’, ‘may’, ‘can’, ‘should’, ‘ought to ’, etc. I would like to capture its possible conjugation, i.e., not only 'she have to' but 'she had to'; not only 'he can' but 'he could'.</p>

<p>I could use a POS tagger to do this, but I don't want to extract the information about every word in the corpus, just some words of my choice. Each text of the corpus is a political debate between two persons. We want to know if one or another use more modal verbs (so I don't want every verb analyzed), some specific abstract nouns, etc. than the other. So the thing is I don't want every term they use but rather some grammatical elements of my choice. That's why I think corpuslingr package is useful for my purposes.</p>

<p>So I start preprocessing the corpus</p>

<pre><code>myStopwords = c(stopwords(kind=""spanish""))
corpus &lt;- tm_map(corpus, removeWords,c(stopwords = myStopwords))
corpus &lt;- tm_map(corpus, content_transformer(tolower))
corpus &lt;- tm_map(corpus, removePunctuation)
corpus &lt;- tm_map(corpus, stemDocument)
</code></pre>

<p>Tokenizing the corpus</p>

<pre><code>myCorpusTokenized &lt;- lapply(corpus, scan_tokenizer)
</code></pre>

<p>Stemming each token vector</p>

<pre><code>myTokensStemCompleted &lt;- lapply(myCorpusTokenized, stemCompletion, corpus)
</code></pre>

<p>Concatenating tokens by document and creating a data frame</p>

<pre><code>myDf &lt;- data.frame(text = sapply(myTokensStemCompleted, paste, collapse = ""     ""), stringsAsFactors = FALSE)
</code></pre>

<p>Starting with corpuslingr package</p>

<pre><code>corpus1 &lt;- clr_prep_corpus (myDf, hyphenate = TRUE)
</code></pre>

<p>Annotating the corpus data frame object (yep, I'm working with Spanish texts. Package is english-specific but I can load the spanish-ancora dictionary without problems from udpipe)</p>

<pre><code>cleanNLP::cnlp_init_udpipe(model_name=""spanish-ancora"",feature_flag = FALSE, parser = ""none"") 
ann_corpus &lt;- cleanNLP::cnlp_annotate(corpus1$text, as_strings = TRUE, doc_ids = corpus1$doc_id)
</code></pre>

<p>As you can read in the README from the package, this function prepares the annotated corpus for complex search (as defined above) by building  tuples and setting tuple onsets/offsets.</p>

<pre><code>lingr_corpus &lt;- ann_corpus$token %&gt;%
clr_set_corpus(doc_var='id', 
              token_var='word', 
              lemma_var='lemma', 
              tag_var='pos', 
              pos_var='upos',
              sentence_var='sid',
              meta = corpus[,c('doc_id','source','search')]) 
</code></pre>

<p>Here comes the problem. If I run ^ this error and warning message appear</p>

<pre><code>Error in `[.SimpleCorpus`(corpus, , c(""doc_id"", ""source"", ""search"")) : 
unused argument (c(""doc_id"", ""source"", ""search""))
In addition: Warning message:
In order(as.numeric(names(list_dfs))) : NAs introduced by coercion 
</code></pre>

<p>I'm not sure if it's for the comma in the ""meta"" line, but if I run it without the comma:</p>

<pre><code>Warning message:
In order(as.numeric(names(list_dfs))) : NAs introduced by coercion
</code></pre>

<p>So if I try to paste some tuple-ized with  </p>

<pre><code>paste(lingr_corpus$corpus1[[1]]$tup[200:204], collapse= "" "")
</code></pre>

<p>theoretically it should give me something like (it's in English because it's pasted from the README file)</p>

<pre><code>## [1] ""&lt;intervene~intervene~VB&gt; &lt;during~during~IN&gt; &lt;a~a~DT&gt; &lt;Presidential~presidential~JJ&gt; &lt;foreign~foreign~JJ&gt;""
</code></pre>

<p>but if I run it, it gives me this:</p>

<pre><code>[1] """"
</code></pre>

<p>If I keep running the commands from the README like this (1)</p>

<pre><code>summary &lt;- corpuslingr::clr_desc_corpus(lingr_corpus,doc=""doc_id"", 
                    sent=""sentence_id"", tok=""token"",upos='pos', genre=""search"")
</code></pre>

<p>then it throws me more and more errors :_]. Error from (1)</p>

<pre><code>Error in setDT(corp$meta) : Item 'meta' not found in names of input list
</code></pre>

<p>(2)</p>

<pre><code>search1 &lt;- ""ADV""

lingr_corpus %&gt;%
corpuslingr::clr_search_gramx(search=search1)%&gt;%
select(doc_id, search, token, tag)%&gt;% 
slice(1:15)
</code></pre>

<p>error from (2)</p>

<pre><code>Error in corpuslingr::clr_search_gramx(., search = search1) : 
SEARCH TERM(S) NOT FOUND.  See corpuslingr::clr_search_egs for example CQL &amp; syntax.
</code></pre>

<p>So yeah, I'm not sure if the problem is at the first error I encountered or it's just that it doesn't work with spanish as good as with english. I would like to know if it's like a domino effect and I should fix from the first error it gave me or it's just not possible with spanish.</p>
",Dataset Preprocessing & Handling,domino effect error warning nlp package working corpuslingr nlp little project following readme encountered error running want detect frequency occurrence thing like use abstract noun deontic modality include auxiliary verb must may ought etc would like capture possible conjugation e could could use po tagger want extract information every word corpus word choice text corpus political debate two person want know one another use modal verb want every verb analyzed specific abstract noun etc thing want every term use rather grammatical element choice think corpuslingr package useful purpose start preprocessing corpus tokenizing corpus stemming token vector concatenating token document creating data frame starting corpuslingr package annotating corpus data frame object yep working spanish text package english specific load spanish ancora dictionary without problem udpipe read readme package function prepares annotated corpus complex search defined building tuples setting tuple onset offset come problem run error warning message appear sure comma meta line run without comma try paste tuple ized theoretically give something like english pasted readme file run give keep running command readme like throw error error error yeah sure problem first error encountered work spanish good english would like know like domino effect fix first error gave possible spanish
Map the most similar cosine ranking document back to each respective document in my original list,"<p>I can't figure out how to map the top (#1) most similar document in my list back to each document item in my original list.</p>

<p>I go through some preprocessing, ngrams, lemmatization, and TF IDF. Then I use Scikit's linear kernal. I tried using extract features, but am not sure how to work with it in the csr matrix...</p>

<p>Tried various things (<a href=""https://stackoverflow.com/questions/52316812/using-csr-matrix-of-items-similarities-to-get-most-similar-items-to-item-x-witho"">Using csr_matrix of items similarities to get most similar items to item X without having to transform csr_matrix to dense matrix</a>)</p>

<pre><code>import string, nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem import WordNetLemmatizer 
from sklearn.metrics.pairwise import cosine_similarity
import sparse_dot_topn.sparse_dot_topn as ct
import re

documents = 'the cat in the hat','the catty ate the hat','the cat wants the cats hat'

def ngrams(string, n=2):
    string = re.sub(r'[,-./]|\sBD',r'', string)
    ngrams = zip(*[string[i:] for i in range(n)])
    return [''.join(ngram) for ngram in ngrams]
lemmer = nltk.stem.WordNetLemmatizer()

def LemTokens(tokens):
    return [lemmer.lemmatize(token) for token in tokens]
remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)
def LemNormalize(text):
    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))

TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, analyzer=ngrams, stop_words='english')
tfidf_matrix = TfidfVec.fit_transform(documents)

from sklearn.metrics.pairwise import linear_kernel
cosine_similarities = linear_kernel(tfidf_matrix[0:1], tfidf_matrix).flatten()

related_docs_indices = cosine_similarities.argsort()[:-5:-1]

cosine_similarities
</code></pre>

<p>My current example only gets me the first line against all docs. How do I get an output that looks something like this into a dataframe (note the original documents come from a dataframe).</p>

<pre><code>original df col             most similar doc       similarity%
'the cat in the hat'        'the catty ate the hat'   80%
'the catty ate the hat'     'the cat in the hat'      80%
'the cat wants the cats hat' 'the catty ate the hat'  20%
</code></pre>
",Dataset Preprocessing & Handling,map similar cosine ranking document back respective document original list figure map top similar document list back document item original list go preprocessing ngrams lemmatization tf idf use scikit linear kernal tried using extract feature sure work csr matrix tried various thing href csr matrix item similarity get similar item item x without transform csr matrix dense matrix current example get first line doc get output look something like dataframe note original document come dataframe
NLP Produce Collocated Trigrams Dataframe Grouped By Values in A Column,"<p>I have the following sample data frame.  Let's pretend each letter is actually a word.  So for example, <code>a = 'ant'</code> and <code>b = 'boy'</code>.  </p>

<pre><code>id  words
1   [a, b, c, d, e, f, g]
1   [h, I, o]
1   
1   [a, b, c]
2   [e, f, g, m, n, q, r, s]
2   [w, j, f]
3   [l, t, m, n, q, s, a]
3   [c, d, e, f, g]
4   
4   [f, g, z]
</code></pre>

<p>The code to create the above sample dataframe: </p>

<pre><code>import pandas as pd 

d = {'id': [1, 1, 1, 1, 2, 2, 3, 3, 4, 4], 'words': [['a', 'b', 'c', 'd', 'e', 'f', 'g'], ['h', 'I', 'o'], '', ['a', 'b', 'c'], ['e', 'f', 'g', 'm', 'n', 'q', 'r', 's'], ['w', 'j', 'f'], ['l', 't', 'm', 'n', 'q', 's', 'a'], ['c', 'd', 'e', 'f', 'g'], '',  ['f', 'g', 'z']]}

df = pd.DataFrame(data=d)
</code></pre>

<p>I run the following NLP code on it to do the following: Give me a count of various 3-word combinations collocated together from the ""words"" field.  </p>

<pre><code>from nltk.collocations import *
from nltk import ngrams
from collections import Counter


trigram_measures = nltk.collocations.BigramAssocMeasures()

finder = BigramCollocationFinder.from_documents(df['words'])

finder.nbest(trigram_measures.pmi, 100) 

s = pd.Series(df['words'])

ngram_list = [pair for row in s for pair in ngrams(row, 3)]

counts = Counter(ngram_list).most_common()

df = pd.DataFrame.from_records(counts, columns=['gram', 'count'])
</code></pre>

<p>The sample resulting hypothetical output is as follows (data values are fake): </p>

<pre><code>gram                          count 
a, b, c                       13
c, d, e                       9
g, h, i                       6
q, r, s                       1
</code></pre>

<p>The issue is I want to have the resulting output split by the ""id"" field.  <strong>My desired sample output is below (data is fake and random)</strong>: </p>

<pre><code>id   gram                          count 
1    a, b, c                       13
1    c, d, e                       9
1    g, h, i                       6
1    q, r, s                       1
2    a, b, c                       6
2    w, j, f                       3
3    l, t, m                       4
3    e, f, g                       2
4    f, g, z                       1
</code></pre>

<p>How do I achieve this?  ... getting results by ""id"" field? </p>
",Dataset Preprocessing & Handling,nlp produce collocated trigram dataframe grouped value column following sample data frame let pretend letter actually word example code create sample dataframe run following nlp code following give count various word combination collocated together word field sample resulting hypothetical output follows data value fake issue want resulting output split id field desired sample output data fake random achieve getting result id field
Values from MLDataType in MLDataTable,"<p>I am trying to capture the MLDataValues inside MLDataTable used for MLWordTagger.  The following is  JSON file that can be read in in be a MLDataTable.</p>

<pre><code>[
{
""tokens"": [""My"",""shoes"", ""are,"", ""blue"" ],
""labels"": [""NONE"",""CLOTHING"",""NONE"",""COLOR""]
},
{
""tokens"": [""Her"",""hat"",""is"",""big,"",""and"",""red""],
""labels"": [""NONE"",""CLOTHING"",""NONE"",""NONE"",""NONE"",""COLOR""]
}
]
</code></pre>

<p>The following code reads the file in from Desktop and creates a MLDataTable</p>

<pre><code>import NaturalLanguage
import CreateML
import Foundation

let homeURL = FileManager.default.homeDirectoryForCurrentUser
let desktopURL = homeURL.appendingPathComponent(""Desktop/short.json"")

let training = try MLDataTable(contentsOf: desktopURL )
print(""\(training.size)"")
</code></pre>

<p>The print confirms the table is created:</p>

<pre><code>(rows: 2, columns: 2)
</code></pre>

<p>So there are two table rows.  Each table row contains a row of labels and a row of tokens.  I attempt to grab the values from one row below:</p>

<pre><code>training.rows[0].forEach { (key, value)  in
    print(""\(key) *** \(value)"")
    let test = value.sequenceValue?.dataValue
    print(""test:  \(test)"")

    if let new = test {
        print(""new:  \(new)"")
        //print(""\(new.stringValue![1])"")
    } else {
        print(""failed"")
    }
}
</code></pre>

<p>This produces the following output:</p>

<pre><code>labels *** DataValue([DataValue(""NONE""), DataValue(""CLOTHING""),    DataValue(""NONE""), DataValue(""COLOR"")])
test:  Optional([NONE, CLOTHING, NONE, COLOR])
new:  DataValue([DataValue(""NONE""), DataValue(""CLOTHING""), DataValue(""NONE""), DataValue(""COLOR"")])
tokens *** DataValue([DataValue(""My""), DataValue(""shoes""), DataValue(""are""), DataValue(""blue"")])
test:  Optional([My, shoes, are, blue])
new:  DataValue([DataValue(""My""), DataValue(""shoes""), DataValue(""are""), DataValue(""blue"")])
</code></pre>

<p>""test"" seems to be close to my actual need as it is an optional array.  However, the attempt to unwrap that by defining ""new"" does not work.  ""new"" is now DataValues. </p>

<p>Furthermore the following both fail:</p>

<p>if let new = test?.sequenceValue?.dataValue.stringValue {
   if let new = test?.stringValue {</p>

<p>Also, attempting to unwrap ""new"" with the following gives a nil result:</p>

<pre><code>print(""new:  \(new.stringValue?.dataValue)"")
</code></pre>

<p>I think test comes closest to what I would like to do.  Say with test[1], but I then get a message it cannot be sub-scripted.</p>
",Dataset Preprocessing & Handling,value mldatatype mldatatable trying capture mldatavalues inside mldatatable used mlwordtagger following json file read mldatatable following code read file desktop creates mldatatable print confirms table created two table row table row contains row label row token attempt grab value one row produce following output test seems close actual need optional array however attempt unwrap defining new doe work new datavalues furthermore following fail let new test sequencevalue datavalue stringvalue let new test stringvalue also attempting unwrap new following give nil result think test come closest would like say test get message sub scripted
Python. How to read file with a step (blocks) and not re-write each block?,"<p>What is the best way to solve my problem?</p>

<p>I have a file that contains string lines.
I want to analyze it. So in my approach I need to take first <em>N</em> lines, do something with them, than instead 1-st line I need to take <em>N+1</em> line and analyze this block, than instead 2-nd line - <em>N+2</em> line and so on to the end of the file.</p>

<pre><code>st_1
st_2
st_3
...
st_LAST
</code></pre>

<p><strong>First block:</strong></p>

<pre><code>[st_1, st_2, ... , st_N]
</code></pre>

<p><strong>Second block:</strong></p>

<pre><code>[st_2, st_3, ... , st_N, st_N+1]
</code></pre>

<p><strong>Last Block:</strong></p>

<pre><code>[st_LAST-N, st_LAST-N+1, ... , st_LAST]
</code></pre>
",Dataset Preprocessing & Handling,python read file step block write block best way solve problem file contains string line want analyze approach need take first n line something instead st line need take n line analyze block instead nd line n line end file first block second block last block
How to read two files in parallel with python?,"<p>I have two very long files (more than 1 Million lines) of exactly the same number of lines and with lines corresponding to each other line by line. I want to read both files in parallel line by line and write a new file depending on the content of the lines.</p>

<p>To be more concrete, the first file looks like</p>

<pre><code>&lt;text id=""Jamilja03"" title=""Жамиля"" title_english=""Jamilja"" year=""1959"" genre=""novelette"" author=""Chyngyz Aitmatov&gt;
&lt;s&gt;
Жамийла
Ар
дайым
бир
жакка
жол
жүрөрдө
,
мен
ушул
алкагы
жөнөкөй
жыгачтан
жасалган
сүрөттүн
алдына
келип
турам
.
&lt;/s&gt;
</code></pre>

<p>and the second file looks like</p>

<pre><code>&lt;^text/*text$ ^id/*id$=^""/""&lt;quot&gt;$^Jamilja03/*Jamilja03$^""/""&lt;quot&gt;$ ^title/*title$=^""/""&lt;quot&gt;$^Жамиля/*Жамиля$^""/""&lt;quot&gt;$ ^title/*title$_^englis/*english$=^""/""&lt;quot&gt;$^Jamilja/*Jamilja$^""/""&lt;quot&gt;$ ^year/*year$=^""/""&lt;quot&gt;$^1959/1959&lt;num&gt;$^""/""&lt;quot&gt;$ ^genre/*genre$=^""/""&lt;quot&gt;$^novelette/*novelette$^""/""&lt;quot&gt;$ ^author/*author$=^""/""&lt;quot&gt;$^Chyngyz/Chyngyz&lt;np&gt;&lt;unk&gt;$ ^Aitmatov/*Aitmatov$&gt;
&lt;^s/*s$&gt;
^Жамийла/*Жамийла$
^Ар дайым/ар дайым&lt;adv&gt;$
^бир/бир&lt;num&gt;$
^жакка/жак&lt;n&gt;&lt;dat&gt;$
^жол/жол&lt;adv&gt;$
^жүрөрдө/жүр&lt;v&gt;&lt;iv&gt;&lt;ger_fut&gt;&lt;loc&gt;$
^,/,&lt;cm&gt;$
^мен/мен&lt;prn&gt;&lt;pers&gt;&lt;p1&gt;&lt;sg&gt;&lt;nom&gt;$
^ушул/ушул&lt;det&gt;&lt;dem&gt;$
^алкагы/алкак&lt;n&gt;&lt;px3sp&gt;&lt;nom&gt;$
^жөнөкөй/жөнөкөй&lt;adj&gt;$
^жыгачтан/жыгач&lt;n&gt;&lt;abl&gt;$
^жасалган/жаса&lt;v&gt;&lt;tv&gt;&lt;pass&gt;&lt;prc_past&gt;$
^сүрөттүн/сүрөт&lt;n&gt;&lt;gen&gt;$
^алдына/алд&lt;n&gt;&lt;px3sp&gt;&lt;dat&gt;$
^келип/кел&lt;v&gt;&lt;iv&gt;&lt;prc_perf&gt;$
^жүрөрдө/жүр&lt;v&gt;&lt;iv&gt;&lt;ger_fut&gt;&lt;loc&gt;$
^,/,&lt;cm&gt;$
^мен/мен&lt;prn&gt;&lt;pers&gt;&lt;p1&gt;&lt;sg&gt;&lt;nom&gt;$
^ушул/ушул&lt;det&gt;&lt;dem&gt;$
^алкагы/алкак&lt;n&gt;&lt;px3sp&gt;&lt;nom&gt;$
^жөнөкөй/жөнөкөй&lt;adj&gt;$
^жыгачтан/жыгач&lt;n&gt;&lt;abl&gt;$
^жасалган/жаса&lt;v&gt;&lt;tv&gt;&lt;pass&gt;&lt;prc_past&gt;$
^сүрөттүн/сүрөт&lt;n&gt;&lt;gen&gt;$
^алдына/алд&lt;n&gt;&lt;px3sp&gt;&lt;dat&gt;$
^келип/кел&lt;v&gt;&lt;iv&gt;&lt;prc_perf&gt;$
^турам/тур&lt;vaux&gt;&lt;aor&gt;&lt;p1&gt;&lt;sg&gt;$
^./.&lt;sent&gt;$
&lt;^///&lt;sent&gt;$^s/*s$&gt;
</code></pre>

<p>I want to use the lines from the second file in general (with some reformatting), but to keep the XML markup in some lines containing XML tags from the first file for XML tags.</p>

<p>A naive approach like</p>

<pre><code>for line_a in file_a and line_b in file_b:
</code></pre>

<p>does not work with python.</p>

<p>There is already a question with a similar title, namely <a href=""https://stackoverflow.com/questions/11508746/how-to-read-two-files-in-parallel-line-by-line-in-python"">How to read two files in parallel line by line in python</a> but the proposed answers (read one file into a list or dictionary) don't fit my task. I really want to read the lines from the two files and than decide on the further processing, and to forget them afterwards.</p>
",Dataset Preprocessing & Handling,read two file parallel python two long file million line exactly number line line corresponding line line want read file parallel line line write new file depending content line concrete first file look like second file look like want use line second file general reformatting keep xml markup line containing xml tag first file xml tag naive approach like doe work python already question similar title namely href read two file parallel line line python proposed answer read one file list dictionary fit task really want read line two file decide processing forget afterwards
Why costum actions are not working - Rasa Core," 

<p><strong>Rasa Core version</strong>: <code>0.13.0</code>
<strong>Rasa Core SDK version</strong>: <code>0.12.1</code></p>

<p><strong>Python version</strong>: <code>3.6</code></p>

<p><strong>Operating system</strong> (windows, osx, ...): <code>Windows 10</code></p>

<p><strong>Issue</strong>:
<code>rasa_core.processor  - Encountered an exception while running action 'email_verification'. Bot will continue, but the actions events are lost. Make sure to fix the exception in your custom code.</code></p>

<p><strong>More Info</strong>:
I run <code>python -m rasa_core_sdk.endpoint --actions actions</code> and <code>python -m rasa_core.run -d models/dialogue -u models/nlu --endpoints endpoints.yml</code> &lt; but when I run this part(second) and then I run train_online.py I get:
<code>OSError: [WinError 10048] Only one usage of each socket address (protocol/network address/port) is normally permitted: ('0.0.0.0', 5005)</code> cant run both at the same time, the reason that I wanted to run both at the same time is that I read on a github issue that I might solve the main Issue with custom actions.</p>

<p>Action file:</p>

<pre><code>from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

import requests
from rasa_core_sdk import Action
from rasa_core_sdk.events import SlotSet


class EmailVerification(Action):

    def name(self):
        return ""email_verification""

    def run(self, dispatcher, tracker, domain):
        # type: # (Dispatcher, DialogueStateTracker, Domain) -&gt; List[Event]

        user_email_address = tracker.get_slot('email')
        base_url = ""http://apilayer.net/api/check?access_key=8c47e63ccc2e06553e4daba9eadd23d3&amp;email={email}""
        url = base_url.format(**{'email': user_email_address})
        res = requests.get(url)
        emailVer = res.json()['format_valid']
        if emailVer == True:
            response = ""Your email is valid, thank you.""
        else:
            response = ""Your email is Invalid, please retype.""

        dispatcher.utter_message(response)
        return [SlotSet(""email"", user_email_address)]
</code></pre>

<p><strong>Content of endpoint file</strong> (if used &amp; relevant):</p>

<pre><code>action_endpoint:
  url: http://localhost:5055/webhook

#nlg:
#url: http://localhost:5056/nlg

nlu:
    url: http://localhost:5000

core_endpoint:
  url: http://localhost:5056
</code></pre>
",Dataset Preprocessing & Handling,costum action working rasa core rasa core version rasa core sdk version python version operating system window osx issue info run run part second run train online py get cant run time reason wanted run time read github issue might solve main issue custom action action file content endpoint file used relevant
Predicting correct cluster for unseen data using a trained K-Means model,"<p>I know that K-Means is a lazy learner and will have to be retrained from scratch with new points, but still would like to know if there's any workaround to use a trained model to predict on a new unseen data.</p>

<p>I'm using K-Means algorithm to cluster a medical corpus. I'm creating a term-document matrix to represent this corpus. Before feeding the data to kmeans algorithm, I perform truncated singular value decomposition on the data for dimensionality reduction. I've been thinking if there's a way to cluster a new unseen document without retraining the entire model. </p>

<p>To get the vector representation of the new document and predict its cluster using the trained model, I need to ensure that it has the same vocabulary as that of the trained model and also maintains the same order in the term-document matrix. This can be done considering that these documents have a similar kind of vocabulary. But, how do I get SVD representation of this document? Now here's where my understanding gets a little shaky, so correct me if I'm wrong but to perform SVD on this vector representation, I'll need to append it to the original term-document matrix. Now, if I append this new document to original term-document matrix and perform SVD on it to get the vector representation with limited features (100 in this case), then I'm not sure how things will change? Will the new features selected by the SVD correspond semantically to that of the original ones? i.e. it won't make sense to measure the distance of new document from cluster centroids (with 100 features) if the corresponding features grasp different concepts.  </p>

<p>Is there a way to use a trained kmeans model for new text data? Or any other better-suited clustering approach for this task?</p>
",Dataset Preprocessing & Handling,predicting correct cluster unseen data using trained k mean model know k mean lazy learner retrained scratch new point still would like know workaround use trained model predict new unseen data using k mean algorithm cluster medical corpus creating term document matrix represent corpus feeding data kmeans algorithm perform truncated singular value decomposition data dimensionality reduction thinking way cluster new unseen document without retraining entire model get vector representation new document predict cluster using trained model need ensure ha vocabulary trained model also maintains order term document matrix done considering document similar kind vocabulary get svd representation document understanding get little shaky correct wrong perform svd vector representation need append original term document matrix append new document original term document matrix perform svd get vector representation limited feature case sure thing change new feature selected svd correspond semantically original one e make sense measure distance new document cluster centroid feature corresponding feature grasp different concept way use trained kmeans model new text data better suited clustering approach task
Extract text based on character position returned from gregexpr,"<p>I'm working in R, trying to prepare text documents for analysis. Each document is stored in a column (aptly named, ""document"") of dataframe called ""metaDataFrame."" The documents are strings containing articles and their BibTex citation info.  Data frame looks like this: </p>

<pre><code>[1] filename         document                          doc_number
[2] lithuania2016    Commentary highlights Estonian...    1
[3] lithuania2016    Norwegian police, immigration ...    2
[4] lithuania2016    Portugal to deply over 1,000 m...    3
</code></pre>

<p>I want to extract the BibTex information from each document into a new column. The citation information begins with ""Credit:"" but some articles contain multiple ""Credit:"" instances, so I need to extract all of the text after the last instance. Unfortunately, the string is only sometimes preceded by a new line. </p>

<p>My solution so far has been to find all of the instances of the string and save the location of the last instance of ""Credit:"" in each document in a list:</p>

<pre><code>locate.last.credit &lt;- lapply(gregexpr('Credit:', metaDataFrame$document), tail, 1)
</code></pre>

<p>This provides a list of integer locations of the last ""Credit:"" string in each document or a value of ""-1"" where no instance is found. (Those missing values pose a separate but related problem I think I can tackle after resolving this issue). </p>

<p>I've tried variations of strsplit, substr, stri_match_last, and rm_between...but can't figure out a way to use the character position in lieu of regular expression to extract this part of the string.  </p>

<p>How can I use the location of characters to manipulate a string instead of regular expressions? Is there a better approach to this (perhaps with regex)?</p>
",Dataset Preprocessing & Handling,extract text based character position returned gregexpr working r trying prepare text document analysis document stored column aptly named document dataframe called metadataframe document string containing article bibtex citation info data frame look like want extract bibtex information document new column citation information begin credit article contain multiple credit instance need extract text last instance unfortunately string sometimes preceded new line solution far ha find instance string save location last instance credit document list provides list integer location last credit string document value instance found missing value pose separate related problem think tackle resolving issue tried variation strsplit substr stri match last rm figure way use character position lieu regular expression extract part string use location character manipulate string instead regular expression better approach perhaps regex
r text mining extract keywords,"<p>I am new to R and working on a machine learning problem, I understand that machine learning requires labelled data to make accurate prediction.</p>

<p>I am working with text data where the reviews given by users for a particular mobile app in text format. My primary task is to first extract the main keywords(features) </p>

<p>The text data is as follows in the CSV file in the 'review' column</p>

<pre><code>review1  - ""the gps does not work"",
review2  - ""tracking of phone is inconsistent"",
review3  - ""the battery is draining fast"",
review4  - ""the tracks disappear after some time"",
review5  - ""the app consumes the battery lot because of gps""
</code></pre>

<p>now I want to extract the feature mentioned in the each review such as
""gps"", ""tracking"", ""battery"", ""tracks"",""battery gps"" and add it as a label next to it respectively in the CSV file; so there would be one more column created in the CSV file as 'Feature'. 
So my CSV will have 2 columns, one review and one feature column that will highlight the feature mentioned in the review.The snapshot of the data in the CSV will be as follows <a href=""https://i.sstatic.net/H1VGs.jpg"" rel=""nofollow noreferrer"">new csv file data</a></p>

<p>I have written a sample code mentioned below for the same but since I need to deal with thousands of reviews I need to get the Feature column in my csv file which will act as a label for feature prediction</p>

<pre><code>#Feature Prediction
library(tm)
library(e1071)
texts &lt;- c(""the gps does not work"",
           ""tracking of phone is inconsistent"",
           ""the battery is draining fast"",
           ""the tracks disappear after some time"",
           ""the app consumes the battery a lot"")

features &lt;- c(""gps"", ""tracking"", ""battery"", ""tracks"",""battery"")
docs &lt;- VCorpus(VectorSource(texts))
# Clean corpus
docs &lt;- tm_map(docs, content_transformer(tolower))
docs &lt;- tm_map(docs, removeNumbers)
docs &lt;- tm_map(docs, removeWords, stopwords(""english""))
docs &lt;- tm_map(docs, removePunctuation)
docs &lt;- tm_map(docs, stripWhitespace)
dtm &lt;- DocumentTermMatrix(docs)
# Transform dtm to matrix to data frame - df is easier to work with
mat.df &lt;- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df &lt;- cbind(mat.df, features)
View(mat.df)
# Split data by rownumber into two equal portions (Train and Test Data)
train &lt;- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
test &lt;- (1:nrow(mat.df))[- train]
# Isolate classifier
cl &lt;- mat.df[, ""features""]
# Create model data and remove ""features""
modeldata &lt;- mat.df[,!colnames(mat.df) %in% ""features""]
feature_pred &lt;- naiveBayes(modeldata[train,], cl[train])
naiv_pred &lt;- predict(feature_pred, modeldata[test,])
conf.mat &lt;- table(""Predictions"" = naiv_pred, Actual = cl[test])
conf.mat
(accuracy &lt;- sum(diag(conf.mat))/length(test) * 100)
</code></pre>
",Dataset Preprocessing & Handling,r text mining extract keywords new r working machine learning problem understand machine learning requires labelled data make accurate prediction working text data review given user particular mobile app text format primary task first extract main keywords feature text data follows csv file review column want extract feature mentioned review gps tracking battery track battery gps add label next respectively csv file would one column created csv file feature csv column one review one feature column highlight feature mentioned review snapshot data csv follows new csv file data written sample code mentioned since need deal thousand review need get feature column csv file act label feature prediction
How to convert strings to numeric values?,"<p>I am cleaning a csv file on jupyter to do machine learning.
However, several columns have string values, like the column ""description"":</p>

<p><img src=""https://i.sstatic.net/4LSpi.png"" alt=""enter image description here""></p>

<p>I know I need to use NLP to clean, but could not find how to do it on jupyter.
Could you advice me how to convert these values to numeric values?</p>

<p>Thank you</p>
",Dataset Preprocessing & Handling,convert string numeric value cleaning csv file jupyter machine learning however several column string value like column description know need use nlp clean could find jupyter could advice convert value numeric value thank
Efficient calculation of point mutual information in the text corpus in Python,"<p>I have a corpus, in which I calculate the frequency of unigrams and skipgrams, normalize the values by dividing them by the sum of all frequencies, and feed them into pandas data frames. Now, I would like to calculate the point mutual information of each skipgram, which is the log of normalized frequency of skipgram divided by the multiplied normalized frequencies of both unigrams in the skipgram.</p>

<p>My data frames look like this:</p>

<pre><code>unigram_df.head()
              word  count      prob
0          nordisk      1  0.000007
1           lments      1  0.000007
2             four     91  0.000593
3          travaux      1  0.000007
4  cancerestimated      1  0.000007

skipgram_df.head()
                      words  count      prob
0                 (o, odds)      1  0.000002
1  (reported, pretreatment)      1  0.000002
2       (diagnosis, simply)      1  0.000002
3           (compared, sbx)      1  0.000002
4             (imaging, or)      1  0.000002
</code></pre>

<p>For now, I calculate the PMI values of each skipgram, by iterating through each row of skipgram_df, extracting the prob value of the skipgram, extracting prob values of both unigrams, and then calculating the log, and appending the results into the list. </p>

<p>The code looks like this, and it works fine:</p>

<pre><code>for row in skipgram_df.itertuples():
    skipgram_prob = float(row[3])
    x_unigram_prob = float(unigram_df.loc[unigram_df['word'] == str(row[1][0])]['prob'])
    y_unigram_prob = float(unigram_df.loc[unigram_df['word'] == str(row[1][1])]['prob'])
    pmi = math.log10(skipgram_prob/(x_unigram_prob*y_unigram_prob))
    pmi_list.append(pmi)
</code></pre>

<p>The problem is that it takes long to iterate through the whole dataframe (around 30 minutes on 300,000 skipgrams). I will have to work on corpora that are even 10-20 times bigger than that, so I am looking for a more efficient way to do that. Can anyone suggest another solution that will be quicker? Thank you.</p>
",Dataset Preprocessing & Handling,efficient calculation point mutual information text corpus python corpus calculate frequency unigrams skipgrams normalize value dividing sum frequency feed panda data frame would like calculate point mutual information skipgram log normalized frequency skipgram divided multiplied normalized frequency unigrams skipgram data frame look like calculate pmi value skipgram iterating row skipgram df extracting prob value skipgram extracting prob value unigrams calculating log appending result list code look like work fine problem take long iterate whole dataframe around minute skipgrams work corpus even time bigger looking efficient way anyone suggest another solution quicker thank
integrating api into spyder in python,"<p>this is the code for integration of Api( sentimental analysis python).i have taken this code from meaning cloud tool.
here i want to know the what is the value for url and document in payload. </p>

<p>input-set of words like (good,excellent,great,bad)opinion words
output-P,P+,N,N+</p>

<pre><code>import requests

url = ""https://api.meaningcloud.com/sentiment-2.1""

payload = ""key=YOUR_KEY_VALUE&amp;lang=YOUR_LANG_VALUE&amp;txt=YOUR_TXT_VALUE&amp;txtf=plain&amp;url=YOUR_URL_VALUE&amp;doc=YOUR_DOC_VALUE""
headers = {'content-type': 'application/x-www-form-urlencoded'}

response = requests.request(""POST"", url, data=payload, headers=headers)

print(response.text)
</code></pre>
",Dataset Preprocessing & Handling,integrating api spyder python code integration api sentimental analysis python taken code meaning cloud tool want know value url document payload input set word like good excellent great bad opinion word output p p n n
Creating Data Frame form a loop,"<p>I need to create a dataframe from a loop. the idea is that the loop will read a data frame of texts (train_vs) and search for specific key words ['govern', 'data'] and then calculate their frequency or TF. what I want is an outcome of two columns with the TF of the words for each text inside them. the code I am using is the following:</p>

<pre><code>d = pd.DataFrame()
key = ['govern', 'data']
for k in key:
    for w in range(0, len(train_vs)):
        wordcount = Counter(train_vs['doc_text'].iloc[w])
        a_vs = (wordcount[k]/len(train_v.iloc[w])*1)
        temp = pd.DataFrame([{k: a_vs}] )
        d = pd.concat([d, temp])
</code></pre>

<p>however, I am getting two columns but with values for the first key word and nan for second for the whole texts column and then nan for the first and values for the second again for the whole texts column. so the number of the rows of the outcome dataframe is double.</p>

<p>I want to have both values next to each other.<br>
Your help is highly appreciated.<br>
Thanks. </p>
",Dataset Preprocessing & Handling,creating data frame form loop need create dataframe loop idea loop read data frame text train v search specific key word govern data calculate frequency tf want outcome two column tf word text inside code using following however getting two column value first key word nan second whole text column nan first value second whole text column number row outcome dataframe double want value next help highly appreciated thanks
Clustering words into groups,"<p>This is a Homework question. I have a huge document full of words. My challenge is to classify these words into different groups/clusters that adequately represent the words. My strategy to deal with it is using the K-Means algorithm, which as you know takes the following steps.</p>

<ol>
<li>Generate k random means for the entire group</li>
<li>Create K clusters by associating each word with the nearest mean</li>
<li>Compute centroid of each cluster, which becomes the new mean</li>
<li>Repeat Step 2 and Step 3 until a certain benchmark/convergence has been reached.</li>
</ol>

<p>Theoretically, I kind of get it, but not quite. I think at each step, I have questions that correspond to it, these are:</p>

<ol>
<li><p>How do I decide on k random means, technically I could say 5, but that may not necessarily be a good random number. So is this k purely a random number or is it actually driven by heuristics such as size of the dataset, number of words involved etc</p></li>
<li><p>How do you associate each word with the nearest mean? Theoretically I can conclude that each word is associated by its distance to the nearest mean, hence if there are 3 means, any word that belongs to a specific cluster is dependent on which mean it has the shortest distance to. However, how is this actually computed? Between two words ""group"", ""textword"" and assume a mean word ""pencil"", how do I create a similarity matrix.</p></li>
<li><p>How do you calculate the centroid?</p></li>
<li><p>When you repeat step 2 and step 3, you are assuming each previous cluster as a new data set?</p></li>
</ol>

<p>Lots of questions, and I am obviously not clear. If there are any resources that I can read from, it would be great. Wikipedia did not suffice :( </p>
",Dataset Preprocessing & Handling,clustering word group homework question huge document full word challenge classify word different group cluster adequately represent word strategy deal using k mean algorithm know take following step generate k random mean entire group create k cluster associating word nearest mean compute centroid cluster becomes new mean repeat step step certain benchmark convergence ha reached theoretically kind get quite think step question correspond decide k random mean technically could say may necessarily good random number k purely random number actually driven heuristic size dataset number word involved etc associate word nearest mean theoretically conclude word associated distance nearest mean hence mean word belongs specific cluster dependent mean ha shortest distance however actually computed two word group textword assume mean word pencil create similarity matrix calculate centroid repeat step step assuming previous cluster new data set lot question obviously clear resource read would great wikipedia suffice
How to solve - IOERROR : Fatal Error: File &#39;path/to/file&#39; could not be located or is not readable,"<p>I have been trying to open a file by passing it as an argument. 
I have been providing the correct file path and the file is also readable.</p>

<p>I created another script in order to determine whether the file is readable or not. I got the contents of the text file which shows that the file is readable and the path is also correct.</p>

<p>here is the piece trying to read the file - </p>

<pre><code>try:
    with open(args.filepath, ""r"") as file:
        data = file.read()

except IOError:
    print(""Fatal Error: File ""+args.filepath+"" could not be located or is not readable."")
    exit()
</code></pre>

<p>The file should be readable in the first place.</p>
",Dataset Preprocessing & Handling,solve ioerror fatal error file path file could located readable trying open file passing argument providing correct file path file also readable created another script order determine whether file readable got content text file show file readable path also correct piece trying read file file readable first place
How can we use artificial neural networks to find similar documents?,"<p>How can we use ANN to find some similar documents? I know its a silly question, but I am new to this NLP field.
I have made a model using kNN and bag-of-words approach to solve my problem. Using that I can get n number of documents (along with their closeness) that are somewhat similar to the input, but now I want to implement the same using ANN and I am not getting any idea.</p>

<p>Thanks in advance for any help or suggestions.</p>
",Dataset Preprocessing & Handling,use artificial neural network find similar document use ann find similar document know silly question new nlp field made model using knn bag word approach solve problem using get n number document along closeness somewhat similar input want implement using ann getting idea thanks advance help suggestion
How to write an @overrides allennlp predictor.load_line?,"<p>AllenNLP has a predictor function that takes in a JSON file then output a JSON file. From the documentation one can @overrides predictor.load_line and write in a function to take in, say, a text file. </p>

<p>How would you write this function? And how to implement the function (ie import it as a module)?</p>

<p>AllenNLP load_line API: <a href=""https://allenai.github.io/allennlp-docs/api/allennlp.predictors.html?highlight=sentencetaggerpredictor#allennlp.predictors.predictor.Predictor"" rel=""nofollow noreferrer"">https://allenai.github.io/allennlp-docs/api/allennlp.predictors.html?highlight=sentencetaggerpredictor#allennlp.predictors.predictor.Predictor</a></p>

<p>I am following the tutorial here:
<a href=""https://github.com/allenai/allennlp/blob/master/tutorials/tagger/README.md"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp/blob/master/tutorials/tagger/README.md</a></p>
",Dataset Preprocessing & Handling,write override allennlp predictor load line allennlp ha predictor function take json file output json file documentation one override predictor load line write function take say text file would write function implement function ie import module allennlp load line api following tutorial
pandas reading csv with large texts fpr nlp,"<p>I have a large csv file made of 1 column and with a single cell in each row, around 18800 of them. Each cell contains a large text. The file size is big; around 318 Mb. I am using Jupyter notebooks.</p>

<p>I am trying to read the csv file into a pandas dataframe, however I am receiving the following error:</p>

<p><code>ParserError: Error tokenizing data. C error: Expected 1 fields in line 12, saw 2</code></p>

<p>I tried some tricks, however, I am not getting the original text or even the number of rows. Instead I am getting something like 200 rows. </p>
",Dataset Preprocessing & Handling,panda reading csv large text fpr nlp large csv file made column single cell row around cell contains large text file size big around mb using jupyter notebook trying read csv file panda dataframe however receiving following error tried trick however getting original text even number row instead getting something like row
Second-order cooccurrence of terms in texts,"<p>Basically, <a href=""https://www.youtube.com/watch?v=sI7VpFNiy_I&amp;t=20m"" rel=""nofollow noreferrer"">I want to reimplement this video</a>. </p>

<p>Given a corpus of documents, I want to find the terms that are most similar to each other. </p>

<p>I was able to generate a cooccurrence matrix using <a href=""https://stackoverflow.com/a/37822989"">this SO thread</a> and use the video to generate an association matrix. Next I, would like to generate a second order cooccurrence matrix.</p>

<p>Problem statement: Consider a matrix where the rows of the matrix correspond to a term and the entries in the rows correspond to the top k terms similar to that term. Say, k = 4, and we have n terms in our dictionary, then the matrix <code>M</code> has <code>n</code> rows and <code>4</code> columns.</p>

<p>HAVE: </p>

<pre><code>M = [[18,34,54,65],   # Term IDs similar to Term t_0
     [18,12,54,65],   # Term IDs similar to Term t_1
     ...
     [21,43,55,78]]   # Term IDs similar to Term t_n.
</code></pre>

<p>So, M contains for each term ID, the most similar term IDs. Now, I would like to check how many of those similar terms match. In the example of <code>M</code> above, it seems that term <code>t_0</code> and term <code>t_1</code> are quite similar, because three out of four terms match, where as terms <code>t_0</code> and <code>t_n</code>are not similar, because no terms match. Let's write <code>M</code> as a series of lists.</p>

<pre><code>M = [list_0,   # Term IDs similar to Term t_0
     list_1,   # Term IDs similar to Term t_1
     ...
     list_n]   # Term IDs similar to Term t_n.
</code></pre>

<p>WANT:</p>

<pre><code>C = [[f(list_0, list_0), f(list_0, list_1), ..., f(list_0, list_n)],
     [f(list_1, list_0), f(list_1, list_1), ..., f(list_1, list_n)],
     ...
     [f(list_n, list_0), f(list_n, list_1), ..., f(list_n, list_n)]]
</code></pre>

<p>I'd like to find the matrix <code>C</code>, that has as its elements, a function <code>f</code> applied to the lists of <code>M</code>. <code>f(a,b)</code> measures the degree of similarity between two lists <code>a</code> and <code>b</code>. Going, with the example above, the degree of similarity between <code>t_0</code> and <code>t_1</code> should be high, whereas the degree of similarity of <code>t_0</code> and <code>t_n</code> should be low. </p>

<p>My questions:   </p>

<ol>
<li>What is a good choice for comparing the ordering of two lists? That is, what is a good choice for function <code>f</code>?   </li>
<li>Is there a transformation already available that takes as an input a matrix like <code>M</code> and produces a matrix like <code>C</code>? Preferably a python package?</li>
</ol>

<p>Thank you, r0f1</p>
",Dataset Preprocessing & Handling,second order cooccurrence term text basically want reimplement video given corpus document want find term similar wa able generate cooccurrence matrix using href thread use video generate association matrix next would like generate second order cooccurrence matrix problem statement consider matrix row matrix correspond term entry row correspond top k term similar term say k n term dictionary matrix ha row column contains term id similar term id would like check many similar term match example seems term term quite similar three four term match term similar term match let write series list want like find matrix ha element function applied list measure degree similarity two list going example degree similarity high whereas degree similarity low question good choice comparing ordering two list good choice function transformation already available take input matrix like produce matrix like preferably python package thank r f
How to filter out non-English data from csv using pandas,"<p>I'm currently writing a code to extract frequently used words from my csv file, and it works just fine until I get a barplot of strange words listed. I don't know why, probably because there are some foreign words involved. However, I don't know how to fix this. </p>

<pre><code>import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.feature_extraction.text import CountVectorizer, 
TfidfVectorizer
from sklearn.model_selection import train_test_split, KFold
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
import matplotlib
from matplotlib import pyplot as plt
import sys
sys.setrecursionlimit(100000)
# import seaborn as sns
%matplotlib inline
%config InlineBackend.figure_format = 'retina'

data = pd.read_csv(""C:\\Users\\Administrator\\Desktop\\nlp_dataset\\commitment.csv"", encoding='cp1252',na_values="" NaN"")

data.shape
data['text'] = data.fillna({'text':'none'})
def remove_punctuation(text):
    '' 'a function for removing punctuation'''
    import string
    #replacing the punctuations with no space,
    #which in effect deletes the punctuation marks
    translator = str.maketrans('', '', string.punctuation)
    #return the text stripped of punctuation marks
    return text.translate(translator)

#Apply the function to each examples 
data['text'] = data['text'].apply(remove_punctuation)
data.head(10)

#Removing stopwords -- extract the stopwords
#extracting the stopwords from nltk library
sw= stopwords.words('english')
#displaying the stopwords
np.array(sw)

# function to remove stopwords
def stopwords(text):
    '''a function for removing stopwords'''
        #removing the stop words and lowercasing the selected words
        text = [word.lower() for word in text.split()  if word.lower() not in sw]
        #joining the list of words with space separator
        return  "" "". join(text)

# Apply the function to each examples
data['text'] = data ['text'].apply(stopwords)
data.head(10)

# Top words before stemming  
# create a count vectorizer object
count_vectorizer = CountVectorizer()
# fit the count vectorizer using the text dta
count_vectorizer.fit(data['text'])
# collect the vocabulary items used in the vectorizer
dictionary = count_vectorizer.vocabulary_.items() 

#store the vocab and counts in a pandas dataframe
vocab = []
count = []
#iterate through each vocav and count append the value to designated lists
for key, value in dictionary:
 vocab.append(key)
 count.append(value)
#store the count in pandas dataframe with vocab as indedx
vocab_bef_stem = pd.Series(count, index=vocab)
#sort the dataframe
vocab_bef_stem = vocab_bef_stem.sort_values(ascending = False)

# Bar plot of top words before stemming
top_vocab = vocab_bef_stem.head(20)
top_vocab.plot(kind = 'barh', figsize=(5,10), xlim = (1000, 5000))
</code></pre>

<p>I want a list of frequent words ordered in a bar-plot, but for now it just gives non-English words with all-same frequency. Please help me out </p>
",Dataset Preprocessing & Handling,filter non english data csv using panda currently writing code extract frequently used word csv file work fine get barplot strange word listed know probably foreign word involved however know fix want list frequent word ordered bar plot give non english word frequency please help
"Python Pandas to PySpark: How to tokenize, remove stopgap words, and do trigrams in PySpark","<p>I have the following sample data frame below.  I execute Python Pandas code in my Jupyter Notebook.   </p>

<pre><code>No  category    problem_definition
175 2521       ['coffee', 'maker', 'brewing', 'properly', '2', '420', '420', '420']
211 1438       ['galley', 'work', 'table', 'stuck']
912 2698       ['cloth', 'stuck']
572 2521       ['stuck', 'coffee']
</code></pre>

<p>I used the code below to tokenize my text column: </p>

<pre><code>from nltk.tokenize import sent_tokenize, word_tokenize 
import pandas as pd 
import re 

df['problem_definition_tokenized'] = df['problem_definition'].apply(word_tokenize)
</code></pre>

<p>I use the code below to remove stopgap words: </p>

<pre><code>set(stopwords.words('english'))

stop_words = set(stopwords.words('english'))

df['problem_definition_stopwords'] = df['problem_definition_tokenized'].apply(lambda x: [i for i in x if i not in stop_words]) 
</code></pre>

<p>Next, I calculated trigrams using collocations package.  </p>

<pre><code>import nltk
from nltk.collocations import *

bigram_measures = nltk.collocations.BigramAssocMeasures()
trigram_measures = nltk.collocations.TrigramAssocMeasures()

finder = BigramCollocationFinder.from_documents(df['problem_definition_stopwords'])

finder.apply_freq_filter(8) 

finder.nbest(trigram_measures.pmi, 100) 

s = pd.Series(df['problem_definition_stopwords'])

from nltk import ngrams
from collections import Counter

ngram_list = [pair for row in s for pair in ngrams(row, 3)]

counts = Counter(ngram_list).most_common()

df = pd.DataFrame.from_records(counts, columns=['gram', 'count'])

df
</code></pre>

<p>The result looks like this ... ""xxx"" stands for a word</p>

<pre><code>gram               count 
(xxx, xxx, xxx)    23
(xxx, xxx, xxx)    14
(xxx, xxx, xxx)    63
(xxx, xxx, xxx)    28
</code></pre>

<p>I can get all the above code running in Pandas Python but when I try to run this code in a PySpark environment it just keeps spinning.  </p>

<p>Is there a way to transpose the code I have written into PySpark code?  I have googled around but couldn't find anything definitive.  </p>
",Dataset Preprocessing & Handling,python panda pyspark tokenize remove stopgap word trigram pyspark following sample data frame execute python panda code jupyter notebook used code tokenize text column use code remove stopgap word next calculated trigram using collocation package result look like xxx stand word get code running panda python try run code pyspark environment keep spinning way transpose code written pyspark code googled around find anything definitive
Creating a document-feature matrix from list of extracted phrases after using phrasemachine (R),"<p>I have a nested list with phrases after applying <code>phrasemachine()</code>. Now I would like to create a document-feature matrix having the documents (user) in the first column and all features as the remaining columns with each user's frequency of usage in the cells. </p>

<pre><code>library(rJava)
library(phrasemachine)
library(quanteda)

#creating dummy data
id &lt;- c(1:2)
text &lt;- c(""Election day is coming up and I am super excited. Election day. Wooho. I voted President Obama."", ""School is boring. Partying is cool. Happy Birthday to me. When is Election Day?"")
test &lt;- data.frame(id, text)
test$text &lt;- as.character(test$text)

corpus_test &lt;- corpus(test[[""text""]], docnames = test[[""id""]])
tokens_test &lt;- tokens(corpus_test)
phrases_test &lt;- phrasemachine(tokens_test, minimum_ngram_length = 2, maximum_ngram_length = 3, return_phrase_vectors = TRUE, return_tag_sequences = TRUE)
phrases_test

# &gt; phrases_test
# [[1]]
# [[1]]$phrases
# [1] ""Election_day""    ""Election_day""    ""President_Obama""
# 
# [[1]]$tags
# [1] ""NN"" ""NN"" ""NN""
# 
# 
# [[2]]
# [[2]]$phrases
# [1] ""Happy_Birthday"" ""Election_Day""  
# 
# [[2]]$tags
# [1] ""AN"" ""NN""
</code></pre>

<p>This is the output I am looking for (a document-feature matrix):</p>

<pre><code># user    Election_day    President_Obama   Happy_Birthday
# 1       2               1                 0
# 2       1               0                 1 
</code></pre>

<p>I tried using <code>lapply</code> but since each user's phrases are of different dimensions, that wouldn't work. </p>

<p>Here is what I tried:</p>

<pre><code>library(plyr)
phrases_user &lt;- laply(phrases_test, function(x) laply(x, identity)) #Error: Results must have the same dimensions.

library(dplyr)
phrases_user &lt;- lapply(phrases_test, `[`, ""phrases"")
</code></pre>

<p>After figuring out the issue in extracting the phrases per Id, I suppose I would have to do the following:</p>

<pre><code>corpus_test_2 &lt;- corpus(phrases_user[[""phrases""]], docnames = phrases_user[[""id""]])
dfm_test &lt;- dfm(corpus_test_2)
</code></pre>

<p>Can anyone help? :)</p>
",Dataset Preprocessing & Handling,creating document feature matrix list extracted phrase using phrasemachine r nested list phrase applying would like create document feature matrix document user first column feature remaining column user frequency usage cell output looking document feature matrix tried using since user phrase different dimension work tried figuring issue extracting phrase per id suppose would following anyone help
R tibble all the coocuring words of the string - coocurences - bigram - dplyr,"<p>I have a Data Frame of this format:</p>

<pre><code>df &lt;- data.frame(names= c('perform data cleansing','information categorisation', ''))
                             names
1           perform data cleansing
2       information categorisation
3 write batch record documentation
</code></pre>

<p>And I would like to obtain this one with all the coocurences:</p>

<pre><code>                             names           tokens1              tokens2
1           perform data cleansing           perform                 data
1           perform data cleansing              data            cleansing 
1           perform data cleansing         cleansing              perform
2       information categorisation       information       categorisation
3 write batch record documentation             write                batch
3 write batch record documentation             write               record
3 write batch record documentation             write        documentation 
3 write batch record documentation             batch               record 
3 write batch record documentation             batch        documentation 
3 write batch record documentation            record        documentation 
</code></pre>

<p>so, for <code>n</code> words in a string, you'll have <code>n x (n-1) / 2</code> coocurencies.</p>
",Dataset Preprocessing & Handling,r tibble coocuring word string coocurences bigram dplyr data frame format would like obtain one coocurences word string coocurencies
How do I use Conll 2003 corpus in python crfsuite,"<p>I have downloaded Conll 2003 corpus (""eng.train""). I want to use it to extract entity using python crfsuite training. But I don't know how to load this file for training.</p>

<p>I found this example, but it is not for English.</p>

<pre><code>train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))
test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))
</code></pre>

<p>Also in future I would like to train new entities other than POS or location. How can I add those.</p>

<p>Also please suggest how to handle multiple words.</p>
",Dataset Preprocessing & Handling,use conll corpus python crfsuite downloaded conll corpus eng train want use extract entity using python crfsuite training know load file training found example english also future would like train new entity po location add also please suggest handle multiple word
Extract only body text from arXiv articles formatted as .tex,"<p>My dataset is composed of arXiv astrophysics articles as .tex files, and I need to extract only text from the article body, not from any other part of the article (e.g. tables, figures, abstract, title, footnotes, acknowledgements, citations, etc.). </p>

<p>I've been trying with Python3 and <a href=""https://drive.google.com/file/d/1tv44UbdA2_pAQqI6iL8-h46kXabkTZKR/view?usp=sharing"" rel=""noreferrer"">tex2py</a>, but I'm struggling with getting a clean corpus, because the files differ in labeling &amp; the text is broken up between labels. </p>

<p>I have attached a SSCCE, a couple sample Latex files and their pdfs, and the parsed corpus. The corpus shows my struggles: Sections and subsections are not extracted in order, text breaks at some labels, and some tables and figures are included.</p>

<p>Code: </p>

<pre><code>import os
from tex2py import tex2py

corpus = open('corpus2.tex', 'a')

def parseFiles():
    """"""
    Parses downloaded document .tex files for word content.
    We are only interested in the article body, defined by /section tags.
    """"""

    for file in os.listdir(""latex""):
        if file.endswith('.tex'):
            print('\nChecking ' + file + '...')
            with open(""latex/"" + file) as f:
                try:
                    toc = tex2py(f) # toc = tree of contents
                    # If file is a document, defined as having \begin{document}
                    if toc.source.document:
                        # Iterate over each section in document
                        for section in toc:
                            # Parse the section
                            getText(section)
                    else:
                        print(file + ' is not a document. Discarded.')
                except (EOFError, TypeError, UnicodeDecodeError): 
                    print('Error: ' + file + ' was not correctly formatted. Discarded.')



def getText(section):
    """"""
    Extracts text from given ""section"" node and any nested ""subsection"" nodes. 

    Parameters
    ----------
    section : list
        A ""section"" node in a .tex document 
    """"""

    # For each element within the section 
    for x in section:
        if hasattr(x.source, 'name'):
            # If it is a subsection or subsubsection, parse it
            if x.source.name == 'subsection' or x.source.name == 'subsubsection':
                corpus.write('\nSUBSECTION!!!!!!!!!!!!!\n')
                getText(x)
            # Avoid parsing past these sections
            elif x.source.name == 'acknowledgements' or x.source.name == 'appendix':
                return
        # If element is text, add it to corpus
        elif isinstance(x.source, str):
            # If element is inline math, worry about it later
            if x.source.startswith('$') and x.source.endswith('$'):
                continue
            corpus.write(str(x))
        # If element is 'RArg' labelled, e.g. \em for italic, add it to corpus
        elif type(x.source).__name__ is 'RArg':
            corpus.write(str(x.source))


if __name__ == '__main__':
    """"""Runs if script called on command line""""""
    parseFiles()
</code></pre>

<p>Links to the rest:</p>

<ul>
<li><a href=""https://drive.google.com/file/d/1tv44UbdA2_pAQqI6iL8-h46kXabkTZKR/view?usp=sharing"" rel=""noreferrer"">Sample .tex file 1</a> and its <a href=""https://drive.google.com/file/d/13XjQ-NbENqrbkgdAL3QyhWC-BbNmWAdl/view?usp=sharing"" rel=""noreferrer"">pdf</a></li>
<li><a href=""https://drive.google.com/file/d/1iCAIuLQtw7wj8J2BPfvvhywWyq4Sey6s/view?usp=sharing"" rel=""noreferrer"">Sample .tex file 2</a> and its <a href=""https://drive.google.com/file/d/1B1e_Ptet1tjMRiZofGanQgNH4Fc9uinL/view?usp=sharing"" rel=""noreferrer"">pdf</a></li>
<li>Resulting <a href=""https://drive.google.com/file/d/1KN5yEGiEh494DS-FQPtPI_oBAz0FiOQz/view?usp=sharing"" rel=""noreferrer"">corpus</a></li>
</ul>

<p>I'm aware of a related question (<a href=""https://stackoverflow.com/questions/4792065/programmatically-converting-parsing-latex-code-to-plain-text?rq=1"">Programatically converting/parsing latex code to plain text</a>), but there seems not to be a conclusive answer. </p>
",Dataset Preprocessing & Handling,extract body text arxiv article formatted tex dataset composed arxiv astrophysics article tex file need extract text article body part article e g table figure abstract title footnote acknowledgement citation etc trying python tex py struggling getting clean corpus file differ labeling text broken label attached sscce couple sample latex file pdfs parsed corpus corpus show struggle section subsection extracted order text break label table figure included code link rest sample tex file pdf sample tex file pdf resulting corpus aware related question href converting parsing latex code plain text seems conclusive answer
ValueError: Cannot create a tensor proto whose content is larger than 2GB,"<p>I am using this code to train news article dataset. </p>

<p><a href=""https://github.com/borislavmavrin/stance-detection/blob/master/model_matchingLSTM_wdev.py"" rel=""nofollow noreferrer"">https://github.com/borislavmavrin/stance-detection/blob/master/model_matchingLSTM_wdev.py</a></p>

<p>When I load GoogleNews word2vec file, it gives me error. </p>

<blockquote>
  <p>ValueError: Cannot create a tensor proto whose content is larger than 2GB.</p>
</blockquote>

<p>The stacktrace starts from line <a href=""https://github.com/borislavmavrin/stance-detection/blob/master/model_matchingLSTM_wdev.py#L614"" rel=""nofollow noreferrer"">https://github.com/borislavmavrin/stance-detection/blob/master/model_matchingLSTM_wdev.py#L614</a>, </p>

<p>and then goes to <a href=""https://github.com/borislavmavrin/stance-detection/blob/master/model_matchingLSTM_wdev.py#L154"" rel=""nofollow noreferrer"">https://github.com/borislavmavrin/stance-detection/blob/master/model_matchingLSTM_wdev.py#L154</a> </p>

<p>Any help here would be appreciated. I don't want to change the structure of this code right now, I am just focused more on results for now as this is just a prototype I want to do on this dataset. If the results are good enough, I might write my own model or improve the existing one. </p>
",Dataset Preprocessing & Handling,valueerror create tensor proto whose content larger gb using code train news article dataset load googlenews word vec file give error valueerror create tensor proto whose content larger gb stacktrace start line go help would appreciated want change structure code right focused result prototype want dataset result good enough might write model improve existing one
How to load an aligned word2vec model in Gensim?,"<p>I have a time series dataset. therefore, for each time period I trained a word2vec model and realigned the models.</p>

<p>However, when I try to load the aligned word2vec models as follows I get the below mentioned error.</p>

<pre><code>#Load model
model = word2vec.Word2Vec.load('model_1970')
</code></pre>

<p>Error:</p>

<pre><code>train_words_pow += wv.vocab[wv.index2word[word_index]].count**power
KeyError: 'ctrx'
</code></pre>

<p>Is there a way to resolve this error? :)</p>

<p>I have attached a sample trained word2vec model that gives error for testing purposes</p>

<p>Link: <a href=""https://drive.google.com/file/d/1IBbUgeAubr2xzNYLKZgPt34xOEsW92bO/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1IBbUgeAubr2xzNYLKZgPt34xOEsW92bO/view?usp=sharing</a></p>

<p>EDIT:
Mention below is the log of my program.</p>

<pre><code>2018-11-30 14:23:43,897 : INFO : loading Word2Vec object from model_1970
2018-11-30 14:23:43,961 : INFO : loading wv recursively from model_1970.wv.* with mmap=None
2018-11-30 14:23:43,965 : INFO : loading vectors from model_1970.wv.vectors.npy with mmap=None
2018-11-30 14:23:44,005 : INFO : setting ignored attribute vectors_norm to None
2018-11-30 14:23:44,009 : INFO : loading vocabulary recursively from model_1970.vocabulary.* with mmap=None
2018-11-30 14:23:44,009 : INFO : loading trainables recursively from model_1970.trainables.* with mmap=None
2018-11-30 14:23:44,009 : INFO : loading syn1neg from model_1970.trainables.syn1neg.npy with mmap=None
2018-11-30 14:23:44,053 : INFO : setting ignored attribute cum_table to None
2018-11-30 14:23:44,053 : INFO : loaded model_1970
Reloaded modules: __mp_main__
Traceback (most recent call last):

  File ""&lt;ipython-input-3-3b9230dacba9&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/Emi/Desktop/code/word2vec_distance.py', wdir='C:/Users/Emi/Desktop/code')

  File ""C:\Users\Emi\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 668, in runfile
    execfile(filename, namespace)

  File ""C:\Users\Emi\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 108, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/Emi/Desktop/code/word2vec_distance.py"", line 26, in &lt;module&gt;
    model_1 = word2vec.Word2Vec.load(word2vec_model_name_1)

  File ""C:\Users\Emi\Anaconda3\lib\site-packages\gensim\models\word2vec.py"", line 975, in load
    return super(Word2Vec, cls).load(*args, **kwargs)

  File ""C:\Users\Emi\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py"", line 631, in load
    model.vocabulary.make_cum_table(model.wv)  # rebuild cum_table from vocabulary

  File ""C:\Users\Emi\Anaconda3\lib\site-packages\gensim\models\word2vec.py"", line 1383, in make_cum_table
    train_words_pow += wv.vocab[wv.index2word[word_index]].count**power

KeyError: 'cmnx'
</code></pre>
",Dataset Preprocessing & Handling,load aligned word vec model gensim time series dataset therefore time period trained word vec model realigned model however try load aligned word vec model follows get mentioned error error way resolve error attached sample trained word vec model give error testing purpose link edit mention log program
Finding cosine similarity of documents and their removal from R dataframe,"<p>I am working on the data frame which contains data per row doc number and text only. This data was exported from  xml file. The data is of form dataframe in variable <code>text_df</code> :</p>

<p>line/ text<br>
       </p>

<pre><code> 1 when uploading objective file bugzilla se
 2 spelling mistake docs section searching fo…
 3 editparams cgi won save updates iis instal…
 4 editparams cgi won save updates            
 5 rfe unsubscribe from bug you reported      
 6 unsubscribe from bug you reported  
</code></pre>

<p>I am using the following code to identify and remove the duplicates. </p>

<pre><code>doc_set_1 = text_df
it1 = itoken(doc_set_1$text, progressbar = FALSE)

# specially take different number of docs in second set
doc_set_2 = text_df
it2 = itoken(doc_set_2$text, progressbar = FALSE)
it = itoken(text_df$text, progressbar = FALSE)
 v = create_vocabulary(it) %&gt;% prune_vocabulary(doc_proportion_max = 
 0.1, term_count_min = 5)
 vectorizer = vocab_vectorizer(v)
 dtm1 = create_dtm(it1, vectorizer)
 dtm2 = create_dtm(it2, vectorizer)
 d1_d2_cos_sim = sim2(dtm1, dtm2, method = ""cosine"", norm = ""l2"")
  mat&lt;-(d1_d2_cos_sim)
  mat[lower.tri(mat,diag=TRUE)] &lt;- 0
  ## for converting a sparse matrix into dataframe
  mdf&lt;- as.data.frame(as.matrix(mat))
  datalist = list()
  for (i in 1:nrow(mat)) {
   t&lt;-which(mat[i,]&gt;0.8)
   if(length(t)&gt;1){
   datalist[[i]] &lt;- t # add it to your list
      }
    }

  #Number of Duplicates Found
  length(unique(unlist(datalist)))

   tmdf&lt;- subset(mdf,select=-c(unique(unlist(datalist))))

  # Removing the similar documents
  text_df&lt;-text_df[names(tmdf),]
  nrow(text_df)
</code></pre>

<p>This code takes lot of time for solving, Any suggestions to make it better are welcome. </p>
",Dataset Preprocessing & Handling,finding cosine similarity document removal r dataframe working data frame contains data per row doc number text data wa exported xml file data form dataframe variable line text using following code identify remove duplicate code take lot time solving suggestion make better welcome
Python / Read and group data from text file with Pandas,"<p>I have a text file as follows:</p>

<pre><code>Sentence:1 Polarity:N 5puan verdim o da anistonun güzel yüzünün hatırına.
Sentence:2 Polarity:N son derece sıkıcı bir filim olduğunu söyleyebilirim.
Sentence:3 Polarity:N ..saçma bir konuyu nasılda filim yapmışlar maşallah
Sentence:4 Polarity:P bence hoş vakit geçirmek için seyredilebilir.
Sentence:5 Polarity:P hoş ve sevimli bir film.
Sentence:6 Polarity:O eşcinsellere pek sempati duymamakla beraber bu filmde sanki onları sevimli göstermeye çalışmışlar gibi geldi.
Sentence:7 Polarity:O itici bir film değildi sonuçta.
Sentence:8 Polarity:N seyrederken bu kadar sinirlendiğim film hatırlamıyorum.
Sentence:9 Polarity:O  J.Aniston ın hiç mi umut yok diye sorduğu sahnede kıracaktım televizyonu!
Sentence:10 Polarity:O kimse yazmamış ben yazıyım:)
Sentence:11 Polarity:P  güzel bi pazar günü şirin bi film izlemek isteyenler için çok güzel.
</code></pre>

<p>I want to split this data in to a table like this:</p>

<pre><code>Sentence_No - Sentence_Polarity - Sentence_txt
1 - N - 5puan verdim o da anistonun güzel yüzünün hatırına.
2 - N - son derece sıkıcı bir filim olduğunu söyleyebilirim.
3 - N - ..saçma bir konuyu nasılda filim yapmışlar maşallah
4 - P - bence hoş vakit geçirmek için seyredilebilir.
</code></pre>

<p>So I think I need to get the part from after ""Sentence:"", ""Polarity"" and the last txt part. I want it this way so I can classify the data.</p>

<p>I wrote the code below but it is not working for this purpose:</p>

<pre><code>df = pd.read_csv('SU-Movie-Reviews-Sentences.txt', lineterminator='\n', names=['Sentence_No', 'Sentence_Polarity' , 'Sentence_txt'])
</code></pre>
",Dataset Preprocessing & Handling,python read group data text file panda text file follows want split data table like think need get part sentence polarity last txt part want way classify data wrote code working purpose
Splitting and grouping plain text (grouping text by chapter in dataframe)?,"<p>I have a data frame/tibble where I've imported a file of plain text (txt). The text very consistent and is grouped by chapter. Sometimes the chapter text is only one row, sometimes it's multiple row. Data is in one column like this:</p>

<pre><code># A tibble: 10,708 x 1
   x                                                                     
   &lt;chr&gt;                                                                                                                                   
 1 ""Chapter 1 ""                                                          
 2 ""Chapter text. ""     
 3 ""Chapter 2 ""                                                          
 4 ""Chapter text. ""    
 5 ""Chapter 3 ""
 6 ""Chapter text. ""
 7 ""Chapter text. ""
 8 ""Chapter 4 ""   
</code></pre>

<p>I'm trying to clean the data to have a new column for Chapter and the text from each chapter in another column, like this:</p>

<pre><code># A tibble: 10,548 x 2
   x                                Chapter   
   &lt;chr&gt;                             &lt;chr&gt;
 1 ""Chapter text. ""               ""Chapter 1 ""
 2 ""Chapter text. ""               ""Chapter 2 ""
 3 ""Chapter text. ""               ""Chapter 3 "" 
 4 ""Chapter text. ""               ""Chapter 4 "" 
</code></pre>

<p>I've been trying to use regex to split the and group the data at each occurance of the word 'Chapter #' (chapter followed by a number, but cannot get the result I want. Any advice is much appreciated.</p>
",Dataset Preprocessing & Handling,splitting grouping plain text grouping text chapter dataframe data frame tibble imported file plain text txt text consistent grouped chapter sometimes chapter text one row sometimes multiple row data one column like trying clean data new column chapter text chapter another column like trying use regex split group data occurance word chapter chapter followed number get result want advice much appreciated
Parsing Index page in a PDF text book with Python,"<p>I have to extract text from PDF pages as it is with the indentation into a CSV file.</p>

<p>Index page from PDF text book:
<img src=""https://i.sstatic.net/WNdtb.jpg"" alt=""""></p>

<p>I should split the text into class and subclass type hierarchy along with the page numbers. For example in the image,
<strong><em>Application server</em></strong> is the class and <strong><em>Apache Tomcat</em></strong> is the subclass in the page number <strong>275</strong></p>

<p>This is the  expected output of the CSV:
<img src=""https://i.sstatic.net/3HGnL.jpg"" alt=""""></p>

<p>I have used Tika parser to parse the PDF, but the indentation is not maintained properly (not unique) in the parsed content for splitting the text into class and subclasses.</p>

<p>This is how the parsed text looks like:</p>

<p><img src=""https://i.sstatic.net/tsNV9.jpg"" alt=""""></p>

<p>Can anyone suggest me the right approach for this requirement?</p>
",Dataset Preprocessing & Handling,parsing index page pdf text book python extract text pdf page indentation csv file index page pdf text book split text class subclass type hierarchy along page number example image application server class apache tomcat subclass page number expected output csv used tika parser parse pdf indentation maintained properly unique parsed content splitting text class subclass parsed text look like anyone suggest right approach requirement
Create a Heatmap for findAssocs results based on time,"<p>I have the following data for which I create a document-term matrix first and then I add the time variable to the dtm. In order to see the correlations to the term ""updat"" in the different years, I would like to have a heat-map for findAssoc in a way that x-axis shows the time. </p>

<pre><code>library(tm) 
library(ggplot2)

df &lt;- structure(list(Description = structure(c(5L, 8L, 6L, 4L, 1L,
2L, 7L, 9L, 10L, 3L), .Label = c(""general topics done"", ""keep the general topics updated"",
 ""rejected topic "", ""several topics in hand"", ""this is a genetal topic"",
 ""topic 333555 needs to be updated"", ""topic 5647 is handed over"",
 ""topic is updated"", ""update the topic "", ""updating the topic is done ""
  ), class = ""factor"")), class = ""data.frame"", row.names = c(NA,
  -10L))
corpus=Corpus(VectorSource(df$Description))
corpus=tm_map(corpus,tolower)
corpus=tm_map(corpus,removePunctuation)
corpus=tm_map(corpus,removeWords,c(stopwords(""english"")))
corpus=tm_map(corpus,stemDocument,""english"")

frequenciescontrol=DocumentTermMatrix(corpus)
frequenciescontrol$time=c(""2015"",""2015"",""2015"",""2015"",""2015"",""2016"",""2016"",""2016"",""2016"",""2016"")
findAssocs(frequenciescontrol, ""updat"", 0.01)
</code></pre>

<p>So I want a heatmap with y axis showing the words correlated to ""updat"" and x axis showing the years. </p>
",Dataset Preprocessing & Handling,create heatmap findassocs result based time following data create document term matrix first add time variable dtm order see correlation term updat different year would like heat map findassoc way x axis show time want heatmap axis showing word correlated updat x axis showing year
Compute TF-IDF word score with relevant and random corpus,"<p>Given a corpus of relevant documents (CORPUS) and a corpus of random documents (ran_CORPUS) I want to compute TF-IDF scores for all words in CORPUS, using ran_CORPUS as a base line. In my project, the ran_CORPUS has approximately 10 times as many documents as CORPUS.</p>

<pre><code>CORPUS = ['this is a relevant document',
          'this one is a relevant text too']
ran_CORPUS = ['the sky is blue',
              'my cat has a furry tail']
</code></pre>

<p>My plan is to normalize the documents, make all documents in CORPUS to one document (CORPUS being now a list with one long string element). To CORPUS I append all ran_CORPUS documents. Using <code>sklearn's TfidfTransformer</code> I then would compute the TF-IDF matrix for the corpus (consisting now of CORPUS and ran_CORPUS). And finally select the first row of that CORPUS to get the TF-IDF scores for my initial relevant CORPUS.</p>

<p>Does anybody know whether this approach could work and if there is a simple way to code it?</p>
",Dataset Preprocessing & Handling,compute tf idf word score relevant random corpus given corpus relevant document corpus corpus random document ran corpus want compute tf idf score word corpus using ran corpus base line project ran corpus ha approximately time many document corpus plan normalize document make document corpus one document corpus list one long string element corpus append ran corpus document using would compute tf idf matrix corpus consisting corpus ran corpus finally select first row corpus get tf idf score initial relevant corpus doe anybody know whether approach could work simple way code
Count word frequency across multiple columns in R,"<p>I have a data frame in R with multiple columns with multi-word text responses, that looks something like this:</p>

<pre><code>1a        1b             1c       2a          2b             2c
student   job prospects  money    professors  students       campus
future    career         unsure   my grades   opportunities  university
success   reputation     my job   earnings    courses        unsure
</code></pre>

<p>I want to be able to count the frequency of words in columns 1a, 1b, and 1c combined, as well as 2a, 2b, and 2b combined. </p>

<p>Currently, I'm using this code to count word frequency in each column individually. </p>

<pre><code>data.frame(table(unlist(strsplit(tolower(dat$1a), "" ""))))
</code></pre>

<p>Ideally, I want to be able to combine the two sets of columns into just two columns and then use this same code to count word frequency, but I'm open to other options.</p>

<p>The combined columns would look something like this:</p>

<pre><code>1              2
student        professors
future         my grades
success        earnings
job prospects  students
career         opportunities
reputation     courses
money          campus
unsure         university
my job         unsure
</code></pre>
",Dataset Preprocessing & Handling,count word frequency across multiple column r data frame r multiple column multi word text response look something like want able count frequency word column b c combined well b b combined currently using code count word frequency column individually ideally want able combine two set column two column use code count word frequency open option combined column would look something like
Reading .eml files with Python 3.6 using emaildata 0.3.4,"<p>I am using python 3.6.1 and I want to read in email files (.eml) for processing. I am using the <a href=""https://pypi.python.org/pypi/emaildata/0.3.4"" rel=""noreferrer"">emaildata 0.3.4</a> package, however whenever I try to import the Text class as in the documentation, I get the module errors:</p>

<pre><code>import email
from email.text import Text
&gt;&gt;&gt; ModuleNotFoundError: No module named 'cStringIO'
</code></pre>

<p>When I tried to correct using <a href=""https://stackoverflow.com/questions/28200366/python-3-4-0-email-package-install-importerror-no-module-named-cstringio"">this update</a>, I get the next error relating to <code>mimetools</code></p>

<pre><code>&gt;&gt;&gt; ModuleNotFoundError: No module named 'mimetools'
</code></pre>

<p>Is it possible to use emaildata 0.3.4 with python 3.6 to parse .eml files? Or are there any other packages I can use to parse .eml files? Thanks</p>
",Dataset Preprocessing & Handling,reading eml file python using emaildata using python want read email file eml processing using emaildata package however whenever try import text class documentation get module error tried correct using href update get next error relating possible use emaildata python parse eml file package use parse eml file thanks
Unable to detect a unicode in R,"<p>In R we are trying to detect check boxes and checked boxes. The complete PDF is read through pdftools package and stored in the form of dataframe. <a href=""https://i.sstatic.net/S0sYs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/S0sYs.png"" alt=""enter image description here""></a></p>

<p>The check boxes are stored in the form of ""U+F0A8"" character (removed &lt; &amp; > sign enclosing the character ""U+F0A8""as it's not visible with &lt; &amp; > signs ) While performing string detect or gref functions or just printing it these characters are not detected or printed. Kindly help. I have attached the screenshot for reference.</p>

<p><a href=""https://i.sstatic.net/mrebd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mrebd.png"" alt=""enter image description here""></a></p>

<p>Please let me know if you need more details. Thanks in advance.</p>
",Dataset Preprocessing & Handling,unable detect unicode r r trying detect check box checked box complete pdf read pdftools package stored form dataframe check box stored form u f character removed sign enclosing character u f visible sign performing string detect gref function printing character detected printed kindly help attached screenshot reference please let know need detail thanks advance
Extract Information/cleaning data from CSV file using Python,"<p>I have a dataset provided properties.csv (4000 rows and 6 columns). The csv file including many features some of these features are numerical and some of them are nominal (features contain text). Suppose the features in this dataset are</p>

<pre><code>id
F1 
F2
F3
F4 
Price 
</code></pre>

<p>Examples of the content of each feature:</p>

<p>id (row 1 to 3 in CSV File) ---> 44525
                                 44859
                                 45465</p>

<p>F1 (row 1 to 3 in CSV File) ---> ""Stunning 6 bedroom villa in the heart of the 
                                  Golden Mile, Marbella"" 
                                 ""Villa for sale in Rocio de NagÃ¼eles, Marbella 
                                  Golden Mile""
                                 ""One level 5 bedroom villa for sale in 
                                  NagÃ¼eles""</p>

<p>F2 (row 1 to 3 in CSV File) --->  ""Fireplace, Elevator, Terrace, Mountain view, 
                                   Freight Elevator, Air conditioning, Patio, 
                                   Guest toilet, Garden, Balcony, Sea/lake view, 
                                   Built-in kitchen""
                                   ""Mountain view""
                                   ""Elevator, Terrace, Alarm system, Mountain 
                                    view, Swimming pool, Air conditioning, 
                                    Basement, Sea/lake view""</p>

<p>F3 (row 1 to 3 in CSV File) - contains numerical values --->  0
                                                              0
                                                              0</p>

<p>F4 (row 1 to 3 in CSV File) - contains numerical values ---> 393
                                                             640
                                                             4903
F3 (row 1 to 3 in CSV File) - contains numerical values ---> 4400000
                                                             2400000
                                                             1900000</p>

<p>In F1, I am looking to do the following:</p>

<p>1- Extract the type of the properties (apartment’, ‘house’ or ‘Villa’) and put it in a separate feature (independent variable) calls ""Type"" in CSV file. After that, I want to separate them in groups (apartments group, houses group, Vilas group) with calculating the mean price of each type group.
2- Extract the location of each property (locations can be: Alenquer, Quinta da Marinha, Golden Mile, Nagüeles) and put it in a separate feature (independent variable) calls ""Location"" in csv file. </p>

<p>I am a beginner in NLP. I tried to write this code to extract information ""Apartment"" from F1, but it does not work probably:</p>

<pre><code>import pandas as pd
from pandas import DataFrame
import re

properties = pd.read_csv (r'C:/Users/User/Desktop/properties.csv')
Extract ""Apartment"" from F1
Title= DataFrame(properties,columns= ['F1'])

for line in F1: 
        #return list of apartments in that line
        x = re.findall(""\apartment"", line) 
        #if a date is found
         if len(x) != 0:
         print(x)
</code></pre>

<p>I need your help to fix this code and what should I do to extract the other information ‘houses’ and ‘Villa’ from F1.</p>

<p>After that, Create a property dataset in this format and save it as a csv file:</p>

<pre><code>id
Location (Information extracted from F1)
type (information extracted from F1 in groups ""apartments’, ‘houses’, ‘Villas’"")
F1
F2
F3
F4
Price
</code></pre>

<p>In case, F1 does not contain the type of some properties ""Blank field (no text)"", what should I do to deal with the blanks fields (no text) in F1 and extract the type of the properties from other properties?   </p>
",Dataset Preprocessing & Handling,extract information cleaning data csv file using python dataset provided property csv row column csv file including many feature feature numerical nominal feature contain text suppose feature dataset example content feature id row csv file f row csv file stunning bedroom villa heart golden mile marbella villa sale rocio de nag eles marbella golden mile one level bedroom villa sale nag eles f row csv file fireplace elevator terrace mountain view freight elevator air conditioning patio guest toilet garden balcony sea lake view built kitchen mountain view elevator terrace alarm system mountain view swimming pool air conditioning basement sea lake view f row csv file contains numerical value f row csv file contains numerical value f row csv file contains numerical value f looking following extract type property apartment house villa put separate feature independent variable call type csv file want separate group apartment group house group vila group calculating mean price type group extract location property location alenquer quinta da marinha golden mile nag eles put separate feature independent variable call location csv file beginner nlp tried write code extract information apartment f doe work probably need help fix code extract information house villa f create property dataset format save csv file case f doe contain type property blank field text deal blank field text f extract type property property
read word from each row in a dataframe,"<p>I want to read the word ""risk"" from every row in dataframe. If a row have the word risk in it then the dataframe should make a new column which will put 1 in it else 0. How can I achieve this ?</p>
",Dataset Preprocessing & Handling,read word row dataframe want read word risk every row dataframe row word risk dataframe make new column put else achieve
How to predict the location based on training data from CSV file in python and NLPK,"<p>I want to predict the location-based one training data. I have data in below format.</p>

<p>Training Data:</p>

<pre><code>Address Location_id Location_name
Flat No.201, MIDC, Andheri East, Mumbai, Maharashtra    121 Andheri East
Business Park, Goregaon, Mumbai, Maharashtra    122 Goregaon
Powai, Mumbai   123 Powai
Andheri East, Mumbai    121 Andheri East
Best Business Park, Goregaon, Mumbai    122 Goregaon
Hiranandani Park, Powai, Mumbai 123 Powai
</code></pre>

<p>Test Data:</p>

<pre><code>plot no. 121, MIDC Area, Andheri East, Mumbai
</code></pre>

<p>Expected output:</p>

<pre><code>To predict the location ID and Location Name.
</code></pre>

<p>Please suggest.</p>
",Dataset Preprocessing & Handling,predict location based training data csv file python nlpk want predict location based one training data data format training data test data expected output please suggest
"Open and read all text Files in your directory and filter them using regular expression, python","<p>So, I was given a use case. The use case is to find PHI in multiple text files using regular expressions and python at once.</p>

<p>So basically, Open all text files in your directory and then filter the content of each file with regular expressions to see which file has PHI in them. </p>

<p>Any ideas?</p>
",Dataset Preprocessing & Handling,open read text file directory filter using regular expression python wa given use case use case find phi multiple text file using regular expression python basically open text file directory filter content file regular expression see file ha phi idea
How to incorporate metadata into NLTK corpus for efficient processing,"<p>I have a folder of txt files and also a csv file with additional data like categories a particular txt document belongs to and the original source file (pdf) path. The Txt file name is used as key into the CSV file.</p>

<p>I have created a basic nltk corpus but I would like to know if that's the best way of structuring my data given I want to carry out a range of NLP tasks like NER on the corpus and eventually identify the entities which occur in each category and be able to link back to the source pdf files so each entity can be seen in context.</p>

<p>Most NLP examples (find NERs) go from corpus to python lists of entities but doesn't that mean I will loose the association back to the txt file which contained the entities and all the other associated data?</p>

<p>Categorical corpus appears to help with keeping the category data but my question is </p>

<p>What is the best way to structure and work with my corpus that avoids having to roundtrip between
- process corpus to identify interesting information outputted to list
-  search corpus again to get files which contains the interested element from the list
- search CSV (data frame) by file id to get the rest of the metadata</p>
",Dataset Preprocessing & Handling,incorporate metadata nltk corpus efficient processing folder txt file also csv file additional data like category particular txt document belongs original source file pdf path txt file name used key csv file created basic nltk corpus would like know best way structuring data given want carry range nlp task like ner corpus eventually identify entity occur category able link back source pdf file entity seen context nlp example find ners go corpus python list entity mean loose association back txt file contained entity associated data categorical corpus appears help keeping category data question best way structure work corpus avoids roundtrip process corpus identify interesting information outputted list search corpus get file contains interested element list search csv data frame file id get rest metadata
Text classification on CNN model,"<p>The problem is i need to load the file which i have in h5 format as below</p>
<pre><code>from keras.models import load_model
model = load_model('my_model.h5')
model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])
classes = model.predict_classes(&quot;How is the weather today&quot;)
print classes
</code></pre>
<p>And also i need that percentage value of the prediction to be printed</p>
<p><a href=""https://github.com/jatana-research/Text-Classification/blob/master/CNN.ipynb"" rel=""nofollow noreferrer"">Here is the link that i refered to while generating this model and saving the file</a></p>
",Dataset Preprocessing & Handling,text classification cnn model problem need load file h format also need percentage value prediction printed link refered generating model saving file
Create a code in python to get the most frequent tag and value pair from a list,"<p>I have a .txt file with 3 columns: word position, word and tag (NN, VB, JJ, etc.).</p>

<p>Example of txt file: </p>

<pre><code>1   i   PRP

2   want    VBP

3   to  TO

4   go  VB
</code></pre>

<p>I want to find the frequency of the word and tag as a pair in the list in order to find the most frequently assigned tag to a word. 
Example of Results:
3 (food, NN), 2 (Brave, ADJ)</p>

<p>My idea is to start by opening the file from the folder, read the file line by line and split, set a counter using dictionary and print with the most common to uncommon in descending order. </p>

<p>My code is extremely rough (I'm almost embarrassed to post it):</p>

<pre><code>file=open(""/Users/Desktop/Folder1/trained.txt"")
wordcount={}
for word in file.read().split():
    from collections import Counter
    c = Counter()
    for d in dicts.values():
        c += Counter(d)

print(c.most_common())

file.close()
</code></pre>

<p>Obviously, i'm getting no results. Anything will help. Thanks.</p>

<p>UPDATE:</p>

<p>so i got this code posted on here which worked, but my results are kinda funky. here's the code (the author removed it so i don't know who to credit):</p>

<pre><code>file=open(""/Users/Desktop/Folder1/trained.txt"").read().split('\n')

d = {}
for i in file:
    if i[1:] in d.keys():
        d[i[1:]] += 1
    else:
        d[i[1:]] = 1

print (sorted(d.items(), key=lambda x: x[1], reverse=True))
</code></pre>

<p>here are my results:</p>

<pre><code>[('', 15866), ('\t.\t.', 9479), ('\ti\tPRP', 7234), ('\tto\tTO', 4329), ('\tlike\tVB', 2533), ('\tabout\tIN', 2518), ('\tthe\tDT', 2389), ('\tfood\tNN', 2092), ('\ta\tDT', 2053), ('\tme\tPRP', 1870), ('\twant\tVBP', 1713), ('\twould\tMD', 1507), ('0\t.\t.', 1427), ('\teat\tVB', 1390), ('\trestaurant\tNN', 1371), ('\tuh\tUH', 1356), ('1\t.\t.', 1265), ('\ton\tIN', 1237), (""\t'd\tMD"", 1221), ('\tyou\tPRP', 1145), ('\thave\tVB', 1127), ('\tis\tVBZ', 1098), ('\ttell\tVB', 1030), ('\tfor\tIN', 987), ('\tdollars\tNNS', 959), ('\tdo\tVBP', 956), ('\tgo\tVB', 931), ('2\t.\t.', 912), ('\trestaurants\tNNS', 899),
</code></pre>

<p>there seem to be a mix of good results with words and other results with space or random numbers, anyone know a way to remove what aren't real words? also, i know \t is supposed to signify a tab, is there a way to remove that as well? you guys really helped a lot </p>
",Dataset Preprocessing & Handling,create code python get frequent tag value pair list txt file column word position word tag nn vb jj etc example txt file want find frequency word tag pair list order find frequently assigned tag word example result food nn brave adj idea start opening file folder read file line line split set counter using dictionary print common uncommon descending order code extremely rough almost embarrassed post obviously getting result anything help thanks update got code posted worked result kinda funky code author removed know credit result seem mix good result word result space random number anyone know way remove real word also know supposed signify tab way remove well guy really helped lot
Extracting italic text from a document,"<p>I have a word document with a list of species names and then various text about each species. I'd like to just extract all the species names. The obvious way to do this is to just extract all text in italics. However, I can't find a way to do this in python, does anyone have any ideas?</p>

<p>E.g. input: <em>Acanthognathus
rudis</em>
Small prey Solitary – 1 ? 1 ? Recruitment: Solitary, frequently catch collembola and other
small prey (GRONENBERG &amp; al. 1998).
Size: Small, can be retrieved by one <em>Acromyrmex
coronatus</em></p>

<p>ouput: Acanthognathus
rudis, Acromyrmex
coronatus</p>
",Dataset Preprocessing & Handling,extracting italic text document word document list specie name various text specie like extract specie name obvious way extract text italic however find way python doe anyone idea e g input acanthognathus rudis small prey solitary recruitment solitary frequently catch collembola small prey gronenberg al size small retrieved one acromyrmex coronatus ouput acanthognathus rudis acromyrmex coronatus
R package &#39;TranslateR&#39; does not show the translated results,"<p>I was using the R package ""translateR"" with google translation API key, with the default dataset 'enron' like following:</p>

<blockquote>
  <p>data(enron) is a data frame with 10 observations on the following 3 variables.
  email A character vector of the email’s body.
  date The email’s timestamp as a ’Date’ type.
  subject A character vector containing the email’s subject line</p>
</blockquote>

<h3>The code</h3>

<pre><code>library(translateR) # Loading the package
data(enron) # Loading the dataset to be translated
tc &lt;- translate(dataset = enron, 
                content.field = 'email', 
                google.api.key = 'my google.api.key', 
                source.lang = 'en', 
                target.lang = 'de')
print(tc)
</code></pre>

<h3>Expected</h3>

<p>dataset 'tc' is supposed to be the orginal dataset with an addtional column ""translatedContent"" appended.</p>

<h3>Result</h3>

<p>dataset 'tc' is the same as the input dataset 'enron', no translated contents can be seen.</p>

<h3>Addition information</h3>

<p>My google translation API key works well, its working for the other r package 'translate'.</p>

<p>I am using RStudio Version 1.1.383 with the latest R version 3.4.2
on OS X 10.10.5</p>
",Dataset Preprocessing & Handling,r package translater doe show translated result wa using r package translater google translation api key default dataset enron like following data enron data frame observation following variable email character vector email body date email timestamp date type subject character vector containing email subject line code expected dataset tc supposed orginal dataset addtional column translatedcontent appended result dataset tc input dataset enron translated content seen addition information google translation api key work well working r package translate using rstudio version latest r version x
Add NLP NGRAM/Term results to a new column in existing data frame,"<p>Good evening Stackers! I have another doozy for you :)</p>

<p>I have an excel file with approx 54k rows, and in this file, I have a column called ""short description"", this is the ""short description"" of an end users IT issue -  ""I can't print"", ""I am unable to open X from server Y"", etc. I am using NLP to find the most common ""words and phrases"" identified in the data-set or in the Short Description... here is a look into what that output looks like:
<a href=""https://i.sstatic.net/E9zbZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/E9zbZ.png"" alt=""enter image description here""></a></p>

<p>To get to this point, I have created a corpus from my short description, I have done all of the ""cleaning"", meaning, changing everything to lower case, removed stop words, white space, etc.  The output in the image above was done by using term_stats(corpus, ngrams = 3:3)</p>

<p>Now what I would like to do, for example, if you look at UPC Scanning issues from the image above, I would like to create a new column in the ""db"" data-source called 'phrase', and for every row that UPC Scanning issues was found in the short description, I simply want to add that in a new row of the new phrase column.. like a one to one relationship if you will. Trying to explain myself further, if short description row 1  said ""I am having issues with my upc scanning platform"" the new column ""phrase' row one would be ""upc scanning issues""</p>

<p>This is how i am building my corpus, and other parts of the script:</p>

<pre><code>###### GET RID OF FUNKY CHARS ########
db$SHORT_DESCRIPTION &lt;- sapply(db$SHORT_DESCRIPTION,function(row) iconv(row, ""latin1"", ""ASCII"", sub=""""))

###### CREATE CORPUS ########
corpus &lt;- Corpus(VectorSource(db$SHORT_DESCRIPTION))

###### INSPECT CORPUS ########
inspect(corpus[1:20])

###### TRANSFORM/MANIPULATE TEXT IN CORPUS ########
corpus &lt;- tm_map(corpus, content_transformer(tolower))
corpus &lt;- tm_map(corpus, removePunctuation)
corpus &lt;- tm_map(corpus, stripWhitespace)
corpus &lt;- tm_map(corpus, removeWords, stopwords(""english""))
corpus &lt;- tm_map(corpus, removeWords, c(""csd"",""CSD"",""please"",""request"",""service"",""servic"",""active"",""activ"",""resourc"",""reset"",""8000"",""self"",""please"",'compucom"",""self"",""pleas"",""notif"",""csdplease"",""caller"",""record"",""issue"",""temp"",""csdemail"",""csdneed"",""csdact""))
str_replace_all(db$SHORT_DESCRIPTION, ""[[:punct:]]"", "" "")
corpus &lt;- tm_map(corpus, stripWhitespace)

##### CREATE PHRASES #####
phrases &lt;- term_stats(corpus, ngrams = 3)
phrases

##### NOW THIS IS WHERE I AM STUCK ######
db$phrases &lt;- phrases????? 
#I get an error saying that phrases contains aprpox 200k rows and my data set only containts 54k rows.. 
</code></pre>

<p>I want to take the results of phrases, create a column in the db dataset, and add the results of phrases from above into that column.</p>

<p>Sorry for the image, but it's the easiest way for me to show the output of the phrases line that I created. </p>
",Dataset Preprocessing & Handling,add nlp ngram term result new column existing data frame good evening stacker another doozy excel file approx k row file column called short description short description end user issue print unable open x server etc using nlp find common word phrase identified data set short description look output look like get point created corpus short description done cleaning meaning changing everything lower case removed stop word white space etc output image wa done using term stats corpus ngrams would like example look upc scanning issue image would like create new column db data source called phrase every row upc scanning issue wa found short description simply want add new row new phrase column like one one relationship trying explain short description row said issue upc scanning platform new column phrase row one would upc scanning issue building corpus part script want take result phrase create column db dataset add result phrase column sorry image easiest way show output phrase line created
Keyword-Search with synonyms in information retrieval systems,"<p>I have development a chatbot to give right answer for user input. Right now, I struggle how to read the DB/Knowledge Base or just the json file properly to extract the right answer. In my use case I have very much Keyword names/entities together with synonyms. So it would be a bad idea to write the synonyms in the NLU training file manually. My database file has first listed the keyword and afterwards the answers are separated for each intent belonging to this entity. How do you handle in practice such keyword search together with slightly different keywords/synonyms given by the user?</p>

<p>I am interested in standard approach to this and in future I would like to apply NLP maybe like word emebedding for my custom case.</p>
",Dataset Preprocessing & Handling,keyword search synonym information retrieval system development chatbot give right answer user input right struggle read db knowledge base json file properly extract right answer use case much keyword name entity together synonym would bad idea write synonym nlu training file manually database file ha first listed keyword afterwards answer separated intent belonging entity handle practice keyword search together slightly different keywords synonym given user interested standard approach future would like apply nlp maybe like word emebedding custom case
R - How to apply terms from training document-term-matrix (dtm) to test dtm (both unigrams and bigrams)?,"<p>I am training a simple text classification method on 1,000 training examples and would like to make predictions on unseen test data (about 500,000 observations).</p>

<p>The script is working fine, when I work only with unigrams. However, I am not sure how to use <code>control = list(dictionary=Terms(dtm_train_unigram))</code> when working with unigrams and bigrams as I have two separate document-term-matrices (one for unigrams, one for bigrams, see below):</p>

<pre><code>  UnigramTokenizer &lt;- function(x) unlist(lapply(NLP::ngrams(words(x), 1), paste, collapse = "" ""), use.names = FALSE)
  dtm_train_unigram &lt;- DocumentTermMatrix(processed_dataset, control = list(tokenize = UnigramTokenizer, wordLengths=c(3,20), bounds = list(global = c(4,Inf))))

  BigramTokenizer &lt;- function(x) unlist(lapply(NLP::ngrams(words(x), 2), paste, collapse = "" ""), use.names = FALSE)
  dtm_train_bigram &lt;- DocumentTermMatrix(processed_dataset, control = list(tokenize = BigramTokenizer, wordLengths=c(6,20), bounds = list(global = c(7,Inf))))
</code></pre>

<p>To ensure that the test set has the same terms as the training set, I use the following function:</p>

<pre><code>corpus_test &lt;- VCorpus(VectorSource(test_set))
dtm_test &lt;- DocumentTermMatrix(corpus_test, control = list(dictionary=Terms(dtm_train_unigram), wordLengths = c(3,20)))
</code></pre>

<p>How do I feed the terms of both the <code>dtm_train_unigram</code> and the <code>dtm_train_bigram</code> to the dtm_test?</p>

<ol>
<li>Can I combine <code>dtm_train_unigram</code> and <code>dtm_train_bigram</code> to a single dtm after creating them separately (as currently done)?  </li>
<li>Can I simplify my two-step Tokenizer function so I only create a single
dtm with unigrams and bigrams in the first place?</li>
</ol>

<p>Thank you!</p>
",Dataset Preprocessing & Handling,r apply term training document term matrix dtm test dtm unigrams bigram training simple text classification method training example would like make prediction unseen test data observation script working fine work unigrams however sure use working unigrams bigram two separate document term matrix one unigrams one bigram see ensure test set ha term training set use following function feed term dtm test combine single dtm creating separately currently done simplify two step tokenizer function create single dtm unigrams bigram first place thank
Extracting information from text in python,"<p>I am new to the text mining. I have a CSV file. I need to go through each line and extract some information then write them into another CSV file. I am looking for specific information which I have in a dictionary. Consider below sentence: </p>

<blockquote>
  <p>""the application version is 1.8.2 and the variable skt.len passes the required information. file ReadMe.txt has the specifications.""</p>
</blockquote>

<p>My dictionary is: [""application version"", ""variable"", ""file""]</p>

<p>I need to extract:</p>

<ul>
<li>application version: 1.8.2</li>
<li>variable: skt.len</li>
<li>file: ReadMe.txt</li>
</ul>

<p>What is the best way to extract such information from text? I am playing with NLTK and StanfordCoreNLP features. But, I could not extract the information yet. I am thinking to use regex to extract the application version. Any idea?</p>

<p>PS: I know that this may make the task more complicated. But, sentences in each line of the CSV file may have different structures. For example: ""application version"" in one line, may be ""app version"" in another line. Or ""file"" in one line may be ""filename"" in another line. </p>
",Dataset Preprocessing & Handling,extracting information text python new text mining csv file need go line extract information write another csv file looking specific information dictionary consider sentence application version variable skt len pass required information file readme txt ha specification dictionary application version variable file need extract application version variable skt len file readme txt best way extract information text playing nltk stanfordcorenlp feature could extract information yet thinking use regex extract application version idea p know may make task complicated sentence line csv file may different structure example application version one line may app version another line file one line may filename another line
Find the most frequent string in a Data frame,"<p>I am new to Python  programming.I have a pandas data frame in which two string columns are present.</p>

<p>Data frame is like below:</p>

<pre><code>Case    Action
Create   Create New Account
         Create New Account
         Create New Account
         Create New Account
         Create Old Account
Delete   Delete New Account
         Delete New Account
         Delete Old Account
         Delete Old Account
         Delete Old Account
</code></pre>

<p>Here we can see in  <code>Create</code>, out 5 actions 4 actions has been <code>Create New Account</code>. Means 4/5(=80%) .Similarly in <code>Delete</code> case maximum cases are <code>Delete Old Account</code>. So my requirement is when next time any case comes like <code>Create</code>, i should get o/p as <code>Crate New Account</code> with frequency score.</p>

<p>Expected O/P :</p>

<pre><code>Case    Action              Score
Create  Create New Account  80
Delete  Delete Old Account  60
</code></pre>
",Dataset Preprocessing & Handling,find frequent string data frame new python programming panda data frame two string column present data frame like see action action ha mean similarly case maximum case requirement next time case come like get p frequency score expected p
R tm package and Spark/python give different vocabulary size for Document Term Frequency task,"<p>I have a csv with a single column, each row is a text document. All text has been normalized:</p>

<ul>
<li>all lowercase</li>
<li>no punctuation </li>
<li>no numbers </li>
<li>no more than one whitespace between words</li>
<li>no tags(xml, html)</li>
</ul>

<p>I have also this R script which constructs the Document Term Matrix on these documents and does some machine learning analysis. I need to convert this in Spark.</p>

<p>The first step is to produce the Document Term Matrix where for each term there is the relative frequency count in the document. The problem is that I am getting different vocabularies size using R, respect to spark api or python sklearn (spark and python are consistent in the result).</p>

<p>This is the relevant code for R:</p>

<pre><code>library(RJDBC)
library(Matrix)
library(tm)
library(wordcloud)
library(devtools)
library(lsa)
library(data.table)
library(dplyr)
library(lubridate)

corpus &lt;- read.csv(paste(inputDir, ""corpus.csv"", sep=""/""), stringsAsFactors=FALSE)
DescriptionDocuments&lt;-c(corpus$doc_clean)
DescriptionDocuments &lt;- VCorpus(VectorSource(DescriptionDocuments))
DescriptionDocuments.DTM &lt;- DocumentTermMatrix(DescriptionDocuments, control = list(tolower = FALSE,
                                                                                    stopwords = FALSE,
                                                                                    removeNumbers = FALSE,
                                                                                    removePunctuation = FALSE,
                                                                                    stemming=FALSE))

# VOCABULARY SIZE = 83758
</code></pre>

<p><br><br>
This is the relevant code in Spark (1.6.0, Scala 2.10):</p>

<pre><code>import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, RegexTokenizer}

var corpus = sqlContext.read.format(""com.databricks.spark.csv"").option(""header"", ""true"").option(""inferSchema"", ""false"").load(""/path/to/corpus.csv"")

// RegexTokenizer splits by default on one or more spaces, which is ok
val rTokenizer = new RegexTokenizer().setInputCol(""doc"").setOutputCol(""words"")
val words = rTokenizer.transform(corpus)

val cv = new CountVectorizer().setInputCol(""words"").setOutputCol(""tf"")
val cv_model = cv.fit(words)
var dtf = cv_model.transform(words)

// VOCABULARY SIZE = 84290
</code></pre>

<p><br><br>
I've also checked in python sklearn and I got consistent result with spark:</p>

<pre><code>import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

corpus = pd.read_csv(""/path/to/corpus.csv"")
docs = corpus.loc[:, ""doc""].values

def tokenizer(text):
    return text.split

cv = CountTokenizer(tokenizer=tokenizer, stop_words=None)
dtf = cv.fit_transform(docs)
print len(dtf.vocabulary_)

# VOCABULARY SIZE = 84290
</code></pre>

<p><br><br>
I don't know very much <a href=""https://cran.r-project.org/web/packages/tm/tm.pdf"" rel=""nofollow noreferrer"">R tm package</a> but it seems to me that by default should tokenize on white spaces by default. Someone has any hint why am I getting different vocabulary size?</p>
",Dataset Preprocessing & Handling,r tm package spark python give different vocabulary size document term frequency task csv single column row text document text ha normalized lowercase punctuation number one whitespace word tag xml html also r script construct document term matrix document doe machine learning analysis need convert spark first step produce document term matrix term relative frequency count document problem getting different vocabulary size using r respect spark api python sklearn spark python consistent result relevant code r relevant code spark scala also checked python sklearn got consistent result spark know much r tm package seems default tokenize white space default someone ha hint getting different vocabulary size
Find the similarity between a string input and a string column of a Data Frame,"<p>I have a pandas data frame with two columns containing strings, like below:</p>

<pre><code>Col-1                 Col-2
Animal                have an apple
Fruit                 tiger safari
Veg                   Vegetable Market
Flower                Garden
</code></pre>

<p>From this i have to create a function which takes a string as argument. </p>

<p>This function then checks the <code>fuzziwuzzy</code> similarity between the input string and the elements of <code>Col-2</code> and outputs the elements of <code>Col-1</code> and <code>Col-2</code> corresponding of the highest computed similarity.</p>

<p>For instance suppose input string is <code>Gardening Hobby</code>, here it will check similarity with all the elements of <code>df['Col-2']</code>. The function finds this ways that <code>Garden</code> as the highest similarity with <code>Gardening Hobby</code> with a score of 90. Then Expected output is:</p>

<pre><code>I/P               O/P
Gardening Hobby   Garden(60),Flower
</code></pre>
",Dataset Preprocessing & Handling,find similarity string input string column data frame panda data frame two column containing string like create function take string argument function check similarity input string element output element corresponding highest computed similarity instance suppose input string check similarity element function find way highest similarity score expected output
How to Split pandas data frame string entry to separate rows?,"<p>I have a pandas dataframe in which one column of text strings contains new line separated values. 
I want to split each CSV field and create a new row per entry.</p>

<p>My Data Frame is like:</p>

<pre><code>Col-1   Col-2
A       Notifications
        Returning Value
        Both
B       mine
        Why Not?
</code></pre>

<p>Expected output is:</p>

<pre><code>Col-1   Col-2
A       Notifications 
A       Returning Value
A       Both
B       mine
B       Why Not?
</code></pre>
",Dataset Preprocessing & Handling,split panda data frame string entry separate row panda dataframe one column text string contains new line separated value want split csv field create new row per entry data frame like expected output
Applying &quot;String matching to estimate similarity&quot; to data frame,"<p><a href=""https://stackoverflow.com/questions/22936951/string-matching-to-estimate-similarity"">String matching to estimate similarity</a></p>

<p>The above code is exactly what I am looking for, except I cannot seem to figure out how to compare the strings between columns (the ""correct"" answer and ""given"" answer) in a data frame and then storing the output from sim.per as a new column (""similarity"") in that same data frame. I have tried .e.g, </p>

<pre><code>df$similarity &lt;- sim.per(df$answer, df$given) 

df$similarity &lt;- mapply(sim.per, df$answer, df$given)
</code></pre>

<p>The latter also results in an error when the row is empty, which is acceptable in my dataset and should be calculated as 0 instead. </p>

<pre><code>Error in str2[[1]] : subscript out of bounds
</code></pre>

<p>Expected output should be:</p>

<pre><code>    answer                   given                              similarity
1   Best way to waste money  Instrument to waste money and time 0.6
2   Roy travels to Africa    He is in Africa                    0.25
3   I go to work                                                0
</code></pre>

<p>Any help would be appreciated! Thanks!</p>

<p>Subset of the data:</p>

<pre><code>df &lt;- structure(list(trial = 1:10, answer = structure(c(9L, 2L, 4L, 7L, 1L, 5L, 3L, 6L, 8L, 10L), .Label = c(""Best way to waste money"", ""He ran out of money, so he had to stop playing poker"", ""I go to work"", ""Lets all be unique together until we realise we are all the same"", ""Roy travels to Africa"", ""She borrowed the book from him many years ago and did not returned it yet"", ""She did her best to help him"", ""Students did not cheat on the test, for it was not the right thing to do"", ""The stranger officiates the meal"", ""We have a lot of rain in June""), class = ""factor""), given = structure(c(10L, 3L, 6L, 8L, 4L, 2L, 1L, 7L, 9L, 5L), .Label = c("""", ""He is in Africa Roy"", ""He lost money because he had played poker"", ""Instrument to waste money and time"", ""It was raining in June"", ""People are unique until they try to fit in"", ""She borrowed the book from the library and forgot to return it"", ""She did her very best to help him out"", ""Students know not to cheat"", ""The guests ate the meal""), class = ""factor"")), class = ""data.frame"", row.names = c(NA, -10L))
</code></pre>
",Dataset Preprocessing & Handling,applying string matching estimate similarity data frame latter also result error row empty acceptable dataset calculated instead expected output help would appreciated thanks subset data
Find the similarity between two string columns of a DataFrame,"<p>I am new to programming.I have a pandas data frame in which two string columns are present.</p>

<p>Data frame is like below:</p>

<pre><code>Col-1             Col-2
Update            have a account
Account           account summary
AccountDTH        Cancel
Balance           Balance Summary
Credit Card       Update credit card
</code></pre>

<p>Here i  need to check the similarity of Col-2 elements with each element of Col-1.
It Means i have to compare <code>have a account</code> with all the elements of <code>Col-1</code>.
Then find the top 3 similar one. Suppose the similarity scores are :<code>Account(85),AccountDTH(80),Balance(60),Update(45),Credit Card(35)</code>.</p>

<p>Expected Output is:</p>

<pre><code>Col-2              Output
have a account     Account(85),AccountDTH(80),Balance(60)
</code></pre>
",Dataset Preprocessing & Handling,find similarity two string column dataframe new programming panda data frame two string column present data frame like need check similarity col element element col mean compare element find top similar one suppose similarity score expected output
Remove Column values having Single word only from a Data Frame using Python,"<p>I am new to programming. I have a DataFrame shown in as below:</p>

<pre><code>Col-2               Col-3
have a account      A
account summary     B
Cancel              C
Both                D
Update credit card  E
Block Credit card   F
</code></pre>

<p>I need my output as:</p>

<pre><code>Col-2               Col-3
have a account      A
account summary     B
Update credit card  E
Block Credit card   F
</code></pre>

<p>Means I need those values where <code>Col-2</code> is having more than one word. Single word present in <code>Col-2</code> should be removed. <code>Both</code> and <code>Cancel</code> are single words, that's why those rows have been removed from the output.</p>
",Dataset Preprocessing & Handling,remove column value single word data frame using python new programming dataframe shown need output mean need value one word single word present removed single word row removed output
how to implement fast spellchecker in Python with Pandas?,"<p>I work on text mining problem and need to extract all mentioned of certain keywords. For example, given the list:</p>

<pre><code>list_of_keywords = ['citalopram', 'trazodone', 'aspirin']
</code></pre>

<p>I need to find all occurrences of the keywords in a text. That could be easily done with Pandas (assuming my text is read in from a csv file):</p>

<p>import pandas as pd</p>

<pre><code>df_text = pd.read_csv('text.csv')
df_text['matches'] = df_text.str.findall('|'.join(list_of_keywords))
</code></pre>

<p>However, there are spelling mistakes in the text and some times my keywords will be written as: </p>

<pre><code>'citalopram' as 'cetalopram'
</code></pre>

<p>or </p>

<pre><code>'trazodone' as 'trazadon'
</code></pre>

<p>Searching on the web, I found some suggestions how to implement the <a href=""http://norvig.com/spell-correct.html"" rel=""nofollow noreferrer"">spell checker</a>, but I am not sure where to insert the spell checker and I reckon that it may slow down the search in the case a very large text.</p>

<p>As another option, it has been suggested to use a wild card with regex and insert in the potential locations of confusion (<strong>conceptually</strong> written)</p>

<pre><code>.findall('c*t*l*pr*m')
</code></pre>

<p>However I am not convinced that I can capture all possible problematic cases. I tried some out-of-the-box spell checkers, but my texts are some-what specific and I need a spell checker that 'knows' my domain (medical domain).</p>

<p><strong>QUESTION</strong></p>

<p>Is there any efficient way to extract keywords from a text including spelling mistakes?</p>
",Dataset Preprocessing & Handling,implement fast spellchecker python panda work text mining problem need extract mentioned certain keywords example given list need find occurrence keywords text could easily done panda assuming text read csv file import panda pd however spelling mistake text time keywords written searching web found suggestion implement spell checker sure insert spell checker reckon may slow search case large text another option ha suggested use wild card regex insert potential location confusion conceptually written however convinced capture possible problematic case tried box spell checker text specific need spell checker know domain medical domain question efficient way extract keywords text including spelling mistake
Expanding Window Size for NLTK from_document Collocation Method,"<p>Using NLTK to analyze a nestled list of numbers. Each sublist is independent from the others, so I used the <code>from_document</code> method. However, unlike the <code>from_words</code> method, <code>from_document</code> does not have a window size input. I want to expand the window size such that it matches each document size. My code so far:</p>

<pre><code> split_list = [[6, 3, 7, 8, 7, 5, 8, 8, 8, 3, 2, 1, 4],
 [5, 7, 8, 1, 8, 10, 3, 5, 5, 6, 8, 8, 5],
 [8, 9, 1, 2, 3, 8, 6, 3, 11],...]


bigram_measures = nltk.collocations.BigramAssocMeasures()
finder = BigramCollocationFinder.from_documents(split_list)
finder.score_ngrams(bigram_measures.pmi)
</code></pre>

<p>output:</p>

<pre><code> [((10, 4), 2.6544750245287965),
 ((1, 4), 2.270073203392851),
 ((2, 1), 1.6606985694144463),
 ((10, 10), 1.3898880959117932),
 ((4, 1), 1.2139301253553185),...]
</code></pre>

<p>But this only solves for bigrams with a window size of 2, when I want all possible bigrams from a document (e.g. window size = document size). I could go through and calculate everything manually using <code>itertools.combinations</code> to make all the combinations of bigrams, calculate their frequency, and use the non-iterated frequency of the unigrams to eventually get the pmi. However, this seems like a very roundabout way. Is there any way I could get NLTK to expand the window size?</p>
",Dataset Preprocessing & Handling,expanding window size nltk document collocation method using nltk analyze nestled list number sublist independent others used method however unlike method doe window size input want expand window size match document size code far output solves bigram window size want possible bigram document e g window size document size could go calculate everything manually using make combination bigram calculate frequency use non iterated frequency unigrams eventually get pmi however seems like roundabout way way could get nltk expand window size
How to train Open NLP without file,"<p>i have the following code for training Open NLP POS Tagger</p>

<pre><code>Trainer(String trainingData, String modelSavePath, String dictionary){

    try {
        dataIn = new MarkableFileInputStreamFactory(
                new File(trainingData));

        lineStream = new PlainTextByLineStream(dataIn, ""UTF-8"");
        ObjectStream&lt;POSSample&gt; sampleStream = new WordTagSampleStream(lineStream);

        POSTaggerFactory fac=new POSTaggerFactory();
        if(dictionary!=null &amp;&amp; dictionary.length()&gt;0)
        {
            fac.setDictionary(new Dictionary(new FileInputStream(dictionary)));
        }
        model = POSTaggerME.train(""en"", sampleStream, TrainingParameters.defaultParams(), fac);

    } catch (IOException e) {
        // Failed to read or parse training data, training failed
        e.printStackTrace();
    } finally {
        if (lineStream != null) {
            try {
                lineStream.close();
            } catch (IOException e) {
                // Not an issue, training already finished.
                // The exception should be logged and investigated
                // if part of a production system.
                e.printStackTrace();
            }
        }
    }
}
</code></pre>

<p>and this works just fine. Now, is it possible to do the same without involving files? I want to store the training data in a database somewhere. Then i can read it as a stream or chunks and feed it to the trainer. I do not want to create a temp file. Is this possible?</p>
",Dataset Preprocessing & Handling,train open nlp without file following code training open nlp po tagger work fine possible without involving file want store training data database somewhere read stream chunk feed trainer want create temp file possible
How to classify Data frame of characters into categories using keywords in R?,"<p>I'm trying to classify a data frame of customer reviews into the respective categories. For example,</p>

<pre><code>x &lt;- data.frame(Reviews = c(""The phone performance and display is good"",""Worth the money"",""Camera is good""))
</code></pre>

<p>The desired output is as below image</p>

<p><img src=""https://i.sstatic.net/4ogpX.png"" alt=""Please click for image""></p>

<p>I tried creating a dictionary as below using R's Quanteda package</p>

<pre><code>dic &lt;- dictionary(list(camera = c(""camera"",""lens"",""pixel"", ""pictures"", 
""pixels"", ""snap""), display = c(""resolution"", ""display"", ""depth"", ""mode"", 
""color"", ""colour"", ""discolour""), performance = c(""performance"", ""speed"", 
""usage"", ""fast"", ""run"", ""running"", ""lag"", ""processor"", ""shut"", ""shut down"", 
""restart"", ""hanging"",""hang""), Value = c(""money"", ""worth"", ""budget"", ""value"", 
""price"", ""specs"", ""specifications"", ""invest"", 
""under"",""expectations"",""expected"",""expecting"",""expect"")))
</code></pre>

<p>I would like to classify the texts based on keywords as stated above. Please help</p>

<p>P.S : dfm is one option. But particularly, I would like to know how to classify a data frame of texts as per the desired output.</p>
",Dataset Preprocessing & Handling,classify data frame character category using keywords r trying classify data frame customer review respective category example desired output image tried creating dictionary using r quanteda package would like classify text based keywords stated please help p dfm one option particularly would like know classify data frame text per desired output
Machine Learning/NLP text classification: training a model from corpus of text files - scikit learn,"<p>I am very new to machine learning and I was wondering if somebody could take me through this code and why it is not working. It is my own variation of the scikit-learn tutorial found at: <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a> which is basically what I am trying to do. I need to train a model with a labelled training set so that when I use my test set, it can predict the label of the test set. Also it would be really useful if somebody could show me how to save and load the model. Thank you very much. This is what I have so far:</p>

<pre><code>import codecs
import os

import numpy as np
import pandas as pd

from Text_Pre_Processing import Pre_Processing

filenames = os.listdir(
    ""...scikit-machine-learning/training_set"")
files = []
array_data = []
array_label = []
for file in filenames:
    with codecs.open(""...scikit-machine-learning/training_set/"" + file, ""r"",
                     encoding='utf-8', errors='ignore') as file_data:
        open_file = file_data.read()
        open_file = Pre_Processing.lower_case(open_file)
        open_file = Pre_Processing.remove_punctuation(open_file)
        open_file = Pre_Processing.clean_text(open_file)
        files.append(open_file)
# ----------------------------------------------------
# PUTTING LABELS INTO LIST
for file in files:
    if 'inheritance' in file:
        array_data.append(file)
        array_label.append('Inheritance (object-oriented programming)')
    elif 'pagerank' in file:
        array_data.append(file)
        array_label.append('PageRank')
    elif 'vector space model' in file:
        array_data.append(file)
        array_label.append('Vector Space Model')
    elif 'bayes' in file:
        array_data.append(file)
        array_label.append('Bayes' + ""'"" + ' Theorem')
    else:
        array_data.append(file)
        array_label.append('Dynamic programming')
#----------------------------------------------------------

csv_array = []
for i in range(0, len(array_data)):
    csv_array.append([array_data[i], array_label[i]])

# format of array [[string, label], [string, label], [string, label]]
import csv

with open('data.csv', 'w') as target:
    writer = csv.writer(target)
    writer.writerows(zip(test_array))

data = pd.read_csv('data.csv')
numpy_array = data.as_matrix()

X = numpy_array[:, 0]
Y = numpy_array[:, 1]

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=42)

from sklearn.feature_extraction.text import CountVectorizer

from sklearn.feature_extraction.text import TfidfTransformer

from sklearn.naive_bayes import MultinomialNB

from sklearn.pipeline import Pipeline

text_clf = Pipeline(['vect', CountVectorizer(stop_words='english'), 'tfidf', TfidfTransformer(),
                     'clf', MultinomialNB()])

text_clf = text_clf.fit(X_train, Y_train)

predicted = text_clf.predict(X_test)
np.mean(predicted == Y_test)
</code></pre>

<p>I saw online people using csv files to input the data so I tried that too, i may not need it so i apologise if that is incorrect. </p>

<p>error being shown: </p>

<pre><code>C:.../scikit-machine-learning/train.py:63: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.
  numpy_array = data.as_matrix()
Traceback (most recent call last):
  File ""C:/...scikit-machine-learning/train.py"", line 66, in &lt;module&gt;
    Y = numpy_array[:,1]
IndexError: index 1 is out of bounds for axis 1 with size 1
</code></pre>

<p>Thank you very much for your help, please let me know if you need further explanation.</p>

<p>sample of two entries in csv:</p>

<pre><code>""['dynamic programming is an algorithmic technique used to solve certain optimization problems where the object is to find the best solution from a number of possibilities it uses a so called bottomup approach meaning that the problem is solved as a set of subproblems which in turn are made up of subsubproblemssubproblems are then selected and used to solve the overall problem these subproblems are only solved once and the solutions are saved so that they will not need to be recalculated again whilst calculated individually they may also overlap when any subproblem is met again it can be found and reused to solve another problem since it searches all possibilities it is also very accurate this method is far more efficient than recalculating and therefore considerably reduces computation it is widely used in computer science and can be applied for example to compress data in high density bar codes dynamic programming is most effective and therefore most often used on objects that are ordered from left to right and whose order cannot be rearranged this means it works well on character chains for example ', 'Dynamic programming']""

""['inheritance is one of the basic concepts of object oriented programming its objective is to add more detail to preexisting classes whilst still allowing the methods and variables of these classes to be reused the easiest way to look at inheritance is as an is a kind of relationship for example a guitar is a kind of string instrument electric acoustic and steel stringed are all types of guitar the further down an inheritance tree you get the more specific the classes become an example here would be books books generally fall into two categories fiction and nonfiction each of these can then be subdivided into more groups fiction for example can be split into fantasy horror romance and many more nonfiction splits the same way into other topics such as history geography cooking etc history of course can be subdivided into time periods like the romans the elizabethans the world wars and so on', 'Inheritance (object-oriented programming)']""
</code></pre>
",Dataset Preprocessing & Handling,machine learning nlp text classification training model corpus text file scikit learn new machine learning wa wondering somebody could take code working variation scikit learn tutorial found basically trying need train model labelled training set use test set predict label test set also would really useful somebody could show save load model thank much far saw online people using csv file input data tried may need apologise incorrect error shown thank much help please let know need explanation sample two entry csv
spacy 2.0.12 / thinc 6.10.3 crashing django on heroku,"<p>I'm having an issue with <code>v2.0.12</code> that I've traced into <code>thinc</code>. <code>pip list</code> shows me:</p>

<pre><code>msgpack (0.5.6)
msgpack-numpy (0.4.3.1)
murmurhash (0.28.0)
regex (2017.4.5)
scikit-learn (0.19.2)
scipy (1.1.0)
spacy (2.0.12)
thinc (6.10.3)
</code></pre>

<p>I have code that works fine on my Mac, but fails in production. The stack trace goes into <code>spacy</code> and then into <code>thinc</code> -- and then django literally crashes. This all worked when I used an earlier version of spacy -- this has only come about since I'm attempting to upgrade to <code>v2.0.12</code>.</p>

<p>My requirements.txt file has these lines:</p>

<pre><code>regex==2017.4.5
spacy==2.0.12
scikit-learn==0.19.2
scipy==1.1.0
https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz
</code></pre>

<p>The last line pulls the <code>en_core_web_sm</code> down during deployment. I'm doing this so I can get those models loaded on Heroku during deployment. </p>

<p>I then load the parser like this:</p>

<pre><code>import en_core_web_sm
en_core_web_sm.load()
</code></pre>

<p>Then the stack trace shows the problem here in <code>thinc:</code></p>

<pre><code>File ""spacy/language.py"", line 352, in __call__
  doc = proc(doc)
File ""pipeline.pyx"", line 426, in spacy.pipeline.Tagger.__call__
File ""pipeline.pyx"", line 438, in spacy.pipeline.Tagger.predict
File ""thinc/neural/_classes/model.py"", line 161, in __call__
  return self.predict(x)
File ""thinc/api.py"", line 55, in predict
  X = layer(X)
File ""thinc/neural/_classes/model.py"", line 161, in __call__
  return self.predict(x)
File ""thinc/api.py"", line 293, in predict
  X = layer(layer.ops.flatten(seqs_in, pad=pad))
File ""thinc/neural/_classes/model.py"", line 161, in __call__
  eturn self.predict(x)
File ""thinc/api.py"", line 55, in predict
  X = layer(X)
File ""thinc/neural/_classes/model.py"", line 161, in __call__
  return self.predict(x)
File ""thinc/neural/_classes/model.py"", line 125, in predict
  y, _ = self.begin_update(X)
File ""thinc/api.py"", line 374, in uniqued_fwd
  Y_uniq, bp_Y_uniq = layer.begin_update(X_uniq, drop=drop)
File ""thinc/api.py"", line 61, in begin_update
  X, inc_layer_grad = layer.begin_update(X, drop=drop)
File ""thinc/neural/_classes/layernorm.py"", line 51, in begin_update
  X, backprop_child = self.child.begin_update(X, drop=0.)
File ""thinc/neural/_classes/maxout.py"", line 69, in begin_update
  output__boc = self.ops.batch_dot(X__bi, W)
File ""gunicorn/workers/base.py"", line 192, in handle_abort
  sys.exit(1)
</code></pre>

<p>Again -- this all works on my laptop.</p>

<p>Is there something wrong with how I'm loading? Or is my version of <code>thinc</code> out of date? If so, what should my <code>requirements.txt</code> file look like?</p>
",Dataset Preprocessing & Handling,spacy thinc crashing django heroku issue traced show code work fine mac fails production stack trace go django literally crash worked used earlier version spacy ha come since attempting upgrade requirement txt file ha line last line pull deployment get model loaded heroku deployment load parser like stack trace show problem work laptop something wrong loading version date file look like
How to search for specific n-grams in a corpus using R,"<p>I'm looking for specific n-grams in a corpus. Let's say I want to find 'asset management' and 'historical yield' in a collection of documents. </p>

<p>This is how I loaded the corpus</p>

<pre><code>my_corpus &lt;- VCorpus(DirSource(directory, pattern = "".pdf""), 
                 readerControl = list(reader = readPDF)
</code></pre>

<p>I cleaned the corpus and did some basic calculations using document term matrices. Now I want to look for particular expressions and put them in a dataframe. This is what I use (thanks to phiver):</p>

<pre><code>ngrams &lt;- c('asset management', 'historical yield')
dtm_ngrams &lt;- DocumentTermMatrix(my_corpus, control = list(dictionary = ngrams))
df_ngrams &lt;- data.frame(Docs = dtm$dimnames$Docs, as.matrix(dtm_ngrams), row.names = NULL )
</code></pre>

<p>This code runs, but the result is 0 for both n-grams. So, I'm guessing the problem is that the library is not defined correctly because R doesn't pick up the space between the words. So far, I tried to put '' between the words, or [:space:] and some other solutions. </p>
",Dataset Preprocessing & Handling,search specific n gram corpus using r looking specific n gram corpus let say want find asset management historical yield collection document loaded corpus cleaned corpus basic calculation using document term matrix want look particular expression put dataframe use thanks phiver code run result n gram guessing problem library defined correctly r pick space word far tried put word space solution
Python: Create a new variable derived from extracting a sentence from a text,"<p>I have a data frame which one of the variables is a fairly long paragraph containing many sentences. Sometimes the sentences are separated by a full stop sometimes by a comma. I'm trying to create a new variable by extracting only selected parts of the text using selected words. Please see below a short sample of the data frame the result I have at the moment, followed by the code I'm using. Note - the text in the first variable is pretty large.</p>

<pre><code>PhysicalMentalDemands           Physical_driving       Physical_telephones

[driving may be necessary       [driving......]        [telephones...]
occasionally. 
as well as telephones will also 
be occasional to frequent.]  
</code></pre>

<p><strong>Code used:</strong></p>

<pre><code>searched_words = ['driving' , 'telephones']

for i in searched_words:
  Test ['Physical' +""_""+  str(i)] = Test ['PhysicalMentalDemands'].apply(lambda text: [sent for sent in sent_tokenize(text)
                       if any(True for w in word_tokenize(sent) 
                                 if w.lower() in searched_words)])
</code></pre>

<p><strong>Issue:</strong></p>

<p>At the moment my code extract the sentences but extract using both of the words. I've seem other similar posts but none managed to solve my issue.</p>

<p><strong>Fixed</strong></p>

<p>searched_words = ['driving', 'physical']</p>

<pre><code>for i in searched_words:
df['Physical' + '_' + i] = result['PhysicalMentalDemands'].str.lower().apply(lambda text: [sent for sent in sent_tokenize(text) 
                                                           if i in word_tokenize(sent)])
</code></pre>
",Dataset Preprocessing & Handling,python create new variable derived extracting sentence text data frame one variable fairly long paragraph containing many sentence sometimes sentence separated full stop sometimes comma trying create new variable extracting selected part text using selected word please see short sample data frame result moment followed code using note text first variable pretty large code used issue moment code extract sentence extract using word seem similar post none managed solve issue fixed searched word driving physical
Using csv files to generate if conditions in python code?,"<p>I am working on a NLP problem where I am given a sentence and I have to print some parts of sentence according to a specific rule (for example for ""a truck is big"" I have to print Nouns and adjective so output will be ""truck big"") Till now I have used POS tagger to tag sentences and used hardcoded If conditions to display output.</p>

<pre><code>if arr[0][1]== 'DT' and arr[1][1]=='NN' and arr[2][1]=='VBZ' and arr[3][1]=='JJ':
    return arr[1][0] + "" ""+ arr[3][0]
</code></pre>

<p>Now I want to write rules in a text or csv and generate if conditions accordingly for example: if</p>

<p>DT NN VBZ JJ , NN JJ </p>

<p>is written in file it should display corresponding NN and JJ words (""truck big"" in this case). In simple words I need inputs of my present if conditions from txt file. I know it's a very specific problem but it would be a big help to me.</p>

<p>Edit: arr is list of tuples.</p>
",Dataset Preprocessing & Handling,using csv file generate condition python code working nlp problem given sentence print part sentence according specific rule example truck big print noun adjective output truck big till used po tagger tag sentence used hardcoded condition display output want write rule text csv generate condition accordingly example dt nn vbz jj nn jj written file display corresponding nn jj word truck big case simple word need input present condition txt file know specific problem would big help edit arr list tuples
Remove the most and the least appearing terms from a Term Document Matrix in R,"<p>I'm reading a Korean text file and trying to remove the most appearing terms(stopwords) and the least appearing terms from a Term Document Matrix which is generated in R. From the code below I'm able to get the TDM, but it has weights for all the terms in the document. Is there any way in which I can remove such terms so that I get the TDM for terms which would make more sense? Thanks</p>

<pre><code>library(ktm)
old &lt;- read_csv(file = ""Past-Korean1.csv"", locale = locale(date_names = ""ko"", 
encoding = ""UTF-8"")) 
q &lt;- tokenizer(old$Description, token = ""tag"")
y_ko &lt;- document_term_frequencies(q[, c(""text_id"", ""word"")])
tdm_ko &lt;- document_term_matrix(y_ko)
tdm_ko &lt;- as.DocumentTermMatrix(tdm_ko, weighting=weightTfIdf)
train1_ko &lt;- as.matrix(tdm_ko)
</code></pre>
",Dataset Preprocessing & Handling,remove least appearing term term document matrix r reading korean text file trying remove appearing term stopwords least appearing term term document matrix generated r code able get tdm ha weight term document way remove term get tdm term would make sense thanks
How to search for specific terms in a DTM,"<p>I have a dataset of 200+ pdf's that I converted into a corpus. I'm using the TM package for R for text pre-processing and mining. 
So far, I've successfully created the DTM (document term matrix) and can find the x most frequently occuring terms.
The goal of my research however, is to check if certain terms are used in the corpus. I'm not so much looking for the most frequent terms, but have my own list of terms that I want to check if they occur, and if so, how many times.</p>

<p>So far, I've tried this:</p>

<pre><code>function &lt;- content_transformer(function(x, pattern)regmatches(x,gregexpr(pattern, x, perl=TRUE, ignore.case = TRUE)))
keep = ""word_1|word_2""
tm_map(my_corpus, function, keep)[[1]]
</code></pre>

<p>and these:</p>

<pre><code>str_detect(my_corpus, ""word_1"", ""word_2"" )
str_locate_all(my_corpus, ""word_1"", ""word_2"")
str_extract(my_corpus, ""funds"")
</code></pre>

<p>This last one seems to come closest giving the output:
    [1] ""funds"" NA      NA </p>

<p>Neither seems to be giving me what I need.</p>
",Dataset Preprocessing & Handling,search specific term dtm dataset pdf converted corpus using tm package r text pre processing mining far successfully created dtm document term matrix find x frequently occuring term goal research however check certain term used corpus much looking frequent term list term want check occur many time far tried last one seems come closest giving output fund na na neither seems giving need
Delete first column and then take them as a index pandas,"<p>I have a <code>word2vec</code> dataframe like this which saved from save_word2vec_format using Gensim under txt file. After using pandas to read this file. (Picture below). How to delete first row and make them as a index?
My txt file: <a href=""https://drive.google.com/file/d/1O206N93hPSmvMjwc0W5ATyqQMdMwhRlF/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1O206N93hPSmvMjwc0W5ATyqQMdMwhRlF/view?usp=sharing</a>
<a href=""https://i.sstatic.net/5qS8O.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5qS8O.png"" alt=""enter image description here""></a></p>
",Dataset Preprocessing & Handling,delete first column take index panda dataframe like saved save word vec format using gensim txt file using panda read file picture delete first row make index txt file
NLP text tagging,"<p>I am a newbie in NLP, just doing it for the first time. 
I am trying to solve a problem. </p>

<p>My problem is I have some documents which are manually tagged like:</p>

<pre><code>doc1 - categoryA, categoryB
doc2 - categoryA, categoryC
doc3 - categoryE, categoryF, categoryG
.
.
.
.
docN - categoryX
</code></pre>

<p>Here I have a fixed set of categories and any document can have any number of tags associated with it.
I want to train the classifier using this input, so that this tagging process can be automated.</p>

<p>Thanks</p>
",Dataset Preprocessing & Handling,nlp text tagging newbie nlp first time trying solve problem problem document manually tagged like fixed set category document number tag associated want train classifier using input tagging process automated thanks
RAKE split sentences function on a Python dictionary,"<p>How would I be able to apply this function to just the values within a python dictionary:</p>

<pre><code>def split_sentences(text):
""""""
Utility function to return a list of sentences.
@param text The text that must be split in to sentences.
""""""
sentence_delimiters = re.compile(u'[\\[\\]\n.!?,;:\t\\-\\""\\(\\)\\\'\u2019\u2013]')

sentences = (sentence_delimiters.split(text))
return sentences
</code></pre>

<p>The code I have used to create the dictionary from a CSV file input:</p>

<pre><code>with open('second_table.csv', mode='r') as infile:
    #Read in the csv file
    reader = csv.reader(infile)
    #Skip the headers
    next(reader, None)
    #Iterates through each row to get the key value pairs
    mydict = {rows[0]:rows[1] for rows in reader}
</code></pre>

<p>The python dictionary looks like so:</p>

<pre><code>{'INC000007581947': '$BREM - CATIAV5 - Catia does not start',
 'INC000007581991': '$SPAI - REACT - react',
 'INC000007582037': 'access request',
 'INC000007582095': '$HAMB - DVOBROWSER - ACCESS RIGHTS',
 'INC000007582136': 'SIGLUM issue by opening a REACT request'}
</code></pre>
",Dataset Preprocessing & Handling,rake split sentence function python dictionary would able apply function value within python dictionary code used create dictionary csv file input python dictionary look like
"Web-scraping text from unstructured webpages, using beautifulSoup","<p>I'm looking to take all the relevant text sections of text of certain web pages and parse it into a structured format, e.g. a CSV file for later use. 
However, the web pages I want to take info from don't strictly follow the same format, for example, the pages:</p>

<p><code>http://www.cs.bham.ac.uk/research/groupings/machine-learning/
http://www.cs.bham.ac.uk/research/groupings/robotics/
http://www.cs.bham.ac.uk/research/groupings/reasoning/
</code></p>

<p>I have been using BeautifulSoup and this has been fine for the web pages that follow a well-defined format, but these particular websites don't follow a standard format. 
How can I write my code to extract the main text from these pages? 
Could I either extract all the text and strip away the irrelevant/commonly occurring text?<br>
Or can I somehow select these larger text bodies even though they don't occur uniformly? 
The websites are formatted differently but not in such a convoluted way that I think this is impossible? </p>

<p>Originally I had code like this for dealing with the structured pages:</p>

<pre><code>from urllib.request import urlopen
from bs4 import BeautifulSoup
import sqlite3

conn = sqlite3.connect('/Users/tom/PycharmProjects/tmc765/Parsing/MScProject.db')
c = conn.cursor()


### Specify URL
programme_list = [""http://www.cs.bham.ac.uk/internal/programmes/2017/0144"",
              ""http://www.cs.bham.ac.uk/internal/programmes/2017/9502"",
              ""http://www.cs.bham.ac.uk/internal/programmes/2017/452B"",
              ""http://www.cs.bham.ac.uk/internal/programmes/2017/4436"",
              ""http://www.cs.bham.ac.uk/internal/programmes/2017/5914"",
              ""http://www.cs.bham.ac.uk/internal/programmes/2017/9503"",
              ""http://www.cs.bham.ac.uk/internal/programmes/2017/9499"",
              ""http://www.cs.bham.ac.uk/internal/programmes/2017/5571"",
              ""http://www.cs.bham.ac.uk/internal/programmes/2017/5955"",
              ""http://www.cs.bham.ac.uk/internal/programmes/2017/4443"",
              ""http://www.cs.bham.ac.uk/internal/programmes/2017/9509"",
              ""http://www.cs.bham.ac.uk/internal/programmes/2017/5576"",
              ""http://www.cs.bham.ac.uk/internal/programmes/2017/9501"",
              ""http://www.cs.bham.ac.uk/internal/programmes/2017/4754"",
              ""http://www.cs.bham.ac.uk/internal/programmes/2017/5196""]

for programme_page in programme_list:
# Query page, return html to a variable
page = urlopen(programme_page)

soupPage = BeautifulSoup(page, 'html.parser')

name_box = soupPage.find('h1')
Programme_Identifier = name_box.text.strip()

Programme_Award = soupPage.find(""td"", text=""Final Award"").find_next_sibling(""td"").text
Interim_Award = soupPage.find(""td"", text=""Interim Award"")
if Interim_Award is not None:
    Interim_Award = Interim_Award = soupPage.find(""td"", text=""Interim Award"").find_next_sibling(""td"").text
Programme_Title = soupPage.find(""td"", text=""Programme Title"").find_next_sibling(""td"").text
School_Department = soupPage.find(""td"", text=""School/Department"").find_next_sibling(""td"").text
Banner_Code = soupPage.find(""td"", text=""Banner Code"").find_next_sibling(""td"").text
Programme_Length = soupPage.find(""td"", text=""Length of Programme"").find_next_sibling(""td"").text
Total_Credits = soupPage.find(""td"", text=""Total Credits"").find_next_sibling(""td"").text
UCAS_Code = soupPage.find(""td"", text=""UCAS Code"").find_next_sibling(""td"").text
Awarding_Institution = soupPage.find(""td"", text=""Awarding Institution"").find_next_sibling(""td"").text
QAA_Benchmarking_Groups = soupPage.find(""td"", text=""QAA Benchmarking Groups"").find_next_sibling(""td"").text

#SQL code for inserting into database
with conn:
    c.execute(""INSERT INTO Programme_Pages VALUES (?,?,?,?,?,?,?,?,?,?,?,?)"",
              (Programme_Identifier, Programme_Award, Interim_Award, Programme_Title,
               School_Department, Banner_Code, Programme_Length, Total_Credits,
               UCAS_Code, Awarding_Institution, QAA_Benchmarking_Groups, programme_page))

print(""Program Title:           "", Programme_Identifier)
print(""Program Award:           "", Programme_Award)
print(""Interim Award:           "", Interim_Award)
print(""Program Title:           "", Programme_Title)
print(""School/Department:       "", School_Department)
print(""Banner Code:             "", Banner_Code)
print(""Length of Program:       "", Programme_Length)
print(""Total Credits:           "", Total_Credits)
print(""UCAS Code:               "", UCAS_Code)
print(""Awarding Institution:    "", Awarding_Institution)
print(""QAA Benchmarking Groups: "", QAA_Benchmarking_Groups)
print(""~~~~~~~~~~\n~~~~~~~~~~"")

Educational_Aims = soupPage.find('div', {""class"": ""programme-text-block""})
Educational_Aims_Title = Educational_Aims.find('h2')
Educational_Aims_Title = Educational_Aims_Title.text.strip()

Educational_Aims_List = Educational_Aims.findAll(""li"")
print(Educational_Aims_Title)
for el in Educational_Aims_List:
    text = el.text.strip()
    with conn:
        c.execute(""INSERT INTO Programme_Info VALUES (?,?,?,?)"", (Programme_Identifier, text,
                  Educational_Aims_Title, programme_page))
    print(el.text.strip())
</code></pre>

<p>However, I've not found a way yet to write a script to pull out the relevant text from the unstructured pages I've linked above. 
I was considering trying to pull all the sections tagged <p> and then processing them as they come. 
I just thought someone might have any insight on an easier way. </p>
",Dataset Preprocessing & Handling,web scraping text unstructured webpage using beautifulsoup looking take relevant text section text certain web page parse structured format e g csv file later use however web page want take info strictly follow format example page using beautifulsoup ha fine web page follow well defined format particular website follow standard format write code extract main text page could either extract text strip away irrelevant commonly occurring text somehow select larger text body even though occur uniformly website formatted differently convoluted way think impossible originally code like dealing structured page however found way yet write script pull relevant text unstructured page linked wa considering trying pull section tagged processing come thought someone might insight easier way
removing words from a list from pandas column - python 2.7,"<p>I have a text file which contains some strings that I want to remove from my data frame. The data frame observations contains those texts which are present in the ext file.</p>

<p>here is the text file - <a href=""https://drive.google.com/open?id=1GApPKvA82tx4CDtlOTqe99zKXS3AHiuD"" rel=""nofollow noreferrer"">https://drive.google.com/open?id=1GApPKvA82tx4CDtlOTqe99zKXS3AHiuD</a></p>

<p>here is the link; Data = <a href=""https://drive.google.com/open?id=1HJbWTUMfiBV54EEtgSXTcsQLzQT1rFgz"" rel=""nofollow noreferrer"">https://drive.google.com/open?id=1HJbWTUMfiBV54EEtgSXTcsQLzQT1rFgz</a></p>

<p>I am using the following code - </p>

<pre><code>import nltk
from nltk.tokenize import word_tokenize 
file = open(""D://Users/Shivam/Desktop/rahulB/fliter.txt"")
result = file.read()
words = word_tokenize(result)
</code></pre>

<p>I loaded the text files and converted them into words/tokens.</p>

<p>Its is my dataframe.</p>

<pre><code>text
0   What Fresh Hell Is This? January 31, 2018 ...A...
1   What Fresh Hell Is This? February 27, 2018 My ...
2   What Fresh Hell Is This? March 31, 2018 Trump ...
3   What Fresh Hell Is This? April 29, 2018 Michel...
4   Join Email List Contribute Join AMERICAblog Ac...
</code></pre>

<p>If you see this, these texts are present in the all rows such as ""What Fresh Hell Is This?"" or ""Join Email List Contribute Join AMERICAblog Ac, ""Sign in Daily Roundup MS Legislature Elected O"" etc.</p>

<p>I used this for loop</p>

<pre><code>for word in words:
    df['text'].replace(word, ' ')
</code></pre>

<p>my error.</p>

<pre><code>error                                     Traceback (most recent call last)
&lt;ipython-input-168-6e0b8109b76a&gt; in &lt;module&gt;()
----&gt; 1 df['text'] = df['text'].str.replace(""|"".join(words), "" "")

D:\Users\Shivam\Anaconda2\lib\site-packages\pandas\core\strings.pyc in replace(self, pat, repl, n, case, flags)
   1577     def replace(self, pat, repl, n=-1, case=None, flags=0):
   1578         result = str_replace(self._data, pat, repl, n=n, case=case,
-&gt; 1579                              flags=flags)
   1580         return self._wrap_result(result)
   1581 

D:\Users\Shivam\Anaconda2\lib\site-packages\pandas\core\strings.pyc in str_replace(arr, pat, repl, n, case, flags)
    422     if use_re:
    423         n = n if n &gt;= 0 else 0
--&gt; 424         regex = re.compile(pat, flags=flags)
    425         f = lambda x: regex.sub(repl=repl, string=x, count=n)
    426     else:

D:\Users\Shivam\Anaconda2\lib\re.pyc in compile(pattern, flags)
    192 def compile(pattern, flags=0):
    193     ""Compile a regular expression pattern, returning a pattern object.""
--&gt; 194     return _compile(pattern, flags)
    195 
    196 def purge():

D:\Users\Shivam\Anaconda2\lib\re.pyc in _compile(*key)
    249         p = sre_compile.compile(pattern, flags)
    250     except error, v:
--&gt; 251         raise error, v # invalid expression
    252     if not bypass_cache:
    253         if len(_cache) &gt;= _MAXCACHE:

error: nothing to repeat
</code></pre>
",Dataset Preprocessing & Handling,removing word list panda column python text file contains string want remove data frame data frame observation contains text present ext file text file link data using following code loaded text file converted word token dataframe see text present row fresh hell join email list contribute join americablog ac sign daily roundup legislature elected etc used loop error
"How to read PDF files which are in asian languages (Chinese, Japanese, Thai, etc.) and store in a string in python","<p>I am using PyPDF2 to read PDF files in python. While it works well for languages in English and European languages (with alphabets in english), the library fails to read Asian languages like Japanese and Chinese. I tried <code>encode('utf-8')</code>, <code>decode('utf-8')</code> but nothing seems to work. It just prints a blank string on extraction of the text.</p>

<p>I have tried other libraries like textract and PDFMiner but no success yet.</p>

<p>When I copy the text from PDF and paste it on a notebook, the characters turn into some random format text (probably in a different encoding). </p>

<pre><code>def convert_pdf_to_text(filename):
    text = ''
    pdf = PyPDF2.PdfFileReader(open(filename, ""rb""))
    if pdf.isEncrypted:
        pdf.decrypt('')
    for page in pdf.pages:
        text = text + page.extractText()
    return text
</code></pre>

<p>Can anyone point me in the right direction?</p>
",Dataset Preprocessing & Handling,read pdf file asian language chinese japanese thai etc store string python using pypdf read pdf file python work well language english european language alphabet english library fails read asian language like japanese chinese tried nothing seems work print blank string extraction text tried library like textract pdfminer success yet copy text pdf paste notebook character turn random format text probably different encoding anyone point right direction
Why is converting words into singular from plural in a for loop taking so long (Python 3)?,"<p>This is my code to read text from a CSV file and convert all the words in a column of it into singular form from plural:</p>

<pre><code>import pandas as pd
from textblob import TextBlob as tb
data = pd.read_csv(r'path\to\data.csv')

for i in range(len(data)):
    blob = tb(data['word'][i])
    singular = blob.words.singularize()  # This makes singular a list
    data['word'][i] = ''.join(singular)  # Converting the list back to a string
</code></pre>

<p>But this code has been running for minutes now (and possibly keep running for hours, if I don't stop it?)! Why is that? When I checked for few words individually, the conversion happens instantly - doesn't take any time at all. There are only 1060 rows (words to convert) in the file.</p>

<p><strong>EDIT:</strong> It finished running in about 10-12 minutes.</p>

<p>Here's some sample data:</p>

<p>Input:</p>

<pre><code>word
development
investment
funds
slow
company
commit
pay
claim
finances
customers
claimed
insurance
comment
rapid
bureaucratic
affairs
reports
policyholders
detailed
</code></pre>

<p>Output:</p>

<pre><code>word
development
investment
fund
slow
company
commit
pay
claim
finance
customer
claimed
insurance
comment
rapid
bureaucratic
affair
report
policyholder
detailed
</code></pre>
",Dataset Preprocessing & Handling,converting word singular plural loop taking long python code read text csv file convert word column singular form plural code ha running minute possibly keep running hour stop checked word individually conversion happens instantly take time row word convert file edit finished running minute sample data input output
fastText and word2vec: NaNs in accuracy computation code,"<p>I downloaded the pre-trained English Wikipedia vectors file (<code>wiki.en.vec</code>) from the fastText Github repository page, and I tried to compute the syntactic and semantic analogy task accuracies as described in the first of Mikolov's word2vec papers as follows:</p>

<p>I built the word2vec repository by simply doing <code>make</code>.</p>

<p>I ran <code>./compute-accuracy wiki.en.vec 0 &lt; questions-words.txt</code>, i.e., I pass the pre-trained vectors file to the compute-accuracy binary from word2vec along with a threshold of 0 in order to consider the entire vocabulary instead of by default restricting it to 30000, and I also send in the accuracy computation dataset <code>questions-words.txt</code> using <code>&lt;</code> because I noticed that the code reads the dataset from stdin.</p>

<p>In response, I simply get a bunch of NaNs like below. This doesn't change even if I change the threshold value to 30000 or anything else.</p>

<pre><code>&gt;capital-common-countries:
ACCURACY TOP1: 0.00 % (0 / 1)
Total accuracy: -nan % Semantic accuracy: -nan % Syntactic accuracy: -nan %
</code></pre>

<p>Can someone please explain why the English pre-trained vectors don't seem to work with word2vec's accuracy computation code? I took a look at <code>compute-accuracy.c</code> and it does look like it expects standard vector file formatting convention and I took a look at <code>wiki.en.vec</code> as well, and it does look like it is formatted in standard convention.</p>

<p>Also, in the fastText paper, word analogy accuracies with fastText vectors are presented and the paper cites Mikolov's word2vec paper there -- clearly, the same dataset was used, and presumably the same word2vec <code>compute-accuracy.c</code> file was used to obtain the presented numbers. So could someone please explain what's going wrong?</p>
",Dataset Preprocessing & Handling,fasttext word vec nan accuracy computation code downloaded pre trained english wikipedia vector file fasttext github repository page tried compute syntactic semantic analogy task accuracy described first mikolov word vec paper follows built word vec repository simply ran e pas pre trained vector file compute accuracy binary word vec along threshold order consider entire vocabulary instead default restricting also send accuracy computation dataset using noticed code read dataset stdin response simply get bunch nan like change even change threshold value anything else someone please explain english pre trained vector seem work word vec accuracy computation code took look doe look like expects standard vector file formatting convention took look well doe look like formatted standard convention also fasttext paper word analogy accuracy fasttext vector presented paper cite mikolov word vec paper clearly dataset wa used presumably word vec file wa used obtain presented number could someone please explain going wrong
Computing top n word pair co-occurrences from document term matrix,"<p>I used gensim to create a bag of words model. Although it is much longer in reality, here is the format outputted when creating a bag of words document-term matrix on the tokenized texts using Gensim:</p>

<pre><code>id2word = corpora.Dictionary(texts)
corpus = [id2word.doc2bow(text) for text in texts]

[[(0, 2),
  (1, 1),
  (2, 1),
  (3, 1),
  (4, 11),
  (385, 1),
  (386, 2),
  (387, 3),
  (388, 1),
  (389, 1),
  (390, 1)],
 [(4, 31),
  (8, 2),
  (13, 2),
  (16, 2),
  (17, 2),
  (26, 1),
  (28, 4),
  (29, 1),
  (30, 1)]]
</code></pre>

<p>This is a sparse matrix representation, and from what I understand other libraries represent the document-term matrix in a similar fashion as well. If the document-term matrix is non-sparse (meaning the zero entries are there as well), I know that I just have to (A.T*A), since A is of dimension (num. of documents by num. of terms), so multiplying the two will give the term co-occurrences. Ultimately, I want to get the top n co-occurrences (so get the top n term pairs that occur together in the same texts). How would I achieve this? I am not attached to Gensim for creating the BOW model. If another library like sklearn can do it more easily, I am very open. I would appreciate any advice/help/code with this problem -- thanks!</p>
",Dataset Preprocessing & Handling,computing top n word pair co occurrence document term matrix used gensim create bag word model although much longer reality format outputted creating bag word document term matrix tokenized text using gensim sparse matrix representation understand library represent document term matrix similar fashion well document term matrix non sparse meaning zero entry well know since dimension num document num term multiplying two give term co occurrence ultimately want get top n co occurrence get top n term pair occur together text would achieve attached gensim creating bow model another library like sklearn easily open would appreciate advice help code problem thanks
Summarizing huge amounts of data,"<p>I have a problem that I have not been able to solve. I have 4 <code>.txt</code> files each between 30-70GB. Each file contains n-gram entries as follows:</p>

<pre><code>blabla1/blabla2/blabla3
word1/word2/word3
...
</code></pre>

<p>What I'm trying to do is count how many times each item appear, and save this data to a new file, e.g:</p>

<pre><code>blabla1/blabla2/blabla3  : 1
word1/word2/word3        : 3
...
</code></pre>

<p>My attempts so far has been simply to save all entries in a dictionary and count them, i.e. </p>

<pre><code>entry_count_dict = defaultdict(int)
with open(file) as f:
    for line in f:
        entry_count_dict[line] += 1
</code></pre>

<p>However, using this method I run into memory errors (I have 8GB RAM available). The data follows a zipfian distribution, e.g. the majority of the items occur only once or twice.
The total number of entries is unclear, but a (very) rough estimate is that there is somewhere around 15,000,000 entries in total.</p>

<p>In addition to this, I've tried <code>h5py</code> where all the entries are saved as a h5py dataset containing the array <code>[1]</code>, which is then updated, e.g:</p>

<pre><code>import h5py
import numpy as np

entry_count_dict = h5py.File(filename)
with open(file) as f:
    for line in f:
        if line in entry_count_dict:
            entry_count_file[line][0] += 1
        else:
            entry_count_file.create_dataset(line, 
                                            data=np.array([1]),
                                            compression=""lzf"")
</code></pre>

<p>However, this method is way to slow. The writing speed gets slower and slower. As such, unless the writing speed can be increased this approach is implausible. Also, processing the data in chunks and opening/closing the h5py file for each chunk did not show any significant difference in processing speed.</p>

<p>I've been thinking about saving entries which start with certain letters in separate files, i.e. all the entries which start with <code>a</code> are saved in <code>a.txt</code>, and so on (this should be doable using <code>defaultdic(int)</code>).
However, to do this the file have to iterated once for every letter, which is implausible given the file sizes (max = 69GB). 
Perhaps when iterating over the file, one could open a pickle and save the entry in a dict, and then close the pickle. But doing this for each item slows down the process quite a lot due to the time it takes to open, load and close the pickle file.</p>

<p>One way of solving this would be to sort all the entries during one pass, then iterate over the sorted file and count the entries alphabetically. However, even sorting the file is painstakingly slow using the linux command:</p>

<p><code>sort file.txt &gt; sorted_file.txt</code></p>

<p>And, I don't really know how to solve this using python given that loading the whole file into memory for sorting would cause memory errors. I have some superficial knowledge of different sorting algorithms, however they all seem to require that the whole object to be sorted needs get loaded into memory. </p>

<p>Any tips on how to approach this would be much appreciated. </p>
",Dataset Preprocessing & Handling,summarizing huge amount data problem able solve file gb file contains n gram entry follows trying count many time item appear save data new file e g attempt far ha simply save entry dictionary count e however using method run memory error gb ram available data follows zipfian distribution e g majority item occur twice total number entry unclear rough estimate somewhere around entry total addition tried entry saved h py dataset containing array updated e g however method way slow writing speed get slower slower unless writing speed increased approach implausible also processing data chunk opening closing h py file chunk show significant difference processing speed thinking saving entry start certain letter separate file e entry start saved doable using however file iterated every letter implausible given file size max gb perhaps iterating file one could open pickle save entry dict close pickle item slows process quite lot due time take open load close pickle file one way solving would sort entry one pas iterate sorted file count entry alphabetically however even sorting file painstakingly slow using linux command really know solve using python given loading whole file memory sorting would cause memory error superficial knowledge different sorting algorithm however seem require whole object sorted need get loaded memory tip approach would much appreciated
How do I convert multiple pdf&#39;s into a corpus for text analysis in R?,"<p>I have a very basic question because I'm an absolute beginner. I've tried to find help online and read different tutorials and handbooks, but can't find the answer.</p>

<p>My project is very simple. I have dozens of pdf's (stored in a folder) that I want to analyse for their contents (unsupervised learning). The ultimate goal is a topic analysis. Now here's the problem: every guide I can find jumps right into pre-processing of these texts without going over the first steps of loading these files into R and defining the corpus.</p>

<p>So, basically, I want to break down all these pdf's in a dataframe for analysis but I'm missing the first step of loading these in R.</p>

<p>Any help would be greatly appreciated.</p>
",Dataset Preprocessing & Handling,convert multiple pdf corpus text analysis r basic question absolute beginner tried find help online read different tutorial handbook find answer project simple dozen pdf stored folder want analyse content unsupervised learning ultimate goal topic analysis problem every guide find jump right pre processing text without going first step loading file r defining corpus basically want break pdf dataframe analysis missing first step loading r help would greatly appreciated
SolrCloud OpenNLP error Can&#39;t find resource &#39;opennlp/en-sent.bin&#39; in classpath or &#39;/configs/_default&#39;,"<p>I have error when using Apache OpenNLP with Solr (ver. 7.3.0) in Cloud mode. 
When I add field type to <em>managed-schema</em> using open nlp like this:</p>

<pre><code>&lt;fieldType name=""text_opennlp"" class=""solr.TextField""&gt;
      &lt;analyzer&gt;
        &lt;tokenizer class=""solr.OpenNLPTokenizerFactory""
          sentenceModel=""opennlp/en-sent.bin""
          tokenizerModel=""opennlp/en-token.bin""
        /&gt;
      &lt;/analyzer&gt;
    &lt;/fieldType&gt;

    &lt;field name=""content"" type=""text_opennlp"" indexed=""true"" termOffsets=""true"" stored=""true"" termPayloads=""true"" termPositions=""true"" docValues=""false"" termVectors=""true"" multiValued=""true"" required=""true""/&gt;
</code></pre>

<p>and I have following error:</p>

<blockquote>
  <p>test_collection_shard1_replica_n1:  org.apache.solr.common.SolrException:org.apache.solr.common.SolrException: Could not load conf for core test_collection_shard1_replica_n1: Can't load  schema managed-schema: org.apache.solr.core.SolrResourceNotFoundException:  Can't find resource 'opennlp/en-sent.bin' in classpath or '/configs/_default',  cwd=D:\utils\solr-7.3.0-7\solr-7.3.0-7\server Please check your logs for more information</p>
</blockquote>

<p>I have downloaded solr var 7.3.0-7 from <a href=""https://builds.apache.org/job/Solr-Artifacts-7.3/lastSuccessfulBuild/artifact/solr/package/"" rel=""nofollow noreferrer"">https://builds.apache.org/job/Solr-Artifacts-7.3/lastSuccessfulBuild/artifact/solr/package/</a> </p>

<p>I tried to put model files to: <code>D:\utils\solr-7.3.0-7\solr-7.3.0-7\server</code> but it didn't help. </p>

<p>Here you are my related question: <a href=""https://stackoverflow.com/questions/49433989/can-not-apply-patch-lucene-2899-patch-to-solr-on-windows/49435028?noredirect=1#comment85988550_49435028"">Can not apply patch LUCENE-2899.patch to SOLR on Windows</a></p>

<p>Maybe I need to put model files somewhere else?</p>
",Dataset Preprocessing & Handling,solrcloud opennlp error find resource opennlp en sent bin classpath configs default error using apache opennlp solr ver cloud mode add field type managed schema using open nlp like following error test collection shard replica n org apache solr common solrexception org apache solr common solrexception could load conf core test collection shard replica n load schema managed schema org apache solr core solrresourcenotfoundexception find resource opennlp en sent bin classpath configs default cwd utils solr solr server please check log information downloaded solr var tried put model file help related question href apply patch lucene patch solr window maybe need put model file somewhere else
How to find the trending phrases or most frequent phrases from text data in Python?,"<p>I have text data in the form of pandas data frame object. Each row of the text column contains single or multiple sentences. I would like to find the most frequent or trending phrases from the text. </p>
",Dataset Preprocessing & Handling,find trending phrase frequent phrase text data python text data form panda data frame object row text column contains single multiple sentence would like find frequent trending phrase text
Fixing error output from seq2seq model,"<p>I want to ask you how we can effectively re-train a trained seq2seq model to remove/mitigate a specific observed error output. I'm going to give an example about Speech Synthesis, but any idea from different domains, such as Machine Translation and Speech Recognition, using seq2seq model will be appreciated.</p>

<p>I learned the basics of seq2seq with attention model, especially for Speech Synthesis such as <a href=""https://github.com/Rayhane-mamah/Tacotron-2"" rel=""nofollow noreferrer"">Tacotron-2</a>.
Using a distributed well-trained model showed me how naturally our computer could speak with the seq2seq (end-to-end) model (you can listen to some audio samples <a href=""https://r9y9.github.io/blog/2018/05/20/tacotron2/"" rel=""nofollow noreferrer"">here</a>). But still, the model fails to read some words properly, e.g., it fails to read ""obey [əˈbā]"" in multiple ways like [əˈbī] and [əˈbē].</p>

<p>The reason is obvious because the word ""obey"" appears too little, only three times out of 225,715 words, in our dataset (<a href=""https://keithito.com/LJ-Speech-Dataset/"" rel=""nofollow noreferrer"">LJ Speech</a>), and the model had no luck.</p>

<p>So, how can we re-train the model to overcome the error? Adding extra audio clips containing the ""obey"" pronunciation sounds impractical, but reusing the three audio clips has the danger of overfitting. And also, I suppose we use a well-trained model and ""simply training more"" is not an effective solution.</p>

<p>Now, this is one of the drawbacks of seq2seq model, which is not talked much. The model successfully simplified the pipelines of the traditional models, e.g., for Speech Synthesis, it replaced an acoustic model and a text analysis frontend etc by a single neural network. But we lost the controllability of our model at all. It's impossible to make the system read in a specific way.</p>

<p>Again, if you use a seq2seq model in any field and get an undesirable output, how do you fix that? Is there a data-scientific workaround to this problem, or maybe a cutting-edge Neural Network mechanism to gain more controllability in seq2seq model?</p>

<p>Thanks.</p>
",Dataset Preprocessing & Handling,fixing error output seq seq model want ask effectively train trained seq seq model remove mitigate specific observed error output going give example speech synthesis idea different domain machine translation speech recognition using seq seq model appreciated learned basic seq seq attention model especially speech synthesis tacotron using distributed well trained model showed naturally computer could speak seq seq end end model listen audio sample still model fails read word properly e g fails read obey b multiple way like b b reason obvious word obey appears little three time word dataset lj speech model luck train model overcome error adding extra audio clip containing obey pronunciation sound impractical reusing three audio clip ha danger overfitting also suppose use well trained model simply training effective solution one drawback seq seq model talked much model successfully simplified pipeline traditional model e g speech synthesis replaced acoustic model text analysis frontend etc single neural network lost controllability model impossible make system read specific way use seq seq model field get undesirable output fix data scientific workaround problem maybe cutting edge neural network mechanism gain controllability seq seq model thanks
Python SpaCy Create nlp Document - Argument &#39;string&#39; has incorrect type,"<p>I'm relatively new to Python NLP and I am trying to process a CSV file with SpaCy. I'm able to load the file just fine using Pandas, but when I attempt to process it with SpaCy's nlp function, the compiler errors out approximately 5% of the way through the file's contents.</p>

<p>Code block follows:</p>

<pre><code>import pandas as pd
df = pd.read_csv('./reviews.washington.dc.csv')

import spacy
nlp = spacy.load('en')

for parsed_doc in nlp.pipe(iter(df['comments']), batch_size=1, n_threads=4):
    print (parsed_doc.text)
</code></pre>

<p>I've also tried:</p>

<pre><code>df['parsed'] = df['comments'].apply(nlp)
</code></pre>

<p>with the same result.</p>

<p>The traceback I'm receiving is:</p>

<pre><code>Traceback (most recent call last):
    File ""/Users/john/Downloads/spacy_load.py"", line 11, in &lt;module&gt;
        for parsed_doc in nlp.pipe(iter(df['comments']), batch_size=1,
        n_threads=4):
    File ""/usr/local/lib/python3.6/site-packages/spacy/language.py"",
        line 352, in pipe for doc in stream:
    File ""spacy/syntax/parser.pyx"", line 239, in pipe
        (spacy/syntax/parser.cpp:8912)
    File ""spacy/matcher.pyx"", line 465, in pipe (spacy/matcher.cpp:9904)
    File ""spacy/syntax/parser.pyx"", line 239, in pipe (spacy/syntax/parser.cpp:8912)
    File ""spacy/tagger.pyx"", line 231, in pipe (spacy/tagger.cpp:6548)
    File ""/usr/local/lib/python3.6/site-packages/spacy/language.py"", line 345,
        in &lt;genexpr&gt; stream = (self.make_doc(text) for text in texts)
    File ""/usr/local/lib/python3.6/site-packages/spacy/language.py"", line 293,
        in &lt;lambda&gt; self.make_doc = lambda text: self.tokenizer(text)
    TypeError: Argument 'string' has incorrect type (expected str, got float)
</code></pre>

<p>Can anyone shed some light on why this is happening, as well as how I might work around it? I've tried various workarounds from the site to no avail. Try/except blocks have had no effect, either.</p>
",Dataset Preprocessing & Handling,python spacy create nlp document argument string ha incorrect type relatively new python nlp trying process csv file spacy able load file fine using panda attempt process spacy nlp function compiler error approximately way file content code block follows also tried result traceback receiving anyone shed light happening well might work around tried various workarounds site avail try except block effect either
Best way to replace sentences/paragraphs with a string in python,"<p>How would I replace all the sentences and paragraphs with a <code>&lt;string&gt;</code> tag in text files? </p>

<p>I want to keep spacing, tabs, and lists in the text document intact:</p>

<p>Example input:</p>

<pre><code>Clause 1:

  a) detail 1. some more about detail 1. Here is more information about this paragraph right here. There is more information that we think sometimes.

  b) detail 2. some more about detail 2. and some more..
</code></pre>

<p>Example output:</p>

<pre><code>&lt;string&gt;

  a) &lt;string&gt;

  b) &lt;string&gt;
</code></pre>
",Dataset Preprocessing & Handling,best way replace sentence paragraph string python would replace sentence paragraph tag text file want keep spacing tab list text document intact example input example output
How to extract the body of a Section in a word document? Python,"<p>I have a document with the following structure.</p>

<p><strong><em>INPUT:</em></strong>
<a href=""https://i.sstatic.net/tIjDH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tIjDH.png"" alt=""Screenshot""></a></p>

<p><strong><em>OUTPUT:</em></strong></p>

<pre><code>Text
Text
Text
I want to extract this body
</code></pre>

<p>In the above image, I want to extract the text of Article I and II in Python. 
These Articles are actually sections, as you can see it on the left Navigation. Those are not raw text, I cannot select the Article, it is just like bullets. I tried the following code, which I found on StackOverflow.</p>

<pre><code>import docx
document = docx.Document(""rwi.docx"")

for paragraph in document.paragraphs:
    if paragraph.style.name == 'Heading 1':
        print(paragraph.text)
</code></pre>

<p>But this just prints the Heading Title but not the body (text) of the title. How can I extract the body of the titles?</p>
",Dataset Preprocessing & Handling,extract body section word document python document following structure input output image want extract text article ii python article actually section see left navigation raw text select article like bullet tried following code found stackoverflow print heading title body text title extract body title
NLP and Machine learning for sentiment analysis,"<p>I'm trying to write a program that takes text(article) as input and outputs the polarity of this text, weather its a positive or a negative sentiment. I've read extensively about different approaches but i am still confused. I read about many techniques like classifiers and machine learning. I would like direction and clear instructions on where to start. For example, i have a classifier which requires a dataset but how do i convert the text(article) into a dataset for the classifier. If anyone can tell me the logical sequence to approach this problem that would be greet. Thanks in advance!
PS: please mention any related algorithms or open-source implementation</p>

<p>Regards,
Mike</p>
",Dataset Preprocessing & Handling,nlp machine learning sentiment analysis trying write program take text article input output polarity text weather positive negative sentiment read extensively different approach still confused read many technique like classifier machine learning would like direction clear instruction start example classifier requires dataset convert text article dataset classifier anyone tell logical sequence approach problem would greet thanks advance p please mention related algorithm open source implementation regard mike
How to generate Word2vec Vectors in Python?,"<p>I am trying to generate Word2vec vectors.</p>

<p>I have pandas data frame.</p>

<p>I transformed it into tokens.</p>

<p><code>df[""token""]</code></p>

<p>Used Word2vec from gensim.models</p>

<pre><code>model = w2v.Word2Vec(
sentences=df[""token""],
seed=seed,
workers=num_workers,
size=num_features,
min_count=min_word_count,
window=context_size,
sample=downsampling
)
</code></pre>

<p>How do I transform my dataframe df now?</p>

<p>That is what is the equivalent of doing </p>

<pre><code>model.transform(df)
</code></pre>
",Dataset Preprocessing & Handling,generate word vec vector python trying generate word vec vector panda data frame transformed token used word vec gensim model transform dataframe df equivalent
I have a csv file and i want to extract each row of csv file into different csv file . how can i do that?,"<p>I have a CSV file and I want to extract each row of CSV file into the different CSV files. how can I do that? </p>
",Dataset Preprocessing & Handling,csv file want extract row csv file different csv file csv file want extract row csv file different csv file
Creating Spark schema for GLoVe word vector files,"<p>GLoVe pre-trained word vectors which can be downloaded here (<a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a>) have the following file format:</p>

<pre><code>government 0.38797 -1.0825 0.45025 -0.23341 0.086307 -0.25721 -0.18281 -0.10037 -0.50099 -0.58361 -0.052635 -0.14224 0.0090217 -0.38308 0.18503 0.42444 0.10611 -0.1487 1.0801 0.065757 0.64552 0.1908 -0.14561 -0.87237 -0.35568 -2.435 0.28428 -0.33436 -0.56139 0.91404 4.0129 0.072234 -1.2478 -0.36592 -0.50236 0.011731 -0.27409 -0.50842 -0.2584 -0.096172 -0.67109 0.40226 0.27912 -0.37317 -0.45049 -0.30662 -1.6426 1.1936 0.65343 -0.76293
</code></pre>

<p>It's a space-delimited file where the first token in each row is the word and the N remaining columns are floating point values for the word vector. N can be 50, 100, 200, or 300 depending on the file being used. The example above is for <code>N=50</code> (i.e. 50-dimensional word vectors).</p>

<p>If I load the data file as a <code>csv</code> with <code>sep=' '</code> and <code>header=False</code> (there is no header in the file), I get the following for a row:</p>

<pre><code>Row(_c0='the', _c1='0.418', _c2='0.24968', _c3='-0.41242', _c4='0.1217', _c5='0.34527', _c6='-0.044457', _c7='-0.49688', _c8='-0.17862', _c9='-0.00066023', _c10='-0.6566', _c11='0.27843', _c12='-0.14767', _c13='-0.55677', _c14='0.14658', _c15='-0.0095095', _c16='0.011658', _c17='0.10204', _c18='-0.12792', _c19='-0.8443', _c20='-0.12181', _c21='-0.016801', _c22='-0.33279', _c23='-0.1552', _c24='-0.23131', _c25='-0.19181', _c26='-1.8823', _c27='-0.76746', _c28='0.099051', _c29='-0.42125', _c30='-0.19526', _c31='4.0071', _c32='-0.18594', _c33='-0.52287', _c34='-0.31681', _c35='0.00059213', _c36='0.0074449', _c37='0.17778', _c38='-0.15897', _c39='0.012041', _c40='-0.054223', _c41='-0.29871', _c42='-0.15749', _c43='-0.34758', _c44='-0.045637', _c45='-0.44251', _c46='0.18785', _c47='0.0027849', _c48='-0.18411', _c49='-0.11514', _c50='-0.78581')
</code></pre>

<p>My question is whether there is a way to specify a schema such that the first column could be read in as a <code>StringType</code> column and the N remaining columns read as an <code>ArrayType</code> of N floating point values?</p>
",Dataset Preprocessing & Handling,creating spark schema glove word vector file glove pre trained word vector downloaded following file format space delimited file first token row word n remaining column floating point value word vector n depending file used example e dimensional word vector load data file header file get following row question whether way specify schema first column could read column n remaining column read n floating point value
Python: extract keywords row by row from csv,"<p>I am trying to extract keywords line by line from a csv file and create a keyword field. Right now I am able to get the full extraction. How do I get keywords for each row/field? </p>

<p>Data: </p>

<pre><code>id,some_text
1,""What is the meaning of the word Himalaya?""
2,""Palindrome is a word, phrase, or sequence that reads the same backward as forward""
</code></pre>

<p>Code: This is search entire text but not row by row. Do I need to put something else besides <code>replace(r'\|', ' ')</code>?</p>

<pre><code>import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

df = pd.read_csv('test-data.csv')
# print(df.head(5))

text_context = df['some_text'].str.lower().str.replace(r'\|', ' ').str.cat(sep=' ') # not put lower case?
print(text_context)
print('')
tokens=nltk.tokenize.word_tokenize(text_context)
word_dist = nltk.FreqDist(tokens)
stop_words = stopwords.words('english')
punctuations = ['(',')',';',':','[',']',',','!','?']
keywords = [word for word in tokens if not word in stop_words and not word in punctuations]
print(keywords)
</code></pre>

<p>final output: </p>

<pre><code>id,some_text,new_keyword_field
1,What is the meaning of the word Himalaya?,""meaning,word,himalaya""
2,""Palindrome is a word, phrase, or sequence that reads the same backward as forward"",""palindrome,word,phrase,sequence,reads,backward,forward""
</code></pre>
",Dataset Preprocessing & Handling,python extract keywords row row csv trying extract keywords line line csv file create keyword field right able get full extraction get keywords row field data code search entire text row row need put something else besides final output
Tokenizing a dataframe of song lyrics,"<p>I have a csv data file of song lyrics where each row is a sentence. I can read this in and use the unnest_tokens function to break the dataframe into a new data frame where each row is a word. Now I am trying to only tokenize the first few rows of the dataframe instead of the whole thing. So, I thought I could just make a new subframe using an index and then use the same function but no luck: </p>

<pre><code>#Works on the whole dataframe

library(dplyr)
library(tokenizers)
library(tidytext)


biggie &lt;- read.csv(""C:/Users/First.Last/Desktop/biggie.csv"", stringsAsFactors=FALSE)

colnames(biggie)[1] &lt;- 'biggie'



bigsplit &lt;- biggie %&gt;% 
  unnest_tokens(word, biggie)
</code></pre>

<p>At first I thought the issue was that indexing changed the original dataframe from a list to a character. So, I added an extra step to turn the subset back into a dataframe. When I test the object types with typeof() they all came back as lists so I thought it would work when I run it through the unnest_tokens line. However, I get this error when I run the lines: </p>

<p>Error in check_input(x) : 
  Input must be a character vector of any length or a list of character
  vectors, each of which has a length of 1.</p>

<pre><code>#Here is where it breaks

bigtest &lt;- biggie[1:10,]
bigtest &lt;- data.frame(bigtest)
colnames(bigtest) &lt;- 'biggie'

bigind &lt;- bigtest %&gt;% 
  unnest_tokens(word, 'biggie')
</code></pre>

<p>I'm really stumped on what the issue is here. In my relatively limited knowledge, this seems like it should work fine. I'd love any input anyone might have.</p>

<pre><code>summary(bigtest)
</code></pre>

<p><a href=""https://i.sstatic.net/yw1Rz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yw1Rz.png"" alt=""summary of bigtest""></a></p>
",Dataset Preprocessing & Handling,tokenizing dataframe song lyric csv data file song lyric row sentence read use unnest token function break dataframe new data frame row word trying tokenize first row dataframe instead whole thing thought could make new subframe using index use function luck first thought issue wa indexing changed original dataframe list character added extra step turn subset back dataframe test object type typeof came back list thought would work run unnest token line however get error run line error check input x input must character vector length list character vector ha length really stumped issue relatively limited knowledge seems like work fine love input anyone might
Text Analytics in R (Creating word cloud with Hindi characters),"<p>I am working on text analytics project, having a text file with both hindi and English text messages. I am able to read English text but special characters are coming where ever there is a message in hindi.</p>

<p>I want to create a word cloud and do sentiment analysis of both hindi and English words simultaneously. Can some body please help me? How can i read hindi message also in R.</p>

<p>First of all I am not able to start as I am not able to read the file?</p>

<p>I have used file encoding as UTF-8 also to handle special characters.</p>

<p><a href=""https://i.sstatic.net/1BQzl.png"" rel=""nofollow noreferrer"">Erroneous File</a></p>

<p>Code i am using</p>

<pre><code>Message &lt;- readtext(""C:/Users/admin/Desktop/Text_Body.txt"", 
                    encoding = ""UTF-8"", sep = "","", header = TRUE)
</code></pre>
",Dataset Preprocessing & Handling,text analytics r creating word cloud hindi character working text analytics project text file hindi english text message able read english text special character coming ever message hindi want create word cloud sentiment analysis hindi english word simultaneously body please help read hindi message also r first able start able read file used file encoding utf also handle special character erroneous file code using
How to construct a clean vocabulary from training text?,"<p>I am following the neural machine translation tutorial <a href=""https://www.tensorflow.org/tutorials/seq2seq"" rel=""nofollow noreferrer"">here</a> and notice that the datasets they use provide a <a href=""https://nlp.stanford.edu/projects/nmt/data/wmt15.en-cs/vocab.1K.en"" rel=""nofollow noreferrer"">clean vocab file</a>. But when I come across a dataset (e.g. <a href=""http://statmt.org/wmt17/translation-task.html"" rel=""nofollow noreferrer"">Europarl v8</a>) that does not provide a vocab file, I need to construct a vocabulary myself using the following function.</p>

<pre><code>def construct_vocab_from_file(file, vocab_file):
    # Read file, tokenize it and then sort it
    with open(file, 'r') as f:
        raw_data = f.read()
        tokens = nltk.wordpunct_tokenize(raw_data)
        words = [w.lower() for w in tokens]
        vocab = sorted(set(words))

    # Write vocab to file
    with open(vocab_file, 'w') as f:
        for w in vocab:
            f.write(w + ""\n"")
</code></pre>

<p>However, the vocabulary constructed this way looks a little bit messy. <a href=""https://i.sstatic.net/KfvzJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KfvzJ.png"" alt=""enter image description here""></a></p>

<p>The left one is from the clean vocab file while the right one with the black background (numbers are line number) is from the vocabulary constructed by me. This does not make me feel comfortable especially more than half of the vocabulary consist of these kind of special characters or numbers (e.g. 0, 00, 000, 0000, 0000003).</p>

<p>So my questions are:</p>

<p>1) Is this problematic?</p>

<p>2) Should I process it further and how?</p>
",Dataset Preprocessing & Handling,construct clean vocabulary training text following neural machine translation tutorial notice datasets use provide clean vocab file come across dataset e g europarl v doe provide vocab file need construct vocabulary using following function however vocabulary constructed way look little bit messy left one clean vocab file right one black background number line number vocabulary constructed doe make feel comfortable especially half vocabulary consist kind special character number e g question problematic process
Stanford Core NLP pipeline,"<p>I'm trying to create pipeline with NER tagging.</p>

<p>How to get the NER tagging in this way?</p>

<p>Line triggering the error:
<code>String nerrr = token.ner();</code></p>

<p>Code:</p>

<pre><code>public class NLPpipeline {

public AnnotationPipeline buildPipeline() {

    Properties props = new Properties();
    AnnotationPipeline pl = new AnnotationPipeline();

    pl.addAnnotator( new TokenizerAnnotator( false ) );
    pl.addAnnotator( new WordsToSentencesAnnotator( false ) );
    pl.addAnnotator( new POSTaggerAnnotator( false ) );
    pl.addAnnotator( new MorphaAnnotator( false ) );
    pl.addAnnotator(new TimeAnnotator(""sutime"", props));


    return pl;
}


public static void main(String[] args) {


    NLPpipeline nlp = new NLPpipeline();
    AnnotationPipeline pipeline = nlp.buildPipeline();
    Annotation annotation = new Annotation( ""Last summer, Sali and Nadav met every Tuesday afternoon, from 1:00 pm to 3:00 pm."" );
    pipeline.annotate( annotation );


    for (CoreMap sentence : sentences) {
        for (CoreLabel token : sentence.get( CoreAnnotations.TokensAnnotation.class )) {

            String word = token.word();
            String pos = token.tag();
            String nerrr = token.ner();
            String role = token.lemma();


            System.out.println( ""=====\n"" + word );
            System.out.println( pos );
            System.out.println( nerrr );
            System.out.println( role );
        }
    }
}
</code></pre>

<p>thank you very much for your answer. I tried to create a pipe like you described, but it's very slow because I have a long text and I have to divide it into sentences, and each time it loads the NER files and it takes about 45 seconds for each sentence. My project is converting user stories into test cases, and I need to identify entities in user stories.
I realized I had the opportunity to create the department once:
         SentimentAnalyzer sentimentAnalyzer = new SentimentAnalyzer ();          sentimentAnalyzer.initializeCoreNLP (); // run this only once
And send at a time, but I do not understand how I should do it</p>
",Dataset Preprocessing & Handling,stanford core nlp pipeline trying create pipeline ner tagging get ner tagging way line triggering error code thank much answer tried create pipe like described slow long text divide sentence time load ner file take second sentence project converting user story test case need identify entity user story realized opportunity create department sentimentanalyzer sentimentanalyzer new sentimentanalyzer sentimentanalyzer initializecorenlp run send time understand
TypeError: expected string or bytes-like object HashingVectorizer,"<p>I have been facing this issue while fitting the dataset..Everything seems fine, don't know where the problem is.
Since I'm a beginner could anyone please tell me what I am doing wrong or am I missing something?</p>

<p>The problem seems to be in data preprocessing part</p>

<p>Error trace and the dataframe's head has been attached as image below
`</p>

<pre><code>train = pd.read_csv('train.txt', sep='\t', dtype=str, header=None)
test =  pd.read_csv('test.txt', sep='\t', dtype=str, header=None)

X_train = train.iloc[:,1:]
y_train = train.iloc[:,0:1]

X_test = test.iloc[:,1:]
y_test = test.iloc[:,0:1]

TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\s+)'

steps = [('vectorizer',HashingVectorizer(TOKENS_ALPHANUMERIC,
                                                     norm=None, binary=False, lowercase=False,
                                                     ngram_range=(1,2))),
         ('clf',OneVsRestClassifier(LogisticRegression()))]

pipeline = Pipeline(steps)
pipeline.fit(X_train,y_train)
accuracy = pipeline.score(X_test,y_test)
print(accuracy)
</code></pre>

<p>`</p>

<p><a href=""https://i.sstatic.net/OIZJG.png"" rel=""nofollow noreferrer"">stack trace</a> <a href=""https://i.sstatic.net/I45T3.png"" rel=""nofollow noreferrer"">dataframe head</a></p>
",Dataset Preprocessing & Handling,typeerror expected string byte like object hashingvectorizer facing issue fitting dataset everything seems fine know problem since beginner could anyone please tell wrong missing something problem seems data preprocessing part error trace dataframe head ha attached image stack trace dataframe head
How to prepare test data for textsum?,"<p>I have been able to successfully run the pre-trained model of <a href=""https://github.com/becxer/pointer-generator/"" rel=""nofollow noreferrer"">TextSum</a> (Tensorflow 1.2.1). The output consists of summaries of CNN &amp; Dailymail articles (which are chuncked into bin format prior to testing). </p>

<p>I have also been able to create the aforementioned bin format test data for CNN/Dailymail articles &amp; vocab file (per instructions <a href=""https://github.com/becxer/cnn-dailymail/"" rel=""nofollow noreferrer"">here</a>). However, I am not able to create my own test data to check how good the summary is. I have tried modifying the <code>make_datafiles.py</code> code to remove had coded values. I am able to create tokenized files, but the next step seems to be failing. It'll be great if someone can help me understand what <code>url_lists</code> is being used for. Per the github readme - </p>

<p>""<em>For each of the url lists all_train.txt, all_val.txt and all_test.txt, the corresponding tokenized stories are read from file, lowercased and written to serialized binary files train.bin, val.bin and test.bin. These will be placed in the newly-created finished_files directory.</em>""</p>

<p>How is a URL such as <a href=""http://web.archive.org/web/20150401100102id_/http://www.cnn.com/2015/04/01/europe/france-germanwings-plane-crash-main/"" rel=""nofollow noreferrer"">http://web.archive.org/web/20150401100102id_/http://www.cnn.com/2015/04/01/europe/france-germanwings-plane-crash-main/</a> being mapped to the corresponding story in my data folder? If someone has had success with this, please do let me know how to go about this. Thanks in advance! </p>
",Dataset Preprocessing & Handling,prepare test data textsum able successfully run pre trained model textsum tensorflow output consists summary cnn dailymail article chuncked bin format prior testing also able create aforementioned bin format test data cnn dailymail article vocab file per instruction however able create test data check good summary tried modifying code remove coded value able create tokenized file next step seems failing great someone help understand used per github readme url list train txt val txt test txt corresponding tokenized story read file lowercased written serialized binary file train bin val bin test bin placed newly created finished file directory url mapped corresponding story data folder someone ha success please let know go thanks advance
finding associate terms : findAssocs not returning result,"<p><code>findAssocs</code> returns &lt;0 rows> (or 0-length row.names) for all of my input and I'm not smart enough to figure out why. I've posted the code underneath to show you my steps, thanks in advance!</p>

<p><em>read in files</em></p>

<pre><code>text &lt;- readLines(list.files())
text &lt;- readLines(file.choose())
text &lt;- pdf_text(file.choose())
</code></pre>

<p><em>collapse</em> </p>

<pre><code>text &lt;- paste(unlist(text), collapse ="""")
</code></pre>

<p><em>convert to corpus</em> </p>

<pre><code>docs &lt;- Corpus(VectorSource(text))`
</code></pre>

<p><em>clean</em></p>

<pre><code>toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, "" "", x))
docs &lt;- tm_map(docs, toSpace, ""/"")
docs &lt;- tm_map(docs, toSpace, ""@"")
docs &lt;- tm_map(docs, toSpace, ""\\|"")
docs &lt;- tm_map(docs, content_transformer(tolower))
docs &lt;- tm_map(docs, removeNumbers)
docs &lt;- tm_map(docs, removeWords, stopwords(""dutch""))
docs &lt;- tm_map(docs, removeWords, stopwords(""english""))
docs &lt;- tm_map(docs, removePunctuation)
docs &lt;- tm_map(docs, stripWhitespace)
</code></pre>

<p><em>dtm</em></p>

<pre><code>dtm &lt;- TermDocumentMatrix(docs)
m &lt;- as.matrix(dtm)
v &lt;- sort(rowSums(m),decreasing=TRUE)
d &lt;- data.frame(word = names(v),freq=v)
</code></pre>

<p><em>associate terms</em> </p>

<pre><code>as.data.frame(findAssocs(dtm, terms = ""security"", corlimit = 0.3))
</code></pre>

<p><strong>SUBSET:</strong></p>

<pre><code> [976] ""how to utilize public or hybrid clouds without""                                                                                                                                                                                                                                                                                                         
 [977] ""compromising the overall security posture and at""                                                                                                                                                                                                                                                                                                       
 [978] ""the same time leveraging cost and scaling benefits""                                                                                                                                                                                                                                                                                                     
 [979] ""of the cloud.""                                                                                                                                                                                                                                                                                                                                          
 [980] """"                                                                                                                                                                                                                                                                                                                                                       
 [981] ""If we take a holistic view of the non-technical challenges, it boils down to how the mid-to-large sized""                                                                                                                                                                                                                                                
 [982] ""IT departments are organized. The typical organization is divided into for example a network team,""                                                                                                                                                                                                                                                     
 [983] ""a server team, a security team and an application""                                                                                                                                                                                                                                                                                                      
 [984] ""team working perfectly within their respective silo""                                                                                                                                                                                                                                                                                                    
 [985] ""but rarely frictionless between the silos.""     
</code></pre>
",Dataset Preprocessing & Handling,finding associate term findassocs returning result return row length row name input smart enough figure posted code underneath show step thanks advance read file collapse convert corpus clean dtm associate term subset
The process of removing duplicates taking too long,"<p>I have a very large csv file having around 70,000 tweets containing duplicate vales that i have to remove. The file has three columns (ID, Creation_Date, Text).  </p>

<p>An example of the csv file is given below:</p>

<pre><code>       ID                          Date                                  Text
""745828866334269441""     ""Thu Jun 23 04:05:33 +0000 2017""              ""Any TEXT""
""745828863334269434""     ""Thu Jun 23 04:06:33 +0000 2017""              ""Any TEXT""
""745828343334269425""     ""Thu Jun 23 04:07:33 +0000 2017""              ""Any TEXT""  
      ................ and so on
</code></pre>

<p>I am using sequenceMatcher from Difflib in Python. The script is working perfectly fine. The script is listed below:  </p>

<pre><code>import csv
from difflib import SequenceMatcher

csvInputFile=open('inputFileWithDups.csv', 'r', encoding=""utf-8"", newline='') # Input file name with duplicates
csvOutputFile=open('outputFileWithoutDups.csv', 'w', encoding=""utf-8"", newline='') # Output file name without duplicates

csvReader = csv.reader(csvInputFile)
csvWriter = csv.writer(csvOutputFile, delimiter=',',quotechar='""', quoting=csv.QUOTE_ALL)
cleanData = set() # an empty set that will be used to compare and then store the clean tweets without duplicates

for row in csvReader: # reading the inputfile 
   add=True 
   a=row[2] # our third csv column with tweets text that we have to compare for duplicates
   for cleantweet in cleanData:# reading the cleanData set to compare tweet texts.
        f= SequenceMatcher(None,cleantweet,a).ratio() #cleantweet vs row[2] which is text  
        if f &gt; 0.73:
            print(f)
            add=False

   if add: # This will add all the tweets that have a similarty lower than 0.73 (here 1.0 means a 100 percent similarity)
       cleanData.add(row[2])
       csvWriter.writerow(row) # adding all the tweets without duplicates into the new csv file.
csvOutputFile.close()
csvInputFile.close()  
</code></pre>

<p>but a PC having only 4GB of Ram is taking too much time to process. For exmaple: a file having only 5000 tweets took almost 7 hours to process. The next file i have to compare contains 50,000 tweets which means may b 3 days of work :(<br>
I will really appreciate if someone could help me out to speed up the process.<br>
Thanks</p>
",Dataset Preprocessing & Handling,process removing duplicate taking long large csv file around tweet containing duplicate vale remove file ha three column id creation date text example csv file given using sequencematcher difflib python script working perfectly fine script listed pc gb ram taking much time process exmaple file tweet took almost hour process next file compare contains tweet mean may b day work really appreciate someone could help speed process thanks
LDA and chosing topic,"<p>I have recently digged into LDA, it seems pretty reasonable but i am left with few questions which i am unable to find answers for.</p>

<p>For Lda, we first represent korpus as vector where</p>

<pre><code>           word1 , word2 , word3 , wordN
document1   n       n        n       n
document2   n       n        n       n
documentN   n       n        n       n
</code></pre>

<p>This tells us, how many times word j from vocabullary appears in document i.</p>

<p>1st question)</p>

<p>Do we create vocabullary V randomly from words from all documents or do we chose it such that every word in V is present in every document atleast once?</p>

<p>next we create matrix for every document</p>

<pre><code>        topic1  topic2 topicN
 word1   n         n     n
 word2   n         n     n
 word3   n         n     n
 word4   n         n     n
</code></pre>

<p>We select topics we want to represent in our documents, and randomly assign word to document ( wordi x topij = 1 if word belongs to topic, 0 otherwise )</p>

<p>next for every word we calculate their new topic using formula</p>

<p>P = P1 * P2</p>

<p>Where </p>

<pre><code>P1 = Probability( topic T | document d )
P2 = Probability( word W | topic T )
</code></pre>

<p>Now new topic K is assigned to the word W with probability P.</p>

<p>2nd question)</p>

<p>What topic do we chose as T and to what topic do we assigne probability P for word W?
I failed to find the answer for this.</p>

<p>Thanks for answer</p>
",Dataset Preprocessing & Handling,lda chosing topic recently digged lda seems pretty reasonable left question unable find answer lda first represent korpus vector tell u many time word j vocabullary appears document st question create vocabullary v randomly word document chose every word v present every document atleast next create matrix every document select topic want represent document randomly assign word document wordi x topij word belongs topic otherwise next every word calculate new topic using formula p p p new topic k assigned word w probability p nd question topic chose topic assigne probability p word w failed find answer thanks answer
removing tweets with partial similarity,"<p>I am new to python and also to stackoverlfow. I have a csv file with three columns (ID, Date_Of_creation, Text). There are almost 25,000 entries in the file. I have to remove the duplicate tweets (text column) and the code below works fine to remove duplicates:</p>
<pre><code>import csv

csvInputFile = open('inputFile.csv', 'r',encoding=&quot;utf-8&quot;, newline='')
csvOutputFile = open('outputFile.csv', 'w', encoding=&quot;utf-8&quot;, newline='')

csvReader = csv.reader(csvInputFile)
csvWriter = csv.writer(csvOutputFile)
cleanData = set()

for row in csvReader:
    #print(row[3])
    if row[3] in cleanData: continue
    cleanData.add(row[3])
    csvWriter.writerow(row)

print(cleanData)
csvOutputFile.close()
csvInputFile.close()
</code></pre>
<p>This code is removing all the duplicates with corresponding IDS and creation date.
As a second step of the analysis, i noticed that there are some retweets that don't have the original tweets in the data set. I want to keep those retweets.
In simple, i want to remove all the duplicates, whether its a tweet or retweet, from the Text column. For Example:</p>
<blockquote>
<p>&quot;It will not be easy for them to handle the situation at this stage:…&quot;</p>
<p>&quot;RT @ReutersLobby: It will not be easy for them to handle the situation at this stage:…&quot;</p>
</blockquote>
<p>As the above tweet and retweet shows that &quot;RT @ReutresLobby:&quot; is extra in retweet. So the above code will not remove this retweet from the final set. I want to remove all such tweets that are a copy of a another tweet because the focus is on text of the tweet and creation time and not on other fields.
I tried to search for it but could not find anything related on the forum.I hope someone will help me out with this problem..</p>
",Dataset Preprocessing & Handling,removing tweet partial similarity new python also stackoverlfow csv file three column id date creation text almost entry file remove duplicate tweet text column code work fine remove duplicate code removing duplicate corresponding id creation date second step analysis noticed retweets original tweet data set want keep retweets simple want remove duplicate whether tweet retweet text column example easy handle situation stage rt reuterslobby easy handle situation stage tweet retweet show rt reutreslobby extra retweet code remove retweet final set want remove tweet copy another tweet focus text tweet creation time field tried search could find anything related forum hope someone help problem
How to run JAVA API in Python,"<p>I got a Java API for words stemming but I am unable to run it. I am working on an NLP project in PYTHON 3.x where I read all the text from documents and converted it into words. I want to use this Java API for stemming to stem my words and then process further. I was exploring about running Java API directly into Python program using different libraries and I read a little bit about <a href=""https://www.py4j.org/"" rel=""nofollow noreferrer"">PY4J</a> but unable to run it.
Can anyone please guide me how to use this API in Python or if this is not possible then how to use it in ECLIPSE. </p>

<p>Stemmer API Instructions:</p>

<p>Description:
        Word Stemmer API is a Java application that provides an interface to extract the stems, prefixes, and postfixes of words.</p>

<p>Setup:
        Copy the Data folder into your project directory and add the provided JAR file to your project.</p>

<p>Usage:</p>

<pre><code>    1. loadRules()
        - Purpose:      This function loads the stemming rules from the ./Data/Rules.txt into the program.
        - Syntax:       void loadRules();
        - Parameters:   None
        - Return type:  Void


    2. stemWord()
        - Purpose:      This function accepts as input a single word and returns a HashMap containing its stem, prefix, and postfix.
        - Syntax:       HashMap&lt;String, String&gt; stemWord(String word);
        - Parameters:   String word to be stemmed
        - Return type:  HashMap with the following keys: ""stem"", ""prefix"", ""postfix""

    3. stemFile()
        - Purpose:      This function acecpts as input the path to a UTF-8 text file and writes a new file to the same directory with the suffix ""_stemmed"".
        - Syntax:       void stemFile(String path);
        - Parameters:   String path to text file
        - Return type:  Void
</code></pre>

<p>Example:</p>

<pre><code>    UStemmer stmr = new UStemmer();

    stmr.loadRules();

    stmr.stemFile(String path);

    HashMap&lt;String, String&gt; stemmed = stmr.stemWord(String word);

    String stem = stemmed.get(""stem"");
    String prefix = stemmed.get(""prefix"");
    String postfix = stemmed.get(""postfix"");
</code></pre>

<p>PS: The API folder I have contains a file UStemmer.JAR and two folders, first one is Data which have Rules.txt file and second folder is UStemmer which have two files, one is UStemmer.class (Unable to open or read) and other is MANIFEST.MF
PPS: I cannot use any of the available stemmers because they do not support the language I am working on. (<a href=""https://en.wikipedia.org/wiki/Urdu"" rel=""nofollow noreferrer"">URDU language -Pakistan</a>)</p>
",Dataset Preprocessing & Handling,run java api python got java api word stemming unable run working nlp project python x read text document converted word want use java api stemming stem word process wa exploring running java api directly python program using different library read little bit py j unable run anyone please guide use api python possible use eclipse stemmer api instruction description word stemmer api java application provides interface extract stem prefix postfix word setup copy data folder project directory add provided jar file project usage example p api folder contains file ustemmer jar two folder first one data rule txt file second folder ustemmer two file one ustemmer class unable open read manifest mf pps use available stemmer support language working urdu language pakistan
Spacy Doc not parsing and processing text,"<p>I'm creating a Flask endpoint so I can do some text processing, and in order to get the spacy english model to load I'm downloading the model file through my Pipfile when I push to Heroku.  Whereas before when working locally I was loading the model with:</p>

<pre><code>nlp = spacy.load('en')
doc = nlp(text)
</code></pre>

<p>Now I download the <code>en_core_web_sm</code> during the build on heroku (or even when running <code>heroku local web</code> and do:</p>

<pre><code>import en_core_web_sm

nlp = en_core_web_sm.load()
doc = nlp(text)
</code></pre>

<p>Weirdly the spacy Doc object is still created, the model is loaded, but...nothing happens.  It doesn't tokenize, it doesn't run the entity extractor, nada.  What am I missing?</p>
",Dataset Preprocessing & Handling,spacy doc parsing processing text creating flask endpoint text processing order get spacy english model load downloading model file pipfile push heroku whereas working locally wa loading model download build heroku even running weirdly spacy doc object still created model loaded nothing happens tokenize run entity extractor nada missing
Cannot coerce class &quot;&quot;String&quot;&quot; to a data.frame,"<p>I am working with a data-set that contains reviews of an item. The code runs perfectly for the most of the reviews which normally has around 20-30 words, but the code throws an error whenever reviews with only a single word occurs. </p>

<pre><code>library(NLP)
library(openNLP)
library(stringr)

x &lt;- NLP::as.String(""pathetic"")
wordAnnotation &lt;- NLP::annotate(x, list(Maxent_Sent_Token_Annotator(), 
  Maxent_Word_Token_Annotator()))
POSAnnotation &lt;- NLP::annotate(x, Maxent_POS_Tag_Annotator(), 
  wordAnnotation)
POSwords &lt;- subset(POSAnnotation, type == ""word"")
tags &lt;- sapply(POSwords$features, '[[', ""POS"")
tokenizedAndTagged &lt;- data.frame(Tokens = x[POSwords], Tags = tags, 
  stringsAsFactors = FALSE)
</code></pre>

<blockquote>
<pre><code>Error in as.data.frame.default(x[[i]], optional = TRUE, stringsAsFactors = 
stringsAsFactors) : cannot coerce class """"String"""" to a data.frame
</code></pre>
</blockquote>

<p>I have seen other similar questions, tried the solutions like resolving function overriding issue by using <code>NLP::annotate</code>, restarting R session but didn't work. Please point out how to resolve the issue. Thanks in advance.</p>
",Dataset Preprocessing & Handling,coerce class string data frame working data set contains review item code run perfectly review normally ha around word code throw error whenever review single word occurs seen similar question tried solution like resolving function overriding issue using restarting r session work please point resolve issue thanks advance
ERROR while loading &amp; reading custom 20newsgroups corpus with NLTK,"<p>I am trying to load the <strong>20newsgroups</strong> corpus with the NLTK corpus reader and thereafter I am extracting words from all documents and tagging them. But it is showing error when I am trying to build the word extracted and tagged list. </p>

<p>Here is the <strong>CODE</strong>:</p>

<pre><code>import nltk
import random

from nltk.tokenize import word_tokenize

newsgroups = nltk.corpus.reader.CategorizedPlaintextCorpusReader(
    r""C:\nltk_data\corpora\20newsgroups"",
    r'(?!\.).*\.txt', 
    cat_pattern=r'(not_sports|sports)/.*',
    encoding=""utf8"")

documents = [(list(newsgroups.words(fileid)), category)
             for category in newsgroups.categories()
             for fileid in newsgroups.fileids(category)]

random.shuffle(documents)
</code></pre>

<p>And the corresponding <strong>ERROR</strong> is:</p>

<pre><code>---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
&lt;ipython-input-10-de2a1a6859ea&gt; in &lt;module&gt;()
      1 documents = [(list(newsgroups.words(fileid)), category)
----&gt; 2              for category in newsgroups.categories()
      3              for fileid in newsgroups.fileids(category)]
      4 
      5 random.shuffle(documents)

&lt;ipython-input-10-de2a1a6859ea&gt; in &lt;listcomp&gt;(.0)
      1 documents = [(list(newsgroups.words(fileid)), category)
      2              for category in newsgroups.categories()
----&gt; 3              for fileid in newsgroups.fileids(category)]
      4 
      5 random.shuffle(documents)

C:\ProgramData\Anaconda3\lib\site-packages\nltk\corpus\reader\util.py in __len__(self)
    231             # iterate_from() sets self._len when it reaches the end
    232             # of the file:
--&gt; 233             for tok in self.iterate_from(self._toknum[-1]): pass
    234         return self._len
    235 

C:\ProgramData\Anaconda3\lib\site-packages\nltk\corpus\reader\util.py in iterate_from(self, start_tok)
    294             self._current_toknum = toknum
    295             self._current_blocknum = block_index
--&gt; 296             tokens = self.read_block(self._stream)
    297             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (
    298                 'block reader %s() should return list or tuple.' %

C:\ProgramData\Anaconda3\lib\site-packages\nltk\corpus\reader\plaintext.py in _read_word_block(self, stream)
    120         words = []
    121         for i in range(20): # Read 20 lines at a time.
--&gt; 122             words.extend(self._word_tokenizer.tokenize(stream.readline()))
    123         return words
    124 

C:\ProgramData\Anaconda3\lib\site-packages\nltk\data.py in readline(self, size)
   1166         while True:
   1167             startpos = self.stream.tell() - len(self.bytebuffer)
-&gt; 1168             new_chars = self._read(readsize)
   1169 
   1170             # If we're at a '\r', then read one extra character, since

C:\ProgramData\Anaconda3\lib\site-packages\nltk\data.py in _read(self, size)
   1398 
   1399         # Decode the bytes into unicode characters
-&gt; 1400         chars, bytes_decoded = self._incr_decode(bytes)
   1401 
   1402         # If we got bytes but couldn't decode any, then read further.

C:\ProgramData\Anaconda3\lib\site-packages\nltk\data.py in _incr_decode(self, bytes)
   1429         while True:
   1430             try:
-&gt; 1431                 return self.decode(bytes, 'strict')
   1432             except UnicodeDecodeError as exc:
   1433                 # If the exception occurs at the end of the string,

C:\ProgramData\Anaconda3\lib\encodings\utf_8.py in decode(input, errors)
     14 
     15 def decode(input, errors='strict'):
---&gt; 16     return codecs.utf_8_decode(input, errors, True)
     17 
     18 class IncrementalEncoder(codecs.IncrementalEncoder):

UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 6: invalid start byte
</code></pre>

<p>I have tried changing the encoding in the corpus reader to <strong>ascii</strong> and <strong>utf16</strong> as well. That's not working either. I am not sure whether the regex I have provided is the right one or not. The filenames in the 20newsgroups corpus are in the form of 2 numbers separated by a hyphen(-), such as:</p>

<blockquote>
  <p>5-53286</p>
  
  <p>102-53553</p>
  
  <p>8642-104983</p>
</blockquote>

<p>The second thing that I am worried about is whether the error is being generated from the document contents when they are being read for feature extraction.
Here are a what documents in 20newsgroups corpus look like:</p>

<blockquote>
  <p>From: bil@okcforum.osrhe.edu (Bill Conner) Subject: Re: free moral
  agency</p>
  
  <p>dean.kaflowitz (decay@cbnewsj.cb.att.com) wrote: : >  : > I think
  you're letting atheist mythology</p>
  
  <p>: Great start.  I realize immediately that you are not interested : in
  discussion and are going to thump your babble at me.  I would : much
  prefer an answer from Ms Healy, who seems to have a : reasonable and
  reasoned approach to things.  Say, aren't you the : creationist guy
  who made a lot of silly statements about : evolution some time ago?</p>
  
  <p>: Duh, gee, then we must be talking Christian mythology now.  I : was
  hoping to discuss something with a reasonable, logical : person, but
  all you seem to have for your side is a repetition : of the same
  boring mythology I've seen a thousand times before. : I am deleting
  the rest of your remarks, unless I spot something : that approaches an
  answer, because they are merely a repetition : of some uninteresting
  doctrine or other and contain no thought : at all.</p>
  
  <p>: I have to congratulate you, though, Bill.  You wouldn't : know a
  logical argument if it bit you on the balls.  Such : a persistent lack
  of function in the face of repeated : attempts to assist you in
  learning (which I have seen : in this forum and others in the past)
  speaks of a talent : that goes well beyond my own, meager abilities. 
  I just don't : seem to have that capacity for ignoring outside
  influences.</p>
  
  <p>: Dean Kaflowitz</p>
  
  <p>Dean,</p>
  
  <p>Re-read your comments, do you think that merely characterizing an
  argument is the same as refuting it? Do you think that ad hominum
  attacks are sufficient to make any point other than you disapproval of
  me? Do you have any contribution to make at all?</p>
  
  <p>Bill</p>
</blockquote>

<pre><code>From: cmk@athena.mit.edu (Charles M Kozierok) Subject: Re: Jack Morris

In article &lt;1993Apr19.024222.11181@newshub.ariel.yorku.ca&gt; cs902043@ariel.yorku.ca (SHAWN LUDDINGTON) writes: } In article &lt;1993Apr18.032345.5178@cs.cornell.edu&gt; tedward@cs.cornell.edu (Edward [Ted] Fischer) writes: } &gt;In article &lt;1993Apr18.030412.1210@mnemosyne.cs.du.edu&gt; gspira@nyx.cs.du.edu (Greg Spira) writes: } &gt;&gt;Howard_Wong@mindlink.bc.ca (Howard Wong) writes: }
&gt;&gt; } &gt;&gt;&gt;Has Jack lost a bit of his edge? What is the worst start Jack Morris has had? } &gt;&gt; } &gt;&gt;Uh, Jack lost his edge about 5 years ago, and has had only one above } &gt;&gt;average year in the last 5. } &gt; } &gt;Again goes to prove that it is better to be good than lucky.  You can }
&gt;count on good tomorrow.  Lucky seems to be prone to bad starts (and a } &gt;bad finish last year :-). } &gt; } &gt;(Yes, I am enjoying every last run he gives up.  Who was it who said } &gt;Morris was a better signing than Viola?) }  } Hey Valentine, I don't see Boston with any world series rings on their } fingers.

oooooo. cheap shot. :^)

} Damn, Morris now has three and probably the Hall of Fame in his  } future.

who cares? he had two of them before he came to Toronto; and if the Jays had signed Viola instead of Morris, it would have been Frank who won 20 and got the ring. and he would be on his way to 20 this year, too.

} Therefore, I would have to say Toronto easily made the best  } signing.

your logic is curious, and spurious.

there is no reason to believe that Viola wouldn't have won as many games had *he* signed with Toronto. when you compare their stupid W-L records, be sure to compare their team's offensive averages too.


now, looking at anything like the Morris-Viola sweepstakes a year later is basically hindsight. but there were plenty of reasons why it should have been apparent that Viola was the better pitcher, based on previous recent years and also based on age (Frank is almost 5 years younger! how many knew that?). people got caught up in the '91 World Series, and then on Morris' 21 wins last year. wins are the stupidest, most misleading statistic in baseball, far worse than RBI or R. that he won 21 just means that the Jays got him a lot of runs.

the only really valid retort to Valentine is: weren't the Red Sox trying to get Morris too? oh, sure, they *said* Viola was their first choice afterwards, but what should we have expected they would say?

} And don't tell me Boston will win this year.  They won't  } even be in the top 4 in the division, more like 6th.

if this is true, it won't be for lack of contribution by Viola, so who cares?

-*- charles
</code></pre>

<p>Please suggest me whether the error is while loading the documents or while reading the files and extracting words. What do I need to do to load the corpus correctly?</p>
",Dataset Preprocessing & Handling,error loading reading custom newsgroups corpus nltk trying load newsgroups corpus nltk corpus reader thereafter extracting word document tagging showing error trying build word extracted tagged list code corresponding error tried changing encoding corpus reader ascii utf well working either sure whether regex provided right one filename newsgroups corpus form number separated hyphen second thing worried whether error generated document content read feature extraction document newsgroups corpus look like bil okcforum osrhe edu bill conner subject free moral agency dean kaflowitz decay cbnewsj cb att com wrote think letting atheist mythology great start realize immediately interested discussion going thump babble would much prefer answer healy seems reasonable reasoned approach thing say creationist guy made lot silly statement evolution time ago duh gee must talking christian mythology wa hoping discus something reasonable logical person seem side repetition boring mythology seen thousand time deleting rest remark unless spot something approach answer merely repetition uninteresting doctrine contain thought congratulate though bill know logical argument bit ball persistent lack function face repeated attempt assist learning seen forum others past speaks talent go well beyond meager ability seem capacity ignoring outside influence dean kaflowitz dean read comment think merely characterizing argument refuting think ad hominum attack sufficient make point disapproval contribution make bill please suggest whether error loading document reading file extracting word need load corpus correctly
NLP Sentiment Analysis using TF-IDF Vector Size,"<p>I am relatively new to NLP &amp; Sentiment analysis, but I am enrolled in a Machine Learning class and am creating a Sentiment Analysis NLP that will read a financial article and determine whether or not the overall sentiment is good or bad. </p>

<p>Currently, I have a dataset of about 2000 articles. I know that I need to implement the TF-IDF vector method to cast all the instances in the dataset to the same vector space. Also, I know that TF-IDF requires a ""Vocabulary"" and the size of this ""Vocabulary"" is the length of the vector, each vector representing an article. </p>

<p>My question is, how do I determine this vocabulary? One method I have found is to implement pre-processing (get rid of stop words, noisy words, punctuation, etc.) and then use ALL words in EVERY article in the training set. From here you can remove the words that have very few instances (unimportant words) and remove the words that have too many instances (non-distinguishing words). However, in my opinion, the ""Vocabulary"" is still going to be quite large, hence, the vector size is going to be very large.</p>

<p>Overall, this approach seems logical, but processing heavy. I feel that initially creating a ""Vocabulary"" containing all words in every article is going to be HUGE. And then iterating through every article to see how many times the words in the ""Vocabulary"" have occurred is going to require a lot of processing power. If I am using NLTK and scikit-learn, do I have anything to worry about? If so, is there a better way to create the vocabulary?</p>
",Dataset Preprocessing & Handling,nlp sentiment analysis using tf idf vector size relatively new nlp sentiment analysis enrolled machine learning class creating sentiment analysis nlp read financial article determine whether overall sentiment good bad currently dataset article know need implement tf idf vector method cast instance dataset vector space also know tf idf requires vocabulary size vocabulary length vector vector representing article question determine vocabulary one method found implement pre processing get rid stop word noisy word punctuation etc use word every article training set remove word instance unimportant word remove word many instance non distinguishing word however opinion vocabulary still going quite large hence vector size going large overall approach seems logical processing heavy feel initially creating vocabulary containing word every article going huge iterating every article see many time word vocabulary occurred going require lot processing power using nltk scikit learn anything worry better way create vocabulary
tidytext read files from folder,"<p>I'm trying to read a folder of pdf files into a dataframe in R.  I'm able to read individual pdf files in using the <code>pdftools</code> library and <code>pdf_text(filepath)</code>.  </p>

<p>Ideally, I could grab the author and title of a series of pdf's that are then pushed into a dataframe that has a column with these so that I can then use basic <code>tidytext</code> functions on the text.</p>

<p>For a single file right now, I can just use:</p>

<pre><code>library(pdftools)
library(tidytext)
library(dplyr)
txt &lt;- pdf_text(""filpath"")
txt &lt;- data_frame(txt)
txt %&gt;%
     unnest_tokens(word, txt)
</code></pre>

<p>Here I have a dataframe with single words.  I'd like to get to a dataframe where I have articles unpacked including a title and author column.</p>
",Dataset Preprocessing & Handling,tidytext read file folder trying read folder pdf file dataframe r able read individual pdf file using library ideally could grab author title series pdf pushed dataframe ha column use basic function text single file right use dataframe single word like get dataframe article unpacked including title author column
Stanford NLP how to preprocessing the text,"<p>I have a sentence like this ""The people working in @walman are not good""</p>

<p>I have a preprocessed text file which contains the mappings, similar to the following two lines:</p>

<pre><code>@walman   Walman
@text     Test
</code></pre>

<p>For the above sentence I have to read through the text file and replace the word with any matching word found in the text file.</p>

<p>The above sentence will change to ""The people working in Walman are not good""</p>

<p>I am looking for an API available in Standford NLP to read the input text file and replace the text.</p>
",Dataset Preprocessing & Handling,stanford nlp preprocessing text sentence like people working walman good preprocessed text file contains mapping similar following two line sentence read text file replace word matching word found text file sentence change people working walman good looking api available standford nlp read input text file replace text
Imposing a cap on word count in scikit learn,"<p>I'm analyzing song lyrics where repetition doesn't necessarily mean higher importance, so I'd like to cap the word count <em>per document</em>. For example, if a word appears <code>n</code> times in a song, where <code>n &gt; threshold</code>, then I would replace <code>n</code>with <code>threshold</code>.</p>

<p>I've checked the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">CountVectorizer docs</a>, and there's an option for a <code>min_df</code> and <code>max_df</code>, but these can only disregard words that appear in some m <em>documents</em>, not words that appear n <em>times</em> in a <em>single</em> document.</p>

<p>I was thinking of changing the elements of the sparse matrix (say, find all elements > threshold, then replace), but I couldn't find a way to that either. Thanks in advance!</p>
",Dataset Preprocessing & Handling,imposing cap word count scikit learn analyzing song lyric repetition necessarily mean higher importance like cap word count per document example word appears time song would replace checked countvectorizer doc option disregard word appear document word appear n time single document wa thinking changing element sparse matrix say find element threshold replace find way either thanks advance
compare list of data with CSV file and sort the matching,"<p>I have a data set of product names and a brands list. 
I need to find the how much branded products are there in my list.</p>

<pre><code>**Brands sample :** ['HM International', 'Sara', 'Wildcraft', 'Nike']
**Product name sample :** [Attache backpack11Green Waterproof Backpack
Simba BTSPOKEMON POKÈMON POKÈ BALLS 18 BP Waterproof S...
HM International HMHTPB 24304MK Waterproof Multipurpos...
Chris &amp; Kate CKB_122SS Waterproof School Bag
Simba BTSPRINCESS FOLLOW YOUR DREAMS 16 BP Waterproof ...
Kuber Industries School Bag, Backpack Waterproof School...
Minnie Trio School Bag Waterproof School Bag
Thomas School Bag Waterproof School Bag
Sara Green 002 Shoulder Bag
Disney Frozen Anna &amp; Elsa Pink Sequins 16' ' Backpack
Disney Princess Pink Flap 18' ' Backpack
My Baby Excel Peppa Side Sling Bag Sling Bag
Ranger Black School Bag with laptop compartment Waterpr...
HM International HMHTPB 73279AV Waterproof Multipurpos...
Peppa Peppa Pig Pink Plush Toy Wallet Round Shape Plush...
Disney Frozen Anna &amp; Elsa Pink Sequins 14' ' Backpack
Disney Frozen Magic Blue 16' ' School Bag
Good Friends stylish Waterproof School Bag
ZEVORA Pink 3D Design Children Travel &amp; School Bag, 1 L...
Gleam A103 School Bag
SARA BAGS TG15 Waterproof Backpack
Despicable Me Favourite Subject School Bag 16 inches Tr...
AARIP LTB037 Waterproof School Bag
Simba BTSSMURFS FOOTBALL 18 BP Waterproof School Bag
Gleam JB0402C Waterproof School Bag
Simba BTSSMURFS SMURFETTE SINGING STAR 18 BP Waterproo... ]
</code></pre>
",Dataset Preprocessing & Handling,compare list data csv file sort matching data set product name brand list need find much branded product list
Data frame text filtering with text,"<p>I need some help with running some filter on some data. I have a a data set made up of text. And i also have a list of words. I would like to filter each row of my data such that the remaining text in the rows will be made up of only words in the list object</p>

<pre><code>words

(cell, CDKs, lung, mutations monomeric, Casitas, Background, acquired, evidence, kinases, small, evidence, Oncogenic )


data

ID  Text

0   Cyclin-dependent kinases CDKs regulate a 

1   Abstract Background Non-small cell lung  

2   Abstract Background Non-small cell lung 

3   Recent evidence has demonstrated that acquired

4   Oncogenic mutations in the monomeric Casitas  
</code></pre>

<p>so after my filter i would like the data-frame to look like this </p>

<pre><code>data

ID  Text

0    kinases CDKs  

1   Background cell lung  

2   Background small cell lung 

3   evidence acquired

4   Oncogenic mutations monomeric Casitas  
</code></pre>

<p>I tried using the <code>iloc</code> and similar functions but I dont seem to get it. any help with that?</p>
",Dataset Preprocessing & Handling,data frame text filtering text need help running filter data data set made text also list word would like filter row data remaining text row made word list object filter would like data frame look like tried using similar function dont seem get help
Gensim: how to load precomputed word vectors from text file,"<p>I have a text file with my precomputed word vectors in the following format (example):</p>

<p><code>word -0.0762464299711 0.0128308048976 ... 0.0712385589283\n’</code></p>

<p>on each line for every word (with 297 extra floats in place of the <code>...</code>). I am trying to load these with Gensim as KeyedVectors, because I ultimately would like to compute the cosine similarity, find most similar words, etc. Unfortunately I have not worked with Gensim before and from the documentation it's not quite clear to me how to do this. I have tried the following which I found <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""noreferrer"">here</a>:</p>

<p><code>word_vectors = KeyedVectors.load_word2vec_format('/embeddings/word.vectors', binary=False)</code></p>

<p>However this gives the following error:</p>

<p><code>ValueError: invalid literal for int() with base 10: 'the'</code></p>

<p>'the' is the first word in the text file, so I suspect that the loading function is expecting something to be there that is not. But I can't find any information on what should be there. I would highly appreciate a pointer to such information or any other solution to my problem. Thanks!</p>
",Dataset Preprocessing & Handling,gensim load precomputed word vector text file text file precomputed word vector following format example line every word extra float place trying load gensim keyedvectors ultimately would like compute cosine similarity find similar word etc unfortunately worked gensim documentation quite clear tried following found however give following error first word text file suspect loading function expecting something find information would highly appreciate pointer information solution problem thanks
Add UMLS username and password to Ctakes,"<p>I'm a new user on cTAKES. I followed User Install Guide and add UMLS access rights. After I edited runctakesCPE.bat and runctakesCVD.bat, I got the following:</p>

<blockquote>
  <p>F:\apache-ctakes-4.0.0>set CTAKES_HOME=F:\apache-ctakes-4.0.0</p>
  
  <p>F:\apache-ctakes-4.0.0>if exist
  ""F:\apache-ctakes-4.0.0\bin\runctakesCVD.bat"" goto okHome</p>
  
  <p>F:\apache-ctakes-4.0.0>if exist ""C:\Program
  Files\Java\jdk1.8.0_65\bin\java.exe"" set PATH=C:\Program 
  Files\Java\jdk1.8.0_65\bin;C:\Program
  Files\Java\jdk1.8.0_65\bin;C:\Program Files\Java\jdk1.8.0_65\
  jre\bin;C:\ProgramData\Oracle\Java\javapath;C:\Program Files
  (x86)\Intel\iCLS Client\;C:\Program Fil es\Intel\iCLS
  Client\;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\Wi
  ndowsPowerShell\v1.0\;C:\Program Files\Intel\Intel(R) Management
  Engine Components\DAL;C:\Program Fi les (x86)\Intel\Intel(R)
  Management Engine Components\DAL;C:\Program Files\Intel\Intel(R)
  Management  Engine Components\IPT;C:\Program Files
  (x86)\Intel\Intel(R) Management Engine Components\IPT;c:\Pro gram
  Files (x86)\Hewlett-Packard\HP Performance Advisor;C:\Program
  Files\Microsoft\Web Platform Inst aller\;C:\Program Files
  (x86)\Microsoft ASP.NET\ASP.NET Web Pages\v1.0\;C:\Program Files
  (x86)\Windo ws Kits\8.0\Windows Performance Toolkit\;C:\Program
  Files\Microsoft SQL Server\110\Tools\Binn\;C:\Pr ogram Files\IDM
  Computer Solutions\UltraEdit;C:\Python27;C:\Program Files (x86)\MiKTeX
  2.9\miktex\bi n\;C:\Program Files\MATLAB\R2015b\runtime\win64;C:\Program
  Files\MATLAB\R2015b\bin;C:\Program Files\
  IBM\SPSS\Statistics\24\JRE\bin;C:\cygwin64\bin;C:\Program Files
  (x86)\Skype\Phone\;C:\Program Files
  (x86)\Google\Chrome\Application;C:\Program
  Files\Java\jdk1.8.0_65\bin;C:\Program Files\Java\jdk1.8.0
  _65\jre\bin;C:\ProgramData\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program  Files\Intel\iCLS
  Client\;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System3
  2\WindowsPowerShell\v1.0\;C:\Program Files\Intel\Intel(R) Management
  Engine Components\DAL;C:\Progra m Files (x86)\Intel\Intel(R)
  Management Engine Components\DAL;C:\Program Files\Intel\Intel(R)
  Manage ment Engine Components\IPT;C:\Program Files
  (x86)\Intel\Intel(R) Management Engine Components\IPT;c: \Program
  Files (x86)\Hewlett-Packard\HP Performance Advisor;C:\Program
  Files\Microsoft\Web Platform Installer\;C:\Program Files
  (x86)\Microsoft ASP.NET\ASP.NET Web Pages\v1.0\;C:\Program Files
  (x86)\W indows Kits\8.0\Windows Performance Toolkit\;C:\Program
  Files\Microsoft SQL Server\110\Tools\Binn\;C :\Program Files\IDM
  Computer Solutions\UltraEdit;C:\Python27;C:\Users\yi
  chen\AppData\Local\Programs \Git\cmd;C:\Users\yi
  chen\AppData\Local\Programs\EmEditor</p>
  
  <p>F:\apache-ctakes-4.0.0>cd F:\apache-ctakes-4.0.0</p>
  
  <p>F:\apache-ctakes-4.0.0>IF """" == """" GOTO NoParam</p>
  
  <p>F:\apache-ctakes-4.0.0>echo Use the GUI to select the AE to load Use
  the GUI to select the AE to load</p>
  
  <p>F:\apache-ctakes-4.0.0>java -Dctakes.umlsuser=
  ""F:\apache-ctakes-4.0.0\desc\;F:\apache-ctakes-4.0.0\
  resources\;F:\apache-ctakes-4.0.0\lib*""
  -Dlog4j.configuration=file:\F:\apache-ctakes-4.0.0\config\l og4j.xml -Xms512M -Xmx3g org.apache.uima.tools.cvd.CVD 0-cp The system cannot find the file specified.</p>
  
  <p>F:\apache-ctakes-4.0.0>GOTO ChangeBack</p>
</blockquote>

<p>Can anyone help to figure it? I really need your help.
Thanks!</p>
",Dataset Preprocessing & Handling,add umls username password ctakes new user ctakes followed user install guide add umls access right edited runctakescpe bat runctakescvd bat got following f apache ctakes set ctakes home f apache ctakes f apache ctakes exist f apache ctakes bin runctakescvd bat goto okhome f apache ctakes exist c program file java jdk bin java exe set path c program file java jdk bin c program file java jdk bin c program file java jdk jre bin c programdata oracle java javapath c program file x intel icls client c program fil e intel icls client c window system c window c window system wbem c window system wi ndowspowershell v c program file intel intel r management engine component dal c program fi le x intel intel r management engine component dal c program file intel intel r management engine component ipt c program file x intel intel r management engine component ipt c pro gram file x hewlett packard hp performance advisor c program file microsoft web platform inst aller c program file x microsoft asp net asp net web page v c program file x windo w kit window performance toolkit c program file microsoft sql server tool binn c pr ogram file idm computer solution ultraedit c python c program file x miktex miktex bi n c program file matlab r b runtime win c program file matlab r b bin c program file ibm spss statistic jre bin c cygwin bin c program file x skype phone c program file x google chrome application c program file java jdk bin c program file java jdk jre bin c programdata oracle java javapath c program file x intel icls client c program file intel icls client c window system c window c window system wbem c window system windowspowershell v c program file intel intel r management engine component dal c progra file x intel intel r management engine component dal c program file intel intel r manage ment engine component ipt c program file x intel intel r management engine component ipt c program file x hewlett packard hp performance advisor c program file microsoft web platform installer c program file x microsoft asp net asp net web page v c program file x w indows kit window performance toolkit c program file microsoft sql server tool binn c program file idm computer solution ultraedit c python c user yi chen appdata local program git cmd c user yi chen appdata local program emeditor f apache ctakes cd f apache ctakes f apache ctakes goto noparam f apache ctakes echo use gui select ae load use gui select ae load f apache ctakes java dctakes umlsuser f apache ctakes desc f apache ctakes resource f apache ctakes lib dlog j configuration file f apache ctakes config l og j xml xms xmx g org apache uima tool cvd cvd cp system find file specified f apache ctakes goto changeback anyone help figure really need help thanks
How to See Original Words that Mapped to a Particular Stem Word,"<p>I'm doing some text analysis using tm_map in R. I run the following code (no errors) to produce a Document Term Matrix of (stemmed and otherwise pre-processed) words.</p>

<pre><code>  corpus = Corpus(VectorSource(textVector))
  corpus = tm_map(corpus, tolower)
  corpus = tm_map(corpus, PlainTextDocument) 
  corpus = tm_map(corpus, removePunctuation)
  corpus = tm_map(corpus, removeWords, c(stopwords(""english"")))
  corpus = tm_map(corpus, stemDocument, language=""english"")

  dtm = DocumentTermMatrix(corpus)
  mostFreqTerms = findFreqTerms(dtm, lowfreq=125) 
</code></pre>

<p>But when I look at my (stemmed) mostFreqTerms, I see a couple that make me think, ""hm, what words were stemmed to produce that?"" Also, there may be stem words that make sense to me at first glance, but maybe I'm missing the fact that they actually contain words with different meanings.</p>

<p>I'd like to apply the strategy/technique described in this SO answer on retaining specific terms during stemming (e.g. keeping ""natural"" and ""naturalized"" from becoming the same stemmed term.
<a href=""https://stackoverflow.com/questions/16069406/text-mining-with-the-tm-package-word-stemming"">Text-mining with the tm-package - word stemming</a></p>

<p>But to do so most comprehensively, I'd like to see a list of all the separate words that mapped to my most frequent stem words. Is there a way to find the words that, when stemmed, produced my list of mostFreqTerms?</p>

<p>EDIT: REPRODUCIBLE EXAMPLE</p>

<pre><code>textVector = c(""Trisha Takinawa: Here comes Mayor Adam West 
               himself. Mr. West do you have any words 
               for our viewers?Mayor Adam West: Box toaster
               aluminum maple syrup... no I take that one 
               back. Im gonna hold onto that one. 
               Now MaxPower is adding adamant
               so this example works"")

      corpus = Corpus(VectorSource(textVector))
      corpus = tm_map(corpus, tolower)
      corpus = tm_map(corpus, PlainTextDocument) 
      corpus = tm_map(corpus, removePunctuation)
      corpus = tm_map(corpus, removeWords, c(stopwords(""english"")))
      corpus = tm_map(corpus, stemDocument, language=""english"")

      dtm = DocumentTermMatrix(corpus)
      mostFreqTerms = findFreqTerms(dtm, lowfreq=2) 
      mostFreqTerms
</code></pre>

<p>...The above mostFreqTerms outputs</p>

<blockquote>
  <p>[1] ""adam"" ""one""   ""west""</p>
</blockquote>

<p>I'm looking for a programmatic way to determine that the stem word ""adam"" came from original words ""adam"" and ""adamant"".</p>
",Dataset Preprocessing & Handling,see original word mapped particular stem word text analysis using tm map r run following code error produce document term matrix stemmed otherwise pre processed word look stemmed mostfreqterms see couple make think hm word stemmed produce also may stem word make sense first glance maybe missing fact actually contain word different meaning like apply strategy technique described answer retaining specific term stemming e g keeping natural naturalized becoming stemmed term looking programmatic way determine stem word adam came original word adam adamant
"how to read text files in quanteda, storing each line as a document","<p>I have texts stored in several files.<br>
Within the files each line is a document (text of a blog post, text of a tweet Etc.).<br>
If I read using the readtext package in the default way shown in <a href=""https://cran.r-project.org/web/packages/readtext/vignettes/readtext_vignette.html#plain-text-files-.txt"" rel=""nofollow noreferrer"">doc/examples</a> the content of each file will be a single document instead of each line being a document.</p>

<p>My goal is to use a quanteda corpus, with each line stored as a document.<br>
I am using readtext as it is a companion package to quanteda, but using readtext is not a strict requirement.</p>

<p>I would like to avoid manually splitting the originary files in smaller files each corresponding to a line.</p>
",Dataset Preprocessing & Handling,read text file quanteda storing line document text stored several file within file line document text blog post text tweet etc read using readtext package default way shown doc example content file single document instead line document goal use quanteda corpus line stored document using readtext companion package quanteda using readtext strict requirement would like avoid manually splitting originary file smaller file corresponding line
R define function for comparing number of string occurrences between cells in data frame,"<p>Generally speaking, I am trying to define a function which will compare the number of occurrences of specified strings per row between two columns, and modify the value of a third column depending on the comparisons outcome. </p>

<p>More specifically, I want a function which corrects the sentiment value of a word if there is a negation in the word which is not in the stem - given that the sentiment value currently in the dataframe is associated with the stem.</p>

<p>Example data frame:</p>

<pre><code>df &lt;- data.frame(word=c(""disgraceful"",""ungrateful"",""impatient"",""importantly"",""undisclosed"",""disloyal"",""loyal""), 
             stem=c(""grace"",""grateful"",""patient"",""important"",""disclosed"",""loyal"",""loyal""), 
             sentiment=c(1,1,1,1,1,1,1))


  word        stem      sentiment
1 disgraceful grace     1
2 ungrateful  grateful  1
3 impatiently patient   1
4 important   important 1
5 undisclosed disclosed 1
6 disloyal    loyal     1
7 loyal       loyal     1
</code></pre>

<p>Desired outcome after running the newly defined <code>correct_negation(df,word,stem,sentiment)</code> function:</p>

<pre><code>  word        stem      sentiment
1 disgraceful grace     -1
2 ungrateful  grateful  -1
3 impatiently patient   -1
4 important   important  1
5 undisclosed disclosed -1
6 disloyal    loyal     -1
7 loyal       loyal      1
</code></pre>

<p><strong>EDIT</strong></p>

<p>I have the following now which is working, but failed to put it in a function. It reverses the sentiment if a word has a negation and the stem doesn't (doesn't work for ""undisclosed""): </p>

<pre><code>negations &lt;- c(""dis"", ""un"", ""im"")

df[[""sentiment_adj""]] &lt;- ifelse (
  str_detect(df[[""word""]], paste(negations, collapse = ""|""))==TRUE &amp;
    str_detect(df[[""stem""]], paste(negations, collapse = ""|""))==FALSE,
  df[[""sentiment""]] * -1, df[[""sentiment""]])
</code></pre>

<p><strong>EDIT 2</strong></p>

<p>I managed to get the right sentiments by checking if the extra bits in the word on top of the stem have a negation in them:</p>

<pre><code>df &lt;- df %&gt;%
  rowwise() %&gt;%
  mutate(leftover=paste(unlist(strsplit(as.character(word), split=as.character(stem), fixed=TRUE)),
                     collapse = "" ""))

df[[""sentiment_adj2""]] &lt;- ifelse (
  str_detect(df[[""leftover""]], paste(negations, collapse = ""|""))==TRUE,
  df[[""sentiment""]] * -1, df[[""sentiment""]])
</code></pre>
",Dataset Preprocessing & Handling,r define function comparing number string occurrence cell data frame generally speaking trying define function compare number occurrence specified string per row two column modify value third column depending comparison outcome specifically want function corrects sentiment value word negation word stem given sentiment value currently dataframe associated stem example data frame desired outcome running newly defined function edit following working failed put function revers sentiment word ha negation stem work undisclosed edit managed get right sentiment checking extra bit word top stem negation
nltk taggedcorpusreader error,"<p>I'm building a TaggedCorpusReader in NLTK (using ipython notebooks) to read in some POS tagged files from the ANC. (<a href=""http://www.anc.org/"" rel=""nofollow noreferrer"">http://www.anc.org/</a>) I want to get all of the adjectives from the tagged corpus. Here's what I tried:</p>

<pre><code>anc = nltk.corpus.reader.tagged.TaggedCorpusReader(anc_root, r"".*\.txt"", sep='_')
tagged_words = anc.tagged_words()
anc_adj = {word.lower() for word, pos in tagged_words if pos =='JJ'}
</code></pre>

<p>All of the functions (tagged_words(), words(), sents() etc) work fine. But when I try to do the set comprehension, I get the following assertion error:</p>

<pre><code>---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&lt;ipython-input-70-4ba2a8ab817a&gt; in &lt;module&gt;()
      2 tagged_words = anc.tagged_words()
      3 print(tagged_words)
----&gt; 4 anc_adj = {word.lower() for word, pos in tagged_words if pos =='JJ'}

&lt;ipython-input-70-4ba2a8ab817a&gt; in &lt;setcomp&gt;(.0)
      2 tagged_words = anc.tagged_words()
      3 print(tagged_words)
----&gt; 4 anc_adj = {word.lower() for word, pos in tagged_words if pos =='JJ'}

C:\Program Files\Anaconda3\lib\site-packages\nltk\corpus\reader\util.py in iterate_from(self, start_tok)
    400 
    401             # Get everything we can from this piece.
--&gt; 402             for tok in piece.iterate_from(max(0, start_tok-offset)):
    403                 yield tok
    404 

C:\Program Files\Anaconda3\lib\site-packages\nltk\corpus\reader\util.py in iterate_from(self, start_tok)
    299                 self.read_block.__name__)
    300             num_toks = len(tokens)
--&gt; 301             new_filepos = self._stream.tell()
    302             assert new_filepos &gt; filepos, (
    303                 'block reader %s() should consume at least 1 byte (filepos=%d)' %

C:\Program Files\Anaconda3\lib\site-packages\nltk\data.py in tell(self)
   1364             check1 = self._incr_decode(self.stream.read(50))[0]
   1365             check2 = ''.join(self.linebuffer)
-&gt; 1366             assert check1.startswith(check2) or check2.startswith(check1)
   1367 
   1368         # Return to our original filepos (so we don't have to throw

AssertionError:
</code></pre>

<p>I have no idea what this means! Can someone help me understand what the problem is here? Doing the set comprehension on the Brown corpus works fine...what is going on?</p>
",Dataset Preprocessing & Handling,nltk taggedcorpusreader error building taggedcorpusreader nltk using ipython notebook read po tagged file anc want get adjective tagged corpus tried function tagged word word sent etc work fine try set comprehension get following assertion error idea mean someone help understand problem set comprehension brown corpus work fine going
R - Concatenate as aggregation function,"<p>Let's say I have a data frame consisting of sentence IDs and terms like so:</p>

<pre><code>data.frame(sid = c(1, 1, 2, 2), text = c(""hello"", ""world"", ""whats"", ""up""))
</code></pre>

<p>How could I aggregate it to get a data frame like so:</p>

<pre><code>data.frame(sid = c(1, 2), text = c(""hello world"", ""whats up""))
</code></pre>

<p>Or better yet, as a list with corresponding indices like so:</p>

<pre><code>list(""hello world"", ""whats up"")
</code></pre>
",Dataset Preprocessing & Handling,r concatenate aggregation function let say data frame consisting sentence id term like could aggregate get data frame like better yet list corresponding index like
How to find Most frequently used words used on data using Python?,"<p>I am doing a sentiment analysis project in Python (using Natural Language Processing). I already collected the data from twitter and saved it as a CSV file. The file contains tweets, which are mostly about cryptocurrency. I cleaned the data and applied sentiment analysis using classification algorithms.</p>

<p>Since the data is clean, I want to find the most frequently used words. Here's the code that I used to import the libraries and the csv file:</p>

<pre><code># importing Libraries
from pandas import DataFrame, read_csv
import chardet
import matplotlib.pyplot as plt; plt.rcdefaults()
from matplotlib import rc
%matplotlib inline
import pandas as pd
plt.style.use('ggplot')
import numpy as np
import re
import warnings

#Visualisation
import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns
from IPython.display import display
from mpl_toolkits.basemap import Basemap
from wordcloud import WordCloud, STOPWORDS

#nltk
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.sentiment.util import *
from nltk import tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem.snowball import SnowballStemmer


matplotlib.style.use('ggplot')
pd.options.mode.chained_assignment = None
warnings.filterwarnings(""ignore"")

## Reading CSV File and naming the object called crime
ltweet=pd.read_csv(""C:\\Users\\name\\Documents\\python assignment\\bitcoin1.csv"",index_col = None, skipinitialspace = True)
print(btweet)
</code></pre>

<p>There is no need for me to post the other codes because they are very long.
For data cleaning, I got rid of hyperlinks, RT(Retweeted), URL, Punctuation's, put text in lowercase, etc.</p>

<p>Here's the output for the list of positive tweets for example</p>

<pre><code>In [35]: btweet[btweet.sentiment_type == 'POSITIVE'].Tweets.reset_index(drop = True)[0:5]

Out[35]:
0    anizameddine more than just bitcoin blockchain...
1    bitcoinmagazine icymi wyoming house unanimousl...
2    bitracetoken bitrace published the smart contr...
3    unusual and quite promising ico banca banca_of...
4    airdrop coinstocks link it is a exchange so ge...
Name: Tweets, dtype: object
</code></pre>

<p>Is there a way to find the most frequently used words in the data? Can anyone help me write the code for it?</p>
",Dataset Preprocessing & Handling,find frequently used word used data using python sentiment analysis project python using natural language processing already collected data twitter saved csv file file contains tweet mostly cryptocurrency cleaned data applied sentiment analysis using classification algorithm since data clean want find frequently used word code used import library csv file need post code long data cleaning got rid hyperlink rt retweeted url punctuation put text lowercase etc output list positive tweet example way find frequently used word data anyone help write code
How to generate term matrix in guided LDA for topic modeling?,"<p>I am currently working on analyzing online reviews. I would like to try GuidedLDA (<a href=""https://medium.freecodecamp.org/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164"" rel=""nofollow noreferrer"">https://medium.freecodecamp.org/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164</a>) as some of the topics overlap. I have successfully installed the package. 
However, I am not sure on how to generate the document term matrix (referred to as X in the code in the website) and vocab using the excel document as inputs. Can someone please help with this? I tried to search online in various forums and did not find anything that was working.</p>
",Dataset Preprocessing & Handling,generate term matrix guided lda topic modeling currently working analyzing online review would like try guidedlda topic overlap successfully installed package however sure generate document term matrix referred x code website vocab using excel document input someone please help tried search online various forum find anything wa working
Use Multithreading To Speed Up Pandas Dataframe Creation,"<p>The problem I'm having, and there doesn't seem to be any answers out there, is that I need to process a very large textfile (the gmdnTerms.txt file from GUDID), manipulate the data to merge lines with duplicate IDs, create appropriate columns for the key value pairs, and dump the results to a CSV file. I've done everything I can think of to increase efficiency except to implement multithreading. I need to be able to multithread the process of iterating over the text file and building the dataframe. The multithreading tutorials weren't much help. Hoping an experienced Python programmer can give a clear answer. Below is the entire program. Please help, current run time is >20 hours on a 4.7GHz proc (8 cores) with 16GB RAM and an SSD.</p>

<pre><code>#Assumptions this program makes:
#That duplicate product IDs will immediately follow each other
#That the first line of the text file contains only the keys and no values
#That the data lines are delimited by a ""\n"" character
#That the individual values are delimited by a ""|"" character
#The first value in each line will always be a unique product ID
#Each line will have exactly 3 values
#Each line's values will always be in the same order

#Import necessary libraries
import os
import pandas as pd
import mmap
import time

#Time to run
startTime = time.time()

#Parameters of the program
fileLocation = ""C:\\Users\User\....\GMDNTest.txt""
outCSVFile = ""GMDNTermsProcessed.csv""
encodingCSVFile = ""utf-8""

#Sets up variables to be used later on
df = pd.DataFrame()
keys = []
idx = 0
keyNum = 0
firstLine = True
firstValue = True
currentKey = ''

#This loops over each line in text file and collapses lines with duplicate Product IDs while building new columns for appropriate keys and values
#These collapsed lines and new columns are stored in a dataframe
with open (fileLocation, ""r+b"") as myFile:
    map = mmap.mmap(myFile.fileno(), 0, access=mmap.ACCESS_READ)
    for line in iter(map.readline, """"):

        #Gets keys from first line, splits them, stores in list
        if firstLine == True:
            keyRaw = line.split(""|"")
            keyRaw = [x.strip() for x in keyRaw]
            keyOne = keyRaw[0]
            firstLine = False

        #All lines after first go through this
        #Collapses lines by comparing the unique ID
        #Stores collapsed KVPs into a dataframe
        else:
            #Appends which number of key we are at to the key and breaks up the values into a list
            keys = [x + ""_"" + str(keyNum) for x in keyRaw]
            temp = line.split(""|"")
            temp = [x.strip() for x in temp]

            #If the key is the same as the key on the last line this area is run through
            #If this is the first values line it also goes through here
            if temp[0] == currentKey or firstValue == True:

                #Only first values line hits this part; gets first keys and builds first new columns
                if firstValue == True:
                    currentKey = temp[0]
                    df[keyOne] = """"
                    df.at[idx, keyOne] = temp[0]
                    df[keys[1]] = """"
                    df.at[idx, keys[1]] = temp[1]
                    df[keys[2]] = """"
                    df.at[idx, keys[2]] = temp[2]
                    firstValue = False

                #All other lines with the same key as the last line go through here
                else:
                    headers = list(df.columns.values)
                    if keys[1] in headers:
                        df.at[idx, keys[1]] = temp[1]
                        df.at[idx, keys[2]] = temp[2]
                        else:
                        df[keys[1]] = """"
                        df.at[idx, keys[1]] = temp[1]
                        df[keys[2]] = """"
                        df.at[idx, keys[2]] = temp[2]

            #If the current line has a different key than the last line this part is run through
            #Sets new currentKey and adds values from that line to the dataframe
            else:
                idx+=1
                keyNum = 0
                currentKey = temp[0]
                keys = [x + ""_"" + str(keyNum) for x in keyRaw]
                df.at[idx, keyOne] = temp[0]
                df.at[idx, keys[1]] = temp[1]
                df.at[idx, keys[2]] = temp[2]

        #Don't forget to increment that keyNum      
        keyNum+=1

#Dumps dataframe of collapsed values to a new CSV file
df.to_csv(outCSVFile, encoding=encodingCSVFile, index=False)

#Show us the approx runtime
print(""--- %s seconds ---"" % (time.time() - startTime))
</code></pre>
",Dataset Preprocessing & Handling,use multithreading speed panda dataframe creation problem seem answer need process large textfile gmdnterms txt file gudid manipulate data merge line duplicate id create appropriate column key value pair dump result csv file done everything think increase efficiency except implement multithreading need able multithread process iterating text file building dataframe multithreading tutorial much help hoping experienced python programmer give clear answer entire program please help current run time hour ghz proc core gb ram ssd
Cleaning the data from csv file,"<p>I'm doing the sentiment analysis on crpytocurrency. My job is to clean the data from the csv file. The data was generated(From Twitter) and saved in a csv file. Before doing the sentiment Analysis part. I have to clean the data .For example, Delete the punctuations, the URLs, put the test in a lower case. Those are the tweets. </p>

<p>## I already imported useful libraries for example NLTK(Natural language processing), pandas, numpy, and others. </p>

<p>Here's the output for the 'Tweets' Column.</p>

<pre><code>   ctweet['Tweets'][0:6]



 Out[5]:


    0    RT @TheLTCnews: The @LTCFoundation has publish...
    1    RT @WildchildSings: ""https:/ "" + /t.co/""FZrGw6xsZU ac...""
    2    RT @HODL_Whale: 5 days until #LitePay launches...
    3    LTC to USD price $211.92 ""https:/"" + /t.co/""CFjg1mIg...""
    4    LTC to BTC price B0.020218 ""https:/"" +/t.co/""XPL8NI...""
    5    LTC to GBP price £151.89 ""https:/"" +/t.co/""iOIbhgyd...""
    6    Litecoin dropped into the bear zone as sugges...
    Name: Tweets, dtype: object

# the output contains url. Because stackoverflow won't allow me to post the url. I have to change the method for url like adding ""quotes"" and ""//"".  
</code></pre>

<p>My next task was to clean the data. Here's the preprocessing code.</p>

<pre><code>#Preprocessing del RT @blablabla:
ctweet['tweetos'] = '' 

#add tweetos first part
for i in range(len(ctweet['Tweets'])):
    try:
        ctweet['tweetos'][i] = ctweet['Tweets'].str.split(' ')[i][0]
    except AttributeError:    
        ctweet['tweetos'][i] = 'other'

        #Preprocessing tweetos. select tweetos contains 'RT @'
        for i in range(len(ctweet['Tweets'])):
            if ctweet['tweetos'].str.contains('@')[i]  == False:
                ctweet['tweetos'][i] = 'other'

        # remove URLs, RTs, and twitter handles
        for i in range(len(ctweet['Tweets'])):
            ctweet['Tweets'][i] = "" "".join([word for word in ctweet['Tweets'][i].split()
                                        if 'http' not in word and '@' not in word and '&lt;' not in word])

  ctweet['Tweets'][0]
</code></pre>

<p>The code above it will delete the punctuation, the urls, put test in a lower case, extract username for examples. When I run that code it gives an error.</p>

<pre><code>TypeErrorTraceback (most recent call last)
&lt;ipython-input-3-8254e078073a&gt; in &lt;module&gt;()
      5 for i in range(len(ctweet['Tweets'])):
      6     try:
----&gt; 7         ctweet['tweetos'][i] = ctweet['Tweets'].str.split(' ')[i][0]
      8     except AttributeError:
      9         ctweet['tweetos'][i] = 'other'

TypeError: 'float' object has no attribute '__getitem__'
</code></pre>

<p>What does that error mean? How can I solve this problem. Im using Jupyter Notebook 5.4.1</p>

<h2>Update part</h2>

<pre><code>AttributeErrorTraceback (most recent call last)
&lt;ipython-input-7-bb6b24f62739&gt; in &lt;module&gt;()
     16 # remove URLs, RTs, and twitter handles
     17 for i in range(len(ctweet['Tweets'])):
---&gt; 18     ctweet['Tweets'][i] = "" "".join([word for word in ctweet['Tweets'][i].split()
     19                                 if 'http' not in word and '@' not in word and '&lt;' not in word])
     20 

AttributeError: 'float' object has no attribute 'split'
</code></pre>
",Dataset Preprocessing & Handling,cleaning data csv file sentiment analysis crpytocurrency job clean data csv file data wa generated twitter saved csv file sentiment analysis part clean data example delete punctuation url put test lower case tweet already imported useful library example nltk natural language processing panda numpy others output tweet column next task wa clean data preprocessing code code delete punctuation url put test lower case extract username example run code give error doe error mean solve problem im using jupyter notebook update part
Tagging a .txt file from Inaugural Address Corpus,"<p>I'm having a hard time trying to figure this out.  New to coding.  I'm trying to read a .txt file, tokenize it, pos tag the words in it.</p>

<p>Here's what I've got so far:</p>

<pre><code>import nltk
from nltk import word_tokenize
import re

file = open('1865-Lincoln.txt', 'r').readlines()
text = word_tokenize(file)
string = str(text)
nltk.pos_tag(string)
</code></pre>

<p>My problem is, it keeps giving me the <code>TypeError: expected string or bytes-like object</code> error.</p>
",Dataset Preprocessing & Handling,tagging txt file inaugural address corpus hard time trying figure new coding trying read txt file tokenize po tag word got far problem keep giving error
How to save a trained NLTK POS-tagger,"<p>I was wondering how to save a trained NLTK (Unigram)Tagger. I train a <code>Portuguese UnigramTagger</code> with the following code, depending on the corpus it may take a while for it to run, so I'd like to avoid rerunning it.</p>

<pre><code>import nltk
from nltk import mac_morpho

def get_unigram_tagger():
  p_train = 0.9
  tagged_sents = mac_morpho.tagged_sents()
  size = int(len(tagged_sents)*0.9)
  train_sents = tagged_sents[:size]
  test_sents = tagged_sents[size:]
  uni_tagger = nltk.UnigramTagger(train_sents)
  print ""Test accuracy ="", uni_tagger.evaluate(test_sents)
  return uni_tagger
</code></pre>

<p>So I get <code>uni_tagger</code> from this function and I have to recompute it if I'm running the program again. Maybe I can save <code>uni_tagger</code> somehow so that next time I just need to read it (weights and such) from a file.</p>
",Dataset Preprocessing & Handling,save trained nltk po tagger wa wondering save trained nltk unigram tagger train following code depending corpus may take run like avoid rerunning get function recompute running program maybe save somehow next time need read weight file
How to use NLP to parse naturally written commands?,"<p>I am fairly new to NLP in general. My goal is to create some kind of parser that can easily find files on my various hard drives. 
I have no idea how to properly parse the input to transform it into any managable representation that a program can easily use to create a given output.</p>

<p>For example, the following sentences should return a list of documents:</p>

<ol>
<li><p><code>documents created 3 months ago</code></p></li>
<li><p><code>documents modified 2 weeks ago</code></p></li>
<li><p><code>photos taken in china</code> (this one would then use the GPS data within the image file)</p></li>
</ol>

<p>It can probably be easily done using some kind of Regex pattern (<code>&lt;filetype&gt; &lt;action&gt; &lt;time&gt;</code>) but I would love to make it more flexible.</p>

<p>I looked into <a href=""https://github.com/spencermountain/compromise/wiki/Projects"" rel=""nofollow noreferrer"">compromise</a>, a JS library that has some easy to use API to retrieve specific parts of the input. But I kind of doubt that calling methods like <code>calculatedResult.nouns()[0]</code> and <code>calculatedResult.verbs()[0].stem()</code> should be used to parse the commands as those require a fixed kind of syntax. </p>

<p>Any tips on how to achieve my goal? I am not sure if using ML and training a custom model is the way to go. I never use ML and, based on my low knowledge of it, it seems kind of hard to train it constructs like those (as I would need a LOT of example sentences but there is just a finite amount of realisically used combinations that make sense).</p>
",Dataset Preprocessing & Handling,use nlp parse naturally written command fairly new nlp general goal create kind parser easily find file various hard drive idea properly parse input transform managable representation program easily use create given output example following sentence return list document one would use gps data within image file probably easily done using kind regex pattern would love make flexible looked compromise j library ha easy use api retrieve specific part input kind doubt calling method like used parse command require fixed kind syntax tip achieve goal sure using ml training custom model way go never use ml based low knowledge seems kind hard train construct like would need lot example sentence finite amount realisically used combination make sense
"How to compare metrics between two large texts - cosine, Jaccard similarities, Sim_MinEdit (Sim_String) and Sim_Simple in Python","<p>I am working on text analytics project comparing two different reports at a time and saving the results into pandas data frame.</p>

<p>I was able to get cosine and jacard similarities, but need to be sure that I get the right measures. As arguments I use file names that are located at the given folder.</p>

<p>For the cosine_sim I use the following code:</p>

<pre><code>import re, math
from collections import Counter

WORD = re.compile(r'\w+')

def text_to_vector(text):
     words = WORD.findall(text)
     return Counter(words)

def get_cosine(file1, file2):
    t1 = file(Input_path+'/'+file1).read().replace('\n',' ')
    t2 = file(Input_path+'/'+file2).read().replace('\n',' ')    
    vec1 = text_to_vector(t1)
    vec2 = text_to_vector(t2)

    intersection = set(vec1.keys()) &amp; set(vec2.keys())
    numerator = sum([vec1[x] * vec2[x] for x in intersection])
    sum1 = sum([vec1[x]**2 for x in vec1.keys()])
    sum2 = sum([vec2[x]**2 for x in vec2.keys()])
    denominator = math.sqrt(sum1) * math.sqrt(sum2)
    if not denominator:
       return 0.0
    else:
       return float(numerator) / denominator
</code></pre>

<p>For Jaccard I got the following:</p>

<pre><code>def get_jaccard(file1, file2):
    t1 = file(Input_path+'/'+file1).read().replace('\n',' ')
    t2 = file(Input_path+'/'+file2).read().replace('\n',' ')    
    vec1 = text_to_vector(t1)
    vec2 = text_to_vector(t2)

    numerator = len(set(vec1.keys()).intersection(set(vec2.keys())))
    denominator = float(len(set(vec1.keys())) + len(set(vec2.keys())) -numerator) 

    if not denominator:
       return 0.0
    else:
       return float(numerator) / denominator
</code></pre>

<p>The results work fine on small string examples, but I'm not sure if they are correct on big test files, especially Jaccard as my results are a bit different from distance.jaccard (I use lists to feed it, not dictionaries)</p>

<p>Regarding the other metrics, please confirm that sim_simple can be calculated by using the following:</p>

<pre><code>from difflib import SequenceMatcher

def similar(file1, file2):
    s1 = file(Input_path+'/'+file1).read().replace('\n',' ')
    s2 = file(Input_path+'/'+file2).read().replace('\n',' ')  
    list1 = list(set(text_to_list(s1)))
    list2 = list(set(text_to_list(s2)))
    return SequenceMatcher(None, list1, list2).ratio()
</code></pre>

<p>I am not sure about Sim_string (minimum edit). It is like Levenshtein distance, but on word level and I am not sure if distance.levenshtein would help.</p>

<p>Could you please help me to test the metrics I got and advise what to use for Sim_string. </p>

<p>Thank you very much!!!</p>
",Dataset Preprocessing & Handling,compare metric two large text cosine jaccard similarity sim minedit sim string sim simple python working text analytics project comparing two different report time saving result panda data frame wa able get cosine jacard similarity need sure get right measure argument use file name located given folder cosine sim use following code jaccard got following result work fine small string example sure correct big test file especially jaccard result bit different distance jaccard use list feed dictionary regarding metric please confirm sim simple calculated using following sure sim string minimum edit like levenshtein distance word level sure distance levenshtein would help could please help test metric got advise use sim string thank much
How to efficiently compute similarity between documents in a stream of documents,"<p>I gather Text documents (in Node.js) where one document <code>i</code> is represented as a list of words.
What is an efficient way to compute the similarity between these documents, taking into account that new documents are coming as a sort of stream of documents?</p>
<p>I currently use cos-similarity on the Normalized Frequency of the words within each document. I don't use the TF-IDF (Term frequency, Inverse document frequency) because of the scalability issue since I get more and more documents.</p>
<h2>Initially</h2>
<p>My first version was to start with the currently available documents, compute a big Term-Document matrix <code>A</code>, and then compute <code>S = A^T x A</code> so that <code>S(i, j)</code> is (after normalization by both <code>norm(doc(i))</code> and <code>norm(doc(j))</code>) the cos-similarity between documents <code>i</code> and <code>j</code> whose word frequencies are respectively <code>doc(i)</code> and <code>doc(j)</code>.</p>
<h2>For new documents</h2>
<p>What do I do when I get a new document <code>doc(k)</code>? Well, I have to compute the similarity of this document with all the previous ones, which doesn't require to build a whole matrix. I can just take the inner-product of <code>doc(k) dot doc(j)</code> for all previous <code>j</code>, and that result in <code>S(k, j)</code>, which is great.</p>
<h2>The troubles</h2>
<ol>
<li><p>Computing <code>S</code> in Node.js is really long. Way too long in fact! So I decided to create a C++ module which would do the whole thing much faster. And it does! But I cannot wait for it, I should be able to use intermediate results. And what I mean by &quot;not wait for it&quot; is both</p>
<p>a. wait for the computation to be done, but also<br />
b. wait for the matrix <code>A</code> to be built (it's a big one).</p>
</li>
<li><p>Computing new <code>S(k, j)</code> can take advantage of the fact that documents have way less words than the set of all the given words (which I use to build the whole matrix <code>A</code>). Thus, it looks faster to do it in Node.js, avoiding a lot of extra-resource to be taken to access the data.</p>
</li>
</ol>
<p>But is there any better way to do that?</p>
<p><strong>Note</strong> : the reason I started computing <code>S</code> is that I can easily build <code>A</code> in Node.js where I have access to all the data, and then do the matrix multiplication in C++ and get it back in Node.js, which speeds the whole thing a lot. But now that computing <code>S</code> gets impracticable, it looks useless.</p>
<p><strong>Note 2</strong> : yep, I don't have to compute the whole <code>S</code>, I can just compute the upper-right elements (or the lower-left ones), but that's not the issue. The time computation issue is not of that order.</p>
",Dataset Preprocessing & Handling,efficiently compute similarity document stream document gather text document node j one document represented list word efficient way compute similarity document taking account new document coming sort stream document currently use co similarity normalized frequency word within document use tf idf term frequency inverse document frequency scalability issue since get document initially first version wa start currently available document compute big term document matrix compute normalization co similarity document whose word frequency respectively new document get new document well compute similarity document previous one require build whole matrix take inner product previous result great trouble computing node j really long way long fact decided create c module would whole thing much faster doe wait able use intermediate result mean wait wait computation done also b wait matrix built big one computing new take advantage fact document way le word set given word use build whole matrix thus look faster node j avoiding lot extra resource taken access data better way note reason started computing easily build node j access data matrix multiplication c get back node j speed whole thing lot computing get impracticable look useless note yep compute whole compute upper right element lower left one issue time computation issue order
apply function to textreuse corpus,"<p>I have a data frame as follows:</p>

<pre><code>df&lt;-data.frame(revtext=c('the dog that chased the cat', 'the dog which chased the cat', 'World Cup Hair 2014 very funny.i can change', 'BowBow', 'this is'), rid=c('r01','r02','r03','r04','r05'), stringsAsFactors = FALSE)

                             revtext        rid
             the dog that chased the cat    r01
             the dog which chased the cat   r02
World Cup Hair 2014 very funny.i can change r03
             Bow Bow                        r04
             this is                        r05
</code></pre>

<p>I'm using the package <code>textreuse</code> to convert <code>df</code> to a <code>corpus</code> doing:</p>

<pre><code>#install.packages(textreuse)
library(textreuse)
d&lt;-df$revtext
names(d)&lt;-df$rid
corpus &lt;- TextReuseCorpus(text = d,
                      tokenizer = tokenize_character, k=3,
                      progress = FALSE,
                      keep_tokens = TRUE)
</code></pre>

<p>where <code>tokenize_character</code> is a function I programmed as:</p>

<pre><code> tokenize_character &lt;- function(document, k) {
                       shingles&lt;-c()
                 for( i in 1:( nchar(document) - k + 1 ) ) {
                         shingles[i] &lt;- substr(document,start=i,stop= (i+k-1))
                     }
return( unique(shingles) )  
}   
</code></pre>

<p>However, I'm prompted with some warnings: <code>Skipping document with ID 'r04' because it has too few words to create at least two n-grams with n = 3.</code>. But note that my tokenizer works on a character level. The text of <code>r04</code> is long enough. In fact, if we run <code>tokenize_character('BowBow',3)</code> we get: <code>""Bow"" ""owB"" ""wBo""</code> as desired. </p>

<p>Note also that for <code>r01</code>, <code>TextReuseCorpus</code> is working as it is supposed, returning: <code>tokens(corpus)$`r01= ""the"" ""he "" ""e d"" "" do"" ""dog"" ""og "" ""g t"" "" th"" ""tha"" ""hat"" ""at "" ""t c"" "" ch"" ""cha"" ""has"" ""ase"" ""sed"" ""ed "" ""d t"" ""e c"" "" ca"" ""cat""</code></p>

<p>Any suggestions? I don't know what I'm missing here. </p>
",Dataset Preprocessing & Handling,apply function textreuse corpus data frame follows using package convert function programmed however prompted warning note tokenizer work character level text long enough fact run get desired note also working supposed returning suggestion know missing
Convert dfmSparse from Quanteda package to Data Frame or Data Table in R,"<p>I have a dfmSparse object (large, with 2.1GB) which is tokenized and with ngrams (unigrams, bigrams, trigrams and fourgrams), and <strong>I want to convert it to a data frame or a data table object with the columns: Content and Frequency</strong>.</p>

<p>I tried to unlist... but didn't work. I'm new in NLP, and I don't know with method to use, I'm without ideas and didn't found a solution here or with Google.</p>

<p>Some info about the data:</p>

<pre><code>&gt;str(tokfreq)
Formal class 'dfmSparse' [package ""quanteda""] with 11 slots
  ..@ settings    :List of 1
  .. ..$ : NULL
  ..@ weighting   : chr ""frequency""
  ..@ smooth      : num 0
  ..@ ngrams      : int [1:4] 1 2 3 4
  ..@ concatenator: chr ""_""
  ..@ Dim         : int [1:2] 167500 19765478
  ..@ Dimnames    :List of 2
  .. ..$ docs    : chr [1:167500] ""character(0).content"" ""character(0).content"" ""character(0).content"" ""character(0).content"" ...
  .. ..$ features: chr [1:19765478] ""add"" ""lime"" ""juice"" ""tequila"" ...
  ..@ i           : int [1:54488417] 0 75 91 178 247 258 272 327 371 391 ...
  ..@ p           : int [1:19765479] 0 3218 3453 4015 4146 4427 4637 140665 140736 142771 ...
  ..@ x           : num [1:54488417] 1 1 1 1 5 1 1 1 1 1 ...
  ..@ factors     : list()

&gt;summary(tokfreq)
       Length         Class          Mode 
3310717565000     dfmSparse            S4
</code></pre>

<p>Thanks!</p>

<p>EDITED:
This is how I created the dataset from a corpus:</p>

<pre><code># tokenize
tokenized &lt;- tokenize(x = teste, ngrams = 1:4)
# Creating the dfm
tokfreq &lt;- dfm(x = tokenized)
</code></pre>
",Dataset Preprocessing & Handling,convert dfmsparse quanteda package data frame data table r dfmsparse object large gb tokenized ngrams unigrams bigram trigram fourgrams want convert data frame data table object column content frequency tried unlist work new nlp know method use without idea found solution google info data thanks edited created dataset corpus
Splitting a character into separate words in R,"<p>I am working on a project in R (on TED_Talks data set). I have  a data frame with one column called ""tags"" which contains a character like </p>

<p>""gaming,gender,sex,feminism,education,culture"". </p>

<p>The problem is, the whole row is being read as a single character.</p>

<p>I want the output to be a vector containing separate words. eg: </p>

<p>""gaming"",""gender"",""sex"",""feminism"",""education"",""culture""</p>

<p>so I can do further analysis on tags.</p>
",Dataset Preprocessing & Handling,splitting character separate word r working project r ted talk data set data frame one column called tag contains character like gaming gender sex feminism education culture problem whole row read single character want output vector containing separate word eg gaming gender sex feminism education culture analysis tag
Classification of Categories in Text Data,"<p>This may be an abstract question, but I always face difficulties with this kind of problem and it keeps on coming to me.</p>

<p>I crawled data (example: news articles about Tata Steel) extracted the content, manually read the content of each link and classified them as Finance, Operation, Sustainability and so on.</p>

<p>Then I made tf-idf data frame to be the features for classifier model. </p>

<p>I want to train the model to classify these articles. I am only left with either SVM or Logistic using the tf-idf features.</p>

<p>Is there a better approach to clssify text data? Can there be better approach rather then making tf-idf as we may loose information (contextual meaning of sentence) when breaking them into words and use as features.</p>

<p>Any algorithm which can help me to improve classification on text data? </p>
",Dataset Preprocessing & Handling,classification category text data may abstract question always face difficulty kind problem keep coming crawled data example news article tata steel extracted content manually read content link classified finance operation sustainability made tf idf data frame feature classifier model want train model classify article left either svm logistic using tf idf feature better approach clssify text data better approach rather making tf idf may loose information contextual meaning sentence breaking word use feature algorithm help improve classification text data
Generate three level dependency in case a verb is attached with non verb in dependency parsing,"<p>I am using dependency parsing for a use case in R with the corenlp package. However, I need to tweak the dataframe for a specific use case.</p>

<p>I need a dataframe where I have three columns. I have used the below code to reach till the dependency tree.</p>

<pre><code>devtools::install_github(""statsmaths/coreNLP"")
coreNLP::downloadCoreNLP()
initCoreNLP()
inp_cl = ""generate odd numbers from column one and print.""
output = annotateString(inp_cl)
dc = getDependency(output)

 sentence governor dependent      type governorIdx dependentIdx govIndex depIndex
1        1     ROOT  generate      root           0            1       NA        1
2        1  numbers       odd      amod           3            2        3        2
3        1 generate   numbers      dobj           1            3        1        3
4        1   column      from      case           5            4        5        4
5        1 generate    column nmod:from           1            5        1        5
6        1   column       one    nummod           5            6        5        6
7        1   column       and        cc           5            7        5        7
8        1 generate     print nmod:from           1            8        1        8
9        1   column     print  conj:and           5            8        5        8
10       1 generate         .     punct           1            7        1        10
</code></pre>

<p>Using POS tagging with the following code, I ended up with the following data frame.</p>

<pre><code>ps = getToken(output)

ps = ps[,c(1,2,7,3)]

colnames(dc)[8] = ""id""

dp = merge(dc, ps[,c(""sentence"",""id"",""POS"")], 
     by.x=c(""sentence"",""governorIdx""),by.y = c(""sentence"",""id""),all.x = T)

dp = merge(dp, ps[,c(""sentence"",""id"",""POS"")], 
     by.x=c(""sentence"",""dependentIdx""),by.y = c(""sentence"",""id""),all.x = T)

colnames(dp)[9:10] = c(""POS_gov"",""POS_dep"")


  sentence dependentIdx governorIdx governor dependent      type govIndex id POS_gov POS_dep
1         1            1           0     ROOT  generate      root       NA  1    &lt;NA&gt;      VB
2         1            2           3  numbers       odd      amod        3  2     NNS      JJ
3         1            3           1 generate   numbers      dobj        1  3      VB     NNS
4         1            4           5   column      from      case        5  4      NN      IN
5         1            5           1 generate    column nmod:from        1  5      VB      NN
6         1            6           5   column       one    nummod        5  6      NN      CD
7         1            7           5   column       and        cc        5  7      NN      CC
8         1            8           1 generate     print nmod:from        1  8      VB      NN
9         1            8           5   column     print  conj:and        5  8      NN      NN
10        1            9           1 generate         .     punct        1  9      VB       .
</code></pre>

<p>In case a verb(action word) is attached to a non-verb(non action word), but the  non-verb(non-action word) is connected to other non-verb(non-action words) then one row should indicate the entire connection. Eg: generate is a verb connected to numbers and numbers is a non verb connected to odd.</p>

<p>So the intended data frame needs to be</p>

<pre><code>Topic1 Topic2 Action
numbers odd    generate
column  from   generate
column  one    generate
column  and    generate
column  from   print
column  one    print
column  and    print
         .     generate
</code></pre>
",Dataset Preprocessing & Handling,generate three level dependency case verb attached non verb dependency parsing using dependency parsing use case r corenlp package however need tweak dataframe specific use case need dataframe three column used code reach till dependency tree using po tagging following code ended following data frame case verb action word attached non verb non action word non verb non action word connected non verb non action word one row indicate entire connection eg generate verb connected number number non verb connected odd intended data frame need
Compare unequal text parts in natural language processing,"<p>I have two texts, one which is read in from a DOCX file and the other from a TXT file. The layout of the DOCX file is like this:</p>

<blockquote>
  <p>{NAME} </p>
  
  <p>{ADDRESS}</p>
  
  <p>Dear Mr. Doe,</p>
  
  <p>In spite of your application on our website, we regret to inform you
  that you don't get the job.</p>
  
  <p>{END}</p>
  
  <p>TheCompany </p>
  
  <p>Registered under number 12345</p>
</blockquote>

<p>The layout of the TXT file is the same, but whatever is in brackets is filled in, which makes it impossible to just compare string one on one. It would look like this:</p>

<blockquote>
  <p>Jessy</p>
  
  <p>Hillington road 23</p>
  
  <p>Dear Mr. Doe,</p>
  
  <p>In spite of your application on our website, we regret to inform you
  that you >don't get the job.</p>
  
  <p>Best Regards,</p>
  
  <p>TheCompany</p>
  
  <p>Registered under number 12345</p>
</blockquote>

<p>I tried to split the text parts and compare those parts with each other. So a simple:</p>

<pre><code>' '.join(text1.split(split_after)[1:]).split(split_before)[0]
</code></pre>

<p>which is for the DOCX and the same for text2 (TXT) and then compare it. But as the sentence can change, so that it starts with </p>

<blockquote>
  <p>Despite of your application on our website (...)</p>
</blockquote>

<p>which then breaks all the code. Imagine this scenario for DOCX files up to 20 pages. I need something more useful and programmable.Any ideas?</p>
",Dataset Preprocessing & Handling,compare unequal text part natural language processing two text one read docx file txt file layout docx file like name address dear mr doe spite application website regret inform get job end thecompany registered number layout txt file whatever bracket filled make impossible compare string one one would look like jessy hillington road dear mr doe spite application website regret inform get job best regard thecompany registered number tried split text part compare part simple docx text txt compare sentence change start despite application website break code imagine scenario docx file page need something useful programmable idea
Searching for all matches in texts with Pandas,"<p>I have a list of particular words ('tokens') and need to find all of them (if any of them are present) in plain texts. I prefer using Pandas, to load text and perform the search. I'm using pandas as my collection of short text are timestamped and it is quite easy to organise these short text in a single data structure as pandas. </p>

<p><strong>For example:</strong></p>

<p>Consider a collection of fetched twitters uploaded in Pandas:</p>

<pre><code>                                              twitts
0                       today is a great day for BWM
1                    prices of german cars increased
2             Japan introduced a new model of Toyota
3  German car makers, such as BMW, Audi and VW mo...
</code></pre>

<p>and a list of car makers:</p>

<pre><code>list_of_car_makers = ['BMW', 'Audi','Mercedes','Toyota','Honda', 'VW']
</code></pre>

<p>Ideally, I need to get the following data frame:</p>

<pre><code>                                              twitts  cars_mentioned
0                       today is a great day for BMW  [BMW]
1                    prices of german cars increased  []
2             Japan introduced a new model of Toyota  [Toyota]
3  German car makers, such as BMW, Audi and VW mo...  [BMW, Audi, VW]
</code></pre>

<p>I'm very new to NLP and text mining methods, and I read/search on the internet a lot of materials on that topic. My guess is that I can use <code>regex</code> and use <code>re.findall()</code>, but then I need to iterate over the list of tokens (car makers) the entire dataframe.</p>

<p>Are there more succinct ways of doing this simple task, especially with Panads? </p>
",Dataset Preprocessing & Handling,searching match text panda list particular word token need find present plain text prefer using panda load text perform search using panda collection short text timestamped quite easy organise short text single data structure panda example consider collection fetched twitter uploaded panda list car maker ideally need get following data frame new nlp text mining method read search internet lot material topic guess use use need iterate list token car maker entire dataframe succinct way simple task especially panads
how to delete string between two different special characters in csv file in python,"<p>data looks like this:</p>

<pre><code>Aberrant(congenital)|artery(peripheral) ~Q27.8|cerebral Q28.3#
</code></pre>

<p>I want to delete string between ~ and | </p>
",Dataset Preprocessing & Handling,delete string two different special character csv file python data look like want delete string
Maximal term length in Document Term Matrix,"<p>Imagine the following Document Term Matrix created by tm package:</p>

<pre><code>&gt; frequencies
&lt;&lt;DocumentTermMatrix (documents: 255, terms: 470)&gt;&gt;
Non-/sparse entries: 7693/112157
Sparsity           : 94%
Maximal term length: 10
Weighting          : term frequency (tf)
</code></pre>

<p>what is Maximal term length?</p>
",Dataset Preprocessing & Handling,maximal term length document term matrix imagine following document term matrix created tm package maximal term length
Baum Welch (EM Algorithm) likelihood (P(X)) is not monotonically converging,"<p>So I am sort of an amateur when comes to machine learning and I am trying to program the Baum Welch algorithm, which is a derivation of the EM algorithm for Hidden Markov Models. Inside my program I am testing for convergence using the probability of each observation sequence in the new model and then terminating once the new model is less than or equal to the old model. However, when I run the algorithm it seems to converge somewhat and gives results that are far better than random but <strong>when converging it goes down on the last iteration</strong>. Is this a sign of a bug or am I doing something wrong? </p>

<p>It seems to me that I should have been using the <strong>summation of the log of each observation's probability</strong> for the comparison instead since it seems like the function I am maximizing. However, the paper I read said to use the <strong>log of the sum of probabilities</strong>(which I am pretty sure is the same as the sum of the probabilities) of the observations(<a href=""https://www.cs.utah.edu/~piyush/teaching/EM_algorithm.pdf"" rel=""nofollow noreferrer"">https://www.cs.utah.edu/~piyush/teaching/EM_algorithm.pdf</a>). </p>

<p>I fixed this on another project where I implemented backpropogation with feed-forward neural nets by implementing a for loop with pre-set number of epochs instead of a while loop with a condition for the new iteration to be strictly greater than but I am wondering if this is a bad practice.</p>

<p>My code is at <a href=""https://github.com/icantrell/Natural-Language-Processing"" rel=""nofollow noreferrer"">https://github.com/icantrell/Natural-Language-Processing</a>
inside the nlp.py file.</p>

<p>Any advice would be appreciated.
Thank You.</p>
",Dataset Preprocessing & Handling,baum welch em algorithm likelihood p x monotonically converging sort amateur come machine learning trying program baum welch algorithm derivation em algorithm hidden markov model inside program testing convergence using probability observation sequence new model terminating new model le equal old model however run algorithm seems converge somewhat give result far better random converging go last iteration sign bug something wrong seems using summation log observation probability comparison instead since seems like function maximizing however paper read said use log sum probability pretty sure sum probability observation fixed another project implemented backpropogation feed forward neural net implementing loop pre set number epoch instead loop condition new iteration strictly greater wondering bad practice code inside nlp py file advice would appreciated thank
"Do topic models for short text like WNTM, BTM and LF-LDA create something like a Document-term matrix?","<p>I'm dealing with topic modeling for short text and have come across three models that focus on the same: The biterm topic model (BTM), the word network topic model (WNTM) and the latent-feature LDA (LF-LDA).</p>

<p>I know that for conventional LDA (I have implemented it using the R package topicmodels), the unstructured shape of text documents is converted to a computer-readable format via the construction of a Document-Term matrix (DTM). </p>

<p>I'm wondering if the above mentioned models use a similar way for implementation, especially if they also create a matrix that is similar to DTM. Does anyone know that? Unfortunately I couldn't find that information by reading the original papers. </p>

<p>Thank you in advance!</p>
",Dataset Preprocessing & Handling,topic model short text like wntm btm lf lda create something like document term matrix dealing topic modeling short text come across three model focus biterm topic model btm word network topic model wntm latent feature lda lf lda know conventional lda implemented using r package topicmodels unstructured shape text document converted computer readable format via construction document term matrix dtm wondering mentioned model use similar way implementation especially also create matrix similar dtm doe anyone know unfortunately find information reading original paper thank advance
Access document-term matrix without calling .fit_transform() each time,"<p>If I've already called <code>vectorizer.fit_transform(corpus)</code>, is the only way to later print the document-term matrix to call <code>vectorizer.fit_transform(corpus)</code> again?</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
corpus = ['the', 'quick','brown','fox']
vectorizer = CountVectorizer(stop_words='english')
vectorizer.fit_transform(corpus) # Returns the document-term matrix
</code></pre>

<p>My understanding is by doing above, I've now saved terms into the <code>vectorizer</code> object. I assume this because I can now call <code>vectorizer.vocabulary_</code> without passing in <code>corpus</code> again.</p>

<p>So I wondered why there is not a method like <code>.document_term_matrix</code>?</p>

<p>Its seems weird that I have to pass in the <code>corpus</code> again if the data is now already stored in <code>vectorizer</code> object. But per the docs, only <code>.fit</code>, <code>.transform</code>, and <code>.fit_transform</code>return the mattrix.</p>

<p><strong>Docs:</strong> <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit</a></p>

<p><strong>Other Info:</strong></p>

<p>I'm using Anaconda and Jupyter Notebook.</p>
",Dataset Preprocessing & Handling,access document term matrix without calling fit transform time already called way later print document term matrix call understanding saved term object assume call without passing wondered method like seems weird pas data already stored object per doc return mattrix doc info using anaconda jupyter notebook
Dictionary vs nested dictionaries ... for ~7 milion lexical definitions (MULTITEXT v5),"<p>I'm using lexical resource text file, formatted in MULTITEXT v5 format that actually look like this:</p>

<blockquote>
  <p>žvakah    žvakati Vme1s   0   0.000000</p>
  
  <p>žvakahu   žvakati Vme3p   0   0.000000</p>
  
  <p>žvakala   žvakati Vmp-pn  0   0.000000</p>
  
  <p>žvakala   žvakati Vmp-sf  45  0.000081</p>
  
  <p>žvakale   žvakati Vmp-pf  11  0.000020</p>
  
  <p>žvakali   žvakati Vmp-pm  66  0.000119</p>
  
  <p>žvakalo   žvakati Vmp-sn  10  0.000018</p>
  
  <p>žvakan    žvakati Appmsann    0   0.000000</p>
  
  <p>žvakan    žvakati Appmsnn 0   0.000000</p>
  
  <p>žvakan    žvakati Appmsvn 0   0.000000</p>
</blockquote>

<p>The format meaning:</p>

<blockquote>
  <p>[inflected word]  [lemma form] [grammatical context]  ... and tf-idf info that I don't use</p>
</blockquote>

<p>So, in typical scenario I have to match ~5000 of various [inflected word]s to retrieve [lemma form]s and more importantly: [grammatical context]s, where single [inflected word] may actually have more matching lines (like in the case of <strong>žvakan</strong>). The lexical resource to search has about 7 milion lines.</p>

<p>So far, I tried with loading the complete file into List and then running all ~5000 Regexes against each line (List item) using Parallel.ForEach. Regex was used for flexibility to query via [lemma form] or using only part of the word, but for sake of performance I can give up on that. And it took something like 30 minutes to find about 350 entries. So, obviously my approach was completely wrong.</p>

<p>Now I'm thinking to load the complete file into Dictionary where the key would be [inflected word] (so I give up on flexibility) but I wonder:</p>

<ul>
<li>Would it make sense (for greater execution time) to nest two dictionaries like this:</li>
</ul>

<blockquote>
  <p>Dictionary&lt;[first letter], Dictionary&lt;[inflected word], List&lt;[definition
     line]>>></p>
</blockquote>

<p>Would it do any better then loading all into:</p>

<blockquote>
  <p>Dictionary&lt;[inflected word], List&lt;[definition line]>></p>
</blockquote>

<ul>
<li>Is there some better idea?</li>
</ul>

<p>I'm not using ConcurrentDictionary since the content is written into data structure only once, before the use starts. </p>

<p>My preference is solely query execution time - RAM seems not to be an issue - with current code I have 19Gb of RAM available, and I have 8 core CPU so any comments on Parallel execution are also welcome. </p>

<p>In case someone wonders: this is a Natural Language Processing application. </p>
",Dataset Preprocessing & Handling,dictionary v nested dictionary milion lexical definition multitext v using lexical resource text file formatted multitext v format actually look like vakah vakati vme vakahu vakati vme p vakala vakati vmp pn vakala vakati vmp sf vakale vakati vmp pf vakali vakati vmp pm vakalo vakati vmp sn vakan vakati appmsann vakan vakati appmsnn vakan vakati appmsvn format meaning inflected word lemma form grammatical context tf idf info use typical scenario match various inflected word retrieve lemma form importantly grammatical context single inflected word may actually matching line like case vakan lexical resource search ha milion line far tried loading complete file list running regexes line list item using parallel foreach regex wa used flexibility query via lemma form using part word sake performance give took something like minute find entry obviously approach wa completely wrong thinking load complete file dictionary key would inflected word give flexibility wonder would make sense greater execution time nest two dictionary like dictionary first letter dictionary inflected word list definition line would better loading dictionary inflected word list definition line better idea using concurrentdictionary since content written data structure use start preference solely query execution time ram seems issue current code gb ram available core cpu comment parallel execution also welcome case someone wonder natural language processing application
Search over text column in pandas data frame without looping,"<p>I have a pandas data frame where one of the columns is a text description string. I need to create a new column which would identify if one of the strings from a list is in the text description. </p>

<pre><code>df = pd.DataFrame({'Description': ['2 Bedroom/1.5 Bathroom end unit Townhouse.  
Available now!', 'Very spacious studio apartment available', ' Two bedroom, 1 
bathroom condominium, superbly located in downtown']})

list_ = ['unit', 'apartment']
</code></pre>

<p>Then the result should be</p>

<pre><code>                                        Description    in list
0  2 Bedroom/1.5 Bathroom end unit Townhouse.  Av...    True
1           Very spacious studio apartment available    True
2   Two bedroom, 1 bathroom condominium, superbly...   False
</code></pre>

<p>I can do it this way</p>

<pre><code>for i in df.index.values:
    df.loc[i,'in list'] = any(w in df.loc[i,'Description'] for w in list_)
</code></pre>

<p>But with a large data set it takes longer than I'd like to.</p>
",Dataset Preprocessing & Handling,search text column panda data frame without looping panda data frame one column text description string need create new column would identify one string list text description result way large data set take longer like
Preprocessing text data with TFRecords files,"<p>I have a .tfrecord dataset of text documents (emails) with corresponding labels '0' or '1' (for spam/non-spam). All of this dataset is already in the form of a .tfrecord file. I'm trying to  turn the emails into a bag-of-words representation. I have all the helper methods to do it, but I'm still not familiar with tfrecords. This is what I have so far to read the tf_record file:</p>

<pre><code>def read_from_tfrecord(filenames):

    tfrecord_file_queue = tf.train.string_input_producer([filenames], name='queue')
    reader = tf.TFRecordReader()

    _, tfrecord_serialized = reader.read(tfrecord_file_queue)

    tfrecord_features = tf.parse_single_example(tfrecord_serialized,
                        features={
                            'label': tf.FixedLenFeature([], tf.int64),
                            'text': tf.FixedLenFeature([], tf.string),
                        }, name='features')

    text = tfrecord_features['text']
    label = tfrecord_features['label']

    return label, text
</code></pre>

<p>How should I proceed if I want to use my helper methods to modify the 'texts' ? </p>
",Dataset Preprocessing & Handling,preprocessing text data tfrecords file tfrecord dataset text document email corresponding label spam non spam dataset already form tfrecord file trying turn email bag word representation helper method still familiar tfrecords far read tf record file proceed want use helper method modify text
Adding hierarchical encoding to a pointer-generator Text-Summarization model,"<p>I'm working on text summarization, starting with the pointer-generator network described in the paper: <a href=""https://arxiv.org/pdf/1704.04368.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1704.04368.pdf</a>, with a code release at: <a href=""https://github.com/abisee/pointer-generator"" rel=""nofollow noreferrer"">https://github.com/abisee/pointer-generator</a></p>

<p>I want to add hierarchical encoding to this network to be able to handle larger input documents for summarization. Right now, input documents are truncated at a length of 400 words because LSTMs have a keeping memory over very long inputs. We want to reweight the attention distribution considering sentence input.</p>

<p>The reweighting I am considering is from the paper: <a href=""https://arxiv.org/pdf/1602.06023.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1602.06023.pdf</a> where the final distribution is</p>

<p><a href=""https://i.sstatic.net/rthTG.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rthTG.gif"" alt=""enter image description here""></a></p>

<p>where ""P_a^w(j) is the word-level attention weight at jth position of the source document, and s(j) is the ID of the sentence at jth word position, P_a^s(l)is the sentence-level attention weight for the lth sentence in the source, Nd is the number of words in the source document, and P_a(j) is the re-scaled attention at the jth word position""</p>

<p>The Pointer-Generator attention is calculated as </p>

<p><a href=""https://i.sstatic.net/JsF6R.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JsF6R.gif"" alt=""enter image description here""></a></p>

<p>then</p>

<p><code>softmax(e^t_ j)</code></p>

<p>So I think I need to make another Pt-gen attention LSTM, feed the outputs through the same attention calculation with softmax as the words, and then at the end reweight as described in equation 1.</p>

<p>I have added a new bidirectionalLSTM under <code>def _add_encoder</code>. I am wondering how I should handle input into the sentence LSTM. The model needs to have some kind of indication of the position of the sentence and the words in it. How should I structure the sentences to be processed by the sentence LSTM?</p>

<pre><code>def _add_encoder(self, encoder_inputs, seq_len):
    """"""Add a single-layer bidirectional LSTM encoder to the graph.

    Args:
      encoder_inputs: A tensor of shape [batch_size, &lt;=max_enc_steps, emb_size].
      seq_len: Lengths of encoder_inputs (before padding). A tensor of shape [batch_size].

    Returns:
      encoder_outputs:
        A tensor of shape [batch_size, &lt;=max_enc_steps, 2*hidden_dim]. It's 2*hidden_dim because it's the concatenation of the forwards and backwards states.
      fw_state, bw_state:
        Each are LSTMStateTuples of shape ([batch_size,hidden_dim],[batch_size,hidden_dim])
    """"""
    with tf.variable_scope('encoder'):
      cell_fw = tf.contrib.rnn.LSTMCell(self._hps.hidden_dim, initializer=self.rand_unif_init, state_is_tuple=True)
      cell_bw = tf.contrib.rnn.LSTMCell(self._hps.hidden_dim, initializer=self.rand_unif_init, state_is_tuple=True)
      (encoder_outputs, (fw_st, bw_st)) = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, encoder_inputs, dtype=tf.float32, sequence_length=seq_len, swap_memory=True)
      encoder_outputs = tf.concat(axis=2, values=encoder_outputs) # concatenate the forwards and backwards states


      sentence_encoder_inputs = ???
      sentence_lens = len(sentences_encoder_inputs)

      ### NEW CODE ###
      sentence_cell_fw = tf.contrib.rnn.LSTMCell(self._hps.hidden_dim, initializer=self.rand_unif_init, state_is_tuple=True)
      sentence_cell_bw = tf.contrib.rnn.LSTMCell(self._hps.hidden_dim, initializer=self.rand_unif_init, state_is_tuple=True)
      (sentence_encoder_outputs, (fw_st, bw_st)) = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, sentence_encoder_inputs, dtype=tf.float32, sequence_length=sentence_lens, swap_memory=True)
      sentence_encoder_outputs = tf.concat(axis=2, values=sentence_encoder_outputs) # concatenate the forwards and backwards states


    return encoder_outputs, sentence_encoder_outputs, fw_st, bw_st
</code></pre>
",Dataset Preprocessing & Handling,adding hierarchical encoding pointer generator text summarization model working text summarization starting pointer generator network described paper code release want add hierarchical encoding network able handle larger input document summarization right input document truncated length word lstms keeping memory long input want reweight attention distribution considering sentence input reweighting considering paper final distribution p w j word level attention weight jth position source document j id sentence jth word position p l sentence level attention weight lth sentence source nd number word source document p j scaled attention jth word position pointer generator attention calculated think need make another pt gen attention lstm feed output attention calculation softmax word end reweight described equation added new bidirectionallstm wondering handle input sentence lstm model need kind indication position sentence word structure sentence processed sentence lstm
.findall Regular Expression won&#39;t assign to a variable,"<p>I'm trying to create a function that searches a chunk of nltk.text.Text input and outputs all words following ""contribute"" or ""donate"" (see the regular expression below). </p>

<p>The regular expression works perfectly, however when I try to assign it to a variable in order for my function to return it, the variable doesn't update and my function returns nothing.</p>

<p>i.e. type(donation) = NoneType object</p>

<p>I eventually want to apply this function to every row of a data frame and output the donation value to a new column in that data frame, but when I try it now, every output is ""None""</p>

<pre><code>def find_donation_orgs(x):
    text = nltk.Text(nltk.word_tokenize(x))
    donation =  text.findall(r""&lt;\.&gt; &lt;.*&gt;{,15}? &lt;donat.*|contrib.*|Donat.*|Contrib.*&gt; &lt;.*&gt;*? &lt;to&gt; (&lt;.*&gt;+?) &lt;\.|\,|\;&gt; "")
    return donation
</code></pre>

<p>My findall regex does work by itself:</p>

<pre><code>text.findall(r""&lt;\.&gt; &lt;.*&gt;{,15}? &lt;donat.*|contrib.*|Donat.*|Contrib.*&gt; &lt;.*&gt;*? &lt;to&gt; (&lt;.*&gt;+?) &lt;\.|\,|\;&gt; "")
</code></pre>

<p>Returns this for an example piece of text:</p>

<pre><code>visit brother Alfred Fuller; the research of Dr. Giuseppe Giaccone at
Georgetown University
</code></pre>

<p>For your benefit:</p>

<pre><code>text = nltk.Text(nltk.word_tokenize(df.Obit.iloc[7]))
print(text)

x = text.findall(r""&lt;\.&gt; &lt;.*&gt;{,15}? &lt;donat.*|contrib.*|Donat.*|Contrib.*&gt; &lt;.*&gt;*? &lt;to&gt; (&lt;.*&gt;+?) &lt;\.|\,|\;&gt; "")

print(x)
</code></pre>

<p>Returns: </p>

<pre><code>&lt;Text: M. Jay Janssen , age 95 of Zeeland...&gt;
Resthaven Care Community
None
</code></pre>
",Dataset Preprocessing & Handling,findall regular expression assign variable trying create function search chunk nltk text text input output word following contribute donate see regular expression regular expression work perfectly however try assign variable order function return variable update function return nothing e type donation nonetype object eventually want apply function every row data frame output donation value new column data frame try every output none findall regex doe work return example piece text benefit return
.findall Regular Expression won&#39;t assign to a variable,"<p>I'm trying to create a function that searches a chunk of nltk.text.Text input and outputs all words following ""contribute"" or ""donate"" (see the regular expression below). </p>

<p>The regular expression works perfectly, however when I try to assign it to a variable in order for my function to return it, the variable doesn't update and my function returns nothing.</p>

<p>i.e. type(donation) = NoneType object</p>

<p>I eventually want to apply this function to every row of a data frame and output the donation value to a new column in that data frame, but when I try it now, every output is ""None""</p>

<pre><code>def find_donation_orgs(x):
    text = nltk.Text(nltk.word_tokenize(x))
    donation =  text.findall(r""&lt;\.&gt; &lt;.*&gt;{,15}? &lt;donat.*|contrib.*|Donat.*|Contrib.*&gt; &lt;.*&gt;*? &lt;to&gt; (&lt;.*&gt;+?) &lt;\.|\,|\;&gt; "")
    return donation
</code></pre>

<p>My findall regex does work by itself:</p>

<pre><code>text.findall(r""&lt;\.&gt; &lt;.*&gt;{,15}? &lt;donat.*|contrib.*|Donat.*|Contrib.*&gt; &lt;.*&gt;*? &lt;to&gt; (&lt;.*&gt;+?) &lt;\.|\,|\;&gt; "")
</code></pre>

<p>Returns this for an example piece of text:</p>

<pre><code>visit brother Alfred Fuller; the research of Dr. Giuseppe Giaccone at
Georgetown University
</code></pre>

<p>For your benefit:</p>

<pre><code>text = nltk.Text(nltk.word_tokenize(df.Obit.iloc[7]))
print(text)

x = text.findall(r""&lt;\.&gt; &lt;.*&gt;{,15}? &lt;donat.*|contrib.*|Donat.*|Contrib.*&gt; &lt;.*&gt;*? &lt;to&gt; (&lt;.*&gt;+?) &lt;\.|\,|\;&gt; "")

print(x)
</code></pre>

<p>Returns: </p>

<pre><code>&lt;Text: M. Jay Janssen , age 95 of Zeeland...&gt;
Resthaven Care Community
None
</code></pre>
",Dataset Preprocessing & Handling,findall regular expression assign variable trying create function search chunk nltk text text input output word following contribute donate see regular expression regular expression work perfectly however try assign variable order function return variable update function return nothing e type donation nonetype object eventually want apply function every row data frame output donation value new column data frame try every output none findall regex doe work return example piece text benefit return
Is it possible to make SVM probabiility predictions without tm and RTextTools using e1071 in R?,"<p>I am trying to create a topic classifier from an employee satisfaction survey. The survey contains several commentary fields, and therefore want to produce an effective way of classifying what a single comment is about, and later also whether it is positive or negative (pretty standard sentiment analysis).
I already have a sample data from last years survey, where comments have been given a category manually. </p>

<p>The data is structured in a CSV file with three rows:</p>

<p>The document (or comment) - The topic - The sentiment</p>

<p>One example could be:</p>

<p>Document: I am afraid of violence from our customers, since my position does not have sufficient sercurity</p>

<p>Topic: Violence</p>

<p>Sentiment: Negative</p>

<p><em>(Very crude example, but bear with me)</em></p>

<p>My tool for making this classifier is RStudio, but I only have access to a limited number of packages. I do not have access to tm or RTextTools, which are the packages I usually use when I am doing projects outside of work. I pretty much only have access to e1071, and that is why I figured a support vector machine might do the trick. I have bad experiences with NaiveBayes when dealing with text analytics, but I am of course open to any advice. Is it possible at all to do text mining without tm or RTextTools? I have access to the NLP and tau packages</p>
",Dataset Preprocessing & Handling,possible make svm probabiility prediction without tm rtexttools using e r trying create topic classifier employee satisfaction survey survey contains several commentary field therefore want produce effective way classifying single comment later also whether positive negative pretty standard sentiment analysis already sample data last year survey comment given category manually data structured csv file three row document comment topic sentiment one example could document afraid violence customer since position doe sufficient sercurity topic violence sentiment negative crude example bear tool making classifier rstudio access limited number package access tm rtexttools package usually use project outside work pretty much access e figured support vector machine might trick bad experience naivebayes dealing text analytics course open advice possible text mining without tm rtexttools access nlp tau package
How to add another column in dataframe with calculated values,"<p>I have a news dataset and I am carrying NLP over it.
I have 2 functions right now,  One calculates similarity and another one calculates sentiments both of them takes the input from data frame, that I am trying to do is to create another column in the dataframe with the calculated values like similarity &amp; sentiment(Pos/Neg)</p>

<p>the functions are as follows</p>

<pre><code>i=0
for i in range(0, 9):
    text1 = df.description[i]
    text2 = df.title[i]


    vector1 = similarity.text_to_vector(text1)
    vector2 = similarity.text_to_vector(text2)

    token1 = similarity.tokenize(text1)
    token2 = similarity.tokenize(text2)

    jaccard = similarity.jaccard_similarity(token1,token2)
    print ('Jaccard Similarity:', jaccard)

    i=i+1
</code></pre>

<p>Output:</p>

<pre><code>('Jaccard Similarity:', 0.07142857142857142)
('Jaccard Similarity:', 0.125)
('Jaccard Similarity:', 0.03225806451612903)
('Jaccard Similarity:', 0.07692307692307693)
('Jaccard Similarity:', 0.2)
('Jaccard Similarity:', 0.07407407407407407)
('Jaccard Similarity:', 0.12)
('Jaccard Similarity:', 0.043478260869565216)
('Jaccard Similarity:', 0.0)
</code></pre>

<p>Code:</p>

<pre><code>i=0
for i in range(0, 9):
    blob = TextBlob(df.description[i], analyzer=NaiveBayesAnalyzer())
    y = blob.sentiment.classification
    print ('Result', y)
    i=i+1
</code></pre>

<p>Output:</p>

<pre><code>('Result', 'pos')
('Result', 'neg')
('Result', 'pos')
('Result', 'pos')
('Result', 'pos')
('Result', 'neg')
('Result', 'pos')
('Result', 'pos')
('Result', 'neg')
</code></pre>
",Dataset Preprocessing & Handling,add another column dataframe calculated value news dataset carrying nlp function right one calculates similarity another one calculates sentiment take input data frame trying create another column dataframe calculated value like similarity sentiment po neg function follows output code output
How to read multiple text files in Spark for document clustering?,"<p>I want to read multiple text documents from a directory for document clustering.
For that, I want to read data as:</p>

<pre><code>SparkConf sparkConf = new SparkConf().setAppName(appName).setMaster(""local[*]"").set(""spark.executor.memory"", ""2g"");
JavaSparkContext context = new JavaSparkContext(sparkConf);
SparkSession spark = SparkSession.builder().config(sparkConf).getOrCreate();
Dataset&lt;Row&gt; dataset = spark.read().textFile(""path to directory"");
</code></pre>

<p>Here, I don't want to use<br>
    JavaPairRDD data = context.wholeTextFiles(path);
     because I want Dataset as a return type.</p>
",Dataset Preprocessing & Handling,read multiple text file spark document clustering want read multiple text document directory document clustering want read data want use javapairrdd data context wholetextfiles path want dataset return type
How remove few columns from countvectorized sparse dataframe in pandas,"<p>I have  around 2000 text features inside countvectorized  data frame. I have  list of 800 text feature columns which have actual feature importance contribution for prediction model. I want keep only this 800 columns and remove rest 1200 columnns as they do not contribute much towards my prediction.</p>

<p>How can I do that.  I have the list  of columns to be  maintained in text file.</p>

<pre><code>cv = CountVectorizer( max_features = 2000,analyzer='word') 
    cv_text = cv.fit_transform(data.pop('text'))
    for i, col in enumerate(cv.get_feature_names()):
        data[col] = pd.SparseSeries(cv_text[:, i].toarray().ravel(), fill_value=0)
</code></pre>
",Dataset Preprocessing & Handling,remove column countvectorized sparse dataframe panda around text feature inside countvectorized data frame list text feature column actual feature importance contribution prediction model want keep column remove rest columnns contribute much towards prediction list column maintained text file
how to find and write a frequency of occurrence of words inside text data into csv file using pandas,"<p>How Can I create csv file containing  words and its frequency of occurrence in python  .</p>

<p>I removed the stop words, tokenized and countvectorized the  text data</p>

<p>My code</p>

<pre><code> data['Clean_addr'] = data['Adj_Addr'].apply(lambda x: ' '.join([item.lower() for item in x.split()]))
        data['Clean_addr']=data['Clean_addr'].apply(lambda x:"""".join([item.lower() for item in x if  not  item.isdigit()]))
        data['Clean_addr']=data['Clean_addr'].apply(lambda x:"""".join([item.lower() for item in x if item not in string.punctuation]))
        data['Clean_addr'] = data['Clean_addr'].apply(lambda x: ' '.join([item.lower() for item in x.split() if item not in (new_stop_words)]))
        cv = CountVectorizer( max_features = 200,analyzer='word')
        cv_addr = cv.fit_transform(data.pop('Clean_addr'))
</code></pre>

<p>Sample Dump of the File I am using</p>

<p><a href=""https://www.dropbox.com/s/allhfdxni0kfyn6/Test.csv?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/allhfdxni0kfyn6/Test.csv?dl=0</a></p>

<pre><code>**Expected output**
Word       Freq
Industry    40
Limited     23
House       45
flat        56
</code></pre>
",Dataset Preprocessing & Handling,find write frequency occurrence word inside text data csv file using panda create csv file containing word frequency occurrence python removed stop word tokenized countvectorized text data code sample dump file using
Not able to attach extracted POS taged Noun phrases to pandas data frame,"<p>I am trying to extract only noun and noun phrases  to address data (a column a inside csv file). </p>

<p>I was able to  remove the stop words, punctuations and numbers from the data. Also  was able  POS tag the data, but not able Extract Noun Phrases and attach back to data frame. Let me know what went  wrong</p>

<pre><code>    stopwords=nltk.corpus.stopwords.words('english')
    user_defined_stop_words=['hong','kong','hk','kowloon','hongkong']                    
    new_stop_words=stopwords+user_defined_stop_words

    data['Clean_addr'] = data['Adj_Addr'].apply(lambda x: ' '.join([item.lower() for item in x.split()]))
    data['Clean_addr']=data['Clean_addr'].apply(lambda x:"""".join([item.lower() for item in x if  not  item.isdigit()]))
    data['Clean_addr']=data['Clean_addr'].apply(lambda x:"""".join([item.lower() for item in x if item not in string.punctuation]))
    data['Clean_addr'] = data['Clean_addr'].apply(lambda x: ' '.join([item.lower() for item in x.split() if item not in (new_stop_words)]))

texts = data['Clean_addr'].tolist()
tagged_texts = pos_tag_sents(map(word_tokenize, texts))
data['POS']=tagged_texts
data['POS']=data['POS'].apply(lambda x:' '.join([item[0] for item in x if (item[0][1]=='NNP' or item[0][1]=='NNS')]))    
</code></pre>

<p>Sample Dump of the File I am using</p>

<p><a href=""https://www.dropbox.com/s/allhfdxni0kfyn6/Test.csv?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/allhfdxni0kfyn6/Test.csv?dl=0</a></p>
",Dataset Preprocessing & Handling,able attach extracted po taged noun phrase panda data frame trying extract noun noun phrase address data column inside csv file wa able remove stop word punctuation number data also wa able po tag data able extract noun phrase attach back data frame let know went wrong sample dump file using
STM: how to keep metadata when converting from tm to stm document-term matrix?,"<p>I'm trying to run structural topic models (using <code>stm</code> package) on the document-term matrix that was prepared using <code>tm</code> package.</p>

<p>I built a corpus in <code>tm</code> package that contains the following metadata:</p>

<pre><code>library(tm)

myReader2 &lt;- readTabular(mapping=list(content=""text"", id=""id"", sentiment = ""sentiment""))
text_corpus2 &lt;- VCorpus(DataframeSource(bin_stm_df), readerControl = list(reader = myReader2))

meta(text_corpus2[[1]])
  id       : 11
  sentiment: negative
  language : en
</code></pre>

<p>After doing some text-cleaning and saving the results as <code>clean_corpus2</code>(metadata still present), I change it to document-term matrix and then read it as <code>stm</code>-compatible matrix:</p>

<pre><code>library(stm)

chat_DTM2 &lt;- DocumentTermMatrix(clean_corpus2, control = list(wordLengths = c(3, Inf)))
DTM2 &lt;- removeSparseTerms(chat_DTM2 , 0.990)
DTM_st &lt;-readCorpus(DTM2, type = ""slam"")
</code></pre>

<p>So far, so good. However, when I try to specify the metadata using <code>stm</code>-compatible data, the metadata is gone:</p>

<pre><code>docsTM &lt;- DTM_st$documents # works fine
vocabTM &lt;- DTM_st$vocab # works fine
metaTM &lt;- DTM_st$meta # returns NULL

&gt; metaTM
NULL
</code></pre>

<p>How do I keep the metadata from <code>tm</code>-generated Corpus in <code>stm</code>-compatible document-term matrix? Any suggestions welcome, thanks.</p>
",Dataset Preprocessing & Handling,stm keep metadata converting tm stm document term matrix trying run structural topic model using package document term matrix wa prepared using package built corpus package contains following metadata text cleaning saving result metadata still present change document term matrix read compatible matrix far good however try specify metadata using compatible data metadata gone keep metadata generated corpus compatible document term matrix suggestion welcome thanks
How can I make my code more efficient in R - It&#39;s to repetitive,"<p>I have a question regarding the efficiency of my code. I have 9 data frames in my environment and for each of them I need to perform the same steps. The steps and the code is (only shown for two of the data frames):</p>

<pre><code>CDL &lt;- aggregate(A$Frequency, by=list(Category=A$Words), FUN=sum)
wordcloud(words = CDL$Category, freq = CDL$x, min.freq = 2,
      max.words=250, random.order=FALSE, rot.per=0.35, 
      colors=brewer.pal(6, ""Dark2""))

Ltd &lt;- aggregate(B$Frequency, by=list(Category=B$Words), FUN=sum)
wordcloud(words = Ltd$Category, freq = Ltd$x, min.freq = 2,
      max.words=250, random.order=FALSE, rot.per=0.35, 
      colors=brewer.pal(6, ""Dark2""))
</code></pre>

<p>I first aggregate all the same words, sum their frequencies and then create a world cloud based on the aggregated results. </p>

<p>The object names in the environment start from 'A' and go all the way to 'I'. The variable 'Frequency' is just a number, the variable 'Words' contains a list of the words. </p>

<p>For the wordcloud: 
The variable 'Category' contains the unique words taken from the 'Words' variable and 'x' is the aggregated sum of Frequencies taken from 'Frequency'</p>

<p>Is there any way I could perform the same but without repeating my code? Thanks</p>
",Dataset Preprocessing & Handling,make code efficient r repetitive question regarding efficiency code data frame environment need perform step step code shown two data frame first aggregate word sum frequency create world cloud based aggregated result object name environment start go way variable frequency number variable word contains list word wordcloud variable category contains unique word taken word variable x aggregated sum frequency taken frequency way could perform without repeating code thanks
Read my own dataset for NLTK Part of Speech tagging using PerceptronTagger,"<p>I'm new to NLTK and still pretty new to python. I want to use my own dataset to train and test NLTK's Perceptron tagger. The training and testing data has the following format (it's just saved in a txt file): </p>

<pre><code>Pierre  NNP
Vinken  NNP
,       ,
61      CD
years   NNS
old     JJ
,       ,
will    MD
join    VB
the     DT
board   NN
as      IN
a       DT
nonexecutive    JJ
director        NN
Nov.    NNP
29      CD
.       .
</code></pre>

<p>I want to call these functions on the data:</p>

<pre><code>perceptron_tagger = nltk.tag.perceptron.PerceptronTagger(load=False)
perceptron_tagger.train(train_data)
accuracy = perceptron_tagger.evaluate(test_data)
</code></pre>

<p>I've tried a few things but I just can't figure out what format the data is expected to be in. Any help would be appreciated! Thanks</p>
",Dataset Preprocessing & Handling,read dataset nltk part speech tagging using perceptrontagger new nltk still pretty new python want use dataset train test nltk perceptron tagger training testing data ha following format saved txt file want call function data tried thing figure format data expected help would appreciated thanks
Python how to apply bag of words to tweets in csv file,"<p>I am currently working twitter data analysis and have been working on applying bag of words technique in Python and have been having no luck.
Currently I have been able to stream data to be stored in a database with some preprocessing which I then export the tweets into a csv file but stumbling on the next part to use bag of words in order to do machine learning. </p>

<p>I've tried following <a href=""https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words"" rel=""nofollow noreferrer"">https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words</a> however I have had no success and haven't been able to grasp an understanding how how to approach by just looking at either scikit or nltk documentation. Can anyone advise tutorials I can follow to achieve bag of words with Python 3? 
Thanks for the help</p>
",Dataset Preprocessing & Handling,python apply bag word tweet csv file currently working twitter data analysis working applying bag word technique python luck currently able stream data stored database preprocessing export tweet csv file stumbling next part use bag word order machine learning tried following however success able grasp understanding approach looking either scikit nltk documentation anyone advise tutorial follow achieve bag word python thanks help
Load Custom Dataset (which is like 20 news group set) in Scikit for Classification of text documents,"<p>I'm trying to run <a href=""http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html"" rel=""nofollow noreferrer"">this scikit example code</a> for my custom dataset of Ted Talks. 
Each Directory is a Topic under which are text files which contain the description for each Ted Talk.</p>

<p>This is how my datasets tree structure is. As you see, each directory is a topic and below it are text files which carry description. </p>

<pre><code>Topics/
|-- Activism
|   |-- 1149.txt
|   |-- 1444.txt
|   |-- 157.txt
|   |-- 1616.txt
|   |-- 1706.txt
|   |-- 1718.txt
|-- Adventure
|   |-- 1036.txt
|   |-- 1777.txt
|   |-- 2930.txt
|   |-- 2968.txt
|   |-- 3027.txt
|   |-- 3290.txt
|-- Advertising
|   |-- 3673.txt
|   |-- 3685.txt
|   |-- 6567.txt
|   `-- 6925.txt
|-- Africa
|   |-- 1045.txt
|   |-- 1072.txt
|   |-- 1103.txt
|   |-- 1112.txt
|-- Aging
|   |-- 1848.txt
|   |-- 2495.txt
|   |-- 2782.txt
|-- Agriculture
|   |-- 3469.txt
|   |-- 4140.txt
|   |-- 4733.txt
|   |-- 4939.txt
</code></pre>

<p>I have made my dataset in such form to resemble the 20news group whose tree structure is such:</p>

<pre><code>20news-18828/
|-- alt.atheism
|   |-- 49960
|   |-- 51060
|   |-- 51119

|-- comp.graphics
|   |-- 37261
|   |-- 37913
|   |-- 37914
|   |-- 37915
|   |-- 37916
|   |-- 37917
|   |-- 37918
|-- comp.os.ms-windows.misc
|   |-- 10000
|   |-- 10001
|   |-- 10002
|   |-- 10003
|   |-- 10004
|   |-- 10005 
</code></pre>

<p>In the <a href=""http://pastebin.com/wx3pcArQ"" rel=""nofollow noreferrer"">original code</a> (98-124), This is how training and testing data is loaded directly from scikit. </p>

<pre><code>print(""Loading 20 newsgroups dataset for categories:"")
print(categories if categories else ""all"")

data_train = fetch_20newsgroups(subset='train', categories=categories,
                                shuffle=True, random_state=42,
                                remove=remove)

data_test = fetch_20newsgroups(subset='test', categories=categories,
                               shuffle=True, random_state=42,
                               remove=remove)
print('data loaded')

categories = data_train.target_names    # for case categories == None
def size_mb(docs):
    return sum(len(s.encode('utf-8')) for s in docs) / 1e6

data_train_size_mb = size_mb(data_train.data)
data_test_size_mb = size_mb(data_test.data)

print(""%d documents - %0.3fMB (training set)"" % (
    len(data_train.data), data_train_size_mb))
print(""%d documents - %0.3fMB (test set)"" % (
    len(data_test.data), data_test_size_mb))
print(""%d categories"" % len(categories))
print()

# split a training set and a test set
y_train, y_test = data_train.target, data_test.target
</code></pre>

<p>Since this dataset was available with Scikit, Its labels etc were all built in. 
For my case, I know how to load the dataset <a href=""http://pastebin.com/1MGeL3D4"" rel=""nofollow noreferrer"">(Line 84)</a>: </p>

<pre><code>dataset = load_files('./TED_dataset/Topics/')
</code></pre>

<p>I have no idea what I should do after that. I want to know how I should split this data in training and testing and generate these labels from my dataset:  </p>

<pre><code>data_train.data,  data_test.data 
</code></pre>

<p>All in all, I just want to load my dataset, run it on this code error free. I have <a href=""https://drive.google.com/file/d/0B0byAC8kZZPhYVJNVTdJa003U2M/view?usp=sharing"" rel=""nofollow noreferrer"">uploaded the dataset here</a> for those who might want to see it. </p>

<p>I have referred to <a href=""https://stackoverflow.com/questions/27761803/problems-loading-textual-data-with-scikit-learn"">this question</a> which speaks briefly about test-train loading. I also want to know how data_train.target_names should be fetched from my dataset.</p>

<p>Edit: </p>

<p>I tried to get the train and test which returns error:</p>

<pre><code>dataset = load_files('./TED_dataset/Topics/')
train, test = train_test_split(dataset, train_size = 0.8)
</code></pre>

<p>Updated code is <a href=""http://pastebin.com/DcPCwaVS"" rel=""nofollow noreferrer"">here</a>. </p>
",Dataset Preprocessing & Handling,load custom dataset like news group set scikit classification text document trying run scikit example code custom dataset ted talk directory topic text file contain description ted talk datasets tree structure see directory topic text file carry description made dataset form resemble news group whose tree structure original code training testing data loaded directly scikit since dataset wa available scikit label etc built case know load dataset line idea want know split data training testing generate label dataset want load dataset run code error free uploaded dataset might want see referred
Sklearn preprocessing label encoder is throwing error for mutiple columns,"<p>I have pandas Data Frame with following structure</p>

<pre><code>item_condition_id                     category
brand_name                            category
price                                  float64
shipping                              category
main_category                         category
category                              category
sub_category                          category
hashing_feature_aa                     float64
hashing_feature_ab                     float64
</code></pre>

<p>Example with portion of data:</p>

<pre><code>brand_name  shipping  main_category        category
Target         1         Women           Tops &amp; Blouses
unknown        1          Home           Home Décor
unknown        0         Women            Jewelry
unknown        0         Women             Other
</code></pre>

<p>I have converted categorical (Strings) columns to numerical using below code.</p>

<pre><code>from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for i in range(len(X)):
    X.iloc[:,i] = le.fit_transform(X.iloc[:,i])
</code></pre>

<p>After Conversion</p>

<pre><code>   brand_name  shipping  main_category  category
        0         1              1         3
        1         1              0         0
        1         0              1         1
        1         0              1         2
</code></pre>

<p>This is working as expected but while trying apply <strong>inverse_transform</strong> to get the original categories from numerical categories it  is throwing error.</p>

<pre><code>for i in range(len(X)):
    X.iloc[:,i] = le.inverse_transform(X.iloc[:,i])
</code></pre>

<blockquote>
  <p>ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()</p>
</blockquote>

<p>How to resolve this error in my case , what's wrong with my code ?</p>

<p>My goal is convert categorical (strings) features to numerical using Label Encoder in order to apply <em>sklearn.feature_selection.SelectKbest.fit_transform(X,y)</em>, without label encoding this step is failing.</p>

<p>Thanks</p>
",Dataset Preprocessing & Handling,sklearn preprocessing label encoder throwing error mutiple column panda data frame following structure example portion data converted categorical string column numerical using code conversion working expected trying apply inverse transform get original category numerical category throwing error valueerror truth value array one element ambiguous use resolve error case wrong code goal convert categorical string feature numerical using label encoder order apply sklearn feature selection selectkbest fit transform x without label encoding step failing thanks
Count Number of Pages per AGENDA- Text Mining in r,"<p>I have to count the number of pages per AGENDA ITEM. I have extracted text from pdf document into a data frame, essentially one row of this data frame contains one page of text. This is how my data looks like: </p>

<pre><code>mydf &lt;- data.frame(text = c(""AGENDA ITEM 1
        4"", ""This particular row contains a lot of text, really its all text present in one page"", 
        ""So ineffect, one page of text per row"", ""This is another page of text in this row"", 
        ""lets include another page for agenda 1"", ""AGENDA ITEM 2
        9"",
        ""now all the text in agenda 2 is included here"",""the 2nd page text of agenda 2"", 
        ""AGENDA ITEM 3
        12"", ""Now lets just add one row for this agenda, meaning it only has one page inside it""))
</code></pre>

<p>Under the AGENDA TEXT (Same row), the number is the page number and it's in the same row. To count the number of Pages per Agenda, I just need to count the Number of rows until the next AGENDA ITEM appears. Considering the above example the answer should be </p>

<pre><code>AGENDA ITEM 1 = 4 Pages, AGENDA ITEM 2 = 2 Pages and AGENDA ITEM 3 = 1 Page.
</code></pre>

<p>How will I do this? 
I am fairly new to analysing text. Thanks</p>
",Dataset Preprocessing & Handling,count number page per agenda text mining r count number page per agenda item extracted text pdf document data frame essentially one row data frame contains one page text data look like agenda text row number page number row count number page per agenda need count number row next agenda item appears considering example answer fairly new analysing text thanks
How to tokenize documents in R and list tokens by original document title?,"<p>I have data frame <code>D</code> containing a document title and the text as in the following example:</p>

<pre><code>document   content
Doc 1      ""This is an example of a document""
Doc 2      ""And another one""
</code></pre>

<p>I need to use the <code>tokenize</code> function from <code>quanteda</code> package in order to tokenize every document and then return the tokens listed by its original document title as in this example: </p>

<pre><code>document   content
    Doc 1      ""This""
    Doc 1      ""This is""
    Doc 1      ""This is an""
    Doc 1      ""This is an example"" 
</code></pre>

<p>This is my current process to obtain a data frame with tokens from a list of documents:</p>

<pre><code>require(textreadr)
D&lt;-textreadr::read_dir(""myDir"")
D&lt;-paste(D$content,collapse="" "")
strlist&lt;-paste0(c("":"",""\\)"","":"",""'"","";"",""!"",""+"",""&amp;"",""&lt;"",""&gt;"",""\\("",""\\["",""\\]"",""-"",""#"","",""),collapse = ""|"")
D&lt;-gsub(strlist, """", D)
library(quanteda)
require(quanteda)
t&lt;-tokenize(D, what = c(""word"",""sentence"", ""character"",""fastestword"", ""fasterword""), 
            remove_numbers = FALSE, remove_punct = FALSE,
            remove_symbols = FALSE, remove_separators = TRUE,
            remove_twitter = FALSE, remove_hyphens = FALSE, remove_url = FALSE,
            ngrams = 1:10, concatenator = "" "", hash = TRUE,
            verbose = quanteda_options(""verbose""))
t&lt;-unlist(t, use.names=FALSE)
t1&lt;-data.frame(t)
</code></pre>

<p>However, I can't find an easy way to keep the document names after the tokenization process and list the tokens accordingly. Could anyone help with this? </p>
",Dataset Preprocessing & Handling,tokenize document r list token original document title data frame containing document title text following example need use function package order tokenize every document return token listed original document title example current process obtain data frame token list document however find easy way keep document name tokenization process list token accordingly could anyone help
How to read and edit a file based on values in a dictionary?,"<p>I wold like to expand acronyms in text file (<em>e.g fyi--->for your information</em>). I started by reading the acronyms file and store them in a dictionary as keys and values. Next, search each line in the text file for acronym(s) and expand them into complete phrase. After checking <a href=""https://stackoverflow.com/questions/43139174/replacing-words-in-text-file-using-a-dictionary"">this post</a> , I was able to read and create acronyms dictionary, but I have an issue with the replacement. The output file that should include the correct text is empty. here is my code and thanks in advance:</p>

<pre><code>d = {}
with open('acronym.txt', 'r') as acronym:
for line in acronym:
    d[line.split(None, 1)[0]] = (line.split(None, 1)[1]).strip()

ntweetsfile= open('newTweets.txt', 'w')
import fileinput
for line in fileinput.input('tweets1k.txt', inplace=True):
    line = line.rstrip()
    if not line:
        continue
    for key in d:
        if str(key) in line:
            line = line.replace(str(key), d[key])
            ntweetsfile.write(str(line)+'\n')          
        else:
            ntweetsfile.write(str(line)+'\n')
ntweetsfile.close()
</code></pre>
",Dataset Preprocessing & Handling,read edit file based value dictionary wold like expand acronym text file e g fyi information started reading acronym file store dictionary key value next search line text file acronym expand complete phrase checking href post wa able read create acronym dictionary issue replacement output file include correct text empty code thanks advance
ValueError: setting an array element with a sequence - after making TF_IDF vectorization,"<p>I'm new to data science and NLP. I want to perform TF_IDF vectorization on some text documents and after use the results to train different machine learning models. But when I try to train SVC model I obtain the ValueError: setting an array element with a sequence. Here is my code. </p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(min_df=1, stop_words='english')
df['vect_message'] = vectorizer.fit_transform(df['message_encoding'])
X = df['vect_message']
y = df['severity']
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn import svm
model = svm.SVC() 
model.fit(X_train, y_train) 
prediction = model.predict(X_test)
</code></pre>

<p>And I got an error on the line <code>model.fit(X_train, y_train)</code></p>

<p>I have already searched other similar questions and I found one where they advise using <code>.toarray()</code> method to transform sparse matrix into np.array. But this didn't help me. </p>
",Dataset Preprocessing & Handling,valueerror setting array element sequence making tf idf vectorization new data science nlp want perform tf idf vectorization text document use result train different machine learning model try train svc model obtain valueerror setting array element sequence code got error line already searched similar question found one advise using method transform sparse matrix np array help
Context Based Word Co-Occurrence Matrix,"<p>How would I be able to construct a word occurrence matrix for a document VS a set of context words? </p>

<p>For eg, I have a document with the following </p>

<blockquote>
  <p>"" Password protection is key to security "" </p>
</blockquote>

<p>and I have a set of context words I have derived as keywords from a set of documents to which the above document is a subset :</p>

<blockquote>
  <p>"" password,email, security, network .... "" </p>
</blockquote>

<p>Question is, how do I create a co-occurrence matrix for a non-square matrix that would have rows as document vocabulary (all the tokens) VS columns as the context words.</p>

<p>Goal is to find the distance between context words and the rest of the words in the document.</p>
",Dataset Preprocessing & Handling,context based word co occurrence matrix would able construct word occurrence matrix document v set context word eg document following password protection key security set context word derived keywords set document document subset password email security network question create co occurrence matrix non square matrix would row document vocabulary token v column context word goal find distance context word rest word document
Write all lines for each set of a range to new file each time the range changes Python 3.6,"<p>trying to find a way of making this process work pythonically or at all. Basically, I have a really long text file that is split into lines. Every x number of lines there is one that is mainly uppercase, which should roughly be the title of that particular section. Ideally, I'd want the title and everything after to go into a text file using the title as the name for the file. This would have to happen 3039 in this case as that is as many titles will be there. 
My process so far is this: I created a variable that reads through a text file tells me if it's mostly uppercase.</p>

<pre><code>def mostly_uppercase(text):
    threshold = 0.7
    isupper_bools = [character.isupper() for character in text]
    isupper_ints = [int(val) for val in isupper_bools]
    try:
        upper_percentage = np.mean(isupper_ints)
    except:
        return False
    if upper_percentage &gt;= threshold:
        return True
    else:
        return False
</code></pre>

<p>Afterwards, I made a counter so that I could create an index and then I combined it:</p>

<pre><code>counter = 0

headline_indices = []

for line in page_text:
    if mostly_uppercase(line):
        print(line)
        headline_indices.append(counter)
    counter+=1

headlines_with_articles = []
headline_indices_expanded = [0] + headline_indices + [len(page_text)-1]

for first, second in list(zip(headline_indices_expanded, headline_indices_expanded[1:])):
    article_text = (page_text[first:second])
    headlines_with_articles.append(article_text)
</code></pre>

<p>All of that seems to be working fine as far as I can tell. But when I try to print the pieces that I want to files, all I manage to do is print the entire text into all of the txt files. </p>

<pre><code>for i in range(100):
    out_pathname = '/sharedfolder/temp_directory/' + 'new_file_' + str(i) + '.txt'
    with open(out_pathname, 'w') as fo:
        fo.write(articles_filtered[2])
</code></pre>

<p>Edit: This got me halfway there. Now, I just need a way of naming each file with the first line.</p>

<pre><code>for i,text in enumerate(articles_filtered):
    open('/sharedfolder/temp_directory' + str(i + 1) + '.txt', 'w').write(str(text))
</code></pre>
",Dataset Preprocessing & Handling,write line set range new file time range change python trying find way making process work pythonically basically really long text file split line every x number line one mainly uppercase roughly title particular section ideally want title everything go text file using title name file would happen case many title process far created variable read text file tell mostly uppercase afterwards made counter could create index combined seems working fine far tell try print piece want file manage print entire text txt file edit got halfway need way naming file first line
Reading eml file using R,"<p>I cant read emails in R. I was unziping the file and then i was catching out the files that i want to work on it this mean: </p>

<pre><code>data2 &lt;- data[grep("".eml"", data, fixed=T)]
</code></pre>

<p>I was using <code>library(tm.plugin.mail)</code> 
to read this eml files but is not really working 
<code>readMail(DateFormat = ""%d %B %Y %H:%M:%S"")</code>, 
<code>newsgroup &lt;- system.file(""data2"",""eml"", package = ""tm.plugin.mail"")</code>,</p>

<pre><code>text &lt;- Corpus(VectorSource(data2), readerControl = list(reader = readMail))]
</code></pre>

<p>the code is not reading the texts of emails. I need the text of this emails for NLP.
Ps: there are many email that i want to read called data2.
I will appreciate any help</p>
",Dataset Preprocessing & Handling,reading eml file using r cant read email r wa unziping file wa catching file want work mean wa using read eml file really working code reading text email need text email nlp p many email want read called data appreciate help
Creating a Tree from an ANTLR Grammar,"<p>I have written an ANTLR Grammar file and now i need to generate an adjacency matrix telling me which rule in grammar is associated with which one.
for ex :-
start : ('show' | 'give' | 'get') 'me' ('all')? phrase 
        | 'I' 'would' 'like' ('all')? phrase
        | phrase;</p>

<p>phrase : constructPhrase (('and')? constructPhrase)*
        | constructPhrase 'and' ('its' | 'their') constructPhrase
        | constructPhrase functionPhrase
        | functionPhrase
        ;</p>

<p>Here we have I would like associated with each other..so i need to read the grammar file and generate an adjacency matrix. </p>
",Dataset Preprocessing & Handling,creating tree antlr grammar written antlr grammar file need generate adjacency matrix telling rule grammar associated one ex start show give get phrase would like phrase phrase phrase constructphrase constructphrase constructphrase constructphrase constructphrase functionphrase functionphrase would like associated need read grammar file generate adjacency matrix
map reduce - extract text from PDF,"<p>I have a large number of PDF files sitting in a s3 directory. How do I apply map-reduce/parallel process them using pyspark. All I want to do is to extract text from them and then store the text in a RDD; since the number of files is large I would like to do it in a parallel fashion. </p>

<p>pyspark has a method called wholeTextFiles which can read a directory of text files. But, I have it in a PDF format and I would like to pre-process the PDF to extract text from it before I can process the text. </p>

<p>Any help would be appreciated</p>
",Dataset Preprocessing & Handling,map reduce extract text pdf large number pdf file sitting directory apply map reduce parallel process using pyspark want extract text store text rdd since number file large would like parallel fashion pyspark ha method called wholetextfiles read directory text file pdf format would like pre process pdf extract text process text help would appreciated
Count number of verbs for each speech in data frame R,"<p>I have a data frame as the following:</p>

<pre><code>str(data)
'data.frame':   255 obs. of  3 variables:
$ Group      : Factor w/ 255 levels ""AlzGroup1"",""AlzGroup10"",..: 1 112 179 190 201 212 223 234 245 2 ...
$ Gender     : int  1 1 0 0 0 0 0 1 0 0 ...
$ Description: Factor w/ 255 levels ""A boy's on the uh falling off the stool picking up cookies . The girl's reaching up for it . The girl the lady ""| __truncated__,..: 63 69 38 134 111 242 196 85 84 233 ...
</code></pre>

<p>in the Description column I have 255 speeches and I want to add a column to my data frame containing number of verbs in each speech, I know how to get number of verbs but the following code gives me total number of verbs in Description column:</p>

<pre><code>&gt; library(NLP);
&gt; library(tm);
&gt; library(openNLP);
NumOfVerbs=sapply(strsplit(as.character(tagPOS(data$Description)),""[[:punct:]]*/VB.?""),function(x) {res = sub(""(^.*\\s)(\\w+$)"", ""\\2"", x); res[!grepl(""\\s"",res)]} )
</code></pre>

<p>Does anyone know how can I get number of verbs in each speech?</p>

<p>Thanks for any help!</p>

<p>Elahe</p>
",Dataset Preprocessing & Handling,count number verb speech data frame r data frame following description column speech want add column data frame containing number verb speech know get number verb following code give total number verb description column doe anyone know get number verb speech thanks help elahe
"For text mining in R, how do I combine DocumentTermMatrix with original Data Frame?","<p>What I am looking to do is create code that will allow me to classify tweets. So in the example below I would want to take tweets talking about a credit card and determine if they are related to the issue of travel.</p>

<p>Here is the initial dataset: </p>

<pre><code>id&lt;- c(123,124,125,126,127) 
text&lt;- c(""Since I love to travel, this is what I rely on every time."", 
        ""I got this card for the no international transaction fee"", 
        ""I got this card mainly for the flight perks"",
        ""Very good card, easy application process"",
        ""The customer service is outstanding!"") 
travel_cat&lt;- c(1,0,1,0,0) 
df_all&lt;- data.frame(id,text,travel) 
</code></pre>

<p>Output 1:</p>

<pre><code>id  text                                                        travel_cat
123 Since I love to travel, this is what I rely on every time.  1
124 I got this card for the no international transaction fee    0
125 I got this card mainly for the flight perks                 1
126 Very good card, easy application process                    0
127 The customer service is outstanding!                        0
</code></pre>

<p>I am then creating a data frame with only the text field and then doing the text analytics:</p>

<pre><code>myvars&lt;- c(""text"")
df&lt;- df_all[myvars]

library(tm)
corpus&lt;- Corpus(DataframeSource(df))
corpus&lt;- tm_map(corpus, content_transformer(tolower))
corpus&lt;- tm_map(corpus, removePunctuation)
corpus&lt;- tm_map(corpus, removeWords, stopwords(""english""))
corpus&lt;- tm_map(corpus, stripWhitespace)
dtm&lt;- as.matrix(DocumentTermMatrix(corpus))
</code></pre>

<p>Output 2 (dtm):</p>

<pre><code>Docs    application card    customer    easy    every ... etc.
1       0           0       0           1       0
2       0           1       0           0       1
3       0           1       0           0       0
4       1           1       0           0       0
5       0           0       1           0       0
</code></pre>

<p>How do I then tie this back to the original data so that it contains the fields from the original dataset and the matrix (Output 1 + Output 2):
    id,text,travel_cat + application,card,customer,easy,every...</p>
",Dataset Preprocessing & Handling,text mining r combine documenttermmatrix original data frame looking create code allow classify tweet example would want take tweet talking credit card determine related issue travel initial dataset output creating data frame text field text analytics output dtm tie back original data contains field original dataset matrix output output id text travel cat application card customer easy every
Text Mining - Count Frequencies of Phrases (more than one word),"<p>I am familiar with using the tm library to create a tdm and count frequencies of terms. </p>

<p>But these terms are all single-word. </p>

<p>How can do count the # of times a multi-word phrase occurs in a document and/or corpus? </p>

<p>EDIT:</p>

<p>I am adding the code I have now to improve/clarify my post.</p>

<p>This is pretty standard code to build a term-document matrix:</p>

<pre><code>library(tm)


cname &lt;- (""C:/Users/George/Google Drive/R Templates/Gospels corpus"")   

corpus &lt;- Corpus(DirSource(cname))

#Cleaning
corpus &lt;- tm_map(corpus, tolower)
corpus &lt;- tm_map(corpus, removeNumbers)
corpus &lt;- tm_map(corpus, removePunctuation)
corpus &lt;- tm_map(corpus, stripWhitespace)
corpus &lt;- tm_map(corpus, removeWords, c(""a"",""the"",""an"",""that"",""and""))

#convert to a plain text file
corpus &lt;- tm_map(corpus, PlainTextDocument)

#Create a term document matrix
tdm1 &lt;- TermDocumentMatrix(corpus)

m1 &lt;- as.matrix(tdm1)
word.freq &lt;- sort(rowSums(m1), decreasing=T)
word.freq&lt;-word.freq[1:100]
</code></pre>

<p>The problem is that this returns a matrix of <em>single word</em> terms, example:</p>

<pre><code>  all      into      have      from      were       one      came       say       out 
  397       390       385       383       350       348       345       332       321
</code></pre>

<p>I want to be able to search for multi-word terms in the corpus instead. So for example ""came from"" instead of just ""came"" and ""from"" separately.</p>

<p>Thank you.</p>
",Dataset Preprocessing & Handling,text mining count frequency phrase one word familiar using tm library create tdm count frequency term term single word count time multi word phrase occurs document corpus edit adding code improve clarify post pretty standard code build term document matrix problem return matrix single word term example want able search multi word term corpus instead example came instead came separately thank
Removing the first observation string of dataset,"<p>I'm importing a bunch of files and trying to read all the characters into one variable. I need the order preserved. I found this code on <a href=""https://stackoverflow.com/questions/3397885/how-do-you-read-in-multiple-txt-files-into-r/3397940"">Stackoverflow</a> that does what I need but it adds the 0 in the front. so instead of the ""0"" i did</p>

<pre><code>data &lt;- """"
</code></pre>

<p>This still leaves my data variable with a blank at the beginning. I cannot subset it and remove all the """" blanks since my inputs have some that need to remain in there. How can I remove just the first blank """" or read all my files without having to add that blank there in the first place.</p>

<pre><code>setwd(""C:\\Users\\J\\Desktop\\nlp\\brown"")
files &lt;-list.files()
data &lt;- 0
for (f in files) {
  tempData = scan( f, what=""character"")
  data &lt;- c(data,tempData)    

} 
</code></pre>
",Dataset Preprocessing & Handling,removing first observation string dataset importing bunch file trying read character one variable need order preserved found code still leaf data variable blank beginning subset remove blank since input need remain remove first blank read file without add blank first place
Best way to understand the input text before applying ngram,"<p>Currently I am reading text from excel file and applying bigram to it. <em>finalList</em> has list used in below sample code has the list of <strong>input words</strong> read from input excel file.</p>

<p>Removed the stopwords from input with help of following library:</p>

<pre><code>from nltk.corpus import stopwords
</code></pre>

<p>bigram logic applied on list of input text of words</p>

<pre><code>bigram=ngrams(finalList ,2)
</code></pre>

<p>input text: I completed my end-to-end process.</p>

<p>Current output: Completed end, end end, end process.</p>

<p>Desired output: completed end-to-end, end-to-end process.</p>

<p>That means some group of words like (end-to-end) should be considered as 1 word.</p>
",Dataset Preprocessing & Handling,best way understand input text applying ngram currently reading text excel file applying bigram finallist ha list used sample code ha list input word read input excel file removed stopwords input help following library bigram logic applied list input text word input text completed end end process current output completed end end end end process desired output completed end end end end process mean group word like end end considered word
merging two document term matrices by row,"<p>I have customer queries and answers from customer services in a csv file. I need to identify the subject of each question and then later develop a classification model on this. I have created two document term matrices (after cleaning the documents), one for questions and the other for the answers. I have reduced the size by only taking those terms that occur more than 400 times in the whole document (about 40k questions and answers). </p>

<p>I want to create a data frame that merges these two matrices by rows  and retain only the words that are common in question and answer dtm (and add up their frequency. How should I do this in R? I'll use the highest frequency word to label the question. </p>

<p>Any help/ suggestion on the approach is highly appreciated. </p>

<pre><code>&gt; str(inspect(dtmaf))
&lt;&lt;DocumentTermMatrix (documents: 38697, terms: 237)&gt;&gt;
Non-/sparse entries: 326124/8845065
Sparsity           : 96%
Maximal term length: 13
Weighting          : term frequency (tf)
Sample             :
   Terms
Docs    booking card change check confirm confirmation email make port wish
12316       3    1      0     0       0            0     0    0    1    1
137         4    1      2     0       1            0     0    0    0    0
17618       4    1      0     0       0            0     0    2    0    2
18082       2    1      3     1       1            0     0    0    1    0
19141       3    0      2     0       1            0     0    0    1    0
21862       2    0      0     0       0            0     0    1    0    0
2756        1    0      2     0       0            0     0    1    0    1
27578       2    1      5     0       0            0     0    0    0    1
30312       4    1      2     0       0            0     0    2    0    2
9019        1    1      1     0       0            0     0    0    0    0
num [1:10, 1:10] 3 4 4 2 3 2 1 2 4 1 ...
- attr(*, ""dimnames"")=List of 2
 ..$ Docs : chr [1:10] ""12316"" ""137"" ""17618"" ""18082"" ...
 ..$ Terms: chr [1:10] ""booking"" ""card"" ""change"" ""check"" ...

&gt; str(inspect(dtmc))
&lt;&lt;DocumentTermMatrix (documents: 38697, terms: 189)&gt;&gt;
Non-/sparse entries: 204107/7109626
Sparsity           : 97%
Maximal term length: 13
Weighting          : term frequency (tf)
Sample             :
       Terms
Docs    booking car change confirmation like number possible reservation return ticket
  14091       0   0      0            0    2      0        0           2      0      0
  18220       6   0      0            2    0      0        0           0      0      0
  20103       1   0      1            0    0      1        0           0      0      0
  20184       0   3      0            0    0      1        0           4      1      0
  21005       3   5      0            1    2      0        1           0      0      0
  24877       0   1      1            0    0      0        0           2      0      1
  26135       0   0      0            0    0      0        0           1      0      0
  28200       5   2      1            0    0      0        0           1      0      0
  2979       12   7      2            0    1      0        0           0      0      0
  680         0   0      1            2    0      1        0           0      0      0
 num [1:10, 1:10] 0 6 1 0 3 0 0 5 12 0 ...
 - attr(*, ""dimnames"")=List of 2
  ..$ Docs : chr [1:10] ""14091"" ""18220"" ""20103"" ""20184"" ...
  ..$ Terms: chr [1:10] ""booking"" ""car"" ""change"" ""confirmation"" ...
</code></pre>

<p>Expected output is a matrix with (237+189) terms and 38697 rows. Matching terms in both dtms will have one column per term  and their frequencies summed up and the non-matching terms will be reproduced as such. </p>

<p>Here is a reproducible example with 10 documents:</p>

<pre><code>&gt; dput(datamsg)
structure(list(cmessage = c(""No answer ?"", ""Hello  the third  number is . I bought this boarding card immediately after the operator has told me from the previous logbook the number  can not be found in the system. Therefore I request to return money. It was not my fault !"", 
""Hi  I forget probably choose items on the   How can I do this now.  "", 
""Hi  I forget probably choose items  How can i do this now.  "", 
""Hello  I tell if I have booked . If not  is it possible and what would it cost? "", 
""First I wanted to transfer fromThen I wanted to know if you can spontaneously postpone the return "", 
""Hello. Does the have an exact address? With this address I do not find it on the navigation. Have an exact address where I can get the ticets. Where I get the Tikets then. Is the automatic chekin. Or do I then mot the tickets to the Chekin. Thank you.  But rather ask more questions. "", 
""Dear  booked everything again. Also the journey through In my previous message  I stated that it is a complete cancellation and I have booked the return trip. I do not intend to pay twice for travel. "", 
""Thank you. When will the new  registration show ?...as it still shows the . Thanks"", 
""So my phone number is .Please tell me how this works.""), afreply = c(""Hello   afraid there is no space on the September. I have also checked but  are all fully booked. Would you like us to check any other dates for you? "", 
""Hello  As far as we can see the booking No was a valid reservation. We have however contacted  and can confirm that administration fee  was refunded back to your card. "", 
""Good afternoon  You are currently booked as high plane. You have requested an amendment to change the height   which will be more expensive. Could you please confirm the actual height of . We have cancelled you amendment request   please submit a new one with an accurate height ofreply to this message. "", 
""Hello  thanks for your message. I have checked and can see you have amended your height to on your booking. If you require any other assistance with your booking  please contact us."", 
""Hello  you booked any  In order to make a change to your booking  kindly send us a amendment request via"", 
""Dear Mr. what dimensions  you want to take with you? here is only the possibility to change your departure for a change of booking fee and a possible ticket price difference. The ticket price difference can be requested  if you call us an alternative travel date."", 
""Dear Sir or Madam  we will send you the address "", ""Hello  your crossing with was already refunded. As my colleague told you your  with  was still valid. In case you have booked a second ticket with   please send us the new booking reference number  but we cannot guarantee that you will be entitle to a refund. "", 
""if you can authorise us to take the payment from the card you used to make the we can then make the change."", 
""Good morning  we could not reach you by telephone. If you do not have we can send you an invoice via PayPal. The change can not be made until paid. . Do you want to pay the change to 1. ""
)), .Names = c(""cmessage"", ""afreply""), class = ""data.frame"", row.names = c(NA, 
-10L))

corpus1&lt;-Corpus(VectorSource(datamsg$cmessage))
corpus2&lt;-Corpus(VectorSource(datamsg$afreply))
dtmc&lt;-DocumentTermMatrix(corpus1, control = list(weighting = weightTf))
dtmaf&lt;-DocumentTermMatrix(corpus2, control = list(weighting = weightTf))
</code></pre>
",Dataset Preprocessing & Handling,merging two document term matrix row customer query answer customer service csv file need identify subject question later develop classification model created two document term matrix cleaning document one question answer reduced size taking term occur time whole document k question answer want create data frame merges two matrix row retain word common question answer dtm add frequency r use highest frequency word label question help suggestion approach highly appreciated expected output matrix term row matching term dtms one column per term frequency summed non matching term reproduced reproducible example document
"For each bigram in list, print number of times it appears in other lists - python NLTK","<p>I am new to coding and could use help. Here is my task:
I have a csv of online marketing image titles. It is a single column. Each cell in this column holds the marketing image title text for each ad. It is just a string of words. For instance cell A1 reads: ""16 Maddening Tire Fails"" and etc etc. To load csv I do:</p>

<pre><code>with open('usethis.csv', 'rb') as f:
    mycsv = csv.reader(f)
    mycsv = list(mycsv)
</code></pre>

<p>I initialize a list:</p>

<pre><code>mylist = []
</code></pre>

<p>my desire is to take the text in each cell and extract the bigrams. I do that as follows:</p>

<pre><code>for i, c in enumerate(mycsv):
   mylist.append(list(nltk.bigrams(word_tokenize(' '.join(c)))))
</code></pre>

<p>mylist then looks like this, but with more data:</p>

<pre><code>[[('16', 'Maddening'), ('Maddening', 'Tire'), ('Tire', 'Fails')], [('16', 'Maddening'), ('Maddening', 'Tire'), ('Tire', 'Fails'), ('Fails', 'That'), ('That', 'Show'), ('Show', 'What'), ('What', 'True'), ('True', 'Negligence'), ('Negligence', 'Looks'), ('Looks', 'Like')]
</code></pre>

<p>mylist holds individual lists which are the bigrams created from each cell in my csv.</p>

<p>Now I am wanting to loop through every bigram in all lists and next to each bigram print the number of times it appears in another list (cell). This would be the same as a countifs in excel, basically. For instance, if the bigram ""('16', 'Maddening')"" in the first list (cell A1) appears 3 other times in (mylist) then print the number 3 next to it. And so on for each bigram. If it is easier to return this information into a new list that's fine. Just printing it out somewhere that makes sense.</p>

<p>I have done a lot of reading online, for instance this link kind of was along the general idea:
<a href=""https://stackoverflow.com/questions/10666163/how-to-check-if-all-elements-of-a-list-matches-a-condition"">How to check if all elements of a list matches a condition?</a></p>

<p>And also this link about dictionaries was similar in that it is returning a number next to each value as I want to return a count next to each bigram..
<a href=""https://stackoverflow.com/questions/8957750/what-are-python-dictionary-view-objects"">What are Python dictionary view objects?</a>....</p>

<p>But I really am at a loss as to how to do this. Thank you so much in advance for your help! Let me know if I need to explain something better.</p>
",Dataset Preprocessing & Handling,bigram list print number time appears list python nltk new coding could use help task csv online marketing image title single column cell column hold marketing image title text ad string word instance cell read maddening tire fails etc etc load csv initialize list desire take text cell extract bigram follows mylist look like data mylist hold individual list bigram created cell csv wanting loop every bigram list next bigram print number time appears another list cell would countifs excel basically instance bigram maddening first list cell appears time mylist print number next bigram easier return information new list fine printing somewhere make sense done lot reading online instance link kind wa along general idea really loss thank much advance help let know need explain something better
Strategy for computing PMI from counts: dataframes or matrices,"<p>I need to compute PMI scores for co-occurrences of bio-entities e.g. <code>Gene A - Gene B</code>, or <code>Gene C - Disease A</code>. Co-occurrences have been extracted from <a href=""https://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/PubTator/curator_identifier.cgi?page=1&amp;Species_display=1&amp;Chemical_display=1&amp;Gene_display=1&amp;Disease_display=1&amp;Mutation_display=1&amp;pmid=15803373"" rel=""nofollow noreferrer"">Pubtator</a>. I use Python 3.</p>

<p>For a set of documents, I have extracted the individual counts <code>freq(x)</code> and <code>freq(y)</code> of all entities by co-occurrence category e.g. <code>Gene-Gene</code> or <code>Gene-Disease</code>, and I have the co-occurrence counts of entity pairs <code>freq(x,y)</code>. All counts are stored in a <code>Dict</code>. </p>

<p>What would be the best approach for computing Pointwise Mutual Information (PMI) scores from the raw counts: </p>

<ul>
<li>Create two data frames (one for individual counts and one for co-occurrence counts) </li>
<li>Create two matrices? (same as above)</li>
<li>another approach? </li>
</ul>

<p>Consider that one set of data has 3 columns: <code>entity, category, count</code> and the other set has 4 columns: <code>entity_a, category, entity_b, count</code> where category represents the co-occurrence category. I need the category for the individual entity counts because if I were to use their overall total counts it would distort the results for a given co-occurrence type.</p>

<p>I have attempted the data frame approach but can't figure out a way how to create a new PMI column that computes the result using two different data frames (DFs) hence I thought maybe a matrix approach may work better? If so, why?</p>

<p>Examples of the data when transformed into DFs:</p>

<p>df1.head():</p>

<p><code>
       ent        rel count
177   5197  Gene_Gene     2
176  56744  Gene_Gene     2
175  12766  Gene_Gene     2
174   3091  Gene_Gene     2
173   3162  Gene_Gene     2
</code></p>

<p>df2.head():</p>

<p><code>
     ent_a        rel  ent_b count
247   5197  Gene_Gene  56744     1
246  12766  Gene_Gene   5197     1
245  12766  Gene_Gene  56744     1
244   3091  Gene_Gene   3162     1
243   3091  Gene_Gene  54583     1
</code></p>

<p>The PMI formula :  </p>

<p><a href=""https://i.sstatic.net/jeN4o.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jeN4o.gif"" alt=""enter image description here""></a></p>
",Dataset Preprocessing & Handling,strategy computing pmi count dataframes matrix need compute pmi score co occurrence bio entity e g co occurrence extracted pubtator use python set document extracted individual count entity co occurrence category e g co occurrence count entity pair count stored would best approach computing pointwise mutual information pmi score raw count create two data frame one individual count one co occurrence count create two matrix another approach consider one set data ha column set ha column category represents co occurrence category need category individual entity count use overall total count would distort result given co occurrence type attempted data frame approach figure way create new pmi column computes result using two different data frame dfs hence thought maybe matrix approach may work better example data transformed dfs df head df head pmi formula
Understanding LDA / topic modelling -- too much topic overlap,"<p>I'm new to topic modelling / Latent Dirichlet Allocation and have trouble understanding how I can apply the concept to my dataset (or whether it's the correct approach).</p>

<p>I have a small number of literary texts (novels) and would like to extract some general topics using LDA.</p>

<p>I'm using the <code>gensim</code> module in Python along with some <code>nltk</code> features. For a test I've split up my original texts (just 6) into 30 chunks with 1000 words each. Then I converted the chunks into document-term matrices and ran the algorithm. This is the code (although I think it doesn't matter for the question) :</p>

<pre><code># chunks is a 30x1000 words matrix

dictionary = gensim.corpora.dictionary.Dictionary(chunks)
corpus = [ dictionary.doc2bow(chunk) for chunk in chunks ]
lda = gensim.models.ldamodel.LdaModel(corpus = corpus, id2word = dictionary,
    num_topics = 10)
topics = lda.show_topics(5, 5)
</code></pre>

<p>However the result is completely different from any example I've seen in that the topics are full of meaningless words that can be found in <em>all</em> source documents, e.g. ""I"", ""he"", ""said"", ""like"", ... example:</p>

<pre><code>[(2, '0.009*""I"" + 0.007*""\'s"" + 0.007*""The"" + 0.005*""would"" + 0.004*""He""'), 
(8, '0.012*""I"" + 0.010*""He"" + 0.008*""\'s"" + 0.006*""n\'t"" + 0.005*""The""'), 
(9, '0.022*""I"" + 0.014*""\'s"" + 0.009*""``"" + 0.007*""\'\'"" + 0.007*""like""'), 
(7, '0.010*""\'s"" + 0.009*""I"" + 0.006*""He"" + 0.005*""The"" + 0.005*""said""'), 
(1, '0.009*""I"" + 0.009*""\'s"" + 0.007*""n\'t"" + 0.007*""The"" + 0.006*""He""')]
</code></pre>

<p>I don't quite understand why that happens, or why it doesn't happen with the examples I've seen. How do I get the LDA model to find more distinctive topics with less overlap? Is it a matter of filtering out more common words first? How can I adjust how many times the model runs? Is the number of original texts too small?</p>
",Dataset Preprocessing & Handling,understanding lda topic modelling much topic overlap new topic modelling latent dirichlet allocation trouble understanding apply concept dataset whether correct approach small number literary text novel would like extract general topic using lda using module python along feature test split original text chunk word converted chunk document term matrix ran algorithm code although think matter question however result completely different example seen topic full meaningless word found source document e g said like example quite understand happens happen example seen get lda model find distinctive topic le overlap matter filtering common word first adjust many time model run number original text small
How to tune a Machine Translation model with huge language model?,"<p><code>Moses</code> is a software to build machine translation models. And <code>KenLM</code> is the defacto language model software that moses uses.</p>

<p>I have a textfile with 16GB of text and i use it to build a language model as such:</p>

<pre><code>bin/lmplz -o 5 &lt;text &gt; text.arpa
</code></pre>

<p>The resulting file (<code>text.arpa</code>) is 38GB. Then I binarized the language model as such:</p>

<pre><code>bin/build_binary text.arpa text.binary
</code></pre>

<p>And the binarized language model (<code>text.binary</code>) grows to 71GB.</p>

<p>In <code>moses</code>, after training the translation model, you should tune the weights of the model by using <code>MERT</code> algorithm. And this can simply be done with <a href=""https://github.com/moses-smt/mosesdecoder/blob/master/scripts/training/mert-moses.pl"">https://github.com/moses-smt/mosesdecoder/blob/master/scripts/training/mert-moses.pl</a>. </p>

<p>MERT works fine with small language model but with the big language model, it takes quite some days to finish. </p>

<p>I did a google search and found KenLM's filter, which promises to filter the language model to a smaller size: <a href=""https://kheafield.com/code/kenlm/filter/"">https://kheafield.com/code/kenlm/filter/</a></p>

<p>But i'm clueless as to how to make it work. The command help gives:</p>

<pre><code>$ ~/moses/bin/filter
Usage: /home/alvas/moses/bin/filter mode [context] [phrase] [raw|arpa] [threads:m] [batch_size:m] (vocab|model):input_file output_file

copy mode just copies, but makes the format nicer for e.g. irstlm's broken
    parser.
single mode treats the entire input as a single sentence.
multiple mode filters to multiple sentences in parallel.  Each sentence is on
    a separate line.  A separate file is created for each sentence by appending
    the 0-indexed line number to the output file name.
union mode produces one filtered model that is the union of models created by
    multiple mode.

context means only the context (all but last word) has to pass the filter, but
    the entire n-gram is output.

phrase means that the vocabulary is actually tab-delimited phrases and that the
    phrases can generate the n-gram when assembled in arbitrary order and
    clipped.  Currently works with multiple or union mode.

The file format is set by [raw|arpa] with default arpa:
raw means space-separated tokens, optionally followed by a tab and arbitrary
    text.  This is useful for ngram count files.
arpa means the ARPA file format for n-gram language models.

threads:m sets m threads (default: conccurrency detected by boost)
batch_size:m sets the batch size for threading.  Expect memory usage from this
    of 2*threads*batch_size n-grams.

There are two inputs: vocabulary and model.  Either may be given as a file
    while the other is on stdin.  Specify the type given as a file using
    vocab: or model: before the file name.  

For ARPA format, the output must be seekable.  For raw format, it can be a
    stream i.e. /dev/stdout
</code></pre>

<p>But when I tried the following, it gets stuck and does nothing:</p>

<pre><code>$ ~/moses/bin/filter union lm.en.binary lm.filter.binary
Assuming that lm.en.binary is a model file
Reading lm.en.binary
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
</code></pre>

<p><strong>What should one do to the Language Model after binarization? Is there any other steps to manipulate large language models to reduce the
computing load when tuning?</strong></p>

<p><strong>What is the usual way to tune on a large LM file?</strong></p>

<p><strong>How to use KenLM's filter?</strong></p>

<p>(more details on <a href=""https://www.mail-archive.com/moses-support@mit.edu/msg12089.html"">https://www.mail-archive.com/moses-support@mit.edu/msg12089.html</a>)</p>
",Dataset Preprocessing & Handling,tune machine translation model huge language model software build machine translation model defacto language model software moses us textfile gb text use build language model resulting file gb binarized language model binarized language model grows gb training translation model tune weight model using algorithm simply done clueless make work command help give tried following get stuck doe nothing one language model binarization step manipulate large language model reduce computing load tuning usual way tune large lm file use kenlm filter detail href
Validating and cleaning text data,"<p>I'm wondering how one would go about validating that text data you pull and clean, considering that you can't either validate the number values or read every single entry.</p>

<p>My specific case is dealing with email text data, pulled from a .mbox file. So there are all different types of formats - ie signatures, etc - and the text I want to analyze is essentially a subsection of the body.  Let's say I figure out a method to extract what I'd like, then how do I go about validating that the data I will be working with is what I specifically want?</p>
",Dataset Preprocessing & Handling,validating cleaning text data wondering one would go validating text data pull clean considering either validate number value read every single entry specific case dealing email text data pulled mbox file different type format ie signature etc text want analyze essentially subsection body let say figure method extract like go validating data working specifically want
Importing a Term Document Matrix in CSV format into R,"<p>So I already have a TDM but it was on excel. So I saved it as CSV. Now I want to do some analysis but I can´t load it as a TDM using tm package. My CSV looks something like this:</p>

<pre><code>           item01    item02    item03     item04


red         0          1         1           0
circle      1          0         0           1
fame        1          0         0           0
yellow      0          0         1           1 
square      1          0         1           0 
</code></pre>

<p>So I haven't been able to load that file as a TDM, the best I've tried so far is this :</p>

<pre><code>myDTM &lt;- as.DocumentTermMatrix(df, weighting = weightBin)
</code></pre>

<p>But it loads 1's on all cells </p>

<pre><code>&lt;&lt;DocumentTermMatrix (documents: 2529, terms: 1952)&gt;&gt;
Non-/sparse entries: 4936608/0
Sparsity           : 0%
Maximal term length: 27
Weighting          : binary (bin)
Sample             :

             Terms
Docs            item01 item02 item03 item04
      Red        1        1     1       1                
      Circle     1        1     1       1          
      fame       1        1     1       1   
</code></pre>

<p>I've tried converting first to Corpus and other things but if i try to use any function like inspect(tdm) it returns an error, like this or similar.</p>

<pre><code>Error in `[.simple_triplet_matrix`(x, docs, terms) :
</code></pre>

<p>I really don´t believe there isn't a way to import it in the right format, any suggestion? Thanks in advance.</p>
",Dataset Preprocessing & Handling,importing term document matrix csv format r already tdm wa excel saved csv want analysis load tdm using tm package csv look something like able load file tdm best tried far load cell tried converting first corpus thing try use function like inspect tdm return error like similar really believe way import right format suggestion thanks advance
Parts of Speech Tagging in JAVA NLP,"<p>First of, I am not asking for codes. I will be just asking for suggestions or ideas on how to start this project so please help me I want to learn.</p>

<p>INPUT TEXT FILE:
Five little monkeys jumping on the bed
One fell off and bumped his head
Mama called the doctor and the doctor said:
""No more monkeys jumping on the bed!""</p>

<p><strong>OUTPUT:</strong></p>

<p>Noun:
Monkeys
Doctor</p>

<p>(and other parts of speech)</p>

<p>If i remove one word from the text file it will also be gone from the Output. Is this program possible without downloading anything like the Stanford? I'm using Java. I don't know how to start it without ideas :( </p>

<p>Question:
What method am I going to use. </p>

<p><strong>EDIT!!!!!!!!!!!</strong></p>

<pre><code>public static void main(String[] args) throws IOException {

         BufferedReader br = new BufferedReader(new FileReader(""C:/Users/xxxx/Desktop/lyrics.txt""));
         String line = null;
         while ((line = br.readLine()) != null) 

         {
             System.out.println(""Noun: "");

         }
    }

    }
</code></pre>

<p>HERE NOW IT ALREADY READS MY TEXTFILE. I thought of an idea that i can just find a specific word in the text file and print it out as, ""Noun: Monkeys"" but without the user input. What i'm talking about is something like this one</p>

<p>Type word to find: ExampleWord</p>

<p>Output: </p>

<p>Sys.out.print( word + ""Found!"");</p>

<p>Can do something like this without asking for the user? It will just automatically print out every word? </p>
",Dataset Preprocessing & Handling,part speech tagging java nlp first asking code asking suggestion idea start project please help want learn input text file five little monkey jumping bed one fell bumped head mama called doctor doctor said monkey jumping bed output noun monkey doctor part speech remove one word text file also gone output program possible without downloading anything like stanford using java know start without idea question method going use edit already read textfile thought idea find specific word text file print noun monkey without user input talking something like one type word find exampleword output sys print word found something like without asking user automatically print every word
How can I apply a lexicon to a list of sentences?,"<p>I have a lexicon dictionary in this shape </p>

<pre><code>6   ابن جزمه    1
7   ابو جهل -1
8   اتق الله    -1
9   اتقو الله   1
</code></pre>

<p>I want to create a new list containing the score of each sentence based on the lexicon adding the score of each word and if no words exist append zero 
when I implement my code I get <code>len(lex_score) = 3679</code> after I add elif condition I get <code>len(lex_score) = 95079</code></p>

<p>the len(lex_score) should equal 6064</p>

<pre><code>lex_score = []
def lexic(text):
    for tweet in sentences:
        score = 0
        for word in tweet.split():
            if word in lexicon:
                score = score+lexicon[word]
            elif word not in lexicon:
                score = 0
                lex_score.append(score)
</code></pre>

<p>I want to create a new column in the data frame containing the score of each sentence. what am I doing wrong?
and is there a better way to do so ? </p>
",Dataset Preprocessing & Handling,apply lexicon list sentence lexicon dictionary shape want create new list containing score sentence based lexicon adding score word word exist append zero implement code get add elif condition get len lex score equal want create new column data frame containing score sentence wrong better way
Tokenizing and POS tagging in Python from CSV file,"<p>I am a newbie in Python and would like to do POS tagging after importing csv file from my local machine. I looked up some resources from online and found that the following code works.</p>

<pre><code>text = 'Senator Elizabeth Warren from Massachusetts announced her support of 
Social Security in Washington, D.C. on Tuesday. Warren joined other 
Democrats in support.'  
import nltk
from nltk import tokenize
sentences = tokenize.sent_tokenize(text)
sentences

from nltk.tokenize import TreebankWordTokenizer
texttokens = []
for sent in sentences:
 texttokens.append(TreebankWordTokenizer().tokenize(sent))
texttokens

from nltk.tag import pos_tag
taggedsentences = []
for sentencetokens in texttokens:
 taggedsentences.append(pos_tag(sentencetokens))
taggedsentences

print(taggedsentences)
</code></pre>

<p>Since I printed it, the result from the code above looks like this.</p>

<pre><code>[[('Senator', 'NNP'), ('Elizabeth', 'NNP'), ('Warren', 'NNP'), ('from', 
'IN'), ('Massachusetts', 'NNP'), ('announced', 'VBD'), ('her', 'PRP$'), 
('support', 'NN'), ('of', 'IN'), ('Social', 'NNP'), ('Security', 'NNP'), 
('in', 'IN'), ('Washington', 'NNP'), (',', ','), ('D.C.', 'NNP'), ('on', 
'IN'), ('Tuesday', 'NNP'), ('.', '.')], [('Warren', 'NNP'), ('joined', 
'VBD'), ('other', 'JJ'), ('Democrats', 'NNPS'), ('in', 'IN'), ('support', 
'NN'), ('.', '.')]]
</code></pre>

<p>This is a desirable result that I would like to get, but I would like to get the result after importing csv file which contains several rows (in each row, there are several sentences.). For example, the csv file looks like this:</p>

<pre><code>---------------------------------------------------------------
I like this product. This product is beautiful. I love it. 
---------------------------------------------------------------
This product is awesome. It have many convenient features.
---------------------------------------------------------------
I went this restaurant three days ago. The food is too bad.
---------------------------------------------------------------
</code></pre>

<p>In the end, I would like to save the desirable pos tagging results that I displayed above after importing the csv file. I would like to save (write) the (pos tagged) each sentence in each row as a csv format. </p>

<p>Two formats might be possible. First one might be as follows (no header, each (pos tagged) sentence in one row).</p>

<pre><code>----------------------------------------------------------------------------
[[('I', 'PRON'), ('like', 'VBD'), ('this', 'PRON'), ('product', 'NN')]]
----------------------------------------------------------------------------
[[('This', 'PRON'), ('product', 'NN'), ('is', 'VERB'), ('beautiful', 'ADJ')]]
---------------------------------------------------------------------------
[[('I', 'PRON'), ('love', 'VERB'), ('it', 'PRON')]]
----------------------------------------------------------------------------
...
</code></pre>

<p>The second format might look like this (no header, each set of token and pos tagger saved in one cell):</p>

<pre><code>----------------------------------------------------------------------------
('I', 'PRON')    | ('like', 'VBD')   | ('this', 'PRON') | ('product', 'NN')
----------------------------------------------------------------------------
('This', 'PRON') | ('product', 'NN') | ('is', 'VERB')   | ('beautiful', 'ADJ')
---------------------------------------------------------------------------
('I', 'PRON')    | ('love', 'VERB')  | ('it', 'PRON')   |
----------------------------------------------------------------------------
...
</code></pre>

<p>I prefer the second format to the first one. </p>

<p>The python code that I wrote here perfectly works but I would like to do the same thing for csv file and in the end save it in my local machine. </p>

<p>Final purpose of doing this is that I would like to extract only noun types of words (e.g., NN, NNP) from the sentences. </p>

<p>Can somebody help me how to fix the python code?</p>
",Dataset Preprocessing & Handling,tokenizing po tagging python csv file newbie python would like po tagging importing csv file local machine looked resource online found following code work since printed result code look like desirable result would like get would like get result importing csv file contains several row row several sentence example csv file look like end would like save desirable po tagging result displayed importing csv file would like save write po tagged sentence row csv format two format might possible first one might follows header po tagged sentence one row second format might look like header set token po tagger saved one cell prefer second format first one python code wrote perfectly work would like thing csv file end save local machine final purpose would like extract noun type word e g nn nnp sentence somebody help fix python code
How to load Data Frame or csv file in spacy pipeline nlp?,"<p>I am trying to load data frame csv into spacy pipeline. I am getting argument string error here is my code. </p>

<pre><code>from __future__ import unicode_literals
nlp = spacy.load('en')

data = pd.read_csv(""sometextdata.csv"")
text = []
for line in data.Line:
    text.append(clean_text(line))

    text_spacy = nlp(data['Line'])
    data['Line'].apply(nlp)
    document = nlp(text)
TypeError: Argument 'string' has incorrect type (expected unicode, got str)
</code></pre>

<p>I tried to load in different ways i got same error.</p>

<p><strong>Platforms</strong> : OS - Mac and python 2.7 </p>
",Dataset Preprocessing & Handling,load data frame csv file spacy pipeline nlp trying load data frame csv spacy pipeline getting argument string error code tried load different way got error platform mac python
Extract text from search result URLs using R,"<p>I know R a bit, but not a pro. I am working on a text-mining project using R. </p>

<p>I searched Federal Reserve website with a keyword, say ‘inflation’. The second page of the search result has the URL: (<a href=""https://search.newyorkfed.org/board_public/search?start=10&amp;Search=&amp;number=10&amp;text=inflation"" rel=""nofollow noreferrer"">https://search.newyorkfed.org/board_public/search?start=10&amp;Search=&amp;number=10&amp;text=inflation</a>). </p>

<p>This page has 10 search results (10 URLs). I want to write a code in R which will ‘read’ the page corresponding to each of those 10 URLs and extract the texts from those web pages to .txt files. My only input is the above mentioned URL. </p>

<p>I appreciate your help. If there is any similar older post, please refer me that too. Thank you.  </p>
",Dataset Preprocessing & Handling,extract text search result url using r know r bit pro working text mining project using r searched reserve website keyword say inflation second page search result ha url page ha search result url want write code r read page corresponding url extract text web page txt file input mentioned url appreciate help similar older post please refer thank
Why do we calculate cosine similarities using tf-idf weightings?,"<p>Suppose we are trying to measure similarity between two very similar documents.</p>

<pre><code>Document A: ""a b c d""
Document B: ""a b c e""
</code></pre>

<p>This corresponds to a term-frequency matrix </p>

<pre><code>  a b c d e
A 1 1 1 1 0
B 1 1 1 0 1
</code></pre>

<p>where the cosine similarity on the raw vectors is the dot product of the two vectors A and B, divided by the product of their magnitudes:</p>

<blockquote>
  <p>3/4 = (1*1 + 1*1 + 1*1 + 1*0 + 1*0) / (sqrt(4) * sqrt(4)).</p>
</blockquote>

<p>But when we apply an <a href=""https://en.wikipedia.org/wiki/SMART_Information_Retrieval_System"" rel=""nofollow noreferrer"">inverse document frequency</a> transformation by multiplying each term in the matrix by (log(N / df_i), where N is the number of documents in the matrix, 2, and df_i is the number of documents in which a term is present, we get a tf-idf matrix of</p>

<pre><code>   a b c d    e
A: 0 0 0 log2 0
B: 0 0 0 0    1og2
</code></pre>

<p>Since ""a"" appears in both documents, it has an inverse-document-frequency value of 0. This is the same for ""b"" and ""c"". Meanwhile, ""d"" is in document A, but not in document B, so it is multiplied by log(2/1). ""e"" is in document B, but not in document A, so it is also multiplied by log(2/1).</p>

<p>The cosine similarity between these two vectors is 0, suggesting the two are totally different documents. Obviously, this is incorrect. For these two documents to be considered similar to each other using tf-idf weightings, we would need a third document C in the matrix which is vastly different from documents A and B. </p>

<p>Thus, I am wondering whether and/or why we would use tf-idf weightings in combination with a cosine similarity metric to compare highly similar documents. None of the tutorials or StackOverflow questions I've read have been able to answer this question.</p>

<p><a href=""http://www.p-value.info/2013/02/when-tfidf-and-cosine-similarity-fail.html"" rel=""nofollow noreferrer"">This post</a> discusses similar failings with tf-idf weights using cosine similarities, but offers no guidance on what to do about them.</p>

<p>EDIT: as it turns out, the guidance I was looking for was in the comments of that blog post. It recommends using the formula</p>

<p>1 + log ( N / ni + 1)</p>

<p>as the inverse document frequency transformation instead. This would keep the weights of terms which are in every document close to their original weights, while inflating the weights of terms which are not present in a lot of documents by a greater degree. Interesting that this formula is not more prominently found in posts about tf-idf.</p>
",Dataset Preprocessing & Handling,calculate cosine similarity using tf idf weighting suppose trying measure similarity two similar document corresponds term frequency matrix cosine similarity raw vector dot product two vector b divided product magnitude sqrt sqrt apply inverse document frequency transformation multiplying term matrix log n df n number document matrix df number document term present get tf idf matrix since appears document ha inverse document frequency value b c meanwhile document document b multiplied log e document b document also multiplied log cosine similarity two vector suggesting two totally different document obviously incorrect two document considered similar using tf idf weighting would need third document c matrix different document b thus wondering whether would use tf idf weighting combination cosine similarity metric compare highly similar document none tutorial stackoverflow question read able answer question post discus similar failing tf idf weight using cosine similarity offer guidance edit turn guidance wa looking wa comment blog post recommends using formula log n ni inverse document frequency transformation instead would keep weight term every document close original weight inflating weight term present lot document greater degree interesting formula prominently found post tf idf
Analyzing the usage of particular variable in COBOL program using NLTK,"<p>I am trying to extract the impact information,as a result of changing a particular variable.For example, if  there is a change in Variable X, then if  Y and Z are the dependent variables like below:</p>

<pre><code>Move X to Y
Move Y to Z
</code></pre>

<p>Then I will print list containing  dependent variables as in the order they appear in the MOVE statement like <code>X Y Z</code></p>

<p>My program extract all the MOVE statement and write it to a file.Read that file line by do search for the variable X and its dependencies line by line.There are many drawback to my algorithm. One major flaw is it cannot handle backward reference.
Second thing is if the Variable X occur in ADD statement i need to do a separate  logic. </p>

<p>Can this be solved by NLTK? Any pointers or link to start with, will be more handy.As of now I don't have access to any Mainframe and all the programs are in <code>.txt</code> format in Windows.</p>
",Dataset Preprocessing & Handling,analyzing usage particular variable cobol program using nltk trying extract impact information result changing particular variable example change variable x z dependent variable like print list containing dependent variable order appear move statement like program extract move statement write file read file line search variable x dependency line line many drawback algorithm one major flaw handle backward reference second thing variable x occur add statement need separate logic solved nltk pointer link start handy access mainframe program format window
R stemming a string/document/corpus,"<p>I'm trying to do some stemming in R but it only seems to work on individual documents.  My end goal is a term document matrix that shows the frequency of each term in the document.</p>

<p>Here's an example:</p>

<pre><code>require(RWeka)
require(tm)
require(Snowball)

worder1&lt;- c(""I am taking"",""these are the samples"",
""He speaks differently"",""This is distilled"",""It was placed"")
df1 &lt;- data.frame(id=1:5, words=worder1)

&gt; df1
  id                 words
1  1           I am taking
2  2 these are the samples
3  3 He speaks differently
4  4     This is distilled
5  5         It was placed
</code></pre>

<p>This method works for the stemming part but not the term document matrix part:</p>

<pre><code>&gt; corp1 &lt;- Corpus(VectorSource(df1$words))
&gt; inspect(corp1)
A corpus with 5 text documents

The metadata consists of 2 tag-value pairs and a data frame
Available tags are:
  create_date creator 
Available variables in the data frame are:
  MetaID 

[[1]]
I am taking

[[2]]
these are the samples

[[3]]
He speaks differently

[[4]]
This is distilled

[[5]]
It was placed

&gt; corp1 &lt;- tm_map(corp1, SnowballStemmer)
&gt; inspect(corp1)
A corpus with 5 text documents

The metadata consists of 2 tag-value pairs and a data frame
Available tags are:
  create_date creator 
Available variables in the data frame are:
  MetaID 

[[1]]
[1] I am tak

[[2]]
[1] these are the sampl

[[3]]
[1] He speaks differ

[[4]]
[1] This is distil

[[5]]
[1] It was plac

&gt;  class(corp1)
[1] ""VCorpus"" ""Corpus""  ""list""   
&gt; tdm1 &lt;- TermDocumentMatrix(corp1)
Error in UseMethod(""Content"", x) : 
  no applicable method for 'Content' applied to an object of class ""character""
</code></pre>

<p>So instead I tried creating the term document matrix first but this time the words don't get stemmed:</p>

<pre><code>&gt; corp1 &lt;- Corpus(VectorSource(df1$words))
&gt; tdm1 &lt;- TermDocumentMatrix(corp1, control=list(stemDocument=TRUE))
&gt;  as.matrix(tdm1)
             Docs
Terms         1 2 3 4 5
  are         0 1 0 0 0
  differently 0 0 1 0 0
  distilled   0 0 0 1 0
  placed      0 0 0 0 1
  samples     0 1 0 0 0
  speaks      0 0 1 0 0
  taking      1 0 0 0 0
  the         0 1 0 0 0
  these       0 1 0 0 0
  this        0 0 0 1 0
  was         0 0 0 0 1
</code></pre>

<p>Here the words are obviously not stemmed.</p>

<p>Any suggestions?</p>
",Dataset Preprocessing & Handling,r stemming string document corpus trying stemming r seems work individual document end goal term document matrix show frequency term document example method work stemming part term document matrix part instead tried creating term document matrix first time word get stemmed word obviously stemmed suggestion
Use sapply/lapply or foreach to access data attributes R,"<p>this could be a very basic question but honestly, I tried a few solutions on those similar questions but was unable to drive success on my data. It could be because of my data or I am having a hard day and couldn't figure out anything. :(</p>

<p>I have a vector of sentences </p>

<pre><code>vec = c(""having many items"", ""have an apple"", ""item"")
</code></pre>

<p>Also, I have a data frame to lemmatize the data</p>

<pre><code>lem = data.frame(pattern = c(""(items)|(item)"", ""(has)|(have)|(having)|(had)""), replacement = c(""item"", ""have""))
lem$pattern = as.character(lem$pattern)
lem$replacement = as.character(lem$replacement)
</code></pre>

<p>I want to go through each row in the <code>lem</code> data frame to form a replacement command.</p>

<p>Option 1:</p>

<pre><code>library(stringr) #this is said to be quicker than gsub and my data has 3 mil sentences   
vec &lt;- sapply(lem, function(x) str_replace_all(vec, pattern=x$pattern, replacement = x$replacement))

Error in x$pattern : $ operator is invalid for atomic vectors 
</code></pre>

<p>Option 2:</p>

<pre><code>library(doPar)
vec &lt;- foreach(i = 1:nrow(lem)) %dopar% {
str_replace_all(vec, pattern = lem[i, ""pattern""], replacement = lem[i, ""replacement""])
}
</code></pre>

<p>Option 2 returns a list of 2 vectors: the first one is what I want, the second one is the original, which I don't know why. Also, I tested on my machine, <code>doPar</code> (though using parallel programming) is not as fast as <code>sapply</code>. </p>

<p>Since my data is quite big (<strong>3 mil sentences</strong>), could somebody recommend an effective method to lemmatize the text data?</p>
",Dataset Preprocessing & Handling,use sapply lapply foreach access data attribute r could basic question honestly tried solution similar question wa unable drive success data could data hard day figure anything vector sentence also data frame lemmatize data want go row data frame form replacement command option option option return list vector first one want second one original know also tested machine though using parallel programming fast since data quite big mil sentence could somebody recommend effective method lemmatize text data
"Use RDF API (Jena, OpenRDF or Protege) to convert OpenIE outputs","<p>I was recommended to use one of the APIs (Jena, OpenRDF or Protege) to convert the outputs that I generated from OpenIE4.1 jar file (downloadable from <a href=""http://knowitall.github.io/openie/"" rel=""nofollow"">http://knowitall.github.io/openie/</a>). The following is the sample OpenIE4.1 output format: confidence score followed by subject, predicate, object triplet</p>

<pre><code>    The rail launchers are conceptually similar to the underslung SM-1
    0.93 (The rail launchers; are; conceptually similar to the underslung SM-1)
</code></pre>

<p>I planned to produce triples that follow this pattern from above output (in fact, hundreds of above outputs have been generated by processing a sets of free text documents, only confidence score greater than certain value will be processed):</p>

<p>Given</p>

<pre><code>    subject: The rail launchers
    predicate: are
    object: conceptually similar to the underslung SM-1
</code></pre>

<p>(confidence score is ignored)</p>

<ol>
<li>Create a blank node identifier for each distinct :subject in the file (let’s call it bnode_s)</li>
<li>Create a blank node identifier for each distinct :object in the file (let’s call it bnode_o)</li>
<li>Define a URI for each distinct predicate</li>
</ol>

<p>BTW, I do have outputs that produce more than triplets, for instance
         John gave Mary a birthday gift
         (John; gave; Mary; a birthday gift)
which is more complicated to product RDF triplet.</p>

<p>However, I'm not familiar with any of the API mentioned above and don't know the input format that API may take.</p>
",Dataset Preprocessing & Handling,use rdf api jena openrdf protege convert openie output wa recommended use one apis jena openrdf protege convert output generated openie jar file downloadable following sample openie output format confidence score followed subject predicate object triplet planned produce triple follow pattern output fact hundred output generated processing set free text document confidence score greater certain value processed given confidence score ignored create blank node identifier distinct subject file let call bnode create blank node identifier distinct object file let call bnode define uri distinct predicate btw output produce triplet instance john gave mary birthday gift john gave mary birthday gift complicated product rdf triplet however familiar api mentioned know input format api may take
python memory usage: txt file much smaller than python list containing file text,"<p>I have a 543 MB txt file containing a single line of space separated, utf-8 tokens:</p>

<pre><code>aaa algeria americansamoa appliedethics accessiblecomputing ada anarchism ...
</code></pre>

<p>But, when I load this text data into a python list, it uses ~8 GB of memory (~900 MB for the list and ~8 GB for the tokens):</p>

<pre><code>with open('tokens.txt', 'r') as f:
    tokens = f.read().decode('utf-8').split()

import sys

print sys.getsizeof(tokens)
# 917450944 bytes for the list
print sum(sys.getsizeof(t) for t in tokens)
# 7067732908 bytes for the actual tokens
</code></pre>

<p>I expected the memory usage to be approximately file size + list overhead = 1.5 GB. Why do the tokens consume so much more memory when loaded into a list?</p>
",Dataset Preprocessing & Handling,python memory usage txt file much smaller python list containing file text mb txt file containing single line space separated utf token load text data python list us gb memory mb list gb token expected memory usage approximately file size list overhead gb token consume much memory loaded list
dictionaries feature extraction Python,"<p>I'm doing a text categorization experiment. For the feature extraction phase I'm trying to create a feature dictionary per document. For now, I have two features, Type token ratio and n-grams of the relative frequency of function words. When I print my instances, only the feature type token ratio is in the dictionary. This seems to be because an ill functioning <code>get_pos()</code>. It returns empty lists.
This is my code: </p>

<pre><code>instances = []
labels = []
directory = ""\\Users\OneDrive\Data""
for dname, dirs, files in os.walk(directory):
    for fname in files:
        fpath = os.path.join(dname, fname)
        with open(fpath,'r') as f:
             text = csv.reader(f, delimiter='\t')
             vector = {}

             #TTR
             lemmas = get_lemmas(text)
             unique_lem = set(lemmas)
             TTR = str(len(unique_lem) / len(lemmas))
             name = fname[:5]
             vector['TTR'+ '+' + name] = TTR


             #function word ngrams
             pos = get_pos(text)
             fw = []
             regex = re.compile(
               r'(LID)|(VNW)|(ADJ)|(TW)|(VZ)|(VG)|(BW)')
             for tag in pos:
                 if regex.search(tag):
                    fw.append(tag)
             for n in [1,2,3]:  
                 grams = ngrams(fw, n)
                 fdist = FreqDist(grams)
                 total = sum(c for g,c in fdist.items())
                 for gram, count in fdist.items():
                     vector['fw'+str(n)+'+'+' '+ name.join(gram)] = count/total

                 instances.append(vector)
                 labels.append(fname[:1])
print(instances)
</code></pre>

<p>And this is an example of a Dutch input file:<a href=""https://i.sstatic.net/bOlEJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bOlEJ.png"" alt=""Example of Dutch input""></a></p>

<p>This is the code from the get_pos function, which I call from another script:</p>

<blockquote>
<pre><code>   def get_pos(text):
       row4=[]
       pos = []
       for row in text:
           if not row:
              continue  
           else:
              row4.append(row[4])
              pos = [x.split('(')[0] for x in row4] # remove what's between the brackets
        return pos
</code></pre>
</blockquote>

<p>Can you help me find what's wrong with the get_pos function?</p>
",Dataset Preprocessing & Handling,dictionary feature extraction python text categorization experiment feature extraction phase trying create feature dictionary per document two feature type token ratio n gram relative frequency function word print instance feature type token ratio dictionary seems ill functioning return empty list code example dutch input file code get po function call another script help find wrong get po function
search engine based on CSV files,"<p>I am newbie  in NLP, Recently I am doing price comparison tool that selecting information need of user, the whole process behind of the tool is looking up for the product category of information need first and then get most similar products under this category. my datasets are clean, structured, stored in CSV files with attributes including ""id"", ""retailer"",""product category"", ""product name"",""price_unit"",""price"",""quantity"" 
in my product category list, some product category already contains the key word of products (""ASDA Chosen by Kids Cute and Juicy Apples"" is under ""Apples, pears rhunarb""). 
 but there are some exceptions such as  ""selection lemon"" and ""selection lime"" are under ""citrus fruits"" category</p>

<p>so I need to match input query(input product name by user) and product category first, but  how to identify classification / product category of a input product  given by user  is not sure now for me, i got advice like using inverted index.
can anyone give me more advice of this problem.
Thanks</p>
",Dataset Preprocessing & Handling,search engine based csv file newbie nlp recently price comparison tool selecting information need user whole process behind tool looking product category information need first get similar product category datasets clean structured stored csv file attribute including id retailer product category product name price unit price quantity product category list product category already contains key word product asda chosen kid cute juicy apple apple pear rhunarb exception selection lemon selection lime citrus fruit category need match input query input product name user product category first identify classification product category input product given user sure got advice like using inverted index anyone give advice problem thanks
Python: How to calculate tf-idf for a large data set,"<p>I have a following data frame <code>df</code>, which I converted from <code>sframe</code></p>

<pre><code>   URI                                            name           text
0  &lt;http://dbpedia.org/resource/Digby_M...        Digby Morrell  digby morrell born 10 october 1979 i...
1  &lt;http://dbpedia.org/resource/Alfred_...       Alfred J. Lewy  alfred j lewy aka sandy lewy graduat...
2  &lt;http://dbpedia.org/resource/Harpdog...        Harpdog Brown  harpdog brown is a singer and harmon...
3  &lt;http://dbpedia.org/resource/Franz_R...  Franz Rottensteiner  franz rottensteiner born in waidmann...
4  &lt;http://dbpedia.org/resource/G-Enka&gt;                  G-Enka  henry krvits born 30 december 1974 i...
</code></pre>

<p>I have done the following:</p>

<pre><code>from textblob import TextBlob as tb

import math

def tf(word, blob):
    return blob.words.count(word) / len(blob.words)

def n_containing(word, bloblist):
    return sum(1 for blob in bloblist if word in blob.words)

def idf(word, bloblist):
    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))

def tfidf(word, blob, bloblist):
    return tf(word, blob) * idf(word, bloblist)

bloblist = []

for i in range(0, df.shape[0]):
    bloblist.append(tb(df.iloc[i,2]))

for i, blob in enumerate(bloblist):
    print(""Top words in document {}"".format(i + 1))
    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}
    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    for word, score in sorted_words[:3]:
        print(""\tWord: {}, TF-IDF: {}"".format(word, round(score, 5)))
</code></pre>

<p>But this is taking a lot of time as there are <code>59000</code> documents.</p>

<p>Is there a better way to do it?</p>
",Dataset Preprocessing & Handling,python calculate tf idf large data set following data frame converted done following taking lot time document better way
Cosine similarity vs Cosine distance,"<p>I have a set of documents as given in the example below.</p>

<pre><code>doc1 = {'Science': 0, 'History': 0, 'Politics': 0.15,... 'Sports': 0}
doc2 = {'Science': 0.3, 'History': 0.5, 'Politics': 0.1,... 'Sports': 0}
</code></pre>

<p>I clustered these documents using DBSCAN by using the aforementioned vectors (my vectors are mostly sparse vectors). I get to know that 'cosine similarity' is extremely efficient to compute for sparse vectors. However, according to the sklearn.DBSCAN fit documentation you should use a distance matrix as input to DBSCAN. Hence, I want to know if it is wrong if I used 'cosine similarity' instead of 'cosine distance'.</p>

<p>Please let me know what is the most suitable approach for my problem. Is it DBSCAN using cosine distance <strong>or</strong> DBSCAN using cosine similarity?</p>

<pre><code># Fit DBSCAN using cosine distance
db = DBSCAN(min_samples=1, metric='precomputed').fit(pairwise_distances(feature_matrix, metric='cosine'))
</code></pre>

<p><strong>OR</strong></p>

<pre><code># Fit DBSCAN using cosine similarity
    db = DBSCAN(min_samples=1, metric='precomputed').fit(1-pairwise_distances(feature_matrix, metric='cosine'))
</code></pre>
",Dataset Preprocessing & Handling,cosine similarity v cosine distance set document given example clustered document using dbscan using aforementioned vector vector mostly sparse vector get know cosine similarity extremely efficient compute sparse vector however according sklearn dbscan fit documentation use distance matrix input dbscan hence want know wrong used cosine similarity instead cosine distance please let know suitable approach problem dbscan using cosine distance dbscan using cosine similarity
Text analysis-Unable to write output of Python program in csv or xls file,"<p>Hi I am trying to do a sentiment analysis using Naive Bayes classifier in python 2.x. It reads the sentiment using a txt file and then gives output as positive or negative based on the sample txt file sentiments.
I want the output the same form as input e.g. I have a text file of lets sat 1000 raw  sentiments and I want the output to show positive or negative against each sentiment.
Please help.
Below is the code i am using</p>

<pre><code>import math
import string

def Naive_Bayes_Classifier(positive, negative, total_negative, total_positive, test_string):
    y_values = [0,1]
    prob_values = [None, None]

    for y_value in y_values:
        posterior_prob = 1.0

        for word in test_string.split():
            word = word.lower().translate(None,string.punctuation).strip()
            if y_value == 0:
                if word not in negative:
                    posterior_prob *= 0.0
                else:
                    posterior_prob *= negative[word]
            else:
                if word not in positive:
                    posterior_prob *= 0.0
                else:
                    posterior_prob *= positive[word]

        if y_value == 0:
            prob_values[y_value] = posterior_prob * float(total_negative) / (total_negative + total_positive)
        else:
            prob_values[y_value] = posterior_prob * float(total_positive) / (total_negative + total_positive)

    total_prob_values = 0
    for i in prob_values:
        total_prob_values += i

    for i in range(0,len(prob_values)):
        prob_values[i] = float(prob_values[i]) / total_prob_values

    print prob_values

    if prob_values[0] &gt; prob_values[1]:
        return 0
    else:
        return 1


if __name__ == '__main__':
    sentiment = open(r'C:/Users/documents/sample.txt')

    #Preprocessing of training set
    vocabulary = {}
    positive = {}
    negative = {}
    training_set = []
    TOTAL_WORDS = 0
    total_negative = 0
    total_positive = 0

    for line in sentiment:
        words = line.split()
        y = words[-1].strip()
        y = int(y)

        if y == 0:
            total_negative += 1
        else:
            total_positive += 1

        for word in words:
            word = word.lower().translate(None,string.punctuation).strip()
            if word not in vocabulary and word.isdigit() is False:
                vocabulary[word] = 1
                TOTAL_WORDS += 1
            elif word in vocabulary:
                vocabulary[word] += 1
                TOTAL_WORDS += 1

            #Training
            if y == 0:
                if word not in negative:
                    negative[word] = 1
                else:
                    negative[word] += 1
            else:
                if word not in positive:
                    positive[word] = 1
                else:
                    positive[word] += 1

    for word in vocabulary.keys():
        vocabulary[word] = float(vocabulary[word])/TOTAL_WORDS

    for word in positive.keys():
        positive[word] = float(positive[word])/total_positive

    for word in negative.keys():
        negative[word] = float(negative[word])/total_negative

    test_string = raw_input(""Enter the review: \n"")

    classifier = Naive_Bayes_Classifier(positive, negative, total_negative, total_positive, test_string)
    if classifier == 0:
        print ""Negative review""
    else:
        print ""Positive review""
</code></pre>
",Dataset Preprocessing & Handling,text analysis unable write output python program csv xl file hi trying sentiment analysis using naive bayes classifier python x read sentiment using txt file give output positive negative based sample txt file sentiment want output form input e g text file let sat raw sentiment want output show positive negative sentiment please help code using
Stemming in python,"<p>I want to stem my text, which I am reading from CSV file. But after the stem-operator the text is not changed. Than I have read somewhere that I need to use POS tags in order to stem but it didn't help.</p>

<p>Can you please tell me what I am doing wrong? So I am reading the csv, removing punctuation, tokenizing, getting POS tags, and trying to stem but nothing is changing. </p>

<pre><code>from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import PorterStemmer
import nltk
from nltk import pos_tag

stemmer = nltk.PorterStemmer()
data = pd.read_csv(open('data.csv'),sep=';')

translator=str.maketrans('','',string.punctuation)

with open('output.csv', 'w', newline='') as csvfile:
   writer = csv.writer(csvfile, delimiter=';',
                            quotechar='^', quoting=csv.QUOTE_MINIMAL)

   for line in data['sent']:
        line = line.translate(translator)
        tokens = word_tokenize(line)
        tokens_pos = nltk.pos_tag(tokens)
        final = [stemmer.stem(tagged_word[0]) for tagged_word in tokens_pos]
        writer.writerow(tokens_pos)
</code></pre>

<p>Examples of data for stemming:</p>

<pre><code>The question was, what are you going to cut?
Well, again, while you were on the board of the Woods Foundation...
We've got some long-term challenges in this economy.
</code></pre>

<p>Thank you in advance for any help!</p>
",Dataset Preprocessing & Handling,stemming python want stem text reading csv file stem operator text changed read somewhere need use po tag order stem help please tell wrong reading csv removing punctuation tokenizing getting po tag trying stem nothing changing example data stemming thank advance help
rows csv into nested list,"<p>I'm a beginner in programming, but for a natural language processing project I need to work with csv. 
I have this csv file with annotated text. The sentences are separated from each other with an empty row. Each row is a token (word or punctuation with it's annotation). What I need is a nested list like this <code>[[[I,pronoun],[need, verb], [you, pronoun]], [[Do, verb], [you, pronoun], [need, verb], [me, pronoun]]]</code></p>

<p>The text looks like this in the csv:</p>

<pre><code>I  pronoun
need  verb
you  pronoun

Do  pronoun
you  pronoun
need verb
me  pronoun
</code></pre>

<p>I tried the following code, but then I only get one big list and not a nested list. I don't know how to split the sentences into different lists on the empty row.</p>

<pre><code> sentences = []
    for row in text:
        sentences.append(list(row))
 print(sentences)
</code></pre>

<p>Any suggestions?</p>
",Dataset Preprocessing & Handling,row csv nested list beginner programming natural language processing project need work csv csv file annotated text sentence separated empty row row token word punctuation annotation need nested list like text look like csv tried following code get one big list nested list know split sentence different list empty row suggestion
Scikit-Learn - No True Positives - Best Way to Normalize Data,"<p>Thanks for taking the time to read my question!</p>

<p>So I am running an experiment to see if I can predict whether an individual has been diagnosed with depression (or at least says they have been) based on the words (or tokens)they use in their tweets. I found 139 users that at some point tweeted ""I have been diagnosed with depression"" or some variant of this phrase in an earnest context (.e. not joking or sarcastic. Human beings that were native speakers in the language of the tweet  were used to discern whether the tweet being made was genuine or not).</p>

<p>I then collected the entire public timeline of tweets of all of these users' tweets, giving me a ""depressed user tweet corpus"" of about 17000 tweets.</p>

<p>Next I created a database of about 4000 random ""control"" users, and with their timelines created a ""control tweet corpus"" of about 800,000 tweets.</p>

<p>Then I combined them both into a big dataframe,which looks like this:</p>

<pre><code>,class,tweet
0,depressed,tweet text .. *
1,depressed,tweet text.
2,depressed,@ tweet text
3,depressed,저 tweet text
4,depressed,@ tweet text😚
5,depressed,@ tweet text😍
6,depressed,@ tweet text ?
7,depressed,@ tweet text ?
8,depressed,tweet text *
9,depressed,@ tweet text ?
10,depressed,@ tweet text
11,depressed,tweet text *
12,depressed,#tweet text
13,depressed,
14,depressed,tweet text !
15,depressed,tweet text
16,depressed,tweet text. .
17,depressed,tweet text
...
50595,control,@tweet text?
150596,control,""@ tweet text.""
150597,control,@ tweet text.
150598,control,""@ tweet text. *""
150599,control,""@tweet text?""t
150600,control,""@ tweet text?""
150601,control,@ tweet text?
150602,control,@ tweet text.
150603,control,@tweet text~
150604,control,@ tweet text.
</code></pre>

<p>Then I trained a multinomial naive bayes classifier using an object from the CountVectorizer() class imported from the sklearn library:</p>

<pre><code>count_vectorizer = CountVectorizer()
counts = count_vectorizer.fit_transform(tweet_corpus['tweet'].values)

classifier = MultinomialNB()
targets = tweet_corpus['class'].values
classifier.fit(counts, targets)
MultinomialNB(alpha=1.0, class_prior=None, fit_prior= True)
</code></pre>

<p>Unfortunately, after running a 6-fold cross validation test, the results suck and I am trying to figure out why.</p>

<pre><code>Total tweets classified: 613952
Score: 0.0
Confusion matrix:
[[596070    743]
 [ 17139      0]]
</code></pre>

<p>So, I didn't properly predict a single depressed person's tweet! My initial thought is that I have not properly normalized the counts of the control group, and therefore even tokens which appear more frequently among the depressed user corpus are over represented in the control tweet corpus due to its much larger size. I was under the impression that .fit() did this already, so maybe I am on the wrong track here? If not, any suggestions on the most efficient way to normalize the data between two groups of disparate size?</p>
",Dataset Preprocessing & Handling,scikit learn true positive best way normalize data thanks taking time read question running experiment see predict whether individual ha diagnosed depression least say based word token use tweet found user point tweeted diagnosed depression variant phrase earnest context e joking sarcastic human native speaker language tweet used discern whether tweet made wa genuine collected entire public timeline tweet user tweet giving depressed user tweet corpus tweet next created database random control user timeline created control tweet corpus tweet combined big dataframe look like trained multinomial naive bayes classifier using object countvectorizer class imported sklearn library unfortunately running fold cross validation test result suck trying figure properly predict single depressed person tweet initial thought properly normalized count control group therefore even token appear frequently among depressed user corpus represented control tweet corpus due much larger size wa impression fit already maybe wrong track suggestion efficient way normalize data two group disparate size
Does the Google Cloud Natural Language API actually support parsing HTML?,"<p>I'm trying to extract the main body content from news sites &amp; blogs.</p>

<p>The docs make it seem as though <code>documents.analyzeSyntax</code> would work as expected with HTML by passing it a <code>document</code> with the <code>content</code> as the page's raw HTML (utf-8) and the document's <code>type</code> set to <code>HTML</code>. The docs definitely include HTML as a supported content type.</p>

<p>In practice, however, the resulting sentences and tokens are muddled with HTML tags as though the parser thinks the input is plain text. As it stands, this rules out the GC NL API for my use case, and presumably many others as processing web pages via natural language is a pretty common task.</p>

<p>For reference, here is an <a href=""https://dandelion.eu/semantic-text/entity-extraction-demo/?text=http%3A%2F%2Fcontent.time.com%2Ftime%2Fspecials%2F2007%2Farticle%2F0%2C28804%2C1690753_1690757_1695382%2C00.html&amp;lang=auto&amp;min_confidence=0.6&amp;exec=true#results"" rel=""nofollow noreferrer"">example</a> by Dandelion API of the type of output one would expect given HTML input (or rather in this case a URL to an HTML page as input).</p>

<p>My question, then, is am I missing something, possibly invoking the API incorrectly, or does the NL API not support HTML?</p>
",Dataset Preprocessing & Handling,doe google cloud natural language api actually support parsing html trying extract main body content news site blog doc make seem though would work expected html passing page raw html utf document set doc definitely include html supported content type practice however resulting sentence token muddled html tag though parser think input plain text stand rule gc nl api use case presumably many others processing web page via natural language pretty common task reference example dandelion api type output one would expect given html input rather case url html page input question missing something possibly invoking api incorrectly doe nl api support html
Word2vec saved model is not UTF-8 encoded but the sentence input to the Word2vec model is UTF-8 encoded,"<p>I trained a word2vec model using gensim package and saved it with the following name. </p>

<pre><code>model_name = ""300features_1minwords_10context""
model.save(model_name)
</code></pre>

<p>I got these log message info. while the model was getting trained and saved.</p>

<pre><code>INFO : not storing attribute syn0norm
INFO : not storing attribute cum_table
</code></pre>

<p>Then, I tried to load the model using this, </p>

<pre><code>from gensim.models import Word2Vec
model = Word2Vec.load(""300features_1minwords_10context"")
</code></pre>

<p>I got the following error. </p>

<pre><code>2017-06-22 21:27:14,975 : INFO : loading Word2Vec object from 300features_1minwords_10context
2017-06-22 21:27:15,496 : INFO : loading wv recursively from 300features_1minwords_10context.wv.* with mmap=None
2017-06-22 21:27:15,497 : INFO : setting ignored attribute syn0norm to None
2017-06-22 21:27:15,498 : INFO : setting ignored attribute cum_table to None
2017-06-22 21:27:15,499 : INFO : loaded 300features_1minwords_10context
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-25-9d90db0f07c0&gt; in &lt;module&gt;()
      1 from gensim.models import Word2Vec
      2 model = Word2Vec.load(""300features_1minwords_10context"")
----&gt; 3 model.syn0.shape

AttributeError: 'Word2Vec' object has no attribute 'syn0'
</code></pre>

<p>Also, in the file ""300features_1minwords_10context"", it shows that </p>

<pre><code>""300features_1minwords_10context"" is not UTF-8 encoded
Saving disabled.
Open console for more details 
</code></pre>

<p>To fix the above attribute error, I have also tried the following from the google forum, </p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format(""300features_1minwords_10context"")
model.syn0.shape
</code></pre>

<p>It resulted in another error which is </p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>The model is trained with UTF-8 encoded sentences. I am not sure why is it throwing this error ?</p>

<p>More info : </p>

<pre><code>df = pd.read_csv('UNSPSCdataset.csv',encoding='mac_roman',low_memory=False)
features = ['MaterialDescription']
temp_features = df[features]
temp_features.to_csv('materialDescription', encoding='UTF-8')
X = pd.read_csv('materialDescription',encoding='UTF-8')
</code></pre>

<p>Here, I had to use 'mac_roman' encoding in order to access it using pandas dataframe. Since the text in the dataframe has to be in UTF-8 while training the model, I have saved that particular feature in a separate csv file by encoding it with UTF-8 and later, I have the accessed that particular column.</p>

<p>Any help is appreciable </p>
",Dataset Preprocessing & Handling,word vec saved model utf encoded sentence input word vec model utf encoded trained word vec model using gensim package saved following name got log message info model wa getting trained saved tried load model using got following error also file feature minwords context show fix attribute error also tried following google forum resulted another error model trained utf encoded sentence sure throwing error info use mac roman encoding order access using panda dataframe since text dataframe ha utf training model saved particular feature separate csv file encoding utf later accessed particular column help appreciable
Load fasttext model faster by excluding certain vocabulary,"<p>Loading the pretrained fasttext wordvectors released by Facebook Research take a very long time on a local machine, which I do like this:</p>

<pre><code>model =  fs.load_word2vec_format('wiki.en.vec') 
print(model['test']) # get the vector of the word 'test'
</code></pre>

<p>I am seeking to reduce the load time by <strong>removing wordvectors for words that don't appear in my dataset</strong>. I.e. I want to reduce the pretrained vector model to the words that comprise the dataset I need to analyse, which is a subset of the pre-trained model.</p>

<p>I was about to try and build a new model by extracting the wordvectors I need and saving to a new model, but the type would change from <em>FastTextKeyedVectors</em> to <em>FastText</em>:</p>

<pre><code>#model2 = gensim.models.Word2Vec(iter=1)
#model2 = FastText()
for word in words:
    if (word in model):
       model2[] = model[word]
</code></pre>

<p>How can I reduce my load time ? Do my approaches make sense or am I on the wrong path ?</p>
",Dataset Preprocessing & Handling,load fasttext model faster excluding certain vocabulary loading pretrained fasttext wordvectors released facebook research take long time local machine like seeking reduce load time removing wordvectors word appear dataset e want reduce pretrained vector model word comprise dataset need analyse subset pre trained model wa try build new model extracting wordvectors need saving new model type would change fasttextkeyedvectors fasttext reduce load time approach make sense wrong path
How to Create JAPE Grammars Automatically?,"<p>I am having great troubles with JAPE grammars.  I have a small token dictionary  for the words that needs to be matched with  5 types of  document. 
One dictionary for  one type: For example Job, the dictionary of the person would contain  <code>{ ""Engineer"" , ""Doctor"", ""Manager""}</code>.  I need to read this dictionary a create JAPE rules for that.  This is my first try </p>

<p>Phase: Jobtitle<br>
Input: Lookup<br>
Options: control = appelt debug = true  </p>

<pre><code>Rule: Jobs  
(  
 {Lookup.majorType == ""Doctor""}  
 (  
  {Lookup.majorType ==  ""Engineer""}  
 )?  
)  
:jobs
--&gt;  
 :jobs.JobTitle = {rule = ""Jobs""}
</code></pre>

<p>Is there any way to  automatically create  JAPE rules that only for searching tokens in a dictionary to documents?</p>
",Dataset Preprocessing & Handling,create jape grammar automatically great trouble jape grammar small token dictionary word need matched type document one dictionary one type example job dictionary person would contain need read dictionary create jape rule first try phase jobtitle input lookup option control appelt debug true way automatically create jape rule searching token dictionary document
How to parse an xml file in python?,"<p>I have an xml file and it looks like this</p>

<pre><code>&lt;?xml version='1.0' encoding='UTF8'?&gt;
&lt;Reviews&gt;
  &lt;Review rid=""0"" book_title=""O-Apanhador-no-Campo-de-Centeio"" score=""4.0""&gt;
    &lt;sentences&gt;
      &lt;sentence id=""0:0:0"" place=""title"" polarity=""neutral""&gt;
        &lt;text&gt;Está provado:&lt;/text&gt;
        &lt;tokens&gt;
          &lt;word id=""1"" form=""Está"" base=""estar"" postag=""v-fin"" morf=""PR 3S IND VFIN"" extra=""fmc * vK mv"" head=""0"" deprel=""STA"" srl=""PRED"" obj=""O"" opinion=""O"" from=""0"" to=""4""/&gt;
          &lt;word id=""2"" form=""provado"" base=""provar"" postag=""v-fin"" morf=""PCP M S"" extra=""vH jh"" head=""1"" deprel=""Cs"" srl=""ATR"" obj=""O"" opinion=""O"" from=""5"" to=""12""/&gt;
          &lt;word id=""3"" form="":"" base=""--"" postag=""pu"" morf=""--"" extra=""--"" head=""0"" deprel=""PU"" srl="""" obj=""O"" opinion=""O"" from=""12"" to=""13""/&gt;
        &lt;/tokens&gt;
      &lt;/sentence&gt;
      &lt;sentence id=""0:0:1"" place=""title"" polarity=""neutral""&gt;
        &lt;text&gt;Pode existir um livro bom sem uma história boa.&lt;/text&gt;
        &lt;tokens&gt;
          &lt;word id=""1"" form=""Pode"" base=""poder"" postag=""v-fin"" morf=""PR 3S IND VFIN"" extra=""fmc * aux"" head=""0"" deprel=""STA"" srl="""" obj=""O"" opinion=""O"" from=""0"" to=""4""/&gt;
          &lt;word id=""2"" form=""existir"" base=""existir"" postag=""v-inf"" morf=""--"" extra=""mv"" head=""1"" deprel=""Oaux"" srl=""PRED"" obj=""O"" opinion=""O"" from=""5"" to=""12""/&gt;
          &lt;word id=""3"" form=""um"" base=""um"" postag=""pron-indef"" morf=""M S"" extra=""--"" head=""4"" deprel=""DN"" srl="""" obj=""O"" opinion=""O"" from=""13"" to=""15""/&gt;
          &lt;word id=""4"" form=""livro"" base=""livro"" postag=""n"" morf=""M S"" sem=""sem-r"" extra=""--"" head=""1"" deprel=""S"" srl=""TH"" obj=""O"" opinion=""O"" from=""16"" to=""21""/&gt;
          &lt;word id=""5"" form=""bom"" base=""bom"" postag=""adj"" morf=""M S"" extra=""np-close"" head=""4"" deprel=""DN"" srl="""" obj=""O"" opinion=""O"" from=""22"" to=""25""/&gt;
          &lt;word id=""6"" form=""sem"" base=""sem"" postag=""prp"" morf=""--"" extra=""--"" head=""2"" deprel=""fA"" srl="""" obj=""O"" opinion=""O"" from=""26"" to=""29""/&gt;
          &lt;word id=""7"" form=""uma"" base=""um"" postag=""pron-indef"" morf=""F S"" extra=""--"" head=""8"" deprel=""DN"" srl="""" obj=""O"" opinion=""O"" from=""30"" to=""33""/&gt;
          &lt;word id=""8"" form=""história"" base=""história"" postag=""n"" morf=""F S"" sem=""per domain sem-r"" extra=""--"" head=""6"" deprel=""DP"" srl=""COM-ADV"" obj=""O"" opinion=""O"" from=""34"" to=""42""/&gt;
          &lt;word id=""9"" form=""boa"" base=""bom"" postag=""adj"" morf=""F S"" extra=""jh np-close"" head=""8"" deprel=""DN"" srl="""" obj=""O"" opinion=""O"" from=""43"" to=""46""/&gt;
          &lt;word id=""10"" form=""."" base=""--"" postag=""pu"" morf=""--"" extra=""--"" head=""0"" deprel=""PU"" srl="""" from=""46"" to=""47""/&gt;
        &lt;/tokens&gt;
</code></pre>

<p>I want to extract the text field and the polarity to a separate csv file.</p>

<p>I used this to extract polarity successfully, but I can't extract the text</p>

<pre><code>with open('output1.csv', 'w') as f:
    writer = csv.writer(f)
    writer.writerow(('text', 'polarity'))
    root = lxml.etree.fromstring(xmlstr)
    for sent in root.iter('sentence'):
        row = sent.get('text'), sent.get('polarity')
        writer.writerow(row)
</code></pre>

<p>where xmlstr is a string of the content of the xml file.</p>

<p>How can I extract the text field from the file !?</p>

<p>note: This is a link containing the file I'm working with
<a href=""https://raw.githubusercontent.com/pedrobalage/ABSA_Experiments/master/corpus/ReLiPalavras.xml"" rel=""nofollow noreferrer"">sentiment analysis in portuguese</a></p>

<p>can any one help !? </p>

<p>Thanks</p>
",Dataset Preprocessing & Handling,parse xml file python xml file look like want extract text field polarity separate csv file used extract polarity successfully extract text xmlstr string content xml file extract text field file note link containing file working sentiment analysis portuguese one help thanks
How to write sentiment analysis results from twitter into a CSV file,"<p>I am new to python and NLP , i am working on twitter sentiment analysis.i am able to  print the data along with the polarity and subjectivity also but my goal is to write the data to a csv file which i am not able to do, i am getting IO exception while doing that . Here is my code </p>

<pre><code>import tweepy
from textblob import TextBlob
 import csv

consumer_key = 'xxxxxxxxxxxxxxxxxxxxxx'
consumer_secret = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'

access_token = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
access_token_secret = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxx'

auth = tweepy.OAuthHandler(consumer_key,consumer_secret)
auth.set_access_token(access_token,access_token_secret)

api = tweepy.API(auth)

public_tweets = api.search('Uk election')
with open(""sentiment.txt"",'w') as scorefile:
scoreFileWriter = csv.writer(scorefile) 
for tweet in public_tweets:
print(tweet.text)
analysis = TextBlob(tweet.text)
print(analysis.sentiment)
scoreFileWriter.writerow([tweet.text,analysis.sentiment])
</code></pre>

<p>I am getting exception as ,</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/Jeet Chatterjee/NLP/sentiment_analysis_twitter.py"", line 25, in &lt;module&gt;
    scoreFileWriter.writerow([tweet.text,analysis.sentiment])
ValueError: I/O operation on closed file.  
</code></pre>

<p>i just want to have 3 columns like polarity , subjectivity and the content in the csv file . Please help </p>
",Dataset Preprocessing & Handling,write sentiment analysis result twitter csv file new python nlp working twitter sentiment analysis able print data along polarity subjectivity also goal write data csv file able getting io exception code getting exception want column like polarity subjectivity content csv file please help
Data csv file into different text files with Python,"<p>I'm a beginner in programming, but for a Dutch text categorization experiment I want to turn every instance (row) of a csv file into separate .txt files, so that the texts can be analyzed by a NLP tool. My csv looks like this.</p>

<p><a href=""https://i.sstatic.net/21dIK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/21dIK.png"" alt=""enter image description here""></a></p>

<p>As you can see, each instance has text in the column 'Taaloefening1' or in the column 'Taaloefening2'. Now I need to save the text per instance in a .txt file and the name of the file needs to be the id and the label.
I was hoping I could to this automatically by programming a script in Python by using the csv module. I have an idea about how to save the text into a .txt file, but I have no idea how to take the id and label, which match the text, as the file name.
Any ideas?</p>
",Dataset Preprocessing & Handling,data csv file different text file python beginner programming dutch text categorization experiment want turn every instance row csv file separate txt file text analyzed nlp tool csv look like see instance ha text column taaloefening column taaloefening need save text per instance txt file name file need id label wa hoping could automatically programming script python using csv module idea save text txt file idea take id label match text file name idea
.append() in python just gets ignored?,"<p>So am trying to extract nouns from a csv file. 
Checking each value whether its a noun or not using Textblob package. 
The nouns found are appended to an empty list x. 
Then finally when the looping is finished, I print the final list expecting all nouns but Nothing happens..(The data set is huge).
Some one help me out.</p>

<pre><code>from nltk import FreqDist
from textblob import TextBlob
import pandas as p

x = list()


data = p.DataFrame.from_csv('hl.csv', encoding = ""ISO-8859-1"")
data = data.reset_index()

for column in data.columns.values:
    for value in data[column]:
        blob = TextBlob(value)
##        print(blob.noun_phrases)    #this print statement gives nouns
        x.append(blob.noun_phrases)   #so I append the results to an empty list

print(x)      #When printing the final list.. NOTHING HAPPENS no empty list nothing. Python just ignores it why ???
</code></pre>
",Dataset Preprocessing & Handling,append python get ignored trying extract noun csv file checking value whether noun using textblob package noun found appended empty list x finally looping finished print final list expecting noun nothing happens data set huge one help
How to make Doc2Vec document vectors all positive?,"<p>I'm trying to use nonnegative matrix factorization on the output of Doc2Vec. However there is a constraint that there can be no negative input. How do I make it positive without doing something like absolute value that will construe the results? If it helps, I'm loading in a new paragraph and computing similarity but I thought that using Doc2Vec would capture more meaning.</p>
",Dataset Preprocessing & Handling,make doc vec document vector positive trying use nonnegative matrix factorization output doc vec however constraint negative input make positive without something like absolute value construe result help loading new paragraph computing similarity thought using doc vec would capture meaning
Extract id and corresponding tokens and append to dictionary from file python,"<p>I'm trying to create a dictionary from some text files which define a corpus(RCV1 dataset tokens).I've cleaned the file from some stopwords with the help of regex.Originally it looked like this: <a href=""https://ibb.co/h3eG5v"" rel=""nofollow noreferrer"">https://ibb.co/h3eG5v</a>. I cleaned the stopwords with this code: </p>

<pre><code>def cleanFile():
    infile = ""lyrl2004_tokens_train.dat""
    outfile = ""cleaned_file.dat""

    delete_list = ["".W"",'.I ']
    fin = open(infile)
    fout = open(outfile, ""w+"")

    for line in fin:
        for word in delete_list:
            line = line.replace(word, """")
    fout.write(line)
    fin.close()
    fout.close()
</code></pre>

<p>And then used a small code chunk to remove any blank lines as well.Now the text files basically look like this: <a href=""https://ibb.co/e7Ww5v"" rel=""nofollow noreferrer"">https://ibb.co/e7Ww5v</a></p>

<p>So now the format is a line with the document id,integer(2286-26150 for the training data),then multiple lines with the tokens seperated by a single space and then the block repeats:</p>

<p>2286</p>

<p>token token token token token token</p>

<p>token token token token token</p>

<p>2287</p>

<p>token token token..</p>

<p>What I'm trying to accomplish is write a function that will read the entire file which thankfully can fit into memory,then construct a dictionary with the document ID and its corresponding tokens inside a list.It should look like this : {'2286':[token,token,token....],'2287':[token,token,token...],...}. I'm out of ideas because I can't find a way to repeatedly process text between two consecutive numbers,since everything I've searched for usually includes delimiters that are not numbers.
Just for the info,I'm going to construct a text classifier next with this data (which is why I need a dictionary).The test tokens are in the same format as the training ones,with higher integers up to 800.000</p>
",Dataset Preprocessing & Handling,extract id corresponding token append dictionary file python trying create dictionary text file define corpus rcv dataset token cleaned file stopwords help regex originally looked like cleaned stopwords code used small code chunk remove blank line well text file basically look like format line document id integer training data multiple line token seperated single space block repeat token token token token token token token token token token token token token token trying accomplish write function read entire file thankfully fit memory construct dictionary document id corresponding token inside list look like token token token token token token idea find way repeatedly process text two consecutive number since everything searched usually includes delimiters number info going construct text classifier next data need dictionary test token format training one higher integer
how to stem each row in csv file?,"<p>I have a CSV file with two columns  contains sentence. for example
Test.csv:</p>

<pre><code>Col[1]
----------------------
This trip was amazing.

Col[2]
--------------------
The cats are playing.
</code></pre>

<p>so I did some nlp process:</p>

<pre><code>with codecs.open('test.csv','r', encoding='utf-8', errors='ignore') as myfile:
     data = csv.reader(myfile, delimiter=',')
     next(data)
     stops = set(stopwords.words(""english""))
     stemmer = PorterStemmer()
     for row in data:
        word_tokens1 = word_tokenize(row[1].lower())
        word_tokens2 = word_tokenize(row[2].lower())
        remo1 = [w for w in word_tokens1 if w in re.sub(""[^a-zA-Z]"","" "",w )]
        remo2 = [w for w in word_tokens2 if w in re.sub(""[^a-zA-Z]"","" "",w)]
        list1 = [w for w in remo1 if not w in stops]
        list2 = [w for w in remo2 if not w in stops]
        for w in list1:
           l = stemmer.stem(w)
           print(l)
        for w in list2:
           l2 = stemmer.stem(w)
           print(l2)
</code></pre>

<p>my problem is when I do stemming, and when I print it:</p>

<pre><code>trip
amazi
cat 
play
</code></pre>

<p>it print each word in a row. how can I return to the sentence after stemming
like:</p>

<pre><code>Col[1]:
-------------------
trip amazi

Col[2]:
------------------- 
cat play
</code></pre>
",Dataset Preprocessing & Handling,stem row csv file csv file two column contains sentence example test csv nlp process problem stemming print print word row return sentence stemming like
How to do sequence labeling with an unlabeled dataset,"<p>I have 1000 text files which have discharge summary for patients   </p>

<p>SAMPLE_1</p>

<blockquote>
  <p>The patient was admitted on 21/02/99. he appeared to have pneumonia at the time
  of admission so we empirically covered him for community-acquired pneumonia with
  ceftriaxone and azithromycin until day 2 when his blood cultures grew
  out strep pneumoniae that was pan sensitive so we stopped the
  ceftriaxone and completed a 5 day course of azithromycin. But on day 4
  he developed diarrhea so we added flagyl to cover for c.diff, which
  did come back positive on day 6 so he needs 3 more days of that…” this
  can be summarized more concisely as follows: “Completed 5 day course
  of azithromycin for pan sensitive strep pneumoniae pneumonia
  complicated by c.diff colitis. Currently on day 7/10 of flagyl and
  c.diff negative on 9/21.</p>
</blockquote>

<p>SAMPLE_2</p>

<blockquote>
  <p>The patient is an 56-year-old female with history of previous stroke; hypertension;
  COPD, stable; renal carcinoma; presenting after
  a fall and possible syncope.  While walking, she accidentally fell to
  her knees and did hit her head on the ground, near her left eye.  Her
  fall was not observed, but the patient does not profess any loss of
  consciousness, recalling the entire event.  The patient does have a
  history of previous falls, one of which resulted in a hip fracture. 
  She has had physical therapy and recovered completely from that. 
  Initial examination showed bruising around the left eye, normal lung
  examination, normal heart examination, normal neurologic function with
  a baseline decreased mobility of her left arm.  The patient was
  admitted for evaluation of her fall and to rule out syncope and
  possible stroke with her positive histories.</p>
</blockquote>

<p>I also have a csv file which is 1000rows X 5columns. Each row has information entered manually for each of the text file.
So for example for the above two files, someone has manually entered these records in the csv file:</p>

<pre><code>Sex, Primary Disease,Age, Date of admission,Other complications
M,Pneumonia, NA, 21/02/99, Diarhhea
F,(Hypertension,stroke), 56, NA, NA
</code></pre>

<p>My question is:</p>

<ol>
<li><p>How do I represent use this information of text:labels to a machine learning algorithm</p></li>
<li><p>Do I need to do some manual labelling around the areas of interest in all the 1000 text files?</p></li>
</ol>

<p>If yes then how and which method to use. (i.e. like <code>&lt;ADMISSION&gt; was admitted on 21/02/99&lt;/ADMISSION&gt;</code>, 
    <code>&lt;AGE&gt;56-year-old&lt;/AGE&gt;</code>)</p>

<p>So basically how do I use this text:labels data to automate the filling of labels. </p>
",Dataset Preprocessing & Handling,sequence labeling unlabeled dataset text file discharge summary patient sample patient wa admitted appeared pneumonia time admission empirically covered community acquired pneumonia ceftriaxone azithromycin day blood culture strep pneumoniae wa pan sensitive stopped ceftriaxone completed day course azithromycin day developed diarrhea added flagyl cover c diff come back positive day need day summarized concisely follows completed day course azithromycin pan sensitive strep pneumoniae pneumonia complicated c diff colitis currently day flagyl c diff negative sample patient year old female history previous stroke hypertension copd stable renal carcinoma presenting fall possible syncope walking accidentally fell knee hit head ground near left eye fall wa observed patient doe profess loss consciousness recalling entire event patient doe history previous fall one resulted hip fracture ha physical therapy recovered completely initial examination showed bruising around left eye normal lung examination normal heart examination normal neurologic function baseline decreased mobility left arm patient wa admitted evaluation fall rule syncope possible stroke positive history also csv file row x column row ha information entered manually text file example two file someone ha manually entered record csv file question represent use information text label machine learning algorithm need manual labelling around area interest text file yes method use e like basically use text label data automate filling label
Tensor flow word2vec for multiple files input,"<p>I am trying to train a word2vec skip-gram model on some in-house dataset. I am following tensorflow <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"" title=""word2vec_basic.py"">word2vec_basic.py</a> tutorial. The dataset has sentences so I modified generate_batch function and stored (batch,label) in csv files. Since the data is large, these files are divided into part-files. I need to change the tf.session part of the code to accommodate these multiple files. I can't load all files at once because of memory constraints. Here is my code for tf.session:</p>

<pre><code>import tensorflow as tf
import glob

folder_files =  glob.glob(""word2vecIndexes"")

with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print('Initialized')
    for i in len(folder_files):
        indexes_data = getData(folder_files[i])
        average_loss = 0
        index=0
        length_train = len(indexes_data)
        check_range = int(length_train/batch_size)+1
        print(check_range)
        for step in range(check_range):
            print("".....""+step)
            batch_data, batch_labels = generate_batch(index, batch_size, length_train)
            index = index+batch_size
            feed_dict = {train_dataset : batch_data, train_labels : batch_labels}
            _, l = session.run([optimizer, loss], feed_dict=feed_dict)
            average_loss += l
            if step % 2000 == 0:
              if step &gt; 0:
                average_loss = average_loss / 2000
              # The average loss is an estimate of the loss over the last 2000 batches.
              print('Average loss at step %d: %f' % (step, average_loss))
              average_loss = 0
    final_embeddings = normalized_embeddings.eval()
    ## save as textFile ##
    np.savetxt('~/final_embedding_dic.txt',final_embeddings)
    ## save as tensorflow variable ##
    saver.save(self._session,opts.save_path + ""model"")enter code here`
</code></pre>

<p>I am new to tensorflow, so I am confused if ""final_embeddings"" will represent trained embeddings using all files or it gets initialized at every iteration and is trained on that part file only. Is there any optimized way to run this?</p>
",Dataset Preprocessing & Handling,tensor flow word vec multiple file input trying train word vec skip gram model house dataset following tensorflow word vec basic py tutorial dataset ha sentence modified generate batch function stored batch label csv file since data large file divided part file need change tf session part code accommodate multiple file load file memory constraint code tf session new tensorflow confused final embeddings represent trained embeddings using file get initialized every iteration trained part file optimized way run
Decoding/Encoding using sklearn load_files,"<p>I'm following the tutorial here 
<a href=""https://github.com/amueller/introduction_to_ml_with_python/blob/master/07-working-with-text-data.ipynb"" rel=""nofollow noreferrer"">https://github.com/amueller/introduction_to_ml_with_python/blob/master/07-working-with-text-data.ipynb</a>
to learn about machine learning and text.</p>

<p>In my case, I'm using tweets I downloaded, with positive and negative tweets in the exact same directory structure they are using (trying to learn sentiment analysis).</p>

<p>Here in the iPython Notebook I load my data just like they do:</p>

<pre><code>tweets_train =load_files('Path to my training Tweets')
</code></pre>

<p>And then I try to fit them with CountVectorizer</p>

<pre><code>vect = CountVectorizer().fit(text_train)
</code></pre>

<p>I get </p>

<blockquote>
  <p>UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd8 in position
  561: invalid continuation byte</p>
</blockquote>

<p>Is this because my Tweets have all sorts of non standard text in them? I didn't do any cleanup of my Tweets (I assume there are libraries that help with that in order to make a bag of words work?)</p>

<p>EDIT:
Code I use using Twython to download tweets:</p>

<pre><code>def get_tweets(user):
    twitter = Twython(CONSUMER_KEY,CONSUMER_SECRET,ACCESS_KEY,ACCESS_SECRET)
    user_timeline = twitter.get_user_timeline(screen_name=user,count=1)
    lis = user_timeline[0]['id']
    lis = [lis]
    for i in range(0, 16): ## iterate through all tweets
    ## tweet extract method with the last list item as the max_id
        user_timeline = twitter.get_user_timeline(screen_name=user,
        count=200, include_retweets=False, max_id=lis[-1])
        for tweet in user_timeline:
            lis.append(tweet['id']) ## append tweet id's
            text = str(tweet['text']).replace(""'"", """")
            text_file = open(user, ""a"")
            text_file.write(text)
            text_file.close()
</code></pre>
",Dataset Preprocessing & Handling,decoding encoding using sklearn load file following tutorial learn machine learning text case using tweet downloaded positive negative tweet exact directory structure using trying learn sentiment analysis ipython notebook load data like try fit countvectorizer get unicodedecodeerror utf codec decode byte xd position invalid continuation byte tweet sort non standard text cleanup tweet assume library help order make bag word work edit code use using twython download tweet
How can I create a data frame from a text file,"<p>I have a text file.  Each line in the file looks like this, but has a different length:</p>

<pre><code>negative فينو اهبل ابن اهبل
positive فينو اهبل ابن اهبل
neutral فينو اهبل ابن اهبل
</code></pre>

<p>when I use <code>pandas.read_table</code> it reads it into one column, I tried converting the file into Excel or CSV, but still converts it into one column </p>

<p>I want it to be a two column data frame where the <code>negative</code> in its own column and the rest is in another column.</p>
",Dataset Preprocessing & Handling,create data frame text file text file line file look like ha different length use read one column tried converting file excel csv still convert one column want two column data frame column rest another column
What&#39;s the best way to add a specific string to all column names in a dataframe in R?,"<p>I am trying to train a data that's converted from a document term matrix to a dataframe. There are separate fields for the positive and negative comments, so I wanted to add a string to the column names to serve as a ""tag"", to differentiate the same word coming from the different fields - for example, the word hello can appear both in the positive and negative comment fields (and thus, represented as a column in my dataframe), so in my model, I want to differentiate these by making the column names positive_hello and negative_hello.</p>

<p>I am looking for a way to rename columns in such a way that a specific string will be appended to all columns in the dataframe. Say, for <code>mtcars</code>, I want to rename all of the columns to have ""_sample"" at the end, so that the column names would become <code>mpg_sample</code>, <code>cyl_sample</code>, <code>disp_sample</code> and so on, which were originally <code>mpg</code>, <code>cyl</code>, and <code>disp</code>.</p>

<p>I'm considering using <code>sapply</code>or <code>lapply</code>, but I haven't had any progress on it. Any help would be greatly appreciated.</p>
",Dataset Preprocessing & Handling,best way add specific string column name dataframe r trying train data converted document term matrix dataframe separate field positive negative comment wanted add string column name serve tag differentiate word coming different field example word hello appear positive negative comment field thus represented column dataframe model want differentiate making column name positive hello negative hello looking way rename column way specific string appended column dataframe say want rename column sample end column name would become originally considering using progress help would greatly appreciated
Load Custom NER Model Stanford CoreNLP,"<p>I have created my own NER model with Stanford's ""Stanford-NER"" software and by following <a href=""https://nlp.stanford.edu/software/crf-faq.html#a"" rel=""noreferrer"">these</a> directions. </p>

<p>I am aware that CoreNLP loads three NER models out of the box in the following order:</p>

<ol>
<li><code>edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz</code></li>
<li><code>edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz</code></li>
<li><code>edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz</code></li>
</ol>

<p>I now want to include my NER model in the list above and have the text tagged by my NER model first.</p>

<p>I have found two previous StackOverflow questions regarding this topic and they are <a href=""https://stackoverflow.com/questions/41232187/stanford-openie-using-customized-ner-model"">'Stanford OpenIE using customized NER model'</a> and <a href=""https://stackoverflow.com/questions/33905412/why-does-stanford-corenlp-ner-annotator-load-3-models-by-default?rq=1"">'Why does Stanford CoreNLP NER-annotator load 3 models by default?'</a></p>

<p>Both of these posts have good answers. The general message of the answers is that you have to edit code within a file. </p>

<p><strong>Stanford OpenIE using customized NER model</strong></p>

<p>From this post it says to edit <code>corenlpserver.sh</code> but I cannot find this file within the Stanford CoreNLP downloaded software. Can anyone point me to this file's location? </p>

<p><strong>does Stanford CoreNLP NER-annotator load 3 models by default?</strong></p>

<p>This post says that I can use the argument of <code>-ner.model</code> to specifically call which NER models to load. I added this argument to the initial server command (<code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000 -ner.model *modlefilepathhere*</code>). This did not work as the server still loaded all three models. </p>

<p>It also states that you have to change some java code though it does not specifically call out where to make the change. </p>

<p>Do I need to modify or add this code <code>props.put(""ner.model"", ""model_path1,model_path2"");</code> to a specific class file in the CoreNLP software? </p>

<p><strong>QUESTION:</strong> From my research it seems that I need to add/modify some code to call my unique NER model. These 'edits' are outlined above and this information has been pulled from other StackOverflow questions. What files specifically do I need to edit? Where exactly are these files located (i.e. edu/Stanford/nlp/...etc)?</p>

<p><strong>EDIT:</strong> My system is running on a local server and I'm using the API pycorenlp in order to open a pipeline to my local server and to make requests against it. the two critical lines of python/pycorenlp code are:</p>

<ol>
<li><code>nlp = StanfordCoreNLP('http://localhost:9000')</code></li>
<li><code>output = nlp.annotate(evalList[line], properties={'annotators': 'ner, openie','outputFormat': 'json', 'openie.triple.strict':'True', 'openie.max_entailments_per_clause':'1'})</code></li>
</ol>

<p>I do <em>NOT</em> think this will affect my ability to call my unique NER model but I wanted to present all the situational data I can in order to obtain the best possible answer.</p>
",Dataset Preprocessing & Handling,load custom ner model stanford corenlp created ner model stanford stanford ner software following direction aware corenlp load three ner model box following order want include ner model list text tagged ner model first found two previous stackoverflow question regarding topic post good answer general message answer edit code within file stanford openie using customized ner model post say edit find file within stanford corenlp downloaded software anyone point file location doe stanford corenlp ner annotator load model default post say use argument specifically call ner model load added argument initial server command work server still loaded three model also state change java code though doe specifically call make change need modify add code specific class file corenlp software question research seems need add modify code call unique ner model edits outlined information ha pulled stackoverflow question file specifically need edit exactly file located e edu stanford nlp etc edit system running local server using api pycorenlp order open pipeline local server make request two critical line python pycorenlp code think affect ability call unique ner model wanted present situational data order obtain best possible answer
Tokenization and dtMatrix in python with nltk,"<p>I have a csv file with 2 columns — sentence and label.
I want to make a document-term matrix for these sentences. I am new in Python and so far I could reach this:</p>

<pre><code>import nltk
import csv
import numpy
from nltk import sent_tokenize, word_tokenize, pos_tag
reader = csv.reader(open('my_file.csv', 'rU'), delimiter= "";"",quotechar = '""')
for line in reader:
for field in line:
    tokens = word_tokenize(field)
</code></pre>

<p>But I don't get how to take only one column for tokenization and create such matrix. </p>

<p>I have read few topics on stackoverflow regarding the same issue but in all examples I could find, csv-file contained only 1 column or they hardcoded texts. </p>

<p>I would really appreciate any answer. Thank you in advance!</p>
",Dataset Preprocessing & Handling,tokenization dtmatrix python nltk csv file column sentence label want make document term matrix sentence new python far could reach get take one column tokenization create matrix read topic stackoverflow regarding issue example could find csv file contained column hardcoded text would really appreciate answer thank advance
Could not find or load main class NERDemo,"<p>Steps to recreate issue -</p>

<ol>
<li>Clone from here <a href=""https://github.com/BhanuMittal/namedEntityRltnship"" rel=""nofollow noreferrer"">https://github.com/BhanuMittal/namedEntityRltnship</a></li>
<li>Run NERDemo class (It's under Demo) using
<code>java -cp ""stanford-ner.jar:."" NERDemo serializedClassifier nlp.txt</code></li>
</ol>

<p>I tried running above command while I was at these paths -
~/workspace/namedEntityRltnship/demo
and
~/workspace/namedEntityRltnship</p>

<p>Error is -
<code>Could not find or load main class NERDemo</code></p>

<p>Here is what is expected to happen - 
Main method of NERDemo class will take the file as input argument and apply the classifier to classify the data present in text file using Stanford classifier.</p>
",Dataset Preprocessing & Handling,could find load main class nerdemo step recreate issue clone run nerdemo class demo using tried running command wa path workspace namedentityrltnship demo workspace namedentityrltnship error expected happen main method nerdemo class take file input argument apply classifier classify data present text file using stanford classifier
Count POS Tags by column,"<p>I am trying to count all Part-Of-Speech tags in a row and sum it up.</p>

<p>By now I reached two outputs:</p>

<p>1) The/DT question/NN was/VBD ,/, what/WP are/VBP you/PRP going/VBG to/TO cut/VB ?/.</p>

<p>2) c(""DT"", ""NN"", ""VBD"", "","", ""WP"", ""VBP"", ""PRP"", ""VBG"", ""TO"", ""VB"", ""."")</p>

<p>In this particular example desirable output is:</p>

<pre><code>        DT  NN  VBD  WP  VBP  PRP   VBG   TO   VB
1 doc   1   1    1   1    1    1     1     1    1
</code></pre>

<p>But since I want to create it for the whole column in dataframe I want to see there 0 values as well in a columns, which corresponds to a POS tag which was not used in this sentence.</p>

<p>Example:</p>

<pre><code>1 doc = ""The/DT question/NN was/VBD ,/, what/WP are/VBP you/PRP going/VBG to/TO cut/VB ?/"" 

2 doc = ""Response/NN ?/.""
</code></pre>

<p>Output:</p>

<pre><code>        DT  NN  VBD  WP  VBP  PRP   VBG   TO   VB
1 doc   1   1    1   1    1    1     1     1    1
2 doc   0   1    0   0    0    0     0     0    0
</code></pre>

<p>What I did by now:</p>

<pre><code>library(stringr)
#Spliting into sentence based on carriage return

s &lt;- unlist(lapply(df$sentence, function(x) { str_split(x, ""\n"")     }))

library(NLP)
library(openNLP)

tagPOS &lt;-  function(x, ...) {
s &lt;- as.String(x)
word_token_annotator &lt;- Maxent_Word_Token_Annotator()
a2 &lt;- Annotation(1L, ""sentence"", 1L, nchar(s))
a2 &lt;- annotate(s, word_token_annotator, a2)
a3 &lt;- annotate(s, Maxent_POS_Tag_Annotator(), a2)
a3w &lt;- a3[a3$type == ""word""]
POStags &lt;- unlist(lapply(a3w$features, `[[`, ""POS""))
POStagged &lt;- paste(sprintf(""%s/%s"", s[a3w], POStags), collapse = "" "")
list(POStagged = POStagged, POStags = POStags)
}

result &lt;- lapply(s,tagPOS)
result &lt;- as.data.frame(do.call(rbind,result))
</code></pre>

<p>That's how I reached the output which was described at the beginning</p>

<p>I have tried to count occurrences like this:
    occurrences&lt;-as.data.frame (table(unlist(result$POStags)))</p>

<p>But it count occurrences through the whole dataframe. I need to create new column to existing dataframe and count occurrences in the first column.</p>

<p>Can anyone help me please? :(</p>
",Dataset Preprocessing & Handling,count po tag column trying count part speech tag row sum reached two output dt question nn wa vbd wp vbp prp going vbg cut vb c dt nn vbd wp vbp prp vbg vb particular example desirable output since want create whole column dataframe want see value well column corresponds po tag wa used sentence example output reached output wa described beginning tried count occurrence like occurrence data frame table unlist result postags count occurrence whole dataframe need create new column existing dataframe count occurrence first column anyone help please
how to read and write TermDocumentMatrix in r?,"<p>I made wordcloud using a csv file in R. I used <code>TermDocumentMatrix</code> method in the <code>tm</code> package. Here is my code:</p>

<pre><code>csvData &lt;- read.csv(""word"", encoding = ""UTF-8"", stringsAsFactors = FALSE)

Encoding(csvData$content) &lt;- ""UTF-8""
# useSejongDic() - KoNLP package
nouns &lt;- sapply(csvData$content, extractNoun, USE.NAMES = F)
#create Corpus
myCorpus &lt;- Corpus(VectorSource(nouns))

myCorpus &lt;- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus &lt;- tm_map(myCorpus, removeNumbers)
#remove StopWord 
myCorpus &lt;- tm_map(myCorpus, removeWords, myStopwords)

#create Matrix
TDM &lt;- TermDocumentMatrix(myCorpus, control = list(wordLengths=c(2,5)))

m &lt;- as.matrix(TDM)
</code></pre>

<p>This process seemed to take too much time. I think <code>extractNoun</code> is what accounts for too much time being spent. To make the code more time-efficient, I want to save the resulting TDM as a file. When I read this saved file, can I use <code>m &lt;- as.matrix(saved TDM file)</code> completely? Or, is there a better alternative?</p>
",Dataset Preprocessing & Handling,read write termdocumentmatrix r made wordcloud using csv file r used method package code process seemed take much time think account much time spent make code time efficient want save resulting tdm file read saved file use completely better alternative
Tokenize textual content using Spark SQL?,"<p>I an working on implementing a requirement to create a dictionary of words to documents using apache spark and mongodb.</p>

<p>In my scenario I have a mongo collection in which each document has some text type fields along with a field for owner of the document.</p>

<p>I want to parse the text content in collection docs and create a dictionary which maps words to the document and owner fields. Basically, the <strong>key</strong> would be a word and <strong>value</strong> would be <strong>_id</strong> and <strong>owner</strong> field. </p>

<p>The idea is to provide auto-suggestions specific to the user when he/she types in the text box in the UI based on the user's documents.
A user can create multiple documents and a word can be in multiple documents but only one user will be able to create a document.</p>

<p>I used mongo spark connector and I am able to load the collection docs into a data frame using spark sql.</p>

<p>I am not sure how to process the textual data which is in one of the dataframe columns now to extract the words.</p>

<p>Is there a way using Spark SQL to process the text content in the data frame column to extract/tokenize words and map it to <strong>_id</strong> and <strong>owner</strong> fields and write the results to another collection.</p>

<p>If not, can someone please let me know the right approach/steps on  how I can achieve it.</p>
",Dataset Preprocessing & Handling,tokenize textual content using spark sql working implementing requirement create dictionary word document using apache spark mongodb scenario mongo collection document ha text type field along field owner document want parse text content collection doc create dictionary map word document owner field basically key would word value would id owner field idea provide auto suggestion specific user type text box ui based user document user create multiple document word multiple document one user able create document used mongo spark connector able load collection doc data frame using spark sql sure process textual data one dataframe column extract word way using spark sql process text content data frame column extract tokenize word map id owner field write result another collection someone please let know right approach step achieve
High CPU usage by unknown process in multithreading,"<p>We have server with 35gb memory and Intel® Xeon(R) E5-1620 0 @ 3.60GHz × 8 CPU. I am running a multithreaded program designed with akka actors and written in scala. In the program, there are 4 actors with tasks:</p>

<p>1) Lazy reading from file with Scala's BufferedSource and iterator,</p>

<p>2) Tokenizing sentences,</p>

<p>3) Calculating single and bigram words frequency for a given window size, and putting them into a map (one map for single words [String, Int], one for tuple words[WordTuple,Int),</p>

<p>4) Merging returned hasmaps into one hashmap and when all lines read from file and write them into a file.</p>

<h1>My custom jvm settings is as follows:</h1>

<p>-Xms34g</p>

<p>-Xmx34g</p>

<p>-XX:ReservedCodeCacheSize=240m</p>

<p>-XX:+UseParallelGC</p>

<p>-XX:ParallelGCThreads=4</p>

<p>-XX:NewSize=12g</p>

<p>-XX:SoftRefLRUPolicyMSPerMB=50</p>

<p>-ea</p>

<p>-Dsun.io.useCanonCaches=false</p>

<p>-Djava.net.preferIPv4Stack=true</p>

<p>-XX:+HeapDumpOnOutOfMemoryError</p>

<p>-XX:-OmitStackTraceInFastThrow</p>

<p>-Dawt.useSystemAAFontSettings=lcd</p>

<p>-Dsun.java2d.renderer=sun.java2d.marlin.MarlinRenderingEngine</p>

<p>-verbose:gc</p>

<p>-XX:+PrintGCDetails</p>

<p>-Xloggc:gc.log</p>

<h1>My application.conf is as follows:</h1>

<pre><code>systemParameters {
  linesPerActor = 5
  windowSize = 6
  threadPoolSize = 5
}


akka.actor.deployment {

  /wordTokenizerRouter {
    router = round-robin-pool
    nr-of-instances = 5
  }

  /frequencyCalculatorRouter {
    router = round-robin-pool
    nr-of-instances = 5
  }
}
</code></pre>

<h1>The problem:</h1>

<p>I am processing a text file with size 15gb. Program starts working, after a while, say 2 hours, those tokenizing, calculating operations is almost not working, no operations can perform. The operation that takes 300 milliseconds starts taking 100000 seconds. But the cpu usage is %100 for all processors. I've tried using jvisualvm to motinor it but sampler is not working with this high cpu usage, so I could not identify which process is making cpu %100. I check gc activity from jvisualvm and most of the time it is using about %10 cpu. So, what could be the problem with my program, which process is possibly using all the cpu?</p>

<h1>Here some screenshots from jvisualvm when operations in the program is stop but cpu usage is %100:</h1>

<p><a href=""https://i.sstatic.net/yd3Lj.png"" rel=""nofollow noreferrer"">Garbage collector status screenshot</a></p>

<p><a href=""https://i.sstatic.net/nxuuG.png"" rel=""nofollow noreferrer"">Overall status screenshot</a></p>

<p>Hope I explained it clear. Thanks in advance for your answers. </p>
",Dataset Preprocessing & Handling,high cpu usage unknown process multithreading server gb memory intel xeon r e ghz cpu running multithreaded program designed akka actor written scala program actor task lazy reading file scala bufferedsource iterator tokenizing sentence calculating single bigram word frequency given window size putting map one map single word string int one tuple word wordtuple int merging returned hasmaps one hashmap line read file write file custom jvm setting follows xms g xmx g xx reservedcodecachesize xx useparallelgc xx parallelgcthreads xx newsize g xx softreflrupolicymspermb ea dsun io usecanoncaches false djava net preferipv stack true xx heapdumponoutofmemoryerror xx omitstacktraceinfastthrow dawt usesystemaafontsettings lcd dsun java renderer sun java marlin marlinrenderingengine verbose gc xx printgcdetails xloggc gc log application conf follows problem processing text file size gb program start working say hour tokenizing calculating operation almost working operation perform operation take millisecond start taking second cpu usage processor tried using jvisualvm motinor sampler working high cpu usage could identify process making cpu check gc activity jvisualvm time using cpu could problem program process possibly using cpu screenshots jvisualvm operation program stop cpu usage garbage collector status screenshot overall status screenshot hope explained clear thanks advance answer
R - read multiple tables from text files of different format,"<p>I have several text files converted from images using <a href=""https://en.wikipedia.org/wiki/Optical_character_recognition"" rel=""nofollow noreferrer"">OCR</a>. Some of the text files contains multiple tables. These files differ in number of columns, separator and the line on which data starts. Below are the sample 2 files:</p>

<p><strong><em>file1.txt</em>:</strong> contains two tables in single text file</p>

<pre><code>                                         Receipt
Date: 12/05/2015    Page: 1 
Status: Active
Location: Florida, USA

Prod ID Category ID Product Name Received Date Quantity       Price
1       201         ABC          02/01/2015    5              200   
2                                02/01/2015    1              100   
3       204         XYZ          05/02/2015    10              2000   

                                      Total    16             2300


Date: 01/02/2016    Page: 2 
Status: Complete
Location: Florida, USA

Prod ID Category ID Product Name Received Date Quantity       Price
1       202         ABC          02/01/2015    5              200   
2       203         MNO          02/01/2015    1              100   
3       204         XYZ          05/02/2015    10              2000   

                                     Total    16             2300
</code></pre>

<p><strong><em>file2.txt</em>:</strong> contains one table but in different format than above</p>

<pre><code>Receipt Date: 12/05/2015    Page: 1   Location: California, USA     Status: Complete

Prod ID  Product  Received     Sent      Quantity   Price
           Name     Date       Date        
1         ABC     02/01/2015  03/01/2015   5          200   
2         PQR     02/01/2015  03/01/2015   1          100   
3         XYZ     05/02/2015  03/02/2015   10         2000   
</code></pre>

<p>I am looking to read the files and create dataframe for each file/table. Is there any way to apply machine learning/<a href=""https://en.wikipedia.org/wiki/Neuro-linguistic_programming"" rel=""nofollow noreferrer"">NLP</a> to convert these text files into dataframe in R.</p>
",Dataset Preprocessing & Handling,r read multiple table text file different format several text file converted image using ocr text file contains multiple table file differ number column separator line data start sample file file txt contains two table single text file file txt contains one table different format looking read file create dataframe file table way apply machine learning nlp convert text file dataframe r
how to access and open files in folder automatically and check similarity with input file in python,"<p>i am making a desktop tool for plagiarism checking between documents. I use stopwords, vectorizer tf-idf etc and use cosine similarity to check similarity between two documents </p>

<pre><code>{import nltk, string
from sklearn.feature_extraction.text import TfidfVectorizer
from  nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
import nltk

userinput1  = input (""Enter file name:"")
myfile1 = open(userinput1).read()
stop_words = set(stopwords.words(""english""))
word1 = nltk.word_tokenize(myfile1)
filtration_sentence = []
for w in word1:
    word = word_tokenize(myfile1)
    filtered_sentence = [w for w in word if not w in stop_words]
    print(filtered_sentence)

userinput2  = input (""Enter file name:"")
myfile2 = open(userinput2).read()
stop_words = set(stopwords.words(""english""))
word2 = nltk.word_tokenize(myfile2)
filtration_sentence = []
for w in word2:
    word = word_tokenize(myfile2)
    filtered_sentence = [w for w in word if not w in stop_words]
    print(filtered_sentence)

stemmer = nltk.stem.porter.PorterStemmer()
remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)

def stem_tokens(tokens):
    return [stemmer.stem(item) for item in tokens]

'''remove punctuation, lowercase, stem'''
def normalize(text):
    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))
vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')

def cosine_sim(myfile1, myfile2):
    tfidf = vectorizer.fit_transform([myfile1, myfile2])
    return ((tfidf * tfidf.T).A)[0,1]
print(cosine_sim(myfile1,myfile2))}
</code></pre>

<p>but the problem is ""i have to check similarity of input file from user with the number of files in folder. i tried my best to access folder ,open files automatically but not succeed. ""anyone here who can tell me how to access folder containing files and open files one by one and compare with input file.i am using python 3.4.4 and window 7</p>
",Dataset Preprocessing & Handling,access open file folder automatically check similarity input file python making desktop tool plagiarism checking document use stopwords vectorizer tf idf etc use cosine similarity check similarity two document problem check similarity input file user number file folder tried best access folder open file automatically succeed anyone tell access folder containing file open file one one compare input file using python window
How to split text column in bigrams to score against sentiment score lexicon (AFINN)?,"<p>I have a data frame with one column as text. I am able to split the text in to words (using unnest_tokens from tidytext), and then take the mean score for all the words. However, i also want to look at the scores for the bigrams/trigrams in my text - as AFINN lexicon also has scores for bi/tri grams, and i think that can help improve the scores to some extent.</p>

<p>Below is the code I am using (data is the dataset, Content is the column with text):
Step 1:</p>

<pre><code>    review_words &lt;- data %&gt;%
    select(Source, Content) %&gt;%
    unnest_tokens(word, Content) %&gt;%
    filter(!word %in% stopwords(kind=""en""),
    str_detect(word, ""^[a-z']+$""))
</code></pre>

<p>Step 2: (AFINN LIST only has 2 columns- word, and respective score</p>

<pre><code>    reviews_sentiment &lt;- review_words %&gt;%
    inner_join(afinn_list, by = ""word"") %&gt;%
    group_by(Source) %&gt;%
    summarize(sentiment = mean(score))
</code></pre>

<p>Step 3: I join the the result back to 'data' to have an output of scores next to original text.</p>

<p>When i try to do the above steps with the following change in STEP 1:</p>

<pre><code>    review_words &lt;- data %&gt;%
    select(Source, Content) %&gt;%
    unnest_tokens(ngram, Content, token = ""ngrams"", n=2) %&gt;%
    filter(!ngram %in% stopwords(kind=""en""),
    str_detect(ngram, ""^[a-z']+$""))
</code></pre>

<p>I get an empty dataframe. What am i missing here? Just need a quick fix to score bigrams just as I scored individual words.</p>
",Dataset Preprocessing & Handling,split text column bigram score sentiment score lexicon afinn data frame one column text able split text word using unnest token tidytext take mean score word however also want look score bigram trigram text afinn lexicon also ha score bi tri gram think help improve score extent code using data dataset content column text step step afinn list ha column word respective score step join result back data output score next original text try step following change step get empty dataframe missing need quick fix score bigram scored individual word
How to convert dictionary values into a csv file?,"<p>I am an absolute beginner in Python. I am doing a textual analysis of greek plays and counting the word frequencies of each word. Because the plays are very long, I am unable to see my full set of data, it only shows the words with the lowest frequencies because there is not enough space in the Python window. I am thinking of converting it to a .csv file. My full code is below: </p>

<pre><code>#read the file as one string and spit the string into a list of separate words
input = open('Aeschylus.txt', 'r')
text = input.read()
wordlist = text.split()

#read file containing stopwords and split the string into a list of separate words
stopwords = open(""stopwords .txt"", 'r').read().split()

#remove stopwords
wordsFiltered = []

for w in wordlist:
    if w not in stopwords:
        wordsFiltered.append(w)

#create dictionary by counting no of occurences of each word in list
wordfreq = [wordsFiltered.count(x) for x in wordsFiltered]

#create word-frequency pairs and create a dictionary 
dictionary = dict(zip(wordsFiltered,wordfreq))

#sort by decreasing frequency and print
aux = [(dictionary[word], word) for word in dictionary]
aux.sort()
aux.reverse()
for y in aux: print y

import csv


with open('Aeschylus.csv', 'w') as csvfile:
    fieldnames = ['dictionary[word]', 'word']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)


    writer.writeheader()
    writer.writerow({'dictionary[word]': '1', 'word': 'inherited'})
    writer.writerow({'dictionary[word]': '1', 'word': 'inheritance'})
    writer.writerow({'dictionary[word]': '1', 'word': 'inherit'})
</code></pre>

<p>I found the code for the csv on the internet. What I'm hoping to get is the full list of data from the highest to lowest frequency. Using this code I have right now, python seems to be totally ignoring the csv part and just printing the data as if I didn't code for the csv.</p>

<p>Any idea on what I should code to see my intended result? </p>

<p>Thank you. </p>
",Dataset Preprocessing & Handling,convert dictionary value csv file absolute beginner python textual analysis greek play counting word frequency word play long unable see full set data show word lowest frequency enough space python window thinking converting csv file full code found code csv internet hoping get full list data highest lowest frequency using code right python seems totally ignoring csv part printing data code csv idea code see intended result thank
Using a list for a feature in an ML model,"<p>I want to run a machine learning algorithm on some data, so I'm exporting the data into a file first.</p>

<p>But one of my features for the text I'm classifying is a list of tags,
 and each text can have multiple tags ex. ([""mystery"", ""thriller""]).</p>

<p>Is it recommended that when I write to my CSV file for exporting the data, that I write that entire list as one of the features for my data (the ""tags"" feature).
Or is it better to make a separate feature for each tag. The only problem then is that most examples will only have one tag, so the other feature columns for those will be blank.</p>

<p>So it seems like writing this list of tags as one feature makes the most sense, but then when parsing it for training, would I then treat every element of that list as its own feature still or no?</p>
",Dataset Preprocessing & Handling,using list feature ml model want run machine learning algorithm data exporting data file first one feature text classifying list tag text multiple tag ex mystery thriller recommended write csv file exporting data write entire list one feature data tag feature better make separate feature tag problem example one tag feature column blank seems like writing list tag one feature make sense parsing training would treat every element list feature still
Omitting Words from Spellcheck in qdap,"<p>This is my first post with StackOverflow, I apologize if I violate any rules.</p>

<p>I am working with the R package <code>qdap</code> on spellchecking very messy medical record text. The goal of this work is to identify misspellings of drug side effects in order to build a side effect misspelling dictionary. The text I am working with contains many, many misspellings, abbreviations, and other things that make a simple spellcheck difficult. After I run a spellcheck on a small doctors note, I get hundreds of words returned to me by the spellcheck program. This makes it difficult to search for the side effect misspellings that I care about. </p>

<p>I attempted to use the following code to create a dictionary consisting only of correctly spelled side effects, so that <code>qdap</code> will trigger closely misspelled words as belonging to this dictionary. The problem is that with this, nearly every word in the text, properly or improperly spelled is not returned as incorrect (i.e. ""notable"" is spelled wrong and ""nausea"" is the suggested replacement from my dictionary). </p>

<pre><code>dictionary &lt;- readLines(""dictionary.txt"")
check_spelling(text$NOTE_TEXT[3379],range = 0, dictionary = dictionary, 
    assume.first.correct=FALSE)
</code></pre>

<p>Here the term ""dictionary"" is my self-built side-effects dictionary, and <code>check_spelling</code> is being run on text contained in a csv file. Is there any way to omit words that are very far away from words contained in my dictionary from appearing in the spellcheck function (such as my previous example)? This way I can cut down the number of words I am seeing in my spell_check output and identify only the misspelled side effects.   </p>

<p>As a small note, changing <code>assume.first.correct</code> to <code>TRUE</code> will not change anything, because the dictionary does not run with it set that way.</p>
",Dataset Preprocessing & Handling,omitting word spellcheck qdap first post stackoverflow apologize violate rule working r package spellchecking messy medical record text goal work identify misspelling drug side effect order build side effect misspelling dictionary text working contains many many misspelling abbreviation thing make simple spellcheck difficult run spellcheck small doctor note get hundred word returned spellcheck program make difficult search side effect misspelling care attempted use following code create dictionary consisting correctly spelled side effect trigger closely misspelled word belonging dictionary problem nearly every word text properly improperly spelled returned incorrect e notable spelled wrong nausea suggested replacement dictionary term dictionary self built side effect dictionary run text contained csv file way omit word far away word contained dictionary appearing spellcheck function previous example way cut number word seeing spell check output identify misspelled side effect small note changing change anything dictionary doe run set way
How to form documents for LDA on twitter data,"<p>We have a requirement to do topic modelling on the twitter tweets on the live stream, the input makes to spark streaming and stores the data to HDFS. A batch job runs on the collected data. The batch job is to find the underlying topics in the tweets. For this we are using Latent Dirichlet Allocation (LDA) alogrithm to find out the topics. We receive data as tweets of max characters 140 and are stored as one row in HDFS.</p>

<p>I'm new to the LDA algorithm and have basic understanding on that, as the topic model are derived based on word co-occurrences across n documents</p>

<p>I understood two options to input the data to the LDA.</p>

<p>Option 1: Use one row tweet as one single document for the LDA ?.</p>

<p>Option 2: Group the rows and form documents pass these documents to LDA ?.</p>

<p>I want to understand how the distribution of the vocabulary(words) to topic is effected for each option. Which option should be considered for better topic modelling.</p>

<p>Also please let me know if any better solution is required to do topic modelling on the twitter data other than these otpions.</p>

<p>Note: When I ran the both options and displayed on the word cloud, I could see the distribution of words to the topics(3) is different for the both.</p>

<p>Any help appreciated.</p>

<p>Thanks in advance.</p>
",Dataset Preprocessing & Handling,form document lda twitter data requirement topic modelling twitter tweet live stream input make spark streaming store data hdfs batch job run collected data batch job find underlying topic tweet using latent dirichlet allocation lda alogrithm find topic receive data tweet max character stored one row hdfs new lda algorithm basic understanding topic model derived based word co occurrence across n document understood two option input data lda option use one row tweet one single document lda option group row form document pas document lda want understand distribution vocabulary word topic effected option option considered better topic modelling also please let know better solution required topic modelling twitter data otpions note ran option displayed word cloud could see distribution word topic different help appreciated thanks advance
Bigram analysis and Term document Matrix,"<p>I am.doing a bigram analyis on my text corpus. My feature vector is a predefined set of bigram and unigram tokens.</p>

<p><strong>Feature vector</strong> = ( good location, bad experience, clean, unfriendly, tidy, excellent, beautiful place) </p>

<p><strong>my text</strong> :  location is good but unfriendly staff. </p>

<p><strong>Cleaned text :</strong> location good unfriendly staff.</p>

<p>I created a tdf using the above dictionary and cleaned text but the ""location good"" bigram is not giving a ""1"". 
But when I changed the cleaned text to  ""good location unfriendly staff"".
In a bigram analysis do the order of the words matter and why ? or am i messing up with the code ? Kindly clarify </p>

<p>""bad experience""    ""tidy""  ""clean"" ""good location"" ""excellent"" ""beautiful"" ""place"" ""unfriendly""</p>

<p>0   0   0   0   0   0   1 -- location good but  unfriendly staff.   </p>

<p>0   0   0   1   0   0   1  --  good location but  unfriendly staff.</p>
",Dataset Preprocessing & Handling,bigram analysis term document matrix bigram analyis text corpus feature vector predefined set bigram unigram token feature vector good location bad experience clean unfriendly tidy excellent beautiful place text location good unfriendly staff cleaned text location good unfriendly staff created tdf using dictionary cleaned text location good bigram giving changed cleaned text good location unfriendly staff bigram analysis order word matter messing code kindly clarify bad experience tidy clean good location excellent beautiful place unfriendly location good unfriendly staff good location unfriendly staff
Using DocumentTermMatrix on a Vector of First and Last Names,"<p>I have a column in my data frame (df) as follows:</p>

<pre><code>&gt; people = df$people
&gt; people[1:3]
[1] ""Christian Slater, Tara Reid, Stephen Dorff, Frank C. Turner""     
[2] ""Ice Cube, Nia Long, Aleisha Allen, Philip Bolden""                
[3] ""John Travolta, Uma Thurman, Vince Vaughn, Cedric the Entertainer""
</code></pre>

<p>The column has 4k+ unique first/last/nick names as a list of full names on each row as shown above. I would like to create a DocumentTermMatrix for this column where full name matches are found and only the names that occur the most are used as columns. I have tried the following code:</p>

<pre><code>&gt; people_list = strsplit(people, "", "")

&gt; corp = Corpus(VectorSource(people_list))

&gt; dtm = DocumentTermMatrix(corp, people_dict)
</code></pre>

<p>where people_dict is a list of the most commonly occurring people (~150 full names of people) from people_list as follows:</p>

<pre><code>&gt; people_dict[1:3]
[[1]]
[1] ""Christian Slater""

[[2]]
[1] ""Tara Reid""

[[3]]
[1] ""Stephen Dorff""
</code></pre>

<p>However, the DocumentTermMatrix function seems to not be using the people_dict at all because I have way more columns than in my people_dict. Also, I think that the DocumentTermMatrix function is splitting each name string into multiple strings. For example, ""Danny Devito"" becomes a column for ""Danny"" and ""Devito"".</p>

<pre><code>&gt; inspect(actors_dtm[1:5,1:10])
&lt;&lt;DocumentTermMatrix (documents: 5, terms: 10)&gt;&gt;
Non-/sparse entries: 0/50
Sparsity           : 100%
Maximal term length: 9
Weighting          : term frequency (tf)

    Terms
Docs 'g. 'jojo' 'ole' 'piolin' 'rampage' 'spank' 'stevvi' a.d. a.j. aaliyah
   1   0      0     0        0         0       0        0    0    0       0
   2   0      0     0        0         0       0        0    0    0       0
   3   0      0     0        0         0       0        0    0    0       0
   4   0      0     0        0         0       0        0    0    0       0
   5   0      0     0        0         0       0        0    0    0       0
</code></pre>

<p>I have read through all the TM documentation that I can find, and I have spent hours searching on stackoverflow for a solution. Please help!</p>
",Dataset Preprocessing & Handling,using documenttermmatrix vector first last name column data frame df follows column ha k unique first last nick name list full name row shown would like create documenttermmatrix column full name match found name occur used column tried following code people dict list commonly occurring people full name people people list follows however documenttermmatrix function seems using people dict way column people dict also think documenttermmatrix function splitting name string multiple string example danny devito becomes column danny devito read tm documentation find spent hour searching stackoverflow solution please help
Most representative document in a list of documents,"<p>Hi I am trying to find out what the most representative document in a list of documents might be. I am wondering if there are any resources or documentation on being able to do that. I have put together some simple statistics that help me do this:</p>

<ul>
<li>Removing stop words, using bigrams</li>
<li>Matrix multiply and sum of TF multiplied by DF to get a score for the document</li>
<li>Whatever document has a TF*DF Score closest to average TF * DF will be retrieved  </li>
</ul>

<p>So the idea is that the higher the DF is, the more representative it is of the corpus. If TF scoring is optimized for the average, so documents that overuse or underuse a high DF word are punished.</p>

<p>It's pretty hacky but wondering if there is something better out there that people have encountered.</p>
",Dataset Preprocessing & Handling,representative document list document hi trying find representative document list document might wondering resource documentation able put together simple statistic help removing stop word using bigram matrix multiply sum tf multiplied df get score document whatever document ha tf df score closest average tf df retrieved idea higher df representative corpus tf scoring optimized average document overuse underuse high df word punished pretty hacky wondering something better people encountered
Why am I missing the last letter in term document matrix?,"<p>I am new to R and I'm trying to create term document matrix with a csv file. But the results show that some of the words are missing the letter ""e"" in the end. How can I make the term document matrix showing the full words? It will be great if you could also let me know when you see a part that doesn't look right. Thank you!</p>

<pre><code>library(tm)
posts&lt;-read.csv(""/abcd.csv"",header=TRUE)
require(tm)
posts&lt;-Corpus(VectorSource(posts))
library(SnowballC)
Corpus&lt;-tm_map(Corpus,content_transformer(tolower))
Corpus&lt;-tm_map(Corpus,stripWhitespace)
Corpus&lt;-tm_map(Corpus,removeWords,stopwords(""english""))
Corpus&lt;-tm_map(Corpus,stemDocument)
inspect(Corpus[9])
tdm&lt;-TermDocumentMatrix(Corpus)
tdm
tdm=as.matrix(TermDocumentMatrix(Corpus,control=list(wordLengths=c(1,Inf))))
tdm
rowSums(tdm)
</code></pre>

<p>Below are some of the words I'm seeing here as the results from the file.</p>

<p>caus<br>
 downtim<br>
 failur<br>
 outag<br>
 unreachabl            </p>
",Dataset Preprocessing & Handling,missing last letter term document matrix new r trying create term document matrix csv file result show word missing letter e end make term document matrix showing full word great could also let know see part look right thank word seeing result file caus downtim failur outag unreachabl
Simple Java Sentence Classification program,"<p>I need some help with a form of sentence classification program.</p>

<p>A program that reads a files and checks each sentence in the file to locate any ‘keywords’. Keywords are in another file with words written inside it. If it finds a keyword, it writes that sentence into another file.</p>

<p>So far I am cool with the reading of the input file and splitting into sentences and writing of the output file</p>

<p>Can you please give some direction with how the program will read each sentence in the first file and compare it  against the words in the second file and if it locates a keyword in the sentence write into a third file?</p>

<p>Many thanks!</p>
",Dataset Preprocessing & Handling,simple java sentence classification program need help form sentence classification program program read file check sentence file locate keywords keywords another file word written inside find keyword writes sentence another file far cool reading input file splitting sentence writing output file please give direction program read sentence first file compare word second file locates keyword sentence write third file many thanks
Write cvs files with two columns in python,"<p>l have two column that l want to write into a csv file using python2.7.</p>

<p>the first column is a set of word  such as:</p>

<pre><code>['ee','aeoiete', 'aatriexa', 'reaaet','cnle','aeocoee','etlancoret']
</code></pre>

<p>the second column is </p>

<pre><code>['hi','ok', 'ok', 'what','are','you','now']
</code></pre>

<p>column 1 and two have the same length.
I tried two ways:</p>

<p>1)</p>

<pre><code>column1= set_of_words_1
column2= set_of_word2_2
temp_df = pd.DataFrame([[column1,column2]], columns = ['words_1', 'word2'])
temp_df.to_csv('/home/words.csv)
</code></pre>

<p>I got all the words of column1 in the same line and all the word of column2 in the same line. But I want to get each word in a new line.
in the first line of first column I got:</p>

<pre><code>'ee','aeoiete', 'aatriexa', 'reaaet','cnle','aeocoee','etlancoret'
</code></pre>

<p>rather than</p>

<pre><code>'ee',
'aeoiete',
 'aatriexa', 
'reaaet',
'cnle',
'aeocoee',
'etlancoret'
</code></pre>

<p>2) I tried also the following:</p>

<pre><code>with open('home/ahmed/internship/results.csv','w',newline='') as fp:
    a = csv.writer(fp,delimeter=',')
    data=[['words_1', 'words_2'],[column1],[column2]]
    a.writerows(data)
</code></pre>

<p>I got the following error </p>

<blockquote>
  <p>TypeError: 'newline' is an invalid keyword argument for this function</p>
</blockquote>
",Dataset Preprocessing & Handling,write cv file two column python l two column l want write csv file using python first column set word second column column two length tried two way got word column line word column line want get word new line first line first column got rather tried also following got following error typeerror newline invalid keyword argument function
apply function on each row of the dataframe and return a dataframe,"<p>I want to use jiebar to segment some Chinese sentences, these sentences are stored in a data frame.One way to do it is to convert the data frame into a list, then use apply_list. </p>

<pre><code>library(jiebar)
mixseg = worker()
c &lt;- apply_list(as.list(weibo$weibo), mixseg)
</code></pre>

<p>dataset: weibo$weibo</p>

<pre><code>[1] ""诸丽娜Kitty 打卡第天运动只为更好的自己运动就是坚持快来加入打卡社区吧""                
[2] ""今天的高尔夫球课高尔夫️教练可是全美排名前的教练哦\U0001f3cc AmericanNew Jersey Hudson""
[3] ""鼓掌鼓掌第一次滑雪居然是在美国体验在雪上的激情与速度过瘾酷 Mount Peter""              
[4] ""伪装幸福学着让自己更坚强  我正在使用假面女孩封面图好漂亮你们都快来试试封面图预览""    
[5] ""抢到啦大家元宵节快乐顺心如意鲜花鲜花 AmericanNew York Queens""                        
[6] "" Happy Chinese New Year 美国City of New York Central Park Conservancy""  
</code></pre>

<p>However, the size of the return list is 3 times larger than the original dataset if I do it in this way. Therefore, I want to use ldply/laply to return the result as a data frame, but I encounter this error</p>

<pre><code>b&lt;- ldply(weibo2, segment(mixseg))
Error in match(x, table, nomatch = 0L) : 
  argument ""jiebar"" is missing, with no default
</code></pre>
",Dataset Preprocessing & Handling,apply function row dataframe return dataframe want use jiebar segment chinese sentence sentence stored data frame one way convert data frame list use apply list dataset weibo weibo however size return list time larger original dataset way therefore want use ldply laply return result data frame encounter error
How word2vec retrieves result from binary files?,"<pre><code>from gensim.models.keyedvectors import KeyedVectors
model = KeyedVectors.load_word2vec_format('google_news.bin', binary=True)
print(model['the']) # this prints the 300D vector for the word 'the'
</code></pre>

<p>the code loads the google_news binary file to model. 
my question is, how the line 3 computes the output from a binary file ( Since Binary files contains 0's and 1's).</p>
",Dataset Preprocessing & Handling,word vec retrieves result binary file code load google news binary file model question line computes output binary file since binary file contains
Creating a custom categorized corpus in NLTK and Python,"<p>I'm experiencing a bit of a problem which has to do with regular expressions and <code>CategorizedPlaintextCorpusReader</code> in Python.</p>

<p>I want to create a custom categorized corpus and train a Naive-Bayes classifier on it. My issue is the following: I want to have two categories, ""pos"" and ""neg"". The positive files are all in one directory, <code>main_dir/pos/*.txt</code>, and the negative ones are in a separate directory, <code>main_dir/neg/*.txt</code>.</p>

<p>How can I use the <code>CategorizedPlaintextCorpusReader</code> to load and label all the positive files in the pos directory, and do the same for the negative ones?</p>

<p>NB: The setup is absolutely the same as the <code>Movie_reviews</code> corpus (<code>~nltk_data\corpora\movie_reviews</code>).</p>
",Dataset Preprocessing & Handling,creating custom categorized corpus nltk python experiencing bit problem ha regular expression python want create custom categorized corpus train naive bayes classifier issue following want two category po neg positive file one directory negative one separate directory use load label positive file po directory negative one nb setup absolutely corpus
Importing external treebank-style BLLIP corpus using NLTK,"<p>I have downloaded the <a href=""https://catalog.ldc.upenn.edu/LDC2000T43"" rel=""nofollow noreferrer"">BLLIP</a> corpus and would like to import it to NLTK. One way that I have found for doing this is described in the answer of the question
<a href=""https://stackoverflow.com/questions/30600975/how-to-read-corpus-of-parsed-sentences-using-nltk-in-python"">How to read corpus of parsed sentences using NLTK in python?</a>. In that answer they are doing it for one data file. I want to do it for a collection of them.</p>

<p>The BLLIP corpus comes as a collection of a few million files, each of which containing a couple of parsed sentences or so. The main folder that contains the data is named <code>bllip_87_89_wsj</code> and it contains 3 subfolders, <code>1987</code>, <code>1988</code>, <code>1989</code> (one for each year). In subfolder <code>1987</code> you have sub-subfolders each containing a number of files corresponding to parsed sentences. A sub-subfolder is named something like <code>w7_001</code> (for folder <code>1987</code>) and the file names are <code>w7_001.000</code>, <code>w7_001.001</code> and so on and so forth.</p>

<p>With all this at hand, my task is the following: <strong>Read all files sequentially using NLTK parsers. Then, convert the corpus to a list of lists, where each sublist is a sentence.</strong> </p>

<p>The second part is easy, its done with the command <code>corpus_name.sents()</code>. It is the first part of the task that I don't know how to approach.</p>

<p>All suggestions are welcome. I would also especially welcome suggestions that propose alternative, more efficient, approaches to the one I have in mind.</p>

<p><strong>UPDATE</strong>:</p>

<p>The parsed sentences of the BLLIP corpus are of the following form:</p>

<pre><code>(S (NP (DT the) (JJ little) (NN dog)) (VP (VBD barked)))
</code></pre>

<p>In a number of sentences there is a syntactic category of the form <code>(-NONE- *-0)</code> so when I read the corpus <code>*-0</code> is considered a word. Is there a way to ignore the syntactic category <code>-NONE-</code>. For example, if I had the sentence</p>

<pre><code>(S (NP-SBJ (-NONE- *-0))
  (VP (TO to)
   (VP (VB sell)
    (NP (NP (PRP$#0 its) (NN TV) (NN station))
     (NN advertising)
     (NN representation)
     (NN operation)
     (CC and)
     (NN program)
     (NN production)
     (NN unit))
</code></pre>

<p>I would like it to become:</p>

<p><code>to sell its TV station advertising representation operation and program production unit</code></p>

<p>and NOT</p>

<p><code>*-0 to sell its TV station advertising representation operation and program production unit</code></p>

<p>which it is currently.</p>
",Dataset Preprocessing & Handling,importing external treebank style bllip corpus using nltk downloaded bllip corpus would like import nltk one way found described answer question href read corpus parsed sentence using nltk python answer one data file want collection bllip corpus come collection million file containing couple parsed sentence main folder contains data named contains subfolders one year subfolder sub subfolders containing number file corresponding parsed sentence sub subfolder named something like folder file name forth hand task following read file sequentially using nltk parser convert corpus list list sublist sentence second part easy done command first part task know approach suggestion welcome would also especially welcome suggestion propose alternative efficient approach one mind update parsed sentence bllip corpus following form number sentence syntactic category form read corpus considered word way ignore syntactic category example sentence would like become currently
Deep learning word2vec for small text,"<p>I am using word2vec from R [here][1]</p>

<p>My data come from a csv file. Following are the data I have:</p>

<pre><code>net
abap
access
account management
accounting
active directory
agile methodologies
agile project management
ajax
algorithms
analysis
android
android development
angularjs
ant
apache
asp
asp net
banking
bb
bpmn
budgets
business analysis
business development
business intelligence
business planning
business process
business process design
business strategy
c
change management
channel partners
cisco technologies
cloud computing
cms
competitive analysis
computer hardware
computer science
consulting
contract negotiation
corporate communications
crm
css
customer service
cvs
data analysis
data center
data migration
data warehousing
database design
databases
db
design patterns
direct sales
drupal
eclipse
ecommerce
economics
editing
ejb
english
enterprise architecture
enterprise software
erp
european union
event management
finance
financial analysis
firewalls
forecasting
french
git
hardware
help desk support
hibernate
html
human resources
iis
incident management
integration
it management
it service management
it strategy
itil
java
java enterprise edition
javascript
jboss application server
jdbc
jee
jira
jms
joomla
jpa
jquery
jsf
json
jsp
junit
key account management
leadership
linux
management
management consulting
market research
marketing
marketing communications
marketing strategy
matlab
maven
microsoft excel
microsoft exchange
microsoft office
microsoft sql server
microsoft word
mobile applications
mobile devices
ms project
mysql
negotiation
netbeans
network administration
network security
networking
new business development
object oriented design
oop
operating systems
oracle
oracle applications
oracle sql
outsourcing
photoshop
php
plsql
pmo
pmp
postgresql
powerpoint
presales
problem solving
product development
product management
product marketing
program management
programming
project management
project planning
project portfolio
public relations
public speaking
python
quality assurance
requirements analysis
requirements gathering
research
rest
retail
risk management
rup
saas
sales
sales management
sales operations
sap
sap erp
sap r
scrum
security
selenium
seo
servers
servlets
sharepoint
shell scripting
soa
soap
social media
social media marketing
social networking
software design
software development
software documentation
software engineering
software installation
software project
software quality
solution architecture
solution selling
spring
spring framework
spss
sql
sql server
startups
strategic planning
strategy
struts
subversion
system administration
systems analysis
tcpip
teaching
team building
team leadership
team management
teamwork
technical support
telecommunications
testing
tomcat
training
troubleshooting
tsql
uml
unix
unix shell scripting
user acceptance testing
vb net
virtualization
visio
visual basic
visual studio
vmware
voip
vpn
web applications
web design
web development
web services
weblogic
windows
windows server
wordpress
xml
xslt
</code></pre>

<p>I would like to extract cluster of text so I could categorize the words. I use the following code from word2vec.</p>

<pre><code>library(wordVectors)
model = train_word2vec(""C:/Users/Desktop/input.csv"",output=""C:/Users/Desktop/output.vectors"",threads = 3,vectors = 100,window=12)
nearest_to(model,model[[""bussiness""]])
</code></pre>

<p>I would expect to see nearest words based on bussiness because from observation of input file I can see that there are exist but I only take NA from output of nearest_to</p>

<pre><code>&gt; nearest_to(model,model[[""bussiness""]])
&lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 
  NA   NA   NA   NA   NA   NA   NA   NA   NA   NA 
</code></pre>

<p>What can I do to fix the problem in code?
      [1]: <a href=""https://github.com/bmschmidt/wordVectors"" rel=""nofollow"">https://github.com/bmschmidt/wordVectors</a></p>
",Dataset Preprocessing & Handling,deep learning word vec small text using word vec r data come csv file following data would like extract cluster text could categorize word use following code word vec would expect see nearest word based bussiness observation input file see exist take na output nearest fix problem code
OpenNLP train Thai language,"<p>I am experimenting with OpenNlp 1.7.2 and maxent-3.0.0.jar to train for thai language , below is the code that reads thai train data and creates the bin model.</p>

<pre><code>public class TrainPerson {
public static void main(String[] args) throws IOException {
    String trainFile = ""/Documents/workspace/ThaiOpenNLP/bin/thaiPerson.train"";
    String modelFile = ""/Documents/workspace/ThaiOpenNLP/bin/th-ner-person.bin""; 
    writePersonModel(trainFile, modelFile);

}
private static void writePersonModel(String trainFile, String modelFile)
        throws FileNotFoundException, IOException {

    Charset charset = Charset.forName(""UTF-8"");
    InputStreamFactory fileInputStream = new MarkableFileInputStreamFactory(new File(trainFile));
    ObjectStream&lt;String&gt; lineStream = new PlainTextByLineStream(fileInputStream, charset);
    ObjectStream&lt;NameSample&gt; sampleStream = new NameSampleDataStream(lineStream);
    TokenNameFinderModel model;

    try {
        model = NameFinderME.train(""th"", ""person"", sampleStream , TrainingParameters.defaultParams(), new TokenNameFinderFactory());
    } finally {
        sampleStream.close();
    }
    BufferedOutputStream modelOut = null;
    try {
        modelOut = new BufferedOutputStream(new FileOutputStream(modelFile));
        model.serialize(modelOut);

    } finally {
        if (modelOut != null) {
            modelOut.close();
        }
    }
}}
</code></pre>

<p>Thai data looks like as attached in the file <a href=""https://drive.google.com/open?id=0B1SogodTE-kJb0xSQXl0SGlWRnM"" rel=""nofollow noreferrer"">trainingData</a></p>

<p>I am using the output model to detect person name as shown in the below programme. It fails to identify the name.</p>

<pre><code>public class ThaiPersonNameFinder {

static String modelFile = ""/Users/avinashpaula/Documents/workspace/ThaiOpenNLP/bin/th-ner-person.bin"";

public static void main(String[] args) {

    try {
        InputStream modelIn = new FileInputStream(new File(modelFile));
      TokenNameFinderModel model = new TokenNameFinderModel(modelIn);
      NameFinderME nameFinder = new NameFinderME(model);
      String sentence[] = new String[]{
                ""จอห์น"",
                ""30"",
                ""ปี"",
                ""จะ"",
                ""เข้าร่วม"",
                ""ก"",
                ""เริ่มต้น"",
                ""ขึ้น"",
                ""บน"",
                ""มกราคม"",
                "".""
                };

    Span nameSpans[] = nameFinder.find(sentence);
    for (int i = 0; i &lt; nameSpans.length; i++) {
        System.out.println(nameSpans[i]);
    }
    }
    catch (IOException e) {
      e.printStackTrace();
    }
}
</code></pre>

<p>}</p>

<p>What am i doing wrong.</p>
",Dataset Preprocessing & Handling,opennlp train thai language experimenting opennlp maxent jar train thai language code read thai train data creates bin model thai data look like attached file trainingdata using output model detect person name shown programme fails identify name wrong
Cleaning dataset in Python,"<p>I'm new to Python. I have a CSV-file with tweet entries formatted like this:</p>

<blockquote>
  <p>15,Oct
  11,785816454042124288,/realDonaldTrump/status/785816454042124288,False,""Despite
  winning the second debate in a landslide (every poll), it is hard to
  do well when Paul Ryan and others give zero support!"",DonaldTrump</p>
</blockquote>

<p>and another</p>

<blockquote>
  <p>16,Oct
  10,785563318652178432,/realDonaldTrump/status/785563318652178432,False,""Wow,
  @CNN got caught fixing their """"focus group"""" in order to make Crooked
  Hillary look better. Really pathetic and totally
  dishonest!"",DonaldTrump</p>
</blockquote>

<p>In Python, I load the contents using Pandas like this:</p>

<pre><code>data = pd.read_csv(arg, sep=',')
</code></pre>

<p>Now, I would like to clean the CSV-file and only save the user ID (3rd entry on each row) and the tweet itself (I think 6th row). As you see I split by using the sep=','. The problem is if some tweets contains commas, I don't want this character to be removed due to the splitting.. If only the separator between tweet number, date, user_id, and so on, would have been something other than comma, it would have been a lot easier. Any suggestions on how to do this? I just want a new CSV-file without the information that I don't need.</p>
",Dataset Preprocessing & Handling,cleaning dataset python new python csv file tweet entry formatted like oct realdonaldtrump status false despite winning second debate landslide every poll hard well paul ryan others give zero support donaldtrump another oct realdonaldtrump status false wow cnn got caught fixing focus group order make crooked hillary look better really pathetic totally dishonest donaldtrump python load content using panda like would like clean csv file save user id rd entry row tweet think th row see split using sep problem tweet contains comma want character removed due splitting separator tweet number date user id would something comma would lot easier suggestion want new csv file without information need
How to determine the number of topics in the LDA (Latent Dirichlet Allocation) alogrithm for text clustering?,"<p>I am using the LDA algorithm to cluster many documents into different topics. The LDA algorithm needs an input parameter: the number of topics. How could I determine this?</p>

<p>I am using the Reuter corpora to benchmark my solution. And Reuter corpora has topic numbers ready. Should I input the the same topic number when I clustering Reuter text? And comparing my clustering result to Reuter's?</p>

<p>But when in production, how could I know the number of topics before I actually cluster based on the topics. It's kind of like a chicken-egg problem.</p>
",Dataset Preprocessing & Handling,determine number topic lda latent dirichlet allocation alogrithm text clustering using lda algorithm cluster many document different topic lda algorithm need input parameter number topic could determine using reuter corpus benchmark solution reuter corpus ha topic number ready input topic number clustering reuter text comparing clustering result reuter production could know number topic actually cluster based topic kind like chicken egg problem
How to define target in multilayer perceptron?,"<p>I want to do a word sense disambiguation project with multi layer perceptron in Persian language. I have some files, each for an ambiguated word. Some words have 2 senses, some 3 and some more. The files contain a lot of sentences and in front of each sentence I put a tab and then the proper sense of the ambiguated word. Then I extracted 1000 most frequent words of the file and made a binary vector (term document). (after omitting stop words and punctuation) If each word in each line is among the 1000 most frequent words, it puts 1 in the vector, unless it puts 0. So for example if a file has 500 sentence each in a line, the result is 500 lines of 1 and 0 and the columns count, is 1000 and a column for the tag of the sense. These are the input of the multi layer perceptron. Now the question is, 1) what is the target? How should I define it and what value it takes? 2) how many neurons does the output should have? 3) Is this XOR problem or And problem? (the number of hidden layers are 2, momentum is 0.02, and training iteration is 500)</p>
",Dataset Preprocessing & Handling,define target multilayer perceptron want word sense disambiguation project multi layer perceptron persian language file ambiguated word word sens file contain lot sentence front sentence put tab proper sense ambiguated word extracted frequent word file made binary vector term document omitting stop word punctuation word line among frequent word put vector unless put example file ha sentence line result line column count column tag sense input multi layer perceptron question target define value take many neuron doe output xor problem problem number hidden layer momentum training iteration
stri_replace_all_fixed slow on big data set - is there an alternative?,"<p>I'm trying to stem ~4000 documents in R, by using the <b>stri_replace_all_fixed</b> function. However, it is VERY slow, since my dictionary of stemmed words consists of approx. 300k words. I am doing this because the documents are in danish and therefore the Porter Stemmer Algortihm is not useful (it is too aggressive). </p>

<p>I have posted the code below. Does anyone know an alternative for doing this?</p>

<p>Logic: Look at each word in each document -> If word = word from voc-table, then replace with tran-word. </p>

<pre><code>##Read in the dictionary
 voc &lt;- read.table(""danish.csv"", header = TRUE, sep="";"")
#Using the library 'stringi' to make the stemming
 library(stringi)
#Split the voc corpus and put the word and stem column into different corpus
 word &lt;- Corpus(VectorSource(voc))[1]
 tran &lt;- Corpus(VectorSource(voc))[2]
#Using stri_replace_all_fixed to stem words
## !! NOTE THAT THE FOLLOWING STEP MIGHT TAKE A FEW MINUTES DEPENDING ON THE SIZE !! ##
 docs &lt;- tm_map(docs, function(x) stri_replace_all_fixed(x, word, tran, vectorize_all = FALSE))
</code></pre>

<p>Structure of ""voc"" data frame:</p>

<pre><code>       Word           Stem
1      abandonnere    abandonner
2      abandonnerede  abandonner
3      abandonnerende abandonner
...
313273 åsyns          åsyn
</code></pre>
",Dataset Preprocessing & Handling,stri replace fixed slow big data set alternative trying stem document r using stri replace fixed function however slow since dictionary stemmed word consists approx k word document danish therefore porter stemmer algortihm useful aggressive posted code doe anyone know alternative logic look word document word word voc table replace tran word structure voc data frame
interpretation of SVD for text mining topic analysis,"<p><strong>Background</strong></p>

<p>I'm learning about text mining by building my own text mining toolkit from scratch - the best way to learn!</p>

<p><strong>SVD</strong></p>

<p>The Singular Value Decomposition is often cited as a good way to:</p>

<ul>
<li>Visualise high dimensional data (word-document matrix) in 2d/3d</li>
<li>Extract key topics by reducing dimensions</li>
</ul>

<p>I've spent about a month learning about the SVD .. I must admit much of the online tutorials, papers, university lecture slides, .. and even proper printed textbooks are not that easy to digest.</p>

<p>Here's my understanding so far: <a href=""http://makeyourowntextminingtoolkit.blogspot.co.uk/2017/02/singular-value-decomposition-demystified.html"" rel=""nofollow noreferrer"">SVD demystified (blog)</a></p>

<p>I think I have understood the following:</p>

<ul>
<li>Any (real) matrix can be decomposed uniquely into 3 multiplied
matrices using SVD, A=U⋅S⋅V^T</li>
<li>S is a diagonal matrix of singular values, in descending order of magnitude</li>
<li>U and V^T are matrices of orthonormal vectors</li>
</ul>

<p>I understand that we can reduce the dimensions by filtering out less significant information by zero-ing the smaller elements of S, and reconstructing the original data. If I wanted to reduce dimensions to 2, I'd only keep the 2 top-left-most elements of the diagonal S to form a new matrix S'</p>

<p><strong>My Problem</strong></p>

<p>To see the documents projected onto the reduced dimension space, I've seen people use S'⋅V^T. Why? What's the interpretation of S'⋅V^T?</p>

<p>Similarly, to see the topics, I've seen people use U⋅S'. Why? What's the interpretation of this?</p>

<p>My limited school maths tells me I should look at these as transformations (rotation, scale) ... but that doesn't help clarify it either.</p>

<p>** Update ** 
I've added an update to my blog explanation at <a href=""http://makeyourowntextminingtoolkit.blogspot.co.uk/2017/02/singular-value-decomposition-demystified.html"" rel=""nofollow noreferrer"">SVD demystified (blog)</a> which reflects the rationale from one of the textbooks I looked at to explain why S'.V^T is a document view, and why U.S' is a word view. Still not really convinced ...</p>
",Dataset Preprocessing & Handling,interpretation svd text mining topic analysis background learning text mining building text mining toolkit scratch best way learn svd singular value decomposition often cited good way visualise high dimensional data word document matrix extract key topic reducing dimension spent month learning svd must admit much online tutorial paper university lecture slide even proper printed textbook easy digest understanding far svd demystified blog think understood following real matrix decomposed multiplied matrix using svd u v diagonal matrix singular value descending order magnitude u v matrix orthonormal vector understand reduce dimension filtering le significant information zero ing smaller element reconstructing original data wanted reduce dimension keep top left element diagonal form new matrix problem see document projected onto reduced dimension space seen people use v interpretation v similarly see topic seen people use u interpretation limited school math tell look transformation rotation scale help clarify either update added update blog explanation svd demystified blog reflects rationale one textbook looked explain v document view u word view still really convinced
How do I build a chatbot interface to a database in Python?,"<p>Database queries can be highly technical. I wish to build a Python chatbot to query a database/csv data-frame. I do not wish to use any ready API services as the data could be proprietary.</p>

<p>Sample example: with internal HR bot employees can ask various queries about their own records, leave balances etc.</p>

<p>Not sure how AIML based pure python chatbot can do this? For a question/category(AIML) on leave-balance, how will it fire a database/data-frame query and put the answer back as response/template(AIML).</p>

<p>Any open-source examples with or without AIML?</p>
",Dataset Preprocessing & Handling,build chatbot interface database python database query highly technical wish build python chatbot query database csv data frame wish use ready api service data could proprietary sample example internal hr bot employee ask various query record leave balance etc sure aiml based pure python chatbot question category aiml leave balance fire database data frame query put answer back response template aiml open source example without aiml
Log Cleaning in R,"<p>this is the structure of a log template which I have loaded in R. How do I clean it to make a dataframe?</p>

<pre><code>{""ask"":{""Id"":001,""TS"":10012001,""Response"":""12""}}
{""ask"":{""Id"":002,""TS"":11012001,""Response"":""10""}}
</code></pre>

<p>The expected output is should be individual columns with their values in a data frame for further analysis.</p>
",Dataset Preprocessing & Handling,log cleaning r structure log template loaded r clean make dataframe expected output individual column value data frame analysis
sklearn pipeline with transformed fitted with different set,"<p>I'm building a text classifier (13 categories) with sklearn and I'm trying to implement a pipeline in order to make a CV grid search for the classifier and vectorizer paramters (min_df, max_df, etc. ).</p>

<p>The classifier is going to be used in a set of already available 50k documents and I have for training around 5k annotated near similar documents.</p>

<p>Since the annotated training documents are not exactly from the same source I don't want to train the classifier using features extract from the annotated corpus but from the main not annotated corpus. Therefore, I first do a TfidfVectorizer fit in the main non annotated documents:</p>

<pre><code>vectorizer = TfidfVectorizer(ngram_range=(1,2),min_df = 0.01,max_df = 0.95,stop_words = None,use_idf=True,smooth_idf = True)
vectorizer.fit(non_annotated_docs)
</code></pre>

<p>and then, from this learned vocabulary, I calculate the features that will be used as input to the classifier:</p>

<pre><code>X_tfidf = vectorizer.transform(annotated_docs)
X_tfidf = X_tfidf.toarray()
</code></pre>

<p>So my question is how can I do the pipeline with gridsearchcsv for the annotated_docs corpus, but using the fit from non_annotated_docs?</p>
",Dataset Preprocessing & Handling,sklearn pipeline transformed fitted different set building text classifier category sklearn trying implement pipeline order make cv grid search classifier vectorizer paramters min df max df etc classifier going used set already available k document training around k annotated near similar document since annotated training document exactly source want train classifier using feature extract annotated corpus main annotated corpus therefore first tfidfvectorizer fit main non annotated document learned vocabulary calculate feature used input classifier question pipeline gridsearchcsv annotated doc corpus using fit non annotated doc
SQL queries to their natural language description,"<p>Are there any open source tools that can generate a natural language description of a given SQL query? If not, some general pointers would be appreciated.</p>

<p>I don't know much about NLP, so I am not sure how difficult this is, although I saw from some previous discussion that the vice versa conversion is still an active area of research. It might help to say that the SQL tables I will be handling are not arbitrary in any sense, yet mine, which means that I know exact semantics of each table and its columns.</p>
",Dataset Preprocessing & Handling,sql query natural language description open source tool generate natural language description given sql query general pointer would appreciated know much nlp sure difficult although saw previous discussion vice versa conversion still active area research might help say sql table handling arbitrary sense yet mine mean know exact semantics table column
How to read the cedict (a space separated file) with regex groups?,"<p><a href=""https://www.mdbg.net/chindict/chindict.php?page=cc-cedict"" rel=""nofollow noreferrer"">CEDICT</a> is a resource for Chinese text analysis</p>

<p>The file plaintext file looks like this:</p>

<pre><code># CC-CEDICT
# Community maintained free Chinese-English dictionary.
# 
# Published by MDBG
% % [pa1] /percent (Tw)/
21三體綜合症 21三体综合症 [er4 shi2 yi1 san1 ti3 zong1 he2 zheng4] /trisomy/Down's syndrome/
3C 3C [san1 C] /abbr. for computers, communications, and consumer electronics/China Compulsory Certificate (CCC)/
3P 3P [san1 P] /(slang) threesome/
A A [A] /(slang) (Tw) to steal/
</code></pre>

<p>There are 4 columns to the files and they are separated by spaces. Any spaces after the 4th is considered as one. Lines that starts with <code>#</code> needs to be skipped. </p>

<p>E.g. for the line: </p>

<blockquote>
  <p>3C 3C [san1 C] /abbr. for computers, communications, and consumer electronics/China Compulsory Certificate (CCC)/</p>
</blockquote>

<p>The content in the columns would be </p>

<ul>
<li>3C</li>
<li>3C</li>
<li>[san1 C]</li>
<li>/abbr. for computers, communications, and consumer electronics/China Compulsory Certificate (CCC)/</li>
</ul>

<p>Currently to read the file I've been tried using a mix of <code>str.split</code> and <code>re.findall</code> and skipping lines by <code>str.startswith()</code>, i.e.:</p>

<pre><code>import re
from collections import namedtuple


DictEntry = namedtuple('Dictionary', 'traditional simplified pinyin glosses')

dictfile = 'cedict_ts.u8'
cedict = {}

with open(dictfile, 'r', encoding='utf8') as fin:
    for line in fin:
        if line.startswith('#'):
            continue
        # Note: lines are NOT separated by tabs.
        line = line.strip()
        trad, sim, *stuff = line.split()
        pinyin = re.findall(r'\[([^]]*)\]',line)[0]
        glosses = re.findall(r'\/.*\/', line)[0].strip('/').split('/')
        entry = DictEntry(traditional=trad, simplified=sim, pinyin=pinyin, glosses=glosses)
        cedict[sim] = entry
</code></pre>

<p>It looks like the str and regex operations can be simiplified into a single regex and the columns can be extracted using groups. <strong>How to read the cedict (a space separated file) with regex groups?</strong></p>

<hr>

<p>I've also tried this regex with 4 groups:</p>

<pre><code>(.*)\s(.*)\s(\[([^]]*)\])\s(\/.*\/)
</code></pre>

<p>But somehow the first <code>(.*)\s</code> is greedy and it captures the whole line: <a href=""https://regex101.com/r/1c0O0E/1"" rel=""nofollow noreferrer"">https://regex101.com/r/1c0O0E/1</a></p>

<hr>

<p>I've tried this:</p>

<pre><code>.+\s(\[([^]]*)\])\s(\/.*\/)
</code></pre>

<p>And the first <code>.+\s</code> captures till it sees <code>[</code>. But that means that I'll have to use <code>str.split()</code> to get the first 2 columns. </p>
",Dataset Preprocessing & Handling,read cedict space separated file regex group cedict resource chinese text analysis file plaintext file look like column file separated space space th considered one line start need skipped e g line c c san c abbr computer communication consumer electronics china compulsory certificate ccc content column would c c san c abbr computer communication consumer electronics china compulsory certificate ccc currently read file tried using mix skipping line e look like str regex operation simiplified single regex column extracted using group read cedict space separated file regex group also tried regex group somehow first greedy capture whole line tried first capture till see mean use get first column
Reading tables and images from PDF using any NLP tools,"<p>In one of my NLP assignments I have to read PDF files and extract information out of them. Using Java I am able to read the textual content from PDF and able to apply our NLP algorithms on the text, but I also need to extract information present in Tables in PDF, I am trying to read them but not able to get them in proper format. Any idea how I can read tables from PDF document , or any hint if any library is available in OpenNLP, GATE, Stanford NLP for achieving these. </p>
",Dataset Preprocessing & Handling,reading table image pdf using nlp tool one nlp assignment read pdf file extract information using java able read textual content pdf able apply nlp algorithm text also need extract information present table pdf trying read able get proper format idea read table pdf document hint library available opennlp gate stanford nlp achieving
nlp- Difference between Sentences and a Document in Stanford OpenNLP?,"<p>Let us say we have an article that we want to annotate. If we input the text as one really long Sentence as opposed to a Document, does Stanford do anything differently between annotating that one long Sentence as opposed to looping through every Sentence in the Document and culminating all of its results together? </p>

<p>EDIT: I ran a test and it seems like the two approaches return two different NER sets. I might be just doing it wrong, but it's certainly super interesting and I'm curious as to why this happens.</p>
",Dataset Preprocessing & Handling,nlp difference sentence document stanford opennlp let u say article want annotate input text one really long sentence opposed document doe stanford anything differently annotating one long sentence opposed looping every sentence document culminating result together edit ran test seems like two approach return two different ner set might wrong certainly super interesting curious happens
NLP: Classification giving wrong result. How to find out that the result from NLP Classification is wrong?,"<p>I have started learning Natural Language Processing and have already started stumbling.</p>

<p>I am using <code>NodeJs</code> for creating my application with the help of <code>NaturalNode library</code>
<a href=""https://github.com/NaturalNode/natural"" rel=""nofollow noreferrer"">Natural Node GitHub project</a></p>

<p><strong>Problem</strong></p>

<p>I am training my document with several scenarios as shown below</p>

<pre><code>/// importing package
var natural = require('natural');
var classifier = new natural.BayesClassifier();



/// traning document
classifier.addDocument(""h"", ""greetings"");
classifier.addDocument(""hi"", ""greetings"");
classifier.addDocument(""hello"", ""greetings"");
classifier.addDocument(""data not working"", ""internet_problem"");
classifier.addDocument(""browser not working"", ""internet_problem"");
classifier.addDocument(""google not working"", ""internet_problem"");
classifier.addDocument(""facebook not working"", ""internet_problem"");
classifier.addDocument(""internet not working"", ""internet_problem"");
classifier.addDocument(""websites not opening"", ""internet_problem"");
classifier.addDocument(""apps not working"", ""internet_problem"");
classifier.addDocument(""call drops"", ""voice_problem"");
classifier.addDocument(""voice not clear"", ""voice_problem"");
classifier.addDocument(""call not connecting"", ""voice_problem"");
classifier.addDocument(""calls not going through"", ""voice_problem"");
classifier.addDocument(""disturbance"", ""voice_problem"");
classifier.addDocument(""bye"", ""close"");
classifier.addDocument(""thank you"", ""feedback_positive"");
classifier.addDocument(""thanks"", ""voice_problem"");
classifier.addDocument(""shit"", ""feedback_negeive"");
classifier.addDocument(""shit"", ""feedback_negeive"");
classifier.addDocument(""useless"", ""feedback_negetive"");
classifier.addDocument(""siebel testing"", ""siebel_testing"")


classifier.train();


/// running classification
console.log('result for hi');
console.log(classifier.classify('hi'));
console.log('result for hii');
console.log(classifier.classify('hii'));
console.log('result for h');
console.log(classifier.classify('h'));
</code></pre>

<blockquote>
  <p>Output</p>

<pre><code>result for hi:
greetings


result for hii:
internet_problem

result for h:
internet_problem
</code></pre>
</blockquote>

<p>As you can see in the result of the key work <code>hi</code> the value is coming correct but if I misspelled <code>hi</code> for <code>hii</code> or <code>ih</code> then it's giving a wrong result. I am not able to understand how does classification works and how should I train the classifier or is there a way to find out that the result of classification is wrong so that I can request an user to input again.</p>

<p>Any help or explanation or anything is highly appreciated. Many thanks in advance.</p>

<p>Please consider me as a noob and forgive for any mistake.</p>
",Dataset Preprocessing & Handling,nlp classification giving wrong result find result nlp classification wrong started learning natural language processing already started stumbling using creating application help natural node github project problem training document several scenario shown output see result key work value coming correct misspelled giving wrong result able understand doe classification work train classifier way find result classification wrong request user input help explanation anything highly appreciated many thanks advance please consider noob forgive mistake
get words before and after a specific word in text files,"<p>I have a folder containing some other folders and each of them contains a lot of text files, about 32214 files. I want to print 5 words before and after a specific word and my code should read all of these files.The code below works but it takes about 8 hours to read all of the files and extracts sentences. How can I change the code so that it reads and prints the sentences just in a few minutes? (The language is Persian)</p>

<pre><code>.
.
.
def extact_sentence ():
    f= open (""پاکت"", ""w"", encoding = ""utf-8"")
    y = ""پاکت""
    text= normal_text(folder_path) # the first function to normalize the files
    for i in text:
        for line in i:
            split_line = line.split()
            if y in split_line:
                index = split_line.index(y)
                d = (' '.join(split_line[max(0,index-5):min(index+6,len(split_line))]))
                f.write(d + ""\n"")
    f.close()
</code></pre>

<p><a href=""https://i.sstatic.net/UqQkQ.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",Dataset Preprocessing & Handling,get word specific word text file folder containing folder contains lot text file file want print word specific word code read file code work take hour read file extract sentence change code read print sentence minute language persian enter image description
How to determine if a piece of text mentions a product,"<p>I'm new to natural language process so I apologize if my question is unclear. I have read a book or two on the subject and done general research of various libraries to figure out how i should be doing this, but I'm not confident yet that know what to do.</p>

<p>I'm playing with an idea for an application and part of it is trying to find product mentions in unstructured text (e.g. tweets, facebook posts, emails, websites, etc.) in real-time. I wont go into what the products are but it can be assumed that they are known (stored in a file or database). Some examples:</p>

<ul>
<li>""starting tomorrow, we have 5 boxes of @hersheys snickers available for $5 each - limit 1 pp"" (snickers is the product from the hershey company [mentioned as ""@hersheys""])</li>
<li>""Big news: 12-oz. bottles of Coke and Pepsi on sale starting Fri."" (coca-cola is the product [aliased as ""coke""] from coca-cola company and Pepsi is the product from the PepsiCo company)</li>
<li>""#OMG, i just bought my dream car. a mustang!!!!"" (mustang is the product from Ford)</li>
</ul>

<p>So basically, given a piece of text, query the text to see if it mentions a product and receive some indication (boolean or confidence number) that it does mention the product.</p>

<p>Some concerns I have are:</p>

<ul>
<li>Missing products because of misspellings. I thought maybe i could use a string similarity check to catch these.</li>
<li>Product names that are also English words or things would get caught. Like mustang the horse versus mustang the car</li>
<li>Needing to keep a list of alternative names for products (e.g. ""coke"" for ""coco-cola"", etc.)</li>
</ul>

<p>I don't really know where to start with this but any help would be appreciated. I've already looked at NLTK and SciKit and didn't really gleam how to do this from there. If you know of examples or papers that explain this, links would be helpful. I'm not specific to any language at this point. Java preferably but Python and Scala are acceptable.</p>
",Dataset Preprocessing & Handling,determine piece text mention product new natural language process apologize question unclear read book two subject done general research various library figure confident yet know playing idea application part trying find product mention unstructured text e g tweet facebook post email website etc real time wont go product assumed known stored file database example starting tomorrow box hershey snicker available limit pp snicker product hershey company mentioned hershey big news bottle coke pepsi sale starting fri coca cola product aliased coke coca cola company pepsi product pepsico company omg bought dream car mustang mustang product ford basically given piece text query text see mention product receive indication boolean confidence number doe mention product concern missing product misspelling thought maybe could use string similarity check catch product name also english word thing would get caught like mustang horse versus mustang car needing keep list alternative name product e g coke coco cola etc really know start help would appreciated already looked nltk scikit really gleam know example paper explain link would helpful specific language point java preferably python scala acceptable
Why does the notion of Paragraph Vector make sense?,"<p>In paragraph vector modeling, they refer paragraph as a memory information, together with context words to predict the target word. I can't see why a paragraph will be useful information to predict the target word.
Should the paragraph include the target word?
<a href=""https://i.sstatic.net/ZqwFo.png"" rel=""nofollow noreferrer"">1</a></p>

<p>Can anyone give me examples of how to do it? What's D here? Is the paragraph ID also a one hot paragraph vector?</p>

<p>For example , I have  paragraph A, B, C and word a,b,c,d,e,f,g.
Paragraph B is the sequence of abcdefg.
The document is A+B +C
If I want to train this document and I want to predict word d.
What's the input paragraph here?
I know the word input should be hot word vector of a,b,c,e,f,g,if the window size is 7.</p>
",Dataset Preprocessing & Handling,doe notion paragraph vector make sense paragraph vector modeling refer paragraph memory information together context word predict target word see paragraph useful information predict target word paragraph include target word anyone give example paragraph id also one hot paragraph vector example paragraph b c word b c e f g paragraph b sequence abcdefg document b c want train document want predict word input paragraph know word input hot word vector b c e f g window size
Gensim word2vec on predefined dictionary and word-indices data,"<p>I need to train a word2vec representation on tweets using gensim. Unlike most tutorials and code I've seen on gensim my data is not raw, but has already been preprocessed. I have a dictionary in a text document containing 65k words (incl. an ""unknown"" token and a EOL token) and the tweets are saved as a numpy matrix with indices into this dictionary. A simple example of the data format can be seen below:</p>

<p><strong>dict.txt</strong></p>

<pre><code>you
love
this
code
</code></pre>

<p><strong>tweets (5 is unknown and 6 is EOL)</strong></p>

<pre><code>[[0, 1, 2, 3, 6],
 [3, 5, 5, 1, 6],
 [0, 1, 3, 6, 6]]
</code></pre>

<p>I'm unsure how I should handle the indices representation. An easy way is just to convert the list of indices to a list of strings (i.e. [0, 1, 2, 3, 6] -> ['0', '1', '2', '3', '6']) as I read it into the word2vec model. However, this must be inefficient as gensim then will try to look up the internal index used for e.g. '2'.</p>

<p>How do I load this data and create the word2vec representation in an efficient manner using gensim? </p>
",Dataset Preprocessing & Handling,gensim word vec predefined dictionary word index data need train word vec representation tweet using gensim unlike tutorial code seen gensim data raw ha already preprocessed dictionary text document containing k word incl unknown token eol token tweet saved numpy matrix index dictionary simple example data format seen dict txt tweet unknown eol unsure handle index representation easy way convert list index list string e read word vec model however must inefficient gensim try look internal index used e g load data create word vec representation efficient manner using gensim
efficient way to calculate distance between combinations of pandas frame columns,"<p><strong>Task</strong></p>

<p>I have a pandas dataframe where:</p>

<ul>
<li>the columns are document names</li>
<li>the rows are words in those documents</li>
<li>numbers inside the frame cells are a measure of word relevance (word count if you want to keep it simple)</li>
</ul>

<p>I need to calculate a new matrix of doc1-doc similarity where:</p>

<ul>
<li>rows and columns are document names</li>
<li>the cells inside the frame are a similarity measure, (1 - cosine distance) between the two documents</li>
</ul>

<p>The cosine distance is conveniently provided by <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html"" rel=""nofollow noreferrer"">script.spatial.distance.cosine</a>.</p>

<p>I'm currently doing this:</p>

<ol>
<li>use itertools to create a list of all 2-combinations of the document names (dataframe columns names)</li>
<li>loop over these and create a update a dictionary of {doc1: {doc2: similarity}} </li>
<li>after the loop, create a new frame using pandas.DataFrame(dict)</li>
</ol>

<p><strong>Problem</strong></p>

<p>But it takes a very very long time. The following shows current speed on a MacBook Pro 13 with 16GB ram and 2.9GHz i5cpu running latest anaconda python 3.5 ... plotting time taken against combinations of docs.</p>

<p><a href=""https://i.sstatic.net/0EZkE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0EZkE.png"" alt=""distance calculation performance""></a></p>

<p>You can see that 100,000 combinations takes 1200 seconds. Extrapolating that to my corpus of <strong>7944</strong> documents, which creates 3<strong>1,549,596</strong> combinations, would take <strong>5 days</strong> to calculate this similarity matrix!</p>

<p><strong>Any ideas?</strong></p>

<ul>
<li>I <a href=""http://makeyourowntextminingtoolkit.blogspot.co.uk/2016/11/more-performance-memory-stuff-to-fix.html"" rel=""nofollow noreferrer"">previously</a> was dynamically creating the dataframe df.ix[doc1,doc2]
= similarity .. which was very very much slower.</li>
<li>I've considered numba @git but it fails with pandas data structures.</li>
<li>I can't find a built in function which will do all the work internally (in C?)</li>
<li>What I have to do tactically is to randomly sample the documents to create a much smaller set to work with ... currently a fraction of 0.02 leads to about 20 minutes of calculation!</li>
</ul>

<p><strong>Here's the code (<a href=""https://github.com/makeyourowntextminingtoolkit/makeyourowntextminingtoolkit/tree/master/text_mining_toolkit"" rel=""nofollow noreferrer"">github</a>)</strong></p>

<pre><code>docs_combinations = itertools.combinations(docs_sample, 2)
for doc1, doc2 in docs_combinations:
    # scipy cosine similarity function includes normalising the vectors but is a distance .. so we need to take it from 1.0
    doc_similarity_dict[doc2].update({doc1: 1.0 - scipy.spatial.distance.cosine(relevance_index[doc1],relevance_index[doc2])})
    pass

#convert dict to pandas dataframe
doc_similarity_matrix = pandas.DataFrame(doc_similarity_dict)
</code></pre>

<p><strong>Simple Example</strong></p>

<p>@MaxU asked for an illustrative example.</p>

<p>Relevance matrix (wordcount here, just to keep it simple):</p>

<pre><code>...     doc1 doc2 doc3
wheel   2.   3.   0.
seat    2.   2.   0.
lights  0.   1.   1.
cake    0.   0.   5.
</code></pre>

<p>calculated similarity matrix based on 2-combinations (doc1, doc2), (doc2, doc3), (doc1, doc3)</p>

<pre><code>...     doc2 doc3
doc1    0.9449  0.
doc2    -       0.052
</code></pre>

<p>Take that top left value 0.889 .. thats the dot product (2*3 + 2*2 + 0 + 0) = 10 but normalised by the lengths of the vectors ... so divide by sqrt(8) and sqrt(14) gives 0.9449. You can see that there is no similarity between doc1 and doc3 .. the dot product is zero.</p>

<p>Scale this from 3 documents with 4 words ... to <strong>7944</strong> documents, which creates 3<strong>1,549,596</strong> combinations ...</p>
",Dataset Preprocessing & Handling,efficient way calculate distance combination panda frame column task panda dataframe column document name row word document number inside frame cell measure word relevance word count want keep simple need calculate new matrix doc doc similarity row column document name cell inside frame similarity measure cosine distance two document cosine distance conveniently provided script spatial distance cosine currently use itertools create list combination document name dataframe column name loop create update dictionary doc doc similarity loop create new frame using panda dataframe dict problem take long time following show current speed macbook pro gb ram ghz cpu running latest anaconda python plotting time taken combination doc see combination take second extrapolating corpus document creates combination would take day calculate similarity matrix idea previously wa dynamically creating dataframe df ix doc doc similarity wa much slower considered numba git fails panda data structure find built function work internally c tactically randomly sample document create much smaller set work currently fraction lead minute calculation code github simple example maxu asked illustrative example relevance matrix wordcount keep simple calculated similarity matrix based combination doc doc doc doc doc doc take top left value thats dot product normalised length vector divide sqrt sqrt give see similarity doc doc dot product zero scale document word document creates combination
Transform Matrix Market matrix into pandas Data frame python,"<p>I have a Market Matrix file, which I have to use for carrying out text analyses.</p>

<p>The market file has the following structure:</p>

<pre><code>%%MatrixMarket matrix coordinate integer general
2000 5000 23000
1 4300 1
1 2200 1
1 3000 1
1 600  1
</code></pre>

<p>The  values in the second lines indicate  the  number  of  rows,  number  of  columns,  and  total number of non-zero values in the matrix. All lines after this contain 3 values: </p>

<ul>
<li>the row (indexed from 1), which represents my text document;</li>
<li>the column (index from 1), which represents a word;</li>
<li>the term frequency.</li>
</ul>

<p>As read in many posts I read this file, using scipy.io.mmread and the new API for dealing with parse data structure.</p>

<p>In particular, I used the following code:</p>

<pre><code>    Matrix = (mmread('file_name.mtx'))
    B = Matrix.todense()
    df = pd.DataFrame(B)
    print(df.head())
</code></pre>

<p>However, from this code I got a data frame indexed from 0:</p>

<pre><code>        0     1     2     3     4     5     6     7     8     9     ...   4872  \
0     1     0     1     0     0     0     0     0     1     0  ...      0   
1     0     0     0     0     0     0     0     0     0     0  ...      0   
2     0     0     0     0     0     0     0     0     0     0  ...      0   
3     1     0     1     0     0     0     0     0     1     0  ...      0   
4     0     0     1     0     0     0     0     0     0     0  ...      0  
</code></pre>

<p>The ideal results will be to preserve the format of the original market matrix with row and columns indexed from 1.</p>

<p>Any ideas how to correct my code?</p>

<p>Thanks!</p>
",Dataset Preprocessing & Handling,transform matrix market matrix panda data frame python market matrix file use carrying text analysis market file ha following structure value second line indicate number row number column total number non zero value matrix line contain value row indexed represents text document column index represents word term frequency read many post read file using scipy io mmread new api dealing parse data structure particular used following code however code got data frame indexed ideal result preserve format original market matrix row column indexed idea correct code thanks
How to create a corpus for sentiment analysis in NLTK?,"<p>I'm looking to use my own created corpus within Visual Studio Code for MacOSX; I have read probably a hundred forums and I can't wrap my head around what I'm doing wrong as I'm pretty new to programming. </p>

<p><a href=""https://stackoverflow.com/questions/15611328/how-to-save-a-custom-categorized-corpus-in-nltk"">This question</a> seems to be the closes thing I can find to what I need to do; however, I am unaware of how to do the following:</p>

<p>""on a Mac it would be in ~/nltk_data/corpora, for instance. And it looks like you also have to append your new corpus to the <code>__init__.py</code> within .../site-packages/nltk/corpus/.""</p>

<p><strong>When answering, please be aware I am using Homebrew</strong> and don't want to permanently disable using another path if I need to use a stock NLTK corpora data set as well within the same coding.</p>

<p>If needed, I can post my attempt at coding using ""PlaintextCorpusReader"" along with the provided traceback below, although I would rather not have to use PlaintextCorpusReader at all for seamless use and would rather just use a simple copy+paste for .txt files into an appropriate location I wish to use in accordance with the append coding. </p>

<p>Thank you.</p>

<pre><code>Traceback (most recent call last):
  File ""/Users/jordanXXX/Documents/NLP/bettertrainingdata"", line 42, in &lt;module&gt;
    short_pos = open(""short_reviews/pos.txt"", ""r"").read
IOError: [Errno 2] No such file or directory: 'short_reviews/pos.txt'
</code></pre>

<hr>

<hr>

<h2>EDIT:</h2>

<hr>

<p>Thank you for your responses.</p>

<p>I have taken your advice and moved the folder out of NLTK's corpora.</p>

<p>I've been doing some experimenting with my folder location and I've gotten different tracebacks.</p>

<p>If you are saying the best way to do it is with PlaintextCorpusReader then so be it; however, maybe for my application I'd want to use CategorizedPlaintextCorpusReader?</p>

<p>sys.argv is definitely not what I meant, so I can read up on that later.</p>

<p>First, here is my code without my attempt to use PlaintextCorpusReader which results in the above traceback when the folder ""short_reviews"" containing the pos.txt and neg.txt files is outside of the NLP folder:</p>

<pre><code>import nltk
import random
from nltk.corpus import movie_reviews
from nltk.classify.scikitlearn import SklearnClassifier
import pickle

from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import SVC, LinearSVC, NuSVC

from nltk.classify import ClassifierI
from statistics import mode

from nltk import word_tokenize

class VoteClassifier(ClassifierI):
    def __init__(self, *classifiers):
        self._classifiers = classifiers

    def classify(self, features):
        votes = []
        for c in self._classifiers:
            v = c.classify(features)
            votes.append(v)
        return mode(votes)

    def confidence(self, features):
        votes = []
        for c in self._classifiers:
            v = c.classify(features)
            votes.append(v)

        choice_votes = votes.count(mode(votes))
        conf = choice_votes / len(votes)
        return conf

# def main():
#     file = open(""short_reviews/pos.txt"", ""r"")
#     short_pos = file.readlines()
#     file.close

short_pos = open(""short_reviews/pos.txt"", ""r"").read
short_neg = open(""short_reviews/neg.txt"", ""r"").read

documents = []

for r in short_pos.split('\n'):
    documents.append( (r, ""pos"") )

for r in short_neg.split('\n'):
    documents.append((r, ""neg""))

all_words = []

short_pos_words = word.tokenize(short_pos)
short_neg_words = word.tokenize(short_neg)

for w in short_pos_words:
    all_words.append(w. lower())

for w in short_neg_words:
    all_words.append(w. lower())

all_words = nltk.FreqDist(all_words)
</code></pre>

<p>However, when I move the folder ""short_reviews"" containing the text files into the NLP folder using the same code as above but without the use of PlaintextCorpusReader the following occurs:</p>

<pre><code>Traceback (most recent call last):
  File ""/Users/jordanXXX/Documents/NLP/bettertrainingdata"", line 47, in &lt;module&gt;
    for r in short_pos.split('\n'):
AttributeError: 'builtin_function_or_method' object has no attribute 'split'
</code></pre>

<p>When I move the folder ""short_reviews"" containing the text files into the NLP folder using the code below with the use of PlaintextCorpusReader the following Traceback occurs:</p>

<pre><code>import nltk
import random
from nltk.corpus import movie_reviews
from nltk.classify.scikitlearn import SklearnClassifier
import pickle

from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import SVC, LinearSVC, NuSVC

from nltk.classify import ClassifierI
from statistics import mode

from nltk import word_tokenize

from nltk.corpus import PlaintextCorpusReader
corpus_root = 'short_reviews'
word_lists = PlaintextCorpusReader(corpus_root, '*')
wordlists.fileids()


class VoteClassifier(ClassifierI):
    def __init__(self, *classifiers):
        self._classifiers = classifiers

    def classify(self, features):
        votes = []
        for c in self._classifiers:
            v = c.classify(features)
            votes.append(v)
        return mode(votes)

    def confidence(self, features):
        votes = []
        for c in self._classifiers:
            v = c.classify(features)
            votes.append(v)

        choice_votes = votes.count(mode(votes))
        conf = choice_votes / len(votes)
        return conf

# def main():
#     file = open(""short_reviews/pos.txt"", ""r"")
#     short_pos = file.readlines()
#     file.close

short_pos = open(""short_reviews/pos.txt"", ""r"").read
short_neg = open(""short_reviews/neg.txt"", ""r"").read

documents = []

for r in short_pos.split('\n'):
    documents.append((r, ""pos""))

for r in short_neg.split('\n'):
    documents.append((r, ""neg""))

all_words = []

short_pos_words = word.tokenize(short_pos)
short_neg_words = word.tokenize(short_neg)

for w in short_pos_words:
    all_words.append(w. lower())

for w in short_neg_words:
    all_words.append(w. lower())

all_words = nltk.FreqDist(all_words)


Traceback (most recent call last):
  File ""/Users/jordanXXX/Documents/NLP/bettertrainingdata2"", line 18, in &lt;module&gt;
    word_lists = PlaintextCorpusReader(corpus_root, '*')
  File ""/Library/Python/2.7/site-packages/nltk/corpus/reader/plaintext.py"", line 62, in __init__
    CorpusReader.__init__(self, root, fileids, encoding)
  File ""/Library/Python/2.7/site-packages/nltk/corpus/reader/api.py"", line 87, in __init__
    fileids = find_corpus_fileids(root, fileids)
  File ""/Library/Python/2.7/site-packages/nltk/corpus/reader/util.py"", line 763, in find_corpus_fileids
    if re.match(regexp, prefix+fileid)]
  File ""/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/re.py"", line 141, in match
    return _compile(pattern, flags).match(string)
  File ""/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/re.py"", line 251, in _compile
    raise error, v # invalid expression
error: nothing to repeat
</code></pre>
",Dataset Preprocessing & Handling,create corpus sentiment analysis nltk looking use created corpus within visual studio code macosx read probably hundred forum wrap head around wrong pretty new programming answering please aware using homebrew want permanently disable using another path need use stock nltk corpus data set well within coding needed post attempt coding using plaintextcorpusreader along provided traceback although would rather use plaintextcorpusreader seamless use would rather use simple copy paste txt file appropriate location wish use accordance append coding thank edit thank response taken advice moved folder nltk corpus experimenting folder location gotten different tracebacks saying best way plaintextcorpusreader however maybe application want use categorizedplaintextcorpusreader sys definitely meant read later first code without attempt use plaintextcorpusreader result traceback folder short review containing po txt neg txt file outside nlp folder however move folder short review containing text file nlp folder using code without use plaintextcorpusreader following occurs move folder short review containing text file nlp folder using code use plaintextcorpusreader following traceback occurs
In Python how can I generate a frequency word counter that aggreagtes to different levels?,"<p>I've seen other python word counters that read CSV files and give a word count for the entire column. I'd like to see the count of a word per row, except I'd like it on the ""project"" and ""sub-project"" level (other columns in my data). This way I could see if a sub-project had a higher word count than another for a specific word. I'd like to have the final columns be: Project, Sub-Project, Word, Word Count(per sub-project, not total). I'd appreciate any help!</p>

<p>Input:</p>

<p>Columns - Project/Sub-project/Corpus</p>

<p>Project1/Sub 1/The red car is the best car</p>

<p>Project1/Sub 2/The blue is better</p>

<p>Export doc should read:</p>

<p>Columns - Project/Sub-Project/Word/Frequency</p>

<p>Project1/Sub1/The/2</p>

<p>Project1/Sub2/The/1</p>
",Dataset Preprocessing & Handling,python generate frequency word counter aggreagtes different level seen python word counter read csv file give word count entire column like see count word per row except like project sub project level column data way could see sub project higher word count another specific word like final column project sub project word word count per sub project total appreciate help input column project sub project corpus project sub red car best car project sub blue better export doc read column project sub project word frequency project sub project sub
In Python how can I generate a frequency word counter that aggreagtes to different levels?,"<p>I've seen other python word counters that read CSV files and give a word count for the entire column. I'd like to see the count of a word per row, except I'd like it on the ""project"" and ""sub-project"" level (other columns in my data). This way I could see if a sub-project had a higher word count than another for a specific word. I'd like to have the final columns be: Project, Sub-Project, Word, Word Count(per sub-project, not total). I'd appreciate any help!</p>

<p>Input:</p>

<p>Columns - Project/Sub-project/Corpus</p>

<p>Project1/Sub 1/The red car is the best car</p>

<p>Project1/Sub 2/The blue is better</p>

<p>Export doc should read:</p>

<p>Columns - Project/Sub-Project/Word/Frequency</p>

<p>Project1/Sub1/The/2</p>

<p>Project1/Sub2/The/1</p>
",Dataset Preprocessing & Handling,python generate frequency word counter aggreagtes different level seen python word counter read csv file give word count entire column like see count word per row except like project sub project level column data way could see sub project higher word count another specific word like final column project sub project word word count per sub project total appreciate help input column project sub project corpus project sub red car best car project sub blue better export doc read column project sub project word frequency project sub project sub
Can I read a file object into a gensim Dictionary class?,"<p>The <code>gensim</code> <a href=""https://radimrehurek.com/gensim/corpora/dictionary.html"" rel=""nofollow"">Dictionary</a> object keeps track of the vocabulary of the collection of documents (aka corpus). But to feed the data into the object, the data has to be fed into the memory, e.g.</p>

<pre><code>import io
from gensim.corpora import Dictionary

infile = '/path/to/data'

with io.open(infile, 'r', encoding='utf8') as fin:
    d = Dictionary(map(lambda x: x.split(), fin.readlines()))
    d.save('data.dict')
</code></pre>

<p><strong>Can I read a file object into a gensim Dictionary class?</strong></p>
",Dataset Preprocessing & Handling,read file object gensim dictionary class dictionary object keep track vocabulary collection document aka corpus feed data object data ha fed memory e g read file object gensim dictionary class
Get all.polarity value from qdap package results in R,"<p><img src=""https://i.sstatic.net/JdGjd.png"" alt=""Screenshot of the data frame result"">I wanted to do sentimental analysis in R using qdap package. 
It gives out a data frame containing all.all, all.wc, all.polarity, all.pos.words, all.neg.words etc.</p>

<p>I want to extract the values of all.polarity, all.pos.words,all.neg.words but when i use
sentiment$all.polarity or sentiment$all.pos.words,</p>

<p>I get NULL in result.</p>

<blockquote>
  <p>dput(head(sentiment))</p>
</blockquote>

<pre><code>list(structure(list(all = c(""all"", ""all"", ""all""), wc = c(44L, 
1L, 1L), polarity = c(-0.422115882408869, 0, 0), pos.words = list(
    ""-"", ""-"", ""-""), neg.words = list(c(""disappointed"", ""issue""
), ""-"", ""-""), text.var = c(""list(list(content = \""  misleaded  icici bank customer care  branch excutive    really disappointed   bank dont know  steps   take  get  issue fixed\"", meta = list(author = character(0), datetimestamp = list(sec = 20.097678899765, min = 51, hour = 11, mday = 6, mon = 6, year = 115, wday = 1, yday = 186, isdst = 0), description = character(0), heading = character(0), id = \""1\"", language = \""en\"", origin = character(0))))"", 
""list()"", ""list()"")), row.names = c(NA, -3L), .Names = c(""all"", 
""wc"", ""polarity"", ""pos.words"", ""neg.words"", ""text.var""), class = ""data.frame""), 
    structure(list(all = c(""all"", ""all"", ""all""), wc = c(61L, 
    1L, 1L), polarity = c(0, 0, 0), pos.words = list(""led"", ""-"", 
        ""-""), neg.words = list(""expire"", ""-"", ""-""), text.var = c(""list(list(content = \"" didnt know   customer banking  icici   years will  led    people   looking  student travel card   staff  mg road     treat customers  tried  offer  card  wud expire  one year n told  get  new card  one year    dont know\"", meta = list(author = character(0), datetimestamp = list(sec = 20.3989679813385, min = 51, hour = 11, mday = 6, mon = 6, year = 115, wday = 1, yday = 186, isdst = 0), description = character(0), heading = character(0), id = \""1\"", language = \""en\"", origin = character(0))))"", 
    ""list()"", ""list()"")), row.names = c(NA, -3L), .Names = c(""all"", 
    ""wc"", ""polarity"", ""pos.words"", ""neg.words"", ""text.var""), class = ""data.frame""), 
    structure(list(all = c(""all"", ""all"", ""all""), wc = c(58L, 
    1L, 1L), polarity = c(0, 0, 0), pos.words = list(""top"", ""-"", 
        ""-""), neg.words = list(""worst"", ""-"", ""-""), text.var = c(""list(list(content = \""  asked   staff    can upgrade  platinum  coral card   documentation  fee will  involoved  even  receiving  card poeple sill keep calling   top      levied  rs joining fee    interested  paying     card  one   worst customer care   experienced\"", meta = list(author = character(0), datetimestamp = list(sec = 20.648964881897, min = 51, hour = 11, mday = 6, mon = 6, year = 115, wday = 1, yday = 186, isdst = 0), description = character(0), heading = character(0), id = \""1\"", language = \""en\"", \n    origin = character(0))))"", 
    ""list()"", ""list()"")), row.names = c(NA, -3L), .Names = c(""all"", 
    ""wc"", ""polarity"", ""pos.words"", ""neg.words"", ""text.var""), class = ""data.frame""), 
    structure(list(all = c(""all"", ""all"", ""all""), wc = c(59L, 
    1L, 1L), polarity = c(-0.494717861727131, 0, 0), pos.words = list(
        ""-"", ""-"", ""-""), neg.words = list(c(""long time"", ""long time"", 
    ""disappointed""), ""-"", ""-""), text.var = c(""list(list(content = \"" applied   credit card   corporate scheme long time back got  verification call also long time back initially   getting  least  response   executive  now    longer picking   call neither letting  know  status   application extremely disappointed   service\"", meta = list(author = character(0), datetimestamp = list(sec = 20.8989698886871, min = 51, hour = 11, mday = 6, mon = 6, year = 115, wday = 1, yday = 186, isdst = 0), description = character(0), heading = character(0), id = \""1\"", \n    language = \""en\"", origin = character(0))))"", 
    ""list()"", ""list()"")), row.names = c(NA, -3L), .Names = c(""all"", 
    ""wc"", ""polarity"", ""pos.words"", ""neg.words"", ""text.var""), class = ""data.frame""), 
    structure(list(all = c(""all"", ""all"", ""all""), wc = c(66L, 
    1L, 1L), polarity = c(0.0246182981958665, 0, 0), pos.words = list(
        c(""work"", ""support""), ""-"", ""-""), neg.words = list(""disappointed"", 
        ""-"", ""-""), text.var = c(""list(list(content = \"" otp service   working    used  work   month     decided  change everything im  getting  otp sms   registered mobile number ive tried contacting  customer support several times   keep asking   send  sms   despite  done  several times  several days  havent received  otps ever really disappointed\"", meta = list(author = character(0), datetimestamp = list(sec = 21.1935319900513, min = 51, hour = 11, mday = 6, mon = 6, year = 115, wday = 1, yday = 186, isdst = 0), description = character(0), \n    heading = character(0), id = \""1\"", language = \""en\"", origin = character(0))))"", 
    ""list()"", ""list()"")), row.names = c(NA, -3L), .Names = c(""all"", 
    ""wc"", ""polarity"", ""pos.words"", ""neg.words"", ""text.var""), class = ""data.frame""), 
    structure(list(all = c(""all"", ""all"", ""all""), wc = c(50L, 
    1L, 1L), polarity = c(-0.282842712474619, 0, 0), pos.words = list(
        ""-"", ""-"", ""-""), neg.words = list(c(""pathetic"", ""lied""
    ), ""-"", ""-""), text.var = c(""list(list(content = \""  pathetic service  behavior  icici bank   facing  past  days icici executive lied  luring   upgrade  debit card terms  conditions  just opposite   booklet   received    told  phone\"", meta = list(author = character(0), datetimestamp = list(sec = 21.4258019924164, min = 51, hour = 11, mday = 6, mon = 6, year = 115, wday = 1, yday = 186, isdst = 0), description = character(0), heading = character(0), id = \""1\"", language = \""en\"", origin = character(0))))"", 
    ""list()"", ""list()"")), row.names = c(NA, -3L), .Names = c(""all"", 
    ""wc"", ""polarity"", ""pos.words"", ""neg.words"", ""text.var""), class = ""data.frame""))
</code></pre>

<p>Can anyone suggest how to do this?</p>
",Dataset Preprocessing & Handling,get polarity value qdap package result r wanted sentimental analysis r using qdap package give data frame containing wc polarity po word neg word etc want extract value polarity po word neg word use sentiment polarity sentiment po word get null result dput head sentiment anyone suggest
Retrieval Based Q/A bot,"<p>I am trying to train a retrieval based Q/A chat bot using RNN (classification). I tried training for about 1000 steps, but have hardly got any meaning full results (ACC &lt; 10). Basically, I was trying to map the tensorflow <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/text_classification.py"" rel=""nofollow"">DBPedia example</a> over my dataset. (So I turned my Q/A problem to a classification one). DBPedia is a clean and grammatically correct dataset. However, my dataset is full of short forms and grammatical errors / spelling mistakes. I have tried to correct many of them using the <a href=""http://norvig.com/ngrams/spell-errors.txt"" rel=""nofollow"">(right/wrongs) words pairs</a> and stemming. </p>

<p>I have read that sequence to sequence model works the best for such problems. However, I had not expected the RNN to fail so miserably. </p>

<p>Any ideas why it did ?</p>

<p>[EDIT] : Even Char level CNN gives similar result. </p>
",Dataset Preprocessing & Handling,retrieval based q bot trying train retrieval based q chat bot using rnn classification tried training step hardly got meaning full result acc basically wa trying map tensorflow dbpedia example dataset turned q problem classification one dbpedia clean grammatically correct dataset however dataset full short form grammatical error spelling mistake tried correct many using right wrong word pair stemming read sequence sequence model work best problem however expected rnn fail miserably idea edit even char level cnn give similar result
Row-wise frequency of every word in URL in R,"<p>I am very new to programming and need some help in R programming for my University project. I want to create a table with frequency of each word. Input file has around 70000 rows of data such as IDs and webURLs visited by that ID user separated by comma in a csv file:For example:</p>

<pre><code>ID                 URLs 
m7fdn              privatkunden:handys, tablets, tarife:vorteile &amp; services:ausland &amp; roaming,privatkunden:hilfe:mehr hilfe:ger,privatkunden:hilfe:service-themen:internet  dsl &amp; ltekonfigurieren
9ufdf              mein website:kontostand &amp; rechnung:meinerechnung:6-monate-Ã¼bersicht zu ihrer rufnummer,mein website:kontostand &amp; rechnung:meinerechnung:kosten
09nd7              404 &lt;https://www.website.de/ussa/login/login.ftel?errorcode=2001&amp;name=%20&amp;goto=https%3a%,mein website:login online user:show form:login.ftel / login),mobile,mobile:meinwebsite:kundendaten (mydata.html),mobile:meinwebsite:startseite (index.html),privatkunden:home,privatkunden:meinwebsite:login.ftel
</code></pre>

<p>Below code has removed all the special characters from URLs and is giving frequency of word used in whole document. But I don't want it for whole document at once. I want an output per row. </p>

<pre><code>text &lt;- readLines(""sample.csv"")
docs &lt;- Corpus(VectorSource(text))
inspect(docs)
toSpace &lt;- content_transformer(function (x , pattern)gsub(pattern, "" "", x))  
docs &lt;- tm_map(docs, toSpace, ""/"")
docs &lt;- tm_map(docs, toSpace, ""@"")
docs &lt;- tm_map(docs, toSpace, "","")
docs &lt;- tm_map(docs, toSpace, "";"")
docs &lt;- tm_map(docs, toSpace, ""://"")
docs &lt;- tm_map(docs, toSpace, "":"")
docs &lt;- tm_map(docs, toSpace, ""&lt;"")
docs &lt;- tm_map(docs, toSpace, ""&gt;"")
docs &lt;- tm_map(docs, toSpace, ""-"")
docs &lt;- tm_map(docs, toSpace, ""_"")
docs &lt;- tm_map(docs, toSpace, ""://"")
docs &lt;- tm_map(docs, toSpace, ""&amp;"")
docs &lt;- tm_map(docs, toSpace, "")"")
docs &lt;- tm_map(docs, toSpace, ""%"")


dtm &lt;- TermDocumentMatrix(docs) 
m &lt;- as.matrix(dtm)
v &lt;- sort(rowSums(m),decreasing=TRUE)
d &lt;- data.frame(word = names(v),freq=v)
</code></pre>

<p>Output I am getting is as below:</p>

<pre><code>                       word freq  
mein                   mein 1451  
website             website 1038  
privatkunden   privatkunden  898  
meinwebsite     meinwebsite  479  
rechnung           rechnung  474  
</code></pre>

<p>The output I want should be like this:</p>

<pre><code>ID               privatkunden  website    hilfe    rechnung  kosten      
m7fdn               4             7         2         7       0
9ufdf               3             1         9         3       5
09nd7               5             7         2         8       9
</code></pre>

<p>The above table means that the ID m7fdn has 4 times privatkunden in its URLs and 2 times hilfe and so on. The above table is just for sample and does not count the exact words. This table can be as long as many number of words are there. Please help me to get this output. Once I get this table I have to apply Machine Learning.</p>
",Dataset Preprocessing & Handling,row wise frequency every word url r new programming need help r programming university project want create table frequency word input file ha around row data id weburls visited id user separated comma csv file example code ha removed special character url giving frequency word used whole document want whole document want output per row output getting output want like table mean id fdn ha time privatkunden url time hilfe table sample doe count exact word table long many number word please help get output get table apply machine learning
How to get data within data,"<p>I don't know if I'm gonna be able to explain this properly but here goes. I have a data frame called <code>ZCP</code> that includes tokenized tweets (gonna use them for sentiment analysis) and related metadata. The structure looks like this:</p>

<pre><code>head(ZAD)
num_tokens unique_tokens
1         12            12
2         11            10
3         11            10
4         12            12
5         22            20
6         11            10
text
1 rt, caradelevingne, fam, a, lam, glastonbury, https, t, co, h, ew, oux
2 rt, caradelevingne, home, sweet, home, glastonbury, https, t, co, zolld, ltvt
3 rt, caradelevingne, home, sweet, home, glastonbury, https, t, co, zolld, ltvt
4 rt, caradelevingne, fam, a, lam, glastonbury, https, t, co, h, ew, oux
5 rt, yahoocelebuk, adele, set, to, dominate, the, uk, albums, chart, as, heads, back, to, number, post, glastonbury, https, t, co, cndkufsgo, https
6 rt, caradelevingne, home, sweet, home, glastonbury, https, t, co, zolld, ltvt

favoriteCount                 id retweetCount isRetweet
1             0 747942553010397184          593      TRUE
2             0 747942530340118529          729      TRUE
3             0 747941795988905986          729      TRUE
4             0 747941781820542976          593      TRUE
5             0 747940287847161856            3      TRUE
6             0 747940084603838464          729      TRUE
</code></pre>

<p>Basically I'm only interested in the data in the text column for now. That data looks like this:</p>

<pre><code>head(ZCP$text) 
$to_return [1] ""saw"" ""viola"" ""beach"" ""support"" ""courteeners"" ""in""
[7] ""w"" ""ton"" ""to"" ""see"" ""coldplay"" ""do""
[13] ""the"" ""tribute"" ""to"" ""them"" ""at"" ""glastonbury"" [19] ""was"" ""amazing"" ""so"" ""well"" ""thought"" ""out""
[[2]] [1] ""glastonbury"" ""coldplay"" ""elo"" ""break"" ""viewing"" ""records""
[7] ""muse"" ""s"" ""audience"" ""doubles"" ""https"" ""t""
[13] ""co"" ""eocvqnoeen"" ""coldplay"" ""muse"" ""https"" ""t""
[19] ""co"" ""yd"" ""ie"" ""xr"" ""n""
[[3]] [1] ""another"" ""cheeky"" ""glastonbury"" ""pic"" ""coldplay"" ""pyramidstage""[7] ""https"" ""t"" ""co"" ""qttz"" ""xgjpx"" ""https""
[13] ""t"" ""co"" ""rm"" ""y"" ""pbvml""
[[4]] [1] ""i"" ""m"" ""having"" ""my"" ""very"" ""own""
[7] ""glastonbury"" ""tonight"" ""coldplay"" ""adele""
[[5]] [1] ""that"" ""was"" ""awesome"" ""coldplay"" ""glastonbury"" ""glasto""
[7] ""https"" ""t"" ""co"" ""fz"" ""ly"" ""cvx""
[[6]] [1] ""beegees"" ""barry"" ""gibb"" ""stayin"" ""alive"" ""and""
[7] ""coldplay"" ""en"" ""glastonbury"" ""https"" ""t"" ""co""
[13] ""hoj""
</code></pre>

<p>What operator should i use to get the individual tokens? I'm planning on writing a for loop but I can't the right operator to reach data within data. <code>ZCP$text[1]</code> gives me the below result:</p>

<pre><code>ZCP$text[1] $to_return [1] ""saw"" ""viola"" ""beach"" ""support"" ""courteeners"" ""in""
[7] ""w"" ""ton"" ""to"" ""see"" ""coldplay"" ""do""
[13] ""the"" ""tribute"" ""to"" ""them"" ""at"" ""glastonbury"" [19] ""was"" ""amazing"" ""so"" ""well"" ""thought"" ""out""
</code></pre>

<p>How can I get the first element of this object? I can't find the right operator to do this for some reason. Any help is appreciated. Thanks.</p>

<p>edit: @Sotos asked for a dput for this. Not sure if this is what he wanted (I'm a noob at R and never used dput before) but here it is for <code>head(ZCP)</code>:</p>

<pre><code>structure(list(num_tokens = structure(list(to_return = 24L, 23L, 
17L, 10L, 12L, 16L), .Names = c(""to_return"", """", """", """", 
"""", """")), unique_tokens = structure(list(to_return = 23L, 18L, 
14L, 10L, 12L, 16L), .Names = c(""to_return"", """", """", """", 
"""", """")), text = structure(list(to_return = c(""saw"", ""viola"", 
""beach"", ""support"", ""courteeners"", ""in"", ""w"", ""ton"", ""to"", ""see"", 
""coldplay"", ""do"", ""the"", ""tribute"", ""to"", ""them"", ""at"", ""glastonbury"", 
""was"", ""amazing"", ""so"", ""well"", ""thought"", ""out""), c(""glastonbury"", 
""coldplay"", ""elo"", ""break"", ""viewing"", ""records"", ""muse"", ""s"", 
""audience"", ""doubles"", ""https"", ""t"", ""co"", ""eocvqnoeen"", ""coldplay"", 
""muse"", ""https"", ""t"", ""co"", ""yd"", ""ie"", ""xr"", ""n""), c(""another"", 
""cheeky"", ""glastonbury"", ""pic"", ""coldplay"", ""pyramidstage"", ""https"", 
""t"", ""co"", ""qttz"", ""xgjpx"", ""https"", ""t"", ""co"", ""rm"", ""y"", ""pbvml""
), c(""i"", ""m"", ""having"", ""my"", ""very"", ""own"", ""glastonbury"", 
""tonight"", ""coldplay"", ""adele""), c(""that"", ""was"", ""awesome"", 
""coldplay"", ""glastonbury"", ""glasto"", ""https"", ""t"", ""co"", ""fz"", 
""ly"", ""cvx""), c(""beegees"", ""barry"", ""gibb"", ""stayin"", ""alive"", 
""and"", ""coldplay"", ""en"", ""glastonbury"", ""https"", ""t"", ""co"", ""hoj"", 
""u"", ""j"", ""yz"")), .Names = c(""to_return"", """", """", """", """", """")),
favoriteCount = structure(list(to_return = 2, 1, 0, 0, 0, 
1), .Names = c(""to_return"", """", """", """", """", """")), id = structure(list(
to_return = ""747938975621521408"", ""747938533290049537"", 
""747934687696420864"", ""747934531756384256"", ""747931753373892608"", 
""747928260835696640""), .Names = c(""to_return"", """", """", 
"""", """", """")), retweetCount = structure(list(to_return = 1, 
0, 0, 0, 0, 0), .Names = c(""to_return"", """", """", """", """", 
"""")), isRetweet = structure(list(to_return = FALSE, FALSE, 
FALSE, FALSE, FALSE, FALSE), .Names = c(""to_return"", 
"""", """", """", """", """"))), .Names = c(""num_tokens"", ""unique_tokens"", 
""text"", ""favoriteCount"", ""id"", ""retweetCount"", ""isRetweet""), row.names = c(NA, 
6L), class = ""data.frame"")
</code></pre>
",Dataset Preprocessing & Handling,get data within data know gon na able explain properly go data frame called includes tokenized tweet gon na use sentiment analysis related metadata structure look like basically interested data text column data look like operator use get individual token planning writing loop right operator reach data within data give result get first element object find right operator reason help appreciated thanks edit sotos asked dput sure wanted noob r never used dput
Input data type for sklearn SVD fit_transform function,"<p>I have already processed document data in CSV file, which I read in pandas DataFrame:</p>

<pre><code>+----------+------+------------+
| document | term | count      |
+----------+------+------------+
| 1        | 126  | 1          |
| 1        | 80   | 1          |
| 1        | 1221 | 2          |
| 2        | 2332 | 1          |
</code></pre>

<p>So it consists of document_id, term, and term frequency.</p>

<p>I don't have original documents, but just this processed data, and I want to apply SVD with sklearn, but I can not figure how to prepare this DataFrame for SVD <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD.fit_transform"" rel=""nofollow"">fit_transform()</a>, which expects:</p>

<blockquote>
  <p>X : {array-like, sparse matrix}, shape (n_samples, n_features)</p>
</blockquote>
",Dataset Preprocessing & Handling,input data type sklearn svd fit transform function already processed document data csv file read panda dataframe consists document id term term frequency original document processed data want apply svd sklearn figure prepare dataframe svd fit transform expects x array like sparse matrix shape n sample n feature
Using Apache UIMA to Build NLP Operation Pipeline,"<p>I am new to Apache UIMA and I am trying to build a NLP pipeline using Apache UIMA. When user upload the document (i.e. pdf, words) I want to extract the data using Tika Annotator. I am able to extract Text from pdf and other documents using APache Tika but how to write Annotator for this and how to pass the output of one annotator as input to other annotator. I went through apache UIMA site but was not able to capture much from the site so that I can use it in my project.</p>

<p>for example will be having Tika Annotator to extract Text and then perform Tokenization in TokenAnnotator using the Output Tika Annotator and then use  TokenAnnotator Output as Input to POS Annotator</p>

<p>Any help would be greatly appreciated </p>
",Dataset Preprocessing & Handling,using apache uima build nlp operation pipeline new apache uima trying build nlp pipeline using apache uima user upload document e pdf word want extract data using tika annotator able extract text pdf document using apache tika write annotator pas output one annotator input annotator went apache uima site wa able capture much site use project example tika annotator extract text perform tokenization tokenannotator using output tika annotator use tokenannotator output input po annotator help would greatly appreciated
Why Eigen values of adajcency matrix are actually the sentence scores in Textrank,"<p>Here is the route for TextRank:</p>

<ol>
<li>Document to be summarized expressed as tf-idf matrix</li>
<li>(tf-idf matrix)*(tf-idf matrix).Transpose = Adjacency matrix of some graph whose vertices are
actually the sentences of above document</li>
<li>Page rank is applied on this graph -> returns PR values of each sentence</li>
</ol>

<p>Now, <strong>this PR values are actually Eigen values of that adjacency matrix</strong><br>
What is the physical meaning or intuition behind this.? </p>

<p><strong>Why Eigen values are actually the ranks ?</strong></p>

<p>Here is the link for Page Rank:
<a href=""http://www.cs.princeton.edu/~chazelle/courses/BIB/pagerank.htm"" rel=""nofollow"">http://www.cs.princeton.edu/~chazelle/courses/BIB/pagerank.htm</a></p>

<p>Here is an extract from above page: <br>
<strong>PageRank or PR(A) can be calculated using a simple iterative algorithm, and corresponds to the principal eigenvector of the normalized link matrix of the web.</strong></p>

<p>Link for TextRank:
<a href=""https://joshbohde.com/blog/document-summarization"" rel=""nofollow"">https://joshbohde.com/blog/document-summarization</a></p>
",Dataset Preprocessing & Handling,eigen value adajcency matrix actually sentence score textrank route textrank document summarized expressed tf idf matrix tf idf matrix tf idf matrix transpose adjacency matrix graph whose vertex actually sentence document page rank applied graph return pr value sentence pr value actually eigen value adjacency matrix physical meaning intuition behind eigen value actually rank link page rank extract page pagerank pr calculated using simple iterative algorithm corresponds principal eigenvector normalized link matrix web link textrank
Text analysis within data frame in r,"<p>I am working on Google Store metadata and have it as a data frame. Per each app there is information about the requested permissions within a single cell, as a long text, for example:</p>

<blockquote>
  <p>READ SENSITIVE LOG DATA|RETRIEVE RUNNING APPS|FIND ACCOUNTS ON THE DEVICE|READ YOUR OWN CONTACT CARD|READ YOUR CONTACTS|</p>
</blockquote>

<p>I want to separate the text between the ""|"" character into different cells (columns), so I can analyze existing permissions. I did not analyze text with R before. I tried using strings functions, however, when looking at the info within the cell, it is not recognized as a string.  </p>

<p>Any suggestions, directions? Thanks!</p>
",Dataset Preprocessing & Handling,text analysis within data frame r working google store metadata data frame per app information requested permission within single cell long text example read sensitive log data retrieve running apps find account device read contact card read contact want separate text character different cell column analyze existing permission analyze text r tried using string function however looking info within cell recognized string suggestion direction thanks
String similarity Search in R,"<p>I have a big list of Product Descriptions something like</p>

<pre><code>Water bottles 1L
Water Can 1L
Aerated Drinks 1L
Chips Potato
Doritos Chips
Bread
Yogurt 300ML
Milk
</code></pre>

<p>Ofcourse the list is in Millions. </p>

<p>I am trying to create a search similarity. I want a search function which will Pick Similarity descriptions. If I search on Milk Products, it should bring Milk, Butter, Yogurt something like that. </p>

<p>For that I used <code>levenshteinSim</code> and run all the descriptions in Loop and compare with my search string. By the time it ran for all Million records it is taking long time. Is there any algorithm that will take data frame and string as input parameter and min score to get the values?</p>
",Dataset Preprocessing & Handling,string similarity search r big list product description something like ofcourse list million trying create search similarity want search function pick similarity description search milk product bring milk butter yogurt something like used run description loop compare search string time ran million record taking long time algorithm take data frame string input parameter min score get value
Java desktop application excel upload with Apache POI,"<p>I need to create a Java desktop application which will allow users to upload an excel file. The file then will be read and written by apache poi codes. Then the modified file should be downloadable by the user.</p>

<p>The code I found online below reads a file from a specific location, I would like it to read it directly once it is uploaded by the user.</p>

<pre><code>public class ReadWriteExcelFile {

    public static void readXLSFile() throws IOException
    {
        InputStream ExcelFileToRead = new FileInputStream(""C:/Test.xls"");
        HSSFWorkbook wb = new HSSFWorkbook(ExcelFileToRead);

        HSSFSheet sheet=wb.getSheetAt(0);
        HSSFRow row; 
        HSSFCell cell;

        Iterator rows = sheet.rowIterator();

        while (rows.hasNext())
        {
            row=(HSSFRow) rows.next();
            Iterator cells = row.cellIterator();

            while (cells.hasNext())
            {
                cell=(HSSFCell) cells.next();

                if (cell.getCellType() == HSSFCell.CELL_TYPE_STRING)
                {
                    System.out.print(cell.getStringCellValue()+"" "");
                }
                else if(cell.getCellType() == HSSFCell.CELL_TYPE_NUMERIC)
                {
                    System.out.print(cell.getNumericCellValue()+"" "");
                }
                else
                {
                    //U Can Handel Boolean, Formula, Errors
                }
            }
            System.out.println();
        }

    }
</code></pre>

<p>How can I also get the Apache POI read to just read items in one column and save each cell value in a variable?</p>

<p>Many thanks in advance</p>
",Dataset Preprocessing & Handling,java desktop application excel upload apache poi need create java desktop application allow user upload excel file file read written apache poi code modified file downloadable user code found online read file specific location would like read directly uploaded user also get apache poi read read item one column save cell value variable many thanks advance
Tensorflow How to convert words(Strings) from a csv file to proper vectors,"<p>Hi im trying to make a small classifier in tensorflow. I want to read data from a csv file and use it for my training phase, the problem is the content of my file looks something like this:</p>

<p>object,categorie<br>
the blue balon,toy<br>
a white plastic ship,toy<br>
a big book,other<br>
the wild cat,animal<br>
a wet dolphin,animal
...</p>

<p>So i want to read the sentences and then convert them to vector for use in a tensorflow model. All the information i readed was about numerical data but no idea how to use data like this.</p>

<p>The turorials from the oficial site use numeric data, the best option so far has been use a dictionary but i think there should exist a better option.</p>

<p>Another option is to make my own method but could be imprecise.</p>

<p>Have someone any ideas how can i do that? 
an alternative for mi method or how can i process words in tensorflow?</p>

<p>Sorry if my english is not good.</p>

<p><strong>EDIT</strong></p>

<p>Try to convert sentences into multidimensional arrays but the results were not good, I estimate that the poor results are due to some statements can be short and others long, which affects the final free space on each array and this free space affects the results the probabilistic model. Any recommendation?</p>
",Dataset Preprocessing & Handling,tensorflow convert word string csv file proper vector hi im trying make small classifier tensorflow want read data csv file use training phase problem content file look something like object categorie blue balon toy white plastic ship toy big book wild cat animal wet dolphin animal want read sentence convert vector use tensorflow model information readed wa numerical data idea use data like turorials oficial site use numeric data best option far ha use dictionary think exist better option another option make method could someone idea alternative mi method process word tensorflow sorry english good edit try convert sentence multidimensional array result good estimate poor result due statement short others long affect final free space array free space affect result probabilistic model recommendation
Extract relevant text from a .txt file in R,"<p>I am still on a basic beginner level with r. I am currently working on some natural language stuff and I use the ProQuest Newsstand database. Even though the database allows to download txt files, I don't need everything they provide. The files you can download there look like this:</p>

<pre><code>###############################################################################
____________________________________________________________

Report Information from ProQuest 16 July 2016 09:58
____________________________________________________________




____________________________________________________________

Inhaltsverzeichnis

1. Savills cracks Granite deal to establish US presence ; COMMERCIAL PROPERTY

____________________________________________________________

Dokument 1 von 1

Savills cracks Granite deal to establish US presence ; COMMERCIAL PROPERTY

http:...

Kurzfassung: Savills said that as part of its plans to build...

Links: ...

Volltext: Property agency Savills yesterday snapped up US real estate banking firm Granite Partners...

Unternehmen/Organisation: Name: Granite Partners LP; NAICS: 525910

Titel: Savills cracks Granite deal to establish US presence; COMMERCIAL PROPERTY:   [FIRST Edition]

Autor: Steve Pain Commercial Property Editor

Titel der Publikation: Birmingham Post

Seiten: 30

Seitenanzahl: 0

Erscheinungsjahr: 2007

Publikationsdatum: Aug 2, 2007

Jahr: 2007

Bereich: Business

Herausgeber: Mirror Regional Newspapers

Verlagsort: Birmingham (UK)

Publikationsland: United Kingdom

Publikationsthema: General Interest Periodicals--Great Britain

Quellentyp: Newspapers

Publikationssprache: English

Dokumententyp: NEWSPAPER

ProQuest-Dokument-ID: 324215031

Dokument-URL: ...

Copyright: (Copyright 2007 Birmingham Post and Mail Ltd.)

Zuletzt aktualisiert: 2010-06-19

Datenbank: UK Newsstand

____________________________________________________________

Kontaktieren Sie uns unter: http... Copyright © 2016 ProQuest LLC. Alle Rechte vorbehalten. Allgemeine Geschäftsbedingungen:  ...

###############################################################################
</code></pre>

<p>What I need is a way to extract only the full text to a csv file. The reason is, when I download hundreds of articles within one file it is quite difficult to copy and paste them manually and I think the file is quite structured. However, the length of text varies. Nevertheless, one could use the next header after the full text as a stop sign (I guess).</p>

<p>Is there any way to do this?</p>

<p>I really would appreciate some help.
Kind regards,
Steffen</p>
",Dataset Preprocessing & Handling,extract relevant text txt file r still basic beginner level r currently working natural language stuff use proquest newsstand database even though database allows download txt file need everything provide file download look like need way extract full text csv file reason download hundred article within one file quite difficult copy paste manually think file quite structured however length text varies nevertheless one could use next header full text stop sign guess way really would appreciate help kind regard steffen
How to match tokens in document term matrix to a separate data frame (of POS codes),"<p>Basically I have my bag of words:</p>

<pre><code>source &lt;- VectorSource(text)
corpus &lt;- Corpus(source)
corpus &lt;- tm_map(corpus, content_transformer(tolower))
dtm &lt;- DocumentTermMatrix(cleanset)
</code></pre>

<p>etc etc.</p>

<p>And I have a data frame consisting or just two columns which I called up from a SQLite DB. Column 1 is a list of hundreds of words, and Column 2 is each word's corresponding Part of Speech code.</p>

<p>I am trying to match every token in my dtm to the identical term in column 1 of the dataframe, so that each token then can be matched its corresponding POS code.
Essentially, the dataframe is like a dictionary, and I want to match each token in my dtm to its definition. </p>

<p>I tried a bunch of GREP functions to do this, but to no avail. Anyone have thoughts on the best way to approach this?</p>

<p>Thanks!</p>
",Dataset Preprocessing & Handling,match token document term matrix separate data frame po code basically bag word etc etc data frame consisting two column called sqlite db column list hundred word column word corresponding part speech code trying match every token dtm identical term column dataframe token matched corresponding po code essentially dataframe like dictionary want match token dtm definition tried bunch grep function avail anyone thought best way approach thanks
Caching of the data of a big file in memory in java,"<p>Hi I am working on Spelling Corrector project of Natural Language processing and I am supposed read data from a file whose size is <strike>6.2 MB</strike> 1 GB. While it is working fine, the problem that I am facing is that every time I run the java program I have to load the data in to the memory and it is taking same amount of time every time it is run. </p>

<p>Is there any way this data can cached in to the memory in java?Can any one suggest me some work around of it? </p>

<p>Basically what I want to know is that What is procedure of storing content of a large file in memory so that I dont have to read it again? lets say file is of GB. </p>
",Dataset Preprocessing & Handling,caching data big file memory java hi working spelling corrector project natural language processing supposed read data file whose size mb gb working fine problem facing every time run java program load data memory taking amount time every time run way data cached memory java one suggest work around basically want know procedure storing content large file memory dont read let say file gb
Efficient lag variable creation in large document term matrix in R,"<p>I am working with a rather large document term matrix (~280,000 terms) in R, and am wondering if there is an efficient way to create lag variables for each of my original terms.</p>

<p>The following example gives a document term matrix with three terms. This works for a toy example like this, but would be impossible for my data.</p>

<p>A quick note on the lag structure: I am exploring whether the appearance of any given term may have some cumulative, though diminishing, amount of importance over time.    </p>

<pre><code>dtm &lt;- data.frame(revenue=c(1,2,3,3,5,6), up=c(1,1,0,3,1,1), sale=c(0,1,1,0,1,1))

for (i in 1:nrow(dtm)){
  if (i &gt;=4){
    dtm$revenueLag4days[i] &lt;- dtm$revenue[(i-3):i]%*%c(0.25,0.5,0.75,1)
    dtm$upLag4days[i] &lt;- dtm$up[(i-3):i]%*% c(0.25,0.5,0.75,1)
    dtm$saleLag4days[i] &lt;- dtm$sale[(i-3):i]%*% c(0.25,0.5,0.75,1)
  } else
    dtm$revenueLag4days[i]  &lt;- dtm$upLag4days[i] &lt;- dtm$saleLag4days[i] &lt;- NA
}
</code></pre>

<p>Is it possible to rewrite this in a functional way for a document term matrix (~280,000 terms)?</p>
",Dataset Preprocessing & Handling,efficient lag variable creation large document term matrix r working rather large document term matrix term r wondering efficient way create lag variable original term following example give document term matrix three term work toy example like would impossible data quick note lag structure exploring whether appearance given term may cumulative though diminishing amount importance time possible rewrite functional way document term matrix term
Annotating a Corpus (Syntaxnet),"<p>I downloaded and installed SyntaxNet following <a href=""https://github.com/tensorflow/models/tree/master/syntaxnet"">Syntax official documentation on Github</a>. following  the documentation (annotating corpus) I tried to read a <code>.conll</code> file named <code>wj.conll</code> by SyntaxNet and write the results in <code>wj-tagged.conll</code> but I could not. My questions are:</p>

<ol>
<li><p>does SyntaxNet always reads <code>.conll</code> files? (not <code>.txt</code> files?). I got a bit confused as I knew SyntaxNet reads <code>.conll</code> file for training and testing process but I am a bit suspicious that it is necessary to convert a <code>.txt</code> file to <code>.conll</code> file in order to have their Part Of Speach and Dependancy Parsing.</p></li>
<li><p>How can I make SyntaxNet reads from files (I tired all possible ways explain in GitHub documentation about SyntaxNet and It didn't work for me)</p></li>
</ol>
",Dataset Preprocessing & Handling,annotating corpus syntaxnet downloaded installed syntaxnet following href official documentation github following documentation annotating corpus tried read file named syntaxnet write result could question doe syntaxnet always read file file got bit confused knew syntaxnet read file training testing process bit suspicious necessary convert file file order part speach dependancy parsing make syntaxnet read file tired possible way explain github documentation syntaxnet work
R: Natural Language Processing on Support Vector Machine,"<p>I have started working on a project which requires Natural Language Processing and building a model on Support Vector Machine (SVM) in R (I was requested to do it in R, though I know Python is more developed on this). 
I found an article <a href=""https://rpubs.com/lmullen/nlp-chapter"" rel=""nofollow"">here</a> (Packages: <code>NLP</code>, <code>OpenNLP</code>, <code>rJava</code>, <code>RWeka</code>). However, the article focuses on how to extract key words (ex. Place, names…). </p>

<p>But since I want to build a SVM model, I’d like to generate a Term Document Matrix with all the tokens. I couldn’t get it work since the class of the annotation does not apply in <code>tm</code> package. </p>

<p>Example:</p>

<pre><code>testset &lt;- c(""From month 2 the AST and total bilirubine were not measured."", ""16:OTHER - COMMENT REQUIRED IN COMMENT COLUMN;07/02/2004/GENOTYPING;SF- genotyping consent not offered until T4."",  ""M6 is 13 days out of the visit window"")
word_ann &lt;- Maxent_Word_Token_Annotator()
sent_ann &lt;- Maxent_Sent_Token_Annotator()
test_annotations &lt;- annotate(testset, list(sent_ann, word_ann))
test_doc &lt;- AnnotatedPlainTextDocument(testset, test_annotations)
sents(test_doc)

[[1]]
 [1] ""From""       ""month""      ""2""          ""the""        ""AST""        ""and""        ""total""     
 [8] ""bilirubine"" ""were""       ""not""        ""measured""   "".""         

[[2]]
 [1] ""16:OTHER""                         ""-""                               
 [3] ""COMMENT""                          ""REQUIRED""                        
 [5] ""IN""                               ""COMMENT""                         
 [7] ""COLUMN;07/02/2004/GENOTYPING;SF-"" ""genotyping""                      
 [9] ""consent""                          ""not""                             
[11] ""offered""                          ""until""                           
[13] ""T4""                               "".""                               

[[3]]
[1] ""M6""     ""is""     ""13""     ""days""   ""out""    ""of""     ""the""    ""visit""  ""window"" 
sessionInfo()
R version 3.3.0 (2016-05-03)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows &gt;= 8 x64 (build 9200)

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] tm_0.6-2       openxlsx_3.0.0 magrittr_1.5   RWeka_0.4-28   openNLP_0.2-6  NLP_0.1-9     
[7] rJava_0.9-8   

loaded via a namespace (and not attached):
[1] openNLPdata_1.5.3-2 parallel_3.3.0      tools_3.3.0         Rcpp_0.12.5         slam_0.1-34        
[6] grid_3.3.0          knitr_1.13          RWekajars_3.9.0-1  
</code></pre>

<p>And now I don't know how to generate a TDM from here...
Could anyone please give me some advice on this?</p>
",Dataset Preprocessing & Handling,r natural language processing support vector machine started working project requires natural language processing building model support vector machine svm r wa requested r though know python developed found article package however article focus extract key word ex place name since want build svm model like generate term document matrix token get work since class annotation doe apply package example know generate tdm could anyone please give advice
Stanford NLP: OutOfMemoryError,"<p>I am annotating and analyzing a series of text files.</p>

<p>The pipeline.annotate method becomes increasingly slow each time it reads a file. Eventually, I get an OutOfMemoryError.</p>

<p><strong>Pipeline is initialized ONCE:</strong></p>

<pre><code>protected void initializeNlp()
{
    Log.getLogger().debug(""Starting Stanford NLP"");


    // creates a StanfordCoreNLP object, with POS tagging, lemmatization,
    // NER, parsing, and
    Properties props = new Properties();

    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, regexner, depparse,  natlog,  openie"");
    props.put(""regexner.mapping"", namedEntityPropertiesPath);

    pipeline = new StanfordCoreNLP(props);


    Log.getLogger().debug(""\n\n\nStarted Stanford NLP Successfully\n\n\n"");
}
</code></pre>

<p><strong>I then process each file using same instance of pipeline</strong> (as recommended elsewhere on SO and by Stanford).</p>

<pre><code>     public void processFile(Path file)
{
    try
    {
        Instant start = Instant.now();

        Annotation document = new Annotation(cleanString);
        Log.getLogger().info(""ANNOTATE"");
        pipeline.annotate(document);
        Long millis= Duration.between(start, Instant.now()).toMillis();
        Log.getLogger().info(""Annotation Duration in millis: ""+millis);

        AnalyzedFile af = AnalyzedFileFactory.getAnalyzedFile(AnalyzedFileFactory.GENERIC_JOB_POST, file);

        processSentences(af, document);

        Log.getLogger().info(""\n\n\nFile Processing Complete\n\n\n\n\n"");



        Long millis1= Duration.between(start, Instant.now()).toMillis();
        Log.getLogger().info(""Total Duration in millis: ""+millis1);

        allFiles.put(file.toUri().toString(), af);


    }
    catch (Exception e)
    {
        Log.getLogger().debug(e.getMessage(), e);
    }

}
</code></pre>

<p>To be clear, I expect the problem is with my configuration. However, I am certain that the stall and memory issues occur at the pipeline.annotate(file) method.</p>

<p>I dispose of all references to Stanford-NLP objects other than pipeline (e.g., CoreLabel) after processing each file. That is, I do not keep references to any Stanford objects in my code beyond the method level.</p>

<p>Any tips or guidance would be deeply appreciated</p>
",Dataset Preprocessing & Handling,stanford nlp outofmemoryerror annotating analyzing series text file pipeline annotate method becomes increasingly slow time read file eventually get outofmemoryerror pipeline initialized process file using instance pipeline recommended elsewhere stanford clear expect problem configuration however certain stall memory issue occur pipeline annotate file method dispose reference stanford nlp object pipeline e g corelabel processing file keep reference stanford object code beyond method level tip guidance would deeply appreciated
R: Natural Language Processing on Support Vector Machine-TermDocumentMatrix,"<p>I have started working on a project which requires Natural Language Processing and building a model on Support Vector Machine (SVM) in R. </p>

<p>I’d like to generate a Term Document Matrix with all the tokens. </p>

<p>Example:</p>

<pre><code>testset &lt;- c(""From month 2 the AST and total bilirubine were not measured."", ""16:OTHER - COMMENT REQUIRED IN COMMENT COLUMN;07/02/2004/GENOTYPING;SF- genotyping consent not offered until T4."",  ""M6 is 13 days out of the visit window"")
word_ann &lt;- Maxent_Word_Token_Annotator()
sent_ann &lt;- Maxent_Sent_Token_Annotator()
test_annotations &lt;- annotate(testset, list(sent_ann, word_ann))
test_doc &lt;- AnnotatedPlainTextDocument(testset, test_annotations)
sents(test_doc)

[[1]]
 [1] ""From""       ""month""      ""2""          ""the""        ""AST""        ""and""        ""total""     
 [8] ""bilirubine"" ""were""       ""not""        ""measured""   "".""         

[[2]]
 [1] ""16:OTHER""                         ""-""                               
 [3] ""COMMENT""                          ""REQUIRED""                        
 [5] ""IN""                               ""COMMENT""                         
 [7] ""COLUMN;07/02/2004/GENOTYPING;SF-"" ""genotyping""                      
 [9] ""consent""                          ""not""                             
[11] ""offered""                          ""until""                           
[13] ""T4""                               "".""                               

[[3]]
[1] ""M6""     ""is""     ""13""     ""days""   ""out""    ""of""     ""the""    ""visit""  ""window"" 
</code></pre>

<p>And then I generated a TDM:</p>

<pre><code>tdm &lt;- TermDocumentMatrix(as.VCorpus(list(test_doc)))
inspect(tdm)
&lt;&lt;TermDocumentMatrix (terms: 22, documents: 1)&gt;&gt;
Non-/sparse entries: 22/0
Sparsity           : 0%
Maximal term length: 32
Weighting          : term frequency (tf)

                                  Docs
Terms                              NULL
  16:other                            1
  and                                 1
  ast                                 1
  bilirubine                          1
  column;07/02/2004/genotyping;sf-    1
  comment                             2
  consent                             1
  days                                1
  from                                1
  genotyping                          1
  measured                            1
  month                               1
  not                                 2
  offered                             1
  out                                 1
  required                            1
  the                                 2
  total                               1
  until                               1
  visit                               1
  were                                1
  window                              1
</code></pre>

<p>I actually have three documents in the dataset: 
""From month 2 the AST and total bilirubine were not  measured."", 
""16:OTHER - COMMENT REQUIRED IN COMMENT COLUMN;07/02/2004/GENOTYPING;SF- genotyping consent not offered until  T4."",<br>
""M6 is 13 days out of the visit window"" so it should have shown 3 columns of documents.
But I only have one column shown here.</p>

<p>Could anyone please give me some advice on this?</p>

<pre><code>sessionInfo()
    R version 3.3.0 (2016-05-03)
    Platform: x86_64-w64-mingw32/x64 (64-bit)
    Running under: Windows &gt;= 8 x64 (build 9200)

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] tm_0.6-2       openxlsx_3.0.0 magrittr_1.5   RWeka_0.4-28   openNLP_0.2-6  NLP_0.1-9     
[7] rJava_0.9-8   
</code></pre>
",Dataset Preprocessing & Handling,r natural language processing support vector machine termdocumentmatrix started working project requires natural language processing building model support vector machine svm r like generate term document matrix token example generated tdm actually three document dataset month ast total bilirubine measured comment required comment column genotyping sf genotyping consent offered day visit window shown column document one column shown could anyone please give advice
Stanford NER: AbstractSequenceClassifier vs NamedEntityTagAnnotation,"<p><strong>QUESTIONS</strong></p>

<ol>
<li><p>How do I load a custom properties file using AbstractSequenceClassifier?
e.g.,</p>

<p>Master's Degree\tDEGREE</p>

<p>MBA\tDEGREE</p></li>
<li><p>What are the benefits/drawbacks of each approach?(AbstractSequenceClassifier vs NamedEntityTagAnnotation)</p></li>
<li><p>Is there any accessible documentation/tutorial on the internet. I can play with demo code and read javadocs, but a good tutorial would save me and many others a lot of time.</p></li>
</ol>

<p>During my perusal of the  Stanford NER documentation, I have encountered two java examples.</p>

<p><strong>NamedEntityTagAnnotation</strong></p>

<p>The first uses NamedEntityTagAnnotation. This allows me to add my own properties file for training data (using regexner.mapping).</p>

<p>The key code is as follows:
Initialize Pipeline:</p>

<pre><code>   Properties props = new Properties();
   props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, regexner, depparse,  natlog,  openie"");
   props.put(""regexner.mapping"",  ""mypath/mytraineddatacodes.properties"");

   pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>Initialize document:</p>

<pre><code>   Annotation document = new Annotation(pass4);
   pipeline.annotate(document);
</code></pre>

<p>Then access the NER tokens and any other data needed:</p>

<pre><code>List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);


for (CoreMap sentence : sentences)
{

  for (CoreLabel token : sentence.get(TokensAnnotation.class))
  {
     currNeToken = token.get(NamedEntityTagAnnotation.class);

     String word = token.get(TextAnnotation.class);
  }
}
</code></pre>

<p><strong>AbstractSequenceClassifier</strong></p>

<p>This is the method demonstrated in the Stanford NERDemo.java example. IT seems to provide much deeper access to the API, but I don't know how to load my customized properties file of trained data.</p>

<p>Initialize Classifier (which bi-passes the pipeline)</p>

<pre><code>   String serializedClassifier = ""classifiers/english.all.3class.distsim.crf.ser.gz"";

  AbstractSequenceClassifier classifier =    CRFClassifier.getClassifierNoExceptions(serializedClassifier);
</code></pre>

<p>Load the file to analyze:</p>

<pre><code>      byte[] encoded = Files.readAllBytes(p);
  String s = new String(encoded);
     String fileContents = s;
     List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(fileContents);
     for (List&lt;CoreLabel&gt; sentence : out)
     {
        for (CoreLabel word : sentence)
        {
           Log.getLogger().debug(word.word() + '/' + word.get(AnswerAnnotation.class) + ' ');
        }
        System.out.println();
     }
</code></pre>

<p>And your off to the races, except it hasn't loaded my custom properties file for trained data.</p>

<p><strong>QUESTIONS</strong></p>

<ol>
<li><p>How do I load a custom properties file using AbstractSequenceClassifier?
e.g.,</p>

<p>Master's Degree\tDEGREE</p>

<p>MBA\tDEGREE</p></li>
<li><p>What are the benefits/drawbacks of each method?</p></li>
<li><p>Is there any accessible documentation/tutorial on the internet. I can play with demo code and read javadocs, but a good tutorial would save me and many others a lot of time.</p></li>
</ol>
",Dataset Preprocessing & Handling,stanford ner abstractsequenceclassifier v namedentitytagannotation question load custom property file using abstractsequenceclassifier e g master degree tdegree mba tdegree benefit drawback approach abstractsequenceclassifier v namedentitytagannotation accessible documentation tutorial internet play demo code read javadocs good tutorial would save many others lot time perusal stanford ner documentation encountered two java example namedentitytagannotation first us namedentitytagannotation allows add property file training data using regexner mapping key code follows initialize pipeline initialize document access ner token data needed abstractsequenceclassifier method demonstrated stanford nerdemo java example seems provide much deeper access api know load customized property file trained data initialize classifier bi pass pipeline load file analyze race except loaded custom property file trained data question load custom property file using abstractsequenceclassifier e g master degree tdegree mba tdegree benefit drawback method accessible documentation tutorial internet play demo code read javadocs good tutorial would save many others lot time
&quot;Invalid class “dfmSparse” object&quot; error when running dfm function in quanteda R package,"<p>I'm using quanteda, an R package for managing and analyzing text. I am running into trouble with one of its core functons: ""dfm"" which is used for constructing a document frequency matrix. </p>

<p><strong>Running the function</strong></p>

<pre><code># Install packages
packages &lt;- function(x){
  x &lt;- as.character(match.call()[[2]])
  if (!require(x,character.only=TRUE)){
    install.packages(pkgs=x,repos=""http://cran.r-project.org"")
    require(x,character.only=TRUE)
  }
}

packages(""XML"")
packages(""textcat"")
packages(""tm"")
packages(""RTextTools"")
packages(""stringi"")
packages(""proxy"")
packages(""cluster"")
packages(""topicmodels"")
packages(""dplyr"")
packages(""plyr"")
packages(""stringr"")
packages(""quanteda"")
packages(""ggplot2"")
packages(""RWeka"")

# Build textfile using 2nd field text for analysis
txt &lt;- textfile(""myfile.csv"",textField = 2)

# Build object of class corpus from txt
MyCorpus &lt;- corpus(txt)

# Construct a document-frequency matrix
myDfm &lt;- dfm(MyCorpus)
</code></pre>

<p><strong>Code and error returned</strong></p>

<pre><code>Creating a dfm from a corpus ...
   ... indexing 55 documents
   ... tokenizing texts, found 1,730 total tokens
   ... cleaning the tokens, 17 removed entirely
   ... summing tokens by document
   ... indexing 710 feature types
   ... building sparse matrix
Error in validObject(.Object) : 
  invalid class “dfmSparse” object: superclass ""dCsparseMatrix"" not defined in the environment of the object's class
</code></pre>

<p>As you can see, the function is running but then gets stuck just after ""building sparse matrix"". I do not understand this error or how to approach tackling it. Any advice?</p>
",Dataset Preprocessing & Handling,invalid class dfmsparse object error running dfm function quanteda r package using quanteda r package managing analyzing text running trouble one core functons dfm used constructing document frequency matrix running function code error returned see function running get stuck building sparse matrix understand error approach tackling advice
Faster way of finding PMI with count vectorizer,"<p>First of all I am finding a term document matrix, i.e. a term represented in the dimensions of the number of documents.</p>

<p>For finding PMI, I find the count of bigrams say <code>this is</code>, count of individual words in the bigrams <code>this</code> and <code>is</code> and then calcualte as given in <code>(4.0 * math.log(1.0*bigramLength,2))/(1.0*math.log(word0Length,2)*math.log(1.0*word1Length,2))</code></p>

<p>Is there a faster way to achieve this?  I am not much familiar with <code>numpy</code> or <code>scikit</code>.</p>

<p>Please note that I need to find the value for every possible bigram in the list <code>bigramFeatures</code></p>

<pre><code>f4 = ['this is sentence1','not sentence1 becuase this is not sentence1','why this this is called this is sentence1, its always setence1','fourth time this is not sentene1']


Vcount = CountVectorizer(analyzer='word',ngram_range=(1,2),stop_words='english')
countMatrix = Vcount.fit_transform(f4)

# all unigrams and bigrams
feature_names = Vcount.get_feature_names()

#finding all bigrams
featureBigrams = [item for item in Vcount.get_feature_names() if len(item.split()) == 2 ]

#document term matrix
arrays = countMatrix.toarray()

#term document matrix
arrayTrans = arrays.transpose()

from collections import defaultdict
PMIMatrix = defaultdict(dict)

import math
import numpy
print len(featureBigrams)
i = 0
PMIMatrix = defaultdict(dict)
for item in featureBigrams:
    words = item.split()
    bigramLength = len(numpy.where(arrayTrans[feature_names.index(item)] &gt; 0)[0])
    if bigramLength &lt; 2:
        continue
    word0Length = len(numpy.where(arrayTrans[feature_names.index(words[0])] &gt; 0)[0])
    word1Length = len(numpy.where(arrayTrans[feature_names.index(words[1])] &gt; 0)[0])
    try:
        PMIMatrix[words[0]][words[1]] = (4.0 * math.log(1.0*bigramLength,2))/(1.0*math.log(word0Length,2)*math.log(1.0*word1Length,2))
    except:
        print bigramLength,word0Length,word1Length
</code></pre>
",Dataset Preprocessing & Handling,faster way finding pmi count vectorizer first finding term document matrix e term represented dimension number document finding pmi find count bigram say count individual word bigram calcualte given faster way achieve much familiar please note need find value every possible bigram list
Evaluation of an Regression Model with r&#178;,"<p>Using a Bag of Words Model I count the occurences of words per Document (which are Posts from Boards) and create the vector for every Post. Example:</p>

<pre><code>X = [[0,0,0,1,0,3,0,0]
     [0,0,1,0,0,0,1,0]
     [1,0,1,0,2,0,0,0]]

y = [22,35,87]
</code></pre>

<p>In y are the labels/targets to every vector in X (y = the ages of an author).</p>

<p>After training a Regression Model(Linear Regression, Logistic Regression, ...) I use MAE (Mean Absolute Error)
which compares the predicted age with the true age and I get satisfying results.</p>

<p>However i dont quite understand how to use r²:</p>

<p><strong>Input</strong></p>

<p>Is it correct that I have to use Predicted labels and the true Labels
(In my case using ages between 14-65)</p>

<pre><code>r2_score(y_true, y_pred) 
</code></pre>

<p>Isnt that what MAE is for?</p>

<p><strong>Low r²</strong></p>

<p>In this Example the predictions are pretty alright:</p>

<pre><code>y_predicted = [49, 30, 31, 46, 28, 30]
y_true =      [46, 28, 30, 49, 30, 57]
</code></pre>

<p>All but one prediction are close to the true age.
MAE is 6.3 years, but scikit-learns r²scorer shows -0.008</p>

<p>Why is it so bad? Just because of one wrong prediction?</p>

<p><strong>Pearson r</strong></p>

<p>Also, there is Pearson Correlation ""r"": 
Does Pearson r squared equals r²?</p>
",Dataset Preprocessing & Handling,evaluation regression model r using bag word model count occurences word per document post board create vector every post example label target every vector x age author training regression model linear regression logistic regression use mae mean absolute error compare predicted age true age get satisfying result however dont quite understand use r input correct use predicted label true label case using age isnt mae low r example prediction pretty alright one prediction close true age mae year scikit learns r scorer show bad one wrong prediction pearson r also pearson correlation r doe pearson r squared equal r
Extracting parts of JSON from json.loads list in python,"<p>I have ~100k JSON files, each containing JSON which I am looping through to create a bag of words model - very simple. Each JSON file looks like this:</p>

<pre><code>[{""tokens"":[{""word"":""Voices"",""lemma"":""voice"",""pos"":""NNS"",""ner"":""O""},{""word"":""from"",""lemma"":""from"",""pos"":""IN"",""ner"":""O""},{""word"":""Russia"",""lemma"":""Russia"",""pos"":""NNP"",""ner"":""LOCATION""}],""dependencies"":[{""head"":0,""dep"":2,""label"":""prep_from""}]},{""tokens"":[{""word"":""Wednesday"",""lemma"":""Wednesday"",""pos"":""NNP"",""ner"":""DATE""},{""word"":"","",""lemma"":"","",""pos"":"","",""ner"":""DATE""},{""word"":""11"",""lemma"":""11"",""pos"":""CD"",""ner"":""DATE""},
....
</code></pre>

<p>What I need is to extract only the values of the <code>""word""</code>  keys for each file, and store this array in a new file called so each file has an array like:</p>

<p><code>[""Voices"", ""from"", ""Wednesday"",""Russia"", "","" ,""11""...]</code></p>

<p>And also I have a similiar array for all the files put together, stored in <code>../../data/train_jsons/all_words.json</code></p>

<p>However <code>json.loads</code> creates a list for each item not a dict. How can I achieve what I want just from looping through the list for each file, and store these individual arrays of words in new files that maintain the name of the filepath of the json e.g. new files called <code>../../data/train_jsons/words_for_.........json</code>? </p>

<p>Trying to transform to a dict and using the key ""word"" doesn't seem to work per:</p>

<pre><code>for subdir, dirs, files in os.walk('../../data/train_jsons'):
    for file in files:
        filepath = subdir + os.sep + file
        if filepath.endswith("".json""):
            with open(filepath) as data_file:
                data = json.load(data_file)
                dict = dict(itertools.izip_longest(*[iter(data)] * 2, fillvalue=""""))
</code></pre>

<p>Speed is a key factor in my solution.</p>
",Dataset Preprocessing & Handling,extracting part json json load list python k json file containing json looping create bag word model simple json file look like need extract value key file store array new file called file ha array like also similiar array file put together stored however creates list item dict achieve want looping list file store individual array word new file maintain name filepath json e g new file called trying transform dict using key word seem work per speed key factor solution
"How can I open, read and write Non-english text file (i.e Gujarati) in python 3.5?","<p>I am newbie to the python programming, so I do not know how basic this question is. I Want to process a Gujarati text file in python 3.5. When I tried to execute this block of code, it gave me an error. How can i fix this error?</p>

<pre><code>import tkinter.filedialog
import fileinput
import tkinter

filename1 = tkinter.filedialog.askopenfile()


my_file = open(filename1, ""r"", encoding= ""utf-16"")

content = my_file.read()

print(content)
</code></pre>

<p>Error:</p>

<pre><code>Traceback (most recent call last):
File ""D:\PhD\python workspace\guj.py"", line 8, in &lt;module&gt;
    my_file = open(filename1, ""r"", encoding= ""utf-16"")
TypeError: invalid file: &lt;_io.TextIOWrapper name='D:/PhD/python    workspace/text files/Gujarati.txt' mode='r' encoding='cp1252'&gt;
</code></pre>
",Dataset Preprocessing & Handling,open read write non english text file e gujarati python newbie python programming know basic question want process gujarati text file python tried execute block code gave error fix error error
reading Chinese csvfile in python,"<p>I'm trying to read a csv file with the following code but it still can't print Chinese</p>

<pre><code>#!/usr/bin/python
# -*- coding: utf-8 -*-

import pandas as pd 

df = pd.read_csv ('weibo_status.csv') status = df[1:]
#print (df.head)
</code></pre>

<p>I think this might be the problem that the characters are in a data frame, because the following code works fine in my system  (python 2.7, editor: pycharm)</p>

<pre><code>#!/usr/bin/python
# -*- coding: utf-8 -*-

import jieba

seg_list = jieba.cut(""我来到北京清华大学"", cut_all=True) print(""Full Mode: "" + ""/ "".join(seg_list))
</code></pre>

<p>weibo_status.csv</p>

<pre><code>userid     status
1          我今天吃饭了
2          吃了水果
3          今天感冒了
</code></pre>
",Dataset Preprocessing & Handling,reading chinese csvfile python trying read csv file following code still print chinese think might problem character data frame following code work fine system python editor pycharm weibo status csv
how to exclude sentences containing specific word,"<p>I m reading a sentence from excel(containing bio data) file and want to extract the organizations where they are working. The file also contains sentences which specifies where the person is studying.
ex :</p>

<ul>
<li>i m studying in 'x' instition(university)</li>
<li>i m student in 'y' college</li>
</ul>

<p>i want to skip these type of sentences. </p>

<p>I am using regular expression to match these sentences, and if its related to student then skip the part, and only other lines i want write in a separate excel file. </p>

<p>my code is as below..  </p>

<p>csvdata = pandas.read_csv(""filename.csv"","","");
    for data in csvdata:</p>

<pre><code>        regEX=re.compile('|'.join([r'\bstudent\b',r'\bstudy[ing]\b']),re.I)
        matched_data=re.match(regEX,data)   
        if matched_data is not None:
            continue

        else:
            ## write the sentence to excel
</code></pre>

<p>But, when i check the newly created excel file, it still contains the sentences that contain 'student', 'study'.
How regular expression can be modified to get the result.</p>
",Dataset Preprocessing & Handling,exclude sentence containing specific word reading sentence excel containing bio data file want extract organization working file also contains sentence specifies person studying ex studying x instition university student college want skip type sentence using regular expression match sentence related student skip part line want write separate excel file code csvdata panda read csv filename csv data csvdata check newly created excel file still contains sentence contain student study regular expression modified get result
"PANDAS DROP ROWS based on filtered items, my solution - not satisfied","<p>I am working on cleaning a list domain names.</p>

<p>I want to drop certain rows that ""fit"" a criteria. I have succeeded in identifying the first criteria, the second will be easy to do.</p>

<p>However, I cannot drop the rows. I have tried several solution but the best I have is the following.</p>

<pre><code>from wordsegment import segment
import pandas as pd

def assignname():
    dfr = pd.read_csv('data.net.date.csv')

    for domainwtld in dfr.domain:
        dprice = dfr.price
        domainwotld = domainwtld.replace("".net"", """")
        seperate = wordsegment.segment(domainwotld)
        dlnt = (min(seperate, key=len))
        slnt = len(dlnt)
        if slnt &lt;= 1:
            baddomains = domainwtld
            a = dfr.loc[dfr['domain'] &lt; (baddomains)]
            print (a)
</code></pre>

<p>When I run this code, I receive a output that after dropping the first item in  ""baddomains"", prints the entire item in ""dfr"". It does this until the loop is complete.</p>

<p>How can I can filter the ""original"" csv file based on baddomains?</p>
",Dataset Preprocessing & Handling,panda drop row based filtered item solution satisfied working cleaning list domain name want drop certain row fit criterion succeeded identifying first criterion second easy however drop row tried several solution best following run code receive output dropping first item baddomains print entire item dfr doe loop complete filter original csv file based baddomains
Cannot load OpenIE model when using the open source version of CoreNLP,"<p>I downloaded the source code of coreNLP from <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow"">this page</a> and the model recommended in the README file. I create a new project in eclipse and tried to run openie by it return the following exception:</p>

<pre><code>Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Could not load clause splitter model at edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz
    at edu.stanford.nlp.naturalli.OpenIE.&lt;init&gt;(OpenIE.java:201)
    at edu.stanford.nlp.pipeline.AnnotatorImplementations.openie(AnnotatorImplementations.java:272)
    at edu.stanford.nlp.pipeline.AnnotatorFactories$20.create(AnnotatorFactories.java:654)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:89)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:403)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:142)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:138)
    at edu.stanford.nlp.naturalli.demo.Demo.main(Demo.java:37)
Caused by: java.io.InvalidClassException: edu.stanford.nlp.naturalli.ClauseSplitterSearchProblem$8; local class incompatible: stream classdesc serialVersionUID = 4145523451314579506, local class serialVersionUID = -7360029270983346606
    at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:621)
    at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1623)
    at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
    at edu.stanford.nlp.io.IOUtils.readObjectFromURLOrClasspathOrFileSystem(IOUtils.java:325)
    at edu.stanford.nlp.naturalli.ClauseSplitter.load(ClauseSplitter.java:283)
    at edu.stanford.nlp.naturalli.OpenIE.&lt;init&gt;(OpenIE.java:196)
    ... 7 more
</code></pre>
",Dataset Preprocessing & Handling,load openie model using open source version corenlp downloaded source code corenlp page model recommended readme file create new project eclipse tried run openie return following exception
How can I load a .conll file into an Annotation object with Corenlp?,"<p>I have some files outputted from CoreNLP in <code>.conll</code> format, and I want to deserialize them into an <code>Annotation</code> object. Does CoreNLP provide a <code>CONLL-X DocumentReader</code> method to transform <code>.conll</code> file to an <code>Annotation</code> object or do I have to create my own DocumentReader?</p>
",Dataset Preprocessing & Handling,load conll file annotation object corenlp file outputted corenlp format want deserialize object doe corenlp provide method transform file object create documentreader
JAVA: How use Gazettes with Stanford NLP?,"<p>I read this <a href=""http://www-nlp.stanford.edu/software/crf-faq.shtml#gazette"" rel=""nofollow"">faq</a> but i not understand. I try with this code:</p>

<pre><code>   Properties pp=new Properties();  
   pp.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse"");
   pp.put(""ner.useSUTime"",""false"");

   pp.put(""useGazettes"",""true"");
   pp.put(""gazette"",""C:\\gaz.txt"");

   StanfordCoreNLP s=new StanfordCoreNLP(pp);
</code></pre>

<p>This is String: ""Dan became a member of the Music friends association in 2008""</p>

<p>the gazette file is:</p>

<pre><code>  CLASS Music friends association 
</code></pre>

<p>But ""Music friends association"" is not recognized by NER.</p>

<p>Where am I wrong?</p>
",Dataset Preprocessing & Handling,java use gazette stanford nlp read faq understand try code string dan became member music friend association gazette file music friend association recognized ner wrong
nlp python clean text from code,"<p>I need to clean a lot of text files from useless code or exception, in order to make some  text analysis, for example:</p>
<h1>start-text: 7001</h1>
<ol>
<li>Add a working set</li>
<li>Search for something in that working set</li>
<li>Remove the working set</li>
<li>Search via context menu</li>
</ol>
<p>==&gt;</p>
<p>Log: Mon Dec 17 17:23:54 GMT+01:00 2001
4 org.eclipse.ui 0 java.util.ConcurrentModificationException</p>
<pre><code>java.util.ConcurrentModificationException
    at java.util.AbstractList$Itr.checkForComodification(AbstractList.java(Compiled
Code))
    at java.util.AbstractList$Itr.next(AbstractList.java(Compiled Code))
    at

org.eclipse.jdt.internal.ui.search.JavaSearchSubGroup.fill(JavaSearchSubGroup.java:30)
    at org.eclipse.jdt.internal.ui.search.JavaSearchGroup.fill(JavaSearchGroup.java:51)
    at org.eclipse.jdt.internal.ui.actions.ContextMenuGroup.add(ContextMenuGroup.java:25)
    at
org.eclipse.jdt.internal.ui.packageview.PackageExplorerPart.menuAboutToShow(PackageExplorerPart.java:498)
    at org.eclipse.jface.action.MenuManager.fireAboutToShow(MenuManager.java:220)
    at org.eclipse.jface.action.MenuManager.handleAboutToShow(MenuManager.java:253)
    at org.eclipse.jface.action.MenuManager.access$0(MenuManager.java:250)
    at org.eclipse.jface.action.MenuManager$1.menuShown(MenuManager.java:280)
</code></pre>
<p>&lt;==</p>
<h1>end-text: 7001</h1>
<p>or:</p>
<h1>start-text: 7019</h1>
<p>20011211
Ran the following compilation unit under the debugger with the breakpoint
indicated. To get Windows to hit the breakpoint, you have to have the right dl
and run an accessibility client. If you cannot replicate this problem with a
simpler little example, I can walk you through the steps to do this.
The only thing different about this CU is that it contains a non-public class
as well as a public class. When I hit the breakpoint in the debugger, I got a
dialog that told me that it can't find the source for the non-public class. The
dialog is very persistent - I have told it OK and Cancel, but it keeps coming
back. Even if I switch to the Java perspective, I still get the nagging dialog
. If I kill the process, the dialog does not come back. But the point is
that the debugger should be able to see the source for this class - it is right
in my eclipse workspace. It isn't even hidden in some jar somewhere - it's very
visible. I suspect that it's the non-public class thing that is confusing the
source lookup. If it helps any, I will attach the dialog. Here's the code:</p>
<p>==&gt;</p>
<pre><code>package test;

import org.eclipse.swt.*;
import org.eclipse.swt.graphics.*;
import org.eclipse.swt.widgets.*;
import org.eclipse.swt.layout.*;
import org.eclipse.swt.events.*;
import org.eclipse.swt.internal.win32.*;
import org.eclipse.swt.internal.ole.win32.*;
import org.eclipse.swt.ole.win32.*;

public class AccessibilityTest {
    static Display display;
    static Shell shell;
    static FakeWidget fakeWidget;

    public static void main(String[] args) {
        display = new Display();
        shell = new Shell(display);
        shell.setLayout(new GridLayout());
        shell.setText(&quot;Accessibility Test&quot;);

        fakeWidget = new FakeWidget(shell, SWT.MULTI);
        fakeWidget.setLayoutData(new GridData(GridData.FILL_BOTH));
        shell.setSize(140, 110);
        shell.open();
        while (!shell.isDisposed()) {
            if (!display.readAndDispatch())
                display.sleep();
        }
    }
}



private static GUID IIDFromString(String lpsz) {
    char[] buffer = (lpsz + &quot;\0&quot;).toCharArray();
    GUID lpiid = new GUID();
    if (COM.IIDFromString(buffer, lpiid) == COM.S_OK)
        return lpiid;
    return null;
}
</code></pre>
<p>&lt;==</p>
<h1>end-text: 7019</h1>
<p>The results must be:</p>
<h1>start-text: 7001</h1>
<ol>
<li>Add a working set</li>
<li>Search for something in that working set</li>
<li>Remove the working set</li>
<li>Search via context menu</li>
</ol>
<h1>end-text: 7001</h1>
<p>and</p>
<h1>start-text: 7019</h1>
<p>20011211
Ran the following compilation unit under the debugger with the breakpoint
indicated. To get Windows to hit the breakpoint, you have to have the right dl
and run an accessibility client. If you cannot replicate this problem with a
simpler little example, I can walk you through the steps to do this.
The only thing different about this CU is that it contains a non-public class
as well as a public class. When I hit the breakpoint in the debugger, I got a
dialog that told me that it can't find the source for the non-public class. The
dialog is very persistent - I have told it OK and Cancel, but it keeps coming
back. Even if I switch to the Java perspective, I still get the nagging dialog
. If I kill the process, the dialog does not come back. But the point is
that the debugger should be able to see the source for this class - it is right
in my eclipse workspace. It isn't even hidden in some jar somewhere - it's very
visible. I suspect that it's the non-public class thing that is confusing the
source lookup. If it helps any, I will attach the dialog. Here's the code:</p>
<h1>end-text: 7019</h1>
<p>in above cases the useless text is between &quot;==&gt;&quot; code &quot;&lt;==&quot; (the arrows aren't in the text)
...I'm using python now... But I need a tool that clean all the text from code or exceptions... does it exist? because I think that could be useless and wrong to make nlp in these dirty texts...</p>
",Dataset Preprocessing & Handling,nlp python clean text code need clean lot text file useless code exception order make text analysis example start text add working set search something working set remove working set search via context menu log mon dec gmt org eclipse ui java util concurrentmodificationexception end text start text ran following compilation unit debugger breakpoint indicated get window hit breakpoint right dl run accessibility client replicate problem simpler little example walk step thing different cu contains non public class well public class hit breakpoint debugger got dialog told find source non public class dialog persistent told ok cancel keep coming back even switch java perspective still get nagging dialog kill process dialog doe come back point debugger able see source class right eclipse workspace even hidden jar somewhere visible suspect non public class thing confusing source lookup help attach dialog code end text result must start text add working set search something working set remove working set search via context menu end text start text ran following compilation unit debugger breakpoint indicated get window hit breakpoint right dl run accessibility client replicate problem simpler little example walk step thing different cu contains non public class well public class hit breakpoint debugger got dialog told find source non public class dialog persistent told ok cancel keep coming back even switch java perspective still get nagging dialog kill process dialog doe come back point debugger able see source class right eclipse workspace even hidden jar somewhere visible suspect non public class thing confusing source lookup help attach dialog code end text case useless text code arrow text using python need tool clean text code exception doe exist think could useless wrong make nlp dirty text
how could I use complete penn treebank dataset inside python/nltk,"<p>I'm trying to learn using <a href=""http://www.nltk.org/"" rel=""noreferrer"">NLTK</a> package in python. In particular, I need to use penn tree bank dataset in NLTK. As far as I know, If I call <code>nltk.download('treebank')</code> I can get the 5% of the dataset. However, I have a complete dataset in tar.gz file and I want to use it. In   <a href=""http://www.nltk.org/howto/corpus.html#parsed-corpora"" rel=""noreferrer"">here</a> it is said that: </p>

<blockquote>
  <p>If you have access to a full installation of the Penn Treebank, NLTK
  can be configured to load it as well. Download the ptb package, and in
  the directory nltk_data/corpora/ptb place the BROWN and WSJ
  directories of the Treebank installation (symlinks work as well). Then
  use the ptb module instead of treebank:</p>
</blockquote>

<p>So, I opened the python from terminal, imported nltk and typed <code>nltk.download('ptb')</code> . With this command, ""ptb"" directory has been created under my <code>~/nltk_data</code> directory. At the end, now I have <code>~/nltk_data/ptb</code> directory. Inside there, as suggested in the link I gave above, I've put my dataset folder. So this is my final directory hierarchy.</p>

<pre><code>    $: pwd
    $: ~/nltk_data/corpora/ptb/WSJ
    $: ls
    $:00  02  04  06  08  10  12  14  16  18  20  22  24
      01  03  05  07  09  11  13  15  17  19  21  23  merge.log
</code></pre>

<p>Inside all of the folders from 00 to 24, there are many  <code>.mrg</code> files such as <code>wsj_0001.mrg , wsj_0002.mrg</code>  and so on.</p>

<p>Now, Lets return my question. Again, according to <a href=""http://www.nltk.org/howto/corpus.html#parsed-corpora"" rel=""noreferrer"">here</a> :</p>

<p>I should be able to obtain the file ids if I write the followings:</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import ptb
&gt;&gt;&gt; print(ptb.fileids()) # doctest: +SKIP
['BROWN/CF/CF01.MRG', 'BROWN/CF/CF02.MRG', 'BROWN/CF/CF03.MRG', 'BROWN/CF/CF04.MRG', ...]
</code></pre>

<p>Unfortunately, when I type <code>print(ptb.fileids())</code> I got empty array.</p>

<pre><code>&gt;&gt;&gt; print(ptb.fileids())
[]
</code></pre>

<p>Is there anyone who could help me ? </p>

<p><strong>EDIT</strong>
here is the content of my ptb directory and some of allcats.txt file : </p>

<pre><code>   $: pwd
    $: ~/nltk_data/corpora/ptb
    $: ls
    $: allcats.txt  WSJ
    $: cat allcats.txt
    $: WSJ/00/WSJ_0001.MRG news
    WSJ/00/WSJ_0002.MRG news
    WSJ/00/WSJ_0003.MRG news
    WSJ/00/WSJ_0004.MRG news
    WSJ/00/WSJ_0005.MRG news

    and so on ..
</code></pre>
",Dataset Preprocessing & Handling,could use complete penn treebank dataset inside python nltk trying learn using nltk package python particular need use penn tree bank dataset nltk far know call get dataset however complete dataset tar gz file want use said access full installation penn treebank nltk configured load well download ptb package directory nltk data corpus ptb place brown wsj directory treebank installation symlinks work well use ptb module instead treebank opened python terminal imported nltk typed command ptb directory ha created directory end directory inside suggested link gave put dataset folder final directory hierarchy inside folder many file let return question according able obtain file id write following unfortunately type got empty array anyone could help edit content ptb directory allcats txt file
Computation of clusters,"<p>I am testing out a few clustering algorithms on a dataset  of text documents (with word frequencies as features). Running some of the methods of <a href=""http://scikit-learn.org/stable/modules/clustering.html"" rel=""nofollow"">Scikit Learn Clustering</a> one after the other, below is how long they take on ~ 50,000 files with 26 features per file. There are big differences in how long each take to converge that get more extreme the more data I put in; some of them (e.g. MeanShift) just stop working after the dataset grows to a certain size. </p>

<p>(Times given below are totals from the start of the script, i.e. KMeans took 0.004 minutes, Meanshift (2.56 - 0.004) minutes, etc. )</p>

<pre><code>shape of input: (4957, 26)

KMeans:    0.00491824944814
MeanShift:     2.56759268443
AffinityPropagation:     4.04678163528
SpectralClustering:     4.1573699673
DBSCAN:     4.16347868443
Gaussian:     4.16394021908
AgglomerativeClustering:     5.52318491936
Birch:     5.52657626867
</code></pre>

<p>I know that some clustering algorithms are inherently more computing intensive (e.g. the chapter <a href=""https://www-users.cs.umn.edu/~kumar/dmbook/ch8.pdf"" rel=""nofollow"">here</a> outlines that Kmeans' demand is linear to number of data points while hierarchical models are <em>O(m<sup>2</sup>logm)</em>). 
So I was wondering</p>

<ul>
<li>How can I determine how many data points each of these algorithms can
handle; and are the number of input files / input features equally
relevant in this equation? </li>
<li>How much does the computation intensity depend on the clustering
settings -- e.g. the distance metric in Kmeans or the <em>e</em> in DBSCAN?</li>
<li>Does clustering success influence computation time? Some algorithms
such as DBSCAN finish very quickly - mabe because they don't find
any clustering in the data; Meanshift does not find clusters either
and still takes forever. (I'm using the default settings here). Might
that change drastically once they discover structure in the data?</li>
<li>How much is raw computing power a limiting factor for these kind of
algorithms? Will I be able to cluster ~ 300,000 files with ~ 30
features each on a regular desktop computer? Or does it make sense to
use a computer cluster for these kind    of things?</li>
</ul>

<p>Any help is greatly appreciated! The tests were run on an Mac mini, 2.6 Ghz, 8 GB. The data input is a <a href=""http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.array.html"" rel=""nofollow"">numpy array</a>.  </p>
",Dataset Preprocessing & Handling,computation cluster testing clustering algorithm dataset text document word frequency feature running method scikit learn clustering one long take file feature per file big difference long take converge get extreme data put e g meanshift stop working dataset grows certain size time given total start script e kmeans took minute meanshift minute etc know clustering algorithm inherently computing intensive e g chapter outline kmeans demand linear number data point hierarchical model logm wa wondering determine many data point algorithm handle number input file input feature equally relevant equation much doe computation intensity depend clustering setting e g distance metric kmeans e dbscan doe clustering success influence computation time algorithm dbscan finish quickly mabe find clustering data meanshift doe find cluster either still take forever using default setting might change drastically discover structure data much raw computing power limiting factor kind algorithm able cluster file feature regular desktop computer doe make sense use computer cluster kind thing help greatly appreciated test run mac mini ghz gb data input numpy array
How to Preserve Original Line Numbering in the Output of Stanford CoreNLP?,"<p>Text corpora are often distributed as large files containing specific documents on each new line.  For instance, I have a file with 10 million product reviews, one per line, and each review contains multiple sentences.  </p>

<p>When processing such files with Stanford CoreNLP, using the command line, for instance</p>

<pre><code>java -cp ""*"" -Xmx16g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma -file test.txt
</code></pre>

<p>the output, whether in text or xml format, will number all sentences from 1 to <code>n</code>, ignoring the original line numbering that separates the documents.  </p>

<p>I would like to keep track of the original file's line numbering (e.g. in xml format, to have an output tree like <code>&lt;original_line id=1&gt;</code>, then <code>&lt;sentence id=1&gt;</code>, then <code>&lt;token id=1&gt;</code>).  Or else, to be able to reset the numbering of sentences at the start of each new line in the original file. </p>

<p>I have tried the answer to a <a href=""https://stackoverflow.com/questions/12140683/stanford-pos-tagger-how-to-preserve-newlines-in-the-output"">similar question</a> about Stanford's POS tagger, without success.  Those options do not keep track of the original line numbers.  </p>

<p>A quick solution could be to split the original file in multiple files, then processing each of them with CoreNLP and the <code>-filelist</code> input option.  However, for large files with millions of documents, creating millions of individual files just to preserve the original line/document numbering seems inefficient.     </p>

<p>I suppose it would be possible to modify the source code of Stanford CoreNLP, but I am unfamiliar with Java.  </p>

<p>Any solution to preserve the original line numbering in the output would be very helpful, whether through the command line or by showing an example Java code that would achieve that.       </p>
",Dataset Preprocessing & Handling,preserve original line numbering output stanford corenlp text corpus often distributed large file containing specific document new line instance file million product review one per line review contains multiple sentence processing file stanford corenlp using command line instance output whether text xml format number sentence ignoring original line numbering separate document would like keep track original file line numbering e g xml format output tree like else able reset numbering sentence start new line original file tried answer href question stanford po tagger without success option keep track original line number quick solution could split original file multiple file processing corenlp input option however large file million document creating million individual file preserve original line document numbering seems inefficient suppose would possible modify source code stanford corenlp unfamiliar java solution preserve original line numbering output would helpful whether command line showing example java code would achieve
Error extracting noun in R using KoNLP,"<p>I tried to extract noun for R. When using program R, an error appears. I wrote the following code:</p>

<pre><code>setwd(""C:\\Users\\kyu\\Desktop\\1-1file"")
library(KoNLP)
useSejongDic()

txt &lt;- readLines(file(""1_2000.csv""))
nouns &lt;- sapply(txt, extractNoun, USE.NAMES = F)
</code></pre>

<p>and, the error appear like this: </p>

<pre><code>setwd(""C:\\Users\\kyu\\Desktop\\1-1file"")
library(KoNLP)
useSejongDic() 

Backup was just finished!
87007 words were added to dic_user.txt. 

txt &lt;- readLines(file(""1_2000.csv""))
nouns &lt;- sapply(txt, extractNoun, USE.NAMES = F) 
</code></pre>

<blockquote>
  <p>java.lang.ArrayIndexOutOfBoundsException Error in
  <code>Encoding&lt;-</code>(<code>*tmp*</code>, value = ""UTF-8"") :    a character vector
  argument expected</p>
</blockquote>

<p>Why is this happening? I load <a href=""https://drive.google.com/file/d/0Bw40xYYVtv3nY0owOW5DMDNrZmM/view?usp=sharing%22"" rel=""nofollow"">1_2000.csv</a> file, there are 2000 lines of data. Is this too much data? How do I extract noun like large data file? I use R 3.2.4 with RStudio, and Excel version 2016 on Windows 8.1 x64.</p>
",Dataset Preprocessing & Handling,error extracting noun r using konlp tried extract noun r using program r error appears wrote following code error appear like java lang arrayindexoutofboundsexception error value utf character vector argument expected happening load csv file line data much data extract noun like large data file use r rstudio excel version window x
How do I load a *.corpus file into Python?,"<p>I am trying to load a *.corpus file into Python in order to perform a few NLP tasks. But I do not seem to be able to find any information on how to do this. </p>
",Dataset Preprocessing & Handling,load corpus file python trying load corpus file python order perform nlp task seem able find information
Using R Studio to pull phrases from a CSV,"<p>Using R I would like to take a single CSV and pull out the most common two and three word phrases.  I've been searching Google and Stackoverflow and could not find a simple way to do this.</p>

<p>I know how to read a CSV into R but I have not found out how to extract the data into the appropriate datatype and perform operations on to get what I am looking for.</p>

<p>Requirements:</p>

<ol>
<li>Remove all non alpha numeric text from the CSV</li>
<li>Replace words using a synonym list</li>
<li>Remove words with no meaning (at, the, etc)</li>
<li>Get a count of the common phrases for both two word phrases and three word phrases</li>
<li>Make all text lowercase</li>
</ol>

<p>Also, what data types are best suited for this type of analysis?  dataframe?  tm? corpus?  etc?</p>

<pre><code>My_SRs &lt;- read.csv(""C:/example_folder/username/Documents/my_data.csv"")
</code></pre>

<p>Thanks in advance!</p>
",Dataset Preprocessing & Handling,using r studio pull phrase csv using r would like take single csv pull common two three word phrase searching google stackoverflow could find simple way know read csv r found extract data appropriate datatype perform operation get looking requirement remove non alpha numeric text csv replace word using synonym list remove word meaning etc get count common phrase two word phrase three word phrase make text lowercase also data type best suited type analysis dataframe tm corpus etc thanks advance
Multi-Threaded NLP with Spacy pipe,"<p>I'm trying to apply Spacy NLP (Natural Language Processing) pipline to a big text file like Wikipedia Dump. Here is my code based on Spacy's <a href=""https://spacy.io/docs"" rel=""noreferrer"">documentation</a> example:</p>

<pre><code>from spacy.en import English

input = open(""big_file.txt"")
big_text= input.read()
input.close()

nlp= English()    

out = nlp.pipe([unicode(big_text, errors='ignore')], n_threads=-1)
doc = out.next() 
</code></pre>

<p>Spacy applies all nlp operations like POS tagging, Lemmatizing and etc all at once. It is like a pipeline for NLP that takes care of everything you need in one step. Applying pipe method tho is supposed to make the process a lot faster by multithreading the expensive parts of the pipeline. But I don't see big improvement in speed and my CPU usage is around 25% (only one of 4 cores working). I also tried to read the file in multiple chuncks and increase the batch of input texts:</p>

<pre><code>out = nlp.pipe([part1, part2, ..., part4], n_threads=-1)
</code></pre>

<p>but still the same performance. Is there anyway to speed up the process? I suspect that OpenMP feature should be enabled compiling Spacy to utilize multi-threading feature. But there is no instructions on how to do it on Windows. </p>
",Dataset Preprocessing & Handling,multi threaded nlp spacy pipe trying apply spacy nlp natural language processing pipline big text file like wikipedia dump code based spacy documentation example spacy applies nlp operation like po tagging lemmatizing etc like pipeline nlp take care everything need one step applying pipe method tho supposed make process lot faster multithreading expensive part pipeline see big improvement speed cpu usage around one core working also tried read file multiple chuncks increase batch input text still performance anyway speed process suspect openmp feature enabled compiling spacy utilize multi threading feature instruction window
Performance Issues With Natural Language Processing in MATLAB,"<p>For a class, I'm processing raw text documents (our examples include novels that can be downloaded from the Gutenberg project) and turning them into a dictionary data structure. For each word, I need to know which paragraph(s) it occurs in, and how many times in each paragraph.</p>

<p>The procedure can be divided roughly as</p>

<ol>
<li>Break document into words, removing whitespace, commas, periods, etc.</li>
<li>For each document, iterate over the words. Look up in dictionary. If the word exists, update its entry. If it doesn't exist, create a new entry.</li>
</ol>

<p>I'm doing this MATLAB because the rest of my work is in MATLAB and I didn't want to have to deal with another language/environment. It turns out MATLAB has some pretty good string processing functions. However, I'm disturbed by how slow my code is running. The first part I mentioned above is not a problem; I use a <code>parfor</code> loop and it goes pretty fast. The second part is where my problem is.</p>

<p>My initial attempt at creating this dictionary was to use structs, a MATLAB built-in data type. The idea was to create a struct called <code>dictionary</code>, whose field names are the actual words, e.g., <code>dictionary.the</code> or <code>dictionary.gnome</code> for the words 'the' and 'gnome.' This worked provided the word was a valid field name (it couldn't be a contraction, for instance). But it ran slow, so I sought a different solution. My next attempt was to use a <code>containers.Map</code>, which is MATLAB's equivalent of a hash map or dictionary object. (One of my coworkers informed me that MATLAB is somewhat inefficient at looking up field names of structs, whereas the hash table has O(1) lookup time.) However, making this substitution actually <em>slowed down</em> my performance!</p>

<p>At this point, I feel I've some pretty substantial attempts at optimizing my code and am starting to wonder if MATLAB is really a sensible choice here. I'm basically trying to figure out whether the slowness is a product of MATLAB or of me being a terrible programmer (normally MATLAB is quite fast when it comes to linear algebra, arrays, and matrices). Rather than have someone read over my code, I'd greatly appreciate whatever feedback the community has to offer on either of the following:</p>

<ul>
<li><p>Does anyone do this type of thing (i.e., language processing) in MATLAB or a similar language, such as Python? If so, I'd like to try to benchmark some of my timings. (I've got a MacBook, 2.8 GHz processor, and I'm currently getting about 10-20K words per second.)</p></li>
<li><p>Is it possible I'd get better results if I switched to a compiled language like Java or C++? Ballpark estimate, what kind of improvement?</p></li>
</ul>
",Dataset Preprocessing & Handling,performance issue natural language processing matlab class processing raw text document example include novel downloaded gutenberg project turning dictionary data structure word need know paragraph occurs many time paragraph procedure divided roughly break document word removing whitespace comma period etc document iterate word look dictionary word exists update entry exist create new entry matlab rest work matlab want deal another language environment turn matlab ha pretty good string processing function however disturbed slow code running first part mentioned problem use loop go pretty fast second part problem initial attempt creating dictionary wa use structs matlab built data type idea wa create struct called whose field name actual word e g word gnome worked provided word wa valid field name contraction instance ran slow sought different solution next attempt wa use matlab equivalent hash map dictionary object one coworkers informed matlab somewhat inefficient looking field name structs whereas hash table ha lookup time however making substitution actually slowed performance point feel pretty substantial attempt optimizing code starting wonder matlab really sensible choice basically trying figure whether slowness product matlab terrible programmer normally matlab quite fast come linear algebra array matrix rather someone read code greatly appreciate whatever feedback community ha offer either following doe anyone type thing e language processing matlab similar language python like try benchmark timing got macbook ghz processor currently getting k word per second possible get better result switched compiled language like java c ballpark estimate kind improvement
How to prepare feature vectors for text classification when the words in the text is not frequently repeating?,"<p>I need to perform the text classification on set of emails. But all the words in my text are thinly sparse i.e frequency of each word with respect to all the documents are very less. words are not that much frequently repeating. Since to train the classifiers I think document term matrix with frequency as weightage is not suitable. Can you please suggest me what kind of other methods I need to use . </p>

<p>Thanks</p>
",Dataset Preprocessing & Handling,prepare feature vector text classification word text frequently repeating need perform text classification set email word text thinly sparse e frequency word respect document le word much frequently repeating since train classifier think document term matrix frequency weightage suitable please suggest kind method need use thanks
[simple issue]: import .net file (word/occurences) into cytoscape...which attributes are which?,"<p>I took a corpus of text and put it into VosViewer to create a network for me. When I import this .net file into gephi, it works fine: I get a semantic network. Though I'm a little stuck for what attributes to select to import into cytoscape. Here is a CSV file of the network (.net import wouldn't work), and I just need to know which column to select as what. </p>

<p>Each column has the option of being imported as one of the following:</p>

<ul>
<li>Source Node</li>
<li>Interaction type</li>
<li>Edge Attribute</li>
<li>Source Node Attribute</li>
<li>Target Node Attribute</li>
</ul>

<p><a href=""https://i.sstatic.net/fHBnO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fHBnO.png"" alt=""CSV File Attributes""></a></p>
",Dataset Preprocessing & Handling,simple issue import net file word occurences cytoscape attribute took corpus text put vosviewer create network import net file gephi work fine get semantic network though little stuck attribute select import cytoscape csv file network net import work need know column select column ha option imported one following source node interaction type edge attribute source node attribute target node attribute
Performing clustering on a text column in a data frame,"<p>I have a dataframe (df) which has two column called ""id"" and ""text""</p>

<pre><code>id  text
1   TV
2   Tv
3   T.V
4   Radio/TV
5   Car
6   CAR
7   car 
</code></pre>

<p>I want to tag/flag the similar types of rows in the ""text"" column</p>

<p>Expected Output:</p>

<pre><code>id  text     type
1   TV       tv
2   Tv       tv
3   T.V      tv
4   Radio/TV tv
5   Car      car
6   CAR      car
7   car      car
</code></pre>

<p>I found the following while researching, I get the logic here and it executes too, but I can't figure out how I can recreate what I have in my mind (expected output)</p>

<pre><code># Importing the library
library(tm)

# Importing the data
corpus.tmp&lt;-Corpus(VectorSource(df$text))

#Cleaning up
corpus.tmp&lt;- tm_map(corpus.tmp,removePunctuation)
corpus.tmp&lt;- tm_map(corpus.tmp, stripWhitespace)
corpus.tmp &lt;- tm_map(corpus, content_transformer(tolower))
corpus.tmp&lt;- tm_map(corpus.tmp, removeWords, stopwords(""english""))

# Document Matrix
TDM &lt;- TermDocumentMatrix(corpus.tmp)
inspect(TDM)

tdm_tfxidf&lt;-weightTfIdf(TDM)

# Converting to matrix
m&lt;- as.matrix(tdm_tfxidf)
rownames(m)&lt;- 1:nrow(m)

norm_eucl&lt;- function(m)
  m/apply(m,1,function(x) sum(x^2)^.5)

m_norm&lt;-norm_eucl(m)

# Performing K means clustering
results&lt;-kmeans(m_norm,5,5)
</code></pre>
",Dataset Preprocessing & Handling,performing clustering text column data frame dataframe df ha two column called id text want tag flag similar type row text column expected output found following researching get logic executes figure recreate mind expected output
Performing Metadata extractions and Tagging in r and rapid minner,"<p>I have a problem statement to perform a matadata extraction from the power presentation files and perform document tagging using R or Rapid minner.So i need help in how to read the ppt files in both the tools and then perform the text processing.</p>
",Dataset Preprocessing & Handling,performing metadata extraction tagging r rapid minner problem statement perform matadata extraction power presentation file perform document tagging using r rapid minner need help read ppt file tool perform text processing
How I can optimize my PHP script to get phonetical readings of Japanese sentences from Yahoo! Japan API?,"<p>I wrote a PHP script which reads Japanese sentences from file, get the phonetical reading of each sentence using Yahoo! Japan API and writes them to output file. But the script is incredibly slow, it has processed only 50,000 sentences in the last 12 hours on the Apache running on my Mac OS X. Is the call to API the main bottleneck? How can I optimize it? Should I use a language other than PHP? Thanks!</p>

<p>Here's how the first 4 lines of the input (examples-utf.utf) file look like:</p>

<pre><code>A: ムーリエルは２０歳になりました。 Muiriel is 20 now.#ID=1282_4707
B: は 二十歳(はたち){２０歳} になる[01]{になりました}
A: すぐに戻ります。 I will be back soon.#ID=1284_4709
B: 直ぐに{すぐに} 戻る{戻ります}
</code></pre>

<p>Here's the XML returned by API on the sentence ""私は学生です"": <a href=""http://jlp.yahooapis.jp/FuriganaService/V1/furigana?appid=YuLAPtSxg64LZ2dsAQnC334w1wGLxuq9cqp0MIGSO3QjZ1tbZCYaRRWkeRKdUCft7qej73DqEg--&amp;grade=1&amp;sentence=%E7%A7%81%E3%81%AF%E5%AD%A6%E7%94%9F%E3%81%A7%E3%81%99"" rel=""nofollow"">http://jlp.yahooapis.jp/FuriganaService/V1/furigana?appid=YuLAPtSxg64LZ2dsAQnC334w1wGLxuq9cqp0MIGSO3QjZ1tbZCYaRRWkeRKdUCft7qej73DqEg--&amp;grade=1&amp;sentence=%E7%A7%81%E3%81%AF%E5%AD%A6%E7%94%9F%E3%81%A7%E3%81%99</a></p>

<p>My script follows:</p>

<pre><code>&lt;?php
    function getReading($wabun)
    {
        $res = """";
        $applicationID = ""YuLAPtSxg64LZ2dsAQnC334w1wGLxuq9cqp0MIGSO3QjZ1tbZCYaRRWkeRKdUCft7qej73DqEg--"";
        $grade = 1;
        $url = ""http://jlp.yahooapis.jp/FuriganaService/V1/furigana?appid="".$applicationID.""&amp;grade="".$grade.""&amp;sentence="".$wabun;    
        $doc = new DOMDocument();
        $doc-&gt;load($url);
        foreach ($doc-&gt;getElementsByTagName('Word') as $node) {
            $surface = $node-&gt;getElementsByTagName('Surface')-&gt;item(0)-&gt;nodeValue;
            $furigana = $node-&gt;getElementsByTagName('Furigana')-&gt;item(0)-&gt;nodeValue;
            $reading = (isset($furigana)) ? $furigana : $surface;
            $res .= $reading;
        }
        return $res;
    }
?&gt;
&lt;?php
    header('Content-Type: text/html;charset=utf-8');    
    $myFile = ""examples-utf.utf"";
    $outFile = ""examples-output.utf"";
    $file = fopen($myFile, 'r') or die(""can't open read file"");
    $out = fopen($outFile, 'w') or die(""can't open write file"");
    $i = 1; // line number
    $start = 3; // beginning of japanese sentence, after ""A: ""
    while($line = fgets($file))
    {
        // line starts at ""A: ""
        if($i&amp;1)
        {
            $pos = strpos($line, ""\t"");
            $japanese = substr($line, $start, $pos - $start);

            $end = strpos($line, ""#ID="", $pos + 1);
            $english = substr($line, $pos + 1, $end - $pos - 1);
            $reading = getReading($japanese);

            fwrite($out, $japanese.""\n"");
            fwrite($out, $english.""\n"");
            fwrite($out, $reading.""\n"");

        }
        ++$i;
    }
    fclose($out);
?&gt;
</code></pre>
",Dataset Preprocessing & Handling,optimize php script get phonetical reading japanese sentence yahoo japan api wrote php script read japanese sentence file get phonetical reading sentence using yahoo japan api writes output file script incredibly slow ha processed sentence last hour apache running mac x call api main bottleneck optimize use language php thanks first line input example utf utf file look like xml returned api sentence script follows
Labelled LDA usage,"<p>I am working on a project which requires applying the topic model LDA. Because each document in my case is short, I have to use Labelled LDA. I do not have much knowledge in this area, and all I need to do is to apply the LLDA to my data. </p>

<p>After searching on web I found an LLDA implementation on <a href=""http://nlp.stanford.edu/software/tmt/tmt-0.4/"" rel=""nofollow"">Stanford TMT</a>. What I understand from section <em>Training a Labeled LDA model</em> is: I should label each input document before training. Am I misunderstanding something?</p>

<p>If my understanding is correct, this will involves too much work on labeling documents. Instead, can I provide a separate list of topics, and train the documents without labels?</p>
",Dataset Preprocessing & Handling,labelled lda usage working project requires applying topic model lda document case short use labelled lda much knowledge area need apply llda data searching web found llda implementation stanford tmt understand section training labeled lda model label input document training misunderstanding something understanding correct involves much work labeling document instead provide separate list topic train document without label
Stanford Glove : Dimension anomaly in glove.twitter.27B.200d,"<p>I downloaded Glove-twitter pretrained vectors from <a href=""http://nlp.stanford.edu/data/glove.twitter.27B.zip"" rel=""nofollow"">http://nlp.stanford.edu/data/glove.twitter.27B.zip</a></p>

<p>When I load the vectors (using glove.twitter.27B.200d.txt) in memory I find 900 words, whose vectors are of 199 dimensions, while for rest of all the words,  whose vectors are of 200 dimensions.  <strong>As per my understanding - every vector in this file is supposed to be of exactly 200 dimensions. No ?</strong> </p>

<p>I am using the following python code to arrive at my conclusion</p>

<pre><code>import pickle
import numpy as np

glove_model_path = './glove.twitter.27B.200d.txt'

f = open(glove_model_path,'r')

model = {}
counter = 0

vary_length = 0
anamolies = []

for line in f:
    counter += 1
    items = line.replace('\r','').replace('\n','').split(' ')
    word = items[0]
    vect = np.array([float(i) for i in items[1:] if len(i) &gt; 1])
    if (len(vect) != 200):
        vary_length += 1
        anamolies.append(word)

f.close()

print vary_length
</code></pre>

<hr>

<pre><code>Output is : 900
</code></pre>
",Dataset Preprocessing & Handling,stanford glove dimension anomaly glove twitter b downloaded glove twitter pretrained vector load vector using glove twitter b txt memory find word whose vector dimension rest word whose vector dimension per understanding every vector file supposed exactly dimension using following python code arrive conclusion
document term matrix using list of terms,"<p>I am trying to build a document term matrix using preidentifed terms.  The corpus is identified in the variable cname and the file with the preidentified terms are read into the terms variable which is then converted into a list. When I run the code below I get an empty DTM. The code I am using below.  Any ideas on what I did wrong? Thank you!!!</p>

<p>tom </p>

<pre><code>library(tm) 
library(Rmpfr) 
library(stm)

#Loading Documents
cname &lt;- file.path("""", ""corpus"", ""goodsmoklss"")
library(tm)
corp &lt;- VCorpus(DirSource(cname))

#Transformations
docs&lt;-tm_map(corp,tolower) #AllLowerCase
docs&lt;-tm_map(corp,removeNumbers) #RemoveNumbers

#Remove Stopwords like is, was, the etc
docs&lt;-tm_map(corp, removeWords, stopwords(""english""))

#make Sure it is a PLainTextDocument
documents&lt;-tm_map(docs,PlainTextDocument)


#read in list of preidentified terms
terms=read.delim(""C:/corpus/TermList.csv"", header=F, stringsAsFactor=F)
tokenizing.phrases &lt;- c(terms)

library(""RWeka"")

phraseTokenizer &lt;- function(x) {
  require(stringr)

  x &lt;- as.character(x) # extract the plain text from TextDocument object
  x &lt;- str_trim(x)
  if (is.na(x)) return("""")

  phrase.hits &lt;- str_detect(x, coll(tokenizing.phrases))


  if (any(phrase.hits)) {
    # only split once on the first hit, so we don't have to worry about    #multiple occurences of the same phrase
    split.phrase &lt;- tokenizing.phrases[which(phrase.hits)[1]] 
    #warning(paste(""split phrase:"", split.phrase))
    temp &lt;- unlist(str_split(x, coll(split.phrase), 2))
    out &lt;- c(phraseTokenizer(temp[1]), split.phrase, phraseTokenizer(temp[2])) 
  } else {
    #out &lt;- MC_tokenizer(x)
    out &lt;- "" ""
  }

  # get rid of any extraneous empty strings, which can happen if a phrase occurs just before a punctuation
  out[out != """"]
}

dtm &lt;- DocumentTermMatrix(documents, control = list(tokenize = phraseTokenizer))
</code></pre>
",Dataset Preprocessing & Handling,document term matrix using list term trying build document term matrix using preidentifed term corpus identified variable cname file preidentified term read term variable converted list run code get empty dtm code using idea wrong thank tom
How to break apart a play script with the form **Speaker: Dialogue** to get all of a character&#39;s dialogue into a single text block?,"<ul>
<li>The text I am using is below.</li>
</ul>

<p>So far, I have imported the text:</p>

<pre><code>tempest.v &lt;- scan(""data/plainText/tempest.txt"", what=""character"", sep=""\n"")
</code></pre>

<p>Identified where all of the speaker positions begin:</p>

<pre><code>speaker.positions.v &lt;- grep('^[^\\s]\\w+:', tempest.v)
</code></pre>

<p>Added a marker at the end of the text:</p>

<pre><code>tempest.v &lt;- c(tempest.v, ""END:"")
</code></pre>

<p>Here's the part where I'm having difficulty (assuming what I've already done is useful):</p>

<pre><code>    for(i in 1:length(speaker.positions.v)){
    if(i != length(speaker.positions.v)){
        speaker.name &lt;- debate.v[speaker.positions.v[i]]
        speaker.name &lt;- strsplit(speaker.name, "":"")
        speaker.name &lt;- unlist(speaker.name)
        start &lt;- speaker.positions.v[i]+1
        end &lt;- speaker.positions.v[i+1]-1
        speaker.lines.v &lt;- debate.v[start:end]
  }
}
</code></pre>

<p>Now I have variable <strong>speaker.name</strong> that has, on the left-hand side of the split, the name of the character who is speaking. The right-hand side of the split is the dialogue only up through the first line break.</p>

<p>I set the <strong>start</strong> of the dialogue block at position [i]+1 and
the <strong>end</strong> at [i+1]-1 (i.e., one position back from the beginning of the subsequent speaker's name).</p>

<p>Now I have a variable, <strong>speaker.lines.v</strong> with all of the lines of dialogue for that speaker for that one speech.</p>

<p>How can I collect all of Prospero's then Miranda's (then any other character's) dialogue into a single (list? vector? data frame?) for analysis?</p>

<p>Any help with this would be greatly appreciated.</p>

<p>Happy New Year!</p>

<p>--- *TEXT ---</p>

<p>*Miranda: If by your art, my dearest father, you have 
Put the wild waters in this roar, allay them. 
The sky, it seems, would pour down stinking pitch, 
But that the sea, mounting to the welkin's cheek, 
Dashes the fire out. O, I have suffered 
With those that I saw suffer -- a brave vessel, </p>

<p>Who had, no doubt, some noble creature in her, 
Dash'd all to pieces. O, the cry did knock 
Against my very heart. Poor souls, they perish'd. 
Had I been any god of power, I would 
Have sunk the sea within the earth or ere 
It should the good ship so have swallow'd and 
The fraughting souls within her.</p>

<p>Prospero: Be collected: 
No more amazement: tell your piteous heart 
There's no harm done. </p>

<p>Miranda: O, woe the day!</p>

<p>Prospero: No harm.
I have done nothing but in care of thee, 
Of thee, my dear one, thee, my daughter, who 
Art ignorant of what thou art, nought knowing 
Of whence I am, nor that I am more better 
Than Prospero, master of a full poor cell, 
And thy no greater father.</p>

<p>Miranda: More to know 
Did never meddle with my thoughts. </p>

<p>Prospero: 'Tis time 
I should inform thee farther. Lend thy hand, 
And pluck my magic garment from me. So:</p>

<p>[Lays down his mantle] </p>

<p>Lie there, my art. Wipe thou thine eyes; have comfort. 
The direful spectacle of the wreck, which touch'd 
The very virtue of compassion in thee, 
I have with such provision in mine art 
So safely ordered that there is no soul— 
No, not so much perdition as an hair 
Betid to any creature in the vessel 
Which thou heard'st cry, which thou saw'st sink. Sit down; 
For thou must now know farther.</p>

<p>--- END TEXT ---</p>
",Dataset Preprocessing & Handling,break apart play script form speaker dialogue get character dialogue single text block text using far imported text identified speaker position begin added marker end text part difficulty assuming already done useful variable speaker name ha left hand side split name character speaking right hand side split dialogue first line break set start dialogue block position end e one position back beginning subsequent speaker name variable speaker line v line dialogue speaker one speech collect prospero miranda character dialogue single list vector data frame analysis help would greatly appreciated happy new year text miranda art dearest father put wild water roar allay sky seems would pour stinking pitch sea mounting welkin cheek dash fire suffered saw suffer brave vessel doubt noble creature dash piece cry knock heart poor soul perish god power would sunk sea within earth ere good ship swallow fraughting soul within prospero collected amazement tell piteous heart harm done miranda woe day prospero harm done nothing care thee thee dear one thee daughter art ignorant thou art nought knowing whence better prospero master full poor cell thy greater father miranda know never meddle thought prospero ti time inform thee farther lend thy hand pluck magic garment lay mantle lie art wipe thou thine eye direful spectacle wreck touch virtue compassion thee provision mine art ordered soul much perdition hair betid creature vessel thou heard st cry thou saw st sink sit thou must know farther end text
&quot;Bag of characters&quot; n-grams in R,"<p>I would like to create a Term document matrix containing character n-grams. For example, take the following sentence: </p>

<p>""In this paper, we focus on a different but simple text representation.""</p>

<p>Character 4-grams would be: |In_t|, |n_th|, |_thi|, |this|, |his__|, |is_p|, |s_pa|, |_pap|, |pape|, |aper|, etc.</p>

<p>I have used the R/Weka package to work with ""bag of words"" n-grams, but I'm having difficulty adapting tokenizers such as the one below to work with characters:</p>

<pre><code>BigramTokenizer &lt;- function(x){
    NGramTokenizer(x, Weka_control(min = 2, max = 2))}

tdm_bigram &lt;- TermDocumentMatrix(corpus,
                                 control = list(
                                 tokenize = BigramTokenizer, wordLengths=c(2,Inf)))
</code></pre>

<p>Any thoughts on how to use R/Weka or an other package to create character n-grams?</p>
",Dataset Preprocessing & Handling,bag character n gram r would like create term document matrix containing character n gram example take following sentence paper focus different simple text representation character gram would n th thi p pa pap pape aper etc used r weka package work bag word n gram difficulty adapting tokenizers one work character thought use r weka package create character n gram
Import GATE features to csv,"<p>I am new to GATE Developer. I want use annotation generated by  GATE pipeline as features to the classifier. How can I save those features into a csv file?</p>
",Dataset Preprocessing & Handling,import gate feature csv new gate developer want use annotation generated gate pipeline feature classifier save feature csv file
Parsing columns lines with a single double quotations in Graphlab.SFrame,"<p>I have lines as such from this dataset (<a href=""https://raw.githubusercontent.com/alvations/stasis/master/sts.csv"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/alvations/stasis/master/sts.csv</a>):</p>

<pre><code>Dataset Domain  Score   Sent1   Sent2
STS2012-gold    surprise.OnWN   5.000   render one language in another language restate (words) from one language into another language.
STS2012-gold    surprise.OnWN   3.250   nations unified by shared interests, history or institutions    a group of nations having common interests.
STS2012-gold    surprise.OnWN   3.250   convert into absorbable substances, (as if) with heat or chemical process   soften or disintegrate by means of chemical action, heat, or moisture.
STS2012-gold    surprise.OnWN   4.000   devote or adapt exclusively to an skill, study, or work devote oneself to a special area of work.
STS2012-gold    surprise.OnWN   3.250   elevated wooden porch of a house    a porch that resembles the deck on a ship.
</code></pre>

<p>I have read it into a <code>graphlab.SFrame</code> using the <code>read_csv()</code> function:</p>

<pre><code>import graphlab
sts = graphlab.SFrame.read_csv('sts.csv', delimiter='\t', column_type_hints=[str, str, float, str, str])
</code></pre>

<p>And there were lines that are not parsed. The traceback is as follows:</p>

<pre><code>PROGRESS: Unable to parse line ""STS2012-gold    MSRpar  3.800   ""She was crying and scared,' said Isa Yasin, the owner of the store.    ""She was crying and she was really scared,"" said Yasin.""
PROGRESS: Unable to parse line ""STS2012-gold    MSRpar  2.200   ""And about eight to 10 seconds down, I hit. ""I was in the water for about eight seconds.""
PROGRESS: Unable to parse line ""STS2012-gold    MSRpar  2.800   ""It's a major victory for Maine, and it's a major victory for other states. The Maine program could be a model for other states.""
PROGRESS: Unable to parse line ""STS2012-gold    MSRpar  4.000   ""Right from the beginning, we didn't want to see anyone take a cut in pay.  But Mr. Crosby told The Associated Press: ""Right from the beginning, we didn't want to see anyone take a cut in pay.""
PROGRESS: Unable to parse line ""STS2014-gold    deft-forum  0.8 ""Then the captain was gone. Then the captain came back.""
PROGRESS: Unable to parse line ""STS2014-gold    deft-forum  1.8 ""Oh, you're such a good person! You're such a bad person!""""
PROGRESS: Unable to parse line ""STS2012-train   MSRpar  3.750   ""We put a lot of effort and energy into improving our patching process, probably later than we should have and now we're just gaining incredible speed. ""We've put a lot of effort and energy into improving our patching progress, p...""
PROGRESS: Unable to parse line ""STS2012-train   MSRpar  4.000   ""Tomorrow at the Mission Inn, I have the opportunity to congratulate the governor-elect of the great state of California.   ""I have the opportunity to congratulate the governor-elect of the great state of California, and I'm lookin...""
PROGRESS: Unable to parse line ""STS2012-train   MSRpar  3.600   ""Unlike many early-stage Internet firms, Google is believed to be profitable.   The privately held Google is believed to be profitable.""
PROGRESS: Unable to parse line ""STS2012-train   MSRpar  4.000   ""It was a final test before delivering the missile to the armed forces. State radio said it was the last test before the missile was delivered to the armed forces.""
PROGRESS: 22 lines failed to parse correctly
PROGRESS: Finished parsing file /home/alvas/git/stasis/sts.csv
PROGRESS: Parsing completed. Parsed 19075 lines in 0.069578 secs.
</code></pre>

<p>Look at these lines there seems to be a problem if any of my <code>Sent1</code> or <code>Sent2</code> columns contains odd-numbered double quotation marks.</p>

<p>Using the <code>error_bad_lines</code> to track the problematic lines:</p>

<pre><code>sts = graphlab.SFrame.read_csv('sts.csv', delimiter='\t', column_type_hints=[str, str, float, str, str],
                              error_bad_lines=True)
</code></pre>

<p>It throws the traceback:</p>

<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-15-a1ec53597af9&gt; in &lt;module&gt;()
      1 sts = graphlab.SFrame.read_csv('sts.csv', delimiter='\t', column_type_hints=[str, str, float, str, str],
----&gt; 2                               error_bad_lines=True)

/usr/local/lib/python2.7/dist-packages/graphlab/data_structures/sframe.pyc in read_csv(cls, url, delimiter, header, error_bad_lines, comment_char, escape_char, double_quote, quote_char, skip_initial_space, column_type_hints, na_values, line_terminator, usecols, nrows, skiprows, verbose, **kwargs)
   1537                                   verbose=verbose,
   1538                                   store_errors=False,
-&gt; 1539                                   **kwargs)[0]
   1540 
   1541 

/usr/local/lib/python2.7/dist-packages/graphlab/data_structures/sframe.pyc in _read_csv_impl(cls, url, delimiter, header, error_bad_lines, comment_char, escape_char, double_quote, quote_char, skip_initial_space, column_type_hints, na_values, line_terminator, usecols, nrows, skiprows, verbose, store_errors, **kwargs)
   1097                 glconnect.get_client().set_log_progress(False)
   1098             with cython_context():
-&gt; 1099                 errors = proxy.load_from_csvs(internal_url, parsing_config, type_hints)
   1100         except Exception as e:
   1101             if type(e) == RuntimeError and ""CSV parsing cancelled"" in e.message:

/usr/local/lib/python2.7/dist-packages/graphlab/cython/context.pyc in __exit__(self, exc_type, exc_value, traceback)
     47             if not self.show_cython_trace:
     48                 # To hide cython trace, we re-raise from here
---&gt; 49                 raise exc_type(exc_value)
     50             else:
     51                 # To show the full trace, we do nothing and let exception propagate

RuntimeError: Runtime Exception. Unable to parse line ""STS2012-gold MSRpar  3.800   ""She was crying and scared,' said Isa Yasin, the owner of the store.    ""She was crying and she was really scared,"" said Yasin.""
Set error_bad_lines=False to skip bad lines
</code></pre>

<p><strong>Is there a way to resolve this problem where my lines contains odd number of double quotes?</strong> </p>

<p><strong>Is there a way to do it without cleaning the data (e.g. identifying the problematic lines and then clean/correct them but keep another SFrame to track the cleaning/correction?</strong></p>

<hr>

<p>As a sanity check, if we do a search <code>\t</code> in the raw csv file, there's a tab in the rows that gives problem but when <code>graphlab</code> parses it, it disappears:</p>

<p><a href=""https://i.sstatic.net/3DzsG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3DzsG.png"" alt=""enter image description here""></a></p>

<hr>

<p>As another sanity check, reading the file line by line and splitting it by <code>\t</code> returns 5 columns for the whole file:</p>

<pre><code>alvas@ubi:~/git/stasis$ head sts.csv 
Dataset Domain  Score   Sent1   Sent2
STS2012-gold    surprise.OnWN   5.000   render one language in another language restate (words) from one language into another language.
STS2012-gold    surprise.OnWN   3.250   nations unified by shared interests, history or institutions    a group of nations having common interests.
STS2012-gold    surprise.OnWN   3.250   convert into absorbable substances, (as if) with heat or chemical process   soften or disintegrate by means of chemical action, heat, or moisture.
STS2012-gold    surprise.OnWN   4.000   devote or adapt exclusively to an skill, study, or work devote oneself to a special area of work.
STS2012-gold    surprise.OnWN   3.250   elevated wooden porch of a house    a porch that resembles the deck on a ship.
STS2012-gold    surprise.OnWN   4.000   either half of an archery bow   either of the two halves of a bow from handle to tip.
STS2012-gold    surprise.OnWN   3.333   a removable device that is an accessory to larger object    a supplementary part or accessory.
STS2012-gold    surprise.OnWN   4.750   restrict or confine place limits on (extent or access).
STS2012-gold    surprise.OnWN   0.500   orient, be positioned   be opposite.
alvas@ubi:~/git/stasis$ python
Python 2.7.10 (default, Jun 30 2015, 15:30:23) 
[GCC 4.8.4] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; with open('sts.csv') as fin:
...     for line in fin:
...             print len(line.split('\t'))
...             break
... 
5

&gt;&gt;&gt; with open('sts.csv') as fin:
...     for line in fin:
...             assert len(line.split('\t')) == 5
... 
&gt;&gt;&gt; 
</code></pre>

<hr>

<p>Even more sanity check that it's the no. of columns, @papayawarrior example of the 4 columns line was correctly parsed in my version of <code>graphlab</code>:</p>

<p><a href=""https://i.sstatic.net/TrjPE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TrjPE.png"" alt=""enter image description here""></a></p>

<hr>

<p>I have manually checked the problematic lines and they're:</p>

<pre><code>STS2012-gold    MSRpar  3.800   ""She was crying and scared,' said Isa Yasin, the owner of the store.    ""She was crying and she was really scared,"" said Yasin.
STS2012-gold    MSRpar  2.200   ""And about eight to 10 seconds down, I hit. ""I was in the water for about eight seconds.
STS2012-gold    MSRpar  2.800   ""It's a major victory for Maine, and it's a major victory for other states. The Maine program could be a model for other states.
STS2012-gold    MSRpar  4.000   ""Right from the beginning, we didn't want to see anyone take a cut in pay.  But Mr. Crosby told The Associated Press: ""Right from the beginning, we didn't want to see anyone take a cut in pay.
STS2012-train   MSRpar  3.750   ""We put a lot of effort and energy into improving our patching process, probably later than we should have and now we're just gaining incredible speed. ""We've put a lot of effort and energy into improving our patching progress, probably later than we should have.
STS2012-train   MSRpar  4.000   ""Tomorrow at the Mission Inn, I have the opportunity to congratulate the governor-elect of the great state of California.   ""I have the opportunity to congratulate the governor-elect of the great state of California, and I'm looking forward to it.""
STS2012-train   MSRpar  3.600   ""Unlike many early-stage Internet firms, Google is believed to be profitable.   The privately held Google is believed to be profitable.
STS2012-train   MSRpar  4.000   ""It was a final test before delivering the missile to the armed forces. State radio said it was the last test before the missile was delivered to the armed forces.
STS2012-train   MSRpar  4.750   ""The economy, nonetheless, has yet to exhibit sustainable growth.   But the economy hasn't shown signs of sustainable growth.
STS2014-gold    deft-forum  0.8 ""Then the captain was gone. Then the captain came back.
STS2014-gold    deft-forum  1.8 ""Oh, you're such a good person! You're such a bad person!""
STS2015-gold    answers-forums      ""Normal, healthy (physically, nutritionally and mentally) individuals have little reason to worry about accidentally consuming too much water.  It's fine to skip arm specific exercises if you are already happy with how they are progressing without direct exercises.
STS2015-gold    answers-forums  1.40    ""The grass family is one of the most widely distributed and abundant groups of plants on Earth. As noted on the Wiki page, grass seed was imported to the new world to improve pasturage for livestock.
STS2015-gold    answers-forums      ""God is exactly this Substance underlying who supports, exist independently of, and persist through time changes in material nature.    I'd argue that matter and energy are substances in the category of empirical scientific knowledge.
STS2015-gold    belief      ""watching the first fight i saw that manny pacquiao was getting tired, and i wasn't.    at the same time, an asian summit is being held in a tourist resort.
STS2015-gold    belief      ""global warming doesn't mean every year will be warmer than the last.   doesn't matter, that will just be obama's fault as well.
STS2015-gold    belief      ""the only reason i'm not as confident that there's something about the birth certificate... the conventional view is that the us and ussr fought it out in the body of vietnam.
STS2015-gold    belief      ""im not playing these bullshit games... if not get the hell out of there.
STS2015-gold    belief      ""that oil is already contaminating our shoreline.   what point are you trying to relay?
STS2015-gold    belief      ""we cannot write history with laws. ""she's not sitting here"" he said.
STS2015-gold    belief      the protest is going well so far.   our request is the same.
STS2015-gold    belief      ""for over 20 years, i have illustrated the absurd with absurdity, three hours a day, five days a week.  for the first 1-2 years he hated me going out with my friends.
</code></pre>

<p>Instead of finding these lines manually by repeatedly cleaning out these lines from the <code>PROGRESS: ...</code> verbose message, <strong>is there a way to just dump these lines out when loading it into a Graphlab SFrame?</strong> </p>
",Dataset Preprocessing & Handling,parsing column line single double quotation graphlab sframe line dataset read using function line parsed traceback follows look line seems problem column contains odd numbered double quotation mark using track problematic line throw traceback way resolve problem line contains odd number double quote way without cleaning data e g identifying problematic line clean correct keep another sframe track cleaning correction sanity check search raw csv file tab row give problem par disappears another sanity check reading file line line splitting return column whole file even sanity check column papayawarrior example column line wa correctly parsed version manually checked problematic line instead finding line manually repeatedly cleaning line verbose message way dump line loading graphlab sframe
finding features from a large data set by stanford corenlp,"<p>I am new Stanford NLP. I can not find any good and complete documentation or tutorial. My work is to do sentiment analysis. I have a very large dataset of product reviews. I already distinguished them by positive and negative according to ""starts"" given by the users. Now I need to find the most occurred positive  and negative adjectives as the features of my algorithm. I understand how to do tokenzation, lemmatization and POS taging from <a href=""http://stanfordnlp.github.io/CoreNLP/api.html"" rel=""nofollow"">here</a>. I got files like this.</p>

<p>The review was </p>

<pre><code>Don't waste your money. This is a short DVD and the host is boring and offers information that is common sense to any idiot. Pass on this and buy something else. Very generic
</code></pre>

<p>and the output was.</p>

<pre><code>Sentence #1 (6 tokens):
Don't waste your money.
[Text=Do CharacterOffsetBegin=0 CharacterOffsetEnd=2 PartOfSpeech=VBP Lemma=do]
[Text=n't CharacterOffsetBegin=2 CharacterOffsetEnd=5 PartOfSpeech=RB Lemma=not]
[Text=waste CharacterOffsetBegin=6 CharacterOffsetEnd=11 PartOfSpeech=VB Lemma=waste]
[Text=your CharacterOffsetBegin=12 CharacterOffsetEnd=16 PartOfSpeech=PRP$ Lemma=you]
[Text=money CharacterOffsetBegin=17 CharacterOffsetEnd=22 PartOfSpeech=NN Lemma=money]
[Text=. CharacterOffsetBegin=22 CharacterOffsetEnd=23 PartOfSpeech=. Lemma=.]
Sentence #2 (21 tokens):
This is a short DVD and the host is boring and offers information that is common sense to any idiot.
[Text=This CharacterOffsetBegin=24 CharacterOffsetEnd=28 PartOfSpeech=DT Lemma=this]
[Text=is CharacterOffsetBegin=29 CharacterOffsetEnd=31 PartOfSpeech=VBZ Lemma=be]
[Text=a CharacterOffsetBegin=32 CharacterOffsetEnd=33 PartOfSpeech=DT Lemma=a]
[Text=short CharacterOffsetBegin=34 CharacterOffsetEnd=39 PartOfSpeech=JJ Lemma=short]
[Text=DVD CharacterOffsetBegin=40 CharacterOffsetEnd=43 PartOfSpeech=NN Lemma=dvd]
[Text=and CharacterOffsetBegin=44 CharacterOffsetEnd=47 PartOfSpeech=CC Lemma=and]
[Text=the CharacterOffsetBegin=48 CharacterOffsetEnd=51 PartOfSpeech=DT Lemma=the]
[Text=host CharacterOffsetBegin=52 CharacterOffsetEnd=56 PartOfSpeech=NN Lemma=host]
[Text=is CharacterOffsetBegin=57 CharacterOffsetEnd=59 PartOfSpeech=VBZ Lemma=be]
[Text=boring CharacterOffsetBegin=60 CharacterOffsetEnd=66 PartOfSpeech=JJ Lemma=boring]
[Text=and CharacterOffsetBegin=67 CharacterOffsetEnd=70 PartOfSpeech=CC Lemma=and]
[Text=offers CharacterOffsetBegin=71 CharacterOffsetEnd=77 PartOfSpeech=VBZ Lemma=offer]
[Text=information CharacterOffsetBegin=78 CharacterOffsetEnd=89 PartOfSpeech=NN Lemma=information]
[Text=that CharacterOffsetBegin=90 CharacterOffsetEnd=94 PartOfSpeech=WDT Lemma=that]
[Text=is CharacterOffsetBegin=95 CharacterOffsetEnd=97 PartOfSpeech=VBZ Lemma=be]
[Text=common CharacterOffsetBegin=98 CharacterOffsetEnd=104 PartOfSpeech=JJ Lemma=common]
[Text=sense CharacterOffsetBegin=105 CharacterOffsetEnd=110 PartOfSpeech=NN Lemma=sense]
[Text=to CharacterOffsetBegin=111 CharacterOffsetEnd=113 PartOfSpeech=TO Lemma=to]
[Text=any CharacterOffsetBegin=114 CharacterOffsetEnd=117 PartOfSpeech=DT Lemma=any]
[Text=idiot CharacterOffsetBegin=118 CharacterOffsetEnd=123 PartOfSpeech=NN Lemma=idiot]
[Text=. CharacterOffsetBegin=123 CharacterOffsetEnd=124 PartOfSpeech=. Lemma=.]
Sentence #3 (8 tokens):
Pass on this and buy something else.
[Text=Pass CharacterOffsetBegin=125 CharacterOffsetEnd=129 PartOfSpeech=VB Lemma=pass]
[Text=on CharacterOffsetBegin=130 CharacterOffsetEnd=132 PartOfSpeech=IN Lemma=on]
[Text=this CharacterOffsetBegin=133 CharacterOffsetEnd=137 PartOfSpeech=DT Lemma=this]
[Text=and CharacterOffsetBegin=138 CharacterOffsetEnd=141 PartOfSpeech=CC Lemma=and]
[Text=buy CharacterOffsetBegin=142 CharacterOffsetEnd=145 PartOfSpeech=VB Lemma=buy]
[Text=something CharacterOffsetBegin=146 CharacterOffsetEnd=155 PartOfSpeech=NN Lemma=something]
[Text=else CharacterOffsetBegin=156 CharacterOffsetEnd=160 PartOfSpeech=RB Lemma=else]
[Text=. CharacterOffsetBegin=160 CharacterOffsetEnd=161 PartOfSpeech=. Lemma=.]
Sentence #4 (2 tokens):
Very generic
[Text=Very CharacterOffsetBegin=162 CharacterOffsetEnd=166 PartOfSpeech=RB Lemma=very]
[Text=generic CharacterOffsetBegin=167 CharacterOffsetEnd=174 PartOfSpeech=JJ Lemma=generic]
</code></pre>

<p>I already have processed 10000 positive and 10000 negative file like this. Now How can I easily find the most occurred positive and negative features(adjectives)? Do i need to read all the output(processed) file and make a list frequency count of the adjectives like this or is there any easy way by stanford corenlp? </p>
",Dataset Preprocessing & Handling,finding feature large data set stanford corenlp new stanford nlp find good complete documentation tutorial work sentiment analysis large dataset product review already distinguished positive negative according start given user need find occurred positive negative adjective feature algorithm understand tokenzation lemmatization po taging got file like review wa output wa already processed positive negative file like easily find occurred positive negative feature adjective need read output processed file make list frequency count adjective like easy way stanford corenlp
How to load text data correctly in scikit-learn?,"<p>I am following <a href=""https://gist.github.com/zacstewart/5978000"" rel=""nofollow"">this example</a> to create a multinomial naive Bayes classifier for text data in scikit-learn. However, the output of the confusion matrix and classifier F-1 score is incorrect. I think the errors have to do with the input data format I am using. I have one csv file per training example. The csv file contains one row with features like so 'blah, blahblah, andsoon'. Each file is either classfied as positive or negative. How, can I correctly read this files?</p>

<p>Here is my code:</p>

<pre><code>import numpy
import csv
from pandas import DataFrame
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.cross_validation import KFold
from sklearn.metrics import confusion_matrix, f1_score

NEWLINE = '\n'

NEGATIVE = 'negative'
POSITIVE = 'positive'

SOURCES = [
    ('negative\\', NEGATIVE),
    ('positive\\', POSITIVE)
]

SKIP_FILES = {'cmds'}


def build_data_frame(policies, path, classification):
    rows = []
    index = []

    for policy in policies:

        current_csv = path + policy + '.csv'

        # check if file exists
        if (os.path.isfile(current_csv)):

            with open(current_csv, 'r') as csvfile:

                reader = csv.reader(csvfile, delimiter=',', quotechar='""')

                # get each row in policy
                for row in reader:
                    # remove all commas from inside the text lists
                    clean_row = ' '.join(row)
                    rows.append({'text': clean_row, 'class': classification})
                    index.append(current_csv)

    data_frame = DataFrame(rows, index=index)
    return data_frame


def policy_analyzer_main(policies, write_pol_path):
    data = DataFrame({'text': [], 'class': []})
    for path, classification in SOURCES:
        data = data.append(build_data_frame(policies, write_pol_path + path, classification))
    classify(data)

pipeline = Pipeline([
    ('count_vectorizer',   CountVectorizer()),
    ('classifier',         MultinomialNB())
])

def classify(data):

    k_fold = KFold(n=len(data), n_folds=10)
    scores = []
    confusion = numpy.array([[0, 0], [0, 0]])
    for train_indices, test_indices in k_fold:
        train_text = data.iloc[train_indices]['text'].values
        train_y = data.iloc[train_indices]['class'].values.astype(str)

        test_text = data.iloc[test_indices]['text'].values
        test_y = data.iloc[test_indices]['class'].values.astype(str)

        pipeline.fit(train_text, train_y)
        predictions = pipeline.predict(test_text)

        confusion += confusion_matrix(test_y, predictions)
        score = f1_score(test_y, predictions, pos_label=POSITIVE)
        scores.append(score)

    print('Total emails classified:', len(data))
    print('Score:', sum(scores)/len(scores))
    print('Confusion matrix:')
    print(confusion)
</code></pre>

<p>Here is an example of the warning message I am getting:</p>

<pre><code>UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
 ('Total emails classified:', 75)
 ('Score:', 0.025000000000000001)
Confusion matrix:
[[39 35]
 [46 24]]
</code></pre>
",Dataset Preprocessing & Handling,load text data correctly scikit learn following example create multinomial naive bayes classifier text data scikit learn however output confusion matrix classifier f score incorrect think error input data format using one csv file per training example csv file contains one row feature like blah blahblah andsoon file either classfied positive negative correctly read file code example warning message getting
Using a regular expression to find all noun phrases in a paragraph following the occurrence of a specific phrase,"<p>I have a data frame of paragraphs, which I have (*can) split into word tokens and sentence tokens and am looking to find all the noun phrases following any instance where the phrase: ""contribute to"" or ""donate to"" occurs.</p>

<p>Or really some form of that, so: </p>

<pre><code>""Contributions are welcome to be made to the charity of your choice."" 

---&gt; would return: ""the charity of your choice""
</code></pre>

<p>and </p>

<pre><code>""blah blah blah donations, in honor of Firstname Lastname, can be made to ABC Foundation""

---&gt; would return: ""ABC Foundation""
</code></pre>

<p>I've created a regular expression work-around that captures the correct phrase about 90% of the time... see below: </p>

<pre><code>text = nltk.Text(nltk.word_tokenize(x))
donation = TokenSearcher(text).findall(r""&lt;\.&gt; &lt;.*&gt;{,15}? &lt;donat.*|contrib.*&gt; &lt;.*&gt;*? &lt;to&gt; (&lt;.*&gt;+?) &lt;\.|\,|\;&gt; "")
donation = [' '.join(tokens) for tokens in donation]
return donation
</code></pre>

<p>I'd like to clean up that regular expression to get rid of the ""{,15}"" requirements because it's missing some of the values that I need. However, I'm not too polished with the ""greedy"" expressions and can't get it to work correctly. </p>

<p>so this phrase: </p>

<pre><code>While she lived a full life , had many achievements and made many 
**contributions** , FirstName is remembered by most for her cheerful smile ,
colorful track suits , and beautiful necklaces hand made by daughter FirstName .
FirstName always cherished her annual visit home for Thanksgiving to visit
brother FirstName LastName
</code></pre>

<p>is returning: ""visit brother FirstName Lastname"" due to the previous mentioning of contributions even though the word ""to"" comes well after 15 words later.</p>
",Dataset Preprocessing & Handling,using regular expression find noun phrase paragraph following occurrence specific phrase data frame paragraph split word token sentence token looking find noun phrase following instance phrase contribute donate occurs really form created regular expression work around capture correct phrase time see like clean regular expression get rid requirement missing value need however polished greedy expression get work correctly phrase returning visit brother firstname lastname due previous mentioning contribution even though word come well word later
Python Naive Bayes Classifier trained on Movie Review Corpus to test on Tweets,"<pre><code>import nltk.classify.util
import csv
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import movie_reviews

def word_feats(words):
    return dict([(word, True) for word in words])

negids = movie_reviews.fileids('neg')
posids = movie_reviews.fileids('pos')

negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]
posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]

negcutoff = len(negfeats)*3/4
poscutoff = len(posfeats)*3/4

trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]
testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]
print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))

classifier = NaiveBayesClassifier.train(trainfeats)
print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)
classifier.show_most_informative_features()
</code></pre>

<p>I am very new to Python and am trying to perform sentiment analysis on Tweets. I am using the Naive Bayes Classifier that is built into the NLTK package. I am testing it on the movie review corpus and want to test in on tweets I have stored into a .txt or .csv file using Tweepy. Can anyone help figure out how to test this classifier off the tweets in my output file? Thanks!</p>
",Dataset Preprocessing & Handling,python naive bayes classifier trained movie review corpus test tweet new python trying perform sentiment analysis tweet using naive bayes classifier built nltk package testing movie review corpus want test tweet stored txt csv file using tweepy anyone help figure test classifier tweet output file thanks
x.findall function returns a value but won&#39;t write to pandas data frame,"<p>I've created a function that searches a NLTK.text.Text object, and returns a value when I run the function. </p>

<p>UPDATE: The problem seems to be that in the function below, the 'donation' variable is not actually being passed a value. The text.findall function however does return a value but doesn't update the variable for some reason.</p>

<pre><code>def find_donation_orgs(x):
    text = nltk.Text(nltk.word_tokenize(x))
    donation =  text.findall(r""&lt;\.&gt; &lt;.*&gt;{,15}? &lt;donat.*|contrib.*|Donat.*|Contrib.*&gt; &lt;.*&gt;*? &lt;to&gt; (&lt;.*&gt;+?) &lt;\.|\,|\;&gt; "")
    return donation
</code></pre>

<p>The output something like this for the following input, however I think the output is from the text.findall and not the actual ""return donation"".</p>

<pre><code>a = ""This is a sentence. I also donate to Mr. T's Tea Party. I contribute to the Boys and Girls club. ""

find_donation_orgs(a)
</code></pre>

<p>Output = </p>

<pre><code>Mr. T 's Tea Party
the Boys and Girls club
</code></pre>

<p>However, when I try to apply that function in order to write the output to a new column in a pandas data frame, it's returning None. See below:</p>

<pre><code>df['donation_orgs'] = df.apply(lambda row: find_donation_orgs(row['Obit']), axis = 1)
</code></pre>

<p>where df['Obit'] is a string of text similar to my a variable above. </p>

<p>UPDATE: So it appears that the output of a text.findall doesn't update the value of the variable its assigned to ... so I need to figure out how to actually assign that output to a variable in order to return it to a dataframe. See below:</p>

<pre><code>text = df.text.iloc[1]

textfindall = text.findall(r""&lt;\.&gt; &lt;.*&gt;{,15}? &lt;donat.*|contrib.*|Donat.*|Contrib.*&gt; &lt;.*&gt;*? &lt;to&gt; (&lt;.*&gt;+?) &lt;\.|\,|\;&gt; "")

print('text is ' + str(type(text)))
print('textfindall is ' + str(type(textfindall)))
print(textfindall)
</code></pre>

<p>Output:</p>

<pre><code>visit brother Alfred Fuller; the research of Dr. Giuseppe Giaccone at
Georgetown University
text is &lt;class 'nltk.text.Text'&gt;
textfindall is &lt;class 'NoneType'&gt;
none
</code></pre>
",Dataset Preprocessing & Handling,x findall function return value write panda data frame created function search nltk text text object return value run function update problem seems function donation variable actually passed value text findall function however doe return value update variable reason output something like following input however think output text findall actual return donation output however try apply function order write output new column panda data frame returning none see df obit string text similar variable update appears output text findall update value variable assigned need figure actually assign output variable order return dataframe see output
Can I parallelize code that utilizes pandas?,"<p>I'm doing some NLP, and need to clean my data.  I've written three functions to 1) clean the data, 2) check if the data is on topic, and 3) check of the data is english.</p>

<p>I have something like 8 million rows of data, and most of the computations don't rely on one another. I was thinking of parallelizing the code using Pool, but I'm not sure if that would be wise, since all the data is housed in a Pandas dataframe (I know numba doesn't play well with dataframes).</p>

<p>Could I parallelize my code using Pool?  Is it as simple as the code I've found in the documentation?  Is Pool even the right library?</p>

<p>Should note, I'm running this on Mac OSX.
Here is my code for reference:</p>

<pre><code>import pandas as pd
import re
from bs4 import BeautifulSoup
import nltk
from nltk.corpus import stopwords
import enchant
import numpy as np

sf = pd.read_csv('timeandtweet.csv')

def clean_tweet(x):

    cleaning = BeautifulSoup(x,""lxml"")

    letters_only = re.sub(""[^a-zA-Z]"","" "", cleaning.get_text())

    words = letters_only.lower().split()

    words = [w for w in words if not w in (stopwords.words(""english"")+[u'rt'])]

    return "" "".join(words)



def on_topic(x):

    topics = [u'measles',u'mmr',u'vaccine',u'vaccines']

    if any(j in topics for j in x.split()):
        return 1
    else:
        return -1

def is_english(x):

    lang = enchant.Dict('en_US')
    L = len(x.split())

    words = []
    for i in x.split():
        words.append(lang.check(i))

    if float(sum(words))/L &lt;0.6:
        return -1
    else:
        return 1





sf['Clean Tweet'] = np.zeros_like(sf.Tweet)

sf['English-Topic'] = np.zeros_like(sf.Tweet)

for i in xrange(len(sf)): #Loop instead of df.apply for speed?
    if( (i+1)%1000 == 0 ):
        print ""Review %d of %d\n"" % ( i+1, len(sf) ) 

    sf['Clean Tweet'][i] = clean_tweet(sf.Tweet[i])

    sf['English-Topic'][i] = (on_topic(sf['Clean Tweet'][i]), is_english(sf['Clean Tweet'][i])  )


sf.to_csv('cleaned_processed.csv', index = False)
</code></pre>

<p>My Attempt to parallelize</p>

<pre><code>sf['Clean Tweet'] = np.zeros_like(sf.Tweet)

sf['English-Topic'] = np.zeros_like(sf.Tweet)

from multiprocessing import Pool

pool = Pool()

result1 = pool.apply_async(clean_tweet,[sf.Tweet])
answer1 = result1.get()
</code></pre>

<p>But I keep getting a value error.</p>

<pre><code>ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
</code></pre>
",Dataset Preprocessing & Handling,parallelize code utilizes panda nlp need clean data written three function clean data check data topic check data english something like million row data computation rely one another wa thinking parallelizing code using pool sure would wise since data housed panda dataframe know numba play well dataframes could parallelize code using pool simple code found documentation pool even right library note running mac osx code reference attempt parallelize keep getting value error
Mallet Natural Language Processing Mallet,"<p>I am trying to learn MALLET developed by UMASS Amhrest. I am pretty new to this and hence this may be a silly question.</p>

<p>I just ran a sample example given on their website using following command.</p>

<pre><code>bin/mallet import-dir --input sample-data/web/* --output web.mallet
</code></pre>

<p>Now, I have the web.mallet output file and I dont know how to open it.
I am using Linux and gedit cant read this file.</p>

<p>How can I open the output file and see its contents?</p>

<p>Thank You.</p>
",Dataset Preprocessing & Handling,mallet natural language processing mallet trying learn mallet developed uma amhrest pretty new hence may silly question ran sample example given website using following command web mallet output file dont know open using linux gedit cant read file open output file see content thank
Operation between 2 data frames in R,"<p>I have this data frame:</p>

<pre><code> word       value
 very good    2 
 this is      2
 we are      -1
 very bad    -2
      . 
      .
      .
</code></pre>

<p>And:</p>

<pre><code>Sentences &lt;- c('good good','very good very bad','we are we are','very good very good very bad')
</code></pre>

<p>How can I make a data frame that takes the 'score' of each centance:</p>

<pre><code>                         Sentences    score
    1                    good good        0
    2           very good very bad        0
    3                we are we are       -2
    4 very good very good very bad        2
</code></pre>
",Dataset Preprocessing & Handling,operation data frame r data frame make data frame take score centance
Stanford NLP/Petrarch: Discarding sentence,"<p>I am seeking to parse a set of news stories with <em>Petrarch</em>. According to its  <a href=""http://petrarch.readthedocs.org/en/latest/inputs.html"" rel=""nofollow"">official document</a>:</p>

<blockquote>
  <p>The main input format for PETRARCH is an XML document with each entry
  in the document a sentence or story to be parsed. The inputs can be
  either individual sentences or entire stories. Additionally, the input
  can contain pre-parsed information from StanfordNLP or just the plain
  text with the Stanford parse left up to TABARI. Whether the input is
  parsed or not is indicated using the -P flag in the PETRARCH
  command-line arguments.</p>
</blockquote>

<p>In other words, Petrarch uses StanfordNLP as a part of its parsing tool.</p>

<p>My news documents are all in one txt file without XML structure (so, no sentence attribute, id, but has date). But I want to try out with a sample text to see if this works, in which case I will reprogram those texts into corresponding format. Below is the sample:</p>

<pre><code>&lt;document&gt;
&lt;Sentences&gt;

&lt;Sentence sentence = ""Boolean"" id = ""1"" date = ""20151026""&gt;
    &lt;Text&gt;China, Japan and South Korea will hold a summit in South Korea when Chinese Premier Li Keqiang visits.&lt;/Text&gt; 
&lt;/Sentence&gt;
&lt;Sentence sentence = ""Boolean"" id = ""2"" date = ""20151027""&gt;
    &lt;Text&gt;It is the first China-Japan-South Korea meeting since they were discontinued in 2012 amid tension dating back to World War Two.&lt;/Text&gt;    
&lt;/Sentence&gt;
&lt;Sentence sentence = ""Boolean"" id = ""3"" date = ""20151027""&gt;
    &lt;Text&gt;Marry has a happy life.&lt;/Text&gt;    
&lt;/Sentence&gt;

&lt;/Sentences&gt;
&lt;/document&gt;
</code></pre>

<p>The format is accept by Petrarch and the program runs without error, but there is no output. Below is my python code:</p>

<pre><code>cd 
virtualenv venv
source venv/bin/activate
petrarch parse -i reuter1025.xml -o output.txt
</code></pre>

<p>And below is the log I copied from the Terminal:</p>

<pre><code>(venv)d-172-26-7-114:~ Carl$ petrarch parse -i reuter1025.xml -o output.txt

new_actor_length = 0
stop_on_error = False
write_actor_root = False
write_actor_text = False
require_dyad = True
code-by-sentence True
pause_by_sentence False
pause_by_story False
Comma-delimited clause elimination:
Initial : deactivated
Internal: min = 2    max = 8
Terminal: min = 2    max = 8
Verb dictionary: CAMEO.verbpatterns.150430.txt
Actor dictionaries: [u'Phoenix.Countries.actors.txt', u'Phoenix.International.actors.txt', u'Phoenix.MilNonState.actors.txt']
Agent dictionary: Phoenix.agents.txt
Discard dictionary: Phoenix.discards.txt
Issues dictionary: Phoenix.IssueCoding.txt

Setting up StanfordNLP. The program isn't dead. Promise.
Stanford setup complete. Starting parse of 3 stories...
Done with StanfordNLP parse...

Discard sentence:   CHINA FIRST 2012
Discard sentence:   CHINA FIRST 2012
Summary:
Stories read: 0    Sentences coded: 0   Events generated: 0
Discards:  Sentence 2   Story 0   Sentences without events: 0
Coding time: 5.003469944
Finished
</code></pre>

<p>The problem appears to be that StanfordNLP disregards all of my sentences. For those of you with experience, is there anything wrong with my original format? I would really like to make this work and any thoughts will be appreciated!</p>
",Dataset Preprocessing & Handling,stanford nlp petrarch discarding sentence seeking parse set news story petrarch according official document main input format petrarch xml document entry document sentence story parsed input either individual sentence entire story additionally input contain pre parsed information stanfordnlp plain text stanford parse left tabari whether input parsed indicated using p flag petrarch command line argument word petrarch us stanfordnlp part parsing tool news document one txt file without xml structure sentence attribute id ha date want try sample text see work case reprogram text corresponding format sample format accept petrarch program run without error output python code log copied terminal problem appears stanfordnlp disregard sentence experience anything wrong original format would really like make work thought appreciated
Python Scikit-Learn: Custom Analyzer for TfidfVectorizer,"<p>So i am trying to understand how to write a custom analyzer for python scikit-learn's TfidfVectorizer.</p>

<p>I am working on the following Kaggle competition</p>

<p><a href=""https://www.kaggle.com/c/whats-cooking"" rel=""nofollow"">https://www.kaggle.com/c/whats-cooking</a></p>

<p>as a firs step, i do some clean up on the ingredients column as</p>

<pre><code>traindf = pd.read_json('../../data/train.json')

traindf['ingredients_string'] = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in traindf['ingredients']]   
</code></pre>

<p>after that i create a pipeline using TfidfVectorizer and the LogisticRegression classifier</p>

<pre><code>pip = Pipeline([
    ('vect', TfidfVectorizer(
                             stop_words='english',
                             sublinear_tf=True,
                             use_idf=bestParameters['vect__use_idf'],
                             max_df=bestParameters['vect__max_df'],
                             ngram_range=bestParameters['vect__ngram_range']
                             )),         

    ('clf', LogisticRegression(C=bestParameters['clf__C']))
    ])
</code></pre>

<p>then i fit my training set and finally, i predict</p>

<pre><code>X, y = traindf['ingredients_string'], traindf['cuisine'].as_matrix()

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)

parameters = {}

grid_searchTS = GridSearchCV(pip,parameters,n_jobs=3, verbose=1, scoring='accuracy')
grid_searchTS.fit(X_train, y_train)

predictions = grid_searchTS.predict(X_test)
</code></pre>

<p>lastly, i check how my classifier did by</p>

<pre><code>print ('Accuracy:', accuracy_score(y_test, predictions))
print ('Confusion Matrix:', confusion_matrix(y_test, predictions))
print ('Classification Report:', classification_report(y_test, predictions))
</code></pre>

<p>now this gives me around 78% accuracy. fine. now i basically perform the same steps but with one change. instead of creating a new column in the dataframe for a cleaned up version of the ingredients, i want to create a custom analyzer that will do the same thing. so write</p>

<pre><code>def customAnalyzer(text):

    lemTxt = ["""".join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', ingred)) for ingred in lines.lower()]) for lines in sorted(text)]


    return "" "".join(lemTxt).strip()
</code></pre>

<p>and of course i change the pipeline as</p>

<pre><code>pip = Pipeline([
    ('vect', TfidfVectorizer(
                             stop_words='english',
                             sublinear_tf=True,
                             use_idf=bestParameters['vect__use_idf'],
                             max_df=bestParameters['vect__max_df'],
                             ngram_range=bestParameters['vect__ngram_range'],
                             analyzer=customAnalyzer
                             )),         

    ('clf', LogisticRegression(C=bestParameters['clf__C']))
    ])
</code></pre>

<p>lastly, since i think my customAnalyzer will take care of everything, i create my train test split as</p>

<pre><code>X, y = traindf['ingredients'], traindf['cuisine'].as_matrix()

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)
</code></pre>

<p>but to my surprise, my accuracy drops to 24% !</p>

<ol>
<li>Is my intuition of using the custom analyzer in this way correct?</li>
<li>Do i also need to implement a custom tokenizer?</li>
</ol>

<p>My intention is to use each ingredient as an independent entity. I do not want to deal with words. When i create my ngrams, i want the ngrams to be made out of each individual ingredient instead of each word.</p>

<p>How would i achieve this?</p>
",Dataset Preprocessing & Handling,python scikit learn custom analyzer tfidfvectorizer trying understand write custom analyzer python scikit learn tfidfvectorizer working following kaggle competition fir step clean ingredient column create pipeline using tfidfvectorizer logisticregression classifier fit training set finally predict lastly check classifier give around accuracy fine basically perform step one change instead creating new column dataframe cleaned version ingredient want create custom analyzer thing write course change pipeline lastly since think customanalyzer take care everything create train test split surprise accuracy drop intuition using custom analyzer way correct also need implement custom tokenizer intention use ingredient independent entity want deal word create ngrams want ngrams made individual ingredient instead word would achieve
Term Document Matrix for Letters in R,"<p>I would like to build a n-gram 'letter document matrix', which basically uses letter sequences of up to n letters instead of the typical words. Here's a simplified example of what I'd like to achieve:</p>

<pre><code>&gt; letterDocumentMatrix(c('ea','ab','ca'), c('sea','abs','cab'))
    [,sea] [,abs] [,cab]
[ea,] TRUE   FALSE  FALSE  
[ab,] FALSE  TRUE   TRUE   
[ca,] FALSE  FALSE  TRUE
</code></pre>

<p>Is there a name for this type of operation? And are there any prebuilt functions that handles this?</p>

<p>Finally, I tried outer with grepl but to no avail:</p>

<pre><code>&gt; outer(c('ea','ab','ca'), c('sea','abs','cab'), grepl)
          [,1]  [,2]  [,3]
     [1,] TRUE  FALSE FALSE  
     [2,] TRUE  FALSE FALSE
     [3,] TRUE  FALSE FALSE  
     Warning message:
     In FUN(X, Y, ...) :
       argument 'pattern' has length &gt; 1 and only the first element will be used
</code></pre>

<p>Seems like outer passes the whole of the first argument to grepl, instead of one entry at a time, causing grepl to just search for the first term, which is 'a' in this case.</p>
",Dataset Preprocessing & Handling,term document matrix letter r would like build n gram letter document matrix basically us letter sequence n letter instead typical word simplified example like achieve name type operation prebuilt function handle finally tried outer grepl avail seems like outer pass whole first argument grepl instead one entry time causing grepl search first term case
Applied NLP: how to score a document against a lexicon of multi-word terms?,"<p>This is probably a fairly basic NLP question but I have the following task at hand: I have a collection of text documents that I need to score against an (English) lexicon of terms that could be 1-, 2-, 3- etc <code>N</code>-word long. <code>N</code> is bounded by some ""reasonable"" number but the distribution of various terms in the dictionary for various values of <code>n = 1, ..., N</code> might be fairly uniform. This lexicon can, for example, contain a list of devices of certain type and I want to see if a given document is likely about any of these devices. So I would want to score a document high(er) if it has one or more occurrences of any of the lexicon entries. </p>

<p>What is a standard NLP technique to do the scoring while accounting for various forms of the words that may appear in the lexicon? What sort of preprocessing would be required for both the input documents and the lexicon to be able to perform the scoring? What sort of open-source tools exist for both the preprocessing and the scoring?</p>
",Dataset Preprocessing & Handling,applied nlp score document lexicon multi word term probably fairly basic nlp question following task hand collection text document need score english lexicon term could etc word long bounded reasonable number distribution various term dictionary various value might fairly uniform lexicon example contain list device certain type want see given document likely device would want score document high er ha one occurrence lexicon entry standard nlp technique scoring accounting various form word may appear lexicon sort preprocessing would required input document lexicon able perform scoring sort open source tool exist preprocessing scoring
How to edit .csv in python to proceed NLP,"<p>Hello i am not very familiar with programming and found Stackoverflow while researching my task. I want to do natural language processing on a .csv file that looks like this and has about 15.000 rows</p>

<pre><code>    ID | Title        | Body
    ----------------------------------------
    1  | Who is Jack? | Jack is a teacher... 
    2  | Who is Sam?  | Sam is a dog.... 
    3  | Who is Sarah?| Sarah is a doctor...
    4  | Who is Amy?  | Amy is a wrestler... 
</code></pre>

<p>I want to read the .csv file and do some basic NLP operations and write the results back in a new or in the same file. After some research python and nltk seams to be the technologies i need. (i hope thats right). After tokenizing i want my .csv file to look like this</p>

<pre><code>    ID | Title                 | Body
    -----------------------------------------------------------
    1  | ""Who"" ""is"" ""Jack"" ""?"" | ""Jack"" ""is"" ""a"" ""teacher""... 
    2  | ""Who"" ""is"" ""Sam"" ""?""  | ""Sam"" ""is"" ""a"" ""dog"".... 
    3  | ""Who"" ""is"" ""Sarah"" ""?""| ""Sarah"" ""is"" ""a"" ""doctor""...
    4  | ""Who"" ""is"" ""Amy"" ""?""  | ""Amy"" ""is"" ""a"" ""wrestler""... 
</code></pre>

<p>What i have achieved after a day of research and putting pieces together looks like this</p>

<pre><code>    ID | Title                 | Body
    ----------------------------------------------------------
    1  | ""Who"" ""is"" ""Jack"" ""?"" | ""Jack"" ""is"" ""a"" ""teacher""... 
    2  | ""Who"" ""is"" ""Sam"" ""?""  | ""Jack"" ""is"" ""a"" ""teacher""...
    3  | ""Who"" ""is"" ""Sarah"" ""?""| ""Jack"" ""is"" ""a"" ""teacher""...
    4  | ""Who"" ""is"" ""Amy"" ""?""  | ""Jack"" ""is"" ""a"" ""teacher""... 
</code></pre>

<p>My first idea was to read a specific cell in the .csv ,do an operation and write it back to the same cell. And than somehow do that automatically on all rows. Obviously i managed to read a cell and tokenize it. But i could not manage to write it back in that specific cell. And i am far away from ""do that automatically to all rows"". I would appreciate some help if possible.</p>

<p>My code:</p>

<pre><code>    import csv
    from nltk.tokenize import word_tokenize 

    ############Read CSV File######################
    ########## ID , Title, Body#################### 

    line_number = 1 #line to read (need some kind of loop here)
    column_number = 2 # column to read (need some kind of loop here)
    with open('test10in.csv', 'rb') as f:
        reader = csv.reader(f)
        reader = list(reader)
        text = reader[line_number][column_number] 


        stringtext = ''.join(text) #tokenizing just work on strings 
        tokenizedtext = (word_tokenize(stringtext))
        print(tokenizedtext)

    #############Write back in same cell in new CSV File######

    with open('test11out.csv', 'wb') as g:
        writer = csv.writer(g)
        for row in reader:
            row[2] = tokenizedtext
            writer.writerow(row)
</code></pre>

<p>I hope i asked the question correctly and someone can help me out. </p>
",Dataset Preprocessing & Handling,edit csv python proceed nlp hello familiar programming found stackoverflow researching task want natural language processing csv file look like ha row want read csv file basic nlp operation write result back new file research python nltk seam technology need hope thats right tokenizing want csv file look like achieved day research putting piece together look like first idea wa read specific cell csv operation write back cell somehow automatically row obviously managed read cell tokenize could manage write back specific cell far away automatically row would appreciate help possible code hope asked question correctly someone help
How can i cluster document using k-means (Flann with python)?,"<p>I want to cluster documents based on similarity.</p>

<p>I haved tried ssdeep (similarity hashing), very fast but i was told that k-means is faster and flann is fastest of all implementations, and more accurate so i am trying flann with python bindings but i can't find any example how to do it on text (it only support array of numbers).</p>

<p>I am very very new to this field (k-means, natural language processing). What i need is speed and accuracy.</p>

<p>My questions are: </p>

<ol>
<li>Can we do document similarity grouping / Clustering using KMeans (Flann do not allow any text input it seems )</li>
<li>Is Flann the right choice? If not please suggest me High performance library that support text/docs clustering, that have python wrapper/API.</li>
<li>Is k-means the right algorithm?</li>
</ol>
",Dataset Preprocessing & Handling,cluster document using k mean flann python want cluster document based similarity haved tried ssdeep similarity hashing fast wa told k mean faster flann fastest implementation accurate trying flann python binding find example text support array number new field k mean natural language processing need speed accuracy question document similarity grouping clustering using kmeans flann allow text input seems flann right choice please suggest high performance library support text doc clustering python wrapper api k mean right algorithm
How to store Sparsity and Maximum term length of a Term document matrix from tm,"<p>how to store the sparsity and maximum term length of Term Document Matrix in separate variable in R while finding ngrams ?</p>

<pre><code>library(tm)
library(RWeka)

#stdout &lt;- vector('character')
#con &lt;- textConnection('stdout','wr',local = TRUE)

#reading the csv file
worklog &lt;- read.csv(""To_Kamal_WorkLogs.csv"");


#removing the unwanted columns
cols &lt;- c(""A"",""B"",""C"",""D"",""E"",""F"");
colnames(worklog)&lt;-cols;
worklog2 &lt;- worklog[c(""F"")]

#removing non-ASCII characters
z=iconv(worklog2, ""latin1"", ""ASCII"", sub="""")

#cleaning the data Removing Date and Time
worklog2$F=gsub(""[0-9]+/[0-9]+/[0-9]+ [0-9]+:[0-9]+:[0-9]+ [A,P][M]"","""",worklog2$F);


#loading the vector Data to corpus
a &lt;- Corpus(VectorSource(worklog2$F))

#cleaning the data
a &lt;- tm_map(a,removeNumbers)
a &lt;- tm_map(a,removePunctuation)
a &lt;- tm_map(a,stripWhitespace)
a &lt;- tm_map(a,tolower)
a &lt;- tm_map(a, PlainTextDocument)
a &lt;- tm_map(a,removeWords,stopwords(""english"")) 
a &lt;- tm_map(a,stemDocument,language = ""english"")

#removing custom stopwords
stopwords=""open"";
if(!is.null(stopwords)) a &lt;- tm_map(a, removeWords, words=as.character(stopwords))


#finding 2,3,4 grams
bigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm2 &lt;- TermDocumentMatrix(a, control = list(tokenize = bigramTokenizer))
tdm2 &lt;- removeSparseTerms(tdm2, 0.75)

#output

&gt; tdm2
&lt;&lt;TermDocumentMatrix (terms: 27, documents: 8747)&gt;&gt;

Non-/sparse entries: 87804/148365

Sparsity           : 63%

Maximal term length: 20

Weighting          : term frequency (tf)
</code></pre>

<p>how to store the above Sparsity, Maximal term length, Weighting, Non-/sparse entries in separate variables.</p>
",Dataset Preprocessing & Handling,store sparsity maximum term length term document matrix tm store sparsity maximum term length term document matrix separate variable r finding ngrams store sparsity maximal term length weighting non sparse entry separate variable
Calculate distance from densest part of cosine similarity 2d distribution,"<p>Forgive me in advance if my terminology sounds a bit vague, but I am trying to explain my problem in plain English.</p>

<p>Let's say I have 10 sets of documents and for each set I have calculated the cosine similarity matrix based on the term frequency matrix of the set.</p>

<p>In R we can simulate my list of cosine similarity matrices like this</p>

<pre><code>cosine_simil_mat &lt;- list()
for (i in 1:10) {
  cosine_simil_mat[[i]] &lt;-
    matrix(rnorm(n=100, mean=0.89, sd=.2),ncol=10)
}
</code></pre>

<p>Now, for each matrix, I can visualise the distance separating each document on a plane with this</p>

<pre><code>mds &lt;- cmdscale(1-cosine_simil_mat[[1]], eig=TRUE, k=2)
x &lt;- mds$points[,1]
y &lt;- mds$points[,2]
</code></pre>

<p>(note the <code>1-</code> part, since make more sense to visualise the dissimilarities (or distance) not the similarities)</p>

<p>I can plot my <code>mds</code> with </p>

<pre><code>mds_df &lt;-
  data.frame(x,y, type=c(""alpha"", c(rep(""betas"",dim(cosine_simil_mat[[i]])[1]-1)) ) )

require(ggplot2)
ggplot(mds_df, aes(x,y)) +
  geom_point(aes(colour = type)) + geom_density2d() + theme_bw()
</code></pre>

<p>which plots</p>

<p><a href=""https://i.sstatic.net/pL8Br.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pL8Br.png"" alt=""enter image description here""></a></p>

<p>Now, what I want to do is to understand how my document of interest (<code>alpha</code>), which is always the first row/column of the cosine similarity matrix, behaves in the different sets. Specifically, I want to measure the distance of my document <code>alpha</code> from the densest part of each plot, in order to understand whether the document <code>alpha</code> is at the core of the sets, measured in terms of relative term frequencies, or at the periphery and if its position change in the different sets. </p>

<p>Does any statistic capture this distance from the densest part of the plot? Does it make any sense?</p>
",Dataset Preprocessing & Handling,calculate distance densest part cosine similarity distribution forgive advance terminology sound bit vague trying explain problem plain english let say set document set calculated cosine similarity matrix based term frequency matrix set r simulate list cosine similarity matrix like matrix visualise distance separating document plane note part since make sense visualise dissimilarity distance similarity plot plot want understand document interest always first row column cosine similarity matrix behaves different set specifically want measure distance document densest part plot order understand whether document core set measured term relative term frequency periphery position change different set doe statistic capture distance densest part plot doe make sense
Replace words in corpus according to dictionary data frame,"<p>I am interested in replacing all words in a <code>tm</code> Corpus object according to a dictionary made of a two columns data frame, where the first column is the word to be matched and the second column is the replacement word.</p>

<p>I am stuck with the <code>translate</code> function. I saw <a href=""https://stackoverflow.com/a/17110944/1707938"">this answer</a> but I can't transform it in a function to be passed to <code>tm_map</code>.</p>

<p>Please consider the following MWE</p>

<pre><code>library(tm)

docs &lt;- c(""first text"", ""second text"")
corp &lt;- Corpus(VectorSource(docs))

dictionary &lt;- data.frame(word = c('first', 'second', 'text'),
                      translation = c('primo', 'secondo', 'testo'))

translate &lt;- function(text, dictionary) {
  # Would like to replace each word of text with corresponding word in dictionary
}

corp_translated &lt;- tm_map (corp, translate)

inspect(corp_translated)

# Expected result

# A corpus with 2 text documents
#
# The metadata consists of 2 tag-value pairs and a data frame
# Available tags are:
#   create_date creator 
# Available variables in the data frame are:
#   MetaID 

# [[1]]
# primo testo

# [[2]]
# secondo testo
</code></pre>
",Dataset Preprocessing & Handling,replace word corpus according dictionary data frame interested replacing word corpus object according dictionary made two column data frame first column word matched second column replacement word stuck function saw href answer transform function passed please consider following mwe
Ten fold classification and using lib svm to calculate accuracy in python,"<p>I have a term-document matrix and the corresponding label matrix I have to divide the data set into 10 parts and use any random 7 parts for training the libsvm classifier and test on the remaining 3 parts.
I have to do this for all possible cases i.e. 10C7.
Here is the code for training and testing using SVM, I am unable to understand how to classify and iterate for all cases.</p>

<pre><code>m = svm_train(labels[0:2000], rows_1[0:2000], '-c '+str(k)+' -g '+str(g))

p_label, p_acc, p_val = svm_predict(labels[2000:0], rows_1[2000:0], m)
acc.append(p_acc)
</code></pre>

<p>Where 'labels' is the label array and 'rows_1' are rows of term document matrix.
I am new to this please help!</p>
",Dataset Preprocessing & Handling,ten fold classification using lib svm calculate accuracy python term document matrix corresponding label matrix divide data set part use random part training libsvm classifier test remaining part possible case e c code training testing using svm unable understand classify iterate case label label array row row term document matrix new please help
What are the methods except Bag Of Words (TF-IDF) for converting textual features into numerical features?,"<p>I have been working on Natural Language Processing these days. My aim is to classify different words in a multi-lingual sentence written in Roman Script based on some criteria. Thus, I need a classifier for it. Unquestionably, there are many. But since my features aren't numerical but textual, and most of the classifiers like Support Vector Machine (SVM) input numerical features, I looked for some methodology to convert my textual features into numerical one. Though the concept of Bag Of Words with the use of Term Frequency and Inverse Document Frequency (TF-IDF) is a generic approach for this purpose, one of my textual feature, namely local context, is of fixed length and i want to know if it is possible to convert it into numerical feature without using TF-IDF. Local context feature refers to considering previous two and next two words (which comprise the context of a particular word). Therefore, I am looking for any other methodology which could prove to be better in this case. I found similar query at Cross Validated <a href=""https://stats.stackexchange.com/questions/144327/convert-categorical-data-into-numerical-data"">here</a>, but that is for document clustering and i want to classify individual words into different classes. I also found one unanswered similar <a href=""https://www.quora.com/What-are-different-techniques-for-converting-qualitative-features-into-numerical-features-for-machine-learning-algorithms"" rel=""nofollow noreferrer"">question</a> on quora.</p>

<p>To serve my purpose, I want either the textual feature to be converted into numerical one or a classifier that can take textual features as input. Is there any one who could help me...</p>
",Dataset Preprocessing & Handling,method except bag word tf idf converting textual feature numerical feature working natural language processing day aim classify different word multi lingual sentence written roman script based criterion thus need classifier unquestionably many since feature numerical textual classifier like support vector machine svm input numerical feature looked methodology convert textual feature numerical one though concept bag word use term frequency inverse document frequency tf idf generic approach purpose one textual feature namely local context fixed length want know possible convert numerical feature without using tf idf local context feature refers considering previous two next two word comprise context particular word therefore looking methodology could prove better case found similar query cross validated question quora serve purpose want either textual feature converted numerical one classifier take textual feature input one could help
&quot;Combine&quot; TF-IDF scores for single class of documents within corpus,"<p>Let's say I've calculated the TF-IDF scores for a corpus of documents, resulting in a matrix of TF-IDF features. If a subset of those documents are of a certain class, can I somehow ""combine"" the scores of that subset to get a single value for each feature in that class in a meaningful way?</p>

<p>For example, if I have a corpus of 1000 documents, and <code>corpus[0:200]</code> are of class A, then can I take rows 0-200 of the tf-idf scores and somehow combine them so that I can say, ""In class A, features[3] has a score of 0.5.""</p>

<p>My hope is to extract the most meaningful terms from each class within the corpus. Is there a reasonable way to do this? Does my question demonstrate a misunderstanding of the concept to begin with? Any feedback at all is wildly appreciated.</p>
",Dataset Preprocessing & Handling,combine tf idf score single class document within corpus let say calculated tf idf score corpus document resulting matrix tf idf feature subset document certain class somehow combine score subset get single value feature class meaningful way example corpus document class take row tf idf score somehow combine say class feature ha score hope extract meaningful term class within corpus reasonable way doe question demonstrate misunderstanding concept begin feedback wildly appreciated
Why is my Python script so much slower than its R equivalent?,"<p><strong>NOTE</strong>: this question covers <em>why</em> the script is so slow. However, if you are more the kind of person who wants to improve something you can take a look at<a href=""https://codereview.stackexchange.com/questions/101648/speed-up-python-execution-time"">my post on CodeReview which aims to improve the performance</a>.</p>

<p>I am working on a project which crunches plain text files (.lst).</p>

<p>The name of the file names (<code>fileName</code>) are important because I'll extract <code>node</code> (e.g. <em>abessijn</em>) and <code>component</code> (e.g. WR-P-E-A) from them into a dataframe. Examples:</p>

<pre><code>abessijn.WR-P-E-A.lst
A-bom.WR-P-E-A.lst
acroniem.WR-P-E-C.lst
acroniem.WR-P-E-G.lst
adapter.WR-P-E-A.lst
adapter.WR-P-E-C.lst
adapter.WR-P-E-G.lst
</code></pre>

<p>Each file consists of one or more line. Each line consists of a sentence (inside <code>&lt;sentence&gt;</code> tags). Example (abessijn.WR-P-E-A.lst)</p>

<pre><code>/home/nobackup/SONAR/COMPACT/WR-P-E-A/WR-P-E-A0000364.data.ids.xml:  &lt;sentence&gt;Vooral mijn abessijn ruikt heerlijk kruidig .. : ) )&lt;/sentence&gt;
/home/nobackup/SONAR/COMPACT/WR-P-E-A/WR-P-E-A0000364.data.ids.xml:  &lt;sentence&gt;Mijn abessijn denkt daar heel anders over .. : ) ) Maar mijn kinderen richt ik ook niet af , zit niet in mijn bloed .&lt;/sentence&gt;
</code></pre>

<p>From each line I extract the sentence, do some small modifications to it, and call it <code>sentence</code>. Up next is an element called <code>leftContext</code>, which takes the first part of the split between <code>node</code> (e.g. <em>abessijn</em>) and the sentence it came from. Finally, from <code>leftContext</code> I get precedingWord, which is the word preceding <code>node</code> in <code>sentence</code>, or the right most word in <code>leftContext</code> (with some limitations such as the option of a compound formed with a hyphen). Example:</p>

<pre><code>ID | filename             | node | component | precedingWord      | leftContext                               |  sentence
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
1    adapter.WR-P-P-F.lst  adapter  WR-P-P-F   aanpassingseenheid  Een aanpassingseenheid (                      Een aanpassingseenheid ( adapter ) , 
2    adapter.WR-P-P-F.lst  adapter  WR-P-P-F   toestel             Het toestel (                                 Het toestel ( adapter ) draagt zorg voor de overbrenging van gegevens
3    adapter.WR-P-P-F.lst  adapter  WR-P-P-F   de                  de aansluiting tussen de sensor en de         de aansluiting tussen de sensor en de adapter , 
4    airbag.WS-U-E-A.lst   airbag   WS-U-E-A   den                 ja voor den                                   ja voor den airbag op te pompen eh :p
5    airbag.WS-U-E-A.lst   airbag   WS-U-E-A   ne                  Dobby , als ze valt heeft ze dan wel al ne    Dobby , als ze valt heeft ze dan wel al ne airbag hee
</code></pre>

<p>That dataframe is exported as dataset.csv.</p>

<p>After that, the intention of my project comes at hand: I create a frequency table that takes <code>node</code> and <code>precedingWord</code> into account. From a variable I define <code>neuter</code> and <code>non_neuter</code>, e.g (in Python)</p>

<pre><code>neuter = [""het"", ""Het""]
non_neuter = [""de"",""De""]
</code></pre>

<p>and a rest category <code>unspecified</code>. When <code>precedingWord</code> is an item from the list, assign it to the variable. Example of a frequency table output:</p>

<pre><code>node    |   neuter   | nonNeuter   | unspecified
-------------------------------------------------
A-bom       0          4             2
acroniem    3          0             2
act         3          2             1
</code></pre>

<p>The frequency list is exported as frequencies.csv.</p>

<hr>

<p>I started out with R, considering that later on I'd do some statistical analyses on the frequencies. My current R script (also available as <a href=""http://pastebin.com/gtCrAdMq"" rel=""nofollow noreferrer"">paste</a>):</p>

<pre><code># ---
# STEP 0: Preparations
  start_time &lt;- Sys.time()
  ## 1. Set working directory in R
    setwd("""")

  ## 2. Load required library/libraries
    library(dplyr)
    library(mclm)
    library(stringi)

  ## 3. Create directory where we'll save our dataset(s)
    dir.create(""../R/dataset"", showWarnings = FALSE)


# ---
# STEP 1: Loop through files, get data from the filename

    ## 1. Create first dataframe, based on filename of all files
    files &lt;- list.files(pattern=""*.lst"", full.names=T, recursive=FALSE)
    d &lt;- data.frame(fileName = unname(sapply(files, basename)), stringsAsFactors = FALSE)

    ## 2. Create additional columns (word &amp; component) based on filename
    d$node &lt;- sub(""\\..+"", """", d$fileName, perl=TRUE)
    d$node &lt;- tolower(d$node)
    d$component &lt;- gsub(""^[^\\.]+\\.|\\.lst$"", """", d$fileName, perl=TRUE)


# ---
# STEP 2: Loop through files again, but now also through its contents
# In other words: get the sentences

    ## 1. Create second set which is an rbind of multiple frames
    ## One two-column data.frame per file
    ## First column is fileName, second column is data from each file
    e &lt;- do.call(rbind, lapply(files, function(x) {
        data.frame(fileName = x, sentence = readLines(x, encoding=""UTF-8""), stringsAsFactors = FALSE)
    }))

    ## 2. Clean fileName
     e$fileName &lt;- sub(""^\\.\\/"", """", e$fileName, perl=TRUE)

    ## 3. Get the sentence and clean
    e$sentence &lt;- gsub("".*?&lt;sentence&gt;(.*?)&lt;/sentence&gt;"", ""\\1"", e$sentence, perl=TRUE)
    e$sentence &lt;- tolower(e$sentence)
        # Remove floating space before/after punctuation
        e$sentence &lt;- gsub(""\\s(?:(?=[.,:;?!) ])|(?&lt;=\\( ))"", ""\\1"", e$sentence, perl=TRUE)
    # Add space after triple dots ...
      e$sentence &lt;- gsub(""\\.{3}(?=[^\\s])"", ""... "", e$sentence, perl=TRUE)

    # Transform HTML entities into characters
    # It is unfortunate that there's no easier way to do this
    # E.g. Python provides the HTML package which can unescape (decode) HTML
    # characters
        e$sentence &lt;- gsub(""&amp;apos;"", ""'"", e$sentence, perl=TRUE)
        e$sentence &lt;- gsub(""&amp;amp;"", ""&amp;"", e$sentence, perl=TRUE)
      # Avoid R from wrongly interpreting "", so replace by single quotes
        e$sentence &lt;- gsub(""&amp;quot;|\"""", ""'"", e$sentence, perl=TRUE)

      # Get rid of some characters we can't use such as ³ and ¾
      e$sentence &lt;- gsub(""[^[:graph:]\\s]"", """", e$sentence, perl=TRUE)


# ---
# STEP 3:
# Create final dataframe

  ## 1. Merge d and e by common column name fileName
    df &lt;- merge(d, e, by=""fileName"", all=TRUE)

  ## 2. Make sure that only those sentences in which df$node is present in df$sentence are taken into account
    matchFunction &lt;- function(x, y) any(x == y)
    matchedFrame &lt;- with(df, mapply(matchFunction, node, stri_split_regex(sentence, ""[ :?.,]"")))
    df &lt;- df[matchedFrame, ]

  ## 3. Create leftContext based on the split of the word and the sentence
    # Use paste0 to make sure we are looking for the node, not a compound
    # node can only be preceded by a space, but can be followed by punctuation as well
    contexts &lt;- strsplit(df$sentence, paste0(""(^| )"", df$node, ""( |[!\"",.:;?})\\]])""), perl=TRUE)
    df$leftContext &lt;- sapply(contexts, `[`, 1)

  ## 4. Get the word preceding the node
    df$precedingWord &lt;- gsub(""^.*\\b(?&lt;!-)(\\w+(?:-\\w+)*)[^\\w]*$"",""\\1"", df$leftContext, perl=TRUE)

  ## 5. Improve readability by sorting columns
    df &lt;- df[c(""fileName"", ""component"", ""precedingWord"", ""node"", ""leftContext"", ""sentence"")]

  ## 6. Write dataset to dataset dir
    write.dataset(df,""../R/dataset/r-dataset.csv"")


# ---
# STEP 4:
# Create dataset with frequencies

  ## 1. Define neuter and nonNeuter classes
    neuter &lt;- c(""het"")
    non.neuter&lt;- c(""de"")

  ## 2. Mutate df to fit into usable frame
    freq &lt;- mutate(df, gender = ifelse(!df$precedingWord %in% c(neuter, non.neuter), ""unspecified"",
      ifelse(df$precedingWord %in% neuter, ""neuter"", ""non_neuter"")))

  ## 3. Transform into table, but still usable as data frame (i.e. matrix)
  ## Also add column name ""node""
    freqTable &lt;- table(freq$node, freq$gender) %&gt;%
      as.data.frame.matrix %&gt;%
      mutate(node = row.names(.))

  ## 4. Small adjustements
    freqTable &lt;- freqTable[,c(4,1:3)]

  ## 5. Write dataset to dataset dir
    write.dataset(freqTable,""../R/dataset/r-frequencies.csv"")


    diff &lt;- Sys.time() - start_time # calculate difference
    print(diff) # print in nice format
</code></pre>

<p>However, since I'm using a big dataset (16,500 files, all with multiple lines) it seemed to take quite long. On my system the whole process took about an hour and a quarter. I thought to myself that there ought to be a language out there that could do this more quickly, so I went and taught myself some Python and asked a lot of question here on SO.</p>

<p>Finally I came up with the following script (<a href=""http://pastebin.com/rPFJKuBV"" rel=""nofollow noreferrer"">paste</a>).</p>

<pre><code>import os, pandas as pd, numpy as np, regex as re

from glob import glob
from datetime import datetime
from html import unescape

start_time = datetime.now()

# Create empty dataframe with correct column names
columnNames = [""fileName"", ""component"", ""precedingWord"", ""node"", ""leftContext"", ""sentence"" ]
df = pd.DataFrame(data=np.zeros((0,len(columnNames))), columns=columnNames)

# Create correct path where to fetch files
subdir = ""rawdata""
path = os.path.abspath(os.path.join(os.getcwd(), os.pardir, subdir))

# ""Cache"" regex
# See http://stackoverflow.com/q/452104/1150683
p_filename = re.compile(r""[./\\]"")

p_sentence = re.compile(r""&lt;sentence&gt;(.*?)&lt;/sentence&gt;"")
p_typography = re.compile(r"" (?:(?=[.,:;?!) ])|(?&lt;=\( ))"")
p_non_graph = re.compile(r""[^\x21-\x7E\s]"")
p_quote = re.compile(r""\"""")
p_ellipsis = re.compile(r""\.{3}(?=[^ ])"")

p_last_word = re.compile(r""^.*\b(?&lt;!-)(\w+(?:-\w+)*)[^\w]*$"", re.U)

# Loop files in folder
for file in glob(path+""\\*.lst""):
    with open(file, encoding=""utf-8"") as f:
        [n, c] = p_filename.split(file.lower())[-3:-1]
        fn = ""."".join([n, c])
        for line in f:
            s = p_sentence.search(unescape(line)).group(1)
            s = s.lower()
            s = p_typography.sub("""", s)
            s = p_non_graph.sub("""", s)
            s = p_quote.sub(""'"", s)
            s = p_ellipsis.sub(""... "", s)

            if n in re.split(r""[ :?.,]"", s):
                lc = re.split(r""(^| )"" + n + ""( |[!\"",.:;?})\]])"", s)[0]

                pw = p_last_word.sub(""\\1"", lc)

                df = df.append([dict(fileName=fn, component=c, 
                                   precedingWord=pw, node=n, 
                                   leftContext=lc, sentence=s)])
            continue

# Reset indices
df.reset_index(drop=True, inplace=True)

# Export dataset
df.to_csv(""dataset/py-dataset.csv"", sep=""\t"", encoding=""utf-8"")

# Let's make a frequency list
# Create new dataframe

# Define neuter and non_neuter
neuter = [""het""]
non_neuter = [""de""]

# Create crosstab
df.loc[df.precedingWord.isin(neuter), ""gender""] = ""neuter""
df.loc[df.precedingWord.isin(non_neuter), ""gender""] = ""non_neuter""
df.loc[df.precedingWord.isin(neuter + non_neuter)==0, ""gender""] = ""rest""

freqDf = pd.crosstab(df.node, df.gender)

freqDf.to_csv(""dataset/py-frequencies.csv"", sep=""\t"", encoding=""utf-8"")

# How long has the script been running?
time_difference = datetime.now() - start_time
print(""Time difference of"", time_difference)
</code></pre>

<p>After making sure that the output of both scripts is identical, I thought I'd put them to the test.</p>

<p>I am running on Windows 10 64 bit with a quad-core processor and 8 GB Ram. For R I'm using RGui 64 bit 3.2.2 and Python runs on version <em>3.4.3</em> (Anaconda) and is executed in Spyder. Note that I'm running Python in 32 bit because I'd like to use the <a href=""http://www.nltk.org/"" rel=""nofollow noreferrer"">nltk module</a> in the future and they discourage users to use  64 bit.</p>

<p>What I found was that R finished in approximately 55 minutes. But Python has been running for two hours straight already and I can see in the variable explorer that it's only at <code>business.wr-p-p-g.lst</code> (files are sorted alphabetically). <strong>It is waaaaayyyy slower!</strong></p>

<p>So what I did was make a test case and see how both scripts perform with a much smaller dataset. I took around 100 files (instead of 16,500) and ran the script. Again, R was much faster. R finished in around 2 seconds, Python in 17!</p>

<p>Seeing that the goal of Python was to make everything go more smoothly, I was confused. I read Python was fast (and R rather slow), so where did I go wrong? What is the problem? Is Python slower in reading files and lines, or in doing regexes? Or is R simply better equipped to dealing with dataframes and can't it be beaten by pandas? <em>Or</em> is my code simply badly optimised and should Python indeed be the victor?</p>

<p><strong>My question is thus: why is Python slower than R in this case, and - if possible - how can we improve Python to shine?</strong></p>

<p>Everyone who is willing to give either script a try can download the test data I used <a href=""http://bramvanroy.be/files/testdata.zip"" rel=""nofollow noreferrer"">here</a>. Please give me a heads-up when you downloaded the files.</p>
",Dataset Preprocessing & Handling,python script much slower r equivalent note question cover script slow however kind person want improve something take look atpaste however since using big dataset file multiple line seemed take quite long system whole process took hour quarter thought ought language could quickly went taught python asked lot question finally came following script paste making sure output script identical thought put test running window bit quad core processor gb ram r using rgui bit python run version anaconda executed spyder note running python bit like use nltk module future discourage user use bit found wa r finished approximately minute python ha running two hour straight already see variable explorer file sorted alphabetically waaaaayyyy slower wa make test case see script perform much smaller dataset took around file instead ran script r wa much faster r finished around second python seeing goal python wa make everything go smoothly wa confused read python wa fast r rather slow go wrong problem python slower reading file line regexes r simply better equipped dealing dataframes beaten panda code simply badly optimised python indeed victor question thus python slower r case possible improve python shine everyone willing give either script try download test data used please give head downloaded file
Splitting DocumentTermMatrix in R,"<p>I'm looking to create a word pair prediction function, but am having trouble working with DocumentTermMatrix to data frame or similar to use in prediction function.  Here is my working code:</p>

<pre><code>library(tm); 
BigramTokenizer &lt;-
function(x)
    unlist(lapply(ngrams(words(x), 2), paste, collapse = "" ""), use.names = FALSE)

tdm_pairs &lt;- DocumentTermMatrix(my_corpus, control = list(tokenize = BigramTokenizer))

freq_pairs &lt;- colSums(as.matrix(tdm_pairs))

freq_pairs[100]

abandon contemporary 
               1 
</code></pre>

<p>I'm looking to split this and put into a dataframe, so I can use for a prediction function.  I use the following:</p>

<pre><code>for (i in 1:10){
df &lt;- rbind(df,(unlist(strsplit(as.character(freq_pairs)[i],"" ""))[1]))
}
</code></pre>

<p>The output is all 1's.  I would like the output to be:</p>

<pre><code> ""abandon"" ""contemporary"" ""1""
</code></pre>
",Dataset Preprocessing & Handling,splitting documenttermmatrix r looking create word pair prediction function trouble working documenttermmatrix data frame similar use prediction function working code looking split put dataframe use prediction function use following output would like output
Store large text corpus in Python,"<p>I am trying to build a large text corpus from the <a href=""http://dumps.wikimedia.org/"" rel=""nofollow"">Wikipedia dump</a>. 
I represent every article as a <em>document</em> object consisting of:</p>

<ul>
<li>the original text: a string</li>
<li>the preprocessed text: a list of tuples, where each tuple contains a (stemmed) word and the position of the word in the original text</li>
<li>some additional information like title and author</li>
</ul>

<p>I am searching for an efficient way to save these objects to disk. The following operations should be possible:</p>

<ul>
<li>adding a new document</li>
<li>accessing documents via an ID</li>
<li>iterate over all documents</li>
</ul>

<p>It is not necessary to remove an object once it was added.</p>

<p>I could imagine the following methods:</p>

<ul>
<li><strong>Serializing each article to a separate file, for example using pickle:</strong> the downside here are probably lots of operating system calls</li>
<li><strong>Store all documents to a single xml file or blocks of documents to several xml files:</strong> Storing the list that represents the preprocessed document in xml format uses a lot of overhead and I think its quite slow to read a list from xml</li>
<li><strong>Using an existing package for storing the corpus:</strong> I found <a href=""https://pypi.python.org/pypi/Corpora/1.0"" rel=""nofollow"">Corpora</a> package, which seems to be very fast and efficient, but it only supports storing strings plus a header including metadata. Simply putting the preprocessed text into the header makes it run incredibly slow.</li>
</ul>

<p>What would be a good way to do this? Maybe a package for that purpose, which i have not found until now?</p>
",Dataset Preprocessing & Handling,store large text corpus python trying build large text corpus wikipedia dump represent every article document object consisting original text string preprocessed text list tuples tuple contains stemmed word position word original text additional information like title author searching efficient way save object disk following operation possible adding new document accessing document via id iterate document necessary remove object wa added could imagine following method serializing article separate file example using pickle downside probably lot operating system call store document single xml file block document several xml file storing list represents preprocessed document xml format us lot overhead think quite slow read list xml using existing package storing corpus found corpus package seems fast efficient support storing string plus header including metadata simply putting preprocessed text header make run incredibly slow would good way maybe package purpose found
Creating TermDocumentMatrix: issue with number of documents,"<p>I'm attempting to create a term document matrix with a text file that is about 3+ million lines of text. I have created a random sample of the text, which results in about 300,000 lines.</p>

<p>Unfortunately when use the following code I end up with 300,000 documents. I just want 1 document with the frequencies for each bigram:</p>

<pre><code>library(RWeka)
library(tm)

corpus &lt;- readLines(""myfile"")
numberLinesCorpus &lt;- 3000000
corpus_sample &lt;- text_corpus[sample(1:numberLinesCorpus, numberLinesCorpus*.1, replace = FALSE)]
myCorpus &lt;- Corpus(VectorSource(corpus_sample))
BigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 2))
tdm &lt;- TermDocumentMatrix(myCorpus, control = list(tokenize = BigramTokenizer))
</code></pre>

<p>The sample contains approximately 300,000 lines. However, the number of documents in tdm is also 300,000.</p>

<p>Any help would be much appreciated. </p>
",Dataset Preprocessing & Handling,creating termdocumentmatrix issue number document attempting create term document matrix text file million line text created random sample text result line unfortunately use following code end document want document frequency bigram sample contains approximately line however number document tdm also help would much appreciated
How to use the language option in synsets (nltk) if you load a wordnet manually?,"<p>For specific purposes I have to use the Wordnet 1.6 instead of the current version implemented in the nltk package. I then downloaded the old version <a href=""http://wordnet.princeton.edu/wordnet/download/old-versions/"" rel=""noreferrer"">here</a> and tried to run a simple extract of code using the french option. </p>

<pre><code>from collections import defaultdict
import nltk
#nltk.download() 
import os
import sys
from nltk.corpus import WordNetCorpusReader

cwd = os.getcwd()
nltk.data.path.append(cwd)
wordnet16_dir=""wordnet-1.6/""
wn16_path = ""{0}/dict"".format(wordnet16_dir)
wn = WordNetCorpusReader(os.path.abspath(""{0}/{1}"".format(cwd, wn16_path)), nltk.data.find(wn16_path))

senses=wn.synsets('gouvernement',lang=u'fre')
</code></pre>

<p>It seems that the wordnet I manually downloaded cannot be linked to the files of the nltk module dealing with foreign languages, the error I get is the following : </p>

<pre><code>Traceback (most recent call last):
File ""C:/Users/Stephanie/Test/temp.py"", line 16, in &lt;module&gt;
senses=wn.synsets('gouvernement',lang=u'fre')
File ""C:\Python27\lib\site-packages\nltk\corpus\reader\wordnet.py"", line 1419, in synsets
self._load_lang_data(lang)
File ""C:\Python27\lib\site-packages\nltk\corpus\reader\wordnet.py"", line 1064, in _load_lang_data
if lang not in self.langs():
File ""C:\Python27\lib\site-packages\nltk\corpus\reader\wordnet.py"", line 1088, in langs
fileids = self._omw_reader.fileids()
AttributeError: 'FileSystemPathPointer' object has no attribute 'fileids'
</code></pre>

<p>Using an english word doesn't generate any error (so it's not that I did not load the dictionary well) : </p>

<pre><code>senses=wn.synsets('government')
print senses

[Synset('government.n.01'), Synset('government.n.02'), Synset('government.n.03'), Synset('politics.n.02')]
</code></pre>

<p>If I use the current version of Wordnet loaded with the nltk module I don't have any problem using french (so it's not a syntax problem with the optional argument) </p>

<pre><code>from nltk.corpus import wordnet as wn
senses=wn.synsets('gouvernement',lang=u'fre')
print senses
[Synset('government.n.02'), Synset('opinion.n.05'), Synset('government.n.03'), Synset('rule.n.01'), Synset('politics.n.02'), Synset('government.n.01'), Synset('regulation.n.03'), Synset('reign.n.03')]
</code></pre>

<p>But, as precised, I really have to use the old version. I guess this might be a path problem. I've been trying to read the code of the WordNetCorpusReader function but I am quite new with python I don't really see what the problem is so far, except that it doesn't find a special file.</p>

<p>The needed file seems to be wn-data-fre.tab which is located in \nltk_data\corpora\omw\fre. I am pretty sure that I have to change the file with a version compatible with wordnet 1.6 but still, why the function WordNetCorpusReader can't find it ? </p>
",Dataset Preprocessing & Handling,use language option synset nltk load wordnet manually specific purpose use wordnet instead current version implemented nltk package downloaded old version tried run simple extract code using french option seems wordnet manually downloaded linked file nltk module dealing foreign language error get following using english word generate error load dictionary well use current version wordnet loaded nltk module problem using french syntax problem optional argument precised really use old version guess might path problem trying read code wordnetcorpusreader function quite new python really see problem far except find special file needed file seems wn data fre tab located nltk data corpus omw fre pretty sure change file version compatible wordnet still function wordnetcorpusreader find
Still have punctuation issues after removePunctuation function,"<p>I have used the removePuncutation from the ""tm"" package in R on a Term Document Matrix. For some reason I am still left with strange characters in my plot of the letters versus their proportion in a corpus I've analyzed.</p>

<p>Below is the code I used to clean the corpus:</p>

<pre><code>docs &lt;- tm_map(docs, toSpace, ""/|@|\\|"")
docs &lt;- tm_map(docs, content_transformer(tolower))
docs &lt;- tm_map(docs, removeNumbers)
docs &lt;- tm_map(docs, removePunctuation)
docs &lt;- tm_map(docs, stripWhitespace)
dtm &lt;- DocumentTermMatrix(docs)
freq &lt;- colSums(as.matrix(dtm))
words &lt;- dtm %&gt;%as.matrix %&gt;%colnames %&gt;% (function(x) x[nchar(x) &lt; 20])
library(dplyr)
library(stringr)
words %&gt;%str_split("""") %&gt;%sapply(function(x) x[-1]) %&gt;%unlist%&gt;%dist_tab %&gt;%mutate(Letter=factor(toupper(interval),levels=toupper(interval[order(freq)]))) %&gt;%ggplot(aes(Letter, weight=percent))+geom_bar()+coord_flip()+ylab(""Proportion"")+scale_y_continuous(breaks=seq(0, 12,2),label=function(x) paste0(x, ""%""),expand=c(0,0), limits=c(0,12))
</code></pre>

<p>I'm left with the following plot:</p>

<p><img src=""https://i.sstatic.net/ZMc94.jpg"" alt=""enter image description here""></p>

<p>I'm trying to figure out what went wrong here. </p>
",Dataset Preprocessing & Handling,still punctuation issue removepunctuation function used removepuncutation tm package r term document matrix reason still left strange character plot letter versus proportion corpus analyzed code used clean corpus left following plot trying figure went wrong
Why isn&#39;t stemDocument stemming?,"<p>I am using the 'tm' package in R to create a term document matrix using stemmed terms. The process is completing, but the resulting matrix includes terms that don't appear to have been stemmed, and I'm trying to understand why that is and how to fix it.</p>

<p>Here is the script for the process, which uses a couple of online news stories as the sandbox:</p>

<pre><code>library(boilerpipeR)
library(RCurl)
library(tm)

# Pull the relevant parts of the news stories using 'boilerpipeR' and 'RCurl'
url &lt;- ""http://blogs.wsj.com/digits/2015/07/14/google-mozilla-disable-flash-over-security-concerns/""
extract &lt;- LargestContentExtractor(getURL(url))
url2 &lt;- ""http://www.cnet.com/news/startup-lands-100-million-to-challenge-smartphone-superpowers-apple-and-google/""
extract2 &lt;- LargestContentExtractor(getURL(url2))

# Now put those text vectors in a corpus and create a tdm
news.corpus &lt;- VCorpus(VectorSource(c(extract, extract2)))
news.tdm &lt;- TermDocumentMatrix(news.corpus,
  control = list(removePunctuation = TRUE,
                 stopwords = TRUE,
                 stripWhitespace = TRUE,
                 stemDocument = TRUE))

# Now inspect the result
findFreqTerms(news, 4)
</code></pre>

<p>Here is the output that last line produces:</p>

<pre><code>[1] ""acadine""       ""adobe""         ""android""       ""browser""       ""challenge""     ""companies""     ""company""       ""devices""       ""firefox""       ""flash""        
[11] ""funding""       ""gong""          ""hackers""       ""international"" ""ios""           ""like""          ""million""       ""mobile""        ""mozilla""       ""mozillas""     
[21] ""new""           ""online""        ""operating""     ""said""          ""security""      ""smartphones""   ""software""      ""startup""       ""system""        ""systems""      
[31] ""tsinghua""      ""unigroup""      ""used""          ""users""         ""videos""        ""web""           ""will""  
</code></pre>

<p>In line 1, for example, we see ""companies"" and ""company"", and we see ""devices"". I thought stemming would reduce ""companies"" and ""company"" to the same stem (""compani""?), and I thought it would trim the ""s"" off plurals like ""devices"". Am I wrong about that? If not, why isn't this code producing the desired result here?</p>
",Dataset Preprocessing & Handling,stemdocument stemming using tm package r create term document matrix using stemmed term process completing resulting matrix includes term appear stemmed trying understand fix script process us couple online news story sandbox output last line produce line example see company company see device thought stemming would reduce company company stem compani thought would trim plural like device wrong code producing desired result
Performing Text Analytics on a text Column in Dataframe in R,"<p>I have imported a CSV file into a dataframe in R and one of the columns contains Text. </p>

<p>I want to perform analysis on the text. How do I go about it? </p>

<p>I tried making a new dataframe containing only the text column.</p>

<pre><code>OnlyTXT= Txtanalytics1 %&gt;%
  select(problem_note_text)
View(OnlyTXT). 
</code></pre>
",Dataset Preprocessing & Handling,performing text analytics text column dataframe r imported csv file dataframe r one column contains text want perform analysis text go tried making new dataframe containing text column
Why is Java&#39;s BreakIterator is adding extra commas to my text?,"<p>I'm using Java's <code>BreakIterator</code> class to break a passage of text into sentences in various languages. It works pretty well, but for some reason it adds commas to the text where they weren't there before.</p>
<p>It looks like it adds:</p>
<pre><code>, ,
</code></pre>
<p>to the text where paragraph breaks are in the original text. It also adds commas before other commas for some reason.</p>
<p>Below is an example of the type of results that I get</p>
<blockquote>
<p>First of all though, I've got to get up,, my train leaves at five</p>
<p>.</p>
<p>&quot;, , And he looked over at the alarm clock, ticking on the chest of, drawers.</p>
<p>&quot;God in Heaven!&quot;</p>
</blockquote>
<p>The text should look more like this:</p>
<pre><code>First of all though, I've got to get up, my train leaves at five.
And he looked over at the alarm clock, ticking on the chest of drawers.
&quot;God in Heaven!&quot; he thought.
</code></pre>
<p>This is the original passage:</p>
<pre><code>First of all though, I've got to get up,
my train leaves at five.&quot;

And he looked over at the alarm clock, ticking on the chest of
drawers.  &quot;God in Heaven!&quot; he thought.
</code></pre>
<p>I get most of what I need done but I still have to go back after I've broken the text up into sentences and manually edit out all of the extra commas.</p>
<p>As you might have imagined, searching for &quot;java breakiterator extra commas&quot; hasn't gotten me many useful results.</p>
<p>Below is the function that I'm using to do the sentence detection.</p>
<pre><code>public ArrayList&lt;String&gt; tokenize(String text, Locale locale)
{
    ArrayList&lt;String&gt; sentences = new ArrayList&lt;String&gt;();
    BreakIterator sentenceIterator = BreakIterator.getSentenceInstance(locale);
    sentenceIterator.setText(text);
    int boundary = sentenceIterator.first();
    int lastBoundary = 0;
    while (boundary != BreakIterator.DONE)
    {
        boundary = sentenceIterator.next();
        if(boundary != BreakIterator.DONE)
        {
            sentences.add(text.substring(lastBoundary, boundary));
        }
        lastBoundary = boundary;
    }
    return sentences;
}
</code></pre>
<p>Below is the section of code that I'm using to read in the files to memory and feed them to my sentence splitter:</p>
<pre><code>FileHelper fileHelper = new FileHelper();
TextTokenizer textTokenizer = new TextTokenizer();
Constants constants = new Constants();


ArrayList&lt;String&gt; enMetamorph = fileHelper.readFileToMemory(
        constants.books(&quot;metamorphosis_en.txt&quot;));

ArrayList&lt;String&gt; enTokenMetamorph = textTokenizer.tokenize(
        enMetamorph.toString(),Locale.US);

fileHelper.writeFile(enTokenMetamorph,constants.tokenized(
        &quot;metamorphosis_en.txt&quot;));
</code></pre>
<p>The text that I'm using is <em>The Metamorphosis</em> by Franz Kafka. You can find a free UTF-8 text version on Project Gutenberg <a href=""http://www.gutenberg.org/cache/epub/5200/pg5200.txt"" rel=""nofollow noreferrer"">here</a>.
The constants object is just used to create filepaths. I use a function called makeFilePath in the books function that will find the books directory no matter what computer the program is run on. That function is below:</p>
<pre><code>public static String makeFilePath(String addition)
{
    String filePath = new File(&quot;&quot;).getAbsolutePath();
    filePath = filePath+addition;
    return filePath;
}
</code></pre>
<p>Does anybody know why I'm getting all of these extra commas in my text?</p>
",Dataset Preprocessing & Handling,java breakiterator adding extra comma text using java class break passage text sentence various language work pretty well reason add comma text look like add text paragraph break original text also add comma comma reason example type result get first though got get train leaf five looked alarm clock ticking chest drawer god heaven text look like original passage get need done still go back broken text sentence manually edit extra comma might imagined searching java breakiterator extra comma gotten many useful result function using sentence detection section code using read file memory feed sentence splitter text using metamorphosis franz kafka find free utf text version project gutenberg constant object used create filepaths use function called makefilepath book function find book directory matter computer program run function doe anybody know getting extra comma text
term frequency over time: how to plot +200 graphs in one plot with Python/pandas/matplotlib?,"<p>I am conducting a textual content analysis of several web blogs, and now focusing on finding emerging trends. In order to do so for one blog, I coded a multi-step process:</p>

<ol>
<li>looping over all the posts, finding the top 5 keywords in each post</li>
<li>adding them to a list, if they are not already in the list</li>
<li>calculate the term frequency for all the terms in the list for every single post</li>
<li>create a list of dictionaries, where for each post I save the date of the post, and the tf for every single word</li>
<li>create a data frame from this list of dictionaries, and plot it</li>
</ol>

<p>This works all fine, except that I get a 1000 graphs in one plot, while I only care for those who peak over a certain threshold.
Meaning that they should have a different set of colours or be easily recognizable in some other way, and they should appear in the legend - the rest not.
Any ideas how to do that?</p>

<p>Here is the code I use now, which produces an unreadable plot:</p>

<pre><code>from pattern.db import Database
import pandas as pd
import matplotlib.pyplot as plt



def plot_trends(keywords_and_date_list):
d1 = pd.DataFrame(keywords_and_date_list)
d1.sort(inplace=True)

grouped = d1.groupby(pd.Grouper(freq='1M', key=""date"")).mean()

plt.style.use(""ggplot"")

fig = plt.figure(figsize=(25,6))

for i in d1.columns:
    if i == 'date':
        continue
    plt.plot(grouped.index, grouped[i], lw=2, label=""monthly average "" + i)        

plt.ylim(0,0.015)
plt.legend(prop={'size':7})
plt.title(""Occurence of various words in blogs"")
plt.xlabel(""Post publication date"")
plt.ylabel(""Term Frequency"")
plt.show()
</code></pre>

<p>Do any of you have any ideas of how to feasibly differentiate between the graphs that peak over, let's say 0.004, and assign them a different colour set and labels?</p>

<p>I played with a small data set in order to achieve this with panda's max function, but I don't get it to work.</p>

<pre><code>import pandas as pd
import numpy as np
from pattern.db import date
import matplotlib.pyplot as plt

l = [dict(date=date('2015-01-02'), one=0.1, two=0.2)]
l.append(dict(date=date('2014-01-01'), one=0.2, two=0.5))
l.append(dict(date=date('2014-02-01'), one=0.5, two=0.6))
l.append(dict(date=date('2014-03-01'), one=0.1, two=0.7))

d1 = pd.DataFrame(l)

d2 = d1.set_index('date')

plt.style.use(""ggplot"")

fig = plt.figure(figsize=(10,6))

for i in d1.columns:
    if d1.max() &gt;= 0.6:
        plt.plot(d1.index, d1[i], lw=2, label=""monthly average "" + i)
else:
    plt.plot(d1.index, d1[i], lw=2)

plt.ylim(0,1)
plt.legend(prop={'size':10})
plt.title(""Occurence of various words in Naoki's blog"")
plt.xlabel(""Post publication date"")
plt.ylabel(""Term Frequency"")
plt.show()
</code></pre>

<p>What I would like to see as a result is one graph with a label, and one without a label. I played with different syntaxes, but either I get two labelled graphs, or a value error, or an error that datetype and float is not comparable.</p>
",Dataset Preprocessing & Handling,term frequency time plot graph one plot python panda matplotlib conducting textual content analysis several web blog focusing finding emerging trend order one blog coded multi step process looping post finding top keywords post adding list already list calculate term frequency term list every single post create list dictionary post save date post tf every single word create data frame list dictionary plot work fine except get graph one plot care peak certain threshold meaning different set colour easily recognizable way appear legend rest idea code use produce unreadable plot idea feasibly differentiate graph peak let say assign different colour set label played small data set order achieve panda max function get work would like see result one graph label one without label played different syntax either get two labelled graph value error error datetype float comparable
R - Removing corpus wordset from larger corpus to find unique words,"<p>I have two corpuses (which I turn into DocumentTermMatrices, data frames, and then wordclouds) of which, one is a subset of another. To be exact, one is a corpus of text regarding just one university and the other is the corpus of text regarding all the universities in that conference.</p>

<p>Is there a way in R to extract just the words unique to the smaller wordset? This is kind of what I've been running so far for each corpus (this is for the 'conference' corpus)</p>

<pre><code>&gt; SECDraft = read.csv(""SECDraftScouting.csv"", stringsAsFactors=FALSE)
&gt; SECcorpus = Corpus(VectorSource(SECDraft$Report))
&gt; SECcorpus = tm_map(SECcorpus, tolower)
&gt; SECcorpus = tm_map(SECcorpus, PlainTextDocument)
&gt; SECcorpus = tm_map(SECcorpus, removePunctuation)
&gt; SECcorpus = tm_map(SECcorpus, removeWords, c(""strengths"", ""weaknesses"", ""notes"", stopwords(""english"")))
&gt; SECfrequencies = DocumentTermMatrix(SECcorpus)
&gt; SECallReports = as.data.frame(as.matrix(SECfrequencies))
&gt; wordcloud(colnames(SECallReports), colSums(SECallReports), random.order = FALSE, max.words = 200, scale=c(2, 0.25))
</code></pre>

<p>thanks guys!</p>
",Dataset Preprocessing & Handling,r removing corpus wordset larger corpus find unique word two corpus turn documenttermmatrices data frame wordclouds one subset another exact one corpus text regarding one university corpus text regarding university conference way r extract word unique smaller wordset kind running far corpus conference corpus thanks guy
Python: Creating Term Document Matrix from list,"<p>So I wanted to train a Naive Bayes Algorithm over some documents and the below code would just run fine if I had documents in the form of strings. But the issues is the strings I have goes through a series of pre-processing step which is more then stopword remove, lemmatization etc rather there are some custom conversion which returns a list of ngrams, where n can [1,2,3] depending on the context of text. 
So now since I have list of ngram instead of a string representing a document I am confused how can I represent the same as an input to CountVectorizer.
Any suggestions?</p>

<p>Code that would work fine with docs as a document array of type string.</p>

<pre><code>count_vectorizer = CountVectorizer(binary='true')
data = count_vectorizer.fit_transform(docs)

tfidf_data = TfidfTransformer(use_idf=False).fit_transform(data)
classifier = BernoulliNB().fit(tfidf_data,op)
</code></pre>
",Dataset Preprocessing & Handling,python creating term document matrix list wanted train naive bayes algorithm document code would run fine document form string issue string go series pre processing step stopword remove lemmatization etc rather custom conversion return list ngrams n depending context text since list ngram instead string representing document confused represent input countvectorizer suggestion code would work fine doc document array type string
How to tabulate term fequency data with or without using document term matrix?,"<p>I am trying to tabulated the following data:</p>

<p>Input</p>

<pre><code>Big Fat Apple          3
Small Fat Apple        2
Little Small Pear      1
</code></pre>

<p>Expected output:</p>

<pre><code>Big = 3
Fat = 3+2=5
Apple = 3+2=5
Small = 2+1=3
Little = 1
Pear = 1
</code></pre>

<p>I was trying to get document term matrix to treat this as corpus, but I am unable to find a way to do in a way that ""Big Fat Apple"" would be actually appearing in the corpus: ""Big Fat Apple Big Fat Apple Big Fat Apple"".</p>

<p>Is there any methods to do such tabulation? Ideally I would love to have it in the form of input into document term matrix so that I could use other functions.</p>
",Dataset Preprocessing & Handling,tabulate term fequency data without using document term matrix trying tabulated following data input expected output wa trying get document term matrix treat corpus unable find way way big fat apple would actually appearing corpus big fat apple big fat apple big fat apple method tabulation ideally would love form input document term matrix could use function
Do I need to standardize data when doing text classification in Scikit,"<p>I am developing a spam filter using <code>Scikit.</code> 
Here are the steps I follow:</p>

<p>Xdata = <code>[""This is spam"" , ""This is Ham"" , ""This is spam again""]</code></p>

<ol>
<li><p><code>Matrix</code> = <code>Countvectorizer (XData)</code> . Matrix will contains count of each word in all documents. So Matrix[i][j] will give me counts of word <code>j</code> in document <code>i</code></p></li>
<li><p><code>Matrix_idfX</code> = <code>TFIDFVectorizer(Matrix)</code>. It will normalize score. </p></li>
<li><p><code>Matrix_idfX_Select</code> = <code>SelectKBest( Matrix_IdfX , 500)</code> . It will reduce matrix to  500 best score columns</p></li>
<li><p><code>Multinomial.train( Matrix_Idfx_Select)</code>  </p></li>
</ol>

<p>Now my question Do I need to perform <strong>normalization or standardization</strong> in any of the above four steps ? If yes then after which step and why?</p>

<p>Thanks</p>
",Dataset Preprocessing & Handling,need standardize data text classification scikit developing spam filter using step follow xdata matrix contains count word document matrix j give count word document normalize score reduce matrix best score column question need perform normalization standardization four step yes step thanks
dynamically populate hashmap with human language dictionary for text analysis,"<p>I'm writing a software project to take as input a text in human language and determine what language it's written in. </p>

<p>My idea is that I'm going to store dictionaries in hashmaps, with the word as a key and a bool as a value. </p>

<p>If the document has that word I will flip the bool to ture.</p>

<p>Right now I'm trying to think of a good way to read in these dictionaries, put them into the hashmaps, the way I'm doing it now is very naieve and looks clunky, is there a better way to populate these hashmaps?</p>

<p>moreover, these dictionaries are huge. maybe this isn't the best way to do this, i.e. populate them all in succession like this. </p>

<p>I was thinking that it might be better to just consider one dictionary at a time, and then create a score, how many words of the input text registered with that document, save that, and then process the next dictionary. that would save on RAM, isn't it? Is that a good solution?</p>

<p>The code so far looks like this:</p>

<pre><code>static HashMap&lt;String, Boolean&gt;  de_map = new HashMap&lt;String, Boolean&gt;();
static HashMap&lt;String, Boolean&gt;  fr_map = new HashMap&lt;String, Boolean&gt;();
static HashMap&lt;String, Boolean&gt;  ru_map = new HashMap&lt;String, Boolean&gt;();
static HashMap&lt;String, Boolean&gt; eng_map = new HashMap&lt;String, Boolean&gt;();

public static void main(String[] args) throws IOException
{
    ArrayList&lt;File&gt; sub_dirs = new ArrayList&lt;File&gt;();

    final String filePath = ""/home/matthias/Desktop/language_detective/word_lists_2"";

    listf( filePath, sub_dirs );

    for(File dir : sub_dirs)
    {
        String word_holding_directory_path = dir.toString().toLowerCase();



        BufferedReader br = new BufferedReader(new FileReader( dir ));
        String line = null;
        while ((line = br.readLine()) != null)
        {
            //System.out.println(line);
            if(word_holding_directory_path.toLowerCase().contains(""/de/"") )
            {
                de_map.put(line, false);    
            }
            if(word_holding_directory_path.toLowerCase().contains(""/ru/"") )
            {
                ru_map.put(line, false);
            }
            if(word_holding_directory_path.toLowerCase().contains(""/fr/"") )
            {
                fr_map.put(line, false);
            }
            if(word_holding_directory_path.toLowerCase().contains(""/eng/"") )
            {
                eng_map.put(line, false);
            }
        }
    }
</code></pre>

<p>So I'm looking for advice on how I might populate them one at a time, and an opinion as to whether that's a good methodology, or suggestions about possibly better methodologies for acheiving this aim. </p>

<p>The full programme can be found <a href=""https://github.com/h1395010/language_detective"" rel=""nofollow"">here on my GitHub page</a>.</p>

<p>27<sup>th</sup></p>
",Dataset Preprocessing & Handling,dynamically populate hashmap human language dictionary text analysis writing software project take input text human language determine language written idea going store dictionary hashmaps word key bool value document ha word flip bool ture right trying think good way read dictionary put hashmaps way naieve look clunky better way populate hashmaps moreover dictionary huge maybe best way e populate succession like wa thinking might better consider one dictionary time create score many word input text registered document save process next dictionary would save ram good solution code far look like looking advice might populate one time opinion whether good methodology suggestion possibly better methodology acheiving aim full programme found github page th
Train data from csv python textblob,"<p>I have a CSV file I am trying to train on a classifier. I am using TextBlob. This is my code that does it.</p>

<pre><code>with open('train.csv', 'r') as fp:
    cl = NaiveBayesClassifier(fp, format='csv', feature_extractor= get_features)
</code></pre>

<p>It however does not seem to work. Is this the correct way to train a classifier using a CSV file?</p>
",Dataset Preprocessing & Handling,train data csv python textblob csv file trying train classifier using textblob code doe however doe seem work correct way train classifier using csv file
Text analysis : What after term-document matrix?,"<p>I am trying to build predictive models from text data. I built document-term matrix from the text data (unigram and bigram) and built different types of models on that (like svm, random forest, nearest neighbor etc). All the techniques gave decent results, but I want to improve the results. I tried tuning the models by changing parameters, but that doesn't seem to improve the performance much. What are the possible next steps for me?</p>
",Dataset Preprocessing & Handling,text analysis term document matrix trying build predictive model text data built document term matrix text data unigram bigram built different type model like svm random forest nearest neighbor etc technique gave decent result want improve result tried tuning model changing parameter seem improve performance much possible next step
svmperf multi-class model output,"<p>I use <a href=""http://svmlight.joachims.org/svm_multiclass.html"" rel=""nofollow"">svm_multiclass_learn</a> to learn the model of <a href=""http://en.wikipedia.org/wiki/Multiclass_classification"" rel=""nofollow"">multi-class classification</a>.</p>

<p>After learning I want to load the model file, read the weights and use them for classification, I do not want to use svm_multiclass_classify.</p>

<p>The problem is I don't understand the format of model file.</p>

<p>I am expecting it to be the matrix (number of classes on number of features), however it's just the list of features weight and I don't know how to interpret it.</p>

<p>It would be great if you could explain me the formal of model file.</p>

<p><strong>Addendum</strong></p>

<p>In addition, the documentation says nothing about bias (should be added as a feature or svm multiclass can handle it by itself)</p>
",Dataset Preprocessing & Handling,svmperf multi class model output use svm multiclass learn learn model multi class classification learning want load model file read weight use classification want use svm multiclass classify problem understand format model file expecting matrix number class number feature however list feature weight know interpret would great could explain formal model file addendum addition documentation say nothing bias added feature svm multiclass handle
"Relation-Triples extraction in R - tm, stringr, perl regex","<p>In a case of ontology learning from texts, suppose I have two concepts and I'm interested in the relation between them:</p>

<pre><code>class &lt;- c(animal.class, dog.class)
individual &lt;- ""Snoopy""

animal.class &lt;- c(""animal"", ""animals"")
dog.class &lt;- c(""dog"", ""dogs"")

sentence1 &lt;- ""Snoopy is an animal.""
sentence2 &lt;- ""Snoopy is a dog.""
</code></pre>

<p>How to extract linguistic context and semantic relations with R in such way that I can collect data frames without having the context/relation (""is a(n)"") defined before.</p>

<pre><code>data.frame(CLASS1=""animal"",CLASS2=""Snoopy"",context=""CLASS2 is an CLASS1"")
data.frame(CLASS1=""dog"",CLASS2=""Snoopy"",context=""CLASS2 is a CLASS1"")
</code></pre>

<p>It's easy to extract this kind of thing with other tools like finite state transducer, but I would like to stay in R and I didn't find anything like that for the moment in R.</p>

<p>I imagine some solutions with perl regex and the packages <code>tm</code> and <code>stringr</code>... are they sufficient?</p>
",Dataset Preprocessing & Handling,relation triple extraction r tm stringr perl regex case ontology learning text suppose two concept interested relation extract linguistic context semantic relation r way collect data frame without context relation n defined easy extract kind thing tool like finite state transducer would like stay r find anything like moment r imagine solution perl regex package sufficient
Giving Single tag to whole document using SVM,"<p>I would like to know, how to train SVM giving a whole document as input and  a single label for that input document. 
I have tagged only word by word  till now.
for example, input document may contain 6 to 10 sentences, and the whole document is to be labelled with a single class for training.</p>
",Dataset Preprocessing & Handling,giving single tag whole document using svm would like know train svm giving whole document input single label input document tagged word word till example input document may contain sentence whole document labelled single class training
How to read a paragraph in natural language processing GATE,"<p>I am using GATE tool for natural language processing.. i am using java code to read lines from the sentence and get the keywords.. what modification has to be done in creole xml to read complete paragraph..</p>
",Dataset Preprocessing & Handling,read paragraph natural language processing gate using gate tool natural language processing using java code read line sentence get keywords modification ha done creole xml read complete paragraph
How to quickly get the collection of words in a corpus (with nltk)?,"<p>I would like to quickly build a word look-up table for a corpus with nltk. Below is what I am doing:</p>

<ol>
<li>Read raw text: file=open(""corpus"",""r"").read().decode('utf-8')</li>
<li>Use a=nltk.word_tokenize(file) to get all tokens;</li>
<li>Use set(a) to get unique tokens, and covert it back to a list.</li>
</ol>

<p>Is this the right way of doing this task? </p>
",Dataset Preprocessing & Handling,quickly get collection word corpus nltk would like quickly build word look table corpus nltk read raw text file open corpus r read decode utf use nltk word tokenize file get token use set get unique token covert back list right way task
Error generating a model reading corpus from a big .txt file,"<p>i'm trying to read the file corpus.txt (training set) and generate a model, the output must be called lexic.txt and contain the word, the tag and the number of ocurrences...for small training sets it works, but for the university given training set (30mb .txt file, millions of lines) the code does not work,I imagine it will be a problem with the efficiency and therefore the system runs out of memory...can anybody help me with the code please?</p>

<p>Here I attach my code:</p>

<pre><code>from collections import Counter

file=open('corpus.txt','r')
data=file.readlines()
file.close()

palabras = []
count_list = []

for linea in data:
   linea.decode('latin_1').encode('UTF-8') # para los acentos
   palabra_tag = linea.split('\n')
   palabras.append(palabra_tag[0])

cuenta = Counter(palabras) # dictionary for count ocurrences for a word + tag 

#Assign for every word + tag the number of times appears
for palabraTag in palabras:
    for i in range(len(palabras)):
        if palabras[i] == palabraTag:       
            count_list.append([palabras[i], str(cuenta[palabraTag])])


#We delete repeated ones
finalList = []
for i in count_list:
    if i not in finalList:
        finalList.append(i)


outfile = open('lexic.txt', 'w') 
outfile.write('Palabra\tTag\tApariciones\n')

for i in range(len(finalList)):
    outfile.write(finalList[i][0]+'\t'+finalList[i][1]+'\n') # finalList[i][0] is the word + tag and finalList[i][1] is the numbr of ocurrences

outfile.close()
</code></pre>

<p>And here you can see a sample of the corpus.txt:</p>

<pre><code>Al  Prep
menos   Adv
cinco   Det
reclusos    Adj
murieron    V
en  Prep
las Det
últimas Adj
24  Num
horas   NC
en  Prep
las Det
cárceles    NC
de  Prep
Valencia    NP
y   Conj
Barcelona   NP
en  Prep
incidentes  NC
en  Prep
los Det
que Pron
su  Det
</code></pre>

<p>Thanks in advance!</p>
",Dataset Preprocessing & Handling,error generating model reading corpus big txt file trying read file corpus txt training set generate model output must called lexic txt contain word tag number ocurrences small training set work university given training set mb txt file million line code doe work imagine problem efficiency therefore system run memory anybody help code please attach code see sample corpus txt thanks advance
OpenNLP - Tokenize an Array of Strings,"<p>I am trying to tokenize a text file using the OpenNLP tokenizer.
What I do, I read in a .txt file and store it in a list, want to iterate over every line, tokenize the line and write the tokenized line to a new file.</p>

<p>In the line:</p>

<pre><code>tokens[i] = tokenizer.tokenize(output[i]);
</code></pre>

<p>I get:</p>

<pre><code>Type mismatch: cannot convert from String[] to String
</code></pre>

<p>This is my code:</p>

<pre><code>public class Tokenizer {

    public static void main(String[] args) throws Exception {

    InputStream modelIn = new FileInputStream(""en-token-max.bin"");

    try {


      TokenizerModel model = new TokenizerModel(modelIn);
      Tokenizer tokenizer = new TokenizerME(model);

      CSVReader reader = new CSVReader(new FileReader(""ParsedRawText1.txt""),',', '""', 1);
      String csv = ""ParsedRawText2.txt"";
      CSVWriter writer = new CSVWriter(new FileWriter(csv),CSVWriter.NO_ESCAPE_CHARACTER,CSVWriter.NO_QUOTE_CHARACTER);

      //Read all rows at once
      List&lt;String[]&gt; allRows = reader.readAll();

      for(String[] output : allRows) {
             //get current row
             String[] tokens=new String[output.length];
             for(int i=0;i&lt;output.length;i++){

                 tokens[i] = tokenizer.tokenize(output[i]);
                 System.out.println(tokens[i]);
             }

             //write line
             writer.writeNext(tokens);
         }
         writer.close();

    }
    catch (IOException e) {
      e.printStackTrace();
    }
    finally {
      if (modelIn != null) {
        try {
          modelIn.close();
        }
        catch (IOException e) {
        }
      }
    }
   }
  }
</code></pre>

<p>Does anyone has any idea how to complete this task?</p>
",Dataset Preprocessing & Handling,opennlp tokenize array string trying tokenize text file using opennlp tokenizer read txt file store list want iterate every line tokenize line write tokenized line new file line get code doe anyone ha idea complete task
Why does the Synopse hyphenation code give different results from TeX&#39;s?,"<p>This question follows <a href=""https://stackoverflow.com/questions/10122297/how-to-use-this-hyphenation-library-in-delphi"">previous question</a> but different. <a href=""http://synopse.info/forum/viewtopic.php?id=74"" rel=""nofollow noreferrer"">Synopse's delphi hyphenation</a> is very fast and builts on OpenOffice <a href=""http://www.openoffice.org/lingucomponent/hyphenator.html"" rel=""nofollow noreferrer"">libhnj library that uses TeX hyphenation</a>.</p>

<p>A simple test is :</p>

<p>If I input 'pronunciation', the Synopse hyphenation outputs 'pro=nun=ci=ation' (4 possible hyphens or syllables). //(not 'pro=nun=ci=a=tion', 5 hyphens or syllables).</p>

<p>I read 2 papers (<a href=""http://eprints.soton.ac.uk/264285/1/MarchandAdsettDamper_ISCA07.pdf"" rel=""nofollow noreferrer"">here</a> and <a href=""http://web.cs.dal.ca/~adsett/publications/AdsMar_CompSyllMeth_2009.pdf"" rel=""nofollow noreferrer"">here</a>) about Tex hyphenation algorithm uses in syllabification. Authors stated about 95% accuracy in syllabification. I tested Synopse hyphenation only for counting syllables on <a href=""http://www.speech.cs.cmu.edu/cgi-bin/cmudict"" rel=""nofollow noreferrer"">CMU Pronouncing Dictionary</a>, but only about 53% accuracy. </p>

<p>Why is the result significantly different? </p>

<p>I reproduce my method in a little detailed way.</p>

<p>I parse the CMU pronuncing dictionary to compute all number of words.
The CMU dic is like: </p>

<pre><code>PRONOUNS  P R OW1 N AW0 N Z
PRONOVOST  P R OW0 N OW1 V OW0 S T
PRONTO  P R AA1 N T OW0
PRONUNCIATION  P R OW0 N AH2 N S IY0 EY1 SH AH0 N
PRONUNCIATION(1)  P R AH0 N AH2 N S IY0 EY1 SH AH0 N
</code></pre>

<p>I will have this result:</p>

<pre><code>PRONOUNS=2
PRONOVOST=3
PRONTO=2
PRONUNCIATION(1)=5 // will be ignored
PRONUNCIATION=5   // use this one
</code></pre>

<p>Words with parentheses will be ignored when compared with the Synopse hyphenation lib. They are alternative or secondary pronunciations (variants).</p>

<p>Similarly, I will use the hyphenation lib to compute the number of syllables of each word in the CMU dictionary. Then I compare the two to see how many match. The words with different numbers of syllables are recorded like below:</p>

<pre><code>...

94814 cmu PROMULGATED=4 | PROMULGATED=3 Synopse Hyphenation
94821 cmu PRONGER=2 | PRONGER=1 Synopse Hyphenation
94829 cmu PRONOUNCES=3 | PRONOUNCES=2 Synopse Hyphenation
94833 cmu PRONTO=2 | PRONTO=1 Synopse Hyphenation
94835 cmu PRONUNCIATION=5 | PRONUNCIATION=4 Synopse Hyphenation

...
</code></pre>

<p>The total line number of CMU is 123611 (excluding lines with parentheses and lines without meaningful words, like quotation mark lines '(').
The total different number of syllables of the <em>same</em> words for the two:  57870.</p>

<p>CMU may not be the standard of syllable numbers. In this test, (123611-57870)/123611=53.183%. This is significantly different from the accuracy rate stated by the author in their paper above. Of course, they used a another database (CELEX) for their tests. Why is the result so different?</p>

<p>The Synopse hyphenation library is very fast. I want to know further if this is due to the pattern file (dic file used for hyphenation originally from libhnj used in OpenOffice). Or did the author of the paper use a different dictionary file?</p>
",Dataset Preprocessing & Handling,doe synopse hyphenation code give different result tex question follows synopse delphi hyphenation fast builts openoffice libhnj library us tex hyphenation simple test input pronunciation synopse hyphenation output pro nun ci ation possible hyphen syllable pro nun ci tion hyphen syllable read paper tex hyphenation algorithm us syllabification author stated accuracy syllabification tested synopse hyphenation counting syllable cmu pronouncing dictionary accuracy result significantly different reproduce method little detailed way parse cmu pronuncing dictionary compute number word cmu dic like result word parenthesis ignored compared synopse hyphenation lib alternative secondary pronunciation variant similarly use hyphenation lib compute number syllable word cmu dictionary compare two see many match word different number syllable recorded like total line number cmu excluding line parenthesis line without meaningful word like quotation mark line total different number syllable word two cmu may standard syllable number test significantly different accuracy rate stated author paper course used another database celex test result different synopse hyphenation library fast want know due pattern file dic file used hyphenation originally libhnj used openoffice author paper use different dictionary file
CSV input file format for a (Sparse) ARFF which has a relational attribute,"<p>I want to convert the CSV file to ARFF format (using CSVToARFFConverter) before other processing in WEKA. 
My ARFF file is in the below format:</p>

<pre><code>@relation Sample

 @attribute CLS string
 @attribute SCLS string
 @attribute key relational
   @attribute key1 string
   @attribute key2 string
   @attribute key3 string
 @end key
 @attribute class {-5,-4,-3,-2,-1,0,1,2,3,4,5}


@data
{0 type, 1 beta, 2 ""3 keyword1\nkeyword2\nkeyword3"", -5}
{0 typeA, 1 gamma, 2 ""3 keyword11\nkeyword21\nkeyword31"", 0}
{0 typeB, 1 alpha, 2 ""3 keyword21\nkeyword22\nkeyword23"", 3}
</code></pre>

<p>What is the equivalent CSV representation of the above Sparse ARFF file? Please assist. Thanks.</p>
",Dataset Preprocessing & Handling,csv input file format sparse arff ha relational attribute want convert csv file arff format using csvtoarffconverter processing weka arff file format equivalent csv representation sparse arff file please assist thanks
use scikit learn tfidf vectorizer starting from counts data frame,"<p>I have a pandas data frame with counts of words for a series of documents. Can I apply <code>sklearn.feature_extraction.text.TfidfVectorizer</code> to it to return a term-document matrix? </p>

<pre><code>import pandas as pd

a = [1,2,3,4]
b = [1,3,4,6]
c = [3,4,6,1]

df = pd.DataFrame([a,b,c])
</code></pre>

<p>How can I get tfidf version of counts in df?</p>
",Dataset Preprocessing & Handling,use scikit learn tfidf vectorizer starting count data frame panda data frame count word series document apply return term document matrix get tfidf version count df
Clustering Using Latent Symantic Analysis,"<p>Suppose I have a corpus of documents and I run LSA algorithm on it. How can I use the final matrix obtained after applying SVD to semantically cluster all the words appearing in my corpus of documents? Wikipedia says LSA can be used to find relation between terms. Is there any library available in Python which can help me accomplish my task of semantically clustering words based on LSA?</p>
",Dataset Preprocessing & Handling,clustering using latent symantic analysis suppose corpus document run lsa algorithm use final matrix obtained applying svd semantically cluster word appearing corpus document wikipedia say lsa used find relation term library available python help accomplish task semantically clustering word based lsa
R - How to have comparative and superlative suffixes count as the same stem word,"<p>When I generate DocumentTermMatrix or TermDocumentMatrix objects and enable their Stemwords feature, it doesn't count comparative nor superlative suffixed words as the same word. Heavier, heaviest are not heavy.</p>

<p>Here's my example:</p>

<pre><code>library(RTextTools)
library(topicmodels)
library(tm)

data &lt;- c(""I'm having trouble with superlative and comparative suffixes"", 
             ""Heaviest."",
            ""this is heavy heavier."")

matrix &lt;- create_matrix(data, language=""english"", toLower = TRUE, removeNumbers=TRUE, stemWords=TRUE, removePunctuation = TRUE, removeStopwords = TRUE, weighting=weightTf)
inspect(matrix)
colnames(matrix) 
</code></pre>

<p>Here's the output:</p>

<pre><code>&gt;.....
Terms
Docs compar heavi heavier heaviest suffix superl troubl
   1      1     0       0        0      1      1      1
   2      0     0       0        1      0      0      0
   3      0     1       1        0      0      0      0

&gt; colnames(matrix) 
[1] ""compar""   ""heavi""    ""heavier""  ""heaviest"" ""suffix""   ""superl""   ""troubl""  
</code></pre>

<p>The ideal output I am looking for is not only that ""heavier"" and ""heaviest"" are the same as ""heavi"", but that ""Heavi"" shows up once in the 2nd document, and twice in the last document. </p>

<p>I have tried enabling and disabling the stemwords, removeStopwords, min/maxwordlength flags. The Min/Maxwordlengths worked for bigger words, but it still did not count ""heaviest"" as ""heavi"" in the term matrix. The stemming that rTextTools seems to use Porter's algorithm. (Reference: <a href=""http://www.inside-r.org/packages/cran/RTextTools/docs/create_matrix"" rel=""nofollow"">http://www.inside-r.org/packages/cran/RTextTools/docs/create_matrix</a>)</p>

<p>I would like to ideally generate a DocumentTermMatrix in the end, so I have been working on manipulating the matrix and trying what kind of pre-processing I could do on the data itself. But no luck thus far.</p>

<p>Thanks in advance!</p>
",Dataset Preprocessing & Handling,r comparative superlative suffix count stem word generate documenttermmatrix termdocumentmatrix object enable stemwords feature count comparative superlative suffixed word word heavier heaviest heavy example output ideal output looking heavier heaviest heavi heavi show nd document twice last document tried enabling disabling stemwords removestopwords min maxwordlength flag min maxwordlengths worked bigger word still count heaviest heavi term matrix stemming rtexttools seems use porter algorithm reference would like ideally generate documenttermmatrix end working manipulating matrix trying kind pre processing could data luck thus far thanks advance
Regex / &quot;token_pattern&quot; for scikit-learn text Vectorizer,"<p>I'm using sklearn to do some NLP vectorizing with a tf-idf Vectorizer object. This object can be constructed with a keyword, ""token_pattern"".</p>

<p>I want to avoid hashtags (#foobar), numerics (and strings that begin with a number(s), i.e. 10mg), any line that begin with 'RT' (retweet), or the line ""Deleted tweet"".</p>

<p>In addition, I want to ignore unicode.</p>

<p>I want to keep, the URL's (not the 'http://') and have them tokenized into any words ( [A-Za-z]+ only ) that may exist in them.</p>

<p>I have some experience with Regex, but have not needed more complex patterns until now.</p>

<p>Below is my stab for everything...it's obviously not the best way to investigate, but it does sum up how I currently think about the Regex rules.</p>

<p>NOTE: the skearn doc <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow"">here</a> shows the default ""token_pattern"" using the unicode flag on the string and I don't understand why...separate question perhaps. </p>

<pre><code>pat2 = r""(?im)([A-Z]+)(?&lt;!^@)([A-Z]+)(?&lt;!^#)([A-Z]+)(?&lt;!^(RT))([A-Z]+)(?&lt;!^Deleted)(?&lt;=^(http://))([A-Z]+)""
</code></pre>

<p>My break down:</p>

<pre><code>(?im)  #Are flags for 'multi-line' and 'case insensitive'

([A-Z]+)(?&lt;!^@) #A negative look back, match [A-Z]+ only if not preceded by 'starts with @'.

(?&lt;=^(http://))([A-Z]+) #A positive look forward, match [A-Z]+ only if 'starts with ""http://""' is present.
</code></pre>

<p>I get the feeling this is not an elegant solution even if it is tweaked into working...</p>

<p>TIA </p>

<p>UPDATE:
RAW DATA EXAMPLE:</p>

<p>If it helps to know, I'm using a pandas data frame to load the data. I'm new to pandas and maybe missing some pandas based solution.</p>

<p>From this raw data, I'd want only words taken from the text and the URL's.
This example sucks...please comment further to help me get it better defined... thx!</p>

<p>raw:</p>

<pre><code>http://foxsportswisconsin.ning.com/profiles/blogs/simvastatin-20-mg-pas-cher-sur-internet-acheter-du-simvastatin-20
</code></pre>

<p>tokenized: </p>

<pre><code>[foxsportswisconsin, ning, com, profiles, blogs, simvastatin, mg, pas, cher, sur, internet, acheter, du, simvastatin]
</code></pre>
",Dataset Preprocessing & Handling,regex token pattern scikit learn text vectorizer using sklearn nlp vectorizing tf idf vectorizer object object constructed keyword token pattern want avoid hashtags foobar numerics string begin number e mg line begin rt retweet line deleted tweet addition want ignore unicode want keep url tokenized word za z may exist experience regex needed complex pattern stab everything obviously best way investigate doe sum currently think regex rule note skearn doc show default token pattern using unicode flag string understand separate question perhaps break get feeling elegant solution even working tia update raw data example help know using panda data frame load data new panda maybe missing panda based solution raw data want word taken text url example suck please comment help get better defined thx raw tokenized
Read google ngrams using google_ngram_downloader,"<p>I am using <strong>google_ngram_downloader</strong> to read the google datasets. </p>

<p><strong>Code :</strong> </p>

<pre><code>from google_ngram_downloader import readline_google_store
fname, url, records = next(readline_google_store(ngram_len=1))
for x in range(0,5):
    print next(records)
</code></pre>

<p>Here I am reading the datasets one by one <strong>starting from 0,1,... a,b,c.. z</strong>. <strong>next(readline_google_store(ngram_len=1))</strong> gives the ngrams one by one. I want to read directly the datasets which will <strong>'a','b'</strong> anything not one by one.</p>

<p><strong>Required</strong>: Read only dataset which starts from letter <strong>'a'</strong> having <strong>1-gram</strong> dataset.</p>
",Dataset Preprocessing & Handling,read google ngrams using google ngram downloader using google ngram downloader read google datasets code reading datasets one one starting b c z next readline google store ngram len give ngrams one one want read directly datasets b anything one one required read dataset start letter gram dataset
R text mining - intersection between text fields,"<p>I was wondering if there's a quick way to find directed intersection between 2 text strings e.g. </p>

<pre><code> t1 &lt;- ""I have achieved my goals over the past 20 years and look forward for my next chalanges""
 t2 &lt;- "" have achieved goals and look my chalanges some other words bla bla""
</code></pre>

<p>t1 isContainedIn t2 would return 7 because 7 words that apeared in t1 also apeared in t2 .
Also, t1 and t2 are 2 columns in a data frame, so I would need to apply that function on the entire data frame and attached the result column to my original data frame.
This is how my data frame 'data.selected' looks like:</p>

<pre><code>        keywords                                         title
1  Samsung UN48H6350 48"" Samsung UN48H6350 48"" Full 1080p Smart HDTV 120Hz with Wi-Fi +$50 Visa Gift Card
2  Samsung UN48H6350 48""     Samsung UN48H6350 48"" Full HD Smart LED TV -Bundle- (See Below for Contents)
3  Samsung UN48H6350 48""      Samsung UN48H6350 48"" Class Full HD Smart LED TV -BUNDLE- See below Details
4  Samsung UN48H6350 48""     Samsung UN48H6350 48"" Full HD Smart LED TV With BD-H5100 Blu-ray Disc Player
5  Samsung UN48H6350 48""                 Samsung UN48H6350 48"" Smart 1080p Clear Motion Rate 240 LED HDTV
6  Samsung UN48H6350 48""            Samsung UN48H6350 - 48-Inch Full HD 1080p Smart HDTV 120Hz with Wi-Fi
7  Samsung UN48H6350 48""               Samsung 6350 Series UN48H6350 48"" 1080p HD LED LCD Internet TV NEW
8  Samsung UN48H6350 48""  Samsung Un48h6350af 75"" 1080p Led-lcd Tv - 16:9 - Hdtv 1080p - (un75h6350afxza)
9  Samsung UN48H6350 48""                         Samsung UN48H6350 - 48"" HD 1080p Smart HDTV 120Hz Bundle
10 Samsung UN48H6350 48""   Samsung UN48H6350 - 48-Inch Full HD 1080p Smart HDTV 120Hz with Wi-Fi, (R#416)
</code></pre>
",Dataset Preprocessing & Handling,r text mining intersection text field wa wondering quick way find directed intersection text string e g iscontainedin would return word apeared also apeared also column data frame would need apply function entire data frame attached result column original data frame data frame data selected look like
Problems fitting vocabulary in scikit-learn?,"<p>I have a directory full of <code>.txt</code> files (documents). First I <code>load</code> the documents and strip some parenthesis and remove some quotes, so the documents looks as follows, for example:</p>

<pre><code>document1:
is a scientific discipline that explores the construction and study of algorithms that can learn from data Such algorithms operate by building a model

document2:
Machine learning can be considered a subfield of computer science and statistics It has strong ties to artificial intelligence and optimization which deliver methods
</code></pre>

<p>So I'm loading the files from the directory like this:</p>

<pre><code>preprocessDocuments =[[' '.join(x) for x in sample[:-1]] for sample in load(directory)]


documents = ''.join( i for i in ''.join(str(v) for v
                                              in preprocessDocuments) if i not in ""',()"")
</code></pre>

<p>Then I'm triying to vectorize <code>document1</code> and <code>document2</code> in order to create a training matrix as follows:</p>

<pre><code>from sklearn.feature_extraction.text import HashingVectorizer
vectorizer = HashingVectorizer(analyzer='word')
X = HashingVectorizer.fit_transform(documents)
X.toarray()
</code></pre>

<p>Then this is the output:</p>

<pre><code>    raise ValueError(""empty vocabulary; perhaps the documents only""
ValueError: empty vocabulary; perhaps the documents only contain stop words
</code></pre>

<p>How can I create a vector representation given this?. I thought that i was carrying the loaded files in <code>documents</code> but it seems that the documents can not be fitted.</p>
",Dataset Preprocessing & Handling,problem fitting vocabulary scikit learn directory full file document first document strip parenthesis remove quote document look follows example loading file directory like triying vectorize order create training matrix follows output create vector representation given thought wa carrying loaded file seems document fitted
load parallel corpora with NLTK and lemmatize english sentences,"<p>I have a corpora which is formatted like this:</p>

<pre><code>sentence in english \t sentence in french \t score
sentence in english \t sentence in french \t score
</code></pre>

<p>Each sentence is tokenized (separated by a whitespac).</p>

<p>Now I need to load this sentences using NLTK. How can I do that ? What method in the CorpusReader may I use ? </p>

<p>In this example, I can load the comtrans corpus provided by NLTK:</p>

<pre><code>from nltk.corpus.util import LazyCorpusLoader
from nltk.corpus.reader import AlignedCorpusReader

comtrans = LazyCorpusLoader(
    'comtrans', AlignedCorpusReader, r'(?!\.).*\.txt',
     encoding='iso-8859-1')

fe=comtrans.aligned_sents('alignment-en-fr.txt')[0]
print fe
</code></pre>

<p>In fact, i need to do the same thing but with a file create by myself.</p>

<p>In the last step, I need to lemmatize each word of english sentences.</p>
",Dataset Preprocessing & Handling,load parallel corpus nltk lemmatize english sentence corpus formatted like sentence tokenized separated whitespac need load sentence using nltk method corpusreader may use example load comtrans corpus provided nltk fact need thing file create last step need lemmatize word english sentence
Big Text Corpus breaks tm_map,"<p>I have been breaking my head over this one over the last few days. I searched all the SO archives and tried the suggested solutions but just can't seem to get this to work. I have sets of txt documents in folders such as 2000 06, 1995 -99 etc, and want to run some basic text mining operations such as creating document term matrix and term document matrix and doing some operations based co-locations of words. My script works on a smaller corpus, however, when I try it with the bigger corpus, it fails me. I have pasted in the code for one such folder operation.</p>

<pre><code>library(tm) # Framework for text mining.
library(SnowballC) # Provides wordStem() for stemming.
library(RColorBrewer) # Generate palette of colours for plots.
library(ggplot2) # Plot word frequencies.
library(magrittr)
library(Rgraphviz)
library(directlabels)

setwd(""/ConvertedText"")
txt &lt;- file.path(""2000 -06"")

docs&lt;-VCorpus(DirSource(txt, encoding = ""UTF-8""),readerControl = list(language = ""UTF-8""))
docs &lt;- tm_map(docs, content_transformer(tolower), mc.cores=1)
docs &lt;- tm_map(docs, removeNumbers, mc.cores=1)
docs &lt;- tm_map(docs, removePunctuation, mc.cores=1)
docs &lt;- tm_map(docs, stripWhitespace, mc.cores=1)
docs &lt;- tm_map(docs, removeWords, stopwords(""SMART""), mc.cores=1)
docs &lt;- tm_map(docs, removeWords, stopwords(""en""), mc.cores=1)
#corpus creation complete

setwd(""/ConvertedText/output"")
dtm&lt;-DocumentTermMatrix(docs)
tdm&lt;-TermDocumentMatrix(docs)
m&lt;-as.matrix(dtm)
write.csv(m, file=""dtm.csv"")
dtms&lt;-removeSparseTerms(dtm, 0.2)
m1&lt;-as.matrix(dtms)
write.csv(m1, file=""dtms.csv"")
# matrix creation/storage complete

freq &lt;- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
wf &lt;- data.frame(word=names(freq), freq=freq)
freq[1:50]
#adjust freq score in next line
p &lt;- ggplot(subset(wf, freq&gt;100), aes(word, freq))+ geom_bar(stat=""identity"")+ theme(axis.text.x=element_text(angle=45, hjust=1))
ggsave(""frequency2000-06.png"", height=12,width=17, dpi=72)
# frequency graph generated


x&lt;-as.matrix(findFreqTerms(dtm, lowfreq=1000))
write.csv(x, file=""freqterms00-06.csv"")
png(""correlation2000-06.png"", width=12, height=12, units=""in"", res=900)
graph.par(list(edges=list(col=""lightblue"", lty=""solid"", lwd=0.3)))
graph.par(list(nodes=list(col=""darkgreen"", lty=""dotted"", lwd=2, fontsize=50)))
plot(dtm, terms=findFreqTerms(dtm, lowfreq=1000)[1:50],corThreshold=0.7)
dev.off()
</code></pre>

<p>When I use the mc.cores=1 argument in tm_map, the operation continues indefinitely. However, if I use the lazy=TRUE argument in tm_map, it seemingly goes well, but subsequent operations give this error. </p>

<pre><code>Error in UseMethod(""meta"", x) : 
  no applicable method for 'meta' applied to an object of class ""try-error""
In addition: Warning messages:
1: In mclapply(x$content[i], function(d) tm_reduce(d, x$lazy$maps)) :
  all scheduled cores encountered errors in user code
2: In mclapply(unname(content(x)), termFreq, control) :
  all scheduled cores encountered errors in user code
</code></pre>

<p>I have been looking all over for a solution but have failed consistently. Any help would be greatly appreciated!</p>

<p>Best!
k</p>
",Dataset Preprocessing & Handling,big text corpus break tm map breaking head one last day searched archive tried suggested solution seem get work set txt document folder etc want run basic text mining operation creating document term matrix term document matrix operation based co location word script work smaller corpus however try bigger corpus fails pasted code one folder operation use mc core argument tm map operation continues indefinitely however use lazy true argument tm map seemingly go well subsequent operation give error looking solution failed consistently help would greatly appreciated best k
Document Processing Library,"<p>I am doing a project which includes document classification component. I need a library which can be used to pre-process the documents and transform it to a feature matrix. Are there any library available for that?</p>
",Dataset Preprocessing & Handling,document processing library project includes document classification component need library used pre process document transform feature matrix library available
how to divide a series of words into &quot;N&quot; chunks?,"<p>first of all forgive me for any ambiguity . i find my problem a bit hard to explain in English . 
basically what i want to do is , to divide a huge set of words to ""N"" parts . </p>

<p>for example read all the words in a file , then divide them between lets say N=10 parts .
to be more precise , i'm working on a data mining project . there are thousands of documents which i need to sort the words of . </p>

<p>say n = 2 . i know i can put a-m and n-z in a file . i need an algorithm which can do this for n > 100 .</p>

<p>PS : my program FIRST has to create the N files ( or chunks ) then read all the words and depending on how they begin , assign them to one of the chunks .</p>

<p>EXAMPLE :
input : 
N = 2
words = [....]</p>

<p>output :
[words starting with a-m] , [words starting with n-z] </p>

<p>in other words i want to divide my words Lexicographically</p>
",Dataset Preprocessing & Handling,divide series word n chunk first forgive ambiguity find problem bit hard explain english basically want divide huge set word n part example read word file divide let say n part precise working data mining project thousand document need sort word say n know put n z file need algorithm n p program first ha create n file chunk read word depending begin assign one chunk example input n word output word starting word starting n z word want divide word lexicographically
how to divide a series of words into &quot;N&quot; chunks?,"<p>first of all forgive me for any ambiguity . i find my problem a bit hard to explain in English . 
basically what i want to do is , to divide a huge set of words to ""N"" parts . </p>

<p>for example read all the words in a file , then divide them between lets say N=10 parts .
to be more precise , i'm working on a data mining project . there are thousands of documents which i need to sort the words of . </p>

<p>say n = 2 . i know i can put a-m and n-z in a file . i need an algorithm which can do this for n > 100 .</p>

<p>PS : my program FIRST has to create the N files ( or chunks ) then read all the words and depending on how they begin , assign them to one of the chunks .</p>

<p>EXAMPLE :
input : 
N = 2
words = [....]</p>

<p>output :
[words starting with a-m] , [words starting with n-z] </p>

<p>in other words i want to divide my words Lexicographically</p>
",Dataset Preprocessing & Handling,divide series word n chunk first forgive ambiguity find problem bit hard explain english basically want divide huge set word n part example read word file divide let say n part precise working data mining project thousand document need sort word say n know put n z file need algorithm n p program first ha create n file chunk read word depending begin assign one chunk example input n word output word starting word starting n z word want divide word lexicographically
How to dynamically map data from multiple source formats?,"<p>I have a problem where I receive multiple CSV files with the same data columns but each file has a unique format / column names.  There are several hundred files and this will continue to grow, so mapping each one manually isn't an option.  Is there a good way to search each file to dynamically map its data to the master DB? The data consists of several fields of text and numeric types.</p>
",Dataset Preprocessing & Handling,dynamically map data multiple source format problem receive multiple csv file data column file ha unique format column name several hundred file continue grow mapping one manually option good way search file dynamically map data master db data consists several field text numeric type
"in R, converting for loop to apply for efficiency","<p>I am doing some NLP and trying to find common 2-grams out of a specific (limited) corpus.  I have written a for loop that does what I want, but it takes a long time to run on any real amount of data.  I feel like I should be able to do this with apply but I cannot for the life of me figure out how.  Any help is much appreciated.</p>

<p>I've tokenized and ngram'ed the corpus into the following data frames (this is obviously just a small subset for example's sake).</p>

<pre><code>tk
     word Freq
5477 with  186
1998  for  182
2644   it  179
3482   on  174
5354  was  168

ng
        ngrams Freq   w1   w2 rate
2434    at the   30   at  the    0
16027 with the   29 with  the    0
140     &lt;&gt; But   28   &lt;&gt;  But    0
223      &lt;&gt; He   28   &lt;&gt;   He    0
6885    I have   28    I have    0
</code></pre>

<p>I have the following for loop that works on these two data frames:</p>

<pre><code>for(i in 1:dim(ng)[1]) {
    tkw1 &lt;- ifelse(length(tk$Freq[tk$word==ng$w1[i]]) &gt; 0, 
                   tk$Freq[tk$word==ng$w1[i]], 0)
    tkw2 &lt;- ifelse(length(tk$Freq[tk$word==ng$w2[i]]) &gt; 0,
                   tk$Freq[tk$word==ng$w2[i]], 0)
    dnm &lt;- tkw1 + tkw2
    dnm &lt;- ifelse(dnm &gt;= 1, dnm, ng$Freq[i])
    ng$rate[i] &lt;- ng$Freq[i] / dnm
}
</code></pre>

<p>The idea is to compute a ""rate"" for each row which is essentially the number of times the 2-gram appears, divided by (the sum of) the number of times each word appears individually.  The for loop does this, but it's very slow when used on a large scale.</p>

<p>Sidenote: there are some ifelse statements which were necessary to debug the fact that sometimes (because of imperfect preprocessing) one of the words in the 2-gram doesn't match a word in the tk data frame.</p>

<p>Sooo, is there a way to do this with apply (or maybe sapply or tapply)?  I've been working on it for hours and hours and I can't figure it out.
Thanks!</p>

<p>In case this helps, my most recent attempt was:</p>

<pre><code>TGrate &lt;- function(ng, w1, w2, Freq){
    tkw1 &lt;- ifelse(length(tk$Freq[tk$word==w1]) &gt; 0, 
           tk$Freq[tk$word==w1], 0)
    tkw2 &lt;- ifelse(length(tk$Freq[tk$word==w2]) &gt; 0,
           tk$Freq[tk$word==w2], 0)
    dnm &lt;- tkw1 + tkw2
    dnm &lt;- ifelse(dnm &gt;= 1, dnm, Freq)
    rate &lt;- as.numeric(Freq) / as.numeric(dnm)
    rate
}
ng$rate &lt;- apply(ng, 1, TGrate, w1=""w1"", w2=""w2"", Freq=""Freq"")
</code></pre>

<p>but this just produces a bunch of NAs.</p>
",Dataset Preprocessing & Handling,r converting loop apply efficiency nlp trying find common gram specific limited corpus written loop doe want take long time run real amount data feel like able apply life figure help much appreciated tokenized ngram ed corpus following data frame obviously small subset example sake following loop work two data frame idea compute rate row essentially number time gram appears divided sum number time word appears individually loop doe slow used large scale sidenote ifelse statement necessary debug fact sometimes imperfect preprocessing one word gram match word tk data frame sooo way apply maybe sapply tapply working hour hour figure thanks case help recent attempt wa produce bunch na
How to load and apply same algorithm on multiple text files using python,"<p>I am new to python programming.
Right now i am doing natural language processing on text files. The problem is that i have around 200 text files so its very difficult to load each file individually and apply the same method.</p>

<p>here's my program:</p>

<pre><code>import nltk
from nltk.collocations import *
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk import FreqDist
with open(""c:/users/user/desktop/datascience/sotu/stopwords.txt"", 'r') as sww:
    sw = sww.read()
**with open(""c:/users/user/desktop/datascience/sotu/a41.txt"", 'r') as a411:
    a41 = a411.read()
    a41c=word_tokenize(str(a41))
    a41c = [w for w in a41c if not w in sw]**
</code></pre>

<p>so i want to apply this method on multiple files. Is there a way i can load all the files in one step and apply the same method.
I tried this but it did not work:</p>

<pre><code>import os
import glob
import nltk
from nltk.collocations import *
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk import FreqDist
with open(""c:/users/user/desktop/datascience/sotu/stopwords.txt"", 'r') as sww:
    sw = sww.read()
for filename in glob.glob(os.path.join(""c:/users/user/desktop/DataScience/sotu/"",'*.txt')):
    filename=word_tokenize(str(filename))
    filename = [w for w in filename if not w in sw]
xqc=FreqDist(filename)
</code></pre>

<p>please help.</p>
",Dataset Preprocessing & Handling,load apply algorithm multiple text file using python new python programming right natural language processing text file problem around text file difficult load file individually apply method program want apply method multiple file way load file one step apply method tried work please help
knowledge based Q-A system not giving most appropriate answer,"<p>I am working on a project which is basically a knowledge based question answering system. My system takes query from the user, download the relevant documents from Wikipedia, strips all the html tags and extracts the plain text. After this, it tokenizes the document into sentences, then forms the term-document(TD) matrix(The query is also passed as a sentence). This TD matrix is then forwarded to pLSA(Probabilistic Latent Symentic Analysis) algorithm. Then, finally calculates the cosine similarity among the document(sentence) vectors with query vector. Based on the similarity with the query vector, the most relevant sentence is displayed as the answer. (Stemming is also done at the formation of TD Matrix). 
The problem is that is does displays the result, but not the most relevant. Where am I going wrong? Is the strategy I am following is correct, or any other algorithm does exists that may help??
Below I show some of the Question and their answers as returned by my system :</p>

<pre><code>What is photosynthesis?
ANSWER  1 :   The stroma contains stacks (grana) of thylakoids, which are the site of photosynthesis 

ANSWER  2 :   Factors leaf is the primary site of photosynthesis in plants 

ANSWER  3 :   Samuel Ruben and Martin Kamen used radioactive isotopes to determine that the oxygen liberated in photosynthesis came from the water 

ANSWER  4 :   In plants, algae and cyanobacteria, photosynthesis releases oxygen 
</code></pre>

<p>Another question</p>

<pre><code>What is Artificial Intelligence?
ANSWER  1 :   the problem of creating 'artificial intelligence' will substantially be solved"" 

ANSWER  2 :   37 The leading-edge definition of artificial intelligence research is changing over time 

ANSWER  3 :   Stories of these creatures and their fates discuss many of the same hopes, fears and ethical concerns that are presented by artificial intelligence 

ANSWER  4 :   History of artificial intelligence and Timeline of artificial intelligence Thinking machines and artificial beings appear in Greek myths , such as Talos of Crete , the bronze robot of Hephaestus , and Pygmalion's Galatea 13 Human likenesses believed to have intelligence were built in every major civilization 
</code></pre>

<p>Another question</p>

<pre><code>Who is a hacker?

ANSWER  1 :   19 Hackers (short stories) Helba from the  

ANSWER  2 :   16 Rafael NÃºÃ±ez aka RaFa was a notorious most wanted hacker by the FBI since 2001 

ANSWER  3 :   Often, this type of 'white hat' hacker is called an ethical hacker 
ANSWER  4 :   Hackers also commonly use port scanners  
</code></pre>

<p>yet another run</p>

<pre><code>What is biology?
ANSWER  1 :   Molecular biology is the study of biology at a molecular level 

ANSWER  2 :   molecular biology studies the complex interactions of systems of biological molecules 

ANSWER  3 :   The similarities and differences between cell types are particularly relevant to molecular biology 

ANSWER  4 :   Contents History Foundations of modern biology 2 
</code></pre>
",Dataset Preprocessing & Handling,knowledge based q system giving appropriate answer working project basically knowledge based question answering system system take query user download relevant document wikipedia strip html tag extract plain text tokenizes document sentence form term document td matrix query also passed sentence td matrix forwarded plsa probabilistic latent symentic analysis algorithm finally calculates cosine similarity among document sentence vector query vector based similarity query vector relevant sentence displayed answer stemming also done formation td matrix problem doe display result relevant going wrong strategy following correct algorithm doe exists may help show question answer returned system another question another question yet another run
Going from corpus to individual .txt files in R&#39;s tm,"<p>I have a .csv file with 6000 rows and 2 columns.I would like to write each row as a separate text file. Any ideas as to how this can be done in tm? I tried <code>writeCorpus()</code> but that function just spits out the 150 .txt files instead of 6000. Is this a memory issue or something I am doing wrong with the code?</p>

<pre><code> library(tm)
 revs&lt;-read.csv(""dprpfinals.csv"",header=TRUE)
 corp&lt;-Corpus(VectorSource(revs$Review))
 writeCorpus(corp,path=""."",filenames=paste(seq_along(revs),"".txt"",sep=""""))
</code></pre>
",Dataset Preprocessing & Handling,going corpus individual txt file r tm csv file row column would like write row separate text file idea done tm tried function spit txt file instead memory issue something wrong code
Mild language processing of csv file in javascript,"<p>I'm looking more suggestions on an approach to processing lines of text using specific searches of words or phrases (predictable to me) to transfer into a csv. So for example lines like this:</p>

<p>A south China-based trading firm offered a cargo of low-volatile coking coal on a 15% basis with November laycan at $167/t.
A Hong Kong based trading firm received an offer of mid-volatile coking coal 24% basis with early November delivery at $145/t.</p>

<p>would become delimited info like: 
South China trader, seller, low-volatile, 15%, November, laycan, $167
Hong Kong trader, buyer, mid-volatile, 24%, early November, delivery, $145</p>

<p>There are existing javascript answers to start me suggesting str.split(""\n"") to set up array. But after that... what coding approach would you recommend to looking for word/phrase matches within a line and then setting up a corresponding comma delimited line of the distilled info (to go into a spreadsheet).</p>

<p>I am an editor looking to speed up data entry, so that's why I have a more general approach question. All the search parameters I can struggle with if I know how to get there. At work we already use javascript to pull in files. People say python is easy but I'd like something easier to use in a Windows at work. I have been google searching my way to an approach without success... just a framework and I can get there.</p>
",Dataset Preprocessing & Handling,mild language processing csv file javascript looking suggestion approach processing line text using specific search word phrase predictable transfer csv example line like south china based trading firm offered cargo low volatile coking coal basis november laycan hong kong based trading firm received offer mid volatile coking coal basis early november delivery would become delimited info like south china trader seller low volatile november laycan hong kong trader buyer mid volatile early november delivery existing javascript answer start suggesting str split n set array coding approach would recommend looking word phrase match within line setting corresponding comma delimited line distilled info go spreadsheet editor looking speed data entry general approach question search parameter struggle know get work already use javascript pull file people say python easy like something easier use window work google searching way approach without success framework get
How to change max attributes in Weka?,"<p>I am using Weka to create a Term Document Matrix using the class <strong>StringToWordVector</strong>. However, irrespective of the size of the input corpus I can see only 500 Terms/""num attributes"" in output while the same corpus is generating 549 Terms when I use the R <strong>tm</strong> package.</p>

<p>I tried changing the <strong>wordsToKeep</strong> setting but that is not affecting the total number of terms generated. To me it seems there is some default setting that I need to change to increase the terms generated. However, I did not found such a configuration.</p>

<p>I am using Weka 3.6.11 and the NGramTokenizer.</p>

<p>How do I make Weka generate more Terms?</p>
",Dataset Preprocessing & Handling,change max attribute weka using weka create term document matrix using class stringtowordvector however irrespective size input corpus see term num attribute output corpus generating term use r tm package tried changing wordstokeep setting affecting total number term generated seems default setting need change increase term generated however found configuration using weka ngramtokenizer make weka generate term
Python NLP: How to map tokenized text back into original structures?,"<p>Objective: Vectorize tokenized text to create term document matrix that enables NLP analysis on unstructured text data. Prediction and text classification will be a big part of the analysis, so being able to associate a label/class with each text is crucial. To this end, I need the term document matrix to be structured such that each row represents one text and each column represents one of the words that appears in the entire corpus. The text's class/label will also be a vector in the matrix.</p>

<p>Problem: Each record in the text is represented as an item in a list (the first record is the first item in the list, the second record is the second item in the list, and so on). Tokenizing the contents of every record in the list and generating a huge list of every word is easy. My problem is that after tokenization I'm unable to retain the original list structure and map each of the tokens with their record from the original list. This renders vectorization impossible.</p>

<p>To illustrate: </p>

<pre><code>record_one = 'I like ham. I also like pineapple.'
record_two = 'I love cheese. I enjoy tomato sauce and dough too.'
record_three = 'Hence, I dig Hawaiian pizza. And beer.'
recordList = [record_one, record_two, record_three]
</code></pre>

<p>Results (three records in list, each with two sentences):</p>

<pre><code>['I like ham. I also like pineapple.', 'I love cheese. I enjoy tomato sauce and dough too.', 'Hence, I dig Hawaiian pizza. And beer.']
</code></pre>

<p>Tokenizing:</p>

<pre><code>from nltk.tokenize import word_tokenize
wordList= []
for r in recordList:
    temp = word_tokenize(r)
    for token in temp:
        wordList.append(token)
</code></pre>

<p>Results (one huge list of every word in recordList):</p>

<pre><code>['I', 'like', 'ham.', 'I', 'also', 'like', 'pineapple', '.', 'I', 'love', 'cheese.', 'I', 'enjoy', 'tomato', 'sauce', 'and', 'dough', 'too', '.', 'Hence', ',', 'I', 'dig', 'Hawaiian', 'pizza.', 'And', 'beer', '.']
</code></pre>

<p>Here's where I'm stuck. Vectorizing at this point gives me a 28x18 matrix, when what I need is a 3x18 matrix. I need to somehow map each term to its record in recordList to give me the 3x18 matrix, but I'm not sure how to do that. I know there's something obvious I'm missing.</p>

<p>I'm new to Python and NLP, so I'm trying to keep things as simple as possible. This means using lists, creating loops to work over those lists, and list comprehensions. I know there are other modules and functions that can do this (scikit), but I'm trying to force myself to use fundamental Python data structures to improve my understanding of the language. If it's possible to create a solution using a basic Python data structure that would be awesome. </p>

<p>Thanks in advance!</p>
",Dataset Preprocessing & Handling,python nlp map tokenized text back original structure objective vectorize tokenized text create term document matrix enables nlp analysis unstructured text data prediction text classification big part analysis able associate label class text crucial end need term document matrix structured row represents one text column represents one word appears entire corpus text class label also vector matrix problem record text represented item list first record first item list second record second item list tokenizing content every record list generating huge list every word easy problem tokenization unable retain original list structure map token record original list render vectorization impossible illustrate result three record list two sentence tokenizing result one huge list every word recordlist stuck vectorizing point give x matrix need x matrix need somehow map term record recordlist give x matrix sure know something obvious missing new python nlp trying keep thing simple possible mean using list creating loop work list list comprehension know module function scikit trying force use fundamental python data structure improve understanding language possible create solution using basic python data structure would awesome thanks advance
Convert a term-document matrix to node/edge list in R,"<p>I've a term-document sparse matrix made iusing the tm package in R</p>

<p>I can convert to a term-term matrix using this snippet of code:</p>

<pre><code>library(""tm"")
data(crude)
couple.of.words &lt;- c(""embargo"", ""energy"", ""oil"", ""environment"", ""estimate"")
tdm &lt;- TermDocumentMatrix(crude, control = list(dictionary = couple.of.words))    
tdm.matrix &lt;- as.matrix(tdm)
tdm.matrix[tdm.matrix&gt;=1] &lt;- 1
tdm.matrix &lt;- tdm.matrix %*% t(tdm.matrix)
</code></pre>

<p>but it's not what I really need, since I have to build a data frame suitable to be loaded in a network analysis tool like Gephi. This data frame should ideally have three columns:</p>

<p>{term1, term2, number of docs where term1 and term2 co-occur}</p>

<p>For example (not from the real data provided in the example above) if the word ""embargo"" and ""energy"" co-occur in three documents (this can be seen in the tdm matrix, where each document fits a column), i have a row like that:</p>

<pre><code>+-----------+-------------+------+
| term1     | term 2      | Freq |
+-----------+-------------+------+
| oil       | energy      |  3   |
+-----------+-------------+------+
</code></pre>

<p>how can I build this nodes/edges dataframe from the term-document or the term-term matrix?</p>
",Dataset Preprocessing & Handling,convert term document matrix node edge list r term document sparse matrix made iusing tm package r convert term term matrix using snippet code really need since build data frame suitable loaded network analysis tool like gephi data frame ideally three column term term number doc term term co occur example real data provided example word embargo energy co occur three document seen tdm matrix document fit column row like build node edge dataframe term document term term matrix
Split multi-paragraph documents into paragraph-numbered sentences,"<p>I have a list of well-parsed, multi-paragraph documents (all paragraphs separated by <em>\n\n</em> and sentences separated by ""."") that I'd like to split into sentences, together with a number indicating the paragraph number within the document. For example, the (two paragraph) input is:</p>

<pre><code>First sentence of the 1st paragraph. Second sentence of the 1st paragraph. \n\n 

First sentence of the 2nd paragraph. Second sentence of the 2nd paragraph. \n\n
</code></pre>

<p>Ideally the output should be:</p>

<pre><code>1 First sentence of the 1st paragraph. 

1 Second sentence of the 1st paragraph. 

2 First sentence of the 2nd paragraph.

2 Second sentence of the 2nd paragraph.
</code></pre>

<p>I'm familiar with the Lingua::Sentences package in Perl that can split documents into sentences. However it is not compatible with paragraph numbering. As such I'm wondering if there's an alternative way to achieve the above (the documents contains no abbreviations). Any help is greatly appreciated. Thanks!</p>
",Dataset Preprocessing & Handling,split multi paragraph document paragraph numbered sentence list well parsed multi paragraph document paragraph separated n n sentence separated like split sentence together number indicating paragraph number within document example two paragraph input ideally output familiar lingua sentence package perl split document sentence however compatible paragraph numbering wondering alternative way achieve document contains abbreviation help greatly appreciated thanks
The options for the first step of document clustering,"<p>I checked several document clustering algorithms, such as LSA, pLSA, LDA, etc. It seems they all require to represent the documents to be clustered as a document-word matrix, where the rows stand for document and the columns stand for words appearing in the document. And the matrix is often very sparse.</p>

<p>I am wondering, is there any other options to represent documents besides using the document-word matrix? Because I believe the way we express a problem has a significant influence on how well we can solve it.</p>
",Dataset Preprocessing & Handling,option first step document clustering checked several document clustering algorithm lsa plsa lda etc seems require represent document clustered document word matrix row stand document column stand word appearing document matrix often sparse wondering option represent document besides using document word matrix believe way express problem ha significant influence well solve
Using n-gram with R for error correction,"<p>I'm working on a project to correct errors from scanned documents by an OCR and I would use n-grams with R.
My algorithm would be first trained using a corpus with know mistakes to create a confusion
matrix of 3-gram. So that when I have a particular 3-gram I can know what is the most probable 
3-gram to replace it when there's a mistake.</p>

<p>For know I only manage to create n-gram for a sentence:</p>

<pre><code>library(tau)
library(tm)

txt1 &lt;- ""The quick brown fox jumps over the lazy dog.""

r1&lt;-textcnt(txt1, method = ""ngram"", n=3)
data.frame(counts = unclass(r1), size = nchar(names(r1)))
format(r1)
</code></pre>

<p>But it gives me the frequency of each 3-gram without keeping the order and I  can't use it to create a confusion matrix.</p>
",Dataset Preprocessing & Handling,using n gram r error correction working project correct error scanned document ocr would use n gram r algorithm would first trained using corpus know mistake create confusion matrix gram particular gram know probable gram replace mistake know manage create n gram sentence give frequency gram without keeping order use create confusion matrix
How to make a .cat file from a .txt file,"<p>I'm having trouble in converting a .txt file into a .cat file. </p>

<p>It's a dictionary (LIWC2007 italian)which separates words into categories; I have to load it on WordStat 6.1. WordStat won't see any non-.cat files when I try to load the dictionary.</p>

<p>How do I convert the file? Looking on the web wasn't helpful.</p>

<p>Thanks in advance for your time.</p>
",Dataset Preprocessing & Handling,make cat file txt file trouble converting txt file cat file dictionary liwc italian separate word category load wordstat wordstat see non cat file try load dictionary convert file looking web helpful thanks advance time
Creating Rules for Word Permutations Based on a word&#39;s POS Tag,"<p>I have another question and since this community has been so great helping me along I thought I would give it another shot.</p>

<p>Right now I have Python 3 code that imports a CSV file where the first column is full of words in the following format:</p>

<pre><code>The
Words
Look
Like
This
In
A
Column
</code></pre>

<p>Once this CSV file is uploaded and read by Python, the words are tagged using an NLTK POS Tagger.  From there, permutations are made from all of the words and then the results are exported to a new CSV file.
Right now, my full code goes like this</p>

<pre><code>Import CSV
with open(r'C:\Users\jkk\Desktop\python.csv', 'r') as f:
    reader = csv.reader(f)
    J = []
    for row in reader:
      J.extend(row)
import nltk
D = nltk.pos_tag(J)
C = list(itertools.permutations(D, 3))
with open('test.csv', 'w') as a_file:
    for result in C:
    result = ' '.join(result)
    a_file.write(result + '\n')
</code></pre>

<p><strong>My question is, how does one make rules for the word permutations based on the word tag?</strong>  More specifically, the reason I am tagging words is because I don't want nonsense permutations (i.e. The This In / A This In / etc).  Once the words are tagged with their respective part of speech, how do I code rules based on their tag (for example): Never have two ""DT"" labeled words follow each other (i.e. ""The"" and ""A"").  Or always have a NN tagged word be followed by a VBG tagged word (i.e. ""Looks"" always comes after ""Words"")?  And then finally, once those rules are implemented, get rid of the tags so that just the original words remain? I realize this is a general question but any guidance would be very much appreciated on how to approach this question as I am still very new and learning every step of the way!  Any resources, code, or even advice would help!  Thank you again for taking the time to read this long post!</p>
",Dataset Preprocessing & Handling,creating rule word permutation based word po tag another question since community ha great helping along thought would give another shot right python code import csv file first column full word following format csv file uploaded read python word tagged using nltk po tagger permutation made word result exported new csv file right full code go like question doe one make rule word permutation based word tag specifically reason tagging word want nonsense permutation e etc word tagged respective part speech code rule based tag example never two dt labeled word follow e always nn tagged word followed vbg tagged word e look always come word finally rule implemented get rid tag original word remain realize general question guidance would much appreciated approach question still new learning every step way resource code even advice would help thank taking time read long post
How to distinguish bigrams and merge them into one CSV file in R Studio,"<p>Alright so I am trying to have R read sentences, pull out bigrams, and merge all of these bigrams together into one csv.  Right now I have the code to pull out bigrams for one sentence:</p>

<pre><code>sentence=gsub('[[:punct:]]','', sentence)
    sentence=gsub('[[:cntrl:]]','', sentence)
    sentence=gsub('\\d+','', sentence)
    sentence=tolower(sentence)
    words&lt;- strsplit(sentence, ""\\s+"")[[1]]
    New=NULL
    for(i in 1:length(words)-1){ 
      New[i]=paste(words[i],words[i+1])     
  }
New=as.matrix(New)
colnames(New)&lt;-""Bigrams""
</code></pre>

<p>However, I want to be able to import a csv filled with different sentences and have the previous line of code pull out bigrams for each sentence and then merge them together into one csv file. I started writing a code (below) but it is not right.  I would greatly appreciate any help I can get.  Pretty new to natural language processing in R.</p>

<pre><code>library(tm)
library(plyr)
library(stringr)
data&lt;-read.csv(""file.csv"")
sentences=as.vector(data$text)

bigrams&lt;-function(sentences){

bigrams2&lt;-mlply(sentences,function(sentence){
    sentence=gsub('[[:punct:]]','', sentence)
    sentence=gsub('[[:cntrl:]]','', sentence)
    sentence=gsub('\\d+','', sentence)
    sentence=tolower(sentence)
    words&lt;- strsplit(sentence, ""\\s+"")[[1]]
    New=NULL
    for(i in 1:length(words)-1){ 
      New[i]=paste(words[i],words[i+1])     
   }
New=as.matrix(New)
colnames(New)&lt;-""Bigrams""
New
})
merge(bigrams2,all=TRUE)

} 
</code></pre>

<p>Thanks!</p>
",Dataset Preprocessing & Handling,distinguish bigram merge one csv file r studio alright trying r read sentence pull bigram merge bigram together one csv right code pull bigram one sentence however want able import csv filled different sentence previous line code pull bigram sentence merge together one csv file started writing code right would greatly appreciate help get pretty new natural language processing r thanks
Best way to output Stanford NLP results,"<p>Hi folks:  I'm using the Stanford CoreNLP software to process hundreds of letters by different people (each about 10KB).  After I get the output, I need to further process it and add information at the level of tokens, sentences, and letters.  I'm quite new to NLP and was wondering what the typical or best way would be to output the pipeline results from Stanford CoreNLP to permit further processing?</p>

<p>I'm guessing the typical approach would be to output to XML.  If I do, I estimate that will take about a GB of disk space, and I wonder, then, how quick and easy it would be to load that much XML back into Java for further processing and adding of information?</p>

<p>An alternative might be to have CoreNLP serialize the annotation objects it produces and load those back for processing.  An advantage:  not having to figure out how to convert a sentence parse string back into a tree for further processing.  A disadvantage:  annotation objects contain a lot of different types of objects I'm still quite rough on manipulating and the documentation on these in Stanford CoreNLP seems slim to me.</p>
",Dataset Preprocessing & Handling,best way output stanford nlp result hi folk using stanford corenlp software process hundred letter different people kb get output need process add information level token sentence letter quite new nlp wa wondering typical best way would output pipeline result stanford corenlp permit processing guessing typical approach would output xml estimate take gb disk space wonder quick easy would load much xml back java processing adding information alternative might corenlp serialize annotation object produce load back processing advantage figure convert sentence parse string back tree processing disadvantage annotation object contain lot different type object still quite rough manipulating documentation stanford corenlp seems slim
Create Term Document Matrix of Bi Grams?,"<p>I am doing text mining on large data set. I was able to create TDM and DTM and was able to perform my analysis using TDF &amp; IDF. But can we create a Term Document Matrix or Document Term Matrix for Bi Grams in R? I know similar facility is available in Mahout but I am looking for a way to do this in R?</p>
",Dataset Preprocessing & Handling,create term document matrix bi gram text mining large data set wa able create tdm dtm wa able perform analysis using tdf idf create term document matrix document term matrix bi gram r know similar facility available mahout looking way r
Newline character before numbers,"<p>I am cleaning a text file and writing a regex to suit my needs. However, an error is creeping which I cannot understand.</p>

<p>Example text: </p>

<pre><code>In the spring of 2014
</code></pre>

<p>Intended Result: </p>

<pre><code>In,the,spring,of,2014
</code></pre>

<p>But my output throws: </p>

<pre><code>In,the,spring,of,
2014
</code></pre>

<p>I don't understand why 2014 goes to the newline when I am removing all \n,\r,\t from the file. This is happening to all numbers in my text block. Any word will help.</p>

<p>Edit: Regex I am using</p>

<pre><code>    newline = re.sub(""[/ --(),.\n\r\t\\\\]+"","","",line)
</code></pre>

<p>Solution: The error was due to the text editor. I was using TextEdit on Mac, no idea why was it showing a newline feed. Vim and emacs showed a clean file.</p>
",Dataset Preprocessing & Handling,newline character number cleaning text file writing regex suit need however error creeping understand example text intended result output throw understand go newline removing n r file happening number text block word help edit regex using solution error wa due text editor wa using textedit mac idea wa showing newline feed vim emacs showed clean file
Is there an established method for tagging your own corpus for supervised learning with NLTK?,"<p>I am getting ready to implement supervised named entity recognition on a specialized corpus. That means I need to label raw text by naming the entities. It seems like NLTK things about a <a href=""http://www.nltk.org/book/ch05.html"" rel=""nofollow noreferrer"">tagged token</a> as a tuple ""consisting of the token and the tag."" So my plan is to pull out some random lines from a file, put a word on each line, manually tag the words that are entities to make a csv file -- then read the csv file back in to create token/tag tuples. </p>

<p>Then do something like this (following the <a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow noreferrer"">example</a> from the docs):</p>

<pre><code>supervised = []
for line in file:
   token, tag = line.split("","")
   supervised.append(token, tag))

featuresets = [(feature_extractor(token), tag) for (token, tag) in supervised]
</code></pre>

<p>Is this how NLP practitioners usually do this? Is there a better way to do this? Is there a gold standard? Do people tag entities within the corpus structure? Sort of like this : <a href=""https://stackoverflow.com/questions/4951751/creating-a-new-corpus-with-nltk"">Creating a new corpus with NLTK</a></p>
",Dataset Preprocessing & Handling,established method tagging corpus supervised learning nltk getting ready implement supervised named entity recognition specialized corpus mean need label raw text naming entity seems like nltk thing tagged token tuple consisting token tag plan pull random line file put word line manually tag word entity make csv file read csv file back create token tag tuples something like following example doc nlp practitioner usually better way gold standard people tag entity within corpus structure sort like href new corpus nltk
Trouble conceptualizing how to have LDA-Ruby read multiple .txt files,"<p>I am attempting to write a Ruby script that will look at a collection of unstructured plain text files and I am struggling with thinking through the best way to process these files. The current working version of my script for topic modeling is the following:</p>

<pre><code>#!/usr/bin/env ruby -w

require 'rubygems'
require 'lda-ruby'

# Input a directory of files
FILES_DIRECTORY = ARGV[0]

File.open(""files.csv"", ""w"") do |f|
  Dir.glob(FILES_DIRECTORY + ""*.txt"") do |filename|
    file_id = File.basename(filename).gsub("".txt"", """")
    text = File.read(filename).clean
    f.puts [file_id, text].join("","")
  end
end

# Read csv
file = File.open(""files.csv"", ""r"") { |f| f.read }

# Train topics and infer
corpus = Lda::Corpus.new
corpus.add_document(Lda::TextDocument.new(corpus, file))

lda = Lda::Lda.new(corpus)
lda.verbose = false
lda.num_topics = 20
lda.em('random')
topics = lda.top_words(10)

puts topics
</code></pre>

<p>What I'm attempting to modify is having this program read through a collection of plain text files rather than a single file. It's not as easy as just tossing all the text files into a single file (as it currently does with <code>files.csv</code>) because, as I understand it, lda-ruby looks for multiple files to do a correct topic model rather than a single file. (I've come to this conclusion because there is little variance between having this script read a single text file [e.g., <code>corpus.txt</code>] that includes all the text, and the <code>files.csv</code> file.)</p>

<p>So, my question is how can I have lda-ruby iterate through these text files differently? Should the contents of the files be placed into a hash instead? If so, any pointers on where I should start with that? Or, should I scrap this and use a different LDA library?</p>

<p>Thanks ahead of time for any advice.</p>
",Dataset Preprocessing & Handling,trouble conceptualizing lda ruby read multiple txt file attempting write ruby script look collection unstructured plain text file struggling thinking best way process file current working version script topic modeling following attempting modify program read collection plain text file rather single file easy tossing text file single file currently doe understand lda ruby look multiple file correct topic model rather single file come conclusion little variance script read single text file e g includes text file question lda ruby iterate text file differently content file placed hash instead pointer start scrap use different lda library thanks ahead time advice
Using Weka gives different results between GUI and API implementation,"<p>I am using Weka to do classification of my dataset. First I did this using the GUI giving me some results (Accurracy, ROC, ...). Now that I'm using the API to implement a small framework around WEKA, I run the exact same configuration yet getting different results.
Let me give you an example:</p>

<p>Config of .ARFF file in GUI:</p>

<pre><code>    @relation 'QueryResult-weka.filters.unsupervised.attribute.NominalToString-Clast
-weka.filters.unsupervised.attribute.StringToWordVector-R2-W1000-prune-rate-1.0-N0
 -stemmerweka.core.stemmers.NullStemmer-M1-O-tokenizerweka.core.tokenizers.WordTokenizer -delimiters \"",
-weka.filters.unsupervised.attribute.StringToNominal-R2-last
-weka.filters.unsupervised.attribute.NumericToBinary-unset-class-temporarily
-weka.filters.unsupervised.attribute.NominalToString-Cfirst
-weka.filters.unsupervised.attribute.StringToWordVector-R1-W1000000-prune-rate-1.0-C-T-I-N0-L-S
 -stemmerweka.core.stemmers.NullStemmer-M1-stopwords/Users/stopwds.txt-tokenizerweka.core.tokenizers.WordTokenizer 
 -delimiters \"" \\r \\t.,;:\\\'\\\""()?!-/&lt;&gt;[]\\t\\r\\n\""-weka.filters.unsupervised.attribute.Remove-R120-1964
-weka.filters.unsupervised.attribute.Remove-R121-123'
</code></pre>

<p>As I said, using the API I use same config:</p>

<pre><code>@relation 'QueryResult-weka.filters.unsupervised.attribute.NominalToString-Clast
-weka.filters.unsupervised.attribute.StringToWordVector-R2-W1000-prune-rate-1.0-N0
 -stemmerweka.core.stemmers.NullStemmer-M1-O-tokenizerweka.core.tokenizers.WordTokenizer -delimiters \"",
-weka.filters.unsupervised.attribute.StringToNominal-R2-last
-weka.filters.unsupervised.attribute.NumericToBinary-unset-class-temporarily
-weka.filters.unsupervised.attribute.NominalToString-Cfirst
-weka.filters.unsupervised.attribute.StringToWordVector-R1-W1000000-prune-rate-1.0-C-T-I-N0-L-S
 -stemmerweka.core.stemmers.NullStemmer-M1-stopwords/Users/stopwds.txt-tokenizerweka.core.tokenizers.WordTokenizer 
 -delimiters \"" \\r \\t.,;:\\\'\\\""()?!-/&lt;&gt;[]\\t\\r\\n\""-weka.filters.unsupervised.attribute.Remove-R120-1964
-weka.filters.unsupervised.attribute.Remove-R121-123'
</code></pre>

<p>Now when I run the classifier, again with same configurations, I get different output! However, the funny part is when I load the .ARFF file generated from my Java code after running the config and then train the classifier there, I DO get the exact same output as expected/required.</p>

<p>Can please someone explain what I'm doing wrong and why the output is different? I read other posts such as <a href=""https://stackoverflow.com/questions/11872974/weka-ui-and-api-code-in-java-gives-different-results?rq=1"">link</a> where a similar problem occurred.</p>

<p>--
To clarify, here is the config of my classifier in the GUI:</p>

<pre><code> weka.classifiers.lazy.IBk -K 30 -W 0 -I -A ""weka.core.neighboursearch.LinearNNSearch -A \""weka.core.EuclideanDistance -R first-last\""""
</code></pre>

<p>And this is how it is in my Java code:</p>

<pre><code>iBk.setOptions(weka.core.Utils.splitOptions(""-K 30 -W 0 -I -A \""weka.core.neighboursearch.LinearNNSearch -A \\\""weka.core.EuclideanDistance -R first-last\\\""\""""));
</code></pre>
",Dataset Preprocessing & Handling,using weka give different result gui api implementation using weka classification dataset first using gui giving result accurracy roc using api implement small framework around weka run exact configuration yet getting different result let give example config arff file gui said using api use config run classifier configuration get different output however funny part load arff file generated java code running config train classifier get exact output expected required please someone explain wrong output different read post href similar problem occurred p clarify config classifier gui java code
Web Crawling: Assigning a score to a URL (using its words composing it) given statistics of words previously crawled,"<p>I'm having a hard time developing an algorithm/formula to determine the score of a link given the words that compose it. This is also applicable to the context (word sentences) that wrap around the URL. For simplicity's sake, the host of the URL is not taken into account.</p>

<p>When processing a web document, a score is computed for that page which will be passed to the outlinks found in the page. There are input words/terms (will be called search tags from now on - may be composed of multiples words) that will determine if a given document is relevant (having a positive page score). Each term has a given weight that will determine how much score it will add to the page's score. So in general, a page's score is function of all found input search tags in the documents, the frequency of these tags in the document, and the weights of each of the tags.</p>

<p>If a page has a positive score (relevant), all the terms/words in the page will have its statistics updated, i.e. the page's score will be accumulated to each of the word's sum of scores. Statistics for search tags themselves will have a ""boost"", i.e. accumulated score will be multiplied by some constant</p>

<p>So given a set of terms/words (terms are multiple words; only search tags) that have been previously crawled, each of this word has these different ""statistics"":</p>

<ol>
<li>Accumulated Score from crawled relevant pages</li>
<li>Number of times this word has been found in <strong>relevant</strong> pages</li>
<li>Number of times this word has been found in <strong>irrelevant</strong> pages</li>
</ol>

<p>Given the statistics of all found words in the crawl, how should I use these statistics to compute the score (or probability of being relevant) of a link or its cohesive text context? Or any new word statistics I should add to make use of certain statistical methods like Bayesian Classification? Any brilliant ideas? Thanks so much!</p>

<p><strong>Edit:</strong>
Note that statistics is the same for synonymous word, regardless of which part of speech they belong to. I will be using WordNet to implement this.</p>
",Dataset Preprocessing & Handling,web crawling assigning score url using word composing given statistic word previously crawled hard time developing algorithm formula determine score link given word compose also applicable context word sentence wrap around url simplicity sake host url taken account processing web document score computed page passed outlinks found page input word term called search tag may composed multiple word determine given document relevant positive page score term ha given weight determine much score add page score general page score function found input search tag document frequency tag document weight tag page ha positive score relevant term word page statistic updated e page score accumulated word sum score statistic search tag boost e accumulated score multiplied constant given set term word term multiple word search tag previously crawled word ha different statistic accumulated score crawled relevant page number time word ha found relevant page number time word ha found irrelevant page given statistic found word crawl use statistic compute score probability relevant link cohesive text context new word statistic add make use certain statistical method like bayesian classification brilliant idea thanks much edit note statistic synonymous word regardless part speech belong using wordnet implement
Stanford parser - count of tags,"<p>I have been using the <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow noreferrer"">Stanford Parser</a> for CFG analysis. I can get the output displayed as a tree, but what I really want is a count of tags.</p>

<p>So I can get out, for example (taken from <a href=""https://stackoverflow.com/questions/14373557/how-to-get-phrase-tags-in-stanford-corenlp"">another query</a> on Stack Overflow):</p>

<pre><code>(ROOT (S (NP (PRP$ My) (NN dog)) (ADVP (RB also)) (VP (VBZ likes) (NP (JJ eating) (NN sausage))) (. .)))
</code></pre>

<p>But what I really want is a count of the tags output in a CSV file:</p>

<pre><code>PRP - 1
JJ - 1
</code></pre>

<p>Is this possible with the Stanford parser, particularly as I want to process several text files, or should I use a different program?</p>
",Dataset Preprocessing & Handling,stanford parser count tag using stanford parser cfg analysis get output displayed tree really want count tag get example taken href query stack overflow really want count tag output csv file possible stanford parser particularly want process several text file use different program
The meaning/implication of the matrices generated by Singular Value Decomposition (SVD) for Latent Semantic Analysis (LSA),"<p>SVD is used in LSA to get the latent semantic information. I am confused about the interpretation about the SVD matrices.</p>

<p>We first build a document-term matrix. And then use SVD to decompose it into 3 matrices.</p>

<p>For example:</p>

<p>The doc-term matrix M1 is  M x N, where:</p>

<pre><code>M = the number of documents
N = the number of terms
</code></pre>

<p>And M1 was decomposed into:</p>

<pre><code>M1 = M2 * M3 * M4, where:

M2: M x k

M3: k x k

M4: k x N
</code></pre>

<p>I see the interpretation like below:</p>

<p>The k <strong>column</strong> of M2 stands for categories of similar <strong>semantics</strong>.
The k <strong>row</strong> of M4 stands for the <strong>topics</strong>.</p>

<p>My questions are:</p>

<ol>
<li><p>Why is k interpreted like above? How do we know it is similar semantics and topics?</p></li>
<li><p>Why the similar semantics equal the topics?</p></li>
<li><p>Why k is interpreted differently between M2 and M4</p></li>
<li><p>How to interpret the M3?</p></li>
</ol>

<p>I am really confused. It seems the interpretation is totally arbitrary. Is that what <strong>latent</strong> meant to be?</p>
",Dataset Preprocessing & Handling,meaning implication matrix generated singular value decomposition svd latent semantic analysis lsa svd used lsa get latent semantic information confused interpretation svd matrix first build document term matrix use svd decompose matrix example doc term matrix x n wa decomposed see interpretation like k column stand category similar semantics k row stand topic question k interpreted like know similar semantics topic similar semantics equal topic k interpreted differently interpret really confused seems interpretation totally arbitrary latent meant
Sharing state between forked worker processes in a high-performance environment,"<p>This is a follow up to my <a href=""https://stackoverflow.com/questions/20955683/python-multiprocessing-sharing-a-complex-object/"">previous question</a>. As suggested by Tim Peters, using a <code>Manager</code> may not necessarily be the best approach. Unfortunately I've got too much scaffolding code to post a <a href=""http://sscce.org"" rel=""nofollow noreferrer"">SSCCE</a>. Instead, I'll try to provide a detailed explanation of my problem. Please feel free to browse the entire codebase on <a href=""https://github.com/mbatchkarov/thesisgenerator"" rel=""nofollow noreferrer"">Github</a>, but it's a bit of a mess right now.</p>

<h2>Background</h2>

<p>I am doing research in Natural Language Processing and I'd like to do (something like) dictionary-based smoothing for document classification. The idea to train a classifier to associate words and phrases with a correct answer. For example, documents containing the word <code>socialist</code> are likely to be about politics, and those containing the phrase <code>lava temperature</code> are likely about geology. The system is trained by looking at a <strong>small number</strong> of pre-labelled examples. Because language is so varied, a classifier will never ""know about"" all possible phrases that it might encounter in production. </p>

<p>This is where the dictionary comes in. Suppose we had <a href=""https://stackoverflow.com/questions/15173225/how-to-calculate-cosine-similarity-given-2-sentence-strings-python/15173821#15173821"">a cheap and easy way</a> of getting synonyms for almost any phrase out there (I'll cite myself because it's poor taste). When the poor classifier is faced with a phrase it doesn't know about, we could look it up in said dictionary and tell the classifier ""Look, you do not know about <code>communism</code>, but it's kinda like <code>socialist</code>, and you know about that!"". If the dictionary is reasonable, the classifier will generally perform better.</p>

<h2>Pseudo code</h2>

<pre><code>data = Load training and testing documents (300MB on disk)
dictionary = Load dictionary (200MB - 2GB on disk) and place into a `dict` for fast look-ups
Repeat 25 times:
    do_work(data, dictionary)

def do_work(data, dictionary)
    X = Select a random sample of data
    Train a classifier on X
    Y = Select a random sample of data
    Using dictionary, classify all documents in Y
    Write results to disk
</code></pre>

<h2>The problem</h2>

<p>The loop above is a perfect candidate for parallelisation. I have been using a Python 2.7  <code>multiprocessing.Pool</code> (through <code>joblib.Parallel</code>, because it’s easy and provides very useful traceback if things go south). All worker processes need read-only access to the dictionary and the document collection. There is no need for the workers to communicate with one another or with the parent process- all they do is spawn, do some magic, write a file and die.</p>

<p>The dictionary needs to support fast random access. I do not know what documents the sample <code>Y</code> will contain, so I cannot easily prune the dictionary and pass just the part of it that is needed to each worker. The dictionary will be queried very often- typical hit counts per run are in the millions.
Currently my code is memory-bound as (I believe) copies of the document collection and dictionary are being made for each worker process. When parsed <code>data</code> and <code>dictionary</code> typically use up several GB of RAM. I’ve tried using <code>multiprocessing.managers.BaseManager</code> to avoid copying the large objects, but that slowed the workers down. </p>

<h2>The question</h2>

<p>What other alternatives are there to speed things up? Things I have thought about include:</p>

<ul>
<li>MongoDB/CouchDB/memcached should handle concurrent access well, but I’m worried about throughput. zeromq was also suggested in a comment to my previous question, haven't had a chance to look into it.</li>
<li>in-memory <code>sqlite</code> databases and database connections cannot be shared across processes, so each worker will need its own connection to an on-disk database. This means a lot of I/O at first and high memory usage as each worker's cache grows.</li>
<li>memory mapping</li>
<li>using threads instead of processes</li>
</ul>

<p><a href=""https://stackoverflow.com/questions/659865/python-multiprocessing-sharing-a-large-read-only-object-between-processes"">This SO question</a> also suggested that many real-world problems that look like they need read-only access to a <code>dict</code> may trigger <code>fork()</code>'s copy-on-write, so it may be impossible to completely avoid making copies of large objects.</p>
",Dataset Preprocessing & Handling,sharing state forked worker process high performance environment follow sscce instead try provide detailed explanation problem please feel free browse entire codebase github bit mess right background research natural language processing like something like dictionary based smoothing document classification idea train classifier associate word phrase correct answer example document containing word likely politics containing phrase likely geology system trained looking small number pre labelled example language varied classifier never know possible phrase might encounter production dictionary come suppose pseudo code problem loop perfect candidate parallelisation using python easy provides useful traceback thing go south worker process need read access dictionary document collection need worker communicate one another parent process spawn magic write file die dictionary need support fast random access know document sample contain easily prune dictionary pas part needed worker dictionary queried often typical hit count per run million currently code memory bound believe copy document collection dictionary made worker process parsed typically use several gb ram tried using avoid copying large object slowed worker question alternative speed thing thing thought include mongodb couchdb memcached handle concurrent access well worried throughput zeromq wa also suggested comment previous question chance look memory database database connection shared across process worker need connection disk database mean lot first high memory usage worker cache grows memory mapping using thread instead process href question also suggested many real world problem look like need read access may trigger copy write may impossible completely avoid making copy large object
extracting the word next to a specific word in a text file using java,"<p>I want to read a text file and print the word previous to known word say xxx for instance in Java.
Ive written this code in java using Scanner class.
but this code prints only half of the word preceding to ""xxx"" some other words preceding ""xxx"" are missing.
I want to know whats the problem and can u troubleshoot this code.</p>

<p>Test file contains stuff like</p>

<pre><code>Blah blah blah.. man xxx create blah blah .. wander xxx blah... then xxx ..
Need to print man,wander,then etc.,
</code></pre>

<hr/> 

<pre><code>public class Searchright {
    public static void main(String[] args) throws IOException {

        Scanner s = null;
        String str;

        try {
            s = new Scanner(new BufferedReader(new FileReader(""doc.txt"")));
            str=s.next();
         do{
            //while (s.hasNext()) {
                 str=s.next();
                if((s.hasNext((""xxx""))||s.hasNext((""X.X.X”))))
               {
                        //System.out.println(s.next()+"" ""+s.next() );
                        //System.out.println(s.next());     
                        System.out.println(str);
                        //s.next();
                    }

              s.next();
            //System.out.println(s.next());

            }while(s.hasNext());
        }

        finally{
            if (s != null) {
                //s.close();
            }
        }
    }
    }
</code></pre>
",Dataset Preprocessing & Handling,extracting word next specific word text file using java want read text file print word previous known word say xxx instance java ive written code java using scanner class code print half word preceding xxx word preceding xxx missing want know whats problem u troubleshoot code test file contains stuff like
Designing an NLP API in Javascript,"<p>I'm working on <a href=""https://github.com/erick-fernandes/nat-js"" rel=""nofollow"">nat-js</a> (a NLP toolkit in Javascript) and I'm planning to extend it to process other languages, like English and Spanish; today it process only Portuguese.</p>

<p>With nat-js, when you need a tokenizer, you write something like this:</p>

<pre><code>var tkz = new nat.tokenizer();
</code></pre>

<p>But how could it be done to process other languages? What do you think about this:</p>

<pre><code>var nat = new natFactory('pt');
var tkz = new nat.tokenizer();
</code></pre>

<p>And there is something I've been thinking about: why load the full library if you're are needing only the tokenizer in Portuguese? Can I offer a simple way to load only the required files?</p>
",Dataset Preprocessing & Handling,designing nlp api javascript working nat j nlp toolkit javascript planning extend process language like english spanish today process portuguese nat j need tokenizer write something like could done process language think something thinking load full library needing tokenizer portuguese offer simple way load required file
Utility to generate performance report of a NLP based text annotator,"<p>I am trying to build a quality test framework for my text annotator. I wrote my annotators using <a href=""http://gate.ac.uk/"" rel=""nofollow"">GATE</a></p>

<p>I do have gold-standard (human annotated) data for every input document.</p>

<p>Here is list of gate resource for quality assurance <a href=""http://gate.ac.uk/sale/tao/splitch10.html#x14-28100010.3.5"" rel=""nofollow"">GATE Embedded API for the measures</a> </p>

<p>So far, I am able to get performance matrix containing <code>FP,TP,FN, Precision, Recall and Fscores</code> using methods in
<a href=""http://gate.ac.uk/gate/src/gate/util/AnnotationDiffer.java"" rel=""nofollow"">AnnotationDiﬀer</a></p>

<p>Now, I want to dive deeper. I would like to look at individual FP,FN on per document basis.
i.e. I want to analyize  each FP and FN so that I can fix my annotator accordingly.</p>

<p><strong>I didn't see any function in any of GATE's classes such as AnnotationDiffer which returns <code>List&lt;Annotation&gt;</code> of FP or FN. They just return count of FP and FN</strong></p>

<pre><code>int fp=annotationDiffer.getFalsePositivesStrict()
int fn=annotationDiffer.getMissing()
</code></pre>

<p>Before I go ahead and create my own utility to get <code>List&lt;Annotation&gt;</code> of FP and FN and couple of surrounding sentences, to create an HTML report per input document for analysis. I wanted to check if there is something like that already exists.</p>
",Dataset Preprocessing & Handling,utility generate performance report nlp based text annotator trying build quality test framework text annotator wrote annotator using gate gold standard human annotated data every input document list gate resource quality assurance gate embedded api measure far able get performance matrix containing using method annotationdi er want dive deeper would like look individual fp fn per document basis e want analyize fp fn fix annotator accordingly see function gate class annotationdiffer return fp fn return count fp fn go ahead create utility get fp fn couple surrounding sentence create html report per input document analysis wanted check something like already exists
Read CSV error in Stanford Topic Modeling Toolbox,"<p>I am trying to use the Stanford Topic Modeling Toolbox (TMT) to try out Topic Modeling [0]. I am a Scala beginner. However, I can't seem to prepare my data set by reading a CSV file. Here's my code</p>

<pre><code>import scalanlp.io._;

val source = CSVFile(""pubmed-oa-subset.csv"") ~&gt; IDColumn(1);

println(source.data.size);
</code></pre>

<p>This throws the following error</p>

<pre><code>Stanford TMT\example-0-test.scala:6: error: not found: value IDColumn
val source = CSVFile(""pubmed-oa-subset.csv"") ~&gt; IDColumn(1);
</code></pre>

<p>Similarly, I also get an error on other data pre-processing functions like Tokenizer. Here's the code</p>

<pre><code>// Stanford TMT Example 0 - Basic data loading
// http://nlp.stanford.edu/software/tmt/0.4/


import scalanlp.io._;
val source = CSVFile(""pubmed-oa-subset.csv"") ;
println(source.data.size);

val tokenizer = {
  SimpleEnglishTokenizer()
 }
</code></pre>

<p>Here's the error received for the above code.</p>

<pre><code>error: not found: value SimpleEnglishTokenizer
  SimpleEnglishTokenizer()
</code></pre>

<p>I am using the same CSV file as given on the TMT homepage [1]. Also, the script and the data are in the same folder.</p>

<p>What is the issue? I am unable to run the exact same test examples from the TMT homepage.</p>

<p>[0] <a href=""http://nlp.stanford.edu/software/tmt/tmt-0.4/"" rel=""nofollow"">http://nlp.stanford.edu/software/tmt/tmt-0.4/</a></p>

<p>[1] <a href=""http://nlp.stanford.edu/software/tmt/tmt-0.4/examples/pubmed-oa-subset.csv"" rel=""nofollow"">http://nlp.stanford.edu/software/tmt/tmt-0.4/examples/pubmed-oa-subset.csv</a></p>
",Dataset Preprocessing & Handling,read csv error stanford topic modeling toolbox trying use stanford topic modeling toolbox tmt try topic modeling scala beginner however seem prepare data set reading csv file code throw following error similarly also get error data pre processing function like tokenizer code error received code using csv file given tmt homepage also script data folder issue unable run exact test example tmt homepage
write regexp to read sentence from text file in matlab,"<pre><code>&lt;s&gt; an evolutionary immune network for data clustering &lt;/s&gt;
&lt;s&gt; an evolutionary immune network for data clustering &lt;/s&gt;
&lt;s&gt; inet an extensible framework for simulating immune network &lt;/s&gt;
&lt;s&gt; immunity based systems a survey &lt;/s&gt;
&lt;s&gt; a recommender system based on the immune network &lt;/s&gt;
</code></pre>

<p>I am working in MATLAB and these sentences are from the text file, I want to read these sentences line by line and want to extract each word as well as count the frequency of each word. How can I use the ""regexp"" function to extract the words?</p>
",Dataset Preprocessing & Handling,write regexp read sentence text file matlab working matlab sentence text file want read sentence line line want extract word well count frequency word use regexp function extract word
Caching large objects in a python Flask/Gevent web service,"<p>I am building a python based web service that provides natural language processing support to our main app API. Since it's so NLP heavy, it requires unpickling a few very large (50-300MB) corpus files from the disk before it can do any kind of analyses. </p>

<p>How can I load these files into memory so that they are available to every request? I experimented with memcached and redis but they seem designed for much smaller objects. I have also been trying to use the Flask <code>g</code> object, but this only persists throughout one request.</p>

<p>Is there any way to do this while using a gevent (or other) server to allow concurrent connections? The corpora are completely read-only so there ought to be a safe way to expose the memory to multiple greenlets/threads/processes.</p>

<p>Thanks so much and sorry if it's a stupid question - I've been working with python for quite a while but I'm relatively new to web programming.</p>
",Dataset Preprocessing & Handling,caching large object python flask gevent web service building python based web service provides natural language processing support main app api since nlp heavy requires unpickling large mb corpus file disk kind analysis load file memory available every request experimented memcached redis seem designed much smaller object also trying use flask object persists throughout one request way using gevent server allow concurrent connection corpus completely read ought safe way expose memory multiple greenlets thread process thanks much sorry stupid question working python quite relatively new web programming
Document Query similarity for very short documents,"<p>I am working on a project which incorporates a basic implementation of the vector space model. A collection of documents d1...dn form the columns of the term document matrix, the rows represent the words in the collection. I use standard tf-idf scoring with cosine similarity to calculate the distance between a query and a document.</p>

<p>My question is, which distance metric can ""tackle"" similarity between short documents. Example: A document containing a single word, which is part of the query, will score very high using cosine similarity, since the norm of such a document is very small. How can I ""punish"" such documents which are obviously irrelevant?</p>
",Dataset Preprocessing & Handling,document query similarity short document working project incorporates basic implementation vector space model collection document dn form column term document matrix row represent word collection use standard tf idf scoring cosine similarity calculate distance query document question distance metric tackle similarity short document example document containing single word part query score high using cosine similarity since norm document small punish document obviously irrelevant
Searching and extracting WH-word from a file line by line with Python and regex,"<p>I have a file that has one sentence per line. I am trying to read the file and search if the sentence is a question using regex and extract the wh-word from the sentences and save them back into another file according the order it appeared in the first file.</p>

<p>This is what I have so far..</p>

<pre><code>def whWordExtractor(inputFile):
    try:
        openFileObject = open(inputFile, ""r"")
        try:

            whPattern = re.compile(r'(.*)who|what|how|where|when|why|which|whom|whose(\.*)', re.IGNORECASE)
            with openFileObject as infile:
                for line in infile:

                    whWord = whPattern.search(line)
                    print whWord

# Save the whWord extracted from inputFile into another whWord.txt file
#                    writeFileObject = open('whWord.txt','a')                   
#                    if not whWord:
#                        writeFileObject.write('None' + '\n')
#                    else:
#                        whQuestion = whWord   
#                        writeFileObject.write(whQuestion+ '\n') 

        finally:
            print 'Done. All WH-word extracted.'
            openFileObject.close()
    except IOError:
        pass

The result after running the code above: set([])
</code></pre>

<p>Is there something I am doing wrong here? I would be grateful if someone can point it out to me.</p>
",Dataset Preprocessing & Handling,searching extracting wh word file line line python regex file ha one sentence per line trying read file search sentence question using regex extract wh word sentence save back another file according order appeared first file far something wrong would grateful someone point
Multi-column layout handling with pdfminer pdf2txt.py module,"<p>So far I am using <a href=""http://www.unixuser.org/~euske/python/pdfminer/index.html#pdf2txt"" rel=""noreferrer"">pdfminer pdf2txt.py</a> module with success.   </p>

<p>But a problem arises in pdf files formatted in two columns. The module retrieves text into a single column which results into many split words, at the end of lines. example:</p>

<blockquote>
  <p>and functional properties of cellu-<br>
  lar components negatively, both physically and chemically.</p>
</blockquote>

<p>*Note that the words are separated by the '-' character.</p>

<p>What I want is to customize the command in order for the words, in the end of the line, to appear as a whole and therefore do not lose information.
Probably by adding a line parameter or a character margin, specific for '-' character to be replaced by a backslash?</p>

<p>I would also like to know if there is way to loop the command and make it parse a directory full of pdf files, each time generating a different output text file named after the original?</p>

<p>I am not sure how to do it though.</p>
",Dataset Preprocessing & Handling,multi column layout handling pdfminer pdf txt py module far using pdfminer pdf txt py module success problem arises pdf file formatted two column module retrieves text single column result many split word end line example functional property cellu lar component negatively physically chemically note word separated character want customize command order word end line appear whole therefore lose information probably adding line parameter character margin specific character replaced backslash would also like know way loop command make parse directory full pdf file time generating different output text file named original sure though
Best library for automatic document classification,"<p>The problem: we have a bunch of documents (magazine articles) that need to be put into ""categories"". Some categories reflect the subject of the article (what the article is about) and some other categories reflect the ""nature"" of the article (where it would be likely to appear if the magazine were printed on paper).</p>

<p>We're currently addressing the problem manually by sending the articles offshore and have people look at them and tag them.</p>

<p>We'd like to automate the process more. I've looked at various libraries but they don't seem designed to solve this problem.</p>

<p>Carrot² does clustering of search results but it's not clear, without diving in further, if it can work with existing (fixed) categories or if it infers categories directly from each input.</p>

<p>NLTK is a generalist solution that does many things, but doesn't have a reputation for speed or accuracy. May be my best bet though?</p>

<p>Ideally I would like to find a solution that given a list of categories and a training set of categorized documents, is able to suggest a category for new documents, and its confidence in the accuracy of its suggestion.</p>

<p>If this doesn't exist ready made, I can try and write something based on NLTK's NaiveBayesClassifier, but what are the other options?</p>
",Dataset Preprocessing & Handling,best library automatic document classification problem bunch document magazine article need put category category reflect subject article article category reflect nature article would likely appear magazine printed paper currently addressing problem manually sending article offshore people look tag like automate process looked various library seem designed solve problem carrot doe clustering search result clear without diving work existing fixed category infers category directly input nltk generalist solution doe many thing reputation speed accuracy may best bet though ideally would like find solution given list category training set categorized document able suggest category new document confidence accuracy suggestion exist ready made try write something based nltk naivebayesclassifier option
What other inputs are there to Word Sense Disambiguation task?,"<p>In <code>Natural Language Processing</code> (NLP), the <code>Word Sense Disambiguation</code> (WSD) task computationally determines the meaning(s) or sense(s) or concept(s) of a polysemous word given a sentence that the word appears in. For example:</p>

<ul>
<li><em>""Some was stupid enough to rob the central</em> <strong>bank*</strong>.""*</li>
<li><em>""The river</em> <strong>bank</strong> <em>is full of stones""</em></li>
</ul>

<p><strong>Do anyone know on WSD performed in paragraph or document level?</strong></p>

<p>Other than disambiguate senses/meaning from context words in one sentence, <strong>what other input could be introduce to perform <code>WSD</code> task?</strong>  (I've seen WSD with images before, <a href=""http://acl.ldc.upenn.edu/W/W03/W03-0601.pdf"" rel=""nofollow"">http://acl.ldc.upenn.edu/W/W03/W03-0601.pdf</a>)</p>
",Dataset Preprocessing & Handling,input word sense disambiguation task nlp wsd task computationally determines meaning sense concept polysemous word given sentence word appears example wa stupid enough rob central bank river bank full stone anyone know wsd performed paragraph document level disambiguate sens meaning context word one sentence input could introduce perform task seen wsd image
(Python Scipy) How to flatten a csr_matrix and append it to another csr_matrix?,"<p>I am representing each XML document as a feature matrix in a csr_matrix format. Now that I have around 3000 XML documents, I got a list of csr_matrices. I want to flatten each of these matrices to become feature vectors, then I want to combine all of these feature vectors to form one csr_matrix representing all the XML documents as one, where each row is a document and each column is a feature. </p>

<p>One way to achieve this is through this code </p>

<pre><code>X= csr_matrix([a.toarray().ravel().tolist() for a in ls])
</code></pre>

<p>where ls is the list of csr_matrices, however, this is highly inefficient, as with 3000 documents, this simply crashes!</p>

<p>In other words, my question is, how to flatten each csr_matrix in that list 'ls' without having to turn it into an array, and how to append the flattened csr_matrices into another csr_matrix.</p>

<p>Please note that I am using python with Scipy</p>

<p>Thanks in advance!</p>
",Dataset Preprocessing & Handling,python scipy flatten csr matrix append another csr matrix representing xml document feature matrix csr matrix format around xml document got list csr matrix want flatten matrix become feature vector want combine feature vector form one csr matrix representing xml document one row document column feature one way achieve code l list csr matrix however highly inefficient document simply crash word question flatten csr matrix list l without turn array append flattened csr matrix another csr matrix please note using python scipy thanks advance
Topic modeling using mallet,"<p>I'm trying to use topic modeling with Mallet but have a question.</p>

<p>How do I know when do I need to rebuild the model? For instance I have this amount of documents I crawled from the web, using topic modeling provided by Mallet I might be able to create the models and infer documents with it. But overtime, with new data that I crawled, new subjects may appear. In that case, how do I know whether I should rebuild the model from start till current? </p>

<p>I was thinking of doing so for documents I crawled each month. Can someone please advise?</p>

<p>So, is topic modeling more suitable for text under a fixed amount of topics (the input parameter k, no. of topics). If not, how do I really determine what number to use?</p>
",Dataset Preprocessing & Handling,topic modeling using mallet trying use topic modeling mallet question know need rebuild model instance amount document crawled web using topic modeling provided mallet might able create model infer document overtime new data crawled new subject may appear case know whether rebuild model start till current wa thinking document crawled month someone please advise topic modeling suitable text fixed amount topic input parameter k topic really determine number use
Getting paragraph count from Tika for both Word and PDF,"<p>I have a scenario where I need to reconcile two documents, an Word (.docx) doc as well as a PDF. The two are supposed to be ""indentical"" to each other (the PDF is just a PDF version of the DOCX file); meaning they should contain the same text, content, etc.</p>

<p>Specifically, I need to make sure that both documents contain the same number of paragraphs. So I need to read the DOCX, get the paragraph count, then read the PDF and grab its paragraph count. If both numbers are the same, then I'm in business.</p>

<p>It looks like Apache Tika (<strong>I'm interested in 1.3</strong>) is the right tool for the job here. I see in <a href=""http://svn.apache.org/repos/asf/tika/trunk/tika-parsers/src/main/java/org/apache/tika/parser/odf/OpenDocumentMetaParser.java"" rel=""nofollow"">this source file</a> that Tika supports the notion of paragraph counting, but trying to figure out how to get the count from both documents. Here's my best attempt but I'm choking on connecting some of the final dots:</p>

<pre><code>InputStream docxStream = new FileInputStream(""some-doc.docx"");
InputStream pdfStream = new FileInputStream(""some-doc.pdf"");

ContentHandler handler = new DefaultContentHandler();
Metadata docxMeta = new Metadata();
Metadata pdfMeta = new Metadata();
Parser parser = new OfficeParser();
ParseContext pc = new ParseContext();

parser.parse(docxStream, handler, docxMeta, pc);
parser.parse(pdfStream, handler, pdfMeta, pc);

docxStream.close();
pdfStream.close();

int docxParagraphCount = docxMeta.getXXX(???);
int pdfParagraphCount = pdfMeta.getXXX(???);

if(docxParagraphCount == pdfParagraphCount)
    setInBusiness(myself, true);
</code></pre>

<p>So I ask: <strong>have I set this up correctly or am I way off base?</strong> If off-base, please lend me some help to get me back on track. And if I have set things up correctly, then how do I get the desired counts out of the two <code>Metadata</code> instances? Thanks in advance.</p>
",Dataset Preprocessing & Handling,getting paragraph count tika word pdf scenario need reconcile two document word docx doc well pdf two supposed indentical pdf pdf version docx file meaning contain text content etc specifically need make sure document contain number paragraph need read docx get paragraph count read pdf grab paragraph count number business look like apache tika interested right tool job see source file tika support notion paragraph counting trying figure get count document best attempt choking connecting final dot ask set correctly way base base please lend help get back track set thing correctly get desired count two instance thanks advance
Scala Large Text file,"<p>I'm a newbie with Scala programming.</p>

<p>I have to deal with an <strong>NLP</strong> task.</p>

<p>I'm having trouble with <strong>processing a large text file</strong> in <strong>Scala</strong>.</p>

<p>I have read the entire text of a 100+ M.B file onto memory (into a string) and have to process it (I believe processing large text files is a common task in Natural Language Processing).</p>

<p><strong>The goal is to count the number of unique substrings/words in the given string</strong> (which is the entire file).  </p>

<p>I wanted to use ""<em>distinct</em>"" method in <em>List</em> object, but <strong>converting the string into a list</strong> using ""<em>.split</em>"" method raises out of memory error (""java.lang.OutOfMemoryError: Java heap space"" Error).</p>

<p>I was wondering if I could accomplish this task without using lists using String or Regular Expression methods in Scala?</p>
",Dataset Preprocessing & Handling,scala large text file newbie scala programming deal nlp task trouble processing large text file scala read entire text b file onto memory string process believe processing large text file common task natural language processing goal count number unique substring word given string entire file wanted use distinct method list object converting string list using split method raise memory error java lang outofmemoryerror java heap space error wa wondering could accomplish task without using list using string regular expression method scala
How to read the contents of a PDF file?,"<p>I'm currently working on my thesis, and the application is going to use <strong>natural language question answering</strong>. I've read about several ideas and followed discussions about natural language question answering, but I can't seem to find good answers.</p>

<p><strong>Question: How do I get answers from PDF, plain text, or MS Word file?</strong></p>

<p>If I want to search for a topic in a PDF file I would use <kbd>Ctrl</kbd>+<kbd>F</kbd> to find the topic/idea, but it wouldn't return all the details; just like a table of contents, it would give the starting page and end page of a chapter. That's what I want for the logic. It would determine where the chapter ends without using pages or numbers. Is there any algorithm capable of doing that?</p>
",Dataset Preprocessing & Handling,read content pdf file currently working thesis application going use natural language question answering read several idea followed discussion natural language question answering seem find good answer question get answer pdf plain text word file want search topic pdf file would use ctrl f find topic idea return detail like table content would give starting page end page chapter want logic would determine chapter end without using page number algorithm capable
How to load multiple XML files of corpora with NLTK and use it as a whole with Text class?,"<p>Folks, I've put together a set of corpora for NLTK which are basically simple XML files. I can load it just fine like that:</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import cicero
&gt;&gt;&gt; print cicero.fileids()
['cicero_academica.xml', 'cicero_arati_phaenomena.xml', ...]
</code></pre>

<p>Now, I understand XMLCorpusReader won't give my the content of all those XML files at once because it expects only one single XML at once to processe, right? I tried to bypass it writing a for loop, putting it all in a list and give it to XMLCorpusReader but no luck...</p>

<p><strong>Simply put: how could I load multiple XML corpora with NLTK and run .words() in all of them at once? Working code examples would be good.</strong></p>

<p>It seems that I can't load all XML at once and use them, say, with class Text() to, say again, print concordances of a word through ALL the XML files, not only through one at a time.</p>

<p>Is there any work around or real NLTK solution for this? Should I write a magical inherited class of XMLCorpusReader that does it? Should I drop XML and go for flat files...?</p>

<p>This is similar to my problem, but so far I think the answers there are not really useful NLTK-wise: <a href=""https://stackoverflow.com/questions/6837566/can-nltks-xmlcorpusreader-be-used-on-a-multi-file-corpus"">Can NLTK&#39;s XMLCorpusReader be used on a multi-file corpus?</a></p>
",Dataset Preprocessing & Handling,load multiple xml file corpus nltk use whole text class folk put together set corpus nltk basically simple xml file load fine like understand xmlcorpusreader give content xml file expects one single xml processe right tried bypass writing loop putting list give xmlcorpusreader luck simply put could load multiple xml corpus nltk run word working code example would good seems load xml use say class text say print concordance word xml file one time work around real nltk solution write magical inherited class xmlcorpusreader doe drop xml go flat file similar problem far think answer really useful nltk wise href nltk xmlcorpusreader used multi file corpus
Guess tags of a paragraph programmatically using python,"<p>I've trying to read about NLP in general and nltk in specific to use with python. I don't know for sure if what am looking for exists out there, or if I perhaps need to develop it. </p>

<p>I have a program that collect text from different files, the text is extremely random and talks about different things. Each file contains a paragraph or 3 maximum, my program opens the files and store them into a table. </p>

<p>My question is, can i guess tags of what the paragraph is about? if anyone knows of an existing technology or approach, I would really appreciate it. </p>

<p>Thanks,</p>
",Dataset Preprocessing & Handling,guess tag paragraph programmatically using python trying read nlp general nltk specific use python know sure looking exists perhaps need develop program collect text different file text extremely random talk different thing file contains paragraph maximum program open file store table question guess tag paragraph anyone know existing technology approach would really appreciate thanks
Reaching an appropriate balance between performance and scalability in a large database,"<p>I'm trying to determine which of the many database models would best support probabilistic record comparison. Specifically, I have approximately 20 million documents defined by a variety of attributes (name, type, author, owner, etc.). Text attributes dominate the data set, yet there are still plenty of images. Read operations are the most crucial vis-a-vis performance, but I expect roughly 20,000 new documents to insert each week. Luckily, insert speed does not matter at all, and I am comfortable queuing the incoming documents for controlled processing.</p>

<p>Database queries will most typically take the following forms:</p>

<ul>
<li><code>Find documents containing at least five sentences that reference someone who'a a member of the military</code></li>
<li><code>Predict whether User A will comment on a specific document written by User B, given User A's entire comment history</code></li>
<li><code>Predict an author for Document X by comparing vocabulary, word ordering, sentence structure, and concept flow</code></li>
</ul>

<p>My first thought was to use a simple <a href=""http://en.wikipedia.org/wiki/Document-oriented_database"" rel=""nofollow"">document store</a> like, like <a href=""http://www.mongodb.org/"" rel=""nofollow"">MongoDB</a>, since each document does not necessarily contain the same data. However, complex queries effectively degrade this to a file system wrapper, since I cannot construct a query yielding the results I desire. As such, this approach corners me into walking the entire database and processing each file separately. Although document stores scale well horizontally, the benefits are not realized here.</p>

<p>This led me to realize that my granularity <em>isn't</em> at the document level, but rather the <em>entity-relationship</em> level. As such, <a href=""http://en.wikipedia.org/wiki/Graph_database"" rel=""nofollow"">graph databases</a> seemed like logical choice, since they facilitate relating each word in a sentence to the next word, next paragraph, current paragraph, part of speech, etc. Graph databases limit data replication, increase the speed of statistical clustering, and scale horizontally, among other things. Unfortunately, ensuring a definitive answer to your query still necessitates traversing the entire graph. Even still, indexing will help with performance.</p>

<p>I've also evaluated the use of relational databases, which are very efficient when designed properly (i.e., by avoiding unnecessary normalization). A relational database excels at finding all documents authored by User A, but fails at structural comparisons (which involves expensive joins). Relational databases also enforce constraints (primary keys, foreign keys, uniqueness, etc.) efficiently--a task with which some NoSQL solutions struggle.</p>

<p><strong>After considering the above-listed requirements, are there any database models that combine the ""exactness"" of relational models (<em>viz.</em>, efficient exhaustion of the domain) with the flexibility of graph databases?</strong></p>
",Dataset Preprocessing & Handling,reaching appropriate balance performance scalability large database trying determine many database model would best support probabilistic record comparison specifically approximately million document defined variety attribute name type author owner etc text attribute dominate data set yet still plenty image read operation crucial vi vi performance expect roughly new document insert week luckily insert speed doe matter comfortable queuing incoming document controlled processing database query typically take following form first thought wa use simple document store like like mongodb since document doe necessarily contain data however complex query effectively degrade file system wrapper since construct query yielding result desire approach corner walking entire database processing file separately although document store scale well horizontally benefit realized led realize granularity document level rather entity relationship level graph database seemed like logical choice since facilitate relating word sentence next word next paragraph current paragraph part speech etc graph database limit data replication increase speed statistical clustering scale horizontally among thing unfortunately ensuring definitive answer query still necessitates traversing entire graph even still indexing help performance also evaluated use relational database efficient designed properly e avoiding unnecessary normalization relational database excels finding document authored user fails structural comparison involves expensive join relational database also enforce constraint primary key foreign key uniqueness etc efficiently task nosql solution struggle considering listed requirement database model combine exactness relational model viz efficient exhaustion domain flexibility graph database
Loading a file into main memory,"<p>I have this text file that contains a list of words and a bunch of scores associated with each of them. I want to read the words off the file and the words and use them in my project. The code is in Java and the file itself is around 13MB. When I try to read file it takes around 15 minutes to complete, sometime even more. Any ideas on how I should optimize or approach this problem from scratch.</p>

<p>The operations being performed on file are - 
1. Read each line (Around 50 chars on the average case)
2. Split into tokens using a single whitespace (Around 10 tokens in the average case)
3. Store the generated tokens in an array (Nothing more at this stage)</p>

<p>EDIT: Sorry for not posting my code before, it just slipped my mind. <a href=""http://pastie.org/3646388"" rel=""nofollow"">http://pastie.org/3646388</a>
There is an line there where I loop to 10000 values which was to see if I got any output as opposed to looping till EOF.</p>
",Dataset Preprocessing & Handling,loading file main memory text file contains list word bunch score associated want read word file word use project code java file around mb try read file take around minute complete sometime even idea optimize approach problem scratch operation performed file read line around char average case split token using single whitespace around token average case store generated token array nothing stage edit sorry posting code slipped mind line loop value wa see got output opposed looping till eof
&quot;pre-built&quot; matrices for latent semantic analysis,"<p>I want to use Latent Semantic Analysis for a small app I'm building, but I don't want to build up the matrices myself. (Partly because the documents I have wouldn't make a very good training collection, because they're kinda short and heterogeneous, and partly because I just got a new computer and I'm finding it a bitch to install the linear algebra and such libraries I would need.)</p>

<p>Are there any ""default""/pre-built LSA implementations available? For example, things I'm looking for include:</p>

<ul>
<li>Default U,S,V matrices (i.e., if D is a term-document matrix from some training set, then D = U S V^T is the singular value decomposition), so that given any query vector q, I can use these matrices to compute the LSA projection of q myself.</li>
<li>Some black-box LSA algorithm that, given a query vector q, returns the LSA projection of q.</li>
</ul>
",Dataset Preprocessing & Handling,pre built matrix latent semantic analysis want use latent semantic analysis small app building want build matrix partly document make good training collection kinda short heterogeneous partly got new computer finding bitch install linear algebra library would need default pre built lsa implementation available example thing looking include default u v matrix e term document matrix training set u v singular value decomposition given query vector q use matrix compute lsa projection q black box lsa algorithm given query vector q return lsa projection q
Python vs Java for natural language processing,"<p>I have been working on java to find the similarity between two documents. I prefer finding semantic similarity , but havent made efforts to find it yet . I am using the following approach . </p>

<ol>
<li>Extract terms / tokens (I am using JAWS with wordnet to remove synonyms thus improves the similarities )</li>
<li>make a term document matrix </li>
<li>LSA </li>
<li>Cosine similarity </li>
</ol>

<p>When i was looking at few stackoverflow pages , i got quite a few links to python implementations. </p>

<p>I would like to know if python is a better language to find the text similarity and would also like to know if i can find semantic similairty between two documents in python  </p>
",Dataset Preprocessing & Handling,python v java natural language processing working java find similarity two document prefer finding semantic similarity havent made effort find yet using following approach extract term token using jaw wordnet remove synonym thus improves similarity make term document matrix lsa cosine similarity wa looking stackoverflow page got quite link python implementation would like know python better language find text similarity would also like know find semantic similairty two document python
remove duplicates matched on 3 columns,"<p>I have a large data frame with the following fields (example data).</p>

<p><code>#dput(data) gives...</code></p>

<p><code>data &lt;- structure(list(idNum = 1:11, personID = c(111L, 112L, 113L, 113L, 
111L, 112L, 114L, 112L, 111L, 113L, 115L), Name = c(""PETER PAN"", 
""RUPERT BEAR"", ""LONG JOHN SILVER"", ""SILVER LONG JOHN"", ""PAN PETER"", 
""BEAR RUPERT"", ""R BEAR"", ""RUPERT BEAR"", ""PETER PAN"", ""LONG J SILVER"", 
""LJ SILVER ""), DOB = c(""1/01/2001"", ""2/01/2001"", ""3/01/2001"", 
""3/01/2001"", ""1/01/2001"", ""2/01/2001"", ""10/01/2001"", ""2/01/2001"", 
""1/01/2001"", ""1/01/2001"", ""5/01/2001""), date = c(""12/01/2012"", 
""12/01/2012"", ""14/01/2012"", ""12/01/2012"", ""14/01/2012"", ""11/01/2012"", 
""10/01/2012"", ""16/01/2012"", ""10/01/2012"", ""16/01/2012"", ""10/01/2012""
), colour = c(""RED"", ""BLUE"", ""RED"", ""GREEN"", ""YELLOW"", ""BLUE"", 
""RED"", ""BLUE"", ""ORGANGE"", ""BLUE"", ""ORANGE""), firstName = c(""PETER"", 
""RUPERT"", ""LONG"", ""SILVER"", ""PAN"", ""BEAR"", ""R"", ""RUPERT"", ""PETER"", 
""LONG"", ""LJ""), lastName = c(""PAN"", ""BEAR"", ""SILVER"", ""JOHN"", 
""PETER"", ""RUPERT"", ""BEAR"", ""BEAR"", ""PAN"", ""SILVER"", ""SILVER"")), .Names = c(""idNum"", 
""personID"", ""Name"", ""DOB"", ""date"", ""colour"", ""firstName"", ""lastName""
), row.names = c(NA, -11L), class = ""data.frame"")</code></p>

<p>The firstName and lastName are not in the original data. The name format in the original data set are generated with a free format entry system. It contains a large number of foreign names so data entry clerks do not accurately collect first name and last name. I derived them using:</p>

<pre><code>data$firstName &lt;-sapply(strsplit(data$Name, split="" ""), head, 1)
data$lastName &lt;- sapply(strsplit(data$Name, split="" ""), tail, 1)
</code></pre>

<p>What I need to achieve is a subset data frame that removes duplicates  matched on personID, Name and DOB such that the value returned contains the most entries with the most recent date for each unique case.</p>

<p>That is, I would like to return rows 5, 7, 8, 10 and 11.</p>

<p>I separated first name and last name because I envisaged that it would work by initially extracting cases where <code>lastName == firstName</code> then ording by date. I then tough that I could use  case where lastName was in firstName and other consitions were met.</p>

<p>None if this worked and now I am lost.</p>

<p>Is there a relatively simple way to remove duplicates matched on columns personID, Name and DOB retaining the most recent unique cases?</p>

<p>Many thanks in advance.</p>
",Dataset Preprocessing & Handling,remove duplicate matched column large data frame following field example data firstname lastname original data name format original data set generated free format entry system contains large number foreign name data entry clerk accurately collect first name last name derived using need achieve subset data frame remove duplicate matched personid name dob value returned contains entry recent date unique case would like return row separated first name last name envisaged would work initially extracting case ording date tough could use case lastname wa firstname consitions met none worked lost relatively simple way remove duplicate matched column personid name dob retaining recent unique case many thanks advance
Algorithm (or C# library) for identifying &#39;keywords&#39; in a set of messages?,"<p>I want to build a list of ~6 keywords (or even better: couple word keyphrases) for each message in a message forum.</p>

<ul>
<li>The primary use of keywords is to replace subject lines in some instances. For example: <em>Message from Terry sent Dec 5, keywords: <strong>norweigan blue, plumage, not dead</em></strong></li>
<li>In a super ideal world keywords would identify both unique phases, and phrases that cluster the discussion into ""topics"", i.e. words that are highly relevant to the message in question, and a few other messages in the forum, but not found frequently in the forum as a whole.</li>
<li>I expect junk phrases to show up, no big deal.</li>
<li>Can't be too computationally expensive: I need something that can handle several hundred messages in several seconds, as I'll need to re-run this every time a new message comes in.</li>
</ul>

<p>Anyone know a good C# library for accomplishing this? Maybe there's a way to bend Lucene.NET into providing this sort of info?</p>

<p>Or, failing that, can anyone suggest an algorithm (or set of algos) to read up on? If I'm implementing myself I need something not terribly complex, I can only tackle this if its tractable in about a week. Right now, the best I've found in terms of simple-but-effective is <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow"">TF-IDF</a>.</p>

<p><strong>UPDATE:</strong> I've uploaded the results of using TF-IDF to select the top 5 keywords from a real dataset here: <a href=""http://jsbin.com/oxanoc/2/edit#preview"" rel=""nofollow"">http://jsbin.com/oxanoc/2/edit#preview</a> </p>

<p>The results are mediocre, but not totally useless... maybe with the addition of detecting multi-word phrases, this would be good enough.</p>
",Dataset Preprocessing & Handling,algorithm c library identifying keywords set message want build list keywords even better couple word keyphrases message message forum primary use keywords replace subject line instance example message terry sent dec keywords norweigan blue plumage dead super ideal world keywords would identify unique phase phrase cluster discussion topic e word highly relevant message question message forum found frequently forum whole expect junk phrase show big deal computationally expensive need something handle several hundred message several second need run every time new message come anyone know good c library accomplishing maybe way bend lucene net providing sort info failing anyone suggest algorithm set algos read implementing need something terribly complex tackle tractable week right best found term simple effective tf idf update uploaded result using tf idf select top keywords real dataset result totally useless maybe addition detecting multi word phrase would good enough
Exact phrase search using lucene without increasing number of fields,"<p>For a phrase search, we want to bring up results only if there's an exact match (without ignoring stopwords). If it's a non-phrase search, we are fine displaying results even if the root form of the word matches etc.</p>

<p>We currently pass our data through standardTokenizer, StopFilter, PorterStemFilter and LowerCaseFilter. Due to this when user wants to search for ""password management"", search brings up results containing ""password manager"".</p>

<p>If I remove StemFilter, then I will not be able to match for the root form of the word for non-phrase queries. I was thinking if I should index the same data as part of two fields in document.</p>

<p>I have asked same question at <a href=""https://stackoverflow.com/questions/8292446/exact-phrase-search-using-lucene"">Different indexing and search strategies on same field without doubling index size?</a>. However folks at office are not happy about indexing the same data as part of two fields. (we currently have around 20 text fields in lucene document). Is there any way to support both the cases I listed above using TokenFilters?</p>

<p>Say, for a StopFilter, make changes so that it emits both the input token and ? (for ignored word) with same position increments. Similarly for StemFilter, it emits both the input token and stemmed token with same position increments. Basically input and output tokens (even ignored ones) have same positions.</p>

<p>Is it safe to go ahead with this approach? Has anyone else faced the requirements listed here? Are there any Filters readily available which do something similar to what I mentioned in my approach?</p>

<p>Thanks</p>
",Dataset Preprocessing & Handling,exact phrase search using lucene without increasing number field phrase search want bring result exact match without ignoring stopwords non phrase search fine displaying result even root form word match etc currently pas data standardtokenizer stopfilter porterstemfilter lowercasefilter due user want search password management search brings result containing password manager remove stemfilter able match root form word non phrase query wa thinking index data part two field document asked question href indexing search strategy field without doubling index size however folk office happy indexing data part two field currently around text field lucene document way support case listed using tokenfilters say stopfilter make change emits input token ignored word position increment similarly stemfilter emits input token stemmed token position increment basically input output token even ignored one position safe go ahead approach ha anyone else faced requirement listed filter readily available something similar mentioned approach thanks
"Is vim able to detect the natural language of a file, then load the correct dictionary?","<p>I am using several languages, and currently I am obliged to indicate to vim with which of these the spell check must be done. Is there a way to set up vim so that it automatically detects the correct one? I vaguely remember that in a previous version of vim, when the spell check was not integrated, the vimspell script made this possible.</p>

<p>It would be even better if this could apply not only to a file but also to a portion of a file, since I frequently mix several languages in a single file. Of course, I would like to avoid to load several dictionaries simultaneously.</p>
",Dataset Preprocessing & Handling,vim able detect natural language file load correct dictionary using several language currently obliged indicate vim spell check must done way set vim automatically detects correct one vaguely remember previous version vim spell check wa integrated vimspell script made possible would even better could apply file also portion file since frequently mix several language single file course would like avoid load several dictionary simultaneously
"NLP, algorithms for determining if block of text is &quot;similar&quot; to other (after already having matched for keyword)","<p>I've been reading up on NLP as much as I can and searching on here but haven't found anything that seems to address exactly what I am trying to do. I am pretty new to NLP, only having had some minor exposure before, so far I have gotten the NLP processor I'm using working to where I am able to extract the POS from the text.</p>

<p>I am just working with a small sample document and then with one ""input phrase"" that I am basically trying to find a match for. The code I've written so far basically does this:</p>

<ul>
<li>takes the input phrase and the ""searchee (document being searched on)"" and breaks them down into Lists of individual words, then also gets the POS for each word. User also puts in one kewyord that is in the input phrase (and should be in doc being searched)</li>
<li>both Lists are searched for the keyword that the user input, then, for the first place this keyword is found in each document, a set number of words before and after are taken (such as 5). These are put into a dataset for processing, so if one article had:</li>
</ul>

<p>keyword: football</p>

<p>""A lot of sports are fun, football is a great, yet very physical sport.""
- Then my process would truncate this down to ""are fun, football is a""</p>

<p>My goal is to compare the pieces, such as the ""are fun, football is a"" for similarity as far as if they are likely to be used in a similar context, etc.</p>

<p>I'm wondering if anyone can point me in the right direction as far as patterns that could be used for this, algorithms, etc. The example above is simplistic, just to give an idea, but I would be planning to make this more complex if I can find the right place to learn more about this. Thanks for any info</p>
",Dataset Preprocessing & Handling,nlp algorithm determining block text similar already matched keyword reading nlp much searching found anything seems address exactly trying pretty new nlp minor exposure far gotten nlp processor using working able extract po text working small sample document one input phrase basically trying find match code written far basically doe take input phrase searchee document searched break list individual word also get po word user also put one kewyord input phrase doc searched list searched keyword user input first place keyword found document set number word taken put dataset processing one article keyword football lot sport fun football great yet physical sport process would truncate fun football goal compare piece fun football similarity far likely used similar context etc wondering anyone point right direction far pattern could used algorithm etc example simplistic give idea would planning make complex find right place learn thanks info
How to Join Arabic letters to form words,"<p>I have to read arabic letters from xml file and display them as a word </p>

<p>input :س ع ا د ة
output :سعادة look like that ..</p>

<p>I dont know how do that in any language , what algorithm to read, I need some start point to acomplish this task </p>

<p>I am also not sure if i have added the right tags, please free to make changes.</p>
",Dataset Preprocessing & Handling,join arabic letter form word read arabic letter xml file display word input output look like dont know language algorithm read need start point acomplish task also sure added right tag please free make change
Reconstructing now-famous 17-year-old&#39;s Markov-chain-based information-retrieval algorithm &quot;Apodora&quot;,"<p>While we were all twiddling our thumbs, a 17-year-old Canadian boy has apparently found an information retrieval algorithm that: </p>

<p>a) performs with twice the precision of the current, and widely-used vector space model</p>

<p>b) is 'fairly accurate' at identifying similar words. </p>

<p>c) makes microsearch more accurate</p>

<p>Here is a good <a href=""http://www.theglobeandmail.com/news/technology/science-fair-gold-medalist-17-invents-better-way-to-search-internet/article2118962/"" rel=""noreferrer"">interview</a>.</p>

<p>Unfortunately, there's no published paper I can find yet, but, from the snatches I remember from the graphical models and machine learning classes I took a few years ago, I think we should be able to reconstruct it from his submision abstract, and what he says about it in interviews.</p>

<p>From interview:</p>

<blockquote>
  <p>Some searches find words that appear in similar contexts. That’s
  pretty good, but that’s following the relationships to the first
  degree. My algorithm tries to follow connections further. Connections
  that are close are deemed more valuable. In theory, it follows
  connections to an infinite degree.</p>
</blockquote>

<p>And the abstract puts it in context:</p>

<blockquote>
  <p>A novel information retrieval algorithm called ""Apodora"" is introduced,
  using limiting powers of Markov chain-like matrices to determine
  models for the documents and making contextual statistical inferences
  about the semantics of words. The system is implemented and compared
  to the vector space model. Especially when the query is short, the
  novel algorithm gives results with approximately twice the precision
  and has interesting applications to microsearch.</p>
</blockquote>

<p>I feel like someone who knows about markov-chain-like matrices or information retrieval would immediately be able to realize what he's doing. </p>

<p>So: what is he doing?</p>
",Dataset Preprocessing & Handling,reconstructing famous year old markov chain based information retrieval algorithm apodora twiddling thumb year old canadian boy ha apparently found information retrieval algorithm performs twice precision current widely used vector space model b fairly accurate identifying similar word c make microsearch accurate good interview unfortunately published paper find yet snatch remember graphical model machine learning class took year ago think able reconstruct submision abstract say interview interview search find word appear similar context pretty good following relationship first degree algorithm try follow connection connection close deemed valuable theory follows connection infinite degree abstract put context novel information retrieval algorithm called apodora introduced using limiting power markov chain like matrix determine model document making contextual statistical inference semantics word system implemented compared vector space model especially query short novel algorithm give result approximately twice precision ha interesting application microsearch feel like someone know markov chain like matrix information retrieval would immediately able realize
Clustering conceptually similar documents together?,"<p>This is more of a conceptual question than an actual implementation and am hoping someone could clarify. My goal is the following: Given a set of documents, I want to cluster them such that documents belonging to the same cluster have the same ""concept"". </p>

<p>From what I understand, <a href=""http://en.wikipedia.org/wiki/Latent_semantic_analysis"" rel=""nofollow noreferrer"">Latent Semantic Analysis</a> lets me find a low rank approximation of a term-document matrix i.e. given a matrix <strong>X</strong>, it will decompose <strong>X</strong> as a product of three matrices, out of which one would be a diagonal matrix <strong>Σ</strong>:</p>

<p><img src=""https://i.sstatic.net/Ejhy5.png"" alt=""SVD""></p>

<p>Now, I would proceed by choosing a low rank approximation i.e. choose only the top-k values from <strong>Σ</strong>, and then calculate <strong>X'</strong>. Once I have this matrix, I have to apply some clustering algorithm and the end result would be set of clusters grouping documents with similar concepts. Is this the right way of applying clustering? I mean, calculating <strong>X'</strong> and then applying clustering on top of it or is there some other method that is followed?</p>

<p>Also, in a somewhat <a href=""https://stackoverflow.com/questions/5751114/nearest-neighbors-in-high-dimensional-data"">related question</a> of mine, I was told that the meaning of a <em>neighbor</em> is lost as the number of dimensions increases. In that case, what is the justification for clustering these high dimensional data points from <strong>X'</strong>? I am guessing that the requirement to cluster similar documents is a real-world requirement in which case, how does one go about addressing this?</p>
",Dataset Preprocessing & Handling,clustering conceptually similar document together conceptual question actual implementation hoping someone could clarify goal following given set document want cluster document belonging cluster concept understand latent semantic analysis let find low rank approximation term document matrix e given matrix x decompose x product three matrix one would diagonal matrix would proceed choosing low rank approximation e choose top k value calculate x matrix apply clustering algorithm end result would set cluster grouping document similar concept right way applying clustering mean calculating x applying clustering top method followed also somewhat href question mine wa told meaning neighbor lost number dimension increase case justification clustering high dimensional data point x guessing requirement cluster similar document real world requirement case doe one go addressing
Word lists for a lot of articles - document-term matrix,"<p>I have nearly 150k articles in Turkish. I will use articles for natural language processing research.
I want to store words and frequency of them per article after processing articles.</p>

<p>I'm storing them in RDBS now.</p>

<p>I have 3 tables:</p>

<p>Articles -> article_id,text<br>
Words -> word_id, type, word<br>
Words-Article -> id, word_id, article_id, frequency (index for word_id, index for article_id )  </p>

<p>I will query for  </p>

<ul>
<li>ALL Words in an article   </li>
<li>one Word's frequency per article  </li>
<li>Word occurrences in all articles and in which articles</li>
</ul>

<p>I have millions of rows in words-article table. I always worked with RDBS in this project. started with mysql and using oracle now. But I don't want to use oracle and want better performance than mysql.</p>

<p>Also I have to handle this job in a machine with 4gb ram.<br>
Simply, how to store document-term matrix and make some query on it? performance is necessary. can ""key-value databases"" beat mysql at performance?  or what can beat mysql?</p>

<p>if your answer programming language depended, I'm writing code in python. But C/C++ , Java is ok. </p>
",Dataset Preprocessing & Handling,word list lot article document term matrix nearly k article turkish use article natural language processing research want store word frequency per article processing article storing rdbs table article article id text word word id type word word article id word id article id frequency index word id index article id query word article one word frequency per article word occurrence article article million row word article table always worked rdbs project started mysql using oracle want use oracle want better performance mysql also handle job machine gb ram simply store document term matrix make query performance necessary key value database beat mysql performance beat mysql answer programming language depended writing code python c c java ok
Text Mining to extract animal types from text,"<p>I need to do an experiment and I am new in NLP. I have read books that explain the theoritical issues but when it comes to practical I found it hard to find a guide. so please who knows anything in NLP especially the practical issues tell me and point me to the right path because I feel I am lost  (useful books, useful tools and useful websites)</p>

<p>what I am trying to do is to take a text and find specific words for example animals such as dogs, cats,...etc in it then I need to extract this word and 2 words on each side. 
For example </p>

<pre><code>I was watching TV with my lovely cat last night.
</code></pre>

<p>the extracted text will be </p>

<pre><code>(my lovely cat last night)
</code></pre>

<p>This will be my training example to the machine tool</p>

<p>Q1: there will be around 100 training examples similar to what I explained above. I used tocknizer to extracts words but how can I extract specific words(for our example all types of animals) with 2 words on each side. do I need to use tags for example or what is your idea?</p>

<p>Q2: If I have these training examples how can I prepare appropriate datasets that I can give it to the machine tool to train it? what should I write in this dataset to specify the animal and should I need to give other features? and how can I arrange it in a dataset .</p>

<p>many words from you might help me a lot please do not hesitate to tell what you know</p>
",Dataset Preprocessing & Handling,text mining extract animal type text need experiment new nlp read book explain theoritical issue come practical found hard find guide please know anything nlp especially practical issue tell point right path feel lost useful book useful tool useful website trying take text find specific word example animal dog cat etc need extract word word side example extracted text training example machine tool q around training example similar explained used tocknizer extract word extract specific word example type animal word side need use tag example idea q training example prepare appropriate datasets give machine tool train write dataset specify animal need give feature arrange dataset many word might help lot please hesitate tell know
reading # char in python,"<p>can someone help with me reading ""#"" char in python? i can't seem to get the file. because this is an output from the stanford postagger, is there any scripts available to convert the stanford postagger <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tagger.shtml</a> file to cwb. <a href=""http://cogsci.uni-osnabrueck.de/~korpora/ws/CWBdoc/CWB_Encoding_Tutorial/node3.html"" rel=""nofollow"">http://cogsci.uni-osnabrueck.de/~korpora/ws/CWBdoc/CWB_Encoding_Tutorial/node3.html</a></p>

<p>so this is the utf-8 txt file that i'm trying to read:</p>

<pre><code> 如果#CS 您#PN 在#P 新加坡#NR 只#AD 能#VV 前往#VV 一#CD 间#M 俱乐部#NN ，#PU 祖卡#NN 酒吧#NN 必然#AD 是#VC 您#PN 的#DEG 不二#JJ 选择#NN 。#PU
    作为#P 或许#AD 是#VC 新加坡#NR 唯一#JJ 一#CD 家#M 国际#NN 知名#VA 的#DEC 夜店#NN ，#PU 祖卡#NN 既#CC 是#VC 一#CD 个#M 公共#JJ 机构#NN ，#PU
</code></pre>

<p>So with this code i'm not readin the # char in the utf-8 txt files:</p>

<pre><code>#!/usr/bin/python # -*- coding: utf-8 -*-

'''
stanford POS tagger to CWB format
'''

import codecs
import nltk
import os, sys, re, glob

reload(sys)
sys.setdefaultencoding('utf-8')

cwd = './path/to/file.txt' #os.getcwd()

for infile in glob.glob(os.path.join(cwd, 'zouk.txt')):
        print infile
        (PATH, FILENAME) = os.path.split(infile)
        reader = codecs.open(infile, 'r', 'utf-8')
        for line in reader:
                for word in line:
                        if word == '\#':
                                print 'hex is here'
</code></pre>
",Dataset Preprocessing & Handling,reading char python someone help reading char python seem get file output stanford postagger script available convert stanford postagger file cwb utf txt file trying read code readin char utf txt file
Hashtables over large natural language word sets,"<p>I'm writing a program in python to do a unigram (and eventually bigram etc) analysis of movie reviews.  The goal is to create feature vectors to feed into libsvm.  I have 50,000 odd unique words in my feature vector (which seems rather large to me, but I ham relatively sure I'm right about that).</p>

<p>I'm using the python dictionary implementation as a hashtable to keep track of new words as I meet them, but I'm noticing an enormous slowdown after the first 1000 odd documents are processed.  Would I have better efficiency (given the distribution of natural language) if I used several smaller hashtable/dictionaries or would it be the same/worse?</p>

<p>More info:</p>

<p>The data is split into 1500 or so documents, 500-ish words each.  There are between 100 and 300 unique words (with respect to all previous documents) in each document.</p>

<p>My current code:</p>

<pre><code>#processes each individual file, tok == filename, v == predefined class
def processtok(tok, v):
    #n is the number of unique words so far, 
    #reference is the mapping reference in case I want to add new data later
    #hash is the hashtable
    #statlist is the massive feature vector I'm trying to build
    global n
    global reference
    global hash
    global statlist
    cin=open(tok, 'r')
    statlist=[0]*43990
    statlist[0] = v
    lines = cin.readlines()
    for l in lines:
        line = l.split("" "")
        for word in line:
            if word in hash.keys():
                if statlist[hash[word]] == 0:
                    statlist[hash[word]] = 1
            else:
                hash[word]=n
                n+=1
                ref.write('['+str(word)+','+str(n)+']'+'\n')
                statlist[hash[word]] = 1
    cin.close()
    return statlist
</code></pre>

<p>Also keep in mind that my input data is about 6mb and my output data is about 300mb.  I'm simply startled at how long this takes, and I feel that it shouldn't be slowing down so dramatically as it's running.</p>

<p>Slowing down: the first 50 documents take about 5 seconds, the last 50 take about 5 minutes.</p>
",Dataset Preprocessing & Handling,hashtables large natural language word set writing program python unigram eventually bigram etc analysis movie review goal create feature vector feed libsvm odd unique word feature vector seems rather large ham relatively sure right using python dictionary implementation hashtable keep track new word meet noticing enormous slowdown first odd document processed would better efficiency given distribution natural language used several smaller hashtable dictionary would worse info data split document ish word unique word respect previous document document current code also keep mind input data mb output data mb simply startled long take feel slowing dramatically running slowing first document take second last take minute
Get words corresponding to a match from SpanNearQuery in Lucene,"<p>I would need to retrieve the words in my text that correspond to a match of Spans returned by SpanNearQuery.getSpans(). For instance, if my text is [a b c d e f] and I use SpanNearQueries with queries 'b' and 'e' (and sufficient slop), then I get a match 'b c d e' in my text. Now, how can I most efficiently retrieve the words as they appear in the match, that is, the sequence of words 'b c d e' itself?</p>

<p>Here is an example code of what I would need:</p>

<pre><code>SpanNearQuery allNear = new SpanNearQuery(spansTermQueries, numWordsInBetween, true);
Spans allSpans = allNear.getSpans(reader);
</code></pre>

<p>Now I would like to iterate over all the matches in allSpans, and for each match retrieve the exact words between the queries 9 the text that correspond to that match.</p>

<p>One indirect way is to get the end and start position of that match, read through the text document using a file reader, and find the string of text between position 'end' and 'start'. But that does not seem a very efficient way. It seems that this information should already be stored in the Lucene Index.</p>

<p>Would anyone know of a more direct way of retrieving the words between the queries in a match?</p>

<p>Thanks.</p>
",Dataset Preprocessing & Handling,get word corresponding match spannearquery lucene would need retrieve word text correspond match span returned spannearquery getspans instance text b c e f use spannearqueries query b e sufficient slop get match b c e text efficiently retrieve word appear match sequence word b c e example code would need would like iterate match allspans match retrieve exact word query text correspond match one indirect way get end start position match read text document using file reader find string text position end start doe seem efficient way seems information already stored lucene index would anyone know direct way retrieving word query match thanks
Medical information extraction using Python,"<p>I am a nurse and I know python but I am not an expert, just used it to process DNA sequences<br>
We got hospital records written in human languages and I am supposed to insert these data into a database or csv file but they are more than 5000 lines and this can be so hard. All the data are written in a consistent format let me show you an example</p>

<pre><code>11/11/2010 - 09:00am : He got nausea, vomiting and died 4 hours later
</code></pre>

<p>I should get the following data</p>

<pre><code>Sex: Male
Symptoms: Nausea
    Vomiting
Death: True
Death Time: 11/11/2010 - 01:00pm
</code></pre>

<p>Another example</p>

<pre><code>11/11/2010 - 09:00am : She got heart burn, vomiting of blood and died 1 hours later in the operation room
</code></pre>

<p>And I get</p>

<pre><code>Sex: Female
Symptoms: Heart burn
    Vomiting of blood
Death: True
Death Time: 11/11/2010 - 10:00am
</code></pre>

<p>the order is not consistent by when I say in ....... so in is a keyword and all the text after is a place until i find another keyword<br>
At the beginnning He or She determine sex, got ........ whatever follows is a group of symptoms that i should split according to the separator which can be a comma, hypen or whatever but it's consistent for the same line<br>
died ..... hours later also should get how many hours, sometimes the patient is stil alive and discharged ....etc<br>
That's to say we have a lot of conventions and I think if i can tokenize the text with keywords and patterns i can get the job done. So please if you know a useful function/modules/tutorial/tool for doing that preferably in python (if not python so a gui tool would be nice)  </p>

<p>Some few information:</p>

<pre><code>there are a lot of rules to express various medical data but here are few examples
- Start with the same date/time format followed by a space followd by a colon followed by a space followed by He/She followed space followed by rules separated by and
- Rules:
    * got &lt;symptoms&gt;,&lt;symptoms&gt;,....
    * investigations were done &lt;investigation&gt;,&lt;investigation&gt;,&lt;investigation&gt;,......
    * received &lt;drug or procedure&gt;,&lt;drug or procedure&gt;,.....
    * discharged &lt;digit&gt; (hour|hours) later
    * kept under observation
    * died &lt;digit&gt; (hour|hours) later
    * died &lt;digit&gt; (hour|hours) later in &lt;place&gt;
other rules do exist but they follow the same idea
</code></pre>
",Dataset Preprocessing & Handling,medical information extraction using python nurse know python expert used process dna sequence got hospital record written human language supposed insert data database csv file line hard data written consistent format let show example get following data another example get order consistent say keyword text place find another keyword beginnning determine sex got whatever follows group symptom split according separator comma hypen whatever consistent line died hour later also get many hour sometimes patient stil alive discharged etc say lot convention think tokenize text keywords pattern get job done please know useful function module tutorial tool preferably python python gui tool would nice information
Building dictionary of words from large text,"<p>I have a text file containing posts in English/Italian. I would like to read the posts into a data matrix so that each row represents a post and each column a word. The cells in the matrix are the counts of how many times each word appears in the post. The dictionary should consist of all the words in the whole file or a non exhaustive English/Italian dictionary. </p>

<p>I know this is a common essential preprocessing step for NLP. And I know it's pretty trivial to code it, sill I'd like to use some NLP domain specific tool so I get stop-words trimmed etc..</p>

<p>Does anyone know of a tool\project that can perform this task?</p>

<p>Someone mentioned apache lucene, do you know if lucene index can be serialized to a data-structure similar to my needs?</p>
",Dataset Preprocessing & Handling,building dictionary word large text text file containing post english italian would like read post data matrix row represents post column word cell matrix count many time word appears post dictionary consist word whole file non exhaustive english italian dictionary know common essential preprocessing step nlp know pretty trivial code sill like use nlp domain specific tool get stop word trimmed etc doe anyone know tool project perform task someone mentioned apache lucene know lucene index serialized data structure similar need
